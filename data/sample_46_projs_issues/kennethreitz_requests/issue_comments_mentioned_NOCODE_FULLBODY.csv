issue_num,datetime,body,login,mention_login
3942,2017-03-27 21:27:30,"Hey @colekas-csc, this is intended functionality in Requests 2.13.0. When Requests receives a redirect, it strips the payload to prevent unintentional transfer of information to the wrong server. We're a tad overzealous with this right now which is why this behavior will be modified in 3.0.0, but for now we must maintain this for backwards compatibility.

Thanks for taking the time to check in on this though!",nateprewitt,colekas-csc
3940,2017-03-26 18:39:39,"@Lukasa Thanks for your comments!

I was able to resolve this by explicitly specifying the certificate like below:
os.environ[""REQUESTS_CA_BUNDLE""] = ""F:/emulator_certificate.cer""

I wanted to avoid checking-in this certificate in my file system for running my automated tests. Is there a way in which Python can look into my Windows Certification Manager store to see if this certificate is installed and use it rather than me having to explicitly export it and provide it as above?
",rnagpal,Lukasa
3940,2017-03-27 04:24:40,"@Lukasa Thanks for the suggestion!
I went ahead and used wincertstore and it works without me having to extract and specify the certificate.",rnagpal,Lukasa
3939,2017-03-24 10:49:09,"Thanks @Lukasa here is the code that failing

",a-sharma11,Lukasa
3939,2017-03-24 11:01:17,"@Lukasa Yes it should be the same in this case(hardcoded shown below). Also I made one simple request using postman which should have only triggered this once and got the same error.
`self.url = base_url + ""/api/platform/v1/session""`

and `base_url` is const and in this case name of the other linked container",a-sharma11,Lukasa
3935,2017-03-23 08:29:52,"@MonsterDeveloper If you have an issue like #2955, have you tried examining the solution provided there? Can you tell me what server you're having trouble with? Can you tell us what version of Python and OpenSSL you're using? The reason is that Requests *does* trust LE certificates.",Lukasa,MonsterDeveloper
3935,2017-03-23 11:37:58,"@MonsterDeveloper I also cannot reproduce this, but there's also too many versions of Python, OpenSSL, and Requests to try to guess at your current problematic configuration. Please supply as much information as possible.",sigmavirus24,MonsterDeveloper
3928,2017-03-23 16:55:33,@kennethreitz perfectly reasonable to do it with 3.0,sigmavirus24,kennethreitz
3924,2017-03-15 08:37:08,"@Lukasa 
Python version: 
Requests version:
Other packages:


Here is my relative code: , and I also use gevent in my app.
",i2it,Lukasa
3924,2017-03-15 08:39:49,"@Lukasa Ok, I'll have a try.",i2it,Lukasa
3922,2017-03-14 15:42:26,"Yep, this is a bug in the last release of pipenv. There's a patch underway, thanks @Lukasa!",nateprewitt,Lukasa
3919,2017-03-08 17:03:16,"@Lukasa, are you sure this is fixed in 3.6? The redirect URL being passed back is encoded as utf-8 but we're decoding it as ISO-8859-1 which is what's causing the issues. I'm getting a `TooManyRedirect` exception when trying to use Python3.6 but the request works if I pass the properly decoded Location header: `http://uae.souq.com/ae-ar/ابل-ايفون-7-مع-فيس-تايم-32-جيجا-الجيل-الرابع-ال-تي-اي-ذهبي-11526690/i/`.",nateprewitt,Lukasa
3917,2017-03-08 08:38:25,"@fazalmajid So, we call into chardet when you ask us to give unicode data back, and the server hasn't given us an encoding to use. That is, if you call `response.text` and we don't know what encoding to use, we sample the data to try to work out what the correct encoding is.

You can circumvent this in two ways: either use `response.content`, which gives you back un-decoded binary data, or you can set `response.encoding` to whatever encoding you know out-of-band to be appropriate before calling `response.text`, which will cause us to just trust your choices.

Otherwise, there is very little we can do about this: Requests is and always will be a pure-Python module, so we cannot ship a C extension. We could optionally allow the use of cchardet, but ultimately the fastest thing to do here is to just not force us to guess at all. :)",Lukasa,fazalmajid
3913,2017-03-07 09:35:52,"Cool, this looks good. I'll wait for the build to go green and merge. Thanks so much @StyXman! :sparkles:",Lukasa,StyXman
3908,2017-03-04 20:56:49,"@Lukasa: Interesting—I'd have thought it's using the system's but apparently it's bundled one.



Would that be the cause?  FWIW, the calls to https://localhost:8080 validate successfully when using `REQUESTS_CA_BUNDLE=cert.pem`, it's everything else that fails.",dbazile,Lukasa
3908,2017-03-04 20:59:58,@dbazile are the cert chains for the other websites in `cert.pem`? If not then that is absolutely the expected behaviour.,sigmavirus24,dbazile
3908,2017-03-04 21:04:50,"@sigmavirus24: They're not in `cert.pem`.

But it's not the same behavior that I saw in Python 3.5 (which checked `REQUESTS_CA_BUNDLE` first and fell back on the system's trust store for everything else).

Is there a way to configure this previous behavior without `cat`ing the entire system's trust chain into cert.pem?",dbazile,sigmavirus24
3908,2017-03-04 21:12:13,"I think you're right; Python 3.5 didn't bundle OpenSSL but 3.6 does:



I think my solution is to grab and concat all the certs my code will be using into an actual bundle and use that instead.

Thank you much, @Lukasa @sigmavirus24!",dbazile,Lukasa
3908,2017-03-04 21:12:13,"I think you're right; Python 3.5 didn't bundle OpenSSL but 3.6 does:



I think my solution is to grab and concat all the certs my code will be using into an actual bundle and use that instead.

Thank you much, @Lukasa @sigmavirus24!",dbazile,sigmavirus24
3906,2017-03-04 13:47:22,"@Lukasa, while I think this should be closed (since it's not a Requests bug), I also wonder if the selectors code in urllib3 should attempt to fallback to less efficient methods in cases like this and only then let the OS error bubble up. Thoughts?",sigmavirus24,Lukasa
3906,2017-03-04 14:11:32,"@sigmavirus24 Yeah, so I said a few comments ago that the selectors module can detect this case by actually trying to *instantiate* the selectors, at least for some of them where the selector itself is an FD (we can't do it so easily for poll/select). That's why I tagged @SethMichaelLarson: this is a bit his baby.",Lukasa,sigmavirus24
3905,2017-03-02 06:32:45,"Hey @theyoprst, you're correct, this should have been resolved in urllib3 in shazow/urllib3#1039. Are you still experiencing this issue in the latest version of requests?",nateprewitt,theyoprst
3905,2017-03-02 06:52:45,"Yep, as it looks like you were referring to the latest version of Requests. This doesn't seem to be reproducible with 2.13.0 which has the patch included in urllib3. Thanks for taking the time to open this issue, @theyoprst!",nateprewitt,theyoprst
3904,2017-03-01 21:50:01,@Lukasa I was just going to that direction. Thanks.,brangi,Lukasa
3904,2017-03-02 00:51:39,"I'm going to close this, as this isn't an issue with Requests. Cheers @brangi @lutzhorn!",sigmavirus24,lutzhorn
3904,2017-03-02 00:51:39,"I'm going to close this, as this isn't an issue with Requests. Cheers @brangi @lutzhorn!",sigmavirus24,brangi
3902,2017-03-22 17:20:37,go for it @garywu!,kennethreitz,garywu
3901,2017-03-01 20:05:39,"Nice first commit, thanks @nedbat!",Lukasa,nedbat
3899,2017-03-01 08:23:34,"Requests will avoid aggressively handling your string if you do what @patallen suggests, which should resolve your problem, but @lutzhorn is right: the server should accept both, and is at fault here. :smile:",Lukasa,lutzhorn
3899,2017-03-01 08:23:34,"Requests will avoid aggressively handling your string if you do what @patallen suggests, which should resolve your problem, but @lutzhorn is right: the server should accept both, and is at fault here. :smile:",Lukasa,patallen
3899,2017-03-01 09:31:49,"Hi All

thank you for your help
@lutzhorn is true, and i have reported the problem as well, but i'm sure that is a bit difficult to solve from the server part, is Asterisk the open source PBX and have a very large list of issues.
@TetraEtc yes but the URL is into the internal network and have no external access

@Lukasa and @patallen , thank you i will use your suggestion, hope is work well

",ogonbat,lutzhorn
3899,2017-03-01 09:31:49,"Hi All

thank you for your help
@lutzhorn is true, and i have reported the problem as well, but i'm sure that is a bit difficult to solve from the server part, is Asterisk the open source PBX and have a very large list of issues.
@TetraEtc yes but the URL is into the internal network and have no external access

@Lukasa and @patallen , thank you i will use your suggestion, hope is work well

",ogonbat,TetraEtc
3899,2017-03-01 09:31:49,"Hi All

thank you for your help
@lutzhorn is true, and i have reported the problem as well, but i'm sure that is a bit difficult to solve from the server part, is Asterisk the open source PBX and have a very large list of issues.
@TetraEtc yes but the URL is into the internal network and have no external access

@Lukasa and @patallen , thank you i will use your suggestion, hope is work well

",ogonbat,Lukasa
3899,2017-03-01 09:31:49,"Hi All

thank you for your help
@lutzhorn is true, and i have reported the problem as well, but i'm sure that is a bit difficult to solve from the server part, is Asterisk the open source PBX and have a very large list of issues.
@TetraEtc yes but the URL is into the internal network and have no external access

@Lukasa and @patallen , thank you i will use your suggestion, hope is work well

",ogonbat,patallen
3897,2017-03-01 17:24:33,"@nateprewitt nice work on this, thanks for getting this over the finish line",davidsoncasey,nateprewitt
3897,2017-03-01 17:33:33,"@davidsoncasey, glad we're able to get all of your work merged, thanks again!

@Lukasa, 3.0-HISTORY is updated :)",nateprewitt,davidsoncasey
3895,2017-02-28 12:33:23,@toasteez Can you show me the proxy dictionary?,Lukasa,toasteez
3892,2017-02-27 16:23:12,"This looks good now, I think. @vpfautz if you feel like it, this can be patched in [urllib3](https://github.com/shazow/urllib3) too.

Also thanks for catching this! I shouldn't be typing without spell check in my IDE :)",nateprewitt,vpfautz
3891,2017-02-26 13:00:57,@SirCmpwn is it possible Amazon is relying on cookies set by JavaScript?,sigmavirus24,SirCmpwn
3888,2017-02-24 17:40:34,"Hi there @atleta,

First, RFC 2616 has been made obsolete by RFCs [7230][], [7231][], [7232][], [7233][], [7234][], and [7235][].

As you'll see when you familiarize yourself with those RFCs, the *default* encoding for Headers is no longer ISO-8859-1 (a.k.a., Latin-1).

I suspect that if you have a specific encoding you want to do you should disallow redirects and take control over URL encoding yourself. @Lukasa may disagree, though.

[7230]: https://tools.ietf.org/html/rfc7230
[7231]: https://tools.ietf.org/html/rfc7231
[7232]: https://tools.ietf.org/html/rfc7232
[7233]: https://tools.ietf.org/html/rfc7233
[7234]: https://tools.ietf.org/html/rfc7234
[7235]: https://tools.ietf.org/html/rfc7235",sigmavirus24,atleta
3888,2017-02-24 17:53:04,"@nateprewitt Nah, I think these are different issues. This is because requests *does* urlencode the redirect header, but it also *character* encodes it, wrongly.

But yeah, I'm prepared to believe that that example is a good example of how we get it wrong. Happily, ISO-8859-1 is round-trippable.",Lukasa,nateprewitt
3885,2017-02-23 21:15:26,@Lukasa thanks for your help with this. As for squashing up all of the commits this PR has accumulated--who should do that? Do the maintainers of this lib do that? Or I can squash locally and push to a new PR (or just force push).,davidfontenot,Lukasa
3881,2017-02-19 20:28:17,@nodakai that all is available in the response history already. I don't understand how the final response having a method would help you identify that. `response.request.method` already exists and (I thought) is documented.,sigmavirus24,nodakai
3881,2017-02-19 21:34:15,"Thanks for this request!

So I agree with @sigmavirus24. While the `response.url` field is a handy convenience, in general we make sure that the request that triggered a given response is always available at `response.request`, and can be introspected. 

If you didn't find that obvious, I'd be delighted to merge a pull request with clarifying documentation. ",Lukasa,sigmavirus24
3880,2017-02-18 13:40:07,"@bkline Ok so some thoughts that leap out to me looking at this capture.

Firstly, the server is responding with `Connection: keep-alive` and then sending a TCP RST immediately after. And I mean *immediately*, it arrives within 1µs of the 301 response. The packet is a RST because it doesn't even wait for the ACK response. That's really not how that's supposed to go: the webserver should really be sending `Connection: close` if it's planning to slam the door in our face like that. But nevermind, that's not where the problem is.

It also redirects you to a new host: cam.cancer.gov, on port 443. That is the actual host that is causing you trouble, I suspect, so it'd be helpful to get a capture that includes both so we can see where in the redirect the problem occurs.

So the question becomes, what exactly is going on here? My *suspicion* is that there is a problem in the TLS handshake, but without the packet capture it'll be hard to see. Mind giving that a shot?",Lukasa,bkline
3880,2017-03-01 16:02:44,Thanks for following up here @bkline! Good luck with the bug report: I hope your problem gets resolved soon! :sparkles:,Lukasa,bkline
3879,2017-02-17 08:37:48,@Lukasa what the highest version that python 2.6.6 supports?,duyanghao,Lukasa
3879,2017-02-17 08:43:20,"@Lukasa i have googled a lot for the error: `ConnectionError: [Errno 2] server failed`,but nothing has been found!
Do you have any idea about this strange error?",duyanghao,Lukasa
3879,2017-02-17 08:50:19,"the second exception:`ConnectionError: [Errno 3] name does not exist` maybe be related to domain name resolving,but the first exception:`ConnectionError: [Errno 2] server failed` is unrelated to the second exception and has nothing to do with domain name resolving.

@Lukasa do you have any idea about the first exception?",duyanghao,Lukasa
3879,2017-02-20 03:03:29,"@Lukasa Why Linux errno 2 `ENOENT` means having problems resolving the domain name,as far as i know,the `ENOENT`  means `No such file or directory`.
",duyanghao,Lukasa
3879,2017-02-20 08:22:29,"@Lukasa it is a mutil-process server,and the error occurs sometimes,so i am afraid it can't be straced easily!",duyanghao,Lukasa
3879,2017-02-20 08:40:16,"@Lukasa anyway,thanks for your response!

still,actually,there is another error:
12 SSLError: [Errno 185090050] _ssl.c:330: error:0B084002:x509 certificate routines:X509_load_cert_crl_file:system lib：
2017-02-20 10:59:31,890 ERROR: Exception on /v1/images/1b6c5dbe537403c916f6ea668632a05099a09cdde9a668543e4da2eab5a6b179/layer [GET]
Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/flask/app.py"", line 1687, in wsgi_app
    response = self.full_dispatch_request()
  File ""/usr/lib/python2.6/site-packages/flask/app.py"", line 1360, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/usr/lib/python2.6/site-packages/flask/app.py"", line 1358, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/usr/lib/python2.6/site-packages/flask/app.py"", line 1344, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/usr/lib/python2.6/site-packages/docker-registry/docker_registry/toolkit.py"", line 264, in wrapper
    return f(*args, **kwargs)
  File ""/usr/lib/python2.6/site-packages/docker-registry/docker_registry/images.py"", line 35, in wrapper
    return f(*args, **kwargs)
  File ""/usr/lib/python2.6/site-packages/docker-registry/docker_registry/images.py"", line 56, in wrapper
    return f(*args, **kwargs)
  File ""/usr/lib/python2.6/site-packages/docker-registry/docker_registry/lib/mirroring.py"", line 140, in wrapper
    flask.request.path, stream=stream, source=source
  File ""/usr/lib/python2.6/site-packages/docker-registry/docker_registry/lib/mirroring.py"", line 42, in lookup_source
    stream=stream
  File ""/usr/lib/python2.6/site-packages/requests/api.py"", line 55, in get
    return request('get', url, **kwargs)
  File ""/usr/lib/python2.6/site-packages/requests/api.py"", line 44, in request
    return session.request(method=method, url=url, **kwargs)
  File ""/usr/lib/python2.6/site-packages/requests/sessions.py"", line 279, in request
    resp = self.send(prep, stream=stream, timeout=timeout, verify=verify, cert=cert, proxies=proxies)
  File ""/usr/lib/python2.6/site-packages/requests/sessions.py"", line 374, in send
    r = adapter.send(request, **kwargs)
  File ""/usr/lib/python2.6/site-packages/requests/adapters.py"", line 213, in send
    raise SSLError(e)
SSLError: [Errno 185090050] _ssl.c:330: error:0B084002:x509 certificate routines:X509_load_cert_crl_file:system lib


",duyanghao,Lukasa
3877,2017-02-16 13:17:30,"@sigmavirus24 That PR was merged on the grounds that it was consistent with the docstrings that relied on that method. If we're changing one, we should change them all. =)",Lukasa,sigmavirus24
3877,2017-02-16 13:18:53,"@Lukasa which docstrings are those? I was under the impression that the docstring format was what was borrowed from, not the content of other docstrings.",sigmavirus24,Lukasa
3875,2017-02-15 03:03:54,"Hey @jamshid, I believe this is a problem with how you're using unicode in Python 2.7 rather than something requests specific. The repro below fails with the same exception without any requests code. If you pass properly encoded utf-8, it should solve this issue.



As for why it's failing on the second, that exception is raised when you try to encode the utf-8 encoded string a second time. It looks like AWS4Auth is doing that for you [here](https://github.com/sam-washington/requests-aws4auth/blob/master/requests_aws4auth/aws4auth.py#L494).

All of this points to a misuse of unicode in Python 2 and possibly a bug with how AWS4Auth interacts with unicode bodies. I don't believe there's much to be done here in requests.
",nateprewitt,jamshid
3875,2017-02-15 05:24:11,"Thanks, @nateprewitt! Turns out AWS4Auth is adding headers as unicode strings, but Python 2.7 httplib.py or `requests` do not expect that and fail when concatenating the (binary?) body string. There's an easy fix to AWS4Auth for it:
https://github.com/sam-washington/requests-aws4auth/issues/29",jamshid,nateprewitt
3874,2017-02-21 18:13:30,"I think it's good to go @jvanasco, we were just waiting for confirmation from @kennethreitz. @Lukasa we can probably just merge though since this isn't actually a change, yeah?",nateprewitt,Lukasa
3874,2017-02-21 18:13:30,"I think it's good to go @jvanasco, we were just waiting for confirmation from @kennethreitz. @Lukasa we can probably just merge though since this isn't actually a change, yeah?",nateprewitt,jvanasco
3873,2017-02-14 15:47:28,"Great! @Lukasa, when you have a free moment, would you mind merging master in proposed/3.0.0 so I can rebase my changes for #3338 onto this?",nateprewitt,Lukasa
3872,2017-02-14 14:08:20,"@Lukasa thanks for the feedback!

I understand the concerns for performance (I care more about being able to do one less request than the security aspects to be frank). I think we could use a disk-based format that makes the lookups fast and memory-efficient?",sylvinus,Lukasa
3872,2017-02-14 14:13:24,"@sylvinus So, the most appropriate way to do it would be to use a sqlite DB, but ultimately that's putting the cart before the horse. If we can get support for HSTS *without* preload that persists to disk, then essentially the preload list becomes nothing more than a priming of the HSTS DB.

Again, I recommend you look at the direction that was taken in shazow/urllib3#608 to get an idea of what we were looking at. If you're interested, I'm sure you can pick the code up and try to bring it back up to date.",Lukasa,sylvinus
3869,2017-02-12 07:33:08,"This is an annoyance, but looks right. Thanks @nateprewitt!",Lukasa,nateprewitt
3868,2017-02-11 21:22:03,@Lukasa of course. No rush. Thanks a lot!,vmalloc,Lukasa
3868,2017-02-12 07:40:38,@Lukasa will rebase and push again.,vmalloc,Lukasa
3866,2017-02-13 17:17:08,@jvanasco yes but please make that commit message easier on the eyes :),sigmavirus24,jvanasco
3864,2017-02-17 08:52:45,"@AraHaan The relevant code is [here](https://github.com/kennethreitz/requests/blob/master/requests/models.py#L109): this is the code path that Requests uses if `files` has been provided. You'll find it's not remarkably helpful to you because it uses a lot of custom code in both Requests and urllib3, but essentially it encodes both the `files` and `data` arguments as multipart form data.
",Lukasa,AraHaan
3859,2017-02-13 13:01:30,@PatrickDChampion @Lukasa and I both have our emails on our profiles so you can do just that. =),sigmavirus24,PatrickDChampion
3859,2017-02-13 13:01:30,@PatrickDChampion @Lukasa and I both have our emails on our profiles so you can do just that. =),sigmavirus24,Lukasa
3855,2017-02-07 14:28:40,"Further there are already some flawed and unofficial hints in https://github.com/python/typeshed. We won't endorse those, but they already exist, so there's less work for you to do.

Also, in the spirit of being explicit, I wholeheartedly agree with @Lukasa.",sigmavirus24,Lukasa
3845,2017-02-06 21:38:06,"<strike>@Lukasa `If you want even more control you can pass a urllib3 Retry object as shown in urllib3's documentation instead of the integer value.` - This does not seem to work. Is this broken?

The code doesn't seem to handle this correctly. Reference: https://github.com/kennethreitz/requests/blob/master/requests/adapters.py#L110

I'll try to set it on the adapter after initialisation with:

and see if that works...</strike>

It seems to be passes through correctly. However I still get the errors. Very odd...

Edit: Turns out `POST` requests are not automatically retried...",lukas-gitl,Lukasa
3842,2017-01-31 20:17:49,"@Lukasa Many thanks. Seeing that is really seems you know what is going on here 3 qs:

1. What other Python cURL lib can I use here that you might know of that works?
2. Why would requests.put() work perfectly not specifying the data param, hence not specifying the source location of file?
3. Not being sure how cURL does this, is it not possible to also add the cURL `-T, --upload-file <file>` like option in the requests library?",Twoflower2,Lukasa
3841,2017-01-31 08:54:26,Thanks @TetraEtc!,Lukasa,TetraEtc
3840,2017-02-13 15:03:23,"@azotlikid Well, let's be clear, it's not *a priori* a bug because it works fine for me using the current Requests build.

*However*, if your problem really is the same as the one discussed in this issue then there is a bug, but it is *server side*: it is sending invalid gzipped data. The server is at fault, and if the data is invalid then there is little scope for us to avoid barfing on it.",Lukasa,azotlikid
3840,2017-02-14 00:22:38,"FWIW, @azotlikid's examples work fine for me now too.  It did not work the other day from this same location.  I had looked up the IP blocks the other day, and they were hosted on the Fastly CDN -- and still are.  They may have corrected things on their system.  I'll reach out to someone at StackExchange I know through a project to see if I can find out anything.

It would still be nice if a future branch had some sort of design detail where bad-servers could be more cleanly caught and handled by developers or the errors could bubble-up a bit better.

Having gone though the code recently to address the ""gzip header when not really gzip"", it would honestly create more problems trying to address this stuff -- so I don't think think it would be worth trying to handle.  

However... it would be *swell* if the raised exceptions could be more constructive for developers, allowing them to do something with the bad request.

Going back to my example, the exception raised is `requests.exceptions.ContentDecodingError`.  IIRC, it bubbles up from this block: https://github.com/kennethreitz/requests/blob/master/requests/models.py#L715-L733

Perhaps these exceptions could be extended to include the `self` response object.  That would allow a developer to examine the instant `Response` and then act upon the `.stream` attribute.  Ultimately, that would allow someone to inspect the exception and determine how/why it is a ""bad server"" -- instead of using a hunt&peck method of what might or might not work.

",jvanasco,azotlikid
3840,2017-02-14 14:17:32,"Thanks for the reply,
@jvanasco Inexplicably this error doesn't happen again since yesterday... Obviously it was a bug in the space-time continuum... And I still don't understand why there was no problem with urllib3 but this will remain a mystery.
@Lukasa the problem is that I use a library which use `requests`, and I can't catch this exception without a global catch.",azotlikid,Lukasa
3840,2017-02-14 14:17:32,"Thanks for the reply,
@jvanasco Inexplicably this error doesn't happen again since yesterday... Obviously it was a bug in the space-time continuum... And I still don't understand why there was no problem with urllib3 but this will remain a mystery.
@Lukasa the problem is that I use a library which use `requests`, and I can't catch this exception without a global catch.",azotlikid,jvanasco
3840,2017-02-14 17:01:56,"> at which point the response object will already be in their hands. 

@Lukasa you are 100% correct.  I've been staring at the inner workings too long.  my apologies!",jvanasco,Lukasa
3840,2017-02-14 17:04:05,";) no need for apologies @jvanasco, this is why we work in groups: it's easy for any one of us to miss the wood for the trees.",Lukasa,jvanasco
3839,2017-01-31 15:02:14,@Lukasa Thanks!,cmanallen,Lukasa
3837,2017-01-29 20:55:18,"regarding the comment from @lukasa on general utility (not the suggested implementation): i disagree about broad utility of following non-location redirects.  i know this means allowing users to operate on ""html"", but please consider that many consumers will eventually consume the HTML redirect (which could be a meta-refresh, rel=""canonical"", type=""og:url"" or several others).  it's not a niche use, but a common one.

regarding the implementation details comment from @sigmavirus24:  I do agree. stashing headers was a way to not suggest a larger patch.

i was a bit surprised the redirect handling used a `while` loop to create a generator that is immediately consumed, and then just overrides the request.   i had looked through the commits and tickets, and it seems like approach was dictated by earlier api behaviors that no longer exist (or I haven't seen).",jvanasco,sigmavirus24
3836,2017-01-27 20:55:23,Thanks @nateprewitt!,Lukasa,nateprewitt
3835,2017-01-26 21:23:36,I believe @nateprewitt is correct. It seemed to me that the conclusion of the discussion was to support 4XX response codes.,mmedal,nateprewitt
3835,2017-01-27 11:51:25,"Hrm. @mmedal, can you rebase on top of the current master? We seem to be having some build problems and I want to check whether they're ones we've seen before and fixed, or new ones.",Lukasa,mmedal
3835,2017-01-27 19:43:49,@Lukasa this is a bug in pipenv (kennethreitz/pipenv#90) that may not have an immediate solution. It may be best to pin Requests' dependency for a bit. `pipenv==3.1.9` should work for now.,nateprewitt,Lukasa
3835,2017-01-27 20:33:02,@nateprewitt I'll merge a PR that does that. 😁,Lukasa,nateprewitt
3832,2017-01-25 15:40:26,"LGTM, thanks @nateprewitt!",Lukasa,nateprewitt
3825,2017-01-20 13:00:06,"Thanks for the quick response @Lukasa 

Please have a look again at the first comment, which I edited shortly after publishing.

Basically: Even if I we discussed and agreed on the inconvenience of the 'leaky bucket' strategy (which will probably not happen), it's clear that this would be a major functionality change and therefore it would make no sense to force its introduction.

However, I think that a parameter could be added to provide such a functionality. Otherwise, as I said, there is no way to actually limit the amount of outgoing connections without taking care of it in a higher level.",csala,Lukasa
3825,2017-01-20 13:05:01,"@Lukasa No, no, what I mean is something else.

I mean another parameter, call it `fail_if_empy` to indicate that, if `block` is `False` and the connection pool is empty, an Exception will be raised instead a new connection created.

Functionally, what I would like to achieve is being able to use the connection pool to make sure that the number of concurrent outgoing connections will never exceed a given number ( `pool_maxsize`)",csala,Lukasa
3825,2017-01-20 13:06:03,"@csala Right, but that is what the `block` parameter does. If you look at the code you'll see that `block`, if set to true, causes an `EmptyPoolError` exception to be raised rather than having a new connection created.",Lukasa,csala
3825,2017-01-20 13:16:06,"@Lukasa Well, that's not entirely true. Basically because that part of the code can never be reached because of how the queue works and because `pool_timeout` is not being used.

At the moment, if `block` is set to `True`, the `pool.get` call will never raise an exception because it will just block the `get` attempt indefinitely until a connection is available.

However, it's true that if the `pool_timeout` is indicated and reached, it would work as I expect. And it could even be set to 0 to make it fail immediately.

But, on the other side, without such a `fail_if_empty` parameter, you cannot have it following the 'leaky_bucket' strategy along with a timeout!
If there was this parameter, you could make it wait for some time and, if no connection was available after the given timeout, go on and create a new one, which currently is not possible.",csala,Lukasa
3825,2017-01-20 13:18:24,@csala I'm in agreement that we should expose the pool timeout. I'm just not in agreement that we should add anything else. All of the relevant behaviours can be controlled by varying `block` and `pool_timeout`.,Lukasa,csala
3825,2017-01-20 13:22:10,"@Lukasa Alright, I'll go for that bit.",csala,Lukasa
3824,2017-01-18 20:38:10,"@Arno0x To be clear, the appearance of the problem was caused by you installing PyOpenSSL. This is because we preferentially use PyOpenSSL if it's available, but PyOpenSSL is very strict about the data that is handed to it.

The fact that you can safely call `decode` on the headers is not sufficient for a test. The headers must be native strings on Python: that means you should manually encode them if they are unicode strings.",Lukasa,Arno0x
3824,2017-01-18 21:21:02,"@Lukasa thank you so much for helping me on this issue.
Ok, so I understand that I'm now using an installed PyOpenSSL lib on this system, rather than the PyOpenSSL that comes embedded with ""requests"", which explains the difference with my other systems.

If I get it right (provided I'm using Python2): all strings in my code are of type 'str' by default and I guess that's what you call native string (is it?). So it looks like all my headers (name or value) are already native python string. Still, just in case, I tried to change my code to add `.encode('ascii')`on all my header strings => no luck, still the same error.

So I've reduced my testing scenario down to the basic following test in python command line:

So it turns out not to be a header issue, but rather the data itself returned from my `Crypto.xor` function, which is very basic:


I hope you'll get to see what's wrong in this piece of code that PyOpenSSL doesn't like... thanks again for your help.",Arno0x,Lukasa
3821,2017-01-14 12:50:19,"@Lukasa hidden in the original message is:

",sigmavirus24,Lukasa
3821,2017-01-14 12:51:31,"@bifeng in your second example, you're not using proxies. Does this mean that you cannot connect to the internet **at all**?",sigmavirus24,bifeng
3821,2017-01-14 14:56:57,"@sigmavirus24 
I am surfing the internet, and run this code , so connect is not the problem.
I am still getting the error:
requests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None))
That's weird! ",bifeng,sigmavirus24
3819,2017-01-15 18:58:19,"@Lukasa thanks for the commet! I already propose the changes on that repository.

",llazzaro,Lukasa
3817,2017-01-14 10:22:03,"It'd also help if I took @Lukasa's advice and actually read the whole issue that spawned this.

I don't consider the current import performance to be a bug, but if someone wants to try to improve it on cryptography (which is probably the dominant source of import time for pyopenssl) I'm happy to review. The biggest single source of time is likely when it loops over the lib object to build a new conditional lib object. That could be optimized if cffi supported a means of conditional binding, but we're likely talking only ~10ms?",reaperhulk,Lukasa
3817,2017-01-14 12:48:17,"@Lukasa @reaperhulk so it's worth noting that `slow` will change based on how many things are installed + how many possible entry-points exist. `pkg_resources`, if I remember correctly, will scan all of `site_packages` for entry-points. So if you're testing `pyOpenSSL` in a fresh virtual environment, you're import speed (given that cryptography apparently scans entry-points at import) is going to be faster than @dsully since they seem to be installing a lot of things in one `site_packages` directory.

Granted, this is a fundamental flaw of how `pkg_resources` works, but I think it's still a legitimate problem. I haven't looked at what cryptography uses `pkg_resources` to find at import, but there would only be ""import time"" benefits to avoiding that scan rather than any real performance benefit to not doing it when cryptography is imported.

I'd also like a better understanding of `slow` from @dsully + maybe a better description of how much is being installed into their site-packages directory.",sigmavirus24,reaperhulk
3817,2017-01-14 12:48:17,"@Lukasa @reaperhulk so it's worth noting that `slow` will change based on how many things are installed + how many possible entry-points exist. `pkg_resources`, if I remember correctly, will scan all of `site_packages` for entry-points. So if you're testing `pyOpenSSL` in a fresh virtual environment, you're import speed (given that cryptography apparently scans entry-points at import) is going to be faster than @dsully since they seem to be installing a lot of things in one `site_packages` directory.

Granted, this is a fundamental flaw of how `pkg_resources` works, but I think it's still a legitimate problem. I haven't looked at what cryptography uses `pkg_resources` to find at import, but there would only be ""import time"" benefits to avoiding that scan rather than any real performance benefit to not doing it when cryptography is imported.

I'd also like a better understanding of `slow` from @dsully + maybe a better description of how much is being installed into their site-packages directory.",sigmavirus24,Lukasa
3817,2017-01-14 12:48:17,"@Lukasa @reaperhulk so it's worth noting that `slow` will change based on how many things are installed + how many possible entry-points exist. `pkg_resources`, if I remember correctly, will scan all of `site_packages` for entry-points. So if you're testing `pyOpenSSL` in a fresh virtual environment, you're import speed (given that cryptography apparently scans entry-points at import) is going to be faster than @dsully since they seem to be installing a lot of things in one `site_packages` directory.

Granted, this is a fundamental flaw of how `pkg_resources` works, but I think it's still a legitimate problem. I haven't looked at what cryptography uses `pkg_resources` to find at import, but there would only be ""import time"" benefits to avoiding that scan rather than any real performance benefit to not doing it when cryptography is imported.

I'd also like a better understanding of `slow` from @dsully + maybe a better description of how much is being installed into their site-packages directory.",sigmavirus24,dsully
3817,2017-01-14 12:49:12,"Also, to set everyone's expectations appropriately, this appears to be a problem @dsully is encountering at work, so I completely expect them to *not* respond until Monday during work hours.",sigmavirus24,dsully
3817,2017-01-15 20:37:22,@reaperhulk If the cryptography team is okay with moving the import inside the function that seems suitable and should eliminate the need for this PR based on your findings.,sigmavirus24,reaperhulk
3817,2017-01-16 20:04:17,"@reaperhulk Yes - removing the pkg_resources import until it's needed will certainly help. I'll create a PR over there to move the import.

@sigmavirus24 nails it - we have a lot installed in site-packages, and are extremely sensitive about CLI tools start up time. Every millisecond counts in the eyes of our users. I do agree this is a fundamental flaw in pkg_resources. (My weekend was pretty packed, so I wasn't able to reply until now).

I do feel that requests automatically trying to use pyopenssl if it's installed without any way to opt-out is surprise functionality. I do have a work around for now, so this PR can be dropped.",dsully,reaperhulk
3817,2017-01-16 20:04:17,"@reaperhulk Yes - removing the pkg_resources import until it's needed will certainly help. I'll create a PR over there to move the import.

@sigmavirus24 nails it - we have a lot installed in site-packages, and are extremely sensitive about CLI tools start up time. Every millisecond counts in the eyes of our users. I do agree this is a fundamental flaw in pkg_resources. (My weekend was pretty packed, so I wasn't able to reply until now).

I do feel that requests automatically trying to use pyopenssl if it's installed without any way to opt-out is surprise functionality. I do have a work around for now, so this PR can be dropped.",dsully,sigmavirus24
3815,2017-01-13 12:56:53,Thanks @Lukasa.,Th30n,Lukasa
3814,2017-01-16 21:16:58,@Lukasa One question: won't implementation of `ssl_wrap_socket` override custom CA certificates by calling load_verify_locations inside?,Kentzo,Lukasa
3814,2017-01-16 21:45:29,"@Lukasa That's how I did it so far:

",Kentzo,Lukasa
3814,2017-01-16 22:18:41,"@Lukasa Something like this:


?",Kentzo,Lukasa
3812,2017-01-12 17:12:51,Thanks @inglesp!,Lukasa,inglesp
3811,2017-01-11 21:53:20,"@Lukasa Hmm, I was hoping that would work. I ran the following and got the same response.

import requests
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.poolmanager import PoolManager
from requests.packages.urllib3.util.ssl_ import create_urllib3_context
CIPHERS = (
    'ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:ECDH+HIGH:'
    'DH+HIGH:ECDH+3DES:DH+3DES:RSA+AESGCM:RSA+AES:RSA+HIGH:RSA+3DES:!aNULL:'
    '!eNULL:!MD5'
)
baseurl='https://webapps.kdads.ks.gov/LSOBP18'
class DESAdapter(HTTPAdapter):
    def init_poolmanager(self, connections, maxsize, block=False,*args, **kwargs):
        context = create_urllib3_context(ciphers=CIPHERS)
        kwargs['ssl_context'] = context
        self.poolmanager = PoolManager(num_pools=connections,
                                       maxsize=maxsize,
                                       block=block,
                                       *args, **kwargs)
s=requests.Session()
s.mount('https://10.192.8.89', DESAdapter())
s.get(baseurl, verify=False)",2tim,Lukasa
3809,2017-01-11 09:49:57,Thanks for this @JungWinter! :sparkles: :cake: :sparkles:,Lukasa,JungWinter
3807,2017-01-11 01:12:05,"Hey @gilessbrown, thanks for opening this issue! This behaviour was exposed by a change made in (327512f) which removed exception handling for this case. It currently only exists in the 2.12.x releases, so using Requests 2.11.1 should work for you.

While you're seeing this behaviour in Requests, it actually seems to be how we're handling chunked responses without a body in urllib3. shazow/urllib3#990 introduced an attempt to catch this problem but the check is just slightly off. While it verifies the existence to `fp`, it doesn't check that `fp` isn't `None`.

I think the simple fix here is to change [the check](https://github.com/shazow/urllib3/blob/master/urllib3/response.py#L525) in urllib3 to `return getattr(self._fp, 'fp', None) is not None` which should give us what we actually want. It verifies that `fp` both exists, and is not the default `None` value.

If @Lukasa or @sigmavirus24 are in agreement, we can address this over in urllib3.",nateprewitt,gilessbrown
3807,2017-01-11 15:15:10,"> I'd say the server is at fault here.

I completely agree the server isn't compliant here, but we also just said we'd *like* to be tolerant of things like this in #3794, which is even more out of spec. The server is definitely returning garbage though, so maybe we choose not to address this.

@sigmavirus24 I'm currently able to reproduce this on Python 2.7.12 and 3.5.2 on Mac OS 10.12.2, Ubuntu 12.04, and [Travis](https://travis-ci.org/nateprewitt/requests/builds/190981913).

The repro won't fail if you start it at the second hop (the https url), so it seems to require this specific set of responses. It's definitely related to the transfer-encoding because chaining a similar set of responses ((http)302->(https)302->(separate server)200) from httpbin won't trigger the failure. 

This ""bug"" was introduced in Requests 2.7.0 (urllib3 1.10.4) but masked by two separate `try/except AssertionError` blocks. The first was removed in 2.8.0 in c6c8d64 but this didn't expose the issue because the `content` exception block was still catching it. 327512f removed the second safeguard which is why 2.12.x is now showing this. I backported 327512f and was able to confirm this only started happening after the chunk transfer work in urllib3 1.10.4. Something causes the underlying socket closed before we can read it for the redirect, but I wasn't able to immediately find what.

At this point this is probably too much digging for a pretty uncommon edge case, but I'll let you two decide.",nateprewitt,sigmavirus24
3807,2017-01-11 18:07:00,"Hi @Lukasa, your analysis of it being caused by the double consumption of raw (first by the `copyfileobj`, and then by the `response.content` in `resolve_redirects`) matches what I saw in the debugger so I'm pretty confident your work-around of setting `._content` will work for me.  Thanks for your deep dive into this.",gilessbrown,Lukasa
3803,2017-01-09 19:30:01,"Hey @codespaced, thanks for opening this issue. It looks like you used a comma instead of a colon to separate the key and value in your header. This is creating a set object instead of the dictionary we're expecting. Replacing that should solve your issue.",nateprewitt,codespaced
3803,2017-01-09 19:41:40,"Yup, this is strictly a Python syntax problem and not a Requests issue. Thanks @nateprewitt. :D",Lukasa,nateprewitt
3802,2017-01-09 11:46:32,"I agree with @Lukasa. While I know a significant number of people use this function, it's not a public API that we support and even so, this is the one of the least optimal ways of accomplishing what you want.",sigmavirus24,Lukasa
3798,2017-01-06 09:41:44,"@withr Unfortunately, not easily. Generally speaking you should look at the proxies dictionary you provided. Alternatively, you can write a custom Transport Adapter that will provide the information you need, though that's quite a lot of work.",Lukasa,withr
3797,2017-01-24 15:51:24,@Lukasa Sorry to bother on closed issue but I'm wondering why should this fix not being updated in Request?,RobGThai,Lukasa
3797,2017-01-24 15:52:14,@RobGThai We pushed requests v2.13 today which contains that fix.,Lukasa,RobGThai
3794,2016-12-29 16:36:30,"Hey @elasti-georgeg, thanks for opening this issue. I am able to reproduce the issue you've listed but only with a 204 response containing a ""Transfer-Encoding: chunked"" header.

[RFC 7230 § 3.3.1](https://tools.ietf.org/html/rfc7230#section-3.3.1) states:

>A server MUST NOT send a Transfer-Encoding header field in any
   response with a status code of 1xx (Informational) or 204 (No
   Content). 

This appears to be what is causing the ChunkedEncodingError which should be what we want in this scenario. Can you confirm you're receiving a Transfer-Encoding header? If that's the case, this is an issue that needs to be fixed with your Rails server.

The code below should allow you to perform the request without triggering the error. Just make sure you aren't calling `.content` on the response.
",nateprewitt,elasti-georgeg
3794,2016-12-29 17:47:44,"@nateprewitt yes, they should fix that there, but we should also be able to tolerate this.",sigmavirus24,nateprewitt
3794,2016-12-29 19:22:28,"@Lukasa no, it won't because that's where the error is being raised from. httplib raises a IncompleteRead error of 0 bytes because the connection is terminated without sending anything.",nateprewitt,Lukasa
3794,2017-01-14 12:42:39,Good catch @nateprewitt ,sigmavirus24,nateprewitt
3790,2016-12-29 09:43:07,"@djbaldey This code optionally injects the PyOpenSSL based TLS backend into Requests. It's extremely useful because in many cases it enables support of advanced features that the standard library TLS module does not support. For this reason, we always attempt to inject it, as this is the most reliably way to detect its presence.

In this case, it's almost certainly the result of you having an older cryptography module with a slow import time. Can you try updating cryptography and PyOpenSSL to the newest versions to see if that resolves your issue?",Lukasa,djbaldey
3790,2016-12-30 03:55:14,"@drpoggi, why instead of trying to solve the problem with slow code - you are trying to shift the responsibility for organizational activities?
",djbaldey,drpoggi
3790,2016-12-30 04:45:45,"@drpoggi, I understand all of this. Therefore, I suggest to remove the trying from urllib3 module initialization. I don't think that the most important component and it is more important than speed loading the module. I am sure that those who need urllib3 with TLS - they can write these strings in the app after loading the module. And it will be right.
Upd:
In addition, in the internal corporate network - [""advanced features TLS""](#issuecomment-269605327) may be banned or make mistakes.
",djbaldey,drpoggi
3790,2017-01-01 15:16:41,"Needless to say, I agree with @Lukasa. Being able to provide the best possible security out-of-the-box (without futher user configuration) is absolutely imperative. We will not sacrifice that.

If you are doing `apt-get install -y python-requests` and it's installing `PyOpenSSL` for you, that means the packagers understand this *and* they understand that the system Python you're using doesn't allow for the best possible security for you and every other user. Now, let's try to get to the bottom of thos.

@djbaldey can you determine how long it takes on your system to do `import cryptography`?",sigmavirus24,djbaldey
3790,2017-01-01 15:16:41,"Needless to say, I agree with @Lukasa. Being able to provide the best possible security out-of-the-box (without futher user configuration) is absolutely imperative. We will not sacrifice that.

If you are doing `apt-get install -y python-requests` and it's installing `PyOpenSSL` for you, that means the packagers understand this *and* they understand that the system Python you're using doesn't allow for the best possible security for you and every other user. Now, let's try to get to the bottom of thos.

@djbaldey can you determine how long it takes on your system to do `import cryptography`?",sigmavirus24,Lukasa
3789,2016-12-26 18:21:05,"@moin18 that will be in place until sigmavirus24 has a chance to review your changes again. Today is still a ""holiday"" for a lot of people working in the US, so sigmavirus24 likely won't respond immediately. Things look pretty cleaned up now, so I think you can leave these changes and he'll respond when he has a spare moment :)",nateprewitt,moin18
3789,2017-01-10 20:55:01,"@moin18 would you be willing to rebase this rather than merging master into this branch? Alternatively, @Lukasa how do you feel about squash merging this?",sigmavirus24,moin18
3789,2017-01-17 15:06:01,@sigmavirus24 @Lukasa cleaned up the commit history. Merged all commits into single commit.,moin18,Lukasa
3789,2017-01-17 15:06:01,@sigmavirus24 @Lukasa cleaned up the commit history. Merged all commits into single commit.,moin18,sigmavirus24
3787,2016-12-23 15:12:58,"@Lukasa : In that case I will be creating a separate patch for urllib3 discarding the urllib3 related changes from this PR. Apart from that, does the `requests` library related changes looks good to you?",moin18,Lukasa
3786,2016-12-22 19:21:03,"Hey @Andriuskislas, thanks for opening this ticket. I can't seem to reproduce the path dropping in 2.12.4 with the information supplied above. However, I think I have an idea of what you're hitting.

First, which version of requests are you using? URIs that I'm passing with a + character seem to be unaffected from our end, so I'm wondering if you're hitting a redirect which isn't parsing the supplied URI correctly on the server side.

`+` is a reserved character, that should represent a space. If you're intending to pass it as a literal `+` character, you'll likely want to encode it as `%2B` instead.

If you could supply a more detailed repro, we can take a deeper look.",nateprewitt,Andriuskislas
3786,2016-12-22 19:56:51,"@moin18 , No my api endpoint is fine. When I hit the url from browser it returns correct data. 

@nateprewitt , Thank you. Firstly my requests version is 

Here is my encoded try 
 
without encoding 

Also tried with encoding whole url  but request didn't made. 
",Andriuskislas,moin18
3786,2016-12-22 19:56:51,"@moin18 , No my api endpoint is fine. When I hit the url from browser it returns correct data. 

@nateprewitt , Thank you. Firstly my requests version is 

Here is my encoded try 
 
without encoding 

Also tried with encoding whole url  but request didn't made. 
",Andriuskislas,nateprewitt
3786,2016-12-22 20:10:32,"Thanks @Andriuskislas, url encoding should only be used for unicode characters, or uses of reserved characters for something other than their assigned meaning, which is why the last example isn't routing.

I can use a variant of your example above on a different server, and it leaves the + unchanged. That points pretty clearly to this being a server behaviour rather than something in Requests.



At this point, it feels like this is probably something better suited for Stackoverflow so we're not bombarding everyone watching issues on Requests.

Your output above is reassuring my redirect hunch, so here's one more test to perform. If you can check the output of `r.history`, that would be helpful. If it's anything other than `[]`, check what `r.history[0].url` is and compare that value to `r.request.url`. If they're different, this is definitely something happening server side.

If that's the case, would you mind opening an issue on https://stackoverflow.com and posting the link here so we have a paper trail? I'll follow up over there.",nateprewitt,Andriuskislas
3786,2016-12-23 01:36:34,"@Andriuskislas I would bet that @nateprewitt is correct about redirects being the root cause here. You can also tell requests not to follow them by passing `allow_redirects=False` to your request. It's plausible the server you're talking to is redirecting based on something like User-Agent. Either way, @nateprewitt's suggestion of asking a question on StackOverflow is correct. This isn't a general support forum, but a place for defects, which I don't think you've found.",sigmavirus24,Andriuskislas
3786,2016-12-23 01:36:34,"@Andriuskislas I would bet that @nateprewitt is correct about redirects being the root cause here. You can also tell requests not to follow them by passing `allow_redirects=False` to your request. It's plausible the server you're talking to is redirecting based on something like User-Agent. Either way, @nateprewitt's suggestion of asking a question on StackOverflow is correct. This isn't a general support forum, but a place for defects, which I don't think you've found.",sigmavirus24,nateprewitt
3785,2016-12-21 18:11:31,"@Lukasa it looks like this is a modification in the appengine module in urllib3. The warning update may need to go over there, otherwise we'll overwrite it the next time we update urllib3.",nateprewitt,Lukasa
3785,2016-12-21 23:10:48,@Adusei would you be interested in submitting this patch to [urllib3](https://github.com/shazow/urllib3) so this can be fixed at the source?,nateprewitt,Adusei
3785,2016-12-23 21:07:21,"Great, things should be set for future releases then. Thanks for taking a look @Adusei!",nateprewitt,Adusei
3783,2016-12-22 00:46:46,@moin18 The problem I ran up against was that `idna` is part of `packages` so whenever `packages` is loaded (e.g. in `models.py`) `idna` will be loaded at that point. So the fix would be for `idna` not to be loaded at that point but somehow still be loaded when needed :/,DanielGibbsNZ,moin18
3781,2016-12-21 13:40:05,"Thanks for this patch @mplonka. However, we would generally prefer to update the bundled packages ourselves at the time of release: that way, we have some confidence that we have done the sensible thing. Can you pare this patch down to just the makefile change, please?",Lukasa,mplonka
3781,2016-12-21 13:48:30,"Thanks for reviewing this PR, @Lukasa .

As per your request, here's the revert commit.",mplonka,Lukasa
3781,2017-01-18 16:00:10,"@Lukasa it seems that idna hasn't been updated yet, but a new version has been released in the mean time. Will idna be updated to fix the Jython issue?",LordGaav,Lukasa
3781,2017-01-18 16:01:10,"@LordGaav Per [our documentation](http://docs.python-requests.org/en/master/community/release-process/#hotfix-releases), we do not update vendored dependencies in patch releases. Please wait for v2.13, which will contain an update.",Lukasa,LordGaav
3780,2016-12-20 20:45:51,"@DanielGibbsNZ Not enormously effectively, no. In principle the check in the `except` block could be used, I suppose.",Lukasa,DanielGibbsNZ
3780,2016-12-22 22:46:14,"@Lukasa: `idna` is also imported in `urllib3/contrib/pyopenssl.py`. Here: https://github.com/kennethreitz/requests/blob/master/requests/packages/urllib3/contrib/pyopenssl.py#L46

In order to remove the reference of `idna` via garbage collector, corresponding changes in urllib3 are also required. The idea is to do lazy loading of idna package only on demand basis. 

Let me know if I am on correct path. If yes, I will raise the bug and pull request to change this behavior in urllib3 as well.

Sample diff is available here: https://github.com/kennethreitz/requests/pull/3787",moin18,Lukasa
3778,2016-12-19 15:00:19,"@arunchandramouli taking a quick look at those websites, it looks as if they require JavaScript in order for you to successfully scrape their data. Requests does not (and has never) executed JavaScript. You need a different tool. Requests is working exactly as expected. There is no bug here.",sigmavirus24,arunchandramouli
3776,2016-12-22 10:40:54,"Hi @Lukasa. Thanks for looking into that.
Wouldn't it be wise to do some sort of escaping of those comments in https://github.com/Lukasa/mkcert itself? Are you OK with me submitting a PR there?",mplonka,Lukasa
3776,2016-12-22 12:02:39,"@mplonka I don't really see any reason to do the escaping there. PEM isn't well specced but so far we have only one extremely unusual implementation that chokes. I don't really see any reason to destroy that output for that, given that it's clearly intended to be human readable. ",Lukasa,mplonka
3774,2016-12-19 04:18:39,"@Lukasa thank you for your reply, but your code doesn't work for me.  
here are my code(minor modified on yours):



same error msg appears:



and under 2.11.1, there was another error which i can fix it by removing the ssl_context key:

",bigbagboom,Lukasa
3774,2016-12-20 01:07:33,"@Lukasa yes,  i installed PyOpenSSL but I don't think it's the cause. I uninstalled it and the error changed to the following:

now the pip freeze is:
C:\Users\bigbagboom>pip freeze
alabaster==0.7.9
anaconda-client==1.4.0
anaconda-navigator==1.1.0
argcomplete==1.0.0
astroid==1.4.8
astropy==1.1.2
Babel==2.3.4
beautifulsoup4==4.4.1
bitarray==0.8.1
blaze==0.9.1
bokeh==0.11.1
boto==2.39.0
Bottleneck==1.0.0
certifi==2016.9.26
cffi==1.9.1
chest==0.2.3
click==6.6
cloudpickle==0.1.1
clyent==1.2.1
colorama==0.3.7
comtypes==1.1.2
conda==4.0.5
conda-build==1.20.0
conda-env==2.4.5
conda-manager==0.3.1
configobj==5.0.6
cryptography==1.7.1
cycler==0.10.0
Cython==0.23.4
cytoolz==0.7.5
dask==0.8.1
datashape==0.5.1
decorator==4.0.10
dill==0.2.4
docutils==0.13.1
dynd===c328ab7
entrypoints==0.2.2
et-xmlfile==1.0.1
fastcache==1.0.2
Flask==0.11.1
Flask-Cors==2.1.2
future==0.16.0
gevent==1.1.2
greenlet==0.4.11
h5py==2.5.0
HeapDict==1.0.0
idna==2.1
imagesize==0.7.1
ipykernel==4.5.2
ipython==5.1.0
ipython-genutils==0.1.0
ipywidgets==4.1.1
isort==4.2.5
itsdangerous==0.24
jdcal==1.3
jedi==0.9.0
Jinja2==2.8
jsonschema==2.5.1
jupyter==1.0.0
jupyter-client==4.4.0
jupyter-console==4.1.1
jupyter-core==4.2.1
lazy-object-proxy==1.2.2
llvmlite==0.9.0
locket==0.2.0
lxml==3.6.0
MarkupSafe==0.23
matplotlib==1.5.1
mccabe==0.5.3
menuinst==1.3.2
mistune==0.7.3
mpmath==0.19
multipledispatch==0.4.8
nbconvert==4.3.0
nbformat==4.2.0
ndg-httpsclient==0.4.2
networkx==1.11
nltk==3.2
nose==1.3.7
notebook==4.1.0
ntlm-auth==1.0.2
numba==0.24.0
numexpr==2.6.1
numpy==1.11.2
odo==0.4.2
openpyxl==2.4.1
ordereddict==1.1
pandas==0.18.0
partd==0.3.2
path.py==0.0.0
patsy==0.4.0
pep8==1.7.0
pickleshare==0.7.4
Pillow==3.4.2
ply==3.8
prompt-toolkit==1.0.9
psutil==5.0.0
py==1.4.31
pyasn1==0.1.9
pycosat==0.6.1
pycparser==2.17
pycrypto==2.6.1
pyexcel==0.3.3
pyexcel-io==0.2.4
pyexcel-xls==0.2.0
pyexcel-xlsx==0.2.3
pyflakes==1.3.0
Pygments==2.1.3
pylint==1.6.4
pyparsing==2.0.3
pyreadline==2.1
pytest==2.8.5
python-dateutil==2.5.1
python-ntlm3==1.0.2
pytz==2016.10
pywin32==220
PyYAML==3.11
pyzmq==16.0.2
QtAwesome==0.3.3
qtconsole==4.2.1
QtPy==1.1.2
requests==2.12.0
requests-ntlm==1.0.0
requests-toolbelt==0.7.0
rope-py3k==0.9.4.post1
scikit-image==0.12.3
scikit-learn==0.17.1
scipy==0.17.0
simplegeneric==0.8.1
singledispatch==3.4.0.3
six==1.10.0
snowballstemmer==1.2.1
sockjs-tornado==1.0.1
sphinx-rtd-theme==0.1.9
spyder==2.3.8
SQLAlchemy==1.0.12
statsmodels==0.6.1
sympy==1.0
tables==3.3.0
texttable==0.8.7
toolz==0.7.4
tornado==4.4.2
traitlets==4.3.1
unicodecsv==0.14.1
urllib3==1.19.1
wcwidth==0.1.7
Werkzeug==0.11.11
win-unicode-console==0.5
wrapt==1.10.8
xlrd==0.9.4
XlsxWriter==0.8.4
xlwings==0.7.0
xlwt==1.1.2
xlwt-future==0.8.0

And for reference, here is some output(successful) with curl 7.50.3:
",bigbagboom,Lukasa
3774,2016-12-20 01:35:27,"@ernestoalejo  thank you but I am not sure...  you know 'https://10.192.8.89:8080/yps_report' and 'https://10.192.8.89:8080' can be 2 different sites in IIS's configuration.

> For reference, the line:
> 
> s.mount('https://10.192.8.89:8080/yps_report',DESAdapter())
> should be:
> 
> s.mount('https://10.192.8.89:8080',DESAdapter())
> or the error adapter won't be used correctly.",bigbagboom,ernestoalejo
3774,2016-12-20 02:19:51,@bigbagboom They cannot possibly have different configurations for TLS though. IIS can't see what path is being used until after the TLS handshake is complete.,Lukasa,bigbagboom
3774,2016-12-20 04:27:46,"my error was gone by adding 'DES-CBC3-SHA' in the CIPHERS string. sigh...

Thank you @Lukasa  and @ernestoalejo .

and it's also ok with following code:
s.mount('https://10.192.8.89:8080/yps_report',DESAdapter()) ",bigbagboom,Lukasa
3774,2016-12-20 04:27:46,"my error was gone by adding 'DES-CBC3-SHA' in the CIPHERS string. sigh...

Thank you @Lukasa  and @ernestoalejo .

and it's also ok with following code:
s.mount('https://10.192.8.89:8080/yps_report',DESAdapter()) ",bigbagboom,ernestoalejo
3772,2016-12-16 18:46:56,"@nateprewitt So we have two role models to look at here:

1. Browsers - If you read the logs for our IRC channel, you'll see that Browsers are doing what we do (although probably not as loosely as we do it)

2. Curl/Wget - They disregard the WWW-Authenticate headers on anything that's not a 401. In the case of the user on IRC they were receiving a 403 with WWW-Authenticate headers and we were authenticating while Curl and Wget were not.

Frankly, I'm not sold on restricting this to 401s only, but I do think we should restrict it to 4xx codes. Having these headers sent back on a 30x, for example, could lead to interesting behaviour. On the one hand, we have to answer the challenge, right? On the other we're being redirected, potentially to a totally different domain. I think the only right answer in non 4xx cases is to not authenticate. Unfortunately, we will answer any challenge we receive at the moment and that's problematic.",sigmavirus24,nateprewitt
3772,2016-12-16 18:47:14,"Also @nateprewitt, please let someone else pick this up.",sigmavirus24,nateprewitt
3772,2016-12-16 20:28:51,"Sorry, I should have clarified further on Authorization. I agree we shouldn't be sending Digest Authorization without an appropriate challenge first. What I'm saying is that the section quoted above seems to also support that responding to an appropriate challenge in a non-401 response is permissible which is what we what we currently do.

The scoping here is obviously in the hands of you and @Lukasa,  I just wanted to make sure we didn't completely constrict that to 401 hastily as the original post noted.",nateprewitt,Lukasa
3772,2016-12-16 20:50:23,"> I agree we shouldn't be sending Digest Authorization without an appropriate challenge first. 

We also *literally* can't.

> I just wanted to make sure we didn't completely constrict that to 401 hastily as the original post noted.

I appreciate that :) I think @Lukasa is starting to feel more and more (as I do) that we should be very strict in our interpretation of specifications. That said, (as you point out) the spec is lenient, so my position is that we should be strict in the spirit of the specification. I can't see any reason why someone would challenge with a 200 response, so it should be safe to restrict it to 4xx responses.",sigmavirus24,Lukasa
3772,2017-01-07 06:57:50,Hey @Lukasa will work on this issue,iamrajhans,Lukasa
3771,2016-12-15 17:40:41,"Hey @kianxineki, I skimmed through your question and responded a bit too quickly. The implications of this change is we would now use a single `Session` object across ALL connections with `requests.request` which may not be what the user wants. The api methods are intended to be single connections that are cleaned up after completion.

As I think you've already noted above, you can accomplish the change by simply adding an extra line  in your code declaring your own Session instance. This will allow you to utilize connection pooling and avoid the new connections.



You can find more information on how to use `Session` [here](http://docs.python-requests.org/en/master/user/advanced/#session-objects).",nateprewitt,kianxineki
3771,2016-12-15 17:47:15,"@kianxineki Thanks for sending us a pull request! It looks to be your first to the project. Welcome!

It seems as though you're trying to ask why we do not use a global session object in `requests.api`. To answer you, let me explain a little bit of history first.

We used to use a global session object and our Session objects used to be thoroughly thread-safe. At some point, something changed about our Session objects that no one has yet been able to identify. This means that they're no longer properly threadsafe. As such, people were having significant problems sharing a session between threads which also applied to using the functional API. Further, having a session that sits around until the process finishes was holding onto sockets that were never being reused. 

The solution to the thread-safety issue and the socket leak was to stop using a global session.

Currently, if you want to eliminate those lines in your logs and utilize connection pooling, you *must* use a Session. There's no way to provide connection pooling to you via our functional API without actively harming other users.

In the future, please direct all of your questions to StackOverflow, which is the proper Question & Answer forum for Requests.

Again, thank you for your pull request, but we can't accept it.",sigmavirus24,kianxineki
3770,2016-12-15 02:06:09,"So we are not doing client auth here.

And the openssl client fails for me because there's no local certificate in my bundle for the issuer of that revoked certificate:



> tl;dr it seems that revoked ssl certs aren't being rejected, which could be an issue

This does seem, at this point, true. That said, I believe this is because requests (and openssl) don't by default turn on OCSP which is what we'd need to use to *detect* a revoked certificate.

There are lots of opinions about OCSP and certificate revocation, but one prevailing one is that it doesn't work very well (which isn't an excuse to not support it). Either way, I believe to support it, we need some work in the standard library ssl library and in cryptography to allow us to add that support to PyOpenSSL.

I'm not 100% confident in any of this, though, so I'd advise we wait until @Lukasa is up or someone like @reaperhulk can take a look.

---

Thanks for reporting this @mendaxi ",sigmavirus24,mendaxi
3770,2016-12-15 08:30:32,"Requests does not respect revocation notices at this time. This is unfortunate but ultimately beyond us: there are no APIs available to extract and validate stapled OCSP responses, in either PyOpenSSL or the standard library.

I should note that there is only one case where revocation is really useful, and that is with OCSP-Must-Staple. In all other cases we basically fail open, which isn't really very helpful at all. 

However, @mendaxi seems to be suggesting that setting the verify mode to CERT_REQUIRED would resolve his issue. That's weird, because Requests has been setting that flag for 6 years.

TL:DR we don't use the regular old wrap socket and haven't for a while. We have been managing our own TLS config for years and validating certificates.",Lukasa,mendaxi
3770,2016-12-16 00:00:49,"@Lukasa to be clear, @mendaxi is using CLIENT_AUTH with an SSLContext object. It's failing because it's the wrong configuration to validate a certificate from a server. :)",sigmavirus24,mendaxi
3770,2016-12-16 00:00:49,"@Lukasa to be clear, @mendaxi is using CLIENT_AUTH with an SSLContext object. It's failing because it's the wrong configuration to validate a certificate from a server. :)",sigmavirus24,Lukasa
3769,2016-12-14 16:12:44,"Hey @ppolewicz, it looks like this is specifically a limitation of how Jython uses the JVM as noted in [this ticket](http://bugs.jython.org/issue1891). I'd ensure you have the latest version of Jython2.7, and attempt to use Requests 2.12.4.

If that doesn't solve it, I'm afraid there's not much we can do at this time since the bug isn't really with Requests.",nateprewitt,ppolewicz
3768,2016-12-19 01:39:13,@yoyoprs I believe @Lukasa wanted you two to `import socks` and then check that the version was the same as what `pip` reports.,sigmavirus24,Lukasa
3768,2016-12-19 01:39:13,@yoyoprs I believe @Lukasa wanted you two to `import socks` and then check that the version was the same as what `pip` reports.,sigmavirus24,yoyoprs
3767,2016-12-14 05:13:25,"Hey @raphaeltm, thanks for opening a ticket. This has been addressed in #3758 and is working on master. It should be fixed when the next release ships.",nateprewitt,raphaeltm
3766,2016-12-13 21:41:16,"@Lukasa I rescinded my last comment on `num_401_calls` because while what I was testing worked, it's not reproducible in the wild. I still think there may be value in testing the hooks individually but I don't think it's a blocker here. These tests cover the use cases we're concerned about in #1979.",nateprewitt,Lukasa
3766,2016-12-14 09:41:08,"Cool, tests are good. Thanks for the work @nateprewitt!",Lukasa,nateprewitt
3759,2016-12-09 13:15:43,"@saft The *end* intent was to unescape escaped quote marks that *surround* a cookie value. These are almost certainly in error. However, we seem to have done that by hitting the problem with a sledgehammer by just unescaping *all* quotes, which was probably unnecessary.

So the goal would be to craft a change here that uses a regular expression to only unquote quote marks at the start and end of the cookie value.",Lukasa,saft
3759,2016-12-09 13:43:21,@piotr-dobrogost is correct.,Lukasa,piotr-dobrogost
3759,2016-12-09 14:07:20,"@saft I'm not sure what you mean: we're agreeing that the current behaviour is wrong. We're just trying to discuss what the fix should be.

As to ""the intent is not in line with RFC 6265"": sure it is. RFC 6265 allows surrounding DQUOTE characters. This is to allow the possibility that someone has mistakenly escaped the DQUOTE values, which is *forbidden* by RFC 6265.

However, it does end up seeming like this might be superfluous to needs. Right now I'm not able to reproduce a situation in which that behaviour is required.",Lukasa,saft
3759,2016-12-09 14:47:26,"> Perhaps. I think this set_cookie is arbitrary. I'm still unconvinced of any justification for any stated intent so far.

And?

No-one is disagreeing with the idea that this method is not achieving what it set out to. I am explaining the train of thought that led us here. I am not asserting that it was correct or useful, only that it's the reasoning. I am explaining this because your original issue was a question: ""why do this?"". I have pointed you to the why.

However, both myself and @piotr-dobrogost have *already said* that we are open to changes in behaviour here. I don't know who you're trying to convince: there is no-one here arguing that the status quo is good. I feel like you're arguing against a non-existent opponent.

The TL;DR is this: this method seems like it's a waste of everyone's time, and probably should be removed. However, as the Requests project has learned all too clearly, *actually removing it* is likely to be a backward incompatible change. There are some hopes that we will remove or drastically rewrite this entire module, though, so I suspect this will go away.",Lukasa,piotr-dobrogost
3759,2016-12-09 15:04:20,"@saft Seems like a traditional case of crossed wires due to text-based communication. Thankyou for apologising: I'm sorry as well, I clearly misread your tone.

Back to being productive. 👍 

So, I think this is wrong. I am absolutely not an expert on cookies: they are the part of HTTP I know least about. However, as I understand from some of my collaborators on this project, our cookie stack is ultimately not very good. Part of this is because of the involvement of the standard library, whose cookie stack is *also* not very good.

So right now the cookie stack was worked on to the point where all the complaints went away. And they have! The cookie functionality is one of the least-complained-about parts of Requests. So that leads to a strong inclination to avoid making incremental changes to it, and to instead more-or-less leave it alone until such time as we can totally replace it.

Ultimately, I think the reason this patch was added was a sense of defensiveness against a problem that didn't fully exist. An enormous part of that is my responsibility: I reviewed and merged #1440, and ultimately probably should have done more investigating to prove that the problem existed. Certainly, trying today, I can't find any immediate adverse affects from removing that block of code. 

However, now that that has happened, there is a problem of inertia. Ultimately, it doesn't seem to be *hurting* many people. It's entirely circumventable, because we allow plugging in other cookiejar objects, so anyone it does inconvenience can nonetheless work around it. And it has been there for three years without so much as a whimper.

My strong inclination, then, is to leave it alone until such time as we consider a substantial change to the cookie stack. At that time, we can also include substantial testing of the cookie functionality so that we can do a much better job of indicating why certain bits of functionality are present.",Lukasa,saft
3758,2016-12-09 08:45:11,LGTM. Thanks for this @nateprewitt! :sparkles:,Lukasa,nateprewitt
3755,2016-12-08 04:51:21,"Hey @luwm001, this appears to be a duplicate of #3710. I believe you'll be able to solve this issue by updating your `cryptography` module version.",nateprewitt,luwm001
3753,2016-12-08 20:19:34,"@oschwald, yeah that was the intent (or I had understood it to be part) of #3673 which ended up causing the failure you're experiencing. The idea was to allow the user to supply unicode encoded strings (bytes) and we'll simply base64 encode bytes rather than messing with string encoding issues.

I can throw a patch for this together tonight or tomorrow morning if you're not interested in working on this.",nateprewitt,oschwald
3752,2016-12-07 21:02:53,"Ok, so right now this looks like a ""can't repro"" situation. Might be worth looking at your install @juokaz, seems to be misbehaving a bit.",Lukasa,juokaz
3748,2016-12-03 20:36:49,"@arunchandramouli have you considered setting a `timeout`?

Beyond that, I'd advise you to not ask questions on a defect (issue) tracker. Instead, ask questions on [StackOverflow](stackoverflow.com).",sigmavirus24,arunchandramouli
3748,2016-12-05 10:25:04,"@arunchandramouli There are two possibilities. Either 1) the network has hung, in which case a timeout will help because it'll throw an exception; or 2) the response is infinite in size. If the response is infinite in size and you don't use the `stream=True` parameter, Requests will attempt to read the entire response body. That obviously won't work. Eventually, such a use of Requests will cause you to run out of memory.",Lukasa,arunchandramouli
3748,2016-12-05 10:32:23,"@Lukasa - Great ! but when I did telnet proxy followed by get urlname, it actually returns me the content.it works that way. But I tried using requests.get(), urllib.urlopen() and httplib, it all fails, sounds a bit interesting. ",arunchandramouli,Lukasa
3748,2016-12-05 11:48:09,"@arunchandramouli That sounds very much like the proxy is misconfigured. Can you show me the *complete* response you got from the proxy, including the headers? The telnet request you sent would also be good.",Lukasa,arunchandramouli
3748,2016-12-05 11:55:51,"@Lukasa The Proxy / Normal request doesn't request anything at all , it keeps hanging unless I issue a timeout. Where as Telnet GET returns the url content. I couldn't share due to privacy and client protocol issues.",arunchandramouli,Lukasa
3748,2016-12-05 11:56:46,"@arunchandramouli If you can't share the data then I'm afraid we're essentially unable to help you. The best I can tell you is that the response is almost certainly ill-formed, which is why none of the Python HTTP libraries you've tried can parse it.",Lukasa,arunchandramouli
3748,2016-12-05 12:08:35,@arunchandramouli Can you provide a stacktrace for your timeout?,Lukasa,arunchandramouli
3748,2016-12-05 12:24:06,"@Lukasa requests.exceptions.ReadTimeOut:HTTPSConnectionPool(host='***',port=443).Read Timed out.

api.py -> sessions.py -> adapters.py (normal flow for HTTP/HTTPS connection)
I couldn't copy the trace from the server",arunchandramouli,Lukasa
3748,2016-12-05 13:20:45,"@arunchandramouli I care mostly about the bit at the end, where we have all this:

`requests.exceptions.ConnectionError: HTTPConnectionPool(host='www.somebadurl.com', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<requests.packages.urll
onnection object at 0x0000000003E2E1D0>: Failed to establish a new connection: [Errno 11004] getaddrinfo failed',))`

I need to know what you see in that section.",Lukasa,arunchandramouli
3748,2016-12-05 14:34:23,"This conversation is now going in circles. To prevent further noise (and little signal) being delivered to roughly 1000 people, I'm going to lock this conversation. @arunchandramouli we've given you sufficient amounts of detail that we require and you consistently ignore it. If you want further help, I suggest you seek out a forum where that is appropriate (not a defect tracker).",sigmavirus24,arunchandramouli
3747,2016-12-02 22:31:37,"Hey @arao-nnl, in order to get the behaviour you're looking for, you'll need to use a slightly different mechanism. `max_redirects` is intended to raise an exception halting the request in the event of a redirection loop, or excessive redirects.

For your issue of trying to simply halt before a redirect is performed, I suggest using the `allow_redirects` parameter. This will retrieve the 302 and then leave the object untouched until you decide how to proceed (such as only reading the Location header).

",nateprewitt,arao-nnl
3745,2016-12-05 12:55:02,"Yeah, I think more testing here would be fab. Not sure how we're going to get it though, especially if @alexwlchan doesn't have time to do it.",Lukasa,alexwlchan
3743,2016-12-02 00:46:01,"Hey @timothyjlaurent, the issue you're hitting is with the server rather than Requests. It looks like the base domain jamanetwork.com (where the redirect points you) may not want it crawled. The server has blacklisted any request with a User-Agent containing the word ""python"" or ""curl"".

Given their robots.txt doesn't specify a restriction on your request though, I think we can shrug and work around it.  Simply supplying a custom 'User-Agent' header, whether it's your own or from a standard browser, will fix this.
",nateprewitt,timothyjlaurent
3743,2016-12-02 00:49:55,Nicely caught @nateprewitt ,sigmavirus24,nateprewitt
3743,2016-12-02 02:30:10,"Thanks @nateprewitt 
",timothyjlaurent,nateprewitt
3742,2016-12-01 12:50:41,"@megies Interesting. However, you should check for that particular pattern of exception handling in your code and dependencies to see if you can find it.",Lukasa,megies
3742,2016-12-01 15:38:04,"@megies Out of interest, do you know what in your project or your dependencies imports future?",Lukasa,megies
3739,2016-11-30 23:52:35,"Hey @obestwalter, I talked to @Lukasa about this a few weeks ago. To quote a bit of his response:

>Requests moved away from Travis because it made a lot of live network requests in its test suite and Travis had frequent network outages that caused real problems with the test run. 

While a lot of these issues have been resolved, I think the general sentiment is Travis generates a fair amount of noise for the repo owner and has historically had quirks. Until recently, Requests ran CI on a Jenkins server, but that's since been removed.

As for tox, I think most of Requests' contributors are running a local tox.ini file for testing changes. If you run that in conjunction with something like [pyenv](https://github.com/yyuu/pyenv), you have an easy local equivalent of a Travis environment which has worked for the most part the last several months.

Note this may no longer be the state of things, I just wanted save Lukasa from having to reiterate what he's conveyed recently via other channels.

Here's the last couple of Kenneth's comments on the state of Requests' CI/Travis in 2016 too. [[Feb](https://github.com/kennethreitz/requests/pull/2991#issuecomment-178813436)] [[Apr](https://github.com/kennethreitz/requests/pull/3096#issuecomment-211701179)]",nateprewitt,obestwalter
3739,2016-12-01 08:18:37,@obestwalter All of these decisions are fundamentally @kennethreitz's: the other maintainers have no objection to re-adding Travis support and a toxfile.,Lukasa,obestwalter
3739,2016-12-01 11:46:33,"Hello @Lukasa and @nateprewitt,

thanks for the explanation. Why is the tox.ini then not part of the project? Is there a problem to get it working for everyone involved? My naive thinking would be that once you have tox properly configured for the project it works for everyone involved. 

So, if there are any concrete problems/bugs that prevent your project from adopting tox properly, just let us know here: https://github.com/tox-dev/tox (yes, we finally moved to Github :)).",obestwalter,Lukasa
3739,2016-12-01 11:46:33,"Hello @Lukasa and @nateprewitt,

thanks for the explanation. Why is the tox.ini then not part of the project? Is there a problem to get it working for everyone involved? My naive thinking would be that once you have tox properly configured for the project it works for everyone involved. 

So, if there are any concrete problems/bugs that prevent your project from adopting tox properly, just let us know here: https://github.com/tox-dev/tox (yes, we finally moved to Github :)).",obestwalter,nateprewitt
3738,2016-11-30 21:24:39,"@Lukasa, I think it would be beneficial readding the test after this reversion. It still provides a useful check going forward and will pass regardless of the tightened scheme check.",nateprewitt,Lukasa
3738,2016-12-01 06:00:01,"@Lukasa sorry for being a hound but do you have an eta on this 2.12.3 release? We just lost a day of dev / test trying to figure out what broke docker-py until we found this. Thx 

- MS",mshahpalerra,Lukasa
3738,2016-12-01 06:04:02,@mshahpalerra just pin your requests dep to an older  2.11.x version and you'll be ok. @Lukasa please release a new version of requests ASAP or this will be another left-pad.,marcosnils,mshahpalerra
3738,2016-12-01 06:04:02,@mshahpalerra just pin your requests dep to an older  2.11.x version and you'll be ok. @Lukasa please release a new version of requests ASAP or this will be another left-pad.,marcosnils,Lukasa
3738,2016-12-01 06:06:58,@marcosnils  - already being done but that has some other complications as we are changing this pinning on code that is pre prod so has to be retested (not your problem i know :) ) - thanks for the tip,mshahpalerra,marcosnils
3738,2016-12-01 08:02:34,"@marcosnils Well that's a ludicrously hyperbolic thing to say.

The left-pad incident broke all users of the package. This breaks docker-py, and docker-py only. The vast majority of our users have not noticed a problem with v2.12.2. There are several bug reports both here and on docker-py, there is a good temporary workaround, so if it's all the same to you I'm going to take long enough over this to get it *right*.

It is much more important to me that we don't keep rushing out v2.12.x releases that require further panicked fixes than it is that we make it possible for you to avoid placing pins in your dependency tree. In this case, the patch that we're reverting was added for a reason, and I'd like to ensure that that reason is still covered rather than re-break someone else.",Lukasa,marcosnils
3738,2016-12-01 08:04:03,"@mshahpalerra As to the ETA of v2.12.3, it will be as soon as I can get confirmation from @tiran that the problem he was originally trying to fix is still fixed in this branch. If it isn't, then it will be as soon as I can work with him to build a new patch that avoids this specific issue.",Lukasa,mshahpalerra
3738,2016-12-01 10:44:47,"ACK

Custodia's test suite for my http+unix adapter is passing with @Lukasa 's branch.",tiran,Lukasa
3738,2016-12-01 10:45:23,"Ok, let's ship a 2.12.3 then. Thanks @tiran!",Lukasa,tiran
3738,2016-12-22 14:03:52,"> Custodia's test suite for my http+unix adapter is passing with @Lukasa 's branch.

I it really amazing that `http+unix` t will work with python-requests-3.0. Unfortunately, python-requests-3.0 has not been released yet. But 2.12.3 is released and already in some distributions: fedora-26 and debian unstable and might get to debian testing in few days.

2.12.0 introduced IDNA-encoding logic which broke `http+unix://` 2.12.3 with this patch reverted the fix due to #3735 but did not provide any alternative solution. And therefore `http+unix://` is broken again.

Is there a reason why cannot be fixed in 2.12.* as well?",lslebodn,Lukasa
3738,2016-12-22 18:34:52,"Hmm, so you're right. This is an issue in that this path will pass through IDNA without throwing an exception, but IDNA is forcing it to lowercase. This is valid behaviour for a hostname which should be case insensitive but doesn't work for paths (which really shouldn't be IDNA encoded in the first place). This is a bug, but the way unix sockets have worked in the past is kind of incorrect too. It supplies a path in place of the hostname we're expecting.

I don't have an immediate solution for this other than suggesting you store the socket file in a lower-cased path. We'll have to wait for @Lukasa's opinion on this and I'll take a deeper dive if it's deemed needed.",nateprewitt,Lukasa
3738,2016-12-22 19:12:43,"So, I did some more investigating and this actually is completely unrelated to IDNA.

shazow/urllib3#911 introduced a forced lowercasing of all host names in urllib3 which was released in urllib3 1.17. The version of urllib3 used in Requests prior to 2.12.0 was 1.16 which is why we hadn't hit this before. urllib3's behaviour here is completely correct because host names should be able to be lowercased without affecting the URIs usability.

As I said above, passing a path has worked in the past but is technically not in line with how we're parsing the URI. Again, I think we'll have to wait for @Lukasa to weigh in here. ",nateprewitt,Lukasa
3738,2016-12-22 19:23:26,"> urllib3's behaviour here is completely correct because host names should be able to be lowercased without affecting the URIs usability.

Right.

> As I said above, passing a path has worked in the past but is technically not in line with how we're parsing the URI. Again, I think we'll have to wait for @Lukasa to weigh in here.

Since there is no specification around UNIX socket URIs, I would think we'd treat that as a path, not a hostname. *shrug*",sigmavirus24,Lukasa
3738,2016-12-23 09:33:24,"@nateprewitt I don't think we do store `/tmp/path/out.socket` as the `host`. I think we only store it if its urlencoded when we receive it, which seems more reasonable to me. Though admittedly I'm not the URI expert (that'd be @sigmavirus24).",Lukasa,sigmavirus24
3738,2016-12-23 09:33:24,"@nateprewitt I don't think we do store `/tmp/path/out.socket` as the `host`. I think we only store it if its urlencoded when we receive it, which seems more reasonable to me. Though admittedly I'm not the URI expert (that'd be @sigmavirus24).",Lukasa,nateprewitt
3738,2016-12-23 16:45:08,"@Lukasa, you're right, I was conflating behaviours in my explanation. %-encoding the path is a hack around `prepare_url` because Requests refuses URLs without a host at the model level, rather than adapter. The host is mandated for http/https schemes but becomes a grey area with the use of ""http+"".

If we want to support ""http+{unix,docker,???}"", which seems poorly defined but in use, we'd probably need to relax the hard stop at no host until we get to the adapter. This would allow someone to pass `/tmp/path/out.socket` and we can treat it correctly as a path, without needing a work around.  Then we can avoid modifying correctly functioning code to accommodate the hack. If we don't intend to support ""http+"" in 3.0.0 then this is probably a moot point.",nateprewitt,Lukasa
3738,2016-12-23 18:22:21,"The % encoding of the path to the Unix socket file or abstract namespace is mandatory. How else would you distinguish between the file path and the HTTP path? Some implementations probe ever path segment until a socket file is found and then magically treat the remaining path segments as HTTP path. That's a dangerous hack. The socket path may include non-ASCII chars or a NULL byte for abstract namespace.

Am 23. Dezember 2016 17:45:25 MEZ, schrieb Nate Prewitt <notifications@github.com>:
>@Lukasa, you're right, I was conflating behaviours in my explanation.
>%-encoding the path is a hack around `prepare_url` because Requests
>refuses URLs without a host at the model level, rather than adapter.
>The host is mandated for http/https schemes but becomes a grey area
>with the use of ""http+"".
>
>If we want to support ""http+{unix,docker,???}"", which seems poorly
>defined but in use, we'd probably need to relax the hard stop at no
>host until we get to the adapter. This would allow someone to pass
>`/tmp/path/out.socket` and we can treat it correctly as a path, without
>needing a work around.  Then we can avoid modifying correctly
>functioning code to accommodate the hack. If we don't intend to support
>""http+"" in 3.0.0 then this is probably a moot point.
>
>-- 
>You are receiving this because you were mentioned.
>Reply to this email directly or view it on GitHub:
>https://github.com/kennethreitz/requests/pull/3738#issuecomment-269016139

Sent from my Android phone with K-9 Mail.",tiran,Lukasa
3738,2016-12-23 18:27:37,"@tiran thanks, for clarifying! I'll defer to those of you that are much more well versed in this space than I am :)

I'll open the issue in urllib3 with Lukasa's originally suggested fix.",nateprewitt,tiran
3736,2016-11-30 19:28:10,@hummus should we file a ticket at [python-gitlab](https://github.com/gpocentek/python-gitlab/issues)'s issue tracker? I would do it myself but I don't know enough about the problem,hangtwenty,hummus
3735,2016-11-30 23:06:24,"Thanks @Lukasa for chiming in and looking into this!

Being able to rely on `requests` to make docker-py work has been a great boon for us, especially the ability to handle different transport formats (HTTP/s, UNIX sockets, and more recently Windows named pipes) through the same interface is a powerful feature for us.

If you see any way we could keep this flexibility while making things easier on the requests library to support our use-case(s), I'd be happy to hear them and consider any changes.

Thanks again for your help.",shin-,Lukasa
3734,2016-11-30 15:18:22,@Lukasa can do,graingert,Lukasa
3734,2016-11-30 15:21:09,@Lukasa it's 'http+docker://localunixsocket/v1.24/images/create' but that gets handled by the mounted `docker.transport.unixconn.UnixAdapter`,graingert,Lukasa
3734,2016-11-30 15:37:01,@Lukasa someone might come along and register the gtld localunixsocket,graingert,Lukasa
3734,2016-11-30 15:39:57,"@graingert That's fine, it won't matter. The adapter is selected based on a longest-prefix match, so the docker-py adapter will still be selected and knows it doesn't have to do a DNS lookup.",Lukasa,graingert
3734,2016-11-30 15:42:10,@graingert has pointed out that in situations like this we should probably throw an exception if we were supposed to add any query parameters to the request.,Lukasa,graingert
3734,2016-11-30 16:02:18,@Lukasa should this handling be moved to the adaptor?,graingert,Lukasa
3729,2016-11-26 11:17:29,"@pdknsk It would be interesting to see if you continue to have this problem on Python 3 with `pip install requests[security]`. That will also bring in PyOpenSSL, which should cause the problem again.",Lukasa,pdknsk
3729,2016-11-28 12:32:10,"Ok, the underlying issue at PyOpenSSL has had a fix merged. This should *drastically* improve our performance in this edge case. Thanks for the report @pdknsk and thanks for working with us to narrow down the problem! :sparkles:",Lukasa,pdknsk
3729,2016-12-05 09:19:04,"...annnnnd it's *also* a bug in MacOS: https://github.com/pyca/pyopenssl/pull/578#issuecomment-264800709

@pdknsk what have you *done*",njsmith,pdknsk
3729,2016-12-05 09:19:09,"So @pdknsk, after further investigation prodded by @njsmith, you've actually stumbled onto *two* bugs that are wasting a lot of CPU and electricity.

The first is that CFFI should be using more efficient allocation. The second, and *much more important*, is that there is a region of allocations between about 128kB and 125MB where macOS (and, presumably, iOS/watchOS/tvOS as well) forcibly page-in and zero memory when they don't need to, wasting a ton of CPU to do it.

Nicely done!",Lukasa,pdknsk
3729,2016-12-05 09:19:09,"So @pdknsk, after further investigation prodded by @njsmith, you've actually stumbled onto *two* bugs that are wasting a lot of CPU and electricity.

The first is that CFFI should be using more efficient allocation. The second, and *much more important*, is that there is a region of allocations between about 128kB and 125MB where macOS (and, presumably, iOS/watchOS/tvOS as well) forcibly page-in and zero memory when they don't need to, wasting a ton of CPU to do it.

Nicely done!",Lukasa,njsmith
3718,2016-11-23 21:38:02,"@Lukasa Do you mean a test that for an empty `requests.Response()` object, the `content` property is None?",nsoranzo,Lukasa
3718,2016-11-23 23:58:51,"@nateprewitt I'd be happy to change the test to:

or anything that makes:

return `None`. I think most programmers would expect this and that the traceback is a regression.",nsoranzo,nateprewitt
3718,2016-11-24 11:22:33,"@Lukasa I'll try to explain why I think that this is the expected behaviour:

- `requests.Response` class is clearly part of the `requests` API and creating such objects is useful when dealing with connection exceptions
- the only `__init__` method of `requests.Response()` has no parameters
- using a property of a legitimately created object shouldn't raise an error, unless there is a very good reason

>  Number one is to change this patch such that we tolerate the actual problem: namely, that response.raw is None.

So, is

a good solution for you?",nsoranzo,Lukasa
3718,2016-11-24 12:36:10,"> I think I'm leaning towards number one, if only because we've handled this kind of problem in the past (admittedly with an overbroad except block).

This makes sense to me. We can add the requirement that raw not be None in 3.0

@nsoranzo adding the explicit check for None works for me.",sigmavirus24,nsoranzo
3718,2016-11-24 12:57:00,@sigmavirus24 I've added the explicit check for None and also a test as requested by @Lukasa.,nsoranzo,Lukasa
3718,2016-11-24 12:57:00,@sigmavirus24 I've added the explicit check for None and also a test as requested by @Lukasa.,nsoranzo,sigmavirus24
3717,2016-11-23 10:17:29,Thanks @afeld and @nateprewitt!,Lukasa,afeld
3717,2016-11-23 10:17:29,Thanks @afeld and @nateprewitt!,Lukasa,nateprewitt
3716,2016-11-22 19:54:19,"@PatriotRDX have you tried using `pip install requests==2.2.1`?

Edit: Sorry, let me clarify for a moment. Are you looking for a version that is lower than 2.12 but higher than 2.2.0? The minor version number is linear, so 2.2.0 is an older version than 2.12.1.",nateprewitt,PatriotRDX
3716,2016-11-22 20:00:40,"@nateprewitt thanks this helped. However, it seems like [this documentation](https://pypi.python.org/pypi/requests/2.2.1) should be updated, since it's just flat out wrong.",PatriotRDX,nateprewitt
3716,2016-11-22 20:01:54,@PatriotRDX which part of the documentation do you feel is incorrect?,nateprewitt,PatriotRDX
3716,2016-11-22 20:04:49,"@nateprewitt the documentation here says to run `pip install requests` in Terminal. However, this only installs 2.12.1. Not to mention the documentation here is exactly the same as the documentation for 2.12.1.",PatriotRDX,nateprewitt
3716,2016-11-22 20:13:55,"@PatriotRDX so this is a bit harder because this is a reflection of the README.rst at the time of release. `pip install requests` is the correct command for installing requests on your machine. If you require an older version, the standard usage of pip has you specify a specific version number (`requests==2.2.1`) or a range (`requests>=2.6.0`). You can find more documentation on pip [here](https://pip.pypa.io/en/stable/).

This should be specified by whichever package you're trying to use in either the requirements.txt file, or setup.py. If it isn't, that should probably be brought up with the package maintainer.

@Lukasa or Kenneth may have different opinions on retroactively updating older release docs, but I'm not sure it's likely.",nateprewitt,PatriotRDX
3716,2016-11-22 20:25:24,@nateprewitt there is no way to update it. @PatriotRDX we expect users to have some understanding of how to specify requirements themselves and use pip.,sigmavirus24,PatriotRDX
3716,2016-11-22 20:25:24,@nateprewitt there is no way to update it. @PatriotRDX we expect users to have some understanding of how to specify requirements themselves and use pip.,sigmavirus24,nateprewitt
3716,2016-11-22 20:35:43,"> we expect users to have some understanding of how to specify requirements themselves and use pip.

@sigmavirus24 wow that's brutal, only time I've used Python has been today.",PatriotRDX,sigmavirus24
3716,2016-11-22 20:43:50,"@PatriotRDX, I think there's just been a bit of confusion with what you were asking. I took a peek at the repository you're using and the requirements.txt is configured correctly. 2.12.1 is the latest release of Requests which satisfies the requirements of `requests>=2.2.0`.

If you use the command `pip install -r requirements.txt` in the future, version handling should be taken care of for you. `pip` is the standard package manager for Python, so you typically won't have to worry about downloading anything via your browser.

Hopefully this helps.",nateprewitt,PatriotRDX
3716,2016-11-22 20:48:53,"@nateprewitt thank you for your help. I still get an error when running 2.12.1, so I'll just stick to 2.2.1 for now. Thank you again.",PatriotRDX,nateprewitt
3716,2016-11-22 20:54:28,@lutzhorn my issue was solved by @nateprewitt at the beginning of this thread. I don't have any other problems that need to be solved.,PatriotRDX,lutzhorn
3716,2016-11-22 20:54:28,@lutzhorn my issue was solved by @nateprewitt at the beginning of this thread. I don't have any other problems that need to be solved.,PatriotRDX,nateprewitt
3716,2016-11-22 20:58:09,"@lutzhorn it doesn't matter, as long as it works, it works. I don't need 2.12.1, this is only one small program I'm testing out.",PatriotRDX,lutzhorn
3716,2016-11-22 21:08:28,"@nateprewitt, @lutzhorn, @sigmavirus24 thank you all for your help. Have a nice day.",PatriotRDX,lutzhorn
3716,2016-11-22 21:08:28,"@nateprewitt, @lutzhorn, @sigmavirus24 thank you all for your help. Have a nice day.",PatriotRDX,sigmavirus24
3716,2016-11-22 21:08:28,"@nateprewitt, @lutzhorn, @sigmavirus24 thank you all for your help. Have a nice day.",PatriotRDX,nateprewitt
3716,2016-11-22 22:13:52,"For posterity, it's not a requests problem. It was due to some pretty poor version checking in the project he was using. @PatriotRDX the error you're getting was fixed [here](https://github.com/wummel/linkchecker/commit/c2ce810c3fb00b895a841a7be6b2e78c64e7b042) if you care to follow it, however that fix isn't on PyPi which is why you're still seeing the error.",drpoggi,PatriotRDX
3716,2016-11-23 14:04:42,"@drpoggi thank you for the extra information on my other issue. I've got it working with 2.2.1 so it doesn't matter to me, however, someone else may find it useful.",PatriotRDX,drpoggi
3714,2016-11-21 19:48:00,Thanks @Lukasa !  Posted an issue in (what I hope is) the main urllib3 repo.  ,aseering,Lukasa
3713,2016-11-21 17:13:39,"This looks entirely reasonable to me, thanks @tiran!",Lukasa,tiran
3713,2016-11-21 19:13:54,"Cool, thanks @tiran! All tests pass, so it seems like it's good to go. Thanks so much! :sparkles: :cake: :sparkles:",Lukasa,tiran
3711,2016-11-21 20:42:58,"@johnabooth It doesn't look like you've actually opened a pull requests against the upstream repo [kjd/idna](https://github.com/kjd/idna). You'll need to visit that repo, use the ""New Pull Request"" button and select your branch `johnabooth-jython27-fix`. The maintainer of that repo should be able to perform a review on your proposed changes.",nateprewitt,johnabooth
3710,2016-11-28 18:10:23,@andreleotorres 🙏🏼,kennethreitz,andreleotorres
3709,2016-11-20 03:54:15,"@drpoggi Can you explain how users can create the same output with r.content.  The output from r.content is different from r.json().  Also how do you provide preferred indentation level and separators. As far as I know it doesn't take any arguments. You can provide preferred indentation level and separators to r.sjson through key value arguments. You can look at the documentation for the dumps function in the json module to give custom arguments.  The whole point of creating the function was so that users can get the json object in a human readable form with simple readable code. Also how do you know there is a freeze in requests. Was there a post somewhere, or is it in the documentation somewhere?

EDIT: Also thanks for the feedback.
",rmhasan,drpoggi
3709,2016-11-20 04:05:51,"@rmhasan, thanks for your interest in contributing to Requests. You can find information about the freeze and other contributing topics in Requests [Contributor's Guide](http://docs.python-requests.org/en/master/dev/contributing). The specific information you're looking for is under [feature requests](http://docs.python-requests.org/en/master/dev/contributing/#feature-requests).
",nateprewitt,rmhasan
3709,2016-11-20 04:14:48,"@nateprewitt Thanks. 
",rmhasan,nateprewitt
3706,2016-11-18 16:51:49,"@kmala No. `client.crt` is the _root_ certificate, and so is not the CA. The `client.crt` is the CA that should be in the CApath.
",Lukasa,kmala
3706,2016-11-21 17:29:20,Thanks @Lukasa @reaperhulk .Its indeed issue with the certificate. I used a certificate  without AKI and it works fine.,kmala,reaperhulk
3706,2016-11-21 17:29:20,Thanks @Lukasa @reaperhulk .Its indeed issue with the certificate. I used a certificate  without AKI and it works fine.,kmala,Lukasa
3705,2016-11-17 19:29:34,"@nateprewitt Thanks!  That was the issue...somewhere in the jython path was a json package by patrickdlogan@[...].com, and requests was finding that one instead.
",j-christ,nateprewitt
3704,2016-11-17 18:19:37,"@nateprewitt Yeah, thought about that...might make sense to give a value of `128` or something reasonable in the example, and just say that it can be changed. I'm no expert on this, so whatever y'all think!
",afeld,nateprewitt
3704,2016-11-18 20:10:14,"@afeld if you think this change is agreeable and have the inclination, feel free to throw the PR together. Otherwise, I'll try to open something next week.
",nateprewitt,afeld
3702,2016-11-17 14:34:16,"@darakt You're using the `cert` argument, but you want to be using the `verify` argument.
",Lukasa,darakt
3701,2016-11-17 13:44:55,"@sapran Ok, in that case I'll need you to try to run this under pdb and investigate why there appears to be no `_x509` attribute on that X509 object. In particular it would be good to check it on line `310` of `requests.packages.urllib3.contrib.pyopenssl`, to see what the return value of `get_peer_certificate` was and to investigate it there.
",Lukasa,sapran
3701,2016-11-18 08:36:07,"@larryhq Can you please do the investigation I outlined above?

To anyone else who hits this issue, _please don't just say ""me too""_. Please _investigate_ this issue as outlined above, because right now none of the maintainers can reproduce the bug.
",Lukasa,larryhq
3701,2016-11-19 20:10:35,"@skob Please please _please_ read the other comments in this thread before posting. I have asked all people reporting to do an investigation and so far no-one has.

**Let me be clear**: any furthe ""me too"" posts that don't acknowledge the investigation I have asked people to do will be summarily deleted.
",Lukasa,skob
3701,2016-11-20 09:39:20,"@Lukasa I had the same error, read your comments, reinstalled both libraries, pyopenssl and cryptography, and it worked. no more error. Thanks for your help. :) 
",camara-,Lukasa
3701,2016-11-21 11:35:22,I had the same issue on a new MBP with a 'vanilla' macOS Sierra.  @Lukasa I uninstalled and re-installed pyOpenSSL and Cryptography and it all worked fine.  Thanks.,markyj,Lukasa
3701,2016-11-21 20:24:40,So can people also let us know the result of running `python -c 'import OpenSSL; print(OpenSSL.__version__)'` I suspect this is exactly what @reaperhulk has suggested and what @zhang-jingwang seems to have confirmed.,sigmavirus24,reaperhulk
3701,2016-11-21 20:24:40,So can people also let us know the result of running `python -c 'import OpenSSL; print(OpenSSL.__version__)'` I suspect this is exactly what @reaperhulk has suggested and what @zhang-jingwang seems to have confirmed.,sigmavirus24,zhang-jingwang
3701,2016-11-25 10:06:47,@jachymb PyOpenSSL *cannot* be not installed. Please open your Python shell and type `import OpenSSL`.,Lukasa,jachymb
3701,2016-12-01 12:06:32,"@sapran Please reinstall PyOpenSSL and its dependencies, and you will find it is not, in fact, broken.",Lukasa,sapran
3701,2016-12-09 13:13:35,"@rjungbeck I cannot stress this enough: this problem is *not* part of Requests.

Try a brand new virtual environment. Please also verify in your current environment that PyOpenSSL is the version you think it is. We do not support PyOpenSSL 0.13 any longer.",Lukasa,rjungbeck
3701,2017-02-03 01:10:40,thanks @ronaldddilley ,puneetloya,ronaldddilley
3700,2016-11-17 18:07:31,"Hi @kennethreitz , I was wondering why my pull request got reverted.
",rmhasan,kennethreitz
3698,2016-11-16 20:35:33,"@Makman2 it's not immediately obvious where the issue lies here. The specific traceback you're hitting is because the response value is None. I don't see any changes in the critical path for Requests since 2.11.1 that would affect this.

I think it's possibly a gap in requests_mock that's not returning a valid response with a Requests-2.12 request.

Would you be able to provide a repro of this without using the requests_mock module?
",nateprewitt,Makman2
3698,2016-11-16 20:59:22,"Yup, I think @nateprewitt has it: if you can't get a repro that doesn't involve requests_mock I think this issue is almost certainly with their code.
",Lukasa,nateprewitt
3698,2016-11-17 00:16:10,"@Makman2 I went **way** down the rabbit hole and believe I've found an answer. This is in fact the product of a combination of changes in 2.12 and the way you're mocking objects in your tests.

You're currently mocking responses with a Response object that only the status_code initialized.
This is a really simple repro of your issue.



`send()` in `requests.Session` always calls `.content` on streams to ensure they're consumed. When this is called on the empty response, it raises an AttributeError previously caught. This exception was removed in 327512f, oddly enough as a fix for requests_mock, because it was deemed too broad. The implication of this is we now require all adapters to support `raw` in their responses which conflicts somewhat with our documentation. I'll leave it to @Lukasa to determine the next steps.

As for getting your tests working with Requests-2.12, I'd suggest simply setting `Response.raw = io.BytesIO()`, or some file-like object, [here](https://github.com/coala/coala-bears/blob/master/tests/general/InvalidLinkBearTest.py#L48).
",nateprewitt,Lukasa
3698,2016-11-17 00:16:10,"@Makman2 I went **way** down the rabbit hole and believe I've found an answer. This is in fact the product of a combination of changes in 2.12 and the way you're mocking objects in your tests.

You're currently mocking responses with a Response object that only the status_code initialized.
This is a really simple repro of your issue.



`send()` in `requests.Session` always calls `.content` on streams to ensure they're consumed. When this is called on the empty response, it raises an AttributeError previously caught. This exception was removed in 327512f, oddly enough as a fix for requests_mock, because it was deemed too broad. The implication of this is we now require all adapters to support `raw` in their responses which conflicts somewhat with our documentation. I'll leave it to @Lukasa to determine the next steps.

As for getting your tests working with Requests-2.12, I'd suggest simply setting `Response.raw = io.BytesIO()`, or some file-like object, [here](https://github.com/coala/coala-bears/blob/master/tests/general/InvalidLinkBearTest.py#L48).
",nateprewitt,Makman2
3695,2016-11-19 20:13:09,"I guess we should probably clarify what we're attempting to do here. The above comment was in reference to the the inability to use IPv6 with Requests 2.12+. Personally, I think that definitely needs to be addressed, the other complaints I have no opinion on.

The fallback approach though essentially gives carte blanche to anything that can be ASCII encoded, which seems to defeat some benefits of doing IDNA encoding. They've special cased `.` delimiters in the IDNA library, I'm assuming for things like subdomains, which allows IPv4 addresses to pass through (unintentionally?). However, they don't appear to have taken into account IPv6 style addresses with `:` delimiters or wrapping brackets.

From a cursory reading of [RFC5891](https://tools.ietf.org/html/rfc5891), it seems like IDNA is only intended for domain names. That would suggest to me we shouldn't be passing IP addresses into this function to begin with.

If we're ONLY attempting to handle IPv6, we can check to skip `host` values that are entirely numeric or delimiters. Otherwise, if we're entertaining allowing things like underscores then maybe the passthrough is the only viable option to avoid constantly programming around use cases.

**Edit**: Sorry @Lukasa I just realized I essentially reiterated your entire thought process from the other thread. Vendoring in a dependency is likely not what we want to do. `any([c.isalpha() for c in host])` is probably kludgy enough to keep out of the main path for Requests. So I guess that leaves the bypass.
",nateprewitt,Lukasa
3695,2016-11-25 13:17:57,"Alright, I'm going to merge this now. @sigmavirus24 feel free to give feedback regardless, we can always revisit this. =) Thanks @nateprewitt!",Lukasa,sigmavirus24
3695,2016-11-25 13:17:57,"Alright, I'm going to merge this now. @sigmavirus24 feel free to give feedback regardless, we can always revisit this. =) Thanks @nateprewitt!",Lukasa,nateprewitt
3691,2016-11-16 08:39:04,"Thanks @Cleod9! :sparkles: :cake: :sparkles:
",Lukasa,Cleod9
3690,2016-11-16 08:38:31,"Thanks @galgeek!
",Lukasa,galgeek
3688,2016-11-15 20:30:20,"Thanks @nateprewitt! :sparkles: :cake: :sparkles:
",Lukasa,nateprewitt
3687,2016-11-16 00:16:33,"I believe we may not need to worry about the case where the URL is a bytestring. `prepare_url` [transforms](https://github.com/kennethreitz/requests/blob/master/requests/models.py#L341-L344) it into unicode at the beginning of the method and re-encodes it on the way out.

@nlevitt were you interested in providing a patch for this? If not, I believe I have a fix ready to go but will gladly defer to you.
",nateprewitt,nlevitt
3687,2016-11-16 06:28:10,"Go for it @nateprewitt
",nlevitt,nateprewitt
3687,2016-11-22 19:13:00,"@jnozsc Thanks for the example, we've had discussion regarding underscores in a separate thread which is now locked but your issue will be resolved with #3695 soon.",nateprewitt,jnozsc
3687,2016-11-23 09:31:18,"I have the same problem, which I reported in kjd/idna#32, but it seems more to be an issue in requests than in idna.

@Lukasa's logic sounds right to me.",quantenschaum,Lukasa
3686,2016-11-15 18:59:25,"Thanks for opening this issue, @afaicode. This is currently being tracked in #3683 which will be the official issue for further discussion. I believe @Lukasa has stated underscores in host names are not behaviour we likely want to support, but you're welcome to raise objections in the appropriate issue.
",nateprewitt,afaicode
3685,2016-11-15 18:42:52,"@jaraco I believe what you're seeing is shazow/urllib3#1025. It has been patched on master for urllib3 but was caught after 1.19 released. This may be enough of an issue to warrant bumping urllib3 to 1.19.1 and adding it to a 2.12.1 release? Until then, you'll likely need to use 2.11.1 unless @sigmavirus24 has a solution via requests-toolbelt.
",nateprewitt,jaraco
3685,2016-11-15 19:44:29,"> This may be enough of an issue to warrant bumping urllib3 to 1.19.1 and adding it to a 2.12.1 release?

@nateprewitt Requests release procedures are clearly documented. Please don't make up policy that we don't follow for us.
",sigmavirus24,nateprewitt
3685,2016-11-15 19:53:38,"Sorry @sigmavirus24, I wasn't attempting to insinuate anything about policy. I was simply asking a question about severity. Thanks for pointing that documentation out, I had previously missed the sentence on vendoring changes.
",nateprewitt,sigmavirus24
3685,2016-11-15 20:11:39,"> I'm not really feeling like rushing up to a new urllib3 release for this bug. So I'm going to say that downgrading to 2.11.1 is the best course of action for now, and the next major release of Requests will fix this issue. 😊

@Lukasa @sigmavirus24 
The last major release was almost 3 years ago. I'm hoping you meant minor? I see procedure allows vendor updates in minor releases. 

Assuming you did is there anything interested parties can do to speed up the release? Minor releases seem to come ever 3-4 months. Which is a bit long to wait for 1 line of code and miss the improvements in 2.12.0. Its certainly not a pleasant situation, but having just done a release is now not the cheapest time to do another one? Given that there are minimal changes to requests to deal with.

http://docs.python-requests.org/en/master/community/release-process/
",tunezaq,Lukasa
3685,2016-11-15 20:11:39,"> I'm not really feeling like rushing up to a new urllib3 release for this bug. So I'm going to say that downgrading to 2.11.1 is the best course of action for now, and the next major release of Requests will fix this issue. 😊

@Lukasa @sigmavirus24 
The last major release was almost 3 years ago. I'm hoping you meant minor? I see procedure allows vendor updates in minor releases. 

Assuming you did is there anything interested parties can do to speed up the release? Minor releases seem to come ever 3-4 months. Which is a bit long to wait for 1 line of code and miss the improvements in 2.12.0. Its certainly not a pleasant situation, but having just done a release is now not the cheapest time to do another one? Given that there are minimal changes to requests to deal with.

http://docs.python-requests.org/en/master/community/release-process/
",tunezaq,sigmavirus24
3685,2016-11-15 20:37:51,"Sorry, I did mean minor.

@tunezaq Sure, except of course that the fix is in urllib3. That means it requires a release of urllib3, and the current master of urllib3 contains _substantial_ functional changes. That means that we'd be forcing a rapid release of urllib3 and a rapid release of Requests to ship the fix for this bug, which means that the pretty substantial release of urllib3 would get dropped before it was really ready for use.

Ultimately, it's just not as easy as it seems like it would be. Sorry!
",Lukasa,tunezaq
3685,2016-11-15 23:31:25,"@Lukasa My bad, I had thought the change was in a released version.

Last attempt :), would you consider a hotfix (cherry-pick) release to get the change into a `1.19.1` urllib3? I wasn't able to find a documented release process for the project, but that would allow this issue to be resolved very timely. I see urllib3 releases a bit more often, I would guess by the time its released again requests may not be in a place for a minor release. Seems like this is a ""critical window"" where the issue can be resolved now or in a couple months.
",tunezaq,Lukasa
3684,2016-11-15 17:51:46,"Thanks @nateprewitt!
",Lukasa,nateprewitt
3683,2016-11-15 22:56:53,"@kottenator we don't add options for things like this.
",sigmavirus24,kottenator
3683,2016-11-16 00:23:26,"Just a quick question @sigmavirus24 / @Lukasa 

Services like Amazon's SES, and Sengrid, use CNAMES with underscores in them for DKIM authentication. 

http://docs.aws.amazon.com/ses/latest/DeveloperGuide/easy-dkim-dns-records.html

As you've mentioned before, these are fine for DNS, but not for Hostnames.

When you say Hostnames, I'm assuming you're talking about DNS records that are used for browsers, or similar tools, to access data, not for records that are used for stuff like the aforementioned DKIM records?
",TetraEtc,Lukasa
3683,2016-11-16 00:23:26,"Just a quick question @sigmavirus24 / @Lukasa 

Services like Amazon's SES, and Sengrid, use CNAMES with underscores in them for DKIM authentication. 

http://docs.aws.amazon.com/ses/latest/DeveloperGuide/easy-dkim-dns-records.html

As you've mentioned before, these are fine for DNS, but not for Hostnames.

When you say Hostnames, I'm assuming you're talking about DNS records that are used for browsers, or similar tools, to access data, not for records that are used for stuff like the aforementioned DKIM records?
",TetraEtc,sigmavirus24
3683,2016-11-16 08:36:09,"@TetraEtc Exactly so. Underscores in DNS names are reasonably common (e.g. SRV records have mandatory underscores in them), but for _hostnames_ (that is, anything in the host part of a URL) they are not allowed.
",Lukasa,TetraEtc
3683,2016-11-17 01:24:35,"@mfriedenhagen the GitLab team is very receptive to bug reports and I consider this to be a problem on their end. Regardless, I believe the solution being worked on in #3695 will help you
",sigmavirus24,mfriedenhagen
3683,2016-11-17 01:39:05,"Unfortunately, I don't think #3695 will address this problem. #3695 is solely for bypassing hostnames that were IDNA encoded before the request was made. I haven't seen any comment by @Lukasa about changing how we are now handling underscores in hostnames. @sigmavirus24's advice to raise a bug report with GitLab is the way to go.
",nateprewitt,Lukasa
3683,2016-11-17 01:39:05,"Unfortunately, I don't think #3695 will address this problem. #3695 is solely for bypassing hostnames that were IDNA encoded before the request was made. I haven't seen any comment by @Lukasa about changing how we are now handling underscores in hostnames. @sigmavirus24's advice to raise a bug report with GitLab is the way to go.
",nateprewitt,sigmavirus24
3683,2016-11-17 12:58:21,"@mfriedenhagen None of those tools tolerate IDNs. In this case it's a matter of picking what you want: do users want us to correctly tolerate internationalized domain names, or do they want to do their own IDN handling? I'm prepared to believe that most users do not want to do their own IDN handling.

@rsaikali So that seems to be a bigger issue. And at this point we're in a bit of a bind because by default there aren't good tools to spot this specific problem (namely, we have an IP address here, not a hostname, so skip IDNA), which means we'd have to vendor the ipaddress module to get that to work. Alternatively, we could use some heuristics to detect possible IP addresses in hostnames and skip IDNA encoding for those as well.

I think that's definitely a problem though.

An alternative solution to this is to say that if the `idna` module fails to encode we'll just go ahead and try to encode as ASCII. If that works then we shrug our shoulders and say everything is probably ok, and if it fails we catch that and throw `InvalidURL`. @sigmavirus24, how does that sound?
",Lukasa,sigmavirus24
3683,2016-11-17 12:58:21,"@mfriedenhagen None of those tools tolerate IDNs. In this case it's a matter of picking what you want: do users want us to correctly tolerate internationalized domain names, or do they want to do their own IDN handling? I'm prepared to believe that most users do not want to do their own IDN handling.

@rsaikali So that seems to be a bigger issue. And at this point we're in a bit of a bind because by default there aren't good tools to spot this specific problem (namely, we have an IP address here, not a hostname, so skip IDNA), which means we'd have to vendor the ipaddress module to get that to work. Alternatively, we could use some heuristics to detect possible IP addresses in hostnames and skip IDNA encoding for those as well.

I think that's definitely a problem though.

An alternative solution to this is to say that if the `idna` module fails to encode we'll just go ahead and try to encode as ASCII. If that works then we shrug our shoulders and say everything is probably ok, and if it fails we catch that and throw `InvalidURL`. @sigmavirus24, how does that sound?
",Lukasa,mfriedenhagen
3683,2016-11-17 12:58:21,"@mfriedenhagen None of those tools tolerate IDNs. In this case it's a matter of picking what you want: do users want us to correctly tolerate internationalized domain names, or do they want to do their own IDN handling? I'm prepared to believe that most users do not want to do their own IDN handling.

@rsaikali So that seems to be a bigger issue. And at this point we're in a bit of a bind because by default there aren't good tools to spot this specific problem (namely, we have an IP address here, not a hostname, so skip IDNA), which means we'd have to vendor the ipaddress module to get that to work. Alternatively, we could use some heuristics to detect possible IP addresses in hostnames and skip IDNA encoding for those as well.

I think that's definitely a problem though.

An alternative solution to this is to say that if the `idna` module fails to encode we'll just go ahead and try to encode as ASCII. If that works then we shrug our shoulders and say everything is probably ok, and if it fails we catch that and throw `InvalidURL`. @sigmavirus24, how does that sound?
",Lukasa,rsaikali
3683,2016-11-21 14:27:58,"@yarikoptic It might have helped to read the discussion more carefully: the core devs are already in agreement with you, and we're taking steps to resolve the problem (see #3695). You could have saved yourself a bit of time by checking that first.

*However*, addressing the substantive portion of your complaint:

> So as far as I see it, it is actually `requests` which by using `idna` to verify that `reg-name` is a standard compliant **hostname**, unnecessarily imposes restriction that URLs must contain only hostnames in `reg-name`.

A fuller read of that section will reveal to you that the only ambiguity is between `reg-name` and `IP-literal`/`IPv4address`. Again, as noted above, the Requests team is already in agreement that Requests has inadvertently blocked IP addresses. However, the `reg-name` argument is a bad one. `reg-name` is defined with a very general ABNF because, as the RFC states, it is not limited to DNS or any specific form of lookups. The host portion of a *general* URI may cover quite a lot of name formats.

However, we aren't dealing with *general* URIs. We only support URIs that contain either IP address literals (in which case we do no name resolution) or hostnames (in which case we do an A lookup). These are not expected to be generalised names.

However, we need to follow the furrow that browsers have ploughed for us, and they have no interest in enforcing restrictions on URL formatting. This is why we're working on a fix.


**To all other readers of this issue: please do not add your 2¢ or ""me too""s. We already acknowledge this to be an issue. We are working on a fix. Please wait patiently for that fix to come through.**",Lukasa,yarikoptic
3683,2016-11-21 14:38:50,"@Lukasa Thank you very much for the rapid reply!!! very much appreciated.  And hopefully now ppl would see your statement in bold (feel free to remove my reply to not shadow it), and not e.g. `Yup, there's no plan to tolerate underscores: they're not valid in hostnames, and people should avoid using them.` just from few days back.  Also if label `Propose Close` was replaced with `Fix pending` or something like that -- could help as well.  another way is to adjust initial bug report description with pointer to a tentative fix.
Cheers!",yarikoptic,Lukasa
3683,2016-11-21 19:00:23,"@mfriedenhagen to attempt to help with your confusion, you should be looking at the IDNA standard documents (which you can discover from https://tools.ietf.org/html/rfc5890)",sigmavirus24,mfriedenhagen
3682,2016-11-15 12:17:24,"@leth That's a Python 3 specific traceback format, but Python 2 doesn't show that. And in this case, we can see where the failing import is, and it doesn't attempt to import the `gaecontrib` module. So it's definitely a different issue.
",Lukasa,leth
3682,2016-11-15 14:31:34,"@leth further, your traceback is from the requests_toolbelt, not requests itself.
",sigmavirus24,leth
3682,2016-11-15 14:35:17,"@moyotar you seem to be using [ycmd](https://github.com/Valloric/ycmd) which uses [git submodules](https://github.com/Valloric/ycmd/tree/master/third_party) to vendory dependencies. Specifically it uses python-future, which has been known to break urllib3 because of the incredibly poor way in which it _attempts_ to provide python 2 and 3 compatibility.
",sigmavirus24,moyotar
3682,2016-11-15 14:36:01,"@leth It's not unsupported, please update requests to 2.12.0 which resolves that problem.
",Lukasa,leth
3682,2016-11-15 14:37:03,"@leth Sorry, that's not quite true: the issue is fixed in urllib3 master and will come through to Requests in due course. Right now urllib3's appengine support doesn't work on Python 3.
",Lukasa,leth
3682,2016-11-15 14:38:40,"@Lukasa I installed `ycmd` which includes `requests` in windows 7. I copied it to windows 10.
",moyotar,Lukasa
3682,2016-11-15 14:41:38,"@moyotar that doesn't really answer our question of what you used to install it.

Either way, this seems (at first glance) to be an issue you should bring up with the [ycmd mailing list](https://groups.google.com/forum/?hl=en#!forum/ycmd-users).
",sigmavirus24,moyotar
3682,2016-11-15 15:35:31,"@sigmavirus24 I don't really understand what you mean. It's about my python version? If yes, it's python27.
",moyotar,sigmavirus24
3682,2016-11-16 16:49:46,"@Lukasa @sigmavirus24 Up to now, I don't kown why I got this error and have no ideas to solve it. I'd be really appreciate if you can give me some help.
",moyotar,Lukasa
3682,2016-11-16 16:49:46,"@Lukasa @sigmavirus24 Up to now, I don't kown why I got this error and have no ideas to solve it. I'd be really appreciate if you can give me some help.
",moyotar,sigmavirus24
3682,2016-11-16 18:35:44,"I agree with @Lukasa. You've also provided us minimal information. I couldn't reproduce this if I tried because I don't know what instructions you followed and while we've asked for more detail you haven't provided it. We can't help you if you won't let us.
",sigmavirus24,Lukasa
3682,2016-11-17 02:56:36,"@sigmavirus24 Sorry.I don't understand what info you need. The error output is as the pic. I followed [ycmd's building](https://github.com/Valloric/ycmd#building) to install it.
",moyotar,sigmavirus24
3682,2016-11-17 03:46:52,"@moyotar Moving ycmd from Win 7 to Win 10 is probably what's causing this, and that's the info we'd need. You're pretty vague about what you meant by ""moving"", did you reinstall, or did you copy a folder? You also mentioned Windows, and didn't actually link to ycmd's instructions for installing on Windows. You linked general build instructions that then pass you off to a different page for Windows.

If you want support for FOSS the goal should be to overflow the maintainers with information and reproducible steps for your bug. Most open source packages at this level function because the maintainers work for almost or literally free.

There's a pretty huge gulf between the stacktrace you get and what library maintainers need to actually resolve the issue, it seems like you're conflating the two.

EDIT: I didn't mean to come off so harsh, it's just that I've seen the work the Requests team puts into this library, and they will go above what's reasonably expected to help you if you give them anything to work with.
",drpoggi,moyotar
3682,2016-11-17 04:46:52,"@drpoggi Thanks! Now I decide to reinstall `ycmd` to get it work.
",moyotar,drpoggi
3681,2016-11-16 02:06:02,"thanks you @Lukasa ,I use `shadowsocks` and the version of my requests is `2.10.0` while it's used to be `2.9.1` and updated with the install of google map api;
btw, I can get a correct json format result if I hardcode the url `https://maps.googleapis.com/maps/api/geocode/json?address=1600+Amphitheatre+Parkway,+Mountain+View,+CA&key=AIzaSyBYdEWfcUNHUP3zLtc17nX-3aE4uPUkFnY` in my web browser.
",LancelotHolmes,Lukasa
3679,2016-11-14 17:58:18,"Thanks @mlissner! :sparkles:
",Lukasa,mlissner
3679,2016-11-14 18:05:04,"I'm fine with removing the second mention if folks want, but I agree with @Lukasa too. I didn't realize for ages that a request not timing out meant my program would hang until I killed it. It's a pretty critical issue in my opinion, so being super-duper, repetitively explicit doesn't seem like a bad thing to me.
",mlissner,Lukasa
3678,2016-11-14 16:55:31,"LGTM, let's do this! Thanks @nateprewitt! :sparkles: :cake: :sparkles:
",Lukasa,nateprewitt
3672,2016-11-12 04:27:43,"Good spot, @Cleod9! `domain` is definitely the keyword we want here. A PR for this would be very welcome.
",nateprewitt,Cleod9
3670,2016-11-10 19:37:44,"FYI, requestsexceptions looks like it doesn't monkey-patch `requests.exceptions`: https://github.com/openstack-infra/requestsexceptions/blob/master/requestsexceptions/__init__.py.

Either way, force reinstalling indicates that this is emphatically not our problem (as @Lukasa has already pointed out).
",sigmavirus24,Lukasa
3670,2016-11-10 19:42:03,"Also, @ssbarnea, http://stackoverflow.com/questions/32986626/python-requests-importerror-cannot-import-name-headerparsingerror seems to be a different (although similar) issue. From the StackOverflow issue it looks like you have some left over bits of old versions of requests on your disk.
",sigmavirus24,ssbarnea
3668,2016-11-09 21:22:03,"@miscellainiac we do not (nor does anything between us and the socket) munge that header. My guess is that there's something infront of the server (e.g., a proxy or load balancer) that does this to your request header. Either way, the correct format is like your first example. See
- https://tools.ietf.org/html/rfc7231#section-5.2
- https://tools.ietf.org/html/rfc7232#section-3.3
- https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/If-Modified-Since

The first two are parts of the standard that define the behaviour and format of that header's value. The latter is a simplistic way of formatting it for ease.
",sigmavirus24,miscellainiac
3666,2016-11-03 10:39:45,"Thanks @hootnot! :sparkles: :cake: :sparkles:
",Lukasa,hootnot
3664,2016-11-03 10:03:59,"@hootnot I think in this case this should be considered a documentation issue, rather than explicitly naming the parameters. Mind opening a docs PR?
",Lukasa,hootnot
3663,2016-11-03 10:06:51,"Thanks @nateprewitt! :sparkles: :cake: :sparkles:
",Lukasa,nateprewitt
3662,2016-11-02 15:54:19,"@Lukasa Gladly, but I'm a bit overburdened at the moment. I'll have spare time in 2-3 weeks, if it's still open, I'll take a peek.
",sztomi,Lukasa
3662,2016-11-12 08:42:57,"@klimenko It does look better that way, but it's unfortunately just moving the problem. Now anyone whose server is expecting a non-UTF-8 encoded username is going to get tripped up, and so we'll have to re-open this issue when someone says ""my server wanted Latin1 and now doesn't get it"".

It's better to use bytestrings because that way we avoid making a guess that is wrong. If the users still want the helpful automatic choice, they can pass a unicode string, but if they want to do something more specific we have an escape hatch for them.
",Lukasa,klimenko
3662,2016-11-16 16:06:58,"@rmhasan thanks for the interest in contributing! It may be important to note that PR #3673 is already open to address this. You may want to keep an eye on the outcome of that before spending time working on a solution.
",nateprewitt,rmhasan
3662,2016-11-17 03:05:09,"@nateprewitt I will keep an eye on it, thanks.
",rmhasan,nateprewitt
3661,2016-10-31 16:32:46,"@Lukasa Thanks for the quick reply. I guess using `max_retries` would make the code a bit more fault-tolerant in generic sense.

However, to fail-fast at least when developing, running tests or continuous integration, I would rather not retry but simply avoid using those connections that are in risk of being closed (I know proxy server's keep-alive timeout so I could just set my value to a second or two lower than that). Do you see this as totally stupid thing to do?
",tuukkamustonen,Lukasa
3660,2016-10-31 13:59:12,"Cool, so this looks really good. You'll note we have some tests with proxies in `test_lowlevel.py`, so if you feel like adding more testing in this area I'll happily merge it. However, for this specific issue, I think this test is sufficient.

Thanks @pawelmhm! :sparkles: :cake: :sparkles:
",Lukasa,pawelmhm
3659,2016-10-31 12:49:50,"This issue looks easily fixed too: the `proxy_headers` method on the HTTPAdapter requires truthy values for both `username` and `password`. I recommend that that be changed to only require a truthy value for `username`, along with a test. Would you like to make that change @pawelmhm?
",Lukasa,pawelmhm
3655,2016-11-02 08:34:01,"Want to do your final tidying up now, @nateprewitt? I don't think I'm going to have any further feedback at this time.
",Lukasa,nateprewitt
3655,2016-11-03 10:25:22,"Thanks for this @nateprewitt!
",Lukasa,nateprewitt
3653,2016-10-28 08:18:11,"Thanks @huntc. In the meantime, the best I can give you is the knowledge that this isn't immediately obviously a requests problem. =( Sorry!
",Lukasa,huntc
3653,2016-10-28 08:38:50,"@huntc That theory has at least one counter-example:



If you run that script and then do this:



then I encounter no error.
",Lukasa,huntc
3651,2016-10-27 03:04:24,"@drpoggi is likely correct. Further, without extra information, it's hard for any of the people you tagged in this bug to help you (let alone the maintainers of this project).

If you have a _question_ please ask it on [StackOverflow](https://stackoverflow.com).
",sigmavirus24,drpoggi
3651,2016-10-29 12:24:06,"here is a example,`import requests
from requests.auth import *
req = requests.get('http://61.91.211.158:81/onvif/snapshot/101',timeout=3)
print req.headers`,but it raise timeout exception,I have used wireshark to capture the packets,It response quikly.`HTTP/1.1 401 Unauthorized
Date: Sat, 29 Oct 2016 19:06:59 GMT
Server: DNVRS-Webs
Cache-Control: no-cache
Content-Type: text/html
Connection: keep-alive
Keep-Alive: timeout=60, max=99
WWW-Authenticate: Basic realm=""DVRNVRDVS""
WWW-Authenticate: Digest realm=""DVRNVRDVS"", domain=""::"", qop=""auth"", nonce=""71225169d5e6c3e80672d195911b6d23"", opaque="""", algorithm=""MD5"", stale=""FALSE""`,why requests will raise timeout exception? ,and when I remove timeout param It will get 401 status code,but will wait a long long time.@Lukasa @sigmavirus24 @drpoggi 
",beruhan,Lukasa
3651,2016-10-29 12:24:06,"here is a example,`import requests
from requests.auth import *
req = requests.get('http://61.91.211.158:81/onvif/snapshot/101',timeout=3)
print req.headers`,but it raise timeout exception,I have used wireshark to capture the packets,It response quikly.`HTTP/1.1 401 Unauthorized
Date: Sat, 29 Oct 2016 19:06:59 GMT
Server: DNVRS-Webs
Cache-Control: no-cache
Content-Type: text/html
Connection: keep-alive
Keep-Alive: timeout=60, max=99
WWW-Authenticate: Basic realm=""DVRNVRDVS""
WWW-Authenticate: Digest realm=""DVRNVRDVS"", domain=""::"", qop=""auth"", nonce=""71225169d5e6c3e80672d195911b6d23"", opaque="""", algorithm=""MD5"", stale=""FALSE""`,why requests will raise timeout exception? ,and when I remove timeout param It will get 401 status code,but will wait a long long time.@Lukasa @sigmavirus24 @drpoggi 
",beruhan,sigmavirus24
3651,2016-10-29 12:24:06,"here is a example,`import requests
from requests.auth import *
req = requests.get('http://61.91.211.158:81/onvif/snapshot/101',timeout=3)
print req.headers`,but it raise timeout exception,I have used wireshark to capture the packets,It response quikly.`HTTP/1.1 401 Unauthorized
Date: Sat, 29 Oct 2016 19:06:59 GMT
Server: DNVRS-Webs
Cache-Control: no-cache
Content-Type: text/html
Connection: keep-alive
Keep-Alive: timeout=60, max=99
WWW-Authenticate: Basic realm=""DVRNVRDVS""
WWW-Authenticate: Digest realm=""DVRNVRDVS"", domain=""::"", qop=""auth"", nonce=""71225169d5e6c3e80672d195911b6d23"", opaque="""", algorithm=""MD5"", stale=""FALSE""`,why requests will raise timeout exception? ,and when I remove timeout param It will get 401 status code,but will wait a long long time.@Lukasa @sigmavirus24 @drpoggi 
",beruhan,drpoggi
3649,2016-10-27 13:06:36,"@gardiac2002 Unfortunately, GitHub doesn't allow assigning issues to anyone other than contributors or the person who opened the issue. However, no-one on the team is currently working on this, so you are welcome to work on it and open a pull request.
",Lukasa,gardiac2002
3649,2016-10-27 13:49:31,"@Lukasa thank you, I am going to take a look at the issue :)
",gardiac2002,Lukasa
3649,2016-11-03 15:57:50,"It looks like this is now resolved thanks to @gardiac2002's work in #3652.
",nateprewitt,gardiac2002
3636,2016-10-25 17:05:55,"Thanks for this @jortel!

I think that @jeremycline is planning a related PR that focuses on changes in urllib3 first. @jeremycline, how does this relate to your plan and what are your thoughts about this approach versus yours?
",Lukasa,jortel
3636,2016-10-25 17:23:17,"Hey @jortel, I filed #3633 and attached a few proposed patches. Let me know what you think. I need to take the time to write a few tests before I submit a PR, but I will probably have time for that tomorrow night. 

I like making a change to urllib3 first because that way the issue of thread safety is all bundled into urllib3 rather than the responsibility of each user of urllib3. 
",jeremycline,jortel
3636,2016-10-25 18:43:43,"@jeremycline I like what you've proposed in #3633 and will close this PR in favor of that effort pending the outcome.
",jortel,jeremycline
3635,2016-10-26 06:49:53,"@Lukasa  Actually , I can't change the proxy authorization, It's not my proxy server. For now, I use the urllib.request  instead. Is it possible this authorization work with requests? 
",Arion-Dsh,Lukasa
3634,2016-10-25 08:56:50,"Hurrah! No need to worry @jeremycline, this is what code review is for! I'm delighted to say that I'm now happy with this patch, and ready to merge. Thanks so much! :sparkles: :cake: :sparkles:
",Lukasa,jeremycline
3627,2016-10-27 19:53:16,"Thanks @nateprewitt! :sparkles:
",Lukasa,nateprewitt
3626,2016-10-18 13:49:09,"@yozel Thanks for this issue!

Per #2807, this is intentional behaviour. `merge_environment_settings` is a public, documented method on the `Session`: if you're planning to move away from using `Session.request` to the prepared request flow, this is one of the things that needs to become part of your workflow if you want its behaviour.
",Lukasa,yozel
3625,2016-10-17 16:13:30,"@mie00 The question isn't ""can `seek()` throw an `IOError`"", it's ""can `seek()` throw an `IOError` in a situation where `tell()` does not"". I feel like the odds of that are pretty low. I'm inclined to want to worry about that when we know it's an issue.
",Lukasa,mie00
3625,2016-10-21 07:20:34,"Ok, rather than block on @sigmavirus24 having time to swing back to this, I'll merge this now. @sigmavirus24 can provide feedback whenever he gets time back. Thanks for the work @mie00!
",Lukasa,mie00
3624,2016-10-17 05:43:28,"Hey @alireza-amirsamimi, thanks for opening this ticket. Requests currently doesn't support passing a `dict` object to Session's `cookies` attribute. You must first convert your dictionary into a `CookieJar`-like object before passing it to the session. I've attached the code below that you'll need to solve your issue. In the future, it's preferred that questions regarding how to use Requests are opened on Stackoverflow instead of the Github ticket tracker. Thanks again!

e.g.



Just to make note of it, this is an occurrence of #3595.
",nateprewitt,alireza-amirsamimi
3620,2016-10-14 10:16:23,"@haypo Without wanting to get too far into the weeds here, it remains the policy of the Requests project that we vendor all dependencies. If you'd like to discuss the amendment of that policy I recommend you email Kenneth, as all such decisions are his to make.

You'll also note that our requirements.txt are _development_ requirements, not our actual requirements for functioning.
",Lukasa,haypo
3620,2016-10-14 10:27:01,"@haypo No problem. =) This is an unusual policy, and it'll be interesting to see how much longer we keep it, but on a personal level I'd be totally happy to unvendor our deps.
",Lukasa,haypo
3619,2016-10-24 07:23:04,"@AndiCui So, to be clear, using Requests gives you the problem but using the code above does not.
",Lukasa,AndiCui
3619,2016-10-24 17:59:29,"To be clear @AndiCui your problem is arising from the standard library, not from within Requests itself. If you're seeing this problem using Requests, we can handle it, but we the ValueError being raised needs to be addressed in the standard library and you should file a bug on bugs.python.org for that.
",sigmavirus24,AndiCui
3618,2016-10-14 16:37:15,"@chadmiller other libraries will do that for you (e.g., `rfc3986`) because they are not in the HTTP specification, they're in other (URI) specifications.
",sigmavirus24,chadmiller
3617,2016-10-13 14:08:22,"Cheers @jeremycline!
",sigmavirus24,jeremycline
3616,2016-10-12 20:19:01,"+1 for me. I agree with @sigmavirus24 on making it non-optional.
",eriol,sigmavirus24
3615,2016-10-12 10:11:41,"Thanks @TetraEtc! :sparkles: :cake: :sparkles:
",Lukasa,TetraEtc
3614,2016-10-07 15:07:05,"Hi @redixin ,

After a quick search I found https://github.com/kennethreitz/requests/issues/713 which is an old issue that provides the reasoning for why Requests does not support this flow. In fact, this is because we are built on top of httplib. If you want to support 100-continue properly in Requests, you need to go convince the core python developers to support this first.

In the future, please search for similar issues.
",sigmavirus24,redixin
3612,2016-10-07 20:15:09,"@drpoggi is correct. There's not much to be done here within requests (or six for that matter). It's usually best to not to shadow the names of standard library modules.
",sigmavirus24,drpoggi
3611,2016-10-06 13:16:52,"@TetraEtc is correct. Cheers!
",sigmavirus24,TetraEtc
3606,2016-09-29 07:09:18,"Looks good to me @bbamsch, thanks for fixing this up!
",Lukasa,bbamsch
3605,2016-10-26 18:30:12,"@IntelBob The security packages don't install a newer OpenSSL on Linux and, even if they did, they don't monkeypatch the standard library. Are you having a specific problem on Ubuntu?
",Lukasa,IntelBob
3605,2016-10-26 19:47:57,"@Lukasa I am getting intermittent `EOF occurred in violation of protocol (_ssl.c:590)` error on Ubuntu, should I just upgrade Ubuntu openssl package? Looks like the latest openssl version on Ubuntu is 1.0.1g, would installing that version help with my issue? I am not explicitly using ssl in my code, just using requests to access https URLs. 
",IntelBob,Lukasa
3605,2017-01-12 07:16:10,"Hi @Lukasa 

I the same error , and it stuck me for couple of days :+1: 
`urllib2.URLError: <urlopen error EOF occurred in violation of protocol (_ssl.c:590)>`
my python version is 2.7.10,  and I use urllib2 
below is my related code:
`cookieprocessor = urllib2.HTTPCookieProcessor()
            opener = urllib2.build_opener(cookieprocessor)
            urllib2.install_opener(opener)
            request = urllib2.Request(url,postParams)
            if sys.version_info < (2, 7, 9):
                file = urllib2.urlopen(request)
            else:
                ctx = ssl.create_default_context()
                ctx.check_hostname = False
                ctx.verify_mode = ssl.CERT_NONE
                print (""---"")
                file = urllib2.urlopen(request, context=ctx, timeout=30)
            fileInfo = file.read()`

Please help me...",szyl111,Lukasa
3605,2017-01-12 08:14:15,@szyl111 Your code is using urllib2. This issue tracker is for support with the Python Requests library only. You'll need to go discuss your problem upstream with the Python Development team.,Lukasa,szyl111
3604,2016-09-28 11:32:51,"Thanks @iamprakashom! :sparkles: :cake: :sparkles:
",Lukasa,iamprakashom
3604,2016-09-28 11:38:27,"Thank you  very much @Lukasa  for merging my PR. :smile: 
",iamprakashom,Lukasa
3603,2016-09-28 07:16:48,"Yup, that looks wrong. I recommend the first link you posted @bbamsch. Would either of you be interested in submitting a pull request to fix that?
",Lukasa,bbamsch
3603,2016-09-28 07:41:05,"@Lukasa, If I understand right, first link you are referring to is http://urllib3.readthedocs.io/en/1.5/pools.html ?
",iamprakashom,Lukasa
3602,2016-09-27 14:00:07,"@Lukasa Thanks for your responses so far. Curl did not use my macos keychain, when I didn't pass the certificate as an argument, curl would not download anything either.
I followed the crt link in the output above, downloaded that certificate, checked the output too and so on to get something that might just be the root certificate. It still does not work but next I first try to find a human who might have the proper root certificate.
",do3cc,Lukasa
3602,2016-09-27 14:12:28,"@do3cc Can you print `curl -V` for me please?
",Lukasa,do3cc
3602,2016-10-05 09:02:17,"Hi, I got an answer and the certificate files. 
It was still failing while adding all certs to my browser made my browser accept the website. Googling for how to validate the cert via ssl gave me this page http://stackoverflow.com/questions/25482199/verify-a-certificate-chain-using-openssl-verify and after that I realized that it is not enough to pass the root certificate to `verify` but also the intermediate certificates. 

Our org uses its own root certificate and intermediate certificates. To verify the https connection, requests needs the complete certificate chain. 
Creating a cert file where I pasted the root certificate and intermediate certificates did just that.

@Lukasa Thank you so much for your quick help!
",do3cc,Lukasa
3600,2016-09-27 07:06:25,"Thanks @frewsxcv!
",Lukasa,frewsxcv
3599,2016-09-27 16:21:07,"@alannorton Here are all the import statements in the requests codebase. I should note that, while I'm happy to help here, this is information that would have been just as easy for you to obtain as it was for me.

Firstly, the stdlib imports:



Then the optional 3rd party imports:


",Lukasa,alannorton
3599,2016-09-27 17:22:22,"I have good news!  The Windows C runtime error goes away when we use Python
2.7.11 instead of 2.7.8.   So no fix is needed.

On Tue, Sep 27, 2016 at 10:21 AM, Cory Benfield notifications@github.com
wrote:

> @alannorton https://github.com/alannorton Here are all the import
> statements in the requests codebase. I should note that, while I'm happy to
> help here, this is information that would have been just as easy for you to
> obtain as it was for me.
> 
> Firstly, the stdlib imports:
> 
> argparse
> base64
> binascii
> calendar
> cgi
> codecs
> collections
> contextlib
> copy
> datetime
> email.utils
> encodings.idna
> errno
> functools
> hashlib
> hmac
> io
> itertools
> logging
> mimetypes
> operator
> os
> os.path
> re
> select
> six
> socket
> ssl
> struct
> sys
> threading
> time
> types
> uuid
> warnings
> zlib
> 
> Then the optional 3rd party imports:
> 
> OpenSSL.SSL
> ntlm
> pyasn1.codec.der
> pyasn1.type
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/kennethreitz/requests/issues/3599#issuecomment-249917076,
> or mute the thread
> https://github.com/notifications/unsubscribe-auth/AG55D_b-uoRWNGNwbI8aGifDJ2FxrAsRks5quUKVgaJpZM4KG1ti
> .
",alannorton,alannorton
3597,2016-09-26 05:32:35,"@Lukasa 
I've attempted a patch of this issue in PR #3598 
Would appreciate your input on the patch.

:smile:
",bbamsch,Lukasa
3595,2016-10-17 07:22:11,"@nateprewitt I think @sigmavirus24 was tentatively in favour of this change, at least as a temporary stopping-off point before something better. He was just asking for a change in the descriptor protocol.
",Lukasa,nateprewitt
3595,2016-10-18 16:31:02,"@sigmavirus24 I agree with you about there being a risk of unintentionally sending cookies over the wire with session cookies, but I think that isn't entirely coupled to this issue. Instead of allowing people to pass a dictionary, we currently have them dump a dictionary into a RequestsCookieJar which has no domain info either. So this is an extra step for the sake of purity, that currently provides the user no extra safety or benefit.

I can see how this modification may reenforce bad behaviour that should probably be removed in 3.0.0, so in the meantime, I'll propose this. I think adding a section in the documentation detailing _how_ to do this correctly would go a long way. We should also at the very least add a warning when setting Session's cookies attribute with a dict, directing them to the documentation.

Something like this could go in the docs with an explanation of why using the domain property is important. An iterable-based version of `set` or `set_cookie` would probably be a useful helper for 3.0.0 too.


",nateprewitt,sigmavirus24
3595,2016-10-24 08:15:36,"Like @nateprewitt said, there is benefits here we can not ignore.
When the user want to use `add_dict_to_cookiejar` this will add the dict to `''`domain. This is most likely not the desired scenario. 

For example:


",OrDuan,nateprewitt
3595,2016-11-09 21:39:13,"Alright @Lukasa, @sigmavirus24,

I took a swing at my [last comment](https://github.com/kennethreitz/requests/pull/3595#issuecomment-254563756). It comes in two pieces. 

3e4e5b7: This adds documentation for adding dictionaries with domains. It's currently based off of the solution below, but if it's decided that isn't a good fit, we can change the code to my comment above. This at least provides direction for the user to do the right thing, rather than identical behaviour to a dictionary, with more code.

e2a4f9f & 4454849: I added in a `**kwargs` option to the `add_dict_to_cookiejar` and `cookiejar_from_dict` to allow cookie parameters. This will allow you to pass dictionary-wide cookie params to use the current proposed solution for session cookies ""safely"". This behaviour resembles what the browser does with set-cookie on a per request basis, allows for _fairly_ easy extensibility and it's a minimal code change. It definitely needs some updated doc strings but I wanted to get feed back before proceeding.

Lastly, I think that a warning, pointing users to the documentation, when setting the session cookie to a dict is low overhead and will prevent issues like #3624.

This is becoming a much longer running discussion than I anticipated after discussing opening this patch with Lukasa. It may be worth closing this PR and moving it into an issue, but I'll defer to your judgements on how you want this cataloged.
",nateprewitt,Lukasa
3595,2016-11-09 21:39:13,"Alright @Lukasa, @sigmavirus24,

I took a swing at my [last comment](https://github.com/kennethreitz/requests/pull/3595#issuecomment-254563756). It comes in two pieces. 

3e4e5b7: This adds documentation for adding dictionaries with domains. It's currently based off of the solution below, but if it's decided that isn't a good fit, we can change the code to my comment above. This at least provides direction for the user to do the right thing, rather than identical behaviour to a dictionary, with more code.

e2a4f9f & 4454849: I added in a `**kwargs` option to the `add_dict_to_cookiejar` and `cookiejar_from_dict` to allow cookie parameters. This will allow you to pass dictionary-wide cookie params to use the current proposed solution for session cookies ""safely"". This behaviour resembles what the browser does with set-cookie on a per request basis, allows for _fairly_ easy extensibility and it's a minimal code change. It definitely needs some updated doc strings but I wanted to get feed back before proceeding.

Lastly, I think that a warning, pointing users to the documentation, when setting the session cookie to a dict is low overhead and will prevent issues like #3624.

This is becoming a much longer running discussion than I anticipated after discussing opening this patch with Lukasa. It may be worth closing this PR and moving it into an issue, but I'll defer to your judgements on how you want this cataloged.
",nateprewitt,sigmavirus24
3595,2016-11-10 10:20:53,"I can get behind moving this to an issue for discussion. It's clear @sigmavirus24 has a bigger goal for this API than I originally did, and so I'd like us to move to a forum where he can participate effectively despite his current high workload.
",Lukasa,sigmavirus24
3593,2016-09-20 18:58:01,"@Sh-Ho So if your system hasn't changed there are only two options. Either the proxy has changed or the server has. In either case, the failure mode is simply that the connection is getting closed in the face of the Client Hello TLS message. It's extremely hard to work out what that is without quite a lot of knowledge and some guess work. Ideally we'd have a packet capture from the previous handshake and one from the new handshake.

However, given the way these kinds of things usually go, I'd start by investigating the proxy. Ideally you should consult with the operator of the proxy to see why it's shutting down your handshake.
",Lukasa,Sh-Ho
3592,2016-09-20 14:57:34,"@sigmavirus24 

I am not sure why this was **closed**.

I cannot provide the code because it contains proprietary credentials.

So, what would cause requests would make repeated ""Starting new HTTPS connection"" followed by repeated method calls?
",jefftune,sigmavirus24
3592,2016-09-20 15:00:32,"@jefftune Multithreaded use of Requests is the most likely case. In the absence of any kind of code, we cannot effectively debug your situation: you are asking us to look at two logs and to discern everything you are doing with the code. We know nothing about your environment, the manner in which you're using the code, whether you're running with multiple sessions, or anything else.

If you could _strip_ the credentials from your code (e.g. replace them with `****`), then we can help. But otherwise, what you're asking us to do is to blindfold ourselves, tie both our hands behind our back, sit on a chair in a darkened room, and then work out what item you're holding in your hand based on the sound it makes when you drop it. We can _try_, but our odds are bad.
",Lukasa,jefftune
3592,2016-09-20 15:01:47,"@Lukasa 

Thanks for the feedback, I will look into **Multithreaded use of Requests**
",jefftune,Lukasa
3592,2016-09-20 15:32:58,"@jefftune this was **closed** because:
- it's not a defect
- this project answers questions on StackOverflow (although maybe not as quickly as you would like)
- we cannot help you without providing some sample

therefore there's nothing actionable to come out of this issue.
",sigmavirus24,jefftune
3592,2016-09-20 15:34:53,"@sigmavirus24 

Understood, thank you
",jefftune,sigmavirus24
3591,2016-09-23 09:32:49,"Ok, per my note above I'm going ahead and merging this. Thanks @nateprewitt!
",Lukasa,nateprewitt
3579,2016-09-17 04:06:39,"Thanks for opening this issue, @agacek. So the problem we're encountering here is a difference in how we ""prepare"" the CookieJar between a `PreparedRequest` and a `Session`.

`Session` will always [merge the cookies into a fresh `RequestsCookieJar`](https://github.com/kennethreitz/requests/blob/master/requests/sessions.py#L375-L376) when `prepare_request()` is called. `RequestsCookieJar` will have the update method, and is a subclass of `cookielib.CookieJar` so everything works fine.

`PreparedRequest` however doesn't perform this same merge, and won't modify the CookieJar if it's an instance of cookielib.CookieJar, so this one remains untouched. I'm not sure if this was an intentional feature of `PreparedRequest` to allow people to do what they wanted, or if it was an oversight. The code base is obviously expecting `cookies` to be `RequestsCookieJar`-like now though. If we're enforcing this in Session, we should probably also implement the same logic in `prepare_cookies` for the `PreparedRequest`. The user can always override the `cookies` param with their own CookieJar after if needed.

@agacek as an interim fix, I would suggest you either use the [`RequestsCookieJar`](https://github.com/kennethreitz/requests/blob/master/requests/cookies.py#L172) class instead of `http.cookiejar.CookieJar`, or use the [`merge_cookies`](https://github.com/kennethreitz/requests/blob/master/requests/cookies.py#L521) function to merge your existing `CookieJar` into a new `RequestsCookieJar`. You can, of course, also continue using `Session.prepare_request()` too. Hope that helps!
",nateprewitt,agacek
3578,2016-09-17 05:48:55,"@rbcarson Can you confirm for me: does adding `import encodings.idna` to your `task` function in your example also fix the problem?
",Lukasa,rbcarson
3578,2016-09-17 06:51:09,"@Lukasa Yes, it does.
",rbcarson,Lukasa
3577,2016-09-20 10:39:48,"Thanks @Lukasa 
However, setting `chunk_size` to `1` or `None` did not change the results in my case. It seems that my issue is related to https://github.com/kennethreitz/requests/issues/2020 . Requests somehow handles chucked-encoding differently as curl does.
The following example shows different results GET from my log-server using curl and requests.





We can see that `iter_content` get the correct data as well as CRLF but chunks them in a different way. Since `iter_lines` internally called `iter_content`, the line split differently accordingly. 
",haocs,Lukasa
3577,2016-09-20 11:33:27,"@haocs Only when using `chunk_size=None`.
",Lukasa,haocs
3576,2016-09-19 18:18:51,"@Lukasa I would like to pull it and run the tests locally. I suspect we might have a locale mismatch which is why this _appears_ to work for @nateprewitt and not for me. This is why I was using bytes explicitly previously.
",sigmavirus24,Lukasa
3576,2016-09-19 18:18:51,"@Lukasa I would like to pull it and run the tests locally. I suspect we might have a locale mismatch which is why this _appears_ to work for @nateprewitt and not for me. This is why I was using bytes explicitly previously.
",sigmavirus24,nateprewitt
3576,2016-09-19 18:27:11,"Hey @sigmavirus24, while this could be a locale mismatch, there are 5 other tests in the same file using unicode characters in unicode strings like this (including \xf6). I _think_ I figured out why we were getting different results, and [detailed it](https://github.com/kennethreitz/requests/pull/3557#issuecomment-246908451), along with most of my other findings in #3557. I had @Lukasa confirm he could reproduce this issue before I opened the PR. Is this patch not working on your system?
",nateprewitt,Lukasa
3576,2016-09-19 18:27:11,"Hey @sigmavirus24, while this could be a locale mismatch, there are 5 other tests in the same file using unicode characters in unicode strings like this (including \xf6). I _think_ I figured out why we were getting different results, and [detailed it](https://github.com/kennethreitz/requests/pull/3557#issuecomment-246908451), along with most of my other findings in #3557. I had @Lukasa confirm he could reproduce this issue before I opened the PR. Is this patch not working on your system?
",nateprewitt,sigmavirus24
3576,2016-09-19 19:00:29,"> ...which is why **this** _appears_ to work for @nateprewitt and not for me.

Sorry @sigmavirus24, I interpreted ""this"" as meaning ""this patch"". I was just trying to clarify.
",nateprewitt,sigmavirus24
3576,2016-09-19 19:00:29,"> ...which is why **this** _appears_ to work for @nateprewitt and not for me.

Sorry @sigmavirus24, I interpreted ""this"" as meaning ""this patch"". I was just trying to clarify.
",nateprewitt,nateprewitt
3576,2016-09-20 14:48:49,"Thanks @nateprewitt. Care to update proposed/3.0.0 too then?
",sigmavirus24,nateprewitt
3573,2016-09-15 07:39:49,"@Lukasa what is the change in urllib3 that will solve this issue?
",stefanfoulis,Lukasa
3572,2016-09-15 06:34:09,"Yup, as @drpoggi said, Requests doesn't carry any patches to urllib3, we just use it as is. Please make this change to urllib3 and open a PR there. =)
",Lukasa,drpoggi
3571,2016-09-14 15:23:26,"@antlong Can you provide your code, please? Can you also tell us what version of Requests you're using, and where you got it from?
",Lukasa,antlong
3570,2016-09-14 07:07:42,"@PegasusWang This suggests that your PySocks isn't working well. `pip install -U pysocks==1.5.6` should resolve your problem.
",Lukasa,PegasusWang
3565,2016-09-09 09:47:56,"@Lukasa - it also works if library authors _don't opt-out_ by using the Session API. Ie. if they just write `request.get` (which I suspect most of them do) instead of setting up a session and calling methods on it, this ""default session factory"" would be used.

Also, perhaps some guidance in form of documentation - ie. ""How to use requests if you're a library author"" - could be useful. It could cover things like: allow the users of your library to customize how requests are made, to set up proxies, etc.
",cdman,Lukasa
3565,2016-09-09 12:38:14,"I have to agree with @Lukasa here.

> it also works if library authors don't opt-out by using the Session API. Ie. if they just write request.get (which I suspect most of them do) instead of setting up a session and calling methods on it, this ""default session factory"" would be used.

If someone is building a library that is interacting with a service using requests and not using a Session they're performing a **severe** disservice to their users. Their users are paying the performance price for the library author not using a Session. Further, if the library is not providing a way for their users to customize the session (or provide their own ready-made one) then they're also hurting their users. Poor usage of requests is not requests responsibility to fix.

> Also, perhaps some guidance in form of documentation - ie. ""How to use requests if you're a library author"" - could be useful. It could cover things like: allow the users of your library to customize how requests are made, to set up proxies, etc.

People don't often like to be told how to do things, and certain authors will violently disagree with what I've said above. We already have people who send pull requests to change the code-style of requests from Kenneth's style to their own personal style. The last thing we need is another avenue for people to send pull requests that are based on their own opinions of how to do a thing.

Blog posts are the better place for this kind of discussion, not Requests' documentation.
",sigmavirus24,Lukasa
3563,2016-11-15 16:47:47,"@Lukasa we'll need to merge master into Proposed/3.0.0 to run the tests normally, but with 1.19 in Requests 2.12 this should be ready for a look.
",nateprewitt,Lukasa
3563,2016-11-15 17:28:16,"Thanks @Lukasa! I'll merge it in now.
",nateprewitt,Lukasa
3563,2016-11-16 13:29:47,"Ok, I think this looks good! Thanks so much @nateprewitt! :sparkles:
",Lukasa,nateprewitt
3561,2016-09-08 09:16:47,"Alright, good start @JohnVillalovos! I've left some feedback in the diff that would be good to have addressed, thanks. =D
",Lukasa,JohnVillalovos
3561,2017-02-10 17:31:31,@JohnVillalovos that's true but we cannot reopen this since your repository seems to have been deleted.,sigmavirus24,JohnVillalovos
3561,2017-02-10 17:45:59,Thanks @JohnVillalovos ,sigmavirus24,JohnVillalovos
3560,2016-12-26 21:54:07,"@Lukasa could you give any pointers to debug this? I'm getting the same error when I try to use pip. I'm trying to understand what's going on, but I don't know where to start",elopio,Lukasa
3560,2016-12-26 22:42:24,@elopio The first question is to work out what OS you're on and where your packages have come from. How did you get pip? How did you get Requests? Where are they coming from in your OS?,Lukasa,elopio
3559,2016-09-07 20:13:20,"Hey @napalmer7, this is the intention of this check. All header values must be passed as strings in Requests. There are unfortunately more complicated datatypes than `int`, such as `datetime`, which won't format correctly in a `str()` call. You can find relevant discussions and reasoning in #3366 and #3477.
",nateprewitt,napalmer7
3559,2016-09-07 20:15:38,"@napalmer7 **Please** search closed issues for relevant discussions before opening new ones.
",sigmavirus24,napalmer7
3558,2016-09-07 13:26:34,"@coderfreedom You almost certainly have the `HTTP_PROXY` environment variable set. Why is that set if you don't want HTTP tools to use your VPS?
",Lukasa,coderfreedom
3558,2016-09-07 13:59:07,"@Lukasa   But I close it in IE settings. 
sometimes I need VPN to climbe the Great Fire Wall    : ) you know, a lot of  websites I can't visit. Sometimes visit github I also need use VPN.
",coderfreedom,Lukasa
3558,2016-09-07 14:09:07,"@Lukasa 



It was my vps's IP, why????
",coderfreedom,Lukasa
3558,2016-09-07 14:19:15,"@Lukasa I use this way before. But sometimes  I need use othre lib which can't set proxies, such as `builtwith.parse('http://example.webscraping.com')`  it also through `Error: <urlopen error [Errno 10061] >`
",coderfreedom,Lukasa
3558,2016-09-07 14:23:06,"@coderfreedom In that case, this is a configuration issue on your machine. I'm afraid I can't really help beyond that. =(
",Lukasa,coderfreedom
3558,2016-09-07 14:24:38,"@coderfreedom you can use `trust_env=False` to avoid using any proxies discovered in your environment as a work around until you figure out your system configuration problem.
",sigmavirus24,coderfreedom
3558,2016-09-07 14:31:30,"@Lukasa  Anyway, thank you for your help  : )
This problem has troubled me for months, It drives me crazy ""_""
I think it's time to use Mac OS.  : )
",coderfreedom,Lukasa
3558,2016-09-07 14:38:47,"@sigmavirus24  But another lib will still not work. 
",coderfreedom,sigmavirus24
3558,2016-09-07 14:40:12,"@Lukasa  @sigmavirus24  Does python has a global variable to carvery it???
",coderfreedom,Lukasa
3558,2016-09-07 14:40:12,"@Lukasa  @sigmavirus24  Does python has a global variable to carvery it???
",coderfreedom,sigmavirus24
3558,2016-09-30 03:10:01,"@coderfreedom : Did you solve this problem? I also met VPN related issue.
",edeuser,coderfreedom
3557,2016-09-06 16:07:47,"Sorry I let Python 2 slip through the cracks. I am, however, still seeing a failure in python 2.7.10 on multiple systems with this merge. The issue appears to be that we're handing a unicode string in Python 2 to the HTTPError which is producing an ""unprintable HTTPError"" message when it can't be decoded to ascii. This was also present in the original test in PR #3385, we just weren't testing the string. Encoding the actual [error message](https://github.com/kennethreitz/requests/blob/master/requests/models.py#L862-L869) string before passing it to the exception fixes this problem in Python 2.

e.g.



**Edit:** @sigmavirus24 can you confirm this is failing for you in Python 2 still and that my tests aren't borked?

**Second Edit:** I can't find anything functionally different in this PR from pre-#3557 except that we're now comparing a string object with a unicode object in the final assert statement. I'm not particularly tied to the encoding ordering, but I do think that this test should match the other unicode tests above it. Is there a reason that the encoding needs to be done during the assertion (bytes->str) instead of at the beginning (unicode->bytes)? These will always be unicode strings, so I'm not sure if we gain anything from dropping the reason phrase down to latin-1 during the final assertion (other than it needs to not be bytes). Apologies if I'm missing something obvious.

This would be [my proposed solution](https://github.com/nateprewitt/requests/commit/0cf4e700d2ce30c9422d788d705c5496d5401d3d) for fixing the failing tests. Like I said, I'm not tied to reverting but I think related tests are easier to understand if they share common pieces.
",nateprewitt,sigmavirus24
3557,2016-09-06 20:26:51,"@nateprewitt this wasn't working on Python 2 _or_ Python 3 before but this passes for me on Linux and OSX. What are you testing on?
",sigmavirus24,nateprewitt
3557,2016-09-07 15:43:30,"@sigmavirus24, here's some tox runs on blank travis instances displaying what I'm seeing locally.

Pre-#3557: https://travis-ci.org/nateprewitt/requests/builds/158050706 (87f9693)
Post-#3557: https://travis-ci.org/nateprewitt/requests/builds/158050156 (2041adb)
My Proposed Patch: https://travis-ci.org/nateprewitt/requests/builds/158051134 (0cf4e70)
",nateprewitt,sigmavirus24
3557,2016-09-14 05:13:31,"Alright @sigmavirus24, I think I've finally gotten to the bottom of the confusion here (or most of it). It looks like you wrote and commited [a separate test](https://github.com/kennethreitz/requests/blob/proposed/3.0.0/tests/test_requests.py#L1054-L1069) for proposed/3.0.0 than you submitted here, which does in fact pass on all versions. This patch as it stands on master though is functionally identical to the test beforehand and still fails for Python 2. 

We could move the test from proposed/3.0.0 over to master to fix the problem, but I feel like it may be unnecessarily verbose. You perform an identical check by simply switching `str(e)` to `e.value.args[0]`. I've updated my commit (39e8c0d) to use this instead, still with the reversion of the encodings for the reasons listed in the comments above. As I said before, not tied to that, but I'd rather keep the tests similar if possible.

I'm fairly confident this is the correct approach at this point, but please let me know if you see something I'm not. I'll open this as a PR tomorrow afternoon unless you'd like to make the changes yourself.

Here's the [Travis build](https://travis-ci.org/nateprewitt/requests/builds/159784486), if you wanna take a look.
",nateprewitt,sigmavirus24
3556,2016-09-06 14:03:49,"@Lukasa too serious
",kennethreitz,Lukasa
3556,2016-09-06 14:03:57,"@mouuff it's a joke :)
",kennethreitz,mouuff
3556,2016-09-06 16:51:54,"@mouuff haha, behold—art!
",kennethreitz,mouuff
3556,2017-03-07 02:43:42,"I don't know the meaning either, and try to figure it out, but still puzzled.  I'm not a native English. @kerstin ",Celthi,kerstin
3556,2017-03-07 03:19:25,@Celthi it's a joke :),kennethreitz,Celthi
3556,2017-03-07 06:21:05,"@kennethreitz  Oh, I see, :) :) :). Thank you.",Celthi,kennethreitz
3555,2016-09-07 01:45:28,"@Lukasa Thank you for response.

The REST API server needs a header leading whitespace. In this system, we must use the header field `""Authorization"" : "" Basic (SECURITY-TOKEN)""` for the authorization to the system. Spaces also must be needed! :sob: 

I think this system's REST API design is so bad :sob: 
",shouh,Lukasa
3552,2016-09-05 16:47:10,"Thanks @Trodis!
",sigmavirus24,Trodis
3551,2016-09-05 16:22:13,"@Trodis Have you seen the fix in the linked PR?
",Lukasa,Trodis
3550,2016-09-06 00:20:31,"@Lukasa thks for reply,i confirm the domain name is valid,because i can get some datas.but sometimes i get the error ""ConnectionError"".
**some error info:**
`[zjhxmjl@localhost phoneregion]$ python main.py --scrapy > log.txt
Traceback (most recent call last):
  File ""/usr/lib64/python2.7/site-packages/gevent/greenlet.py"", line 534, in run
    result = self._run(_self.args, *_self.kwargs)
  File ""/home/zjhxmjl/Documents/phoneregion/scrapy.py"", line 55, in worker
    info = self.validate(phone)
  File ""/home/zjhxmjl/Documents/phoneregion/scrapy.py"", line 62, in validate
    r = requests.get(url)
  File ""/usr/lib/python2.7/site-packages/requests/api.py"", line 70, in get
    return request('get', url, params=params, *_kwargs)
  File ""/usr/lib/python2.7/site-packages/requests/api.py"", line 56, in request
    return session.request(method=method, url=url, *_kwargs)
  File ""/usr/lib/python2.7/site-packages/requests/sessions.py"", line 475, in request
    resp = self.send(prep, *_send_kwargs)
  File ""/usr/lib/python2.7/site-packages/requests/sessions.py"", line 596, in send
    r = adapter.send(request, *_kwargs)
  File ""/usr/lib/python2.7/site-packages/requests/adapters.py"", line 487, in send
    raise ConnectionError(e, request=request)
ConnectionError: HTTPConnectionPool(host='www.ip138.com', port=8080): Max retries exceeded with url: /search.asp?mobile=1459772&action=mobile (Caused by NewConnectionError('<requests.packages.urllib3.connection.HTTPConnection object at 0x3970d90>: Failed to establish a new connection: [Errno -2] Name or service not known',))
<Greenlet at 0x31ae0f0: <bound method Scrapy.worker of <scrapy.Scrapy instance at 0x307ef38>>(['1450000', '1450001', '1450002', '1450003', '1450)> failed with ConnectionError

Traceback (most recent call last):
  File ""/usr/lib64/python2.7/site-packages/gevent/greenlet.py"", line 534, in run
    result = self._run(_self.args, *_self.kwargs)
  File ""/home/zjhxmjl/Documents/phoneregion/scrapy.py"", line 55, in worker
    info = self.validate(phone)
  File ""/home/zjhxmjl/Documents/phoneregion/scrapy.py"", line 62, in validate
    r = requests.get(url)
  File ""/usr/lib/python2.7/site-packages/requests/api.py"", line 70, in get
    return request('get', url, params=params, *_kwargs)
  File ""/usr/lib/python2.7/site-packages/requests/api.py"", line 56, in request
    return session.request(method=method, url=url, *_kwargs)
  File ""/usr/lib/python2.7/site-packages/requests/sessions.py"", line 475, in request
    resp = self.send(prep, *_send_kwargs)
  File ""/usr/lib/python2.7/site-packages/requests/sessions.py"", line 596, in send
    r = adapter.send(request, *_kwargs)
  File ""/usr/lib/python2.7/site-packages/requests/adapters.py"", line 487, in send
    raise ConnectionError(e, request=request)
ConnectionError: HTTPConnectionPool(host='www.ip138.com', port=8080): Max retries exceeded with url: /search.asp?mobile=1459728&action=mobile (Caused by NewConnectionError('<requests.packages.urllib3.connection.HTTPConnection object at 0x34df390>: Failed to establish a new connection: [Errno -2] Name or service not known',))
<Greenlet at 0x31aef50: <bound method Scrapy.worker of <scrapy.Scrapy instance at 0x307ef38>>(['1450000', '1450001', '1450002', '1450003', '1450)> failed with ConnectionError

Traceback (most recent call last):
  File ""/usr/lib64/python2.7/site-packages/gevent/greenlet.py"", line 534, in run
    result = self._run(_self.args, *_self.kwargs)
  File ""/home/zjhxmjl/Documents/phoneregion/scrapy.py"", line 55, in worker
    info = self.validate(phone)
  File ""/home/zjhxmjl/Documents/phoneregion/scrapy.py"", line 62, in validate
    r = requests.get(url)
  File ""/usr/lib/python2.7/site-packages/requests/api.py"", line 70, in get
    return request('get', url, params=params, *_kwargs)
  File ""/usr/lib/python2.7/site-packages/requests/api.py"", line 56, in request
    return session.request(method=method, url=url, *_kwargs)
  File ""/usr/lib/python2.7/site-packages/requests/sessions.py"", line 475, in request
    resp = self.send(prep, *_send_kwargs)
  File ""/usr/lib/python2.7/site-packages/requests/sessions.py"", line 596, in send
    r = adapter.send(request, *_kwargs)
  File ""/usr/lib/python2.7/site-packages/requests/adapters.py"", line 487, in send
    raise ConnectionError(e, request=request)
ConnectionError: HTTPConnectionPool(host='www.ip138.com', port=8080): Max retries exceeded with url: /search.asp?mobile=1459727&action=mobile (Caused by NewConnectionError('<requests.packages.urllib3.connection.HTTPConnection object at 0x361c210>: Failed to establish a new connection: [Errno -2] Name or service not known',))
<Greenlet at 0x306b4b0: <bound method Scrapy.worker of <scrapy.Scrapy instance at 0x307ef38>>(['1450000', '1450001', '1450002', '1450003', '1450)> failed with ConnectionError`

**this is log.txt**
[log.txt](https://github.com/kennethreitz/requests/files/455727/log.txt)
",zhumingyu,Lukasa
3545,2016-09-21 16:31:14,"Ok, things are pared down. The patch is now pretty simple.

The logic for file-like objects (with seek/tell) is now in [`super_len`](https://github.com/kennethreitz/requests/blob/d7227fbb7e07af35f23a0d370ab3b01661af9e40/requests/utils.py#L78-L103). I'm removing [this chunk](https://github.com/kennethreitz/requests/blob/d7227fbb7e07af35f23a0d370ab3b01661af9e40/requests/models.py#L478-L483) since it is now redundant. I'm also adding two tests to verify that Content-Length is set to 0 appropriately.

@Lukasa, @sigmavirus24 feel free to take a peak when you have a moment.
",nateprewitt,Lukasa
3545,2016-09-21 16:31:14,"Ok, things are pared down. The patch is now pretty simple.

The logic for file-like objects (with seek/tell) is now in [`super_len`](https://github.com/kennethreitz/requests/blob/d7227fbb7e07af35f23a0d370ab3b01661af9e40/requests/utils.py#L78-L103). I'm removing [this chunk](https://github.com/kennethreitz/requests/blob/d7227fbb7e07af35f23a0d370ab3b01661af9e40/requests/models.py#L478-L483) since it is now redundant. I'm also adding two tests to verify that Content-Length is set to 0 appropriately.

@Lukasa, @sigmavirus24 feel free to take a peak when you have a moment.
",nateprewitt,sigmavirus24
3545,2016-09-22 12:04:49,"@Lukasa it looks as though @nateprewitt pushed new changes after your approval. Care to re-review?
",sigmavirus24,Lukasa
3545,2016-09-22 12:04:49,"@Lukasa it looks as though @nateprewitt pushed new changes after your approval. Care to re-review?
",sigmavirus24,nateprewitt
3540,2016-08-26 12:23:48,"Yup, that's correct. =) Thanks @TetraEtc!
",Lukasa,TetraEtc
3539,2016-08-26 13:17:03,"Thanks @RichieB2B!
",Lukasa,RichieB2B
3538,2016-08-26 07:34:26,"@Lukasa it came up because Jira when served up with french locale sends french status reasons. Originally it was an exception and when we saw this was fixed I noticed that now the reason is garbled. I don't care _that_ much about it but the current code is definitely incorrect.
",mitsuhiko,Lukasa
3538,2016-08-26 16:18:19,"@mitsuhiko we changed it because there are servers sending back utf-8. I can't search for it at the moment, but you should have little problems finding that pull request and issue.
",sigmavirus24,mitsuhiko
3538,2016-09-06 08:33:02,"Resolved by #3554 instead. Thanks @mitsuhiko!
",Lukasa,mitsuhiko
3536,2016-10-29 19:13:36,"@tzickel At a quick glance, I don't think that urllib3 has any knowledge of whether our request is a redirect or not. We pass each redirect call to urllib3 as if it were a single request.

#3655 addresses the problem posed in your second test, I'm not claiming it's the best solution though. Any input there or clarification on what I'm missing in a urllib3 implementation would be appreciated :)
",nateprewitt,tzickel
3536,2016-11-03 15:36:23,"@Lukasa tzickel's commit was merged into master with #3655, so this can probably be closed.
",nateprewitt,Lukasa
3536,2016-11-03 15:36:57,"Yup. Thanks for the work @tzickel!
",Lukasa,tzickel
3535,2016-08-24 19:08:00,"It'd be really nice to have some more tests around this. Ideally tests that go into each of the branches. @nateprewitt, are you open to adding those?
",Lukasa,nateprewitt
3535,2016-08-25 22:24:07,"@Lukasa, tests are updated with custom classes. I think things should be good to go with tests.
",nateprewitt,Lukasa
3534,2016-09-05 23:56:42,"@lvg01 just a friendly reminder that we still need some information from you here. Thanks in advance!
",sigmavirus24,lvg01
3534,2016-09-06 08:30:11,"Yup, this looks like an error in parsing URLs. @sigmavirus24, you're the resident expert here: any thoughts?
",Lukasa,sigmavirus24
3534,2016-09-06 11:30:39,"@Lukasa I don't see the problem parsing the URL. 



Seems like that _might_ be the problem based on that message but I'm having trouble figuring out why. We [create a ProxyManager](https://github.com/kennethreitz/requests/blob/5259b374512ec4e47785f5004ec6ad30dafe906f/requests/adapters.py#L188) from urllib3 which parses the URL using `parse_url` from it's url utility module. This parses the example without issue and has always handled IPv6 addresses correctly.

If we look at the ProxyManager it has different logic for requests to an [https url versus an http url](https://github.com/kennethreitz/requests/blob/05d90b9379f57ee5f5d0beb268c495793fb5b2d7/requests/packages/urllib3/poolmanager.py#L331). 

I'm failing to see where the URL parsing would fail and why an IPv6 literal address would cause a name resolution failure.
",sigmavirus24,Lukasa
3534,2016-09-06 11:33:42,"@lvg01 Can you confirm that this works for me, please?


",Lukasa,lvg01
3534,2016-09-06 12:37:45,"@Lukasa it should be 



As that's the port specified in the proxy url that @lvg01 is showing us
",sigmavirus24,lvg01
3534,2016-09-06 12:37:45,"@Lukasa it should be 



As that's the port specified in the proxy url that @lvg01 is showing us
",sigmavirus24,Lukasa
3534,2016-09-07 05:04:56,"as for port 3128, the same:

> > > socket.getaddrinfo('[abcd:0123:dcba:3210::1]',3128)
> > > Traceback (most recent call last):
> > >    File ""<stdin>"", line 1, in <module>
> > > socket.gaierror: [Errno -3] Temporary failure in name resolution

On 06-09-16 14:38, Ian Cordasco wrote:

> @Lukasa https://github.com/Lukasa it should be
> 
> import  socket
> socket.getaddrinfo('[abcd:0123:dcba:3210::1]',3128)
> 
> As that's the port specified in the proxy url that @lvg01 
> https://github.com/lvg01 is showing us
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub 
> https://github.com/kennethreitz/requests/issues/3534#issuecomment-244936907, 
> or mute the thread 
> https://github.com/notifications/unsubscribe-auth/APqn3sQEPsDAyL0KKslJrvy1Vc1lJvUwks5qnV66gaJpZM4Jr16P.
",lvg01,lvg01
3534,2016-09-07 05:04:56,"as for port 3128, the same:

> > > socket.getaddrinfo('[abcd:0123:dcba:3210::1]',3128)
> > > Traceback (most recent call last):
> > >    File ""<stdin>"", line 1, in <module>
> > > socket.gaierror: [Errno -3] Temporary failure in name resolution

On 06-09-16 14:38, Ian Cordasco wrote:

> @Lukasa https://github.com/Lukasa it should be
> 
> import  socket
> socket.getaddrinfo('[abcd:0123:dcba:3210::1]',3128)
> 
> As that's the port specified in the proxy url that @lvg01 
> https://github.com/lvg01 is showing us
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub 
> https://github.com/kennethreitz/requests/issues/3534#issuecomment-244936907, 
> or mute the thread 
> https://github.com/notifications/unsubscribe-auth/APqn3sQEPsDAyL0KKslJrvy1Vc1lJvUwks5qnV66gaJpZM4Jr16P.
",lvg01,Lukasa
3534,2016-09-07 07:20:23,"@lvg01 And what results do you get without the square brackets?
",Lukasa,lvg01
3534,2016-09-07 07:29:46,"Hé, thats a good suggestion, then the test-code works!

So there is a parsing problem... Becaus in de proxies dict the ip 
address is surrounded by square brackets to seperate the ipv6 address 
with the tcp port (normal convention). The square brackets have to be 
stripped somewhere in the code OR the library should be able to read 
square bracket enclosed ipv6 addresses...

On 07-09-16 09:20, Cory Benfield wrote:

> @lvg01 https://github.com/lvg01 And what results do you get without 
> the square brackets?
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub 
> https://github.com/kennethreitz/requests/issues/3534#issuecomment-245196498, 
> or mute the thread 
> https://github.com/notifications/unsubscribe-auth/APqn3rUJ7CGgmbzHaY4zyhwxadrL8tVEks5qnmXagaJpZM4Jr16P.
",lvg01,lvg01
3530,2016-08-23 11:54:32,"Thanks @shellhead! :sparkles:
",Lukasa,shellhead
3529,2016-08-22 22:17:30,"OK, Thanks @Lukasa - in my case, I ended up using eventlet module to wrap the entire thing in a more ""global"" timeout.
",haydenth,Lukasa
3527,2016-08-21 22:21:13,"Thanks for this @michelsch92!

Some notes. Firstly, I'm afraid to say that #3518 was already resolved by #3527. I'm sorry about that, the timing was just a bit unlucky.

Secondly, the response helpers, I think we don't want to put them into the core requests library itself. Generally speaking we try to keep the surface area of the API down, because the more methods and properties that there are on the object the harder it gets to understand. However, @sigmavirus24 may be interested in having those properties in a headers helper object in the requests-toolbelt.

Unfortunately, that means this patch won't be accepted at this time. This has nothing to do with the quality of your work: altogether it was high quality work, and we'd have been pleased to accept it. Please do keep looking around for ways to contribute, because you're most welcome here!
",Lukasa,michelsch92
3526,2016-08-21 22:17:28,"This looks really good to me, I'm totally happy with it! Thanks so much @shellhead! :sparkles: :cake: :sparkles:
",Lukasa,shellhead
3524,2016-08-27 11:40:52,"Since @phrozetime hasn't responded and we have multiple people unable to reproduce it. I'm closing this until we have more details.
",sigmavirus24,phrozetime
3523,2016-08-19 13:33:35,"@Lukasa ~~No i dont, i will try and let you know~~
looks like its been auto set to this:


",ryanym,Lukasa
3523,2016-08-23 20:27:20,"@ryanym, does it work if you set the `all_proxy` scheme to `socks5` or `socks4`? Looking [here](https://github.com/kennethreitz/requests/blob/a2e41ba1f1ae81b2ed224030df908a1432ad435e/requests/packages/urllib3/contrib/socks.py#L153), seems like it's looking for the socks version. Unsure why you're getting different behavior between the two versions, though.

Edit: Did some more digging.

FWIW, it looks like part of the 2.11.0 release was to support all_proxy. So in 2.10.0, it didn't pick up the environment variable, all_proxy, and didn't bother using a proxy. In 2.11.0, it does, which causes the SOCKS version check, and the resulting error. 

I was able to get something to work by either setting all_proxy to `socks5://someproxy.com:someport` or sending the `proxies` dictionary (with the `all` key set to the same value as all_proxy environment variable) on the request.

Disclaimer: I'm a total n00b with the code base, just trying to help out. I would defer to @Lukasa for any real, competent help with this issue though :)
",shellhead,ryanym
3523,2016-08-23 20:27:20,"@ryanym, does it work if you set the `all_proxy` scheme to `socks5` or `socks4`? Looking [here](https://github.com/kennethreitz/requests/blob/a2e41ba1f1ae81b2ed224030df908a1432ad435e/requests/packages/urllib3/contrib/socks.py#L153), seems like it's looking for the socks version. Unsure why you're getting different behavior between the two versions, though.

Edit: Did some more digging.

FWIW, it looks like part of the 2.11.0 release was to support all_proxy. So in 2.10.0, it didn't pick up the environment variable, all_proxy, and didn't bother using a proxy. In 2.11.0, it does, which causes the SOCKS version check, and the resulting error. 

I was able to get something to work by either setting all_proxy to `socks5://someproxy.com:someport` or sending the `proxies` dictionary (with the `all` key set to the same value as all_proxy environment variable) on the request.

Disclaimer: I'm a total n00b with the code base, just trying to help out. I would defer to @Lukasa for any real, competent help with this issue though :)
",shellhead,Lukasa
3523,2016-08-24 15:41:30,"@shellhead, thanks for the reply, setting `all_proxy` sheme to `socks5` or `socks4` seems  gives me a timeout


",ryanym,shellhead
3523,2016-08-26 13:28:10,"@Lukasa yes, i have `HTTP_PROXY, HTTPS_PROXY, http_proxy, https_proxy` set to `http://user:pass@someproxy.com:8080`
",ryanym,Lukasa
3523,2016-08-26 15:09:42,"@Lukasa I don't want to use any proxy settings.
r = requests.post(**\* , proxies=None) does not work.

It used to work few days ago. But problem started last night, which is failing my application.

What if I don't want to use any of the proxy configurations?

I tried os.environ.pop('http_proxy') and so on for each protocol. It didn't worked.
However, os.envron.pop('all_proxy') worked as it had the url : socks://***.

Any workaround?
",himanshub16,Lukasa
3523,2016-10-24 13:10:15,"@Rahul529 Sorry, what specific problem are you encountering?
",Lukasa,Rahul529
3523,2016-10-25 04:21:06,"@Lukasa , i have provided the error while the request is sent from python script .
Value Error : unable to determine the version SOCKS from socks://<ip>
",Rahul529,Lukasa
3523,2017-02-21 08:12:21,@craigmaloney Well ultimately what's not clear to me is: what do *Ubuntu* mean with a scheme that just says `socks://`? A citation one way or the other would be really handy before we just leap in to make a call.,Lukasa,craigmaloney
3522,2016-08-19 12:04:22,"@dothebart It's hard to know exactly what's going on here, but the connection will be left open unless the `Session` is leaked, closed, or otherwise lost. The only other way the connection gets lost is if `Connection: close` is set.
",Lukasa,dothebart
3521,2016-08-28 01:41:33,"@KatieLucas-Grapeshot you should get an InvalidHeader exception from requests in 2.11+. Below is a quick example in my terminal showing the exception and then try/except with it. Hope this helps!


",clesiemo3,KatieLucas-Grapeshot
3521,2016-08-29 15:27:31,"@clesiemo3 is correct, I got the same error too. But is it valid to throw error for starting header custom ?with \n?
",kedark3,clesiemo3
3521,2016-08-29 16:08:08,"@kedark3 I don't understand the question. The current version of Requests rejects all headers with newline characters in them.
",Lukasa,kedark3
3521,2016-08-29 16:08:44,"@Lukasa I think they wanted to ask whether this issue should be closed or not.
",untitaker,Lukasa
3517,2016-08-17 07:50:00,"@eduOS In this instance, it's possible that the proxy is setting X-Forwarded-For header, which will contain the IP address of your source. Alternatively, it's possible that you're getting forwarded to a HTTPS address. Either way, it's likely that the connection is still using the proxy: so longa s the scheme of the two URLs match, we _will_ use it.
",Lukasa,eduOS
3516,2016-08-17 09:03:24,"@Lukasa @kennethreitz can i have option to disable the SOCKS proxy???
",alochym01,kennethreitz
3516,2016-08-17 09:03:24,"@Lukasa @kennethreitz can i have option to disable the SOCKS proxy???
",alochym01,Lukasa
3516,2016-08-17 10:23:22,"@alochym01 If you don't want it, why is it set in your environment? Regardless, if you use a session with `trust_env=False` that problem will no longer occur, though you'll lose your environment HTTP proxy. 

However, there does seem like there is a bug here: requests will prefer `all_proxy` to a scheme-specific proxy. I think that's a bad idea. @sigmavirus24, should we re-order that?
",Lukasa,alochym01
3516,2016-08-17 11:52:28,"@Lukasa So as I understand it, `all_proxy` is what should be used if `http(s)_proxy` is unset. And in a more general sense it should be used if `<protocol>_proxy` is unset. So based on that understanding (and assuming I'm correct) yes we definitely should re-order that. Let's file a new bug for that.
",sigmavirus24,Lukasa
3516,2016-08-17 12:17:55,"@alochym01 Another option seems to be to use `proxies={'all': None}`.
",Lukasa,alochym01
3516,2016-08-18 00:57:47,"@Lukasa proxies={'all': None} not work because the network should go through proxy, so i see you did open the new issue :)
let follow the new issue :+1: 
",alochym01,Lukasa
3514,2016-08-15 18:03:32,"@kennethreitz I think @Lukasa and I can still do it for you if you want us to. Just let us know. We'd both be happy to help out.
",sigmavirus24,kennethreitz
3514,2016-08-15 18:03:32,"@kennethreitz I think @Lukasa and I can still do it for you if you want us to. Just let us know. We'd both be happy to help out.
",sigmavirus24,Lukasa
3513,2016-08-15 10:21:33,"@nateprewitt is this related to #2228?
",sigmavirus24,nateprewitt
3513,2016-08-15 13:33:03,"@sigmavirus24 It wasn't originally, I found a few minor hiccups making the `.json` change and then did a pass through the docs from there. I realize this is kind of a mess of unrelated doc changes so I can break it up into different commits or PRs if needed.

It looks like most of the discussion in #2228 is about timeouts documentation. Was there something low hanging you wanted to see integrated?
",nateprewitt,sigmavirus24
3512,2016-08-14 01:59:34,"Hey @listingmirror, you can find more information on this in #3477. 2.11.0 brought in more rigid header validation, requiring values to be passed as either strings or bytes. I'm betting ebaysdk is passing the Content-Length header as an int. This will need to be converted to a string for use with 2.11.0+.
",nateprewitt,listingmirror
3512,2016-08-14 02:06:32,"@nateprewitt  Perfect I will chase it down there ;)
",listingmirror,nateprewitt
3511,2016-08-13 08:42:21,"Thanks @forrestchang! :sparkles: :cake: :sparkles:
",Lukasa,forrestchang
3510,2016-08-18 08:28:16,"Thanks @nateprewitt!
",Lukasa,nateprewitt
3509,2016-08-12 11:16:05,"@breize You're linking to an ancient version of the documentation. Try this: http://docs.python-requests.org/en/latest/user/advanced/
",Lukasa,breize
3508,2016-08-12 07:41:56,"@forrestchang Thanks, would you like to open a PR to change it?
",Lukasa,forrestchang
3508,2016-08-13 02:25:26,"@Lukasa Yes, I would like to. I have modified the mistake in `.rst` file. Should I generate it to the HTML file? 
",forrestchang,Lukasa
3508,2016-08-13 02:39:00,"Hey @forrestchang, there's no need to run the docs build. If you open a PR with the rst changes, read the docs will generate changes on merge.
",nateprewitt,forrestchang
3508,2016-08-13 02:44:43,"@nateprewitt Thanks.
",forrestchang,nateprewitt
3507,2016-08-11 23:54:54,"Hey @ppolewicz, I believe you can find answers addressing this in #3479, primarily @Lukasa's comment [here](https://github.com/kennethreitz/requests/issues/3479#issuecomment-238633044).

pip is currently unable to filter out libraries based on python version, but it was also [noted](https://github.com/kennethreitz/requests/issues/3479#issuecomment-238640627) in #3479 that this may be available in the _distant_ future. I'm not positive on when 3.2 was officially deprecated for Requests (it was before 2.11.0) but it was removed from the trove list in setup.py in May 2013. Unfortunately, I don't believe there's any intent to reintroduce support for 3.2, per @sigmavirus24 [comments](https://github.com/kennethreitz/requests/issues/3479#issuecomment-238640113).
",nateprewitt,ppolewicz
3507,2016-08-12 11:23:07,"> I understand that 3.2 is no longer supported by requests, but pip being broken as it is with the version mapping causes trouble for some people.

I'm sorry, I don't quite understand what you are referring to @ppolewicz. 

> I think a minor change of 2 LOC to prevent this would be acceptable?

@ppolewicz except that minor change implies Python 3.2 support. Python 3.2 as a product itself is unsupported. The number of users it has according to PyPI is incredibly small. I don't see a whole lot of value in changing those two lines, but @Lukasa and @kennethreitz may disagree. I'm not against it, I just don't agree that we should be implicitly supporting an unsupported version that is mostly not used (regardless of how easy it is).
",sigmavirus24,ppolewicz
3507,2016-08-12 11:23:07,"> I understand that 3.2 is no longer supported by requests, but pip being broken as it is with the version mapping causes trouble for some people.

I'm sorry, I don't quite understand what you are referring to @ppolewicz. 

> I think a minor change of 2 LOC to prevent this would be acceptable?

@ppolewicz except that minor change implies Python 3.2 support. Python 3.2 as a product itself is unsupported. The number of users it has according to PyPI is incredibly small. I don't see a whole lot of value in changing those two lines, but @Lukasa and @kennethreitz may disagree. I'm not against it, I just don't agree that we should be implicitly supporting an unsupported version that is mostly not used (regardless of how easy it is).
",sigmavirus24,Lukasa
3507,2016-08-12 11:25:53,"@ppolewicz In principle, yes, we could do that. In practice, doing so postpones the inevitable. You'll note that [pip itself dropped support for Python 3.2 in 8.0.0](https://pip.pypa.io/en/stable/news/#release-notes). People who refuse to upgrade to Python 3.3, which was released _four years ago_, are going to need to get comfortable with the idea that their software ecosystem is going to start breaking.

> Requests are extremely popular, so it is bound to break something somewhere. I think a minor change of 2 LOC to prevent this would be acceptable?

This logic is just fundamentally flawed. The same argument can be made to support Python 3.0 and Python 3.1, and Python 2.5 and earlier. Each change is only a few LoC, but at a certain point we're just holding ourselves back. 3.2 is no longer receiving security patches from Python dev, which means that it is no longer supported by Python-dev: it _should not be used anymore_, because it cannot be made to be secure.

I have exactly no interest in supporting Python 3.2 for free. Users who refuse to upgrade to Python 3.3 can apply patches themselves. Users who cannot upgrade to Python 3.3 because their LTS distribution will not ship it to them should contact their distributor, who can either apply appropriate patches themselves or who can pay the Requests development team to continue to support Python 3.2. Those are the two options I will find acceptable.
",Lukasa,ppolewicz
3493,2016-08-10 16:10:59,"I was on it too. Did not find a way to properly add tests though since httpbin does not seem to be able to post to an endpoint and respond with a 301/302. Am I wrong on this one?

Anyway since @nateprewitt is on it I will not open my PR ;)
",saveman71,nateprewitt
3493,2016-08-10 17:04:49,"@saveman71, sorry, I didn't mean to jump on your PR. I wasn't sure if anyone had started working on it.

@Lukasa I added a test to check the header removal and empty body.
",nateprewitt,saveman71
3493,2016-08-10 17:04:49,"@saveman71, sorry, I didn't mean to jump on your PR. I wasn't sure if anyone had started working on it.

@Lukasa I added a test to check the header removal and empty body.
",nateprewitt,Lukasa
3493,2016-08-10 17:10:28,"@Lukasa re: [this line](https://github.com/kennethreitz/requests/pull/3493/files#diff-28e67177469c0d36b068d68d9f6043bfR151)

Do you have an opinion on moving the declaration farther up to avoid the multiple calls to the headers attribute on the prepare_request? Also as for the conversion from try/except, if that was done would you want it in a separate PR to avoid semi-unrelated changes?
",nateprewitt,Lukasa
3493,2016-08-10 17:16:37,"@nateprewitt no worries at all, I should have stated I started working on it.
As for the line thing, I do not think it is a performance issue (I believe, but I might be mistaken) because well requests is a network library and ""premature optimization is the root of all evil"". (https://www.xkcd.com/1691/)

I was suggesting that merely because it's already been like that [here](https://github.com/nateprewitt/requests/blob/20a2b5ac71f1d6ce674ca2633172ef5964ec9c38/requests/sessions.py#L192) and [here](https://github.com/nateprewitt/requests/blob/20a2b5ac71f1d6ce674ca2633172ef5964ec9c38/requests/sessions.py#L223) for example.

On a side note, :+1: for a separate PR.
",saveman71,nateprewitt
3493,2016-08-10 17:22:28,"@saveman71, I only bring up the optimization because @sigmavirus24 has [brought it up before](https://github.com/kennethreitz/requests/pull/3362#discussion_r68398047). If I'm misunderstanding his original intent though, I vote we completely remove the declaration, in favor of `prepared_request.headers`. `headers` currently doesn't provide a lot of use in `resolve_redirects`.

Edit: Clarity and Link
",nateprewitt,saveman71
3493,2016-08-10 19:33:06,"Quick semi-unrelated question, what would be the timeline of a release on PyPI after this gets merged?

cc @Lukasa 
",saveman71,Lukasa
3491,2016-08-09 21:05:57,"@Lukasa @sigmavirus24 It looks like the [HISTORY.rst change](https://github.com/kennethreitz/requests/blame/master/HISTORY.rst#L33) specifies only integers. Perhaps that should be changed to 'non-string'?
",nateprewitt,Lukasa
3491,2016-08-09 21:05:57,"@Lukasa @sigmavirus24 It looks like the [HISTORY.rst change](https://github.com/kennethreitz/requests/blame/master/HISTORY.rst#L33) specifies only integers. Perhaps that should be changed to 'non-string'?
",nateprewitt,sigmavirus24
3491,2016-08-09 21:10:05,"@nateprewitt Yes, it should.

@jidar How do we represent True/False as strings? ""True""/""False""? Why that over ""true""/""false"", ""1""/""0"", or ""Yes""/""No""? Requests is simply refusing to guess in this case, and asking that you unambiguously decide what you want to send.
",Lukasa,jidar
3491,2016-08-09 21:10:05,"@nateprewitt Yes, it should.

@jidar How do we represent True/False as strings? ""True""/""False""? Why that over ""true""/""false"", ""1""/""0"", or ""Yes""/""No""? Requests is simply refusing to guess in this case, and asking that you unambiguously decide what you want to send.
",Lukasa,nateprewitt
3491,2016-08-09 21:22:10,"@Lukasa str(value) seems acceptable.  No need for special casing anything really.  It's what the user is going to be forced to do on their end now anyway.
",jidar,Lukasa
3491,2016-08-09 21:31:30,"@jidar str(value) is not acceptable, nor is it what anyone will be forced to do. That may work in your case with your data type, but with many data types it will not work and will lead to us emitting bizarre headers that make no sense. For example, consider datetime objects, which print internal representations when str() is called on them. 
",Lukasa,jidar
3491,2016-08-09 21:42:32,"@jidar if that's acceptable for you, I'm very happy. You should only ever be treating your headers as strings though. I would not expect any movement on this.
",sigmavirus24,jidar
3490,2016-08-09 19:44:51,"@Lukasa That was it! I was using the data field preformatted with `urlib.parse.urlencode`, my bad.
Felt bad when I read 2mn later in the official documentation:

> Requests allows you to send organic, grass-fed HTTP/1.1 requests, without the need for manual labor. 
> 
> There's no need to manually add query strings to your URLs, or to form-encode your POST data.

Thank you for your quick and effective answer :+1:
",saveman71,Lukasa
3490,2016-08-10 14:53:58,"Hey @Lukasa,

Yesterday I tested it rapidly and thought it worked. Today I'm not able to reproduce what worked...



The output with the patch commented is:



You can see that the header `Content-Type` is repeated in the redirect although it was not specified explicitly in the request headers as before.

The log if I activate the workaround suppressing the header for the specific request:



Sorry for the french in error messages/URLs. I, unfortunately, cannot reveal what is the website responsible for the misbehaviour (NDA).

Am I doing something wrong here?
",saveman71,Lukasa
3487,2016-08-09 15:40:18,"Thanks @drpoggi!
",Lukasa,drpoggi
3486,2016-08-09 13:53:03,"@sigmavirus24 sorry, I tried the revert button and got an error so did it by hand.
",nateprewitt,sigmavirus24
3485,2016-08-10 09:09:12,"@Alexander3 urllib.request doesn't connection pool. This means that the remote server closes the connection at the end of the request, which is what httplib was expecting. You can try doing this by setting the `Connection` header value to `close`. This is generally a bad idea, as it's very wasteful, but it may help in this case.
",Lukasa,Alexander3
3482,2016-08-09 09:10:46,"@rohitkhatri This is almost certainly a problem with the OpenSSL shipped on your server. You'll likely need to elaborate for us to help you: for example, _what_ error do you see?
",Lukasa,rohitkhatri
3482,2016-08-09 09:12:14,"@Lukasa You can check this out here http://52.40.67.18/testing
",rohitkhatri,Lukasa
3482,2017-01-26 14:07:36,@rohitkhatri How did you resolve this? I'm getting the same error?,softvar,rohitkhatri
3481,2016-08-09 08:39:02,"For you, @jone, at least for the short term I recommend not using `decode_unicode`. It saves relatively little code and probably doesn't really work the way you want it to work anyway.
",Lukasa,jone
3481,2016-08-09 08:46:07,"@Lukasa thanks for looking into it that fast!
Yes, I'm going to decode manually, which is quite an easy change for me. 😉 
",jone,Lukasa
3481,2016-08-10 03:29:09,"@maxpowa #3486 reverts this change and should be included in the upcoming 2.11.1 release. As @Lukasa suggested [above](https://github.com/kennethreitz/requests/issues/3481#issuecomment-238489871), it's recommended not to use `decode_unicode=True` for the time being. You can either manually decode the strings as a temporary fix, or continue using 2.10 if your workflow allows it.
",nateprewitt,maxpowa
3481,2016-08-10 03:29:09,"@maxpowa #3486 reverts this change and should be included in the upcoming 2.11.1 release. As @Lukasa suggested [above](https://github.com/kennethreitz/requests/issues/3481#issuecomment-238489871), it's recommended not to use `decode_unicode=True` for the time being. You can either manually decode the strings as a temporary fix, or continue using 2.10 if your workflow allows it.
",nateprewitt,Lukasa
3480,2016-08-09 03:03:21,"@kennethreitz thanks, tried copy pasting your en dash from above. Apparently GitHub's UI wasn't a fan of my methods.
",drpoggi,kennethreitz
3479,2016-08-08 22:41:17,"Hey @huseyinyilmaz, I think this may have been discussed in #3385 (particularly [here](https://github.com/kennethreitz/requests/pull/3385#discussion_r69560890)). Removing the `u` designation on the string would revert the unicode support added in #3385.
",nateprewitt,huseyinyilmaz
3479,2016-08-08 22:47:24,"Ah, I see @nateprewitt already chimed in over here.
",sigmavirus24,nateprewitt
3479,2016-08-08 22:51:05,"I see. So, python3.2 is not supported by requests library. Than I will just close the issue and pr.

Thanks for the link @nateprewitt .
",huseyinyilmaz,nateprewitt
3479,2016-08-09 17:46:55,"@huseyinyilmaz That is not possible, I'm afraid: you'd need to ask @dstufft to provide tools to do that. It's a nice idea, certainly, although it requires much more formalisation of the trove classifiers system that currently exists.
",Lukasa,huseyinyilmaz
3479,2016-08-09 18:09:36,"@huseyinyilmaz I think you're going to have a bad time as more and more projects find it unreasonable to support Python 3.2. There are _very_ few users on that version and it does not even receive security updates. 
",sigmavirus24,huseyinyilmaz
3478,2016-08-08 22:52:49,"@sigmavirus24 . I did not realize that pyhon3.2 is not supported. Thanks for the explanation. I will close the pr.
",huseyinyilmaz,sigmavirus24
3478,2016-08-09 02:39:45,"Requests no longer supports Python 3.2. As referenced by @sigmavirus24 this has been the case for ever a year.

If you view the setup.py for requests it specifies 3.3 and higher, or 2.6 and higher.
",TetraEtc,sigmavirus24
3477,2016-08-08 21:29:51,"@clarkbreyman-yammer To be clear, this is a bit of a borderline case. You can see the decision making most recently in #3386 and #3388, but the basic argument is: non-string headers were never _intended_ to work, so the fact that they stopped working is acceptable. In fact, that have previously not worked in the past.

@lmazuel You're right that this got missed from the changelog though, and that's 100% my fault: the breakage here was incidental of our tighter verification of header values, and as a result I didn't see it when I was compiling the changelog. I'd welcome a PR that updates the changelog if you'd like to make one.
",Lukasa,clarkbreyman-yammer
3477,2016-08-08 21:29:51,"@clarkbreyman-yammer To be clear, this is a bit of a borderline case. You can see the decision making most recently in #3386 and #3388, but the basic argument is: non-string headers were never _intended_ to work, so the fact that they stopped working is acceptable. In fact, that have previously not worked in the past.

@lmazuel You're right that this got missed from the changelog though, and that's 100% my fault: the breakage here was incidental of our tighter verification of header values, and as a result I didn't see it when I was compiling the changelog. I'd welcome a PR that updates the changelog if you'd like to make one.
",Lukasa,lmazuel
3477,2016-08-08 21:36:28,"Thanks @kennethreitz.
",Lukasa,kennethreitz
3477,2016-08-08 21:40:22,"Ok, thanks for the clarification. I will update my code accordingly.
@kennethreitz if you could add your note in PyPI too, would be amazing.
",lmazuel,kennethreitz
3477,2016-08-08 21:40:46,"@lmazuel on it!
",kennethreitz,lmazuel
3477,2016-08-08 21:41:43,"@lmazuel done ✨🍰✨
",kennethreitz,lmazuel
3477,2016-08-08 21:42:06,"@lmazuel p.s. thank you for caring :P
",kennethreitz,lmazuel
3477,2016-09-26 17:29:59,"Hey @COLDMOUNT, you can find the documentation [here](http://docs.python-requests.org/en/master/user/quickstart/#custom-headers) in the last paragraph from the quickstart section for custom headers. We also published the change in [HISTORY.rst](https://github.com/kennethreitz/requests/blob/master/HISTORY.rst) for 2.11.0 which is Requests' changelog.
",nateprewitt,COLDMOUNT
3477,2016-09-27 09:30:26,"It is better to offer an example of the headers in str type, or how to
translate a headers in dict type to str type, thanks.

2016-09-27 1:30 GMT+08:00 Nate Prewitt notifications@github.com:

> Hey @COLDMOUNT https://github.com/COLDMOUNT, you can find the
> documentation here
> http://docs.python-requests.org/en/master/user/quickstart/#custom-headers
> in the last paragraph from the quickstart section for custom headers. We
> also published the change in (HISTORY.rst)[https://github.
> com/kennethreitz/requests/blob/master/HISTORY.rst] for 2.11.0 which is
> Requests' changelog.
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/kennethreitz/requests/issues/3477#issuecomment-249638650,
> or mute the thread
> https://github.com/notifications/unsubscribe-auth/ARIa89wU7tKL8Br2WWU6lJy8HEbNr72Vks5quAE3gaJpZM4JffFG
> .
",COLDMOUNT,COLDMOUNT
3477,2016-09-27 10:22:55,"@COLDMOUNT `dict` is _absolutely_ accepted, we haven't changed that at all. What we have changed is that the keys and values of that dict must now be strings. Previously it was possible for them to be a few other types by accident, which has now been resolved. The documentation does not need changing.
",Lukasa,COLDMOUNT
3477,2016-09-27 12:14:21,"Got it, cool! Thanks! :)

2016-09-27 18:23 GMT+08:00 Cory Benfield notifications@github.com:

> @COLDMOUNT https://github.com/COLDMOUNT dict is _absolutely_ accepted,
> we haven't changed that at all. What we have changed is that the keys and
> values of that dict must now be strings. Previously it was possible for
> them to be a few other types by accident, which has now been resolved. The
> documentation does not need changing.
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/kennethreitz/requests/issues/3477#issuecomment-249825918,
> or mute the thread
> https://github.com/notifications/unsubscribe-auth/ARIa8z0jZ-ovBtvFWl4hTXnkk_kJXobrks5quO6ggaJpZM4JffFG
> .
",COLDMOUNT,COLDMOUNT
3471,2016-08-04 10:34:16,"> Are you saying requests<=0.7.4 handles multipart file uploads well by default?

No. That is not what @Lukasa is saying.

>  But, I dont think this below code uses multipart format

You're using the files parameter, so yes, it is sending a multipart message.

If you describe what you want the body to look like, we _might_ be able to help you, but you should be using [StackOverflow](stackoverflow.com) for help with this module, not the defect tracker.
",sigmavirus24,Lukasa
3470,2016-08-04 07:13:27,"@Lukasa  are you saying if urllib3 is updated, then requests could be in the position of having this option?
",yeukhon,Lukasa
3469,2016-08-03 20:04:55,"Woop, woop, thanks @roselma! :sparkles: :cake: :sparkles:
",Lukasa,roselma
3467,2016-09-05 23:59:48,"I'm closing this due to about a month of inactivity. If you can provide more information @alextorex0 such that we can continue debugging this, we will happily reopen this.
",sigmavirus24,alextorex0
3465,2016-08-01 12:35:23,"@JamesJGoodwin We will populate the Host header for you, don't worry about it. It will always include the host portion of whatever URL you use: if you send to http://example.org, Host will be ""example.org"".

To see your headers, you can look for `r.request.headers`. That does not show _all_ headers: some automatically added headers (e.g. `Host`) are not provided in that dictionary because the layers below Requests add them. 
",Lukasa,JamesJGoodwin
3464,2016-08-01 04:35:36,"@kennethreitz Oh - in my defense I did try searching closed issues as well :).
",glyph,kennethreitz
3463,2017-02-10 21:51:23,thanks @nateprewitt ,kennethreitz,nateprewitt
3460,2016-08-02 16:26:29,"It looks like the [`RequestsCookieJar` class](http://docs.python-requests.org/en/master/api/#requests.cookies.RequestsCookieJar) is available on the main page of links in the documentation, under the [Developer Interface docs](http://docs.python-requests.org/en/master/#the-api-documentation-guide).

Were you looking to have a code example added in the Quickstart section @roselma, or would a link to the existing documentation suffice? 
",nateprewitt,roselma
3447,2016-07-28 08:18:50,"@Lukasa Thank you very much, it's working!
",gsw945,Lukasa
3444,2016-07-27 13:47:42,"@Lukasa further, in v1.0 didn't @kennethreitz completely tear out what logging requests did provide because people were constantly trying to add tons more logging on top of what we provided? I feel like were were to accept this we'd be in the same situation.

Further, what's being logged here is a very clear overload of information for any logging that I would view as reasonable for requests. This seems to be working very well in Café as it is and I see little reason to include this in Requests.

I appreciate your offer of the code @seemethere, but I'm very strongly against this. If you want review on the code anyway, I'm happy to provide that in a separate forum.
",sigmavirus24,Lukasa
3444,2016-07-27 13:47:42,"@Lukasa further, in v1.0 didn't @kennethreitz completely tear out what logging requests did provide because people were constantly trying to add tons more logging on top of what we provided? I feel like were were to accept this we'd be in the same situation.

Further, what's being logged here is a very clear overload of information for any logging that I would view as reasonable for requests. This seems to be working very well in Café as it is and I see little reason to include this in Requests.

I appreciate your offer of the code @seemethere, but I'm very strongly against this. If you want review on the code anyway, I'm happy to provide that in a separate forum.
",sigmavirus24,seemethere
3441,2016-07-26 12:26:25,"Thanks @scop!
",sigmavirus24,scop
3440,2016-07-26 04:50:04,"@nateprewitt is correct. Thanks for reaching out, though @walkerlala!
",kennethreitz,walkerlala
3440,2016-07-26 04:50:04,"@nateprewitt is correct. Thanks for reaching out, though @walkerlala!
",kennethreitz,nateprewitt
3439,2016-08-04 17:21:49,"Thanks for following up @dhduvall, and thanks for the work, even though we will probably no longer accept it. =)
",Lukasa,dhduvall
3430,2016-07-22 07:20:24,"I'm not averse to this in principle. I suspect we'll find getting this right across things like redirects is a bit tricky, but we can worry about that when we do it. If you want to submit a patch, @dhduvall, I'll review it. =)
",Lukasa,dhduvall
3430,2016-07-22 22:20:24,"I completely agree that the up-stack application should be doing a better job of handling exceptions.  But, like you, @sigmavirus24, I'm not about to expect that from OpenStack, and given how prevalent the problem is, it seems like some relief could be found in the common code, even if it's not an actual fix to the real bug(s) here.

There are a couple of exceptions raised that IMO should include the URL but don't, and don't provide a request object (`InvalidURL` instances seem to be where that would be the most useful). They should probably just have their message strings rewritten, but my initial attempt won't try to change that.
",dhduvall,sigmavirus24
3427,2016-07-20 16:40:58,"@kennethreitz, I kinda want you to take the lead on code review for this: just flag changes you don't like and I'm sure @nateprewitt will undo it.
",Lukasa,kennethreitz
3427,2016-07-20 16:40:58,"@kennethreitz, I kinda want you to take the lead on code review for this: just flag changes you don't like and I'm sure @nateprewitt will undo it.
",Lukasa,nateprewitt
3426,2016-07-20 17:00:51,"@kennethreitz Same reason: https://hynek.me/articles/hasattr/
",Lukasa,kennethreitz
3423,2016-07-18 19:00:37,"@Lukasa Not sure to understand but if you say so... :)
That means that the traceback is wrong and the following should work (it quite hard to test here) ?


",dvasseur,Lukasa
3423,2016-07-18 19:01:26,"@dvasseur Yes. =)
",Lukasa,dvasseur
3423,2016-07-18 19:05:44,"@Lukasa ok, thanks!
",dvasseur,Lukasa
3422,2016-07-18 18:54:56,"@remram44 For what it's worth, I would _heartily_ support a patch to the stdlib's `urllib.request` module's `getproxies` method to implement this kind of checking. That seems like a much more productive place to put the patch. =) If you want to open a bug report for that, I'll happily chime in: I may even volunteer to write the patch myself!
",Lukasa,remram44
3421,2016-07-17 16:52:14,"That's about where I am at in debugging too @Lukasa. It looks like `isclosed` is being set and will evaluate to `True` but the `closed` attribute is permanently `False`. I'm trying to figure out if this is a bug in `httplib` or `urllib3`'s use of it. Our current check in `is_fp_closed` is checking the `closed` attribute.
",nateprewitt,Lukasa
3421,2016-07-17 16:53:52,"The answer is that it doesn't really matter. =)

This behaviour is present in basically all the shipped Python 3 solutions, which means even if it's a bug we need to be bug-compatible with it. I think the best solution is simply to rely on `isclosed` if it's present, in preference to `closed`, which is then in preference to reaching in to the `fp` thing.

Are you interested in writing the PR that fixes this, @nateprewitt?
",Lukasa,nateprewitt
3421,2016-07-17 17:04:18,"Ok, for now let's close this. @nateprewitt, can you open an equivalent issue on the urllib3 repository? You can use this test code as the example (it should live in `test_socketlevel.py` in the urllib3 repo):


",Lukasa,nateprewitt
3417,2016-07-15 07:12:36,"@nateprewitt Thanks for starting this!

Unfortunately, quite a lot of people use non-urllib3 file-like objects to back requests, and I'd like to avoid breaking that if we can. Can you rearrange this to support urllib3 if possible but otherwise fallback to some other behaviour, [per my comment](https://github.com/kennethreitz/requests/issues/2939#issuecomment-166003526).
",Lukasa,nateprewitt
3417,2016-07-16 15:08:45,"Sorry, @sigmavirus24, I just pushed a new copy with partially finished changes. I have a couple of questions on how we're expecting the code to function that I was going to annotate for discussion.
",nateprewitt,sigmavirus24
3417,2016-07-16 20:25:04,"Ok, I'm happy with this. @sigmavirus24?
",Lukasa,sigmavirus24
3417,2016-07-19 14:56:03,"@nateprewitt Yeah, this is a breaking change. =)
",Lukasa,nateprewitt
3417,2016-08-22 15:33:55,"Hey @Lukasa @sigmavirus24, just wanted to check in on this. Let me know if you're waiting on anything from me.
",nateprewitt,Lukasa
3417,2016-08-22 15:33:55,"Hey @Lukasa @sigmavirus24, just wanted to check in on this. Let me know if you're waiting on anything from me.
",nateprewitt,sigmavirus24
3417,2016-08-22 16:30:29,"No problem from me. @sigmavirus24?
",Lukasa,sigmavirus24
3417,2016-08-22 16:41:45,"Looks fine as a start. Thanks @nateprewitt 

@Lukasa since we're sans CI, have you pulled this and run the tests?
",sigmavirus24,Lukasa
3417,2016-08-22 16:41:45,"Looks fine as a start. Thanks @nateprewitt 

@Lukasa since we're sans CI, have you pulled this and run the tests?
",sigmavirus24,nateprewitt
3417,2016-08-22 16:42:24,"@sigmavirus24 I haven't, would you like me to?
",Lukasa,sigmavirus24
3417,2016-08-22 16:48:05,"@nateprewitt I merged the original branch into the head of proposed/3.0.0 and it passed just fine. No need to rebase
",sigmavirus24,nateprewitt
3416,2016-07-15 07:22:48,"@bdusell I'm _strongly_ averse to adding more methods to `Session` if they can possibly be avoided. I'd altogether be happier to have helpers on the `RequestsCookieJar` that `merge_cookies` uses.
",Lukasa,bdusell
3415,2016-07-15 07:07:43,"Thanks @nateprewitt! :sparkles: :cake: :sparkles:
",Lukasa,nateprewitt
3414,2016-07-15 07:08:25,"@seu1tyz You need to provide substantially more information than this for us to be able to help you. Please read [this document](https://github.com/kennethreitz/requests/blob/master/CONTRIBUTING.md) and answer the questions it asks.
",Lukasa,seu1tyz
3401,2016-07-14 08:34:57,"@kennethreitz Python 2.7 also has the `io` module: this change keeps the code working as expected on Python 2.
",Lukasa,kennethreitz
3401,2016-07-14 16:20:01,"> Either way, my above comment is generally relevant. We may want to start making our examples 3.x soon, if we want to ""move the needle"". I don't think there's any rush to do so, though.

@kennethreitz have you looked at the downloads for requests recently? I'm not sure which version is ""winning"" but regardless, I think we can write the examples to work across all supported versions without having to have an opinion on whether they're all Py2 or Py3 or moving any needle.
",sigmavirus24,kennethreitz
3391,2016-07-12 09:44:11,"For anyone with this problem:

I've fixed it by following @Lukasa 's suggestions and added this just after importing requests:



Then, where I was using `requests.get()` before, I used `sess.get()`.

Hopefully this helps, and thanks for your help @Lukasa !
",vandernath,Lukasa
3389,2016-07-07 19:38:33,"> LGPL allows re-distribution, which is what we are doing here. 

Right. It's important to understand that things that requests vendors are not modified when vendored. I work on chardet upstream to ensure it's suitable for our vendoring. Likewise @Lukasa and I work on urllib3.

We don't vendor & modify, we simply vendor to redistribute.
",sigmavirus24,Lukasa
3388,2016-07-07 13:27:44,"While you're here @sigmavirus24 and @Lukasa, I threw together [another commit](https://github.com/nateprewitt/requests/commit/be31a90906deb5553c2e703fb05cf6964ee23ed5) related to this to encapsulate the type error thrown by `re` when non-strings are passed to `check_header_validity`. This is to make it clear that the behaviour is to be expected, but may be overkill with this documentation now. Any thoughts on if this would be worth opening? 
",nateprewitt,Lukasa
3388,2016-07-07 14:56:21,"Thanks @nateprewitt 🍮 🍰 ☕ 
",sigmavirus24,nateprewitt
3387,2016-07-05 16:13:13,"@nateprewitt that pull request was _rejected_ precisely because integers are not defined behaviour within requests and haven't been for _years_.
",sigmavirus24,nateprewitt
3387,2016-07-05 16:29:13,"Sorry, @sigmavirus24, I was suggesting something along this line for the check since it wouldn't be transforming the value in this case. I was trying to use this as a template example. I realize the documentation says that headers must be strings, and that was the assumption I was operating off of when I wrote #3366. I submitted the passthrough as the initial pull request here because it avoids dealing with any defining behaviour by ignoring non-strings.

I just wanted to make sure both options were at least briefly discussed.
",nateprewitt,sigmavirus24
3387,2016-07-05 16:41:43,"Yeah, so the question boils down to whether we handle non-string headers. Clearly we've oscillated around for a while: they didn't work, and then they did, and now they don't again.

However, what I failed to note is Kenneth's original response in #865. With that in mind, I'm inclined to want to continue to respect his wishes and say that non-string headers are not acceptable.

Sorry for having you do this work @nateprewitt!
",Lukasa,nateprewitt
3386,2016-07-05 15:07:50,"@wut0n9 This behavioural change is not in v2.10.0, it's in the current master branch.

However, that's a real bug: #3366 has regressed this. @nateprewitt, are you interested in trying to update with a fix for this?
",Lukasa,wut0n9
3386,2016-07-05 15:13:49,"Yeah, I'll get right on this @Lukasa.
",nateprewitt,Lukasa
3386,2016-07-05 16:42:22,"@sigmavirus24 has linked to the relevant earlier opinions, which suggest that actually we don't allow non-string header values. So that means this is not a bug: it was us making a revision that is within the scope of the API definition.
",Lukasa,sigmavirus24
3386,2016-07-05 16:43:14,"@Lukasa right, I'm frankly surprised this didn't break earlier. The meaning of non-bytes/str as a header value is undefined as far as I'm concerned.
",sigmavirus24,Lukasa
3385,2016-07-05 09:01:30,"@Lukasa is `reason.decode('utf-8', 'ignore')` ok?
",gugu,Lukasa
3385,2016-07-05 09:04:42,"@Lukasa done, updated PR
",gugu,Lukasa
3385,2016-07-05 09:33:51,"@Lukasa done
",gugu,Lukasa
3385,2016-07-05 13:35:22,"@gugu just use `u''`. Further, stop importing six. It's not a dependency of requests.
",sigmavirus24,gugu
3385,2016-07-05 13:45:08,"@sigmavirus24 @lucasa done, updated PR
",gugu,sigmavirus24
3385,2016-07-05 14:01:17,"Looks good to me. @Lukasa, the changes are minimal from when you gave your LGTM, so I'm merging. :)
",sigmavirus24,Lukasa
3385,2016-07-05 14:02:00,"Thanks @gugu! :sparkles: :cake: :sparkles: :fireworks: 
",sigmavirus24,gugu
3382,2016-07-01 22:01:01,"Looks like I failed to set stream=True in my test case. I was misinterpreting the call to `r.content` as the stream read. Thanks @Lukasa, string updated.
",nateprewitt,Lukasa
3382,2016-07-02 18:10:18,"Cool, I'm happy with this! Thanks @nateprewitt! :sparkles: :cake: :sparkles:
",Lukasa,nateprewitt
3370,2016-07-02 20:56:18,":+1: Looks fine to me. Thanks @nateprewitt!
",sigmavirus24,nateprewitt
3368,2016-07-01 00:10:57,"Thanks @joyzheng! :cake: 
",sigmavirus24,joyzheng
3367,2016-06-30 07:29:08,"@toadzhou Does this actually have anything to do with the timeout? If you change the second timeout to `1`, does the error still occur?
",Lukasa,toadzhou
3367,2016-06-30 08:45:35,"@toadzhou Right now it's still not really clear what's happening in your connection. Can you run the script above with something like Wireshark and send me the packet capture?
",Lukasa,toadzhou
3367,2016-06-30 09:01:45,"@Lukasa 

#tcpdump -i eth0 -vnn 164.132.52.143
16:54:31.440888 IP (tos 0x0, ttl 64, id 52147, offset 0, flags [DF], proto TCP (6), length 44)
    x.x.x.x.62004 > 164.132.52.143.3128: Flags [S], cksum 0x4aa1 (correct), seq 1499401970, win 5840, options [mss 1460], length 0
16:54:31.749509 IP (tos 0x0, ttl 52, id 0, offset 0, flags [DF], proto TCP (6), length 44)
    164.132.52.143.3128 > x.x.x.x.62004: Flags [S.], cksum 0x203b (correct), seq 3958367012, ack 1499401971, win 29200, options [mss 1460], length 0
16:54:31.749549 IP (tos 0x0, ttl 64, id 52148, offset 0, flags [DF], proto TCP (6), length 40)
    x.x.x.x.62004 > 164.132.52.143.3128: Flags [.], cksum 0x9338 (correct), ack 1, win 5840, length 0
16:54:31.749663 IP (tos 0x0, ttl 64, id 52149, offset 0, flags [DF], proto TCP (6), length 80)
    x.x.x.x.62004 > 164.132.52.143.3128: Flags [P.], cksum 0xd039 (incorrect -> 0x5679), seq 1:41, ack 1, win 5840, length 40
16:54:32.057972 IP (tos 0x0, ttl 52, id 42614, offset 0, flags [DF], proto TCP (6), length 40)
    164.132.52.143.3128 > x.x.x.x.62004: Flags [.], cksum 0x37d0 (correct), ack 41, win 29200, length 0
16:54:32.058005 IP (tos 0x0, ttl 64, id 52150, offset 0, flags [DF], proto TCP (6), length 42)
    x.x.x.x.62004 > 164.132.52.143.3128: Flags [P.], cksum 0xd013 (incorrect -> 0x85fc), seq 41:43, ack 1, win 5840, length 2
16:54:32.366473 IP (tos 0x0, ttl 52, id 42615, offset 0, flags [DF], proto TCP (6), length 40)
    164.132.52.143.3128 > x.x.x.x.62004: Flags [.], cksum 0x37ce (correct), ack 43, win 29200, length 0
16:54:32.488958 IP (tos 0x0, ttl 52, id 42616, offset 0, flags [DF], proto TCP (6), length 79)
    164.132.52.143.3128 > x.x.x.x.62004: Flags [P.], cksum 0x2de6 (correct), seq 1:40, ack 43, win 29200, length 39
16:54:32.488986 IP (tos 0x0, ttl 64, id 52151, offset 0, flags [DF], proto TCP (6), length 40)
    x.x.x.x.62004 > 164.132.52.143.3128: Flags [.], cksum 0x92e7 (correct), ack 40, win 5840, length 0
16:54:32.490157 IP (tos 0x0, ttl 64, id 52152, offset 0, flags [DF], proto TCP (6), length 337)
    x.x.x.x.62004 > 164.132.52.143.3128: Flags [P.], cksum 0xd13a (incorrect -> 0xf979), seq 43:340, ack 40, win 5840, length 297
16:54:32.798673 IP (tos 0x0, ttl 52, id 42617, offset 0, flags [DF], proto TCP (6), length 40)
    164.132.52.143.3128 > x.x.x.x.62004: Flags [.], cksum 0x334e (correct), ack 340, win 30016, length 0
16:54:35.254479 IP (tos 0x0, ttl 52, id 42618, offset 0, flags [DF], proto TCP (6), length 41)
    164.132.52.143.3128 > x.x.x.x.62004: Flags [P.], cksum 0x1d45 (correct), seq 40:41, ack 340, win 30016, length 1
16:54:35.295080 IP (tos 0x0, ttl 64, id 52153, offset 0, flags [DF], proto TCP (6), length 40)
    x.x.x.x.62004 > 164.132.52.143.3128: Flags [.], cksum 0x91bd (correct), ack 41, win 5840, length 0
16:54:38.045490 IP (tos 0x0, ttl 52, id 42619, offset 0, flags [DF], proto TCP (6), length 41)
    164.132.52.143.3128 > x.x.x.x.62004: Flags [P.], cksum 0x3044 (correct), seq 41:42, ack 340, win 30016, length 1
16:54:38.045518 IP (tos 0x0, ttl 64, id 52154, offset 0, flags [DF], proto TCP (6), length 40)
    x.x.x.x.62004 > 164.132.52.143.3128: Flags [.], cksum 0x91bc (correct), ack 42, win 5840, length 0
16:54:40.817095 IP (tos 0x0, ttl 52, id 42620, offset 0, flags [DF], proto TCP (6), length 41)
    164.132.52.143.3128 > x.x.x.x.62004: Flags [P.], cksum 0x3043 (correct), seq 42:43, ack 340, win 30016, length 1
16:54:40.817125 IP (tos 0x0, ttl 64, id 52155, offset 0, flags [DF], proto TCP (6), length 40)
    x.x.x.x.62004 > 164.132.52.143.3128: Flags [.], cksum 0x91bb (correct), ack 43, win 5840, length 0
16:54:43.574452 IP (tos 0x0, ttl 52, id 42621, offset 0, flags [DF], proto TCP (6), length 41)
    164.132.52.143.3128 > x.x.x.x.62004: Flags [P.], cksum 0x2242 (correct), seq 43:44, ack 340, win 30016, length 1
16:54:43.574483 IP (tos 0x0, ttl 64, id 52156, offset 0, flags [DF], proto TCP (6), length 40)
    x.x.x.x.62004 > 164.132.52.143.3128: Flags [.], cksum 0x91ba (correct), ack 44, win 5840, length 0
16:54:46.349464 IP (tos 0x0, ttl 52, id 42622, offset 0, flags [DF], proto TCP (6), length 41)
    164.132.52.143.3128 > x.x.x.x.62004: Flags [P.], cksum 0xc140 (correct), seq 44:45, ack 340, win 30016, length 1
16:54:46.349495 IP (tos 0x0, ttl 64, id 52157, offset 0, flags [DF], proto TCP (6), length 40)
    x.x.x.x.62004 > 164.132.52.143.3128: Flags [.], cksum 0x91b9 (correct), ack 45, win 5840, length 0
16:54:48.247303 IP (tos 0x0, ttl 64, id 52158, offset 0, flags [DF], proto TCP (6), length 40)
    x.x.x.x.62004 > 164.132.52.143.3128: Flags [F.], cksum 0x91b8 (correct), seq 340, ack 45, win 5840, length 0
16:54:48.556321 IP (tos 0x0, ttl 52, id 42623, offset 0, flags [DF], proto TCP (6), length 40)
    164.132.52.143.3128 > x.x.x.x.62004: Flags [F.], cksum 0x3347 (correct), seq 45, ack 341, win 30016, length 0
16:54:48.556358 IP (tos 0x0, ttl 64, id 52159, offset 0, flags [DF], proto TCP (6), length 40)
    x.x.x.x.62004 > 164.132.52.143.3128: Flags [.], cksum 0x91b7 (correct), ack 46, win 5840, length 0
",toadzhou,Lukasa
3367,2016-06-30 09:28:24,"@toadzhou Is that log _with_ or _without_ Sessions?
",Lukasa,toadzhou
3367,2016-06-30 09:41:12,"@toadzhou The _result_ is the same, but what happens is not. To be clear, I'd like a packet capture where you're using `requests.get`, not `session.get`.
",Lukasa,toadzhou
3367,2016-06-30 10:10:28,"@toadzhou You don't need to apologise, I appreciate that English isn't your first language. =) I'm just trying to find a way to explain what I need from you as clearly as possible.

In this case, I think I need to understand exactly what happens when you run that code, and what you think should happen.
",Lukasa,toadzhou
3367,2016-09-06 00:01:54,"This has been inactive for well over a month. I'm going to close this for now @toadzhou since we're unable to determine what the problem is. If you can gather more information and provide it, we can reopen this if we determine there's a bug in requests.

Thanks for filing this bug!
",sigmavirus24,toadzhou
3366,2016-06-29 21:08:00,"I understand the reasoning behind putting it in adapters, but I do feel like it should be in the prepare_headers section instead, if possible. PreparedRequests should allow users to do stupid things, like this. No need to have saftey tape at that level, imo. 

I won't be surprised if @Lukasa disagrees :)
",kennethreitz,Lukasa
3366,2016-06-29 21:32:12,"If we're treating this as a security issue and trying to prevent injection on dynamically generated headers, I think @Lukasa's internal method allows flexibility while maintaining the smallest surface area for issues. If we're just looking to prevent unintentional foot shooting, prepare_headers seems ""cleaner"" to me.
",nateprewitt,Lukasa
3366,2016-06-30 07:21:54,"Much to @kennethreitz's surprise, I agree with him. =)

I'm worried that users who are unsuspectingly using our high-level APIs aren't subject to risk, but if you're going to drop down to monkeying with PreparedRequests then you should be able to do more-or-less what you like. =) 
",Lukasa,kennethreitz
3366,2016-07-01 14:59:28,"@Lukasa Alright, there's rev 1 for a utility function.
",nateprewitt,Lukasa
3366,2016-07-02 19:13:13,"Squashed and passing. Thanks for taking the time to work through this with me @Lukasa :)
",nateprewitt,Lukasa
3366,2016-07-02 19:32:28,"Thanks @nateprewitt! :sparkles: :cake: :sparkles:
",Lukasa,nateprewitt
3366,2016-07-03 18:39:21,"@Lukasa, stealing my emoji chain hahaha
",kennethreitz,Lukasa
3366,2016-08-11 20:10:45,"We _cannot_ depend on `str()`, its behaviours are consistently and completely unacceptable. The worst of them being the fact that, on Python 3, `str(b'hello') == ""b'hello'""`, meaning that anyone passing a bytestring header value on Python 3 would get their header value totally mangled, despite the fact that passing us a bytestring is actually the _simplest_ case from the perspective of our logic: we pass it through, after checking that no ASCII bytes for newlines or leading/trailing whitespace are present.

Unconditionally casting data to strings does not behave well in Python, and the biggest problem is that it leads to subtle, difficult-to-discover bugs. The bug you hit, @saper, fails loudly and clearly. It is easy to understand what the problem is. However, if we adjust to your proposed behaviour, the failure mode is that someone has passed us something like a `datetime`, leading to Requests sending a header that reads `Date: datetime(days=52, hours=23, minutes=12, seconds=0)`, which is a _clearly_ incorrect header value. That failure mode is much harder to debug: all that you see in your code is a 400 Client Error that needs to be chased down, requiring you to examine everything about your request.

Subtle failure modes are almost always bad. I appreciate that this is an inconvenience for you, but we never intended your use-case to work, and historically it did not. It was by accident that it became acceptable several releases ago, and we have now taken action to formally prevent that use-case from being allowed again.
",Lukasa,saper
3366,2016-08-11 20:31:04,"Thanks for the understanding @saper! =)
",Lukasa,saper
3365,2016-06-28 07:33:27,"Very nice @nateprewitt! Thanks! :sparkles: :cake: :sparkles:
",Lukasa,nateprewitt
3364,2016-06-27 17:00:22,"@coyotlinden Do you make the original requests call with `stream=True`?
",Lukasa,coyotlinden
3364,2016-06-27 18:52:39,"@coyotlinden And that `req` is the same as the one that is called `self.req` elsewhere? How big is the file? Is the URL publicly available?
",Lukasa,coyotlinden
3364,2016-06-27 20:30:53,"@coyotlinden What is `type(self.chunk_size)`?
",Lukasa,coyotlinden
3363,2016-06-25 10:46:50,"@MaxNoe The server does not server the intermediate certificate(s) like it should (browser cache them from other sites, so it may work)
Compare:





Certbot saves the chain to `chain.pem` and `fullchain.pem` (including the leaf cert).
",t-8ch,MaxNoe
3362,2016-06-28 19:21:05,"I think this looks good to me. @sigmavirus24 are you happy to merge this directly? I think I'd call the old behaviour a bug so I'd be fine with merging this for the next release.
",Lukasa,sigmavirus24
3359,2016-06-22 20:59:23,"@mikepelley The API is presently frozen so I don't think we'll be adding those three methods. Besides, `iter_text` likely wouldn't provide much extra value outside of calling `iter_content(decode_unicode=True)`.
",sigmavirus24,mikepelley
3359,2016-09-01 06:05:16,"I was reading through this and the related issues and PRs and I'd love to help out. However, I'm not sure what direction to head with it (or if it's something more suitable for the next version of `requests`).

Just so I understand correctly, we want to get it so `iter_content` and `text` both return Unicode strings. The original PR solved this by grabbing the `apparent_encoding`, which caused a problem when using streaming responses (`chardet.detect` consumes the entire stream).

I'm assuming [detecting the encoding incrementally](http://chardet.readthedocs.io/en/latest/usage.html#example-detecting-encoding-incrementally) will still lead to problems, as part of the stream will be consumed. Is this something that may be best solved via documentation, suggesting encoding is set before using `iter_content`? It seems like we'd need some sort of knowledge about the response, which may be hard to do without consuming it (or part of it).

Could we find some middle ground? For streaming responses, documentation calls out that encoding should be set. If it isn't, return bytes and let the users decode to their heart's desire (seemed like it worked well as a workaround). While nonstreaming responses can use the `apparent_encoding`, as originally suggested.

I know its a bit cold, but thoughts @Lukasa, @sigmavirus24?
",shellhead,Lukasa
3359,2016-09-01 06:05:16,"I was reading through this and the related issues and PRs and I'd love to help out. However, I'm not sure what direction to head with it (or if it's something more suitable for the next version of `requests`).

Just so I understand correctly, we want to get it so `iter_content` and `text` both return Unicode strings. The original PR solved this by grabbing the `apparent_encoding`, which caused a problem when using streaming responses (`chardet.detect` consumes the entire stream).

I'm assuming [detecting the encoding incrementally](http://chardet.readthedocs.io/en/latest/usage.html#example-detecting-encoding-incrementally) will still lead to problems, as part of the stream will be consumed. Is this something that may be best solved via documentation, suggesting encoding is set before using `iter_content`? It seems like we'd need some sort of knowledge about the response, which may be hard to do without consuming it (or part of it).

Could we find some middle ground? For streaming responses, documentation calls out that encoding should be set. If it isn't, return bytes and let the users decode to their heart's desire (seemed like it worked well as a workaround). While nonstreaming responses can use the `apparent_encoding`, as originally suggested.

I know its a bit cold, but thoughts @Lukasa, @sigmavirus24?
",shellhead,sigmavirus24
3359,2016-09-01 13:41:03,"I've been playing around with this since we reverted and I don't think that there's a nice way to avoid the exception. I'm +1 on that but I'm also thinking that an encoding param for `iter_content` that @Lukasa suggested in #3481 would be useful. It allows the user to avoid having to check and set `Response.encoding` on every Response that might be streamed.
",nateprewitt,Lukasa
3359,2016-09-11 02:23:12,"I know @sigmavirus24 brought up a concern about raising an exception if `encoding` isn't specified for the current major version. Would the suggested fix go against the 3.0 branch? @Lukasa, when you say `encoding`, I'm assuming you're referring to the response's encoding rather than the additional parameter on `iter_content`?
",shellhead,Lukasa
3359,2016-09-11 02:23:12,"I know @sigmavirus24 brought up a concern about raising an exception if `encoding` isn't specified for the current major version. Would the suggested fix go against the 3.0 branch? @Lukasa, when you say `encoding`, I'm assuming you're referring to the response's encoding rather than the additional parameter on `iter_content`?
",shellhead,sigmavirus24
3359,2016-09-23 20:03:07,"I think @shellhead's work in #3574 should have this addressed in 3.0.0.
",nateprewitt,shellhead
3358,2016-06-21 18:53:17,"Thanks @petedmarsh! :sparkles: :cake: :sparkles:
",Lukasa,petedmarsh
3355,2016-06-20 07:11:08,"@lufeihaidao HTTP query parameters are not typed: they are _all_ strings. There is no such thing as a boolean parameter in a query string. What do you think a boolean parameter is, in this context?
",Lukasa,lufeihaidao
3355,2016-06-20 07:17:02,"@Lukasa Sorry, in my app, `key2` is supposed to be a boolean field. And many frameworks' boolean validations only check whether the `key2` string is '1' or 'true',rather than 'True'. So when the client assembles params with 'True', it will not pass the server's boolean validator.
",lufeihaidao,Lukasa
3355,2016-06-20 07:25:51,"@lufeihaidao You could do that, or you could just pass the string `true` rather than `True`.


",Lukasa,lufeihaidao
3355,2016-06-20 07:44:04,"@Lukasa OK, thanks.
",lufeihaidao,Lukasa
3354,2016-07-26 03:13:46,"@Lukasa  OK. thank you .
",ljdawn,Lukasa
3353,2016-10-25 18:43:35,"I have a very similar problem (if not the same): an application is randomly hanging (every ~10 to 20 days, roughly) and I connected `gdb` to a hung session today. `py-list` shows



it just hangs here, apparently (I have no experience with `gdb` but when I disconnect and reconnect, `py-list` brings me to the same place above).

@FlxVctr : thanks for the tip but I do not use head requests (nor body content workflow)
",wsw70,FlxVctr
3353,2016-10-25 19:02:25,"@Lukasa : no, my program hung (without any traceback) and I was wondering what could have caused the hanging. I just did a `gdb python3 <pid of my script>` and the only valuable thing I got was the pitput of `py-list`. I just rebooted my machine (it is a RPi) because I needed the script to run but if there is something i can do with the code to make it more talkative please let me know.
",wsw70,Lukasa
3353,2016-10-25 19:16:44,"@wsw70 I should note that requests is not in that stack. 😉 
",Lukasa,wsw70
3341,2016-06-17 12:59:16,"@ikus060 You do not need to monkeypatch the default HTTPAdapter: just install the correct adapters in at the point where you construct the session.
",Lukasa,ikus060
3341,2016-06-17 13:11:36,"@ikus060 You're not monkeypatching, you're subclassing. This is the entirely expected way to interact with Transport Adapters, and it's done throughout the requests community, as you can [see here](https://github.com/sigmavirus24/requests-toolbelt/tree/master/requests_toolbelt/adapters).

It's not clear to me that a huge amount is gained here by defaulting this. A simpler override, if you're always using the same timeout, is just:



At this point we're arguing about saving you 5 LoC. I'm _open_ to having the `HTTPAdapter` have a timeout default on the object, certainly, but it's not an immediately obvious win.
",Lukasa,ikus060
3340,2016-06-18 02:25:28,"This is my first use github to solve issue.
@Lukasa 
Thank you! And my bug solved now.

I use the pyenv to run the python. and I uninstall the python, then install again. It's working!   So happy~~

By the way, I tried the second method firstly, but It not working.  Then I tried the first one.I didn't know the reason. Maybe I had already installed that packages. 

Finally, Thanks very much!
",Kingmaxwang,Lukasa
3339,2016-08-24 18:03:19,"Thanks for that @jseabold, it's helpful when people come back to tidy up when they no longer have the time to dedicate to the patch. All the best with whatever you're spending your time on, and I hope you hang around!
",Lukasa,jseabold
3339,2016-08-24 18:06:53,"Thanks for contributing to Requests @jseabold!
",nateprewitt,jseabold
3338,2016-11-08 07:25:27,"@davidsoncasey, no worries, thanks for taking the time to check back in! 😀

 @Lukasa may have more on this but I think moving [these tests](https://github.com/davidsoncasey/requests/blob/60e0349ce265378b127ae51e1853533b57eaac38/tests/test_requests.py#L1241-L1270) into a separate PR against master will be the most immediate benefit. This will show that the issue is currently fixed on master and allow us to close out #3066.

The work you did for `prepare_body` and `prepare_content_length` simplifies a lot of the logic and would be great for proposed/3.0.0. We'll need to merge master into the proposed/3.0.0 branch in this repo and then have you rebase your commits on top of it. I did some local testing and got this patch merged and working relatively pain free.

I'm also happy to help with any of the reshuffling of your commits if needed, just let us know.
",nateprewitt,davidsoncasey
3338,2016-12-02 20:16:25,"Hey @davidsoncasey, just checking back in. If you don't have any qualms with this, I'd like to open a PR rebasing 1a01007 and lines 1264-1270 in tests/test_requests.py (e5f0993) onto master next week. This will verify the work for #3066 is finished there and then we can revisit the `prepare_body` consolidation here whenever you're ready.",nateprewitt,davidsoncasey
3338,2017-02-10 17:14:06,"@davidsoncasey, checking in again. It looks like @kennethreitz may want to start cleaning some of these older PRs up. There's still a lot of useful stuff in here that didn't make it into master but should be in 3.0.0. Would you be interested in bringing this branch up to date and fixing a few things? If not, I'll tidy up your commits and wrap this up.",nateprewitt,davidsoncasey
3338,2017-02-21 17:21:00,"@nateprewitt feel free to go ahead and tidy up and get this into master. I haven't had a chance to work on this project recently, and it would take me a little to get back up to speed on it.",davidsoncasey,nateprewitt
3338,2017-02-21 17:24:54,"Thanks for checking back in @davidsoncasey, I'll move forward with rebasing this into a new PR then. All the best in whatever you're currently working on :)",nateprewitt,davidsoncasey
3337,2016-06-15 13:10:42,"@Lukasa Already installed those libs, alike, running with --upgrade tells `Requirement already up-to-date`
",PabloLefort,Lukasa
3337,2016-06-15 13:20:49,"@PabloLefort And your OpenSSL version, please?
",Lukasa,PabloLefort
3337,2016-06-15 13:21:21,"@Lukasa OpenSSl version: `OpenSSL 1.0.1e-fips 11 Feb 2013`
",PabloLefort,Lukasa
3337,2016-06-27 14:42:02,"@Lukasa So, after some investigation, installed `tcpdump` to see whats happening with the packets.
On my local env the connection was through `TLS`, but in the server first try to connect with `TLS` and fallback to `SSL`. This raise `EOF Exception`.
Going foward, there was some firewall closing every connection. I changed it and it works like a charm!

Thanks for all.
",PabloLefort,Lukasa
3300,2016-06-11 13:00:24,"@PegasusWang Python 3 should _not_ encode `str` to `bytes`. We cannot guess what encoding you might want, and guessing just leads to _really_ subtle bugs where you send a request you think is valid, get weird 400 errors, and then spend hours of your time and ours trying to work out why and it turns out it's because Requests defaulted to using UTF-8 and your server was expecting Latin-1. You, the user, are responsible for emitting bytes from your generator.

@maxibabyx Your sample code simply cannot be right, I'm afraid: or at least, it cannot be triggering the error you've provided. The branch of code you're in can only be entered in the following situation:
1. The value of your `data` argument does not provide the `__iter__` property, or it _does_ but is either a string, list, tuple, or dict from the perspective of `isinstance`.
2. The `files` keyword argument was also passed.

Your sample code does neither of these things, meaning that when I test it on my machine I do not encounter the bug you have encountered.

Can you confirm for me please that your sample code actually does reproduce your bug, ideally by providing a URL that is publicly reachable for me to test against myself?
",Lukasa,PegasusWang
3299,2016-06-09 18:03:38,"@Lukasa many thanks for the investigation!
",eriol,Lukasa
3295,2016-06-08 16:44:45,"This looks great. Thanks @Lukasa 
",sigmavirus24,Lukasa
3292,2016-06-07 14:19:10,"@hedgeliu I'm afraid not. you can try connecting via IP address directly, but if you don't have access to the machine it's highly likely that your hosting provider doesn't allow you to perform network access at all. You'd have to run the notebook locally.
",Lukasa,hedgeliu
3292,2016-06-07 15:04:04,"Sorry to raise again, Lukasa. I had tried in different places and laptops between Mainland china and Hongkong lots of time. But it still not work. Thus I think it is not the machine problem. @Lukasa 
",hedgeliu,Lukasa
3292,2016-06-07 15:26:26,"@hedgeliu Did you try running the notebook server on lots of different laptops, or just running the notebook inside the web page? Because the notebook server is actually where the code executes, not the web page.
",Lukasa,hedgeliu
3291,2016-06-06 20:36:17,"Hey @panda-34,

Thanks for pointing this out. This was something I hadn't seen before (or considered). Tthe redirect cache is meant to very simplistic. The default cache can be disabled like as described [here](https://github.com/kennethreitz/requests/pull/2095#issuecomment-45977320). You can then use a real cache system, like [cachecontrol](/ionrock/cachecontrol) to do this kind of work.
",sigmavirus24,panda-34
3291,2016-06-07 15:16:02,"@Lukasa I'm in agreement. Let's do that in 3.0
",sigmavirus24,Lukasa
3290,2016-06-07 15:20:57,"@therealjb86 I should have caught that last night. :) Sorry for not being able to be helpful sooner.
",sigmavirus24,therealjb86
3289,2016-06-12 10:58:53,"Ok I'm good with this, leaving it up to @sigmavirus24 to merge when he's happy.
",Lukasa,sigmavirus24
3289,2016-06-17 09:50:54,"@sigmavirus24 , ping ... ?
",jayvdb,sigmavirus24
3287,2016-06-06 23:09:47,"@jayvdb thanks for testing but there is something I don't get. Tests for `DIST=trusty+sid` seem to use Python 2.7.6, but Stretch has Python 2.7.11. Even the stable release, Jessie, has 2.7.9 with several backports (for example cPython >= 2.7.9 has ssl features backported from Python3).
",eriol,jayvdb
3287,2016-06-07 05:17:42,"Hi @eriol, the `DIST=trusty+sid` job in that build is mostly Travis' trusty, with selected packages added from sid.  Travis trusty reports that it is Python 2.7.6, whereas trusty ships with 2.7.5 as far as I can see, so I am a bit confused about that, but it isnt a job I am particularly focused on, as the same problems appear in the other jobs that have a cleaner virtual env at the beginning of the test sequence.

As a consequence of building the additional testing on Travis, I feel more confident that any distro version shipping Python 2.7.9+ is fine with the current `requests.packages` unbundling code.  I still have some more test scenarios to create, though.

So the only 'problem' may be that the requests 2.10 package in stretch / sid states it `Depends: python:any (>= 2.7.5-5~)` , which is why it can be installed onto these Travis trusty environments, and probably any other debian derivatives which are still on Python 2.7.5 - 2.7.8 (are there any?).

Arch Linux also uses the unbundling, but it appears to be only providing Python 3.5.
Contrary to what I said earlier, Fedora isnt using the unbundling fallbacks in `requests.packaging.__init__`, but they are still [using symlinks](https://pkgs.fedoraproject.org/cgit/rpms/python-requests.git/tree/python-requests.spec#n115), however they are Python 2.7.10+ also, so should be safe anyway.
",jayvdb,eriol
3287,2016-06-07 14:15:46,"@eriol, I've been able to provide a better test case for this, and it shows that the problem exists even in 2.7.11 and 3.3.

https://travis-ci.org/jayvdb/requests/builds/135867398 is just #3289 with a `.travis.yml` that shows various combinations all work well with the bundled version of `urllib3`.

https://travis-ci.org/jayvdb/requests/builds/135870520 is a [very simple](https://github.com/jayvdb/requests/commit/09ae4ad78e28087fd5e041f5ce0a7bc603cd6a04) change to the `.travis.yml` that emulates what the Debian package looks like, and shows that `SubjectAltNameWarning` stops occurring on all environments that are using pyopenssl.  As explained in the opening issue, under the covers it is more than just the warning that isnt happening.  A second copy of the modules are being created and configured for pyopenssl mode, and the actual `urllib3` doesnt get into pyopenssl mode.

Finally, https://travis-ci.org/jayvdb/requests/builds/135873388 is #3286 , which fixes the problem.
That patch isnt intended to be the final solution; it is a WIP until we figure out what should be merged (I was told in #2670 to PR early), intended mostly to show what does work.  I have very quickly looked at the pip approach, and it is doing roughly the same thing so it should work.  It does require closer coupling between pip/requests and urllib3, which my patch avoided, for better and for worse.  I am not pushing to have my patch, or any other similar patch, merged, pushing `requests` further down this 'support unbundling' rabbit hole further, if the maintainers don't feel it is appropriate.  The patch is there to prove the bug exists.

My next step is going to be to check what happens with symlinks like what Fedora is doing, to see if that is a way to beat the import machinery. (I'd be surprised if it didnt work, but this problem is full of surprises).
",jayvdb,eriol
3287,2016-06-17 10:58:32,"@jayvdb Thanks for the heads-up.  That was a quick fix which seemed to work for Debian.  I'll take a look at this issue in more detail when I get a chance. 
",warsaw,jayvdb
3287,2016-06-17 11:45:25,"Hi, i did non forgot this. Only busy during this week.

@warsaw my was plan is to use same approach of pip here. What do you think about?
",eriol,warsaw
3287,2016-06-17 12:14:26,"@eriol +1  I think pip has done the best job of making devendorizing easier for downstream redistributors.  My deltas are very small and now that it's been in place for a while, I haven't encountered any issues with it.  It's possible I'm missing something, so pinging @dstufft for additional thoughts.
",warsaw,eriol
3287,2016-06-17 12:38:42,"@jayvdb many thanks for #3289! I'm going to work on this in a few hours or at max tomorrow. Feel free to ping me again if you did not get a report from me tomorrow afternoon. But I hope to be able to work on this today.
",eriol,jayvdb
3287,2016-06-18 17:36:25,"@jayvdb fast report, working on unbundling stuff right now. Testing with your https://github.com/jayvdb/requests_issue_3287.
",eriol,jayvdb
3287,2016-06-18 21:41:34,"@dstufft many thanks for the detailed explanation!

I have just uploaded `requests 2.10.0-2` to unstable (still in upload queue, it'll show up shortly). I cherry picked the unbundling stuff from pip, [this is the patch](https://anonscm.debian.org/cgit/python-modules/packages/requests.git/commit/?h=patched/2.10.0-2&id=3311851f0cffea52cd779d01a6bf31cd8d34a37f) landed on Debian.

@jayvdb can you test again when `requests 2.10.0-2` will be in the archive? Thanks!
Also, what about renaming this bug in a more specific way? _Does not work_ seems to broad to me: we are addressing a problem using Python2 and related to SSL.

One more thing, I can bump the Python dependency to ensure a cPython2 with SSL feature from Python3, but only after we fix this. I was not aware of `trusty+sid` combo, I can understand the use, but mixing packages from different release seems dangerous to me.
",eriol,jayvdb
3287,2016-06-18 21:41:34,"@dstufft many thanks for the detailed explanation!

I have just uploaded `requests 2.10.0-2` to unstable (still in upload queue, it'll show up shortly). I cherry picked the unbundling stuff from pip, [this is the patch](https://anonscm.debian.org/cgit/python-modules/packages/requests.git/commit/?h=patched/2.10.0-2&id=3311851f0cffea52cd779d01a6bf31cd8d34a37f) landed on Debian.

@jayvdb can you test again when `requests 2.10.0-2` will be in the archive? Thanks!
Also, what about renaming this bug in a more specific way? _Does not work_ seems to broad to me: we are addressing a problem using Python2 and related to SSL.

One more thing, I can bump the Python dependency to ensure a cPython2 with SSL feature from Python3, but only after we fix this. I was not aware of `trusty+sid` combo, I can understand the use, but mixing packages from different release seems dangerous to me.
",eriol,dstufft
3286,2016-06-06 12:09:22,"So pip does [something similar](https://github.com/pypa/pip/blob/master/pip/_vendor/__init__.py#L79) but in a far more intelligent way than what is happening here.

That said, I don't think we have evidence that what pip is doing works either.

I also don't agree with your all or nothing mentality @jayvdb. It's not productive.
",sigmavirus24,jayvdb
3286,2016-06-06 22:52:58,"Hello,
sorry for the late reply, I was on trip and once returned I had to work on the backport of betamax for the stable release due the sheduled upload on OpenStack (I don't remember which one).

I definitively agree with @Lukasa here: the problem is on the Debian side.
So, as soon I complete the backport of betamax I will work on this.
This is my plan:
1. use the same pip's `vendored` function to patch requests.packages.**init**;
2. add some tests on Debian CI infrastructure over this specific issue;
3. upload this new revision to experimental suite;
4. make a call for test;

@jayvdb can you share your tests about OpenSSL & SNI support on Debian? Thanks!
",eriol,jayvdb
3286,2016-06-06 22:52:58,"Hello,
sorry for the late reply, I was on trip and once returned I had to work on the backport of betamax for the stable release due the sheduled upload on OpenStack (I don't remember which one).

I definitively agree with @Lukasa here: the problem is on the Debian side.
So, as soon I complete the backport of betamax I will work on this.
This is my plan:
1. use the same pip's `vendored` function to patch requests.packages.**init**;
2. add some tests on Debian CI infrastructure over this specific issue;
3. upload this new revision to experimental suite;
4. make a call for test;

@jayvdb can you share your tests about OpenSSL & SNI support on Debian? Thanks!
",eriol,Lukasa
3286,2016-06-06 23:04:03,"@jayvdb never mind, you were talking about https://github.com/kennethreitz/requests/issues/3287.
",eriol,jayvdb
3286,2016-06-06 23:43:22,"I agree with @Lukasa — this part of the codebase is absolutely abhorrent and should not even exist — however, we have chosen to do so to improve the user experience of our very unfortunate distro-installed users. 

We've already done more than I think we should have. Doing even more, without extremely strong beyond-a-reasonable-doubt purpose, is out of the question.
",kennethreitz,Lukasa
3250,2016-06-01 15:48:56,"@Lukasa to be clear we'd still be able to use the constant we import from `requests.models` that also appropriately helps sphinx determine what is happening.

I am not particularly opinionated about this I guess.
",sigmavirus24,Lukasa
3250,2016-06-01 15:59:29,"@Lukasa that works for me.
",sigmavirus24,Lukasa
3250,2016-06-09 04:49:21,"@Lukasa that was going to be my suggestion. Utilizing class-instance variables just for the doc build is a huge red flag for me. 
",kennethreitz,Lukasa
3236,2016-06-21 02:18:40,"@eriol I'm so sorry. I think I lost track of this while I was travelling to PyCon. I expect the same happened to @Lukasa 
",sigmavirus24,eriol
3223,2016-06-23 00:16:01,"@AlexPHorta I appreciate that but @shreepads is using 2.10.0 on Fedora 23
",sigmavirus24,shreepads
3223,2016-06-23 00:16:01,"@AlexPHorta I appreciate that but @shreepads is using 2.10.0 on Fedora 23
",sigmavirus24,AlexPHorta
3221,2016-05-26 13:27:29,"@Lukasa make sense!! thanks =)
",yangbeom,Lukasa
3217,2016-05-24 07:00:54,"Hey @alanhamlett, thanks for reporting this!

This will be fixed when we release the next minor version of requests, which will bring in an updated version of urllib3.
",Lukasa,alanhamlett
3213,2016-07-13 20:54:54,"@sigmavirus24 as @cournape mentions you can't time it like that; try using `time` on the command line. On my machine (ran each ~6-10 times to try to get a reliable average) for user+sys times (including sys because if there's any strange calls made into the kernel as a result of the import, that should be counted):
- `time python -c """"` (CPy 2.7.11) = 130 ms (time for Python to start up)
- `time python -c ""import requests""` (2.10.0) = 240 ms (above + importing requests)
- `time python -c ""import urllib3""` (1.15.1) = 210 ms (installed separately)

So `urllib3` takes about 80 ms, then about 30 ms more for requests stuff.
",nicktimko,cournape
3213,2016-07-13 20:54:54,"@sigmavirus24 as @cournape mentions you can't time it like that; try using `time` on the command line. On my machine (ran each ~6-10 times to try to get a reliable average) for user+sys times (including sys because if there's any strange calls made into the kernel as a result of the import, that should be counted):
- `time python -c """"` (CPy 2.7.11) = 130 ms (time for Python to start up)
- `time python -c ""import requests""` (2.10.0) = 240 ms (above + importing requests)
- `time python -c ""import urllib3""` (1.15.1) = 210 ms (installed separately)

So `urllib3` takes about 80 ms, then about 30 ms more for requests stuff.
",nicktimko,sigmavirus24
3213,2016-07-13 22:16:57,"@kennethreitz python does not take 120 ms to start, unless you are on a seriously broken environment. It is much closer to 20 ms on decently modern hw (< 5 years), i.e. importing requests means 3x the cost of starting python.

FWIW, on my 2011 Desktop PC (Debian):



On my 2014 macbook (OS X)



A simple `hg` (for help) on my macbook takes ~ 100 ms
",cournape,kennethreitz
3213,2017-01-13 07:21:27,@dsully How would you propose to expose the API for an opt-out?,Lukasa,dsully
3213,2017-01-13 16:27:41,"@Lukasa Unless I'm mistaken, the `.contrib.pyopenssl` ssl wrapper & context is not needed on Python 2.7.9+ and 3.4+. Given that, `requests` is always using pyopenssl when it is installed, even when the core Python libs support SNI, etc. So, changing the import to be:



Less of an opt-out, more of a not-needed.

Now, if there are cases where the pyopenssl SSLContext wrapper is desired for some reason (?) even when the core libraries are sufficient, then I'll make another pass. What do you think?",dsully,Lukasa
3213,2017-01-13 16:29:47,"@dsully you said:

> pkg_resources takes up a lot of time in general.

Where is Requests using pkg_resources? I don't believe either Requests or Urllib3 uses it.

> Now, if there are cases where the pyopenssl SSLContext wrapper is desired for some reason (?) even when the core libraries are sufficient

If I remember correctly, only the more recent versions of 2.7 are actually appropriate. I think there were some minor issues with 2.7.9 and 2.7.10.",sigmavirus24,dsully
3213,2017-01-13 16:32:29,"@sigmavirus24 Indirectly via `cryptography`:



Imported via:

",dsully,sigmavirus24
3213,2017-01-13 16:34:12,"@Lukasa Right.. forgot about that. I don't have that particular issue on macOS, but most people do.

I'll look at coming up with a way to explicitly opt-out then.",dsully,Lukasa
3213,2017-01-13 16:37:39,"@Lukasa - yes and no. Our build environment has real dependency management for modules (https://engineering.linkedin.com/blog/2016/08/introducing--py-gradle--an-open-source-python-plugin-for-gradle), which means just because someone included PyOpenSSL in their dependency tree, doesn't mean that the code you are importing for your upstream uses it. Lots of code gets installed transitively, but not imported.",dsully,Lukasa
3213,2017-01-13 17:12:59,"@Lukasa - without moving the location of the pyopenssl loader, currently in `requests/__init__.py`, would an environment variable be acceptable?",dsully,Lukasa
3213,2017-01-13 17:42:03,"@sigmavirus24 I hear you there. If environment changes aren't ok, an explicit call? I'd have to move the current injection, since it happens in `__init__.py`",dsully,sigmavirus24
3213,2017-01-13 18:22:47,"@dsully so I think what we need to balance is: ""better security for most people"" versus ""avoiding performance penalties for a smaller group of people"".

If I understand your situation correctly, you have recent enough versions of OpenSSL and Python, yes? In that case a version check against the `OPENSSL_VERSION` in `ssl` would be good enough for your needs, right? Is there a reason not to try to check against that (or perhaps a more friendly attribute that isn't a string)?",sigmavirus24,dsully
3213,2017-01-13 18:25:05,@sigmavirus24 I think so.. but I don't follow 100%. Do you mean checking the OPENSSL_VERSION to avoid the pyopenssl injection?,dsully,sigmavirus24
3213,2017-01-13 18:27:23,"@sigmavirus24 And yes, you are correct - we run Python 2.7.11 and Python 3.5 (soon to be 3.6), both compiled against a non-system shipped OpenSSL.$latest. Our build system (PyGradle) also sets build time `CPPFLAGS` and `LDFLAGS` to have modules like `cryptography` and `PyOpenSSL` link to the non-system OpenSSL as well.",dsully,sigmavirus24
3213,2017-01-13 19:19:30,"
>@sigmavirus24 I think so.. but I don't follow 100%. Do you mean
>checking the OPENSSL_VERSION to avoid the pyopenssl injection?

That's exactly what I failed to communicate. :)

-- 
Sent from my Android device with K-9 Mail. Please excuse my brevity.
",sigmavirus24,sigmavirus24
3213,2017-01-13 20:11:53,"On January 13, 2017 1:40:44 PM CST, Dan Sully <notifications@github.com> wrote:
>Ok, so:
>
>
>
>What is the minimum OpenSSL version for the required functionality?
>1.0.1?

I think 1.0.1 is a good minimum but would rather defer to @Lukasa and @reaperhulk on that.

-- 
Sent from my Android device with K-9 Mail. Please excuse my brevity.
",sigmavirus24,Lukasa
3213,2017-01-13 20:24:37,"@Lukasa - It should be set to whatever pyOpenSSL's minimum is for the functionality that is required.

0.9.8 was dropped in pyOpenSSL 16.1.0:

https://pyopenssl.readthedocs.io/en/stable/changelog.html

So 0.9.9 would be the bare minimum, but that's hard to recommend. I still say 1.0.1 or perhaps 1.0.0",dsully,Lukasa
3212,2016-05-23 13:52:13,"@pensnarik Can you try running `pip install -U requests[security]` in your environment and then try again?
",Lukasa,pensnarik
3212,2016-07-04 15:29:59,"@sProject You would need to resolve both of those warnings before we can determine if you have a new problem. If you got requests from pip, run `pip install pyopenssl pyasn1 ndg-httpsclient`. If you got Requests from your system provider, you should install those three packages from there.
",Lukasa,sProject
3212,2016-11-16 14:21:30,"The example from [pygodaddy](https://pypi.python.org/pypi/pygodaddy) raises the same exception



Downgrading certifi as suggested by @pensnarik does not fix it. This is Fedora 24, Python 2.7.12.


",ClodoaldoPinto,pensnarik
3212,2017-01-03 12:50:37,"@Lukasa how can one check if the problem is with requests or certificate issuer?
I have one cert issued by Entrust and my browsers are quite ok with it when I browse to the URL.
But when I try to get to that URL via requests I have `[SSL: CERTIFICATE_VERIFY_FAILED]`

**Full traceback**


**installed packages**


**Cert details**
",hellt,Lukasa
3212,2017-01-03 14:06:01,"Thanks @Lukasa for this educational reply you gave! Really appreciate and sorry for troubling with this, it was my TLS-knowledge-gap =) Mostly I went to issues because in was all good in browsers and didn't work in requests. But now its clear to me what is the root cause.

I believe there is no workaround (download intermediate cert upfront) for this case, but to reconfigure the server? For the time being I will skip verification checks",hellt,Lukasa
3212,2017-01-03 14:09:02,"@hellt No need to apologise: this is a thing that is extremely non-obvious and it's a *really* common error to make, so you're in good company here. =)

There is a workaround, in fact. You can pass a certificate bundle to `verify` to override the use of certifi's bundle. This bundle can, in addition to root certs, contain intermediary certificates that become available to OpenSSL to use to build the chain. As a result, what you can do is take the certifi cert chain (which is in a file you can find by running `python -c 'import certifi; print(certifi.where())'`, copy that out to somewhere else on your filesystem, and then add the intermediate certificate to the end of that file. The intermediate has to be in PEM format. If then pass the path to the new cert bundle to the `verify` kwarg, you'll find everything starts working.",Lukasa,hellt
3199,2016-05-19 18:24:08,"@zachmullen short of rewriting our HTTP stack to not use httplib, no. And none of us presently have the time to do that. :-(
",sigmavirus24,zachmullen
3199,2016-05-19 19:46:29,"(Closing for @Lukasa since he said he was going to and didn't. ;))
",sigmavirus24,Lukasa
3195,2016-05-16 10:41:13,"@Lukasa added tests, please check and merge.
",kumarvaradarajulu,Lukasa
3195,2016-05-16 10:54:28,"@Lukasa addressed comments, pls check
",kumarvaradarajulu,Lukasa
3195,2016-05-16 12:47:30,"Looks good to me. Thanks @kumarvaradarajulu 
",sigmavirus24,kumarvaradarajulu
3192,2016-05-17 15:45:25,"Thanks @brettdh!
",sigmavirus24,brettdh
3189,2016-05-12 20:33:26,"@degroat Yup, that'll do it too. =)
",Lukasa,degroat
3189,2016-05-30 16:51:24,"@goalong It's possible, but it depends on the specifics of your situation. In this instance, all we know is that the server you're connecting to doesn't like your TLS handshake. That can happen for lots of reasons, but without specifics it's hard to know what would fix the problem. 
",Lukasa,goalong
3189,2017-01-19 20:33:18,@Bashar That specific list is now out of date. You should make sure you use `pip install requests[security]` instead: that will always install the correct dependencies.,Lukasa,Bashar
3187,2016-05-12 08:38:59,"@cnicodeme The requests project does not consider GAE formally supported. Put another way, we don't have any special-case code _in requests_ to use GAE properly. The recommended method of using GoogleAppEngine with requests is to use the [requests-toolbelt `AppEngineAdapter`](http://toolbelt.readthedocs.io/en/latest/adapters.html#appengineadapter).
",Lukasa,cnicodeme
3185,2016-05-12 21:05:00,"@brettdh Yeah, I think that would be fine. =)
",Lukasa,brettdh
3185,2016-05-13 17:09:42,"@sigmavirus24 The test server refactoring was mainly done as I added a failing functional test for the feature, in order to prevent that test failure from hanging the server (because it was waiting for a connection that wasn't coming).

Now that the test is passing, I'm happy to separate the test server refactoring into a separate PR if that helps.
",brettdh,sigmavirus24
3185,2016-05-13 19:43:47,"Cool, @sigmavirus24 do you want to re-review?
",Lukasa,sigmavirus24
3185,2016-05-16 21:31:50,"@sigmavirus24 Yeah, sorry; I forgot that I had vim configured to always trim trailing whitespace on save. Thought I had reverted those before committing, but clearly I missed one. :-)
",brettdh,sigmavirus24
3185,2016-05-16 21:56:33,"@brettdh no need to put trailing whitespace back, just change that line to use iter instead =D
",sigmavirus24,brettdh
3185,2016-05-17 07:23:06,"Ok, :+1: from me. @sigmavirus24?
",Lukasa,sigmavirus24
3185,2016-05-17 15:42:29,"Looks good to me. Thanks @brettdh!
",sigmavirus24,brettdh
3184,2016-05-11 13:55:01,"@Lukasa in that case, could we get rid of the first elif statement and just go to the case where `body is not None`? Looking at `super_len`, I think it would calculate length in the same way. So in what I'm proposing, prepare content length would look like this:



All tests pass with this change.
",davidsoncasey,Lukasa
3184,2016-05-11 18:44:36,"@sigmavirus24 Arg, good spot. Hrm. We need to have some extra logic here to get this right.
",Lukasa,sigmavirus24
3184,2016-05-11 19:18:09,"@Lukasa @sigmavirus24 it looks to me like the try/except of using `super_len` to calculate the length of the body could be moved to `prepare_content_length` - and then both the `Content-Length` and `Transfer-Encoding` headers would be set there. If content length can be calculated, then we strip the TE header, and otherwise, we strip the CL header. I think this would be the most clear, and would ensure that the headers are mutually exclusive, regardless of what a user enters.
",davidsoncasey,Lukasa
3184,2016-05-11 19:18:09,"@Lukasa @sigmavirus24 it looks to me like the try/except of using `super_len` to calculate the length of the body could be moved to `prepare_content_length` - and then both the `Content-Length` and `Transfer-Encoding` headers would be set there. If content length can be calculated, then we strip the TE header, and otherwise, we strip the CL header. I think this would be the most clear, and would ensure that the headers are mutually exclusive, regardless of what a user enters.
",davidsoncasey,sigmavirus24
3184,2016-05-11 19:41:22,"@davidsoncasey I think we want to keep the `try...except` in `super_len`: that method gets used outside requests quite frequently. It won't hurt to do the check twice.
",Lukasa,davidsoncasey
3184,2016-05-12 00:02:47,"@Lukasa sounds good. I'll see if I can put something together for that. Thanks for helping work through this.
",davidsoncasey,Lukasa
3184,2016-05-17 16:36:51,"@Lukasa @sigmavirus24 I updated this PR to make the Content-Length and Transfer-Encoding headers mutually exclusive, regardless of whether a user manually provides a value, as @sigmavirus24 showed in his comment. While working on this, I stumbled upon #1648. I hadn't realized that this had been a subject of so much discussion, and that people have differing opinions about how this should work. This is obviously related to that issue, so I understand if you decide that this isn't the fix that you're looking for. It could be refactored to raise an error if a user has provided a CL header when TE is set, and vice versa, instead of silently removing the unneeded header. Let me know what you think.
",davidsoncasey,Lukasa
3184,2016-05-17 16:36:51,"@Lukasa @sigmavirus24 I updated this PR to make the Content-Length and Transfer-Encoding headers mutually exclusive, regardless of whether a user manually provides a value, as @sigmavirus24 showed in his comment. While working on this, I stumbled upon #1648. I hadn't realized that this had been a subject of so much discussion, and that people have differing opinions about how this should work. This is obviously related to that issue, so I understand if you decide that this isn't the fix that you're looking for. It could be refactored to raise an error if a user has provided a CL header when TE is set, and vice versa, instead of silently removing the unneeded header. Let me know what you think.
",davidsoncasey,sigmavirus24
3184,2016-05-18 15:31:21,"@Lukasa @sigmavirus24 I'll go ahead and alter it to raise an exception for now (perhaps `InvalidHeadersError`? Or is there an existing exception that would make sense to raise?). And then we can leave this PR open until it's decided what the best behavior is.

Also, are you planning on coming to Portland for PyCon? I didn't get a ticket in time, but I live in Portland and it would be great to meet up and discuss the project.
",davidsoncasey,Lukasa
3184,2016-05-18 15:31:21,"@Lukasa @sigmavirus24 I'll go ahead and alter it to raise an exception for now (perhaps `InvalidHeadersError`? Or is there an existing exception that would make sense to raise?). And then we can leave this PR open until it's decided what the best behavior is.

Also, are you planning on coming to Portland for PyCon? I didn't get a ticket in time, but I live in Portland and it would be great to meet up and discuss the project.
",davidsoncasey,sigmavirus24
3184,2016-05-22 20:45:24,"@Lukasa @sigmavirus24 I made a few more changes and a bit more refactoring of `prepare_body` and `prepare_content_length`. These include:
- Moving the logic set `Transfer-Encoding` header to the `prepare_content_length` method. This makes it more explicit that the headers should be mutually exclusive, and does not rely on implicitly connected logic in two methods.
- Raises an `InvalidHeaderError` if a user manually sets either header when it should not be set (let me know if you think that there's a preexisting exception that could be raised instead).
- Added tests to check different cases of when each header should be set.

I understand we may need to wait to decide if this is the desired behavior, so no need to review or merge this right away. We can leave this PR open until you've had a chance to discuss and agree upon the desired behavior.
",davidsoncasey,Lukasa
3184,2016-05-22 20:45:24,"@Lukasa @sigmavirus24 I made a few more changes and a bit more refactoring of `prepare_body` and `prepare_content_length`. These include:
- Moving the logic set `Transfer-Encoding` header to the `prepare_content_length` method. This makes it more explicit that the headers should be mutually exclusive, and does not rely on implicitly connected logic in two methods.
- Raises an `InvalidHeaderError` if a user manually sets either header when it should not be set (let me know if you think that there's a preexisting exception that could be raised instead).
- Added tests to check different cases of when each header should be set.

I understand we may need to wait to decide if this is the desired behavior, so no need to review or merge this right away. We can leave this PR open until you've had a chance to discuss and agree upon the desired behavior.
",davidsoncasey,sigmavirus24
3184,2016-05-24 02:12:15,"@sigmavirus24 @Lukasa sounds good. I'll make those small cleanup changes and squash extraneous commits.
",davidsoncasey,Lukasa
3184,2016-05-24 02:12:15,"@sigmavirus24 @Lukasa sounds good. I'll make those small cleanup changes and squash extraneous commits.
",davidsoncasey,sigmavirus24
3184,2016-06-09 16:45:56,"@Lukasa @sigmavirus24 alright, it took me a bit to get back to this, but I've cleaned it up a bit and it should be ready to merge into the proposed branch. The conlflict is in AUTHORS.rst. Let me know if there's anything else here you'd like to see changed, otherwise I'll try to find another issue to get started on.
",davidsoncasey,Lukasa
3184,2016-06-09 16:45:56,"@Lukasa @sigmavirus24 alright, it took me a bit to get back to this, but I've cleaned it up a bit and it should be ready to merge into the proposed branch. The conlflict is in AUTHORS.rst. Let me know if there's anything else here you'd like to see changed, otherwise I'll try to find another issue to get started on.
",davidsoncasey,sigmavirus24
3184,2016-06-09 17:51:46,"Thanks @davidsoncasey, I've left a few more notes.
",Lukasa,davidsoncasey
3184,2016-06-15 07:26:48,"Ok, good by me. @sigmavirus24?
",Lukasa,sigmavirus24
3184,2016-06-16 16:34:50,"@sigmavirus24 I'll open a new PR into proposed/3.0. As far as the conflict, that was only in the contributors.rst, and I was thinking it would be a little cleaner if one of you resolved it on merge, instead of back merging master into this branch and fixing it there. I'm happy to do that if you like though.
",davidsoncasey,sigmavirus24
3182,2016-05-09 13:56:25,"@Logmytech We did. Note our [changelog](http://docs.python-requests.org/en/master/community/updates/#id6), which says that we updated in 2.8.0, released 7 months ago. =)
",Lukasa,Logmytech
3181,2016-05-07 19:11:05,"This looks good @davidsoncasey, thanks so much! Do you want to open a parallel pull request with your idea about refactoring `prepare_body`? I'd love to see both side-by-side.
",Lukasa,davidsoncasey
3181,2016-05-09 16:11:16,"@Lukasa sounds good. I'll work on getting something put together in the next couple of days.
",davidsoncasey,Lukasa
3180,2016-05-05 13:09:58,"Hi @AstinCHOI,

This is a defect tracker, not a question and answer portion of the requests project. If you're looking for help (which it appears you are) please pose questions on [StackOverflow](https://stackoverflow.com/questions/tagged/python-requests). When posting there, please be prepared to answer questions such as:
- Why are you using a generator and then joining it into a single byte string in memory?
- Why does your traceback not include any code from the requests library?
",sigmavirus24,AstinCHOI
3180,2016-05-05 13:54:48,"Hi, @sigmavirus24 
1) http://stackoverflow.com/questions/11662960/ioerror-errno-22-invalid-argument-when-reading-writing-large-bytestring and it's my fault to use like that.
2) Sorry, I couldn't include the traceback with requests due to some logic.. but there is problem when you use data=data which is above 2GB size binary in OS X. If you use OSX, you can test it.

Naver mind. I will post in StackOverflow.

Thanks.
",AstinCHOI,sigmavirus24
3179,2016-05-05 13:26:21,"@Lukasa so we set it to `None` in [the descriptor](https://github.com/kennethreitz/requests/blob/master/requests/models.py#L728) in (2) [error](https://github.com/kennethreitz/requests/blob/master/requests/models.py#L739) [cases](https://github.com/kennethreitz/requests/blob/master/requests/models.py#L744). 

I don't think we can just change it to never be `None` before 3.0 but that's my hesitancy to break user-level code speaking.
",sigmavirus24,Lukasa
3179,2016-05-05 13:40:55,"@sigmavirus24 Yeah, that seems reasonable. In that case, clearly the `json()` code needs to handle it too.

That means I think this patch is fine. @messense are you interested in adding a test to validate that this works as expected?
",Lukasa,messense
3179,2016-05-05 13:40:55,"@sigmavirus24 Yeah, that seems reasonable. In that case, clearly the `json()` code needs to handle it too.

That means I think this patch is fine. @messense are you interested in adding a test to validate that this works as expected?
",Lukasa,sigmavirus24
3179,2016-05-06 07:47:57,"Hurrah, this looks great to me! I'd argue the previous behaviour was a bug so we have no API compatibility concerns to worry about here, but I want @sigmavirus24 to ACK that before merging.
",Lukasa,sigmavirus24
3178,2016-05-05 07:20:25,"@haikuginger Thanks for this! I've left some notes inline.
",Lukasa,haikuginger
3178,2016-05-22 15:51:06,"Sorry @haikuginger! GitHub doesn't ping me when you push new changes. :(
",sigmavirus24,haikuginger
3178,2016-05-22 15:51:41,"I'm :+1: on this if @Lukasa is =D
",sigmavirus24,Lukasa
3178,2016-05-22 16:02:02,"Let's do it! Thanks @haikuginger, you're doing awesome work! :sparkles: :cake: :sparkles:
",Lukasa,haikuginger
3176,2016-05-04 20:34:24,"I agree with @Lukasa. I don't think we need this as an extra parameter to this method. I also don't think we need to keep track of which headers we're sanitizing for a user. I also agree that we might be able to better serve you by refactoring things so you can override these with subclassing.
",sigmavirus24,Lukasa
3176,2016-05-05 08:31:07,"@sigmavirus24, @Lukasa thanks for the comments.
Inheriting `SessionRedirectMixin` was my first idea but then I've seen so much logic in the `resolve_redirects` function so it became a sort of code duplication.
By saying this, I agree with you guys, factoring out the logic might come very handy.
So how should we proceed?
",RonenHoffer,Lukasa
3176,2016-05-05 08:31:07,"@sigmavirus24, @Lukasa thanks for the comments.
Inheriting `SessionRedirectMixin` was my first idea but then I've seen so much logic in the `resolve_redirects` function so it became a sort of code duplication.
By saying this, I agree with you guys, factoring out the logic might come very handy.
So how should we proceed?
",RonenHoffer,sigmavirus24
3175,2016-05-03 18:49:06,"@kuraga Requests does not currently support doing this automatically. =) However, doing so is fairly simple. I don't think we'd accept a pull request adding this functionality (it's not likely to be generally useful), but we'd certainly be happy to add an example to our documentation.
",Lukasa,kuraga
3174,2016-05-03 19:33:20,"Actually, before we conclude this is a urllib3 bug, we need to have a discussion about what the API promises. So let me explain what the problem is.

urllib3 attempts to ensure that connections are thrown away if a problem occurs. This is implemented using a context manager in urllib3 (`_error_catcher`) that checks certain errors and then determines whether the block terminated cleanly. If it did not, it forcibly closes the connection before returning the connection object to the pool to be reopened.

That object treats _any_ exception as an error case that leaves the connection in an indeterminate state. That is mostly a good thing, except for one particularly awkward exception: [`GeneratorExit`](https://docs.python.org/2/library/exceptions.html#exceptions.GeneratorExit). This exception is raised when the `close()` method on a generator is called, and is thrown to allow things like `finally` blocks to execute properly.

_One_ case where this is called is when a generator is garbage collected. That happens in your code, because you call `iter_content` multiple times, once for each tag you search for. When that happens you spin up several generators (`iter_content` returns a generator, and `stream` returns a generator, and `read_chunked` also returns a generator, so there's a chain of at least three generators in this case). Because you don't save the return value from `iter_content`, that generator chain gets leaked. This causes the `read_chunked` generator to throw `GeneratorExit`, which causes the urllib3 `_error_catcher` to conclude that the connection was not left in a clean state and terminates it.

There are therefore a few questions:
1. Should `_error_catcher` consider `GeneratorExit` an error case, or special case it? I'm not sure: the question is whether the connection will get cleaned up properly in situations where the generator really is leaked. Currently my assumption is that it won't, and so `GeneratorExit` really is an error case.
2. Is it safe to open multiple versions of the generators `iter_content`, `stream`, and `read_chunked`? They don't make it clear. `iter_lines` in requests is clear that it is _not_ safe to do that, but the rest are left ambiguous. We need to make a call, where I suspect the answer boils down to whether `decode_content` is `True`: if it is, there is a state object that gets lost when that generator leaks. Given that requests essentially always sets `decode_content` to `True`, I think that means it's also not safe to repeatedly call `iter_content`.

@ducu In the short term, you can fix this by saving off the result of `iter_content` somewhere on your `Summary` object and then re-using that, rather than repeatedly re-calling that method. That logic will _definitely_ work, and we can work out whether or not the alternative _should_ work.

Can I get input from @kennethreitz, @shazow, and @sigmavirus24 on this please?
",Lukasa,ducu
3174,2016-05-04 07:54:32,"@ducu `iter_content` builds on top of `stream`. When I say `stream` here I don't mean the `stream` keyword argument as exposed by requests, I mean the `HTTPResponse.stream()` method exposed by urllib3. Sorry for the confusion there!

So I totally agree you want to load it chunk by chunk and process it iteratively. What I'm asking is whether we think this _is_ safe, and then whether it _should be_ safe:



Put another way, is it safe to repeatedly call `iter_content` and throw away the generator it returns (which is what your code does)?
",Lukasa,ducu
3174,2016-05-04 08:01:54,"Ah gottit, I'll try using a single generator then and see if that works.
Thanks for the clarification, brb

On Wed, May 4, 2016 at 10:55 AM Cory Benfield notifications@github.com
wrote:

> @ducu https://github.com/ducu iter_content builds on top of stream.
> When I say stream here I don't mean the stream keyword argument as
> exposed by requests, I mean the HTTPResponse.stream() method exposed by
> urllib3. Sorry for the confusion there!
> 
> So I totally agree you want to load it chunk by chunk and process it
> iteratively. What I'm asking is whether we think this _is_ safe, and then
> whether it _should be_ safe:
> 
> r = requests.get(some_url, stream=True)
> iter_one = r.iter_content()
> iter_two = r.iter_content()
> while True:
>     print next(iter_one)
>     print next(iter_two)
> 
> Put another way, is it safe to repeatedly call iter_content and throw
> away the generator it returns (which is what your code does)?
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/3174#issuecomment-216770410
",ducu,ducu
3174,2016-08-23 16:37:51,"@Lukasa if we decide to cache the iterators, it would. I'm not sure we should be caching the iterators though.
",sigmavirus24,Lukasa
3174,2016-08-23 20:10:53,"@sigmavirus24 You're right. I wouldn't expect your example to work, and I can see why the surprise with `iter_content()`, because it's a function, which one expects to have side effects. But it still seems valuable to have a way to hook into the stream at whatever point it's currently at. This was a feature, however undesirable, that Requests 2.9 had which Requests 2.10 does not have... and the workaround is clumsy, requiring code in two different points in the code to be coordinated.

It really would be nice if the response object exposed some sort of interface such that one doesn't have to carry both the iterator and the response around anytime streaming is in play.
",jaraco,sigmavirus24
3172,2016-05-03 07:07:20,"Thanks and  I got it @Lukasa 
",iliul,Lukasa
3172,2016-05-03 07:08:18,"The pull request #3173 should add the needed section. Thanks for the report @iliul! :sparkles:
",Lukasa,iliul
3171,2016-05-03 07:44:40,"Thanks for this @luv! :sparkles: :cake: :sparkles:
",Lukasa,luv
3170,2016-05-02 18:51:44,"@nahonnyate Please don't piggyback on unrelated issues. This is particularly problematic as you _already_ have an open issue (#3137) that has us asking questions of you that you have not yet answered.
",Lukasa,nahonnyate
3170,2016-05-03 15:56:32,"@Lukasa Sorry to miss my original thread, Let me respond the questions in my thread. Thanks for your help
",nahonnyate,Lukasa
3139,2016-04-29 16:05:26,"Thanks @markshannon! :tada: 
",sigmavirus24,markshannon
3137,2016-05-03 15:58:55,"@Lukasa  cert.pfx is the SSL certificate which I need to mention in my POST/GET along with basic auth.
@baptistapedro I tried both the option and it is throwing me the following error.

I am attaching the error I encountered in both the option.

Option # 1

> > > import requests
> > > from requests.auth import HTTPBasicAuth
> > > auth = HTTPBasicAuth('auth_user', 'auth_pass')
> > > requests.post('https://my-site.com/rest_service', cert=('/Users/my_user/Desktop/auth_user.pfx', '/path/key'),  auth=auth)
> > > Traceback (most recent call last):
> > >   File ""<stdin>"", line 1, in <module>
> > >   File ""/Users/my_user/anaconda/lib/python2.7/site-packages/requests/api.py"", line 88, in post
> > >     return request('post', url, data=data, *_kwargs)
> > >   File ""/Users/my_user/anaconda/lib/python2.7/site-packages/requests/api.py"", line 44, in request
> > >     return session.request(method=method, url=url, *_kwargs)
> > >   File ""/Users/my_user/anaconda/lib/python2.7/site-packages/requests/sessions.py"", line 456, in request
> > >     resp = self.send(prep, *_send_kwargs)
> > >   File ""/Users/my_user/anaconda/lib/python2.7/site-packages/requests/sessions.py"", line 559, in send
> > >     r = adapter.send(request, *_kwargs)
> > >   File ""/Users/my_user/anaconda/lib/python2.7/site-packages/requests/adapters.py"", line 382, in send
> > >     raise SSLError(e, request=request)
> > > requests.exceptions.SSLError: [SSL] PEM lib (_ssl.c:2580)

Option # 2

> > > r = requests.post(""https://my-site.com/rest_service"", verify='/Users/my_user/Desktop/auth_user.pfx', data={}, auth=('auth_user', 'auth_pass'))
> > > Traceback (most recent call last):
> > >   File ""<stdin>"", line 1, in <module>
> > >   File ""/Users/my_user/anaconda/lib/python2.7/site-packages/requests/api.py"", line 88, in post
> > >     return request('post', url, data=data, *_kwargs)
> > >   File ""/Users/my_user/anaconda/lib/python2.7/site-packages/requests/api.py"", line 44, in request
> > >     return session.request(method=method, url=url, *_kwargs)
> > >   File ""/Users/my_user/anaconda/lib/python2.7/site-packages/requests/sessions.py"", line 456, in request
> > >     resp = self.send(prep, *_send_kwargs)
> > >   File ""/Users/my_user/anaconda/lib/python2.7/site-packages/requests/sessions.py"", line 559, in send
> > >     r = adapter.send(request, *_kwargs)
> > >   File ""/Users/my_user/anaconda/lib/python2.7/site-packages/requests/adapters.py"", line 382, in send
> > >     raise SSLError(e, request=request)
> > > requests.exceptions.SSLError: unknown error (_ssl.c:2825)
",nahonnyate,baptistapedro
3137,2016-05-03 15:58:55,"@Lukasa  cert.pfx is the SSL certificate which I need to mention in my POST/GET along with basic auth.
@baptistapedro I tried both the option and it is throwing me the following error.

I am attaching the error I encountered in both the option.

Option # 1

> > > import requests
> > > from requests.auth import HTTPBasicAuth
> > > auth = HTTPBasicAuth('auth_user', 'auth_pass')
> > > requests.post('https://my-site.com/rest_service', cert=('/Users/my_user/Desktop/auth_user.pfx', '/path/key'),  auth=auth)
> > > Traceback (most recent call last):
> > >   File ""<stdin>"", line 1, in <module>
> > >   File ""/Users/my_user/anaconda/lib/python2.7/site-packages/requests/api.py"", line 88, in post
> > >     return request('post', url, data=data, *_kwargs)
> > >   File ""/Users/my_user/anaconda/lib/python2.7/site-packages/requests/api.py"", line 44, in request
> > >     return session.request(method=method, url=url, *_kwargs)
> > >   File ""/Users/my_user/anaconda/lib/python2.7/site-packages/requests/sessions.py"", line 456, in request
> > >     resp = self.send(prep, *_send_kwargs)
> > >   File ""/Users/my_user/anaconda/lib/python2.7/site-packages/requests/sessions.py"", line 559, in send
> > >     r = adapter.send(request, *_kwargs)
> > >   File ""/Users/my_user/anaconda/lib/python2.7/site-packages/requests/adapters.py"", line 382, in send
> > >     raise SSLError(e, request=request)
> > > requests.exceptions.SSLError: [SSL] PEM lib (_ssl.c:2580)

Option # 2

> > > r = requests.post(""https://my-site.com/rest_service"", verify='/Users/my_user/Desktop/auth_user.pfx', data={}, auth=('auth_user', 'auth_pass'))
> > > Traceback (most recent call last):
> > >   File ""<stdin>"", line 1, in <module>
> > >   File ""/Users/my_user/anaconda/lib/python2.7/site-packages/requests/api.py"", line 88, in post
> > >     return request('post', url, data=data, *_kwargs)
> > >   File ""/Users/my_user/anaconda/lib/python2.7/site-packages/requests/api.py"", line 44, in request
> > >     return session.request(method=method, url=url, *_kwargs)
> > >   File ""/Users/my_user/anaconda/lib/python2.7/site-packages/requests/sessions.py"", line 456, in request
> > >     resp = self.send(prep, *_send_kwargs)
> > >   File ""/Users/my_user/anaconda/lib/python2.7/site-packages/requests/sessions.py"", line 559, in send
> > >     r = adapter.send(request, *_kwargs)
> > >   File ""/Users/my_user/anaconda/lib/python2.7/site-packages/requests/adapters.py"", line 382, in send
> > >     raise SSLError(e, request=request)
> > > requests.exceptions.SSLError: unknown error (_ssl.c:2825)
",nahonnyate,Lukasa
3137,2016-05-03 16:09:37,"@nahonnyate My question was what is the _format_ of that certificate? How is it stored?

The reason I ask is that generally speaking a .pfx file is a Windows representation of a PKCS#12 format certificate. OpenSSL won't handle that format well, at least not as it's used here, so you'll need to convert it to PEM in order to use it. Find some instructions on [this page](https://www.sslshopper.com/article-most-common-openssl-commands.html).
",Lukasa,nahonnyate
3137,2016-05-03 17:59:52,"@Lukasa thanks for the referred page, I just did info on that SSL .pfx file and here is the output: if it helps.

pkcs12 -info -in '/Users/my_user/Desktop/auth_user.pfx'
Enter Import Password:
MAC Iteration 2000
MAC verified OK
PKCS7 Data
Shrouded Keybag: pb***_SHA1**_-Key*****_-**_**, Iteration 2000
Bag Attributes
    localKeyID: 0\* *\* 00 00 
    friendlyName: le-*****_-**_-***_-**__-_********
    Microsoft CSP Name: Microsoft Enhanced Cryptographic Provider v1.0
Key Attributes
    X509v3 Key Usage: 80 
",nahonnyate,Lukasa
3137,2016-05-03 18:21:03,"@Lukasa tried the following as you have suggested , not sure why it is complaining about the certificate now, I am using the same certificate (.pfx version) through SOAP UI in POST request and getting proper response back

## Converted .pfx file to .pem through this

openssl pkcs12 -in '/Users/auth_user/Desktop/cert.pfx' -out cert.pem -nodes

## use the same .pem in my POST

r = requests.post('https://my-site.com/rest_service', verify='/Users/auth_user/Desktop/cert.pem', data={}, auth=('auth_user', 'auth_pass'))
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/auth_user/anaconda/lib/python2.7/site-packages/requests/api.py"", line 88, in post
    return request('post', url, data=data, *_kwargs)
  File ""/Users/auth_user/anaconda/lib/python2.7/site-packages/requests/api.py"", line 44, in request
    return session.request(method=method, url=url, *_kwargs)
  File ""/Users/auth_user/anaconda/lib/python2.7/site-packages/requests/sessions.py"", line 456, in request
    resp = self.send(prep, *_send_kwargs)
  File ""/Users/auth_user/anaconda/lib/python2.7/site-packages/requests/sessions.py"", line 559, in send
    r = adapter.send(request, *_kwargs)
  File ""/Users/auth_user/anaconda/lib/python2.7/site-packages/requests/adapters.py"", line 382, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:590)
",nahonnyate,Lukasa
3137,2016-05-04 05:12:39,"@Lukasa I also tried with urllib2 with the following option and getting the same error as before:

---

import requests
import urllib2
import base64

chimpConfig = {
    ""headers"" : {
    ""Authorization"": ""Basic "" + base64.encodestring(""auth_user:auth_pass"").replace('\n', '')
    },
    ""cert"": ""/Users/user/Desktop/ssl_suth_cert.pem"",
    ""url"": 'https://url.com/'}

#perform authentication
datas = None
timeout = 2
cert = ""/Users/user/Desktop/ssl_suth_cert.pem""
request = urllib2.Request(chimpConfig[""url""], datas,  chimpConfig[""headers""])
result = urllib2.urlopen(request)

print ""Response:"",result
print result.code

---

it is giving me the following error
urllib2.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:590)>

now if I add ""cert"" parameter in my call I am getting the following error:
only change in the above code is: added cert param:
request = urllib2.Request(chimpConfig[""url""], datas, **cert,** chimpConfig[""headers""])

error:

Traceback (most recent call last):
  File ""urlpost.py"", line 16, in <module>
    request = urllib2.Request(chimpConfig[""url""], datas, cert, chimpConfig[""headers""])
  File ""/Users/summukhe/anaconda/lib/python2.7/urllib2.py"", line 238, in **init**
    for key, value in headers.items():
AttributeError: 'str' object has no attribute 'items'
",nahonnyate,Lukasa
3137,2016-05-04 17:55:20,"@Lukasa  , thanks for your direction, after analyzing the log it seems the url was having some '\n' character which was causing the later issue.
simple rstrip() worked and Verify= False was the key as you have mentioned before. 

One more question, how do I post message through this ? if I am adding another param as ""data"", this request is throwing error ""TypeError: post() got multiple values for keyword argument 'data'""

I really appreciate your help and the community to provide the support.

## POST started working

headers = {'content-type': 'application/json'}

> > > r = requests.post(url, headers, verify=False, cert='/Users/user/Desktop/auth_user.pem', auth=('user', 'pass'))
> > > r.content
> > > 'POST REQUEST PING'
> > > r.status_code
> > > 200
> > > ///
",nahonnyate,Lukasa
3137,2016-05-04 18:38:14,"@Lukasa 

## I think we are all clean now, able to post with data-file as well.

Thanks everyone for the help, we can close the issue

payload='/Users/user/Desktop/a.xml'
r = requests.post(url,auth=('auth_user', 'pass'), data=payload, verify=False, cert='/Users/user/Desktop/sinri2683c.pem',headers=headers)
",nahonnyate,Lukasa
3136,2016-04-28 10:43:10,"Thanks so much @Natim!
",Lukasa,Natim
3135,2016-04-26 19:18:40,"@evnm That reading of the code is not right. =)

You'll notice [here](https://github.com/kennethreitz/requests/blob/bbeb0001cdc657ac8c7fef98e154229bc392db0e/requests/cookies.py#L414) that we call `update` with the kwargs. That will override the path if you provide it. Try it: `create_cookie('key', 'value', path='/some/path')` should work just fine.
",Lukasa,evnm
3133,2016-04-26 17:52:59,"Thanks for the report @jribbens! :sparkles:
",Lukasa,jribbens
3131,2016-04-26 14:40:42,"@chipaca keep in mind that whatever adapter you're using will have to deal with older versions of requests and if it's not meant to touch the network should not be inheriting from the HTTPAdapter.
",sigmavirus24,chipaca
3131,2016-04-26 14:48:41,"@sigmavirus24 I'm using https://github.com/msabramo/requests-unixsocket/ which does inherit from `HTTPAdapter`. I'm not sure what you mean, though; where does ""touching the network"" become a factor?
",chipaca,sigmavirus24
3109,2016-04-21 20:59:01,"Thanks for this @jeremycline!

In principle this is fine. However, it changes the interface of the HTTPAdapter which is IMO part of our public interface, so I'd want to sit on this until 3.0.0. Happily, 3.0.0 shouldn't be too far away now so we can progress with this work. =)
",Lukasa,jeremycline
3109,2016-04-22 10:42:59,"@sigmavirus24 Because it sets things on the connection pool directly, which is Not Right. We want to get a different pool if those are different.
",Lukasa,sigmavirus24
3109,2016-09-06 00:11:02,"@jeremycline can you rebase once more? I promise we won't lose track of this again. =(
",sigmavirus24,jeremycline
3109,2016-09-06 13:35:32,"@sigmavirus24 no problem, rebased!
",jeremycline,sigmavirus24
3109,2016-09-06 13:58:45,"wait a second, @Lukasa can this not go into a pre-3.0 release?
",sigmavirus24,Lukasa
3109,2016-09-06 14:32:52,"@sigmavirus24 Nope, we removed cert_verify which is public on the HTTPAdapter.
",Lukasa,sigmavirus24
3109,2016-09-14 15:05:18,"@sigmavirus24 Should I add a 3.0 changelog entry or is that something you'd prefer to handle?
",jeremycline,sigmavirus24
3104,2016-04-18 18:52:43,"@fredthomsen Can you please check with 2.9.0? I believe we fixed this already.
",Lukasa,fredthomsen
3104,2016-04-19 06:19:23,"Determined a better way of reproducing the issue consistently. @Lukasa looks fine on 2.9.0.  Move along nothing to see here.
",fredthomsen,Lukasa
3104,2016-04-19 07:07:01,"Thanks for the report and for clearing it up @fredthomsen! :sparkles:
",Lukasa,fredthomsen
3104,2016-05-08 18:52:19,"@benkuhn The SSL error seems to be caused by a failure to deliver the response, so a server that sends a content-length but then stops sending halfway through the body (without closing the socket) should be enough to trigger this.
",Lukasa,benkuhn
3103,2016-04-18 13:42:50,"@Lukasa Thanks for pointing to the right direction.
",gastlygem,Lukasa
3100,2016-04-17 20:03:22,"@kennethreitz, @sigmavirus24 Thanks! 👍 
",hitstergtd,kennethreitz
3099,2016-04-16 04:00:21,"Hi @emgerner-msft,

For reference, the following are all variations on this theme if not this exact feature request:
- https://github.com/kennethreitz/requests/issues/2327
- https://github.com/kennethreitz/requests/issues/2685
- https://github.com/kennethreitz/requests/issues/1928
- (and I'm sure there are more)

We've also discussed this over on https://github.com/sigmavirus24/requests-toolbelt/issues/51

You'll notice the last link discusses [this package](https://github.com/pnpnpn/timeout-decorator) which should handle this for you without adding it to requests. The reality, is that there's no need for requests to do this when another package already does it very well.
",sigmavirus24,emgerner-msft
3099,2016-04-26 17:09:54,"@jribbens If you can come up with a way that uses neither threads nor processes, that would be amazing. Until then, if you want a wall clock timeout your best bet is that package as it's the most reliable way of achieving that at the moment.
",sigmavirus24,jribbens
3099,2016-04-26 17:25:20,"I don't think @jribbens is saying no threads nor processes. Just that a process _per_ web request is excessive. Many languages have a way of multiple timers sharing a single additional thread or process. I'm just not aware of how to do that best in Python. 

It seems like #1928 has the most discussion of alternatives, but most come with a lot of caveats (this won't work for your use case, etc). I'm fine with having some custom code in my library and writing my own custom solution if this really doesn't belong in requests but I think I need a little more information on what that would look like. The whole reason we use requests is to get away from the low level TCP connection pooling logic but it seems like reading that thread that in order to write this custom code I need to know that logic, and that's what I'm having some trouble with.
",emgerner-msft,jribbens
3099,2016-04-26 17:32:21,"@emgerner-msft is correct. I am bit confused by @sigmavirus24's comment, having a ""total timeout"" without using threads or processes seems quite pedestrian and not at all ""amazing"". Just calculate the deadline at the start of the whole process (e.g. `deadline = time.time() + total_timeout`) and then on any individual operation set the timeout to be `deadline - time.time()`.
",jribbens,emgerner-msft
3099,2016-04-26 17:32:21,"@emgerner-msft is correct. I am bit confused by @sigmavirus24's comment, having a ""total timeout"" without using threads or processes seems quite pedestrian and not at all ""amazing"". Just calculate the deadline at the start of the whole process (e.g. `deadline = time.time() + total_timeout`) and then on any individual operation set the timeout to be `deadline - time.time()`.
",jribbens,sigmavirus24
3099,2016-04-26 20:09:33,"> having a ""total timeout"" without using threads or processes seems quite pedestrian and not at all ""amazing"". 

And your solution is rather primitive. The reason _most_ people want a total (or wall clock) timeout is to prevent a read from ""hanging"", in other words a case like the following:



Where each read takes a long time in the middle of `iter_content` but it's less than the read timeout (I'm assuming we apply that when streaming, but it still may be the case that we don't) they specified. Certainly it would seem like this should be simply handled by your solution @jribbens until you remember how clocks drift and daylight savings time works and them `time.time()` is woefully insufficient.

Finally, it's important to keep in mind that Requests' API is frozen. There is no good or consistent API for specifying a total timeout. And if we implemented a timeout like you suggest, we would have countless bugs that they specified a one minute long total timeout but it took longer because the last time we checked we were under a minute but their configured read timeout was long enough that their timeout error was raised around a minute and a half. That's a _very_ rough wall timeout that would be slightly better for people looking for this, but no different from the person implementing this themselves.
",sigmavirus24,jribbens
3099,2016-04-27 10:27:50,"Apologies if I was unclear @sigmavirus24 , you seem to have critiqued my pseudocode illustration of principle as if you thought it was a literal patch. I should point out though that `time.time()` does not work the way you apparently think - daylight savings time is not relevant, and neither is clock skew on the timescales we're talking about here. Also you have misunderstood the suggestion if you think the bug you describe would occur. Finally I am not sure what you mean by the Requests API being ""frozen"" as the API was changed as recently as version 2.9.0 so clearly whatever you mean it's not what I would normally understand by the word.
",jribbens,sigmavirus24
3099,2016-04-27 20:36:39,"@emgerner-msft Assuming you're using CPython, connection shutdown will happen when the request is no longer continuing. At that point all references to the underlying connection will be lost and the socket will be closed and disposed of.
",Lukasa,emgerner-msft
3099,2016-04-27 20:39:08,"@Lukasa Okay, thanks! How does the library determine the request is no longer continuing? For example, if I used the timeout decorator route and cut off in the middle of the download, when would the download actually stop? Do I need to do anything special with the streaming options?
",emgerner-msft,Lukasa
3099,2016-04-27 21:02:40,"@emgerner-msft That's frustrating. =( 
",Lukasa,emgerner-msft
3099,2016-04-27 22:11:44,"@Lukasa Yup, tried the basic [usage snippet](https://github.com/pnpnpn/timeout-decorator#usage) and it doesn't work on Windows. I read some more of the code/examples and fiddled and it looks like if we don't use signals the package might work, but everything has to be pickable which is not the case for my application. So as far as I can tell, timeout decorator won't solve my problem. Any other ideas?
",emgerner-msft,Lukasa
3099,2016-04-28 07:12:57,"@emgerner-msft Are you confident that none of the Windows-specific signals are suitable?
",Lukasa,emgerner-msft
3099,2016-04-28 16:47:12,"@Lukasa To be blunt, I simply don't know. I haven't used signals before, and much like I didn't realize until you told me that they'd interrupt the request I'm not sure what's appropriate. I'm also not trying to get this just to work on Windows. I need full crossplat support (Windows and Unix) and both Python 2 and Python 3 support. So much of signals looks platform specific it's throwing me. [Timer](https://docs.python.org/2/library/threading.html#timer-objects) was one of the solutions I was looking at that looked less low level and thus might take care of my constraints, but I'm not sure then how I could close the connection. I can do more reading, but this is why I was hoping to get additional guidance from you guys. :)
",emgerner-msft,Lukasa
3099,2016-04-28 17:11:52,"@jribbens There are a few problems with this.

Part 1 is that the complexity of such a patch is very high. To get it to behave correctly you need to repeatedly change timeouts at the socket level. This means that the patch needs to be passed pervasively though httplib, which we've already patched more than we'd like to. Essentially, we'd need to be reaching into httplib and reimplementing about 50% of its more complex methods in order to achieve this functional change.

Part 2 is that the maintenance of such a patch is relatively burdensome. We'd likely need to start maintaining what amounts to a parallel fork of httplib (more properly http.client at this time) in order to successfully do it. Alternatively, we'd need to take on the maintenance burden of a different HTTP stack that is more amenable to this kind of change. This part is, I suspect, commonly missed by those who wish to have such a feature: the cost of implementing it is high, but that is _nothing_ compared to the ongoing maintenance costs of supporting such a feature on all platforms.

Part 3 is that the advantage of such a patch is unclear. It has been my experience that most people who want a total timeout patch are not thinking entirely clearly about what they want. In most cases, total timeout parameters end up having the effect of killing perfectly good requests for no reason.

For example, suppose you've designed a bit of code that downloads files, and you'd like to handle hangs. While it's initially tempting to want to set a flat total timeout (""no request may take more than 30 seconds!""), such a timeout misses the point. For example, if a file changes from being 30MB to being 30GB in size, such a file can _never_ download in that kind of time interval, even though the download may be entirely healthy.

Put another way, total timeouts are an attractive nuisance: they appear to solve a problem, but they don't do it effectively. A more useful approach, in my opinion, is to take advantage of the per-socket-action timeout, combined with `stream=True` and `iter_content`, and assign yourself timeouts for chunks of data. The way `iter_content` works, flow of control will be returned to your code in a somewhat regular interval. That means that you can set yourself socket-level timeouts (e.g. 5s) and then `iter_content` over fairly small chunks (e.g. 1KB of data) and be relatively confident that unless you're being actively attacked, no denial of service is possible here. If you're really worried about denial of service, set your socket-level timeout much lower and your chunk size smaller (0.5s and 512 bytes) to ensure that you're regularly having control flow handed back to you.

The upshot of all this is that I believe that total timeouts are a misfeature in a library like this one. The best kind of timeout is one that is tuned to allow large responses enough time to download in peace, and such a timeout is best served by socket-level timeouts and `iter_content`.
",Lukasa,jribbens
3099,2016-04-28 17:15:07,"Haha, I already know @zooba and @brettcannon. I can discuss with them here or internally as a solution to this would probably help them too.
",emgerner-msft,brettcannon
3099,2016-04-28 17:16:13,"@emgerner-msft I figured you might, but didn't want to presume: MSFT is a big organisation!
",Lukasa,emgerner-msft
3099,2016-04-28 17:21:52,"@Lukasa Just reading through the wall of text you just wrote above -- interesting! On the discussion of stream=True and iter_content to time downloads, what is the equivalent way of handling larger uploads?

_PS_: The paragraph above starting with 'Put another way,..' is the kind of guidance I looked for in the docs. Given the number of requests you get for maximum timeout (and your valid reasons for not doing it), maybe the best thing to do is add some of that information in the [timeout docs](http://docs.python-requests.org/en/master/user/advanced/#timeouts)?
",emgerner-msft,Lukasa
3099,2016-04-28 19:14:54,"@sigmavirus24 

> If a total timeout belongs anywhere, it would be there, but again, it would have to work on Windows, BSD, Linux, and OSX with excellent test coverage and without it being a nightmare to maintain.

Agreed!
",kennethreitz,sigmavirus24
3099,2016-04-29 15:25:52,"> I suppose my thinking is that not only do I want it, in fact nearly all users would want it if they thought about it (or they don't realise it's not already there).

@jribbens we have several years (over a decade if you combine the experiences of all three of us) of talking with and understanding our users needs. What has been necessary for almost all (at least 98%) users has been connection and read timeouts. We understand that a very vocal minority of our users want an overall timeout. Given what we can extrapolate to be the size of the group of potential users for that feature versus what is the potential size of users not needing that feature and the complexity of the maintenance and development of the feature, it's not really something we're going to do.

If you have anything _new_ to share, we'd like to hear that, but all you've said thus far is that in your opinion anything using requests without an overall timeout is buggy and I can imagine that there are a lot of users who would take offense at your assertion that their design decisions are buggy. So, please refrain from insulting the intelligence of our users.
",sigmavirus24,jribbens
3099,2016-04-29 17:04:57,"@sigmavirus24 Throughout this thread you have been needlessly condescending, inflammatory and rude, and I'm asking you politely, please stop.
",jribbens,sigmavirus24
3099,2016-05-04 18:05:23,"@Lukasa I looked in detail at your suggestions for how to do streaming upload and download and read the docs on these topics. If you could validate my assumptions/questions that would be great.
1. For streaming downloads if I use something like a read timeout '(e.g. 5s) and then iter_content over fairly small chunks (e.g. 1KB of data)', that means the requests library will apply the 5s timeout for each read of 1KB and timeout if it takes more than 5s. Correct?
2. For streaming uploads if I use a generator or file like object which returns chunks of data and I set the read timeout to 5s, the request library will apply the 5s timeout for each chunk I return and timeout if it takes longer. Correct?
3. If I don't use a generator for upload and simply pass bytes directly, how does the requests library decide to apply the read timeout I set? For example, if I pass a chunk of size 4MB and a read timeout of 5s, when exactly is that read timeout applied?
4. If I don't use iter_content and simply have requests download all of the content directly into the request with a read timeout of 5s, when exactly is that read timeout applied?

I have a general understanding of sockets/TCP protocol/etc but not exactly how urllib works with these concepts at a lower level or if requests does anything special besides passing the values down. I want to understand exactly how the timeouts are applied as simply getting the control flow back and applying my own timeout scheme doesn't work given the crossplat issues with terminating the thread. If there's additional reading material to answer my questions, feel free to refer me! In any case, this should hopefully be my last set of questions. :)

Thanks for your help so far.
",emgerner-msft,Lukasa
3099,2016-05-04 19:18:46,"@emgerner-msft Ok:
1. No. It's more complex than that, sadly. As discussed, each timeout applies _per socket call_, but we can't guarantee how many socket calls are in a given chunk. The quite complex reason for this is that the standard library wraps the backing socket in a buffer object (usually something like `io.BufferedReader`). That will make as many `recv_into` calls as it needs to make until it has provided enough data. That may be as few as zero (if there's enough data in the buffer already) or as many as exactly the number of bytes you've received if the remote peer is drip-feeding you one byte at a time. There is really very little we can do about that: due to the nature of a `read()` call against such a buffered object we don't even get flow of control back between each `recv_into` call.
   
   That means that the _only_ way to guarantee that you get no more than an n-second wait is to do `iter_content` with a chunk size of `1`. That's an absurdly inefficient way to download a file (spends far too much time in Python code), but it is the only way to obtain the guarantee you want.
2. I also believe the answer to that is no. We currently have no notion of a _send_ timeout. The way to get one is to use `socket.setdefaulttimeout`.
3. Read timeouts are applied only to reads, so it doesn't matter how you pass the body.
4. That read timeout suffers the same concerns as the `iter_content` case: if you have requests download everything then we'll end up emitting as many `recv_into` calls as needed to download the body, and the timeout applies to each one in turn.

You're bumping into the core problem here: requests just does not get close enough to the socket to achieve exactly what you're looking for. We _could_ add a send timeout: that's a feature request work considering, and it doesn't suffer the same problems as the read timeout does, but for everything else we're stuck because `httplib` insists (rightly) on swapping to a buffered socket representation, and then the rest of `httplib` uses that buffered representation.
",Lukasa,emgerner-msft
3099,2016-05-04 19:47:05,"@Lukasa 

Ah, what a mess, haha. I thought that might be the case but I was really hoping I was wrong. 

First, we desperately need a send timeout. I simply can't tell my users that their uploads can just hang infinitely and we don't have a plan to fix the problem. :/

It seems like I'm kind of in an impossible situation at this point. There's no library support for total timeout (which I do understand). There's no guarantees on exactly how the existing timeout works with various chunk sizes -- if there was, I could just sum up the time: connect timeout + read timeout \* chunk size. Being able to interrupt flow with stream mode and generators is nice, but since I don't have a solution to actually abort the threads in a cross platform manner this doesn't help either. Do you see other options to move forward? What are other users doing to solve these issues?
",emgerner-msft,Lukasa
3098,2016-10-24 03:11:21,"Content-Disposition is defined in RFC2183. It states that ""Short"" parameters should be US-ASCII. Long Parameters should be encoded as per RFC-2184. I can't see where in RFC-2184 it says that UTF-8 encoding is valid. (But I may be missing that particular line)

@Lukasa knows much more about the relevant RFCs than I do though.
",TetraEtc,Lukasa
3098,2016-10-24 07:41:44,"@fake-name I don't recall being puritanical about _anything_. Here is, word for word, what I said (literally quoting myself from this thread):

> The header parsing is done by httplib, in the Python standard library; that is the part that failed to parse. The failure to parse is understandable though: servers should not be shoving arbitrary bytes into headers.
> 
> Using UTF-8 for your headers is extremely unwise, as discussed by RFC 7230:
> 
> > Historically, HTTP has allowed field content with text in the ISO-8859-1 charset [ISO-8859-1], supporting other charsets only through use of [RFC2047] encoding. In practice, most HTTP header field values use only a subset of the US-ASCII charset [USASCII].
> 
> In this instance it's not really possible for us to resolve the problem. The server should instead be sending urlencoded URLs, or RFC 2047-encoded header fields. Either way, httplib is getting confused here, and we can't really step in and stop it.

Note the key part of this comment: ""Either way, httplib is getting confused here, and we can't really step in and stop it."".

This is what I mean when I say ""it's not really possible for us to resolve the problem"". The issue here is in a helper library that sits in the Python standard library. Changing the header parsing logic of that standard library module, while _possible_, is something that needs to be done as part of the standard CPython development process. Requests already carries more subclasses and monkeypatches to httplib than we're happy with, and we're strongly disinclined to carry more.

So here are the options for resolving this issue:
1. File a bug with the CPython development team, get the bug fixed and into Python 3.6 or 3.7, upgrade to that Python version.
2. Work out what the minimal possible invasive change is to httplib in order to allow parsing UTF-8 headers, propose that patch to us to see what we think of it.
3. Write a patch that removes Requests' requirement to use httplib at all so that the next time we have a problem that boils down to ""httplib is stupid"", we don't have to have this argument again.

Now, _you_ are welcome to pursue any of those options, but I've been to this rodeo a few times so I'm pursuing (3), which is the only one that actually makes this problem go away for good. Unfortunately, it turns out that replacing our low-level HTTP stack that we have spent 7 years integrating with takes quite a lot of work, and I can't just vomit out the code to fix this on demand.

To sum up: I didn't say I didn't think this was a problem or a bug, I said it was a problem that the Requests team couldn't fix, at least not on a timescale that was going to be helpful to this user. If you disagree, by all means, provide a patch to prove me wrong.

---

And let me make something clear. For the last 9 months or so I have been the most active Requests maintainer by a long margin. Requests is not all I do with my time. I maintain 15 other libraries and actively contribute to more. I have quite a lot of stuff I am supposed to be doing. So I have to _prioritise_ my bug fixing.

Trust me when I say that a bug where the effort required to fix it is extremely high and the flaw comes from a server that is emitting non-RFC-compliant output, that's not a bug that screams out ""must be fixed this second"". Any time there is a bug predicated on the notion that our peer isn't spec compliant that bug drops several spaces down my priority list. [Postel was wrong](https://tools.ietf.org/html/draft-thomson-postel-was-wrong-00).

Browsers are incentivised to support misbehaving servers because they are in a competitive environment, and users only blame them when things go wrong. If Chrome doesn't support ${CRAPPY_WEBSITE_X} then Chrome users will just go to a browser that does when they need access.

That's all fine and good, but the reason that Requests doesn't do this is because we have _two_ regular developers. That's it. There are only so many things two developers can do in a day. Neither of us work on just Requests. Compare this to Chrome, which has tens of full-time developers and hundreds of part-time ones. If you want Requests to work on every site where Chrome does, then I have bad news for you my friend because it's just _never_ going to happen.

I say all of this to say: please don't _berate_ the Requests team because we didn't think your particular pet bug was important. We prioritise bugs and close ones we don't think we'll fix any time soon. If you would like to see this bug fixed, a much better option is to _write the patch yourself_. Shouting at me does not make me look fondly on your request for assistance.
",Lukasa,fake-name
3098,2016-10-25 09:26:38,"So, that patch looks somewhat plausible. I should note for others skimming this thread that it only works on Python 3, but that's ok because this problem only exists on Python 3 (Python 2 doesn't insist on decoding the headers).

There are a few things we'd need to address if we wanted to move forward on this patch:
1. It's slow. Requests includes chardet, and already cops quite a lot of flak for using it to try to guess the encoding of body content because of how slow it is. Doing a chardet check for every response we get is likely to be pretty unpleasant.
2. It makes the assertion that every header is in the response is encoded the same way. Unfortunately I can confidently tell you that that's not true. It's extremely common to have a situation involving reverse or caching proxies where multiple entities add headers to a response, often using different character encodings. Of course, this patch isn't any _worse_ in that case than the current behaviour, but it doesn't necessarily gracefully handle it well either.
3. Monkeypatching the stdlib is asking for trouble. It hasn't entirely stopped us in the past, but that doesn't make me happy about it.

Part of me wonders whether we can address this issue in a more sledgehammery way. For example, can we avoid the chardet requirement by asserting that we only want to try UTF-8 and ISO-8859-1? That will fail in the rare instance that someone sends a header block encoded in UTF-16, but frankly I'd argue that in that instance the server is beyond help and needs to be put out of its misery.

The other advantage of doing it that way around is that it would let us drop down and put the patch into urllib3. In addition to ensuring that more people get the opportunity to benefit from the patch, it is the more natural place to put it: urllib3 is where we do most of our interfacing with httplib. The other advantage of putting it there is that I'm currently involved in trying to _replace_ httplib in urllib3, and having the patch in urllib3 makes me much more likely to _remember_ to remove the patch once we've resolved the issue. ;)

What do you think @fake-name? Would you be open to a more reduced version of this patch getting added to urllib3 as a (hopefully) temporary measure until we can remove httplib entirely?
",Lukasa,fake-name
3098,2016-10-25 20:05:00,"It's definitely a extremely clumsy fix. 

> It's slow. Requests includes chardet, and already cops quite a lot of flak for using it to try to guess the encoding of body content because of how slow it is. Doing a chardet check for every response we get is likely to be pretty unpleasant.

If requests is using chardet, have you looked at cChardet? It's a Cython-based api-compatible replacement for chardet, which according to it's [readme page](https://github.com/PyYoshi/cChardet) is about 1000-4000x faster then plain chardet. 

> It makes the assertion that every header is in the response is encoded the same way. Unfortunately I can confidently tell you that that's not true. It's extremely common to have a situation involving reverse or caching proxies where multiple entities add headers to a response, often using different character encodings. Of course, this patch isn't any worse in that case than the current behaviour, but it doesn't necessarily gracefully handle it well either.

Ugh. Why can't everything just be UTF-8?

I wonder if it's possible to only bother doing a chardet call if the string contains characters outside of the normal ASCII printable chars? Something like `if any([char > 127 for char in rawheader])`. That'd let the system avoid the call entirely when the string is definitively entirely plain ascii.

I can add per-header decoding after I'm home from work.

> Monkeypatching the stdlib is asking for trouble. It hasn't entirely stopped us in the past, but that doesn't make me happy about it.

It's certainly pretty ugly, though this is one of the cleaner monkey-patches I've seen (they were nice enough to break out the entire header decode thing into a single function, rather then having to replace parts of a class or something). Since it looks like `parse_headers` _is_ part of the httplib public API (albeit an undocumented part), the function signature _should_ be pretty fixed. 

I'm not too enthused about it either, but I think trying to convince python core to include cchardet in their dependencies for it is probably not exactly viable.

> Part of me wonders whether we can address this issue in a more sledgehammery way. For example, can we avoid the chardet requirement by asserting that we only want to try UTF-8 and ISO-8859-1? That will fail in the rare instance that someone sends a header block encoded in UTF-16, but frankly I'd argue that in that instance the server is beyond help and needs to be put out of its misery.

The problem with putting servers out of their misery is the people who own datacenters tend to get kind of irritable when you break in and hit someone's server with a baseball-bat. And _then_ you have to deal with the person who actually owns the server. 

> The other advantage of doing it that way around is that it would let us drop down and put the patch into urllib3. In addition to ensuring that more people get the opportunity to benefit from the patch, it is the more natural place to put it: urllib3 is where we do most of our interfacing with httplib. The other advantage of putting it there is that I'm currently involved in trying to replace httplib in urllib3, and having the patch in urllib3 makes me much more likely to remember to remove the patch once we've resolved the issue. ;)
> 
> What do you think @fake-name? Would you be open to a more reduced version of this patch getting added to urllib3 as a (hopefully) temporary measure until we can remove httplib entirely?

I don't know the requests architecture well enough to comment coherently, though you should treat my little hack above as basically public domain (or WTFPL if you really want a actual license). If it's useful, do what you want with it. 
",fake-name,fake-name
3096,2016-04-15 05:07:40,"@piotrjurkiewicz the tests fail on 3.3 and 3.4 now http://ci.kennethreitz.org/job/requests-pr/974/

This reminds me, I need to add Python 3.5 to the CI server. I didn't realize it was missing. 
",kennethreitz,piotrjurkiewicz
3096,2016-04-18 19:18:14,"@kennethreitz the tests here seem to have hung in Jenkins. Is there anyway to add a timeout to test runs?
",sigmavirus24,kennethreitz
3096,2016-04-18 21:34:24,"@sigmavirus24 just added a build timeout plugin, configured for 5 minutes for PR builds. 

Don't worry, I'm getting increasingly frustrated with maintaining Jenkins at the moment. 
",kennethreitz,sigmavirus24
3096,2016-06-07 08:49:48,"The reason the header order is being overridden in your case is because of the way requests merges the two different dictionaries in the `Session` and the request kwargs.

By default, a requests `Session` already contains several keys:



You'll note, then, that when you send headers using the `headers` kwarg, the order of the keys in the Session is preserved in priority to the order of the keys in the `headers` kwarg. This is the expected result of using the dict on the `Session` as the base into which the request dict is merged to update.

Trying to get the entirely intuitive behaviour (where the request header defines the order in preference to the `Session` order) is somewhat frustrating. Right now the code looks like this:



We'd need to change the code to



This would lead to the exact same result as we currently have but would prioritise the _order_ of the request dict rather than the `Session` dict. The cost is that we do substantially extra computation in order to achieve this relatively minor effect.

I am open to making this change, but it does rather feel like using a sledgehammer to crack a nut. @kennethreitz @sigmavirus24?
",Lukasa,kennethreitz
3096,2016-06-07 08:49:48,"The reason the header order is being overridden in your case is because of the way requests merges the two different dictionaries in the `Session` and the request kwargs.

By default, a requests `Session` already contains several keys:



You'll note, then, that when you send headers using the `headers` kwarg, the order of the keys in the Session is preserved in priority to the order of the keys in the `headers` kwarg. This is the expected result of using the dict on the `Session` as the base into which the request dict is merged to update.

Trying to get the entirely intuitive behaviour (where the request header defines the order in preference to the `Session` order) is somewhat frustrating. Right now the code looks like this:



We'd need to change the code to



This would lead to the exact same result as we currently have but would prioritise the _order_ of the request dict rather than the `Session` dict. The cost is that we do substantially extra computation in order to achieve this relatively minor effect.

I am open to making this change, but it does rather feel like using a sledgehammer to crack a nut. @kennethreitz @sigmavirus24?
",Lukasa,sigmavirus24
3096,2016-06-07 22:51:46,"@Lukasa I think this is something we need to document instead of work around. I'm not sure ""fixing"" that particular behaviour wouldn't introduce some other subtle bug.
",sigmavirus24,Lukasa
3096,2016-06-08 06:09:54,"I don't disagree with @sigmavirus24. While I do feel like this pattern should ""just work"" as requested, I feel like it's extremely uncommon for someone to want/need this, and we should not bend over backwards to accomplish this. 
",kennethreitz,sigmavirus24
3095,2016-06-21 17:08:36,"@Lukasa How did you do the quick check with openssl?  I've having the same problem, and suspect I may have the same cause.
",Singletoned,Lukasa
3094,2016-04-14 07:46:36,"Thanks for this report @emgerner-msft!

Firstly, can I check: the proxy string in your code doesn't actually separate the port from the host with an `@` symbol. Was that was a typo? If not, that could very really break our parsing of the URL.

Assuming it's a typo, I'd like you to try something for me. Can you reconfigure Fiddler and then print the output of both `requests.utils.should_bypass_proxies(url)` and `requests.utils.get_environ_proxies(url)`, where `url` is the URL you requested when you had this problem.
",Lukasa,emgerner-msft
3093,2016-04-14 07:33:42,"@Nuruddinjr Can you print the actual error message? It seems like the URL you've built is invalid, which suggests that the code you're using right now isn't correct in some way.
",Lukasa,Nuruddinjr
3093,2016-04-15 21:38:22,"@Nuruddinjr please answer @Lukasa soon. Otherwise, I'm going to close this issue as it does not appear to be a bug.
",sigmavirus24,Lukasa
3093,2016-04-15 21:38:22,"@Nuruddinjr please answer @Lukasa soon. Otherwise, I'm going to close this issue as it does not appear to be a bug.
",sigmavirus24,Nuruddinjr
3091,2016-04-13 18:20:09,"This looks great to me. @bodgit could you add some tests to ensure this doesn't regress?
",sigmavirus24,bodgit
3091,2016-04-13 20:39:37,"To be clear, my code review is +1 on this too. @sigmavirus24 has the best testing approach here, I think we just need to mock it out.
",Lukasa,sigmavirus24
3091,2016-04-15 12:50:12,"Thanks @bodgit! :tada: 
",sigmavirus24,bodgit
3090,2016-04-13 14:37:35,"Great spot, this is definitely a bug!

The bug is in `HTTPAdapter.close`: this currently clears the basic `PoolManager`, but doesn't clear any instantiated `ProxyManager` objects. That means that they inadvertently get preserved, which makes using them for this use-case untenable.

This bug is easily fixed, though: clearing out `HTTPAdapter.proxy_manager` is going to be the way to go.

In the meantime @bodgit, to work around this problem you can not just close the Session but actively mount new `HTTPAdapters`:



This will hopefully all become needless in a future version of requests which will include TLS information in the connection pooling, but we're not there yet.
",Lukasa,bodgit
3090,2016-04-15 13:36:43,"Closed this as #3091 has been merged. Thanks @Lukasa for the workaround and the explanation of the bug.
",bodgit,Lukasa
3089,2016-04-14 01:19:28,"@Lukasa 
But i can't use `cookielib.CookieJar.add_cookie_header` to add a raw cookie to requests
can you give me a example?
thanks!
",liuyang007,Lukasa
3089,2016-04-14 08:54:00,"@Lukasa 
Yes, I copy a cookie from browser want to attach it to a request.
How to do this with `cookielib.CookieJar.add_cookie_header`?
",liuyang007,Lukasa
3089,2016-04-15 01:12:44,"@Lukasa 
Thanks.
 but I think use `Cookie.SimpleCookie` is too complex,use this pull request to very easy to achive.
",liuyang007,Lukasa
3089,2016-04-15 02:40:35,"@sigmavirus24 
Ok.thank you
",liuyang007,sigmavirus24
3089,2016-05-21 17:07:27,"if found a simple way to  achieve my target.



it work well.
Thanks all of you.
@kennethreitz 
@Lukasa 
@sigmavirus24 
",liuyang007,Lukasa
3089,2016-05-21 17:07:27,"if found a simple way to  achieve my target.



it work well.
Thanks all of you.
@kennethreitz 
@Lukasa 
@sigmavirus24 
",liuyang007,sigmavirus24
3088,2016-04-11 17:20:56,"@rillian did you enable verification yourself or did another tool do that?
",kennethreitz,rillian
3085,2016-04-11 07:13:31,"There is no builtin method to make requests do this, nor will there be: servers that don't understand the format requests uses are old an non-standards-compliant. However, as @TetraEtc points out, you can use the [PreparedRequest flow](http://docs.python-requests.org/en/master/user/advanced/#prepared-requests) to mutate the body as you wish to, which would allow you to change the multipart-encoded body to whatever form you like.

You can also manually set the filename yourself by using longer tuples in the [files parameter](http://docs.python-requests.org/en/master/api/#requests.request): in particular, if you use a _bytestring_ in the filename portion then requests will leave it alone.
",Lukasa,TetraEtc
3085,2016-04-11 09:51:22,"@TetraEtc thanks, I have succeed using  urllib2 to send the data which encoded manually, although it is a boring work. I will also try the PreparedRequest method.

@Lukasa  do you mean use  something like `""测试中文视频.mp4"".encode('utf-8')` in the filename portion?
 when I try this, because of `result.encode('ascii')` (at requests/packages/urllib3/fields line38) fail, the post content is also  changed to something like this `filename*='%E6%B5%8B%E8%AF%95%E4%B8%AD%E6%96%87%E...`, which is not I want

Thank you.
",imnisen,TetraEtc
3085,2016-04-11 09:51:22,"@TetraEtc thanks, I have succeed using  urllib2 to send the data which encoded manually, although it is a boring work. I will also try the PreparedRequest method.

@Lukasa  do you mean use  something like `""测试中文视频.mp4"".encode('utf-8')` in the filename portion?
 when I try this, because of `result.encode('ascii')` (at requests/packages/urllib3/fields line38) fail, the post content is also  changed to something like this `filename*='%E6%B5%8B%E8%AF%95%E4%B8%AD%E6%96%87%E...`, which is not I want

Thank you.
",imnisen,Lukasa
3083,2016-04-10 16:31:29,"@JackDandy If you pass `stream=True`, then the callback should fire once the response has been received, so that resolves the problem where timeouts are encountered downloading the response data.

If the timeout is encountered while actually downloading the response, we can't call the response hook because we don't have a response to call it with. If you need to intercept that data the best place to do it is likely to be in the HTTPAdapter itself (by overriding the `send` method, which gets a `PreparedRequest` object), or by using the [prepared request flow](http://docs.python-requests.org/en/master/user/advanced/#prepared-requests) which will give you access to those things, also on a `PreparedRequest` object.

Do either of those work for you?
",Lukasa,JackDandy
3082,2016-04-11 07:21:23,"Ok @kennethreitz, you can hit the big green merge button whenever you're ready.
",Lukasa,kennethreitz
3079,2016-04-08 09:24:52,"@tzickel As far as I know, BytesIO implements `tell()` on all Python versions, so the simplest thing to do is to add logic that does `tell()`, `seek()`, `tell()`, `seek()`. Doing that, incidentally, would allow us to also record the current location in the file-like-object so that we can safely rewind.

@sigmavirus24, you wrote the original `getvalue()` call: what are the odds you remember the rationale at the time?
",Lukasa,tzickel
3079,2016-11-03 15:44:42,"So I believe that #3082, #3535, #3655, and Runscope/httpbin#284 in combination should have addressed all the pieces brought up in this issue. @tzickel, is there anything else you feel is missing? If not, I'm proposing we close this as resolved.
",nateprewitt,tzickel
3079,2016-11-28 17:31:38,"Alright, with 2.12 released and no further comments from @tzickel, I think we can consider this closed.",nateprewitt,tzickel
3077,2016-04-06 07:43:44,"Thanks for this @correcthorsebatterystaple-!

Requests has strong opinions about the correct form of a URL: specifically, it does not percent-encode things that don't normally need percent encoding. I should note that your server is in this instance misbehaving: periods do not need to be percent encoded in query string and there's no reason to do it, and even if there were, for characters that do not need percent encoding both the encoded and non-encoded character are to be treated _exactly identically_.

Regardless, the pragmatic solution to this problem is the [prepared request flow](http://docs.python-requests.org/en/master/user/advanced/#prepared-requests), where you construct a `Request` object, `prepare()` it, and then edit the `url` field on the `PreparedRequest` object you get. Any changes to the URL field will then be kept, so you can (for example) double-encode the period in the query string and then do a `prepped.url.replace()` call to swap it out with the single-encoded period.
",Lukasa,correcthorsebatterystaple-
3077,2016-04-06 17:31:11,"Thanks for the response, @Lukasa.

Fair point regarding equivalency and the server misbehaving. However, one doesn't always have control over the misbehaving server, so sometimes you have to have a way to send what needs to be sent, even if it's technically wrong. :-)

Thanks for the info on how to get around this with a PreparedRequest!

Out of curiosity, I understand a stance of not percent-encoding things that don't typically need it, but why percent-**de**code things (specifically the period in this case)? Does it benefit other parts of the codebase by normalizing encoded and decoded periods when performing operations?

Also, regarding Requests' stance on URL forms and encodings, is there documentation regarding what gets encoded/decoded where? For example, I noticed passing query string params as a dict results in encoding, but passing as bytes doesn't.

I'm mainly asking because I'm writing a tool that walks through a series of requests (to and from servers I don't have any control over ;-) ), and part of the challenge is to ensure certain pieces of data pass through the process in their intended form. If I pull a URL out of a Location header, for example, do I need to do any manually encoding on certain characters before I pass it to Requests (like plus signs)?

Thank you for your time on this! I'm a bit of a Python noob (maybe I've graduated to noob+ at this point ;-) ), and I'm even more of a Requests noob, so I appreciate you taking the time to field my feedback.
",correcthorsebatterystaple-,Lukasa
3076,2016-04-05 13:41:50,"@hachterberg Can you check whether the same problem exists for `resp.content`?
",Lukasa,hachterberg
3076,2016-04-05 14:05:58,"@Lukasa The result is the same for `resp.text` and `resp.content`
",hachterberg,Lukasa
3075,2016-04-05 09:06:01,"Given that this problem is pretty pervasive in urllib3's code, I'm going to close this issue in favour of shazow/urllib3#833. Let's improve both projects at once.

Thanks for the report @correcthorsebatterystaple-, please feel free to subscribe to the issue linked above.
",Lukasa,correcthorsebatterystaple-
3075,2016-04-05 18:11:36,"Hi @Lukasa, thanks for the quick response!

Is the urllib3 issue specific to https? Because it seems to work with mixed case protocol when it's http, and only has issues with https.

Regards,
-Justin
",correcthorsebatterystaple-,Lukasa
3075,2016-04-05 21:15:03,"@Lukasa Gotcha. Luckily, this is relatively easy to workaround in my case. Though, I wonder what would happen if Requests were configured to auto-follow redirects (while using proxies) and it encountered an HTTPS URL in the Location header (which is what would happen in my case, except I'm manually following each redirect, so I can check for the case and adjust it).

Anyway, thanks again for your help. :-)
",correcthorsebatterystaple-,Lukasa
3075,2016-04-05 21:41:04,"@Lukasa Is there a way to workaround the issue in a custom HTTPAdapter subclass? I tried fixing the scheme in the add_headers method, but it looks like that doesn't catch the issue early enough in the flow (I haven't dug through the code enough yet to know exactly what's happening when).
",correcthorsebatterystaple-,Lukasa
3075,2016-04-05 22:02:38,"@Lukasa Nvm, I overrode the get_connection method in my own HTTPAdapter child class to make any https schemes lower case before calling the parent class's get_connection method. :-)
",correcthorsebatterystaple-,Lukasa
3075,2016-04-06 07:49:29,"@correcthorsebatterystaple- Yup, that'd be the best place to work around this for now. =)
",Lukasa,correcthorsebatterystaple-
3073,2016-04-01 12:51:03,"@arcan1s you don't need an OrderedDict, you only need to provide a list of tuples. And questions should be asked on [StackOverflow](https://stackoverflow.com).
",sigmavirus24,arcan1s
3073,2016-04-01 13:03:26,"@MrAureliusR The terseness comes from the fact that we get several new issues a day that require that the core maintainers interrupt their days to address. Each bug report costs at least one maintainer a few minutes and a context switch, and may cost more than that if multiple maintainers stop to address it. That's unfortunate when you consider that the majority of the maintainers are unpaid, and all of us are overstretched.

This is doubly problematic for questions. When you opened this issue you were shown this screen:

![screen shot 2016-04-01 at 13 56 23](https://cloud.githubusercontent.com/assets/1382556/14207380/951c6ec6-f811-11e5-9d51-b7842fd8023a.png)

Notice the yellow bar at the top. Had you clicked through and read that link you'd have seen [this document](https://github.com/kennethreitz/requests/blob/master/CONTRIBUTING.md), which contains in its very first section this text:

> The GitHub issue tracker is for _bug reports_ and _feature requests_. Please do not use it to ask questions about how to use Requests. These questions should instead be directed to Stack Overflow. Make sure that your question is tagged with the `python-requests` tag when asking it on Stack Overflow, to ensure that it is answered promptly and accurately.

This is why the interaction was strained: you demonstrated that you had not read that note. Those notes are important to ensure that everyone interacts as efficiently as possible and to ensure that the limited resources of the project are allocated in the most useful way available.

Each issue opened on this bug tracker costs the core team at least 5 minutes of time. We're at issue 3073 right now, meaning that if these had all been _just questions_ we'd have spent nearly _11 days_ just swapping to handle those questions.

I say all of this not to chastise you, but to remind you that open source is a predominantly volunteer operation where the primary resource is time. Asking questions on bug trackers is a common source of wasted time on the part of maintainers, which is why maintainers can be terse when questions are asked in inappropriate forums.
",Lukasa,MrAureliusR
3073,2016-04-02 15:51:51,"Read through the original thread, and I want it to be known that I think @MrAureliusR's submission was perfectly fine. Not every interaction here has to be handled solely by the maintainers (in this instance, a community member was already stepping in to help).

The ""be cordial or be on your way"" rule applies to maintainers too :) These interactions were fairly cordial, but far from friendly.
",kennethreitz,MrAureliusR
3073,2016-04-02 15:52:49,"No further comment needed. Sorry for your bad experience, @MrAureliusR
",kennethreitz,MrAureliusR
3072,2016-03-31 19:17:34,"@Lukasa I feel dumb! Turns out my `username` and `password` were incorrect, and thus I was not authenticated, and thus no cookie. My bad. Thanks for the super quick reply though, Lukasa!
",jackyliang,Lukasa
3070,2016-03-29 20:44:27,"@sigmavirus24 I don't know the argument pro/con sessions having a timeout, but I'm happy to defer to other's judgement and experience on that, keeping these waters clean, like you say. 

I'm just happy that I haven't gotten a swift education about timeouts.
",mlissner,sigmavirus24
3070,2016-05-13 10:28:10,"@kuraga No. Per @sigmavirus24, and in many many previous discussions:

> I think I could actually still implement it using hooks - create a method which uses the prepared request workflow, and in the hook, just call it again. What would be your suggested solution?
",Lukasa,kuraga
3070,2016-05-13 10:28:10,"@kuraga No. Per @sigmavirus24, and in many many previous discussions:

> I think I could actually still implement it using hooks - create a method which uses the prepared request workflow, and in the hook, just call it again. What would be your suggested solution?
",Lukasa,sigmavirus24
3070,2016-05-13 10:39:02,"@Lukasa ok, but which discussion did you cite? Was it private? Which previous discussions?
",kuraga,Lukasa
3070,2016-05-13 16:23:45,"@kuraga please search _closed_ issues. There are several discussions of this. I really don't want this diversion to distract from the topic of a _default timeout_ though. So can we **please** stop discussing this now as I asked nicely before. You and @justanr are distracting from the important and _attainable_ portion of this issue.
",sigmavirus24,kuraga
3070,2016-05-13 16:23:45,"@kuraga please search _closed_ issues. There are several discussions of this. I really don't want this diversion to distract from the topic of a _default timeout_ though. So can we **please** stop discussing this now as I asked nicely before. You and @justanr are distracting from the important and _attainable_ portion of this issue.
",sigmavirus24,justanr
3070,2016-05-13 16:44:49,"@sigmavirus24 I added a caveat to [my initial comment, above](https://github.com/kennethreitz/requests/issues/3070#issue-144324837). Hopefully that should get this discussion focused.
",mlissner,sigmavirus24
3070,2016-11-13 22:04:40,"> It's extremely unclear to me what ""wrong"" means here. They aren't wrong: they work, as designed.

I think the point that's generally acknowledged by this bug is that the design _was_ wrong. There's a general consensus here that adding a default timeout or requiring it as an argument is a good idea. The  docs could address that, and that seems like a simple step to take until this issue is resolved in the next major version.

What happens in practice is that people grab the examples from the docs, don't read the timeout section carefully (or don't understand its implications -- I didn't for years until I got bit), and then wind up with programs that can hang forever. I completely agree with @chris-martin that until this issue is fixed, all examples in the docs should provide the timeout argument. Otherwise, we're providing examples that can (and probably will) break your programs.
",mlissner,chris-martin
3070,2016-11-14 17:50:19,"That's fair enough, @Lukasa. What about making the Timeout section more explicit then? Right now it says:

> You can tell Requests to stop waiting for a response after a given number of seconds with the timeout parameter.

And then goes on with a long, somewhat complicated warning. Could the first part say:

> You can tell Requests to stop waiting for a response after a given number of seconds with the timeout parameter. Nearly all production code should use this parameter in nearly all requests. Failure to do so can cause your program to hang indefinitely:

Something like that? 
",mlissner,Lukasa
3069,2016-03-29 11:31:53,"\o/ This looks great, thanks @achermes! :sparkles: :cake: :sparkles:
",Lukasa,achermes
3068,2016-03-28 15:20:07,"@medecau Yeah, so you'll almost certainly find that if you call `read()` with no arguments (which is what requests normally does), that hangs the same way that requests does. This is because httplib controls the HTTP framing and is presumably confused about what the actual content-length is.

Strictly speaking the best way to fix this would be to raise a bug against httplib, though the _actual_ best fix is to tell the server to sort itself out.
",Lukasa,medecau
3068,2016-03-28 15:32:16,"Since this is strictly not something we can fix in requests (or urllib3) I think we should close this. Thoughts @Lukasa ?
",sigmavirus24,Lukasa
3066,2016-05-06 16:47:54,"@Lukasa has anyone started work on this? I have some free time today, I was thinking I'd take a look at it. If I come up with something I can put together a PR.
",davidsoncasey,Lukasa
3066,2016-05-06 16:58:03,"@davidsoncasey I don't believe anyone has taken this up yet, no.
",Lukasa,davidsoncasey
3066,2016-05-06 17:40:53,"@Lukasa great, I'm working now on writing a test to replicate the issue. Once I've got that, I may ask you to take a look to verify that I'm correctly capturing the issue, and then I'll proceed with a fix.
",davidsoncasey,Lukasa
3066,2016-05-06 21:54:19,"@Lukasa I just made PR #3181, with the solution to not call `prepare_content_length` if the `Transfer-Encoding` header is present. I went back and forth on whether to do the check there or to short-circuit out of `prepare_content_length` and I'm still not sure which I like better, so if you take a look and think it would be better to do within `prepare_content_length` then by all means.

I was thinking it may be more clear to refactor `prepare_body` slightly to not set the `Content-Length`  header at all in `prepare_body` (line 442 of models.py), and to call `prepare_content_length` regardless of if the data is a stream. I think then it could be a bit more explicit that the two headers should never coexist in the same request. Let me know if you have thoughts.
",davidsoncasey,Lukasa
3066,2016-11-03 20:52:54,"I had a brief chat about this issue with @Lukasa this morning, and wanted to update the status. Due to some recent improvements to `super_len` and `prepare_content_length`, it seems we've inadvertently solved @julian-r's original issue. You'll find the code below (a minor variant of the Stackoverflow question) now works.



~~Note I use the word _works_ loosely. Due to an oversight I made, Requests is currently setting all streamable bodies with a length of 0 to 'Transfer-Encoding: chunked'. This doesn't cause any severely negative behaviour but is likely better if we used 'Content-Length' instead. We can either integrate that into the work in #3338, or I'll open a separate issue/PR for it.~~

---

@davidsoncasey, you've done a lot of great work in #3184 and #3338 that would be beneficial for Requests. Would you be willing to reframe a few parts of it around these recent changes?
1. I think it would be helpful to move your three non-exception tests from #3338 into the master branch. This would show that the functionality is currently working and will prevent us from regressing it moving forward.
2. We can then integrate your simplification of `prepare_body` and `prepare_content_length`, along with your exceptions, with the current changes on master. That would be an excellent addition in 3.0.0.

How does that sound?
",nateprewitt,davidsoncasey
3066,2016-11-03 20:52:54,"I had a brief chat about this issue with @Lukasa this morning, and wanted to update the status. Due to some recent improvements to `super_len` and `prepare_content_length`, it seems we've inadvertently solved @julian-r's original issue. You'll find the code below (a minor variant of the Stackoverflow question) now works.



~~Note I use the word _works_ loosely. Due to an oversight I made, Requests is currently setting all streamable bodies with a length of 0 to 'Transfer-Encoding: chunked'. This doesn't cause any severely negative behaviour but is likely better if we used 'Content-Length' instead. We can either integrate that into the work in #3338, or I'll open a separate issue/PR for it.~~

---

@davidsoncasey, you've done a lot of great work in #3184 and #3338 that would be beneficial for Requests. Would you be willing to reframe a few parts of it around these recent changes?
1. I think it would be helpful to move your three non-exception tests from #3338 into the master branch. This would show that the functionality is currently working and will prevent us from regressing it moving forward.
2. We can then integrate your simplification of `prepare_body` and `prepare_content_length`, along with your exceptions, with the current changes on master. That would be an excellent addition in 3.0.0.

How does that sound?
",nateprewitt,julian-r
3066,2016-11-03 20:52:54,"I had a brief chat about this issue with @Lukasa this morning, and wanted to update the status. Due to some recent improvements to `super_len` and `prepare_content_length`, it seems we've inadvertently solved @julian-r's original issue. You'll find the code below (a minor variant of the Stackoverflow question) now works.



~~Note I use the word _works_ loosely. Due to an oversight I made, Requests is currently setting all streamable bodies with a length of 0 to 'Transfer-Encoding: chunked'. This doesn't cause any severely negative behaviour but is likely better if we used 'Content-Length' instead. We can either integrate that into the work in #3338, or I'll open a separate issue/PR for it.~~

---

@davidsoncasey, you've done a lot of great work in #3184 and #3338 that would be beneficial for Requests. Would you be willing to reframe a few parts of it around these recent changes?
1. I think it would be helpful to move your three non-exception tests from #3338 into the master branch. This would show that the functionality is currently working and will prevent us from regressing it moving forward.
2. We can then integrate your simplification of `prepare_body` and `prepare_content_length`, along with your exceptions, with the current changes on master. That would be an excellent addition in 3.0.0.

How does that sound?
",nateprewitt,Lukasa
3065,2016-03-24 19:40:59,"@sigmavirus24 Any luck with this?
",james-hoegerl,sigmavirus24
3065,2016-03-25 09:38:09,"@james-hoegerl I don't really know anything about lambda I'm afraid.
",Lukasa,james-hoegerl
3065,2016-03-25 14:16:58,"Yah was just about to do that. Thanks for your help @sigmavirus24 @Lukasa. Sorry this ended up being so dumb. Heading to lambda forums. 
",james-hoegerl,Lukasa
3065,2016-03-25 14:16:58,"Yah was just about to do that. Thanks for your help @sigmavirus24 @Lukasa. Sorry this ended up being so dumb. Heading to lambda forums. 
",james-hoegerl,sigmavirus24
3063,2016-03-23 12:36:20,"@asieira This is an idea we've been interested in for a long time: see shazow/urllib3#607.

Unfortunately, any support we're likely to have will be of minimal use in practice unless deliberate effort is taken to harden your application. We'd need a cache in non-ephemeral storage (or the exit of your app cleans the in-memory cache), and PKP headers are extremely uncommon.

Regardless, the issue to track is the one linked above.
",Lukasa,asieira
3061,2016-03-20 16:33:30,"@Lukasa It contains the service-specified header X-ProxyMesh-IP which could be used to control request-rate. I could manage a proxies-count to do deeper requests-handling.

I have searched for a long time, found nothing could help me to figure it out. Could you offer me some advices on getting tunnel CONNECT responses headers. It is so significant to my application.
",jkryanchou,Lukasa
3061,2016-03-20 16:42:31,"@Lukasa https://bugs.python.org/issue24964 Here is a patch to python httplib which is same to mine. However it was python 3 patch not python 2.7. On the other hand, I think the rotating proxy service design was not easy handful. Oops...  I have no idea how could I works with the proxy in a convenient way. 
",jkryanchou,Lukasa
3061,2016-03-21 12:32:14,"@Lukasa Awesome. Thanks for your suggestion. I'm so sorry for making mistake the purpose of ssl.wrap_socket() method. I will try to do some monkey-patch hacking based on your reply. 
",jkryanchou,Lukasa
3061,2016-05-22 04:57:06,"@moacirmoda Yeah. I have tried to work it with the requests. While it couldn't work well. I emailed to the Proxymesh author. He offered me a solution.  You could email him for the solution. : ) He would give U a short code snippet for U.
",jkryanchou,moacirmoda
3061,2016-05-22 05:44:16,"@moacirmoda You're welcome.  I originally mail the issue to the maintainer. While I re-post to him some code snippets. and we worked together to figure it out. I finally re-implement by inheriting the `httplib.HTTPSConnection` to do the stuff based on the code snippet from the ProxyMesh maintainer. 

While my code snippets was so complicated so that I couldn't give you a clear example for u to work with. You could refer the example by email to him. Hope it could help you
",jkryanchou,moacirmoda
3061,2016-05-25 02:03:10,"@moacirmoda Sorry, I haven't checked my github notifications yesterday. Have you email him? Or I could offer the code snippets I worked together with him. Here is the code snippet link 

https://gist.github.com/ryanchou1991/e95d6ef76fbc2ed8d395
",jkryanchou,moacirmoda
3061,2016-05-25 02:18:31,"@moacirmoda I remind of a discussion on stackoverflow. It may help you work it out with the issue.

Here is the discussion `https://stackoverflow.com/questions/14665064/using-python-requests-with-existing-socket-connection`
",jkryanchou,moacirmoda
3061,2016-08-25 19:39:56,"I ran into this issue as well. I couldn't find any nice way to modify `HTTPResponse` to include the proxy headers separately, so instead I just merged them into the final headers dictionary.

The below code is copied from my [StackOverflow question](https://stackoverflow.com/questions/39068998/reading-connect-headers):



You can then install the adapter onto a session and pretend the proxy injected the headers into the proxied response, like for HTTP:



@moacirmoda The gist you linked to didn't work for me either. Mine does, however: https://gist.github.com/Blender3D/b8763f7b12099198fe1d947613c2739a
",Blender3D,moacirmoda
3060,2016-03-18 10:01:21,"So, I'm totally happy with this code change: I think it's a reasonable refactoring that helps improve some clarity, _as well_ as allow @benweatherman to perform the override you want.

I'm also delighted to see some extra tests added for this. I think this change is wonderful! Pinging @kennethreitz or @sigmavirus24 for an extra round of review before merging.
",Lukasa,benweatherman
3057,2016-03-17 15:19:57,"Thanks @kevinburke! :sparkles: :cake: :sparkles:
",Lukasa,kevinburke
3056,2016-03-16 19:13:39,"@kennethreitz I actually was going to rename complexjson to json but there are json variable references all over the models.py file and decided it was probably best not to refactor it. If you still think it's a good idea I'm game to change it.
",digitaldavenyc,kennethreitz
3053,2016-03-16 15:16:23,"@Lukasa That's a good point. There is actually only file that is importing json, [models.py](https://github.com/kennethreitz/requests/blob/master/requests/models.py#L34) So the diff would be about the same. Updated the PR
",digitaldavenyc,Lukasa
3052,2016-03-15 21:29:57,"@digitaldavenyc the only acceptable way to control that import flow would be to support an environment variable e.g. `REQUESTS_NO_SIMPLEJSON`. That is not going to happen.
",kennethreitz,digitaldavenyc
3052,2016-03-15 22:06:22,"@kennethreitz Exactly. You just mentioned that not many people use Python 2.6 anymore. Is it still a nice to have feature that's worth including?
",digitaldavenyc,kennethreitz
3052,2016-03-15 22:25:58,"I agree. A few options here:
1. Keeping things as they are (always preferred)
2. Removing simplejson logic completely (I think this would be fine)
3. Limiting simplejson logic to 2.6 only (unideal, but would limit potential side-effects)

I like **1** the best, with a ""if it ain't broke, don't fix it!"" mentality, very loosely held. I know **2** is what @sigmavirus24 and @Lukasa prefer, and if it's worth the (minimal) effort, I'm not against it if they're for it. 
",kennethreitz,Lukasa
3052,2016-03-15 22:25:58,"I agree. A few options here:
1. Keeping things as they are (always preferred)
2. Removing simplejson logic completely (I think this would be fine)
3. Limiting simplejson logic to 2.6 only (unideal, but would limit potential side-effects)

I like **1** the best, with a ""if it ain't broke, don't fix it!"" mentality, very loosely held. I know **2** is what @sigmavirus24 and @Lukasa prefer, and if it's worth the (minimal) effort, I'm not against it if they're for it. 
",kennethreitz,sigmavirus24
3050,2016-03-13 12:49:24,"@alexanderad The `ProxyError` you see in that traceback is actually a urllib3 `ProxyError` class. If we wanted to try to abstract the `ProxyError` out we could in principle do that, and it looks like the same change that we had to work around for the timeout error we also have to workaround for the `ProxyError`. 

A pull request to fix this should be fairly simple: do you want to tackle it @alexanderad?
",Lukasa,alexanderad
3050,2016-03-14 07:48:47,"@Lukasa I think I can take a look and provide a patch, thanks for the confirmation. I see some `ProxyError` handling on lines https://github.com/kennethreitz/requests/blob/master/requests/adapters.py#L442-L443, need to take closer look what _that_ `ProxyError` refers to in it's original purpose and why we don't see it in this case (perhaps it comes from system-wide configured proxy?)
",alexanderad,Lukasa
3050,2016-03-14 08:29:54,"@alexanderad So you can work out why we don't see it in this case by looking at the exception itself, which is a nested collection of ever-lower-level exceptions, which you can see more clearly here



So the specific problem is that this _kind_ of `ProxyError` now causes a `MaxRetryError`, where previously it did not. So we just need some error handling code in the `MaxRetryError` branch to look for a `ProxyError` in the `reason`, and reraise appropriately.

Given that `ProxyError` is a subclass of `ConnectionError`, this change should be non-breaking.
",Lukasa,alexanderad
3049,2016-03-13 11:23:35,"This also seems reasonable to me, but @Stranger6667 is sat right next to me. ;) @sigmavirus24/@kennethreitz, mind doing an extra review for me? This is +1 from me.
",Lukasa,Stranger6667
3049,2016-03-13 11:37:35,"Hello folks seems somehow I got placed on this email alias By mistake.
Anyway I can be removed.

Thank you
-Ryan
On Sun, Mar 13, 2016 at 7:24 AM Cory Benfield notifications@github.com
wrote:

> This also seems reasonable to me, but @Stranger6667
> https://github.com/Stranger6667 is sat right next to me. ;)
> @sigmavirus24/@kennethreitz https://github.com/kennethreitz, mind doing
> an extra review for me? This is +1 from me.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/pull/3049#issuecomment-195938314
> .
",ryandebruyn,Stranger6667
3049,2016-03-13 11:42:28,"@ryandebruyn This is managed by GitHub, it's not an email alias. You'll need to take some action to remove yourself via GitHub.
",Lukasa,ryandebruyn
3049,2016-03-13 14:58:08,"@ryandebruyn you may be ""Watching"" this repository in which case only you can fix that by visiting the project page while logged in and ""un-watching"" it.
",sigmavirus24,ryandebruyn
3049,2016-03-15 13:46:15,"Hello, @sigmavirus24 @Lukasa !
I've updated this PR :)
",Stranger6667,Lukasa
3049,2016-03-15 13:46:15,"Hello, @sigmavirus24 @Lukasa !
I've updated this PR :)
",Stranger6667,sigmavirus24
3049,2016-04-06 19:04:11,"@Stranger6667 I don't like having tons of pull requests open, so I'm merging this. If you wouldn't mind, it would be appreciated if you opened another PR that addresses the notes @Lukasa left on your test. 
",kennethreitz,Stranger6667
3049,2016-04-06 19:04:11,"@Stranger6667 I don't like having tons of pull requests open, so I'm merging this. If you wouldn't mind, it would be appreciated if you opened another PR that addresses the notes @Lukasa left on your test. 
",kennethreitz,Lukasa
3048,2016-03-13 11:13:15,"@sigmavirus24 @kennethreitz Can one of you two do a separate code review of this? @Stranger6667 is here with me at PyCon SK writing some of these tests. I'm happy with these, but it'd be good if one of you two gave the ok/not-ok.
",Lukasa,Stranger6667
3048,2016-03-15 13:51:17,"Hello @Lukasa @sigmavirus24 !
I've updated this PR :)
",Stranger6667,Lukasa
3048,2016-03-15 14:10:28,"Awesome, we're very close now @Stranger6667. One small note. =)
",Lukasa,Stranger6667
3047,2016-03-12 09:17:18,"@chengshuyi Almost certainly, yes. =)
",Lukasa,chengshuyi
3046,2016-03-11 12:54:56,"@pc10201 Did you change your code to match mine, _including_ the unicode string for opening the file?
",Lukasa,pc10201
3046,2016-03-11 13:23:06,"@pc10201 In what sense is that not working? I see the filename field formatted correctly.
",Lukasa,pc10201
3045,2016-03-10 22:04:48,"@Lukasa both Chrome and Safari according to their ""Conditions"" section.
",sigmavirus24,Lukasa
3044,2016-03-10 09:01:47,"@aahancoc Requests ships the `cacert.pem` bundle by default, which means that if you're on a Linux distribution and it cannot be found your distribution removed that file themselves. This means your distribution broke your install.

I recommend you take it up with the people breaking our software downstream. =)
",Lukasa,aahancoc
3043,2016-03-09 15:34:53,"Hey @tusharmakkar08,

Thanks for sending this pull request. Could you explain what bug you were seeing that made you change this? What instigated this change? It's not clear that this exhibits the same behaviour it did before, especially since tests have failed pretty consistently across all versions of Python.
",sigmavirus24,tusharmakkar08
3043,2016-03-09 15:47:57,"@sigmavirus24 : In line 174 in `adapters.py` `if url.lower().startswith('https') and verify`  is there and then `if not verify` is there. Since `and` condition is there, `if not verify` won't be `True`. The other thing is more pythonic way of if/else i.e in line 200 
",tusharmakkar08,sigmavirus24
3043,2016-03-09 17:01:37,"@tusharmakkar08 your commit no longer affects that region and simply turns an easy to read if/else statement into a ternary which is not ""pythonic"" nor is it easier to read.

I'll ask again: What bug are you seeing that prompted this (if there was any bug)?
",sigmavirus24,tusharmakkar08
3043,2016-03-09 18:11:26,"We're not interested in random refactorings that change logic for no apparent reason. Sorry @tusharmakkar08 
",sigmavirus24,tusharmakkar08
3043,2016-03-09 18:28:08,"@Lukasa : I have done couple of more changes and few of them are [anti-patterns](https://www.quantifiedcode.com/app/issue_class/62b0f5b7b69e4a2498568b32bfa30991) as suggested by http://docs.quantifiedcode.com/python-code-patterns/ . This isn't random refactoring, we are removing anti-patterns from code @sigmavirus24. 

Thanks. 
",tusharmakkar08,Lukasa
3043,2016-03-09 18:28:08,"@Lukasa : I have done couple of more changes and few of them are [anti-patterns](https://www.quantifiedcode.com/app/issue_class/62b0f5b7b69e4a2498568b32bfa30991) as suggested by http://docs.quantifiedcode.com/python-code-patterns/ . This isn't random refactoring, we are removing anti-patterns from code @sigmavirus24. 

Thanks. 
",tusharmakkar08,sigmavirus24
3043,2016-03-09 21:18:14,"@tusharmakkar08 while your contributions are appreciated, I want to give you feedback — all of the changes present in this pull request actually _reduce_ readability, significantly.
",kennethreitz,tusharmakkar08
3043,2016-03-16 07:37:33,"@sigmavirus24 @Lukasa @kennethreitz: Since requests is being mentioned over [here](http://docs.python-guide.org/en/latest/writing/reading/) among the best python codes, I believe these kind of antipatterns shouldn't exist over here. 
",tusharmakkar08,kennethreitz
3043,2016-03-16 07:37:33,"@sigmavirus24 @Lukasa @kennethreitz: Since requests is being mentioned over [here](http://docs.python-guide.org/en/latest/writing/reading/) among the best python codes, I believe these kind of antipatterns shouldn't exist over here. 
",tusharmakkar08,Lukasa
3043,2016-03-16 07:37:33,"@sigmavirus24 @Lukasa @kennethreitz: Since requests is being mentioned over [here](http://docs.python-guide.org/en/latest/writing/reading/) among the best python codes, I believe these kind of antipatterns shouldn't exist over here. 
",tusharmakkar08,sigmavirus24
3043,2016-03-16 20:19:44,"@tusharmakkar08 please keep in mind that your continuous arguments are emailing roughly 841 people. Since it doesn't seem as if you will respectfully let this conversation end and it serves to benefit none of those 841 people (because it has little value to the project as a whole and is likely _not_ why they are subscribed) I am ending the conversation forcefully by locking it.
",sigmavirus24,tusharmakkar08
3040,2016-03-08 09:13:05,"@zjulmh This is a known problem. Unfortunately, the underlying HTTP library we're using makes it essentially impossible to actually do this: we only find out about the error response when the connection gets closed.

We'd need to replace httplib to avoid this problem, which is a substantial amount of work.
",Lukasa,zjulmh
3039,2016-03-08 09:41:48,"@janscas What operating system are you using?
",Lukasa,janscas
3039,2016-03-08 10:13:05,"@janscas I have no idea. If you want to try to dive deeper into what urllib2 is doing that we're not, you could use Wireshark to check the differences between the request that requests sends and the one that urllib2 sends.
",Lukasa,janscas
3038,2016-03-08 10:03:18,"@kennethreitz Nope, that got broken a _long_ time ago for headers. Back in 2012 it looks like.
",Lukasa,kennethreitz
3038,2016-03-08 10:04:39,"@kennethreitz We never had a test that enforces that invariant, which means that commit 366e8e849877aea44ce96abebd4f26f5bcce12fb didn't spot that we broke it in the rewrite for v2.0.
",Lukasa,kennethreitz
3038,2016-03-08 10:10:56,"@kennethreitz We could always replace the `CaseInsensitiveDict` used in this place with the [`HTTPHeaderMap`](https://github.com/Lukasa/hyper/blob/development/hyper/common/headers.py) from hyper.
",Lukasa,kennethreitz
3038,2016-04-14 04:24:46,"@Lukasa if @piotrjurkiewicz's patch has no unintended side-effects, I'm +1 for incorporating this. 
",kennethreitz,piotrjurkiewicz
3038,2016-04-14 04:24:46,"@Lukasa if @piotrjurkiewicz's patch has no unintended side-effects, I'm +1 for incorporating this. 
",kennethreitz,Lukasa
3038,2016-04-14 07:34:38,"I'm fine with it as well, would you like to raise a PR @piotrjurkiewicz?
",Lukasa,piotrjurkiewicz
3036,2016-03-11 00:31:23,"@Lukasa can you add release notes for this too?
",sigmavirus24,Lukasa
3036,2016-03-11 09:58:09,"@sigmavirus24 Done. =)
",Lukasa,sigmavirus24
3036,2016-03-11 16:15:13,"@kennethreitz Each time we change something from guessing a content length to just saying ""chunked is fine for this"", I feel like we're striking a victory for the way the web _should_ have been, rather than the status quo. 
",Lukasa,kennethreitz
3036,2016-03-11 16:43:11,"@Lukasa Thanks for the fix! 

Btw, I think it would useful if there was a warning in the docs about that resulting TCP inefficiency of simply passing a file object you mention above. The current example is simply `data=f`.
",jakubroztocil,Lukasa
3034,2016-03-09 21:31:41,"@erydo expressed on the original issue that they were going hands off. That implies to me that they're done working on this if we're not going to accept their changes. If I'm wrong, I'll be happy to reopen this if they wish to follow our recommendations for the appropriate fix. Otherwise, this will sit open for a while and I'd rather that not happen.
",sigmavirus24,erydo
3032,2016-03-05 07:07:55,"Thanks for this @davidsoncasey, this pretty much looks right! I left a few small notes inline for you to take a look at, but one those are addressed we can merge this!
",Lukasa,davidsoncasey
3032,2016-03-05 17:08:57,"@Lukasa great, thanks for taking a look! I'll have a chance later today to make those edits.
",davidsoncasey,Lukasa
3032,2016-03-06 04:40:20,"@Lukasa I added a couple fixes, take a look and let me know if there's anything else!
",davidsoncasey,Lukasa
3032,2016-03-06 18:07:57,"\o/ Thanks so much @davidsoncasey! :sparkles: :cake: :sparkles:
",Lukasa,davidsoncasey
3031,2016-11-23 10:18:48,@tlc Cookies should be enough.,Lukasa,tlc
3030,2016-03-04 08:13:41,"@coveritytest That has nothing to do with requests, and everything to do with Python: as the error reports to you, your _syntax_ is wrong.

Try: `cookies = {""hottest-count"": ""working""}`.
",Lukasa,coveritytest
3029,2016-03-03 22:26:47,"Hi @vaibhavkaul,

Thanks for sending a pull request. I'm afraid we likely won't be accepting it. Here's why:
- This is drastically backwards incompatible behaviour. 
- It takes a great deal of freedom away from the user which we have intentionally placed into their hands.
- This code does not just change this for the `multipart/form-data` case.
- This also prevents people from writing API fuzzers which provide incorrect content-type headers for a totally different kind of content and drastically reduces requests flexibility.
- This affects the cases where people provide something to `data=` or `json=` with a custom content-type header where they expect their explicit decision to specify that header to be honored.

We could consider this for version 3.0.0 but I think it would not fit there either for several of the reasons listed above and also mostly due to the fact that we hope to not break things too drastically in 3.0.

I won't close this just yet because I hope @Lukasa and @kennethreitz will weigh in as well.

Cheers,
Ian
",sigmavirus24,vaibhavkaul
3029,2016-03-03 22:30:53,"@sigmavirus24 I understand. I figured it might be worth pointing out. 

> It takes a great deal of freedom away from the user which we have intentionally placed into their hands.

As long as this is an intentional choice.

> This code does not just change this for the multipart/form-data case.

You are right. I missed that, i will at-least make it specific to the files case, even if we end up not merging this.

The silent nature of the problem can cause a lot of headaches. I don't think a majority of the users using `requests` are going for the over-ridden behavior or API Fuzzers.
",vaibhavkaul,sigmavirus24
3029,2016-03-04 16:25:18,"@sigmavirus24 I made the change so it doesn't affect other things besides files.

> Sometimes overriding requests default behaviour is correct for user

Yes, which which is why `sometimes` should not dictate default behavior.

> Will make the correct body with an incorrect header. 

I dont think thats true since for the body to be parseable by any standard multi-part library (cgi etc) it would need to have the correct header. Just having the data in the body does not make it correct. I would argue the body is wrong in this case for a multipart request.

My motive right now is not to get this merged in at all. Happy to close the PR. But we should be clear about what the issue is here. Specifically I would like to address things like:

> That means that one of the cases above prevents the user from doing exactly what they intend to do

and 

> then changing the behaviour to differently silently subvert expectations isn't a great direction

The specific part of code I am referring to deals [exclusively with Multipart Files](https://github.com/kennethreitz/requests/blob/master/requests/models.py#L445-L447). The whole [idea of adding a `file=` param](http://docs.python-requests.org/en/master/user/quickstart/#post-a-multipart-encoded-file) is to encode the request as a multi-part request. `Most` developers using that code path clearly want the request to be posted as multipart.

Multipart requests must [have a boundary specified in the content-type](https://www.w3.org/Protocols/rfc1341/7_2_Multipart.html), if this is missing, the whole purpose for passing a `files=` param is defeated. This is no longer a multipart request. This is not a `correct body with the wrong header`. Clearly the `requests` framework [does not allow a user to pass their own boundary](https://github.com/kennethreitz/requests/blob/master/requests/models.py#L156) (notice the missing boundary kwarg) and use that. It `always` relies on a auto generated boundary string. We should either give people the ability to be in control of the `content-type` (by letting them specify a boundary string) or do the right think in the library.

> In general the requests header dictionary lets people do stupid things. If the user wants to change content-length, they can. If they want to change transfer-encoding, they can

I agree. Devs do stupid things. But then why make [this](https://github.com/kennethreitz/requests/blob/master/requests/models.py#L434-L438) raise an exception. There should at least be a warning somewhere about the request not being encoded as a multi part request if the headers being passed include a `content-type`, even if that content type is `multipart/form-data`.

I completely agree that we should not make this change if you feel this breaks the existing API, so maybe this should be considered for a future release or maybe not. I'll leave it up to @kennethreitz.
",vaibhavkaul,sigmavirus24
3029,2016-03-04 17:16:49,"> FWIW, do you guys think a better solution would be to allow the user to pass a boundary string and use that instead?

You already can with the toolbelt's [`MultipartEncoder`](https://toolbelt.readthedocs.org/en/latest/uploading-data.html#streaming-multipart-data-encoder) and since 99% of our users don't need that in requests itself, it makes perfect sense to have something that lives elsewhere to do that.

> But then why make [this](https://github.com/kennethreitz/requests/blob/46184236dc177fb68c7863445609149d0ac243ea/requests/models.py#L434-L438) raise an exception.

Because one is exceptional. We could issue a warning when overriding the content-type header when a user specifies both `data=` and `files=` but I think that will only make a lot of people very angry with us.

> The specific part of code I am referring to deals exclusively with Multipart Files.

Right you're special casing **silent** behaviour for one very narrow case of something that should either be the case for everything or not. APIs cease being for humans when you have to say to yourself ""Wait, will requests overwrite this header for me if I'm doing this or is it only in this other case?"" every time they go back to do something. That is not a human API, that is an awful API design.

> > Sometimes overriding requests default behaviour is correct for user
> 
> Yes, which which is why sometimes should not dictate default behavior.

The default behaviour of requests is not wrong. The default behaviour is to trust the user that they know what they're doing.

> Most developers using that code path clearly want the request to be posted as multipart.

And they will get a body that is correct for a `multipart/form-data` request. If they're intentionally changing the header and the server they're talking to does not understand the header, they'll figure that out. People using requests are generally rather intelligent. Most have read the documentation and understand what they need to do when creating a valid `multipart/form-data` request. It is often the people who do not read the documentation that run into this, go and read the documentation, and fix it themselves.

> I completely agree that we should not make this change if you feel this breaks the existing API.

It's not a matter of _feeling_ that this is backwards incompatible. It is backwards incompatible.

And let me just strongly reinforce a point @Lukasa and I seem to be unable to convey appropriately:

This is taking documented and expected behaviour that you consider to be _silently_ broken and replacing it with behaviour that will be undocumented, unexpected, and will _silently_ break users' code. Your justification is to fix _silent_ behaviour but you're replacing it with inconsistent _silent_ behaviour.
",sigmavirus24,Lukasa
3029,2016-03-04 17:46:33,"@sigmavirus24 

> It's not a matter of feeling that this is backwards incompatible. It is backwards incompatible.

Yes, that was a bad choice of words. My Bad.

> People using requests are generally rather intelligent. Most have read the documentation and understand what they need to do when creating a valid multipart/form-data request. 

Thats great!

> It is often the people who do not read the documentation that run into this, go and read the documentation, and fix it themselves.

Can you point to to where in the documentation I would have seen the `content-type` behavior. I was able to find it because I looked at the code.
",vaibhavkaul,sigmavirus24
3029,2016-03-04 19:56:09,"FWIW, this becomes a much bigger problem when you are trying to Proxy requests and want to maintain most of the original headers. 

Having a list of headers that would potentially cause issues would be great for documentation.

@Lukasa big :+1: 
",vaibhavkaul,Lukasa
3029,2016-03-04 19:57:34,"@Lukasa likely shouldn't provide _but are certainly able to, if needed_. This is a feature, not a bug. 
",kennethreitz,Lukasa
3028,2016-03-04 19:51:12,"@davidsoncasey Awesome!
",kennethreitz,davidsoncasey
3027,2016-03-02 21:55:53,"Thanks for the tip @Lukasa. I'm a newbie when it comes to Python web serving. Been struggling getting Apache WSGI module + Flask on OS X el capitan all day. First it was SIP (system integrity protection) and now it's getting Apache/OS X python to recognize my anaconda python modules. This whole process is extremely unpleasant. Do you happen to know what ""proper WSGI server"" folks tend to use?
",rajrsingh,Lukasa
3027,2016-03-02 22:29:43,"@rajrsingh there are too many to list, but here are a few:
- Apache+mod_wsgi
- gunicorn
- uwsgi
- twisted
",sigmavirus24,rajrsingh
3025,2016-02-27 05:49:53,"1. If you look at my script (the one @kennethreitz  posted), that's what I did. 
   proxies = {'https': 'https:131.96.228.236:9064',
          'https': 'https:54.85.61.208:80',
          'https': 'https:54.209.36.19:80',}
2. The proxies I tried to use are HTTPS
3. If you look at the cookies for http://epiccodes.com, you'll see your IP in the cookies, I was expecting to see the proxy's IP in the cookie when I use proxies=proxies in the get request, instead of my actual IP.
4. My bad, I didn't know. Will do in the future if I have any questions.
",Tosible,kennethreitz
3025,2016-02-27 05:52:27,"> If you look at my script (the one @kennethreitz posted), that's what I did. 

It isn't. Please look closely at what you wrote and what I wrote. Further, which proxy do you expect to be used? Creating a dictionary with a repeated key is undefined behaviour (as to which value is kept) in Python.
",sigmavirus24,kennethreitz
3023,2016-02-18 19:19:15,"I'm with @kennethreitz this isn't necessary. Because of the line @kennethreitz is citing, users can already just do



Both work perfectly fine. Beyond that, we don't need a gitignore line for PyCharm/idea projects because that should already be in your [global gitignore](https://help.github.com/articles/ignoring-files/#create-a-global-gitignore).
",sigmavirus24,kennethreitz
3023,2016-02-18 19:50:45,"@kennethreitz Thanks, I was wondering about that line. Two questions:
1. That import makes them available as `requests.RequestException`. It's a subtle difference, but [this doc page](http://docs.python-requests.org/en/master/api/#exceptions) instructs users to include them as `requests.exceptions.RequestException`. Which one is the preferred or supported method?
   - If we want to include both, we could specify both in `__init__.py`.
   - If `requests.RequestException` is preferred (which I like), perhaps we should update the docs.
2. It looks like `ConnectTimeout` is missing from the list. Should it be added?
",jtpereyda,kennethreitz
3023,2016-02-18 19:55:03,"@sigmavirus24 Thanks for the global gitignore tip! I hadn't heard about that.
",jtpereyda,sigmavirus24
3023,2016-02-18 20:07:29,"@jtpereyda it's a subtle but unimportant difference. Try this in an interactive session:



Both already work perfectly fine. You can use either one or both interchangeably. If `ConnectTimeout` is missing, that should be added to the list of exceptions imported. But like I said, as it is now, we do not need to change anything for `requests.exceptions.RequestException` to work. It will simply work.
",sigmavirus24,jtpereyda
3023,2016-02-18 21:29:29,"@jtpereyda the ""recommended"" way is, in fact, `requests.RequestException`, as `requests.exceptions` is really just an internal namespacing.

That section of the docs are auto-generated from the codebase. I'd love to fix that section if we can. 

However, at this time, there's nothing broken about the docs or the code. We're just discussing room for possible usability improvement (which is the main goal of this library). 
",kennethreitz,jtpereyda
3023,2016-02-19 01:49:15,"@jtpereyda fixed the docs! http://docs.python-requests.org/en/master/api/#exceptions
",kennethreitz,jtpereyda
3023,2016-02-21 01:27:46,"@kennethreitz Thanks!
",jtpereyda,kennethreitz
3022,2016-02-25 18:32:42,"@jtpereyda oddly, those two exceptions are being _raised_ during the sphinx build. I haven't dug into it yet. 

But.... anyone can investigate! ;)
",kennethreitz,jtpereyda
3021,2016-02-18 07:01:39,"@geckon thank you!

For the record, this is a potentially breaking change for _someone_, hence why it has to wait for inclusion in the 3.x series. 
",kennethreitz,geckon
3021,2016-02-18 22:30:35,"Thanks for this @geckon 
",sigmavirus24,geckon
3020,2016-02-18 00:48:15,"@geckon please rebase this against that branch instead. If you need help, let me know and I can give you some instructions that should work in this case. Thanks for handling this! :cake: 
",sigmavirus24,geckon
3019,2016-02-17 23:42:16,"@geckon although, we're going to be merging this into the `proposed/3.0.0` branch. would you like to make your PR against that branch instead?
",kennethreitz,geckon
3017,2016-02-16 13:57:56,"I'm fine with us doing it; it won't hurt anything. But, I think it's dumb. 

The whole thing, that is, not your suggestion @geckon :)
",kennethreitz,geckon
3017,2016-02-16 14:24:21,"@geckon sorry I missed that!
",kennethreitz,geckon
3014,2016-02-14 20:46:01,"@Lukasa hmmm, that works.
",kennethreitz,Lukasa
3014,2016-02-14 20:51:33,"@EmilStenstrom `render` has been renamed to `dump`.
",kennethreitz,EmilStenstrom
3014,2016-02-14 21:26:19,"> Not that fond of the ""REQUESTS/2.9.1"" string. Maybe we could do PreparedRequest<[string_here]> as a way to mimic **repr**?

I agree with @EmilStenstrom. I don't like that much either. It's confusing and may make some people think we're doing some weird protocol instead of HTTP.
",sigmavirus24,EmilStenstrom
3014,2016-02-16 04:50:47,"@EmilStenstrom hmm, that's not a bad way to present it! I like it. 

For the record, I'm still not 100% on including this. I'm still tossing the idea back and forth in my mind. This syntax definitely helps, though.

This syntax also helps support the inclusion of `Response.dump`. It just didn't make sense with the other syntax.
",kennethreitz,EmilStenstrom
3014,2016-02-16 05:35:11,"@EmilStenstrom updated the syntax. 


",kennethreitz,EmilStenstrom
3014,2016-02-16 06:58:29,"@EmilStenstrom as soon as I'm done perfecting `PreparedRequest`, I'll move on to `Response` :)
",kennethreitz,EmilStenstrom
3014,2016-02-16 08:17:34,"@EmilStenstrom Response is now included (see above). :cake:

Thanks a lot for your help with this, you've been instrumental ;)
",kennethreitz,EmilStenstrom
3014,2016-02-16 08:35:51,"@sigmavirus24 @Lukasa last call for comments. Will merge/release tomorrow. 
",kennethreitz,Lukasa
3014,2016-02-16 08:35:51,"@sigmavirus24 @Lukasa last call for comments. Will merge/release tomorrow. 
",kennethreitz,sigmavirus24
3014,2016-02-16 14:00:13,"Yeah. So, I agree with @Lukasa whole-heartedly. I don't think we should be vendoring libraries just for colorizing this output. This feels like a vast step outside the domain of this library - HTTP. That kind of behaviour belongs elsewhere. We also don't even remotely attempt to reflect whether or not the user is attempting to use proxies, which I think will be far more confusing for those users when they expect to see that output here.

To be clear:
- I think we're violating the law of constraints of limiting this library to HTTP
- I think we're doing our users a grave disservice by having this functionality here in such a naive implementation
- I think we're adding more (vendored) dependencies than we should ever rightfully need
",sigmavirus24,Lukasa
3014,2016-02-16 17:00:13,"@kennethreitz the proxy information is tricky. We have a hacky way of incorporating it into the information in the toolbelt, but I don't think this implementation would have the same ability to determine if a proxy is in use.
",sigmavirus24,kennethreitz
3014,2016-02-16 23:13:52,"@TetraEtc thanks for your thoughts!

I think the colors are enjoyable and make the information easier to parse. Why do you not care for them? Just preference?
",kennethreitz,TetraEtc
3014,2016-02-16 23:41:51,"@TetraEtc for that, there is `pretty(colors=False)`. Also, colors are automatically disabled if the code is not running in an interactive console. 
",kennethreitz,TetraEtc
3014,2016-02-25 21:51:36,"@EmilStenstrom absolutely, that'll happen no matter what. :)

(unless I change my mind, for some reason)
",kennethreitz,EmilStenstrom
3014,2016-02-25 21:59:07,"I don't actually think including the pretty formatting matters in any way, shape, or form, to be completely honest. Meaning, I see absolutely no reason not to. I think it's a nice feature, and I would be very pleasantly surprised to find that level of polish in my favorite HTTP library. 

That being said, I'm taking extra time to be considerate of @Lukasa and @sigmavirus24's objections (regardless of final result). 
",kennethreitz,Lukasa
3014,2016-02-25 21:59:07,"I don't actually think including the pretty formatting matters in any way, shape, or form, to be completely honest. Meaning, I see absolutely no reason not to. I think it's a nice feature, and I would be very pleasantly surprised to find that level of polish in my favorite HTTP library. 

That being said, I'm taking extra time to be considerate of @Lukasa and @sigmavirus24's objections (regardless of final result). 
",kennethreitz,sigmavirus24
3014,2016-02-25 22:44:26,"> I would be very pleasantly surprised to find that level of polish in my favorite HTTP library.

And I'd be very confused why an HTTP library is doing anything with colorized output. `¯\_(ツ)_/¯` I think we have different definitions of the word ""polish"" w/r/t what a library should do.

> I'm taking extra time to be considerate of @Lukasa and @sigmavirus24's objections (regardless of final result). 

Sounds like you're going to merge this regardless of our objections? If so, I'm not sure why you would take extra time.
",sigmavirus24,Lukasa
3014,2016-02-25 22:44:26,"> I would be very pleasantly surprised to find that level of polish in my favorite HTTP library.

And I'd be very confused why an HTTP library is doing anything with colorized output. `¯\_(ツ)_/¯` I think we have different definitions of the word ""polish"" w/r/t what a library should do.

> I'm taking extra time to be considerate of @Lukasa and @sigmavirus24's objections (regardless of final result). 

Sounds like you're going to merge this regardless of our objections? If so, I'm not sure why you would take extra time.
",sigmavirus24,sigmavirus24
3014,2016-02-25 23:38:24,"@sigmavirus24 if I was sure, I would have merged it a week ago :)
",kennethreitz,sigmavirus24
3014,2016-02-26 14:26:28,"So, while we're taking feedback, here's my 2¢:

The philosophy of this project since about v1.0, at least as I understand it, has been to aggressively resist scope creep. We have rejected innumerably more feature requests than we've accepted, saying ""no"" to almost all feature requests that failed to meet one of three criteria:
1. It's extremely difficult or impossible to implement the feature from outside the library.
2. It's a convenience feature that would be used by a substantial majority of our users.
3. There is some subtlety in ""correctness"" of the feature that many users would miss, and that we have an opportunity to get right for them.

As far as I can see this feature fits into none of those categories.

To the first point, it is not difficult to implement this feature from outside Requests: it is exactly as hard there as it is here. The objects being used here are public and are regularly exposed to users: there's nothing that users would ordinarily be unable to find or struggle to reach.

To the second point, this feature would not be used by a majority of our users. The colorizing feature in particular suffers here because it only works when printing directly to a terminal and having a user observe those responses. That is a vanishingly small use-case of requests compared to automated use, and while it's important that those users have a good experience, I don't see value in implementing something solely for them unless it meets criteria 1 or 3. The dumping feature, even ignoring colorizing, is also something that is unlikely to be used by a substantial majority of our users, particularly because it is thoroughly ill-suited to logfiles. This is for two reasons: first, the dump contains newlines which make parsing logfiles extremely unpleasant; and second, the dump is really quite verbose and contains a lot of extraneous information that will serve only to bloat log files.

To the third point, the only subtlety in correctness here is not dealt with by this code, it is _encouraged_ by this code, and that is the fact that the representation dumped here is not the representation as received from or sent to the network. It is _related_ to those representations, but in no way conforms to them. Users who do not grasp this subtlety are, in my opinion, likely to be confused by the representation used here. The example used in the original post is great for this:



In this example the `Host` header is missing entirely, and the request URI is not what is actually sent on the network (that would be `/ip`, unless a proxy is in use in which case it _is_ what would be sent on the network, unless we're making a HTTPS requests and then there's a whole separate web request that we aren't showing at all here). I should note also that the REQUESTS/2.9.1 block is not in the same place as the HTTP/1.1 block would be in a request, though I don't know if that's intentional or not. The `Response` suffers even more from this because the response headers have been thoroughly transformed by the time they get to requests and frequently look almost nothing like they did when they were received from the network.

All of that goes to say that I don't believe we'd accept this feature if it came from outside the project. I think it represents a maintenance burden for the future, because once we implement this we'll need to support it, and I don't believe that it pays for that burden in utility. And I haven't even begun to mention that to implement this feature we are adding two implicit dependencies that will now require updates in order to fix bugs that affect this rendering and that will create substantial work downstream of us to unbundle.

Generally speaking we have been fairly aggressive about getting features _out_ of this library that are not required. I think this represents a step in the other direction, and I don't believe that we gain enough by doing it to justify it.

Of course, @kennethreitz, this is still your project and so my opinion is only that. =) If you merge this, I'll help maintain it.
",Lukasa,kennethreitz
3014,2016-02-26 17:39:24,"@Lukasa failurel! The above post stated:

> No further :+1:s or :-1:s needed at this point, but random anecdotes are more than welcome!

My cat won't leave me alone today.
",kennethreitz,Lukasa
3014,2016-03-21 13:46:08,"Thanks for playing @jwg4 but the instructions in this thread clearly state that only random anecdotes are welcome at this point.
",sigmavirus24,jwg4
3014,2016-03-22 22:05:26,"You have a cool avatar @jwg4.
",kennethreitz,jwg4
3013,2016-02-14 14:05:07,"@EmilStenstrom Generally this is a good idea, but we tend to oppose having 'utility' functions be merged into requests itself: we want to restrict the scope of the main library so that it remains really good at doing its specific roles.

For that reason, you should check out the [requests toolbelt](https://toolbelt.readthedocs.org/en/latest/dumputils.html), which has exactly the utilities you want.
",Lukasa,EmilStenstrom
3013,2016-02-14 16:50:31,"@kennethreitz How ""near exact"" is ""near exact""?

Note, for example, that the `PreparedRequest` does not have a `Host:` header: this is because httplib attaches it for us. It is also missing some `Accept-Encoding` headers that urllib3 adds for us. The reality is that Requests does not have access to enough information to render this out unless we want to guess at what the rest of the stack will do, and if we do that then we'll start getting bugs raised when the output of `render` does not match the actual transmitted bytes.
",Lukasa,kennethreitz
3013,2016-02-14 17:08:19,"@kennethreitz I have no objection to rendering a representation of the preparedrequest object, but I suspect it's _extremely_ unwise to render in a structure that suggests completeness.

In particular, the structure given here (which renders out to a 'valid' HTTP/1.1 request/response) doesn't work in the following cases:
1. If using HTTP/2, when the request/response are structured entirely differently and made of binary frames.
2. If we send a file or send using chunked transfer-encoding, where we cannot render the body before we send it.
3. If we _receive_ data using chunked transfer-encoding, where we cannot accurately represent it because urllib3 has transparently removed the chunking.
4. If the request is sent via a proxy there's an extra CONNECT request we're simply not printing.

There's nothing wrong with wanting to have a printable representation of all the data on the `PreparedRequest`, but we should be very wary before we format that like a HTTP/1.1 request.
",Lukasa,kennethreitz
3013,2016-02-14 20:28:02,"@EmilStenstrom what do you think of this? https://github.com/kennethreitz/requests/pull/3014
",kennethreitz,EmilStenstrom
3012,2016-02-14 01:23:31,"@IDSninja do you have a list of the packages you upgraded? Requests doesn't import anything external, with a few small exceptions. 
",kennethreitz,IDSninja
3012,2016-02-14 01:26:32,"@IDSninja can you show us the output of `$ pip2.7 freeze`?
",kennethreitz,IDSninja
3012,2016-02-14 01:41:20,"@IDSninja that's a complex question. Basically, the standard library's `ssl` library has some limitations, which `pyOpenSSL` fixes. Using it is best practice if security is a large priority for application (or you need the extra functionality it provides), but there's a slight cost. 
",kennethreitz,IDSninja
3011,2016-02-13 08:49:23,"That version of OpenSSL is _very_ old, which may be why you're having problems but @kennethreitz is not. Unfortunately, our PyOpenSSL-based workaround does not currently work on Python 3, though we're working on a fix for that. 

An alternative approach would be to install a newer OpenSSL from Homebrew and then get your Python 3 from Homebrew as well. That would likely resolve your problem. 
",Lukasa,kennethreitz
3010,2016-02-13 03:55:06,"@jaron92: It looks like this server is sending a phony redirect based on the `User-Agent` header the request used. 

This works for me:





This is why you were seeing differing results with cURL. 
",kennethreitz,jaron92
3010,2016-02-13 03:56:53,"@jaron92 sure thing! :cake:
",kennethreitz,jaron92
3009,2016-03-06 19:57:56,"Extra datapoint, I've been bitten by this too while interacting with the API of a Spanish-language server. The correct fixes here are, I believe:
1. Properly decode the HTTP Reason Phrase field as described by @denis-ryzhkov in [#1181](https://github.com/kennethreitz/requests/pull/1181#issuecomment-13423623) for header values.
   - According to [RFC2616](https://tools.ietf.org/html/rfc2616#section-6.1.1), Reason Phrase is `TEXT` which defaults to `latin1` and supports other characters under RFC2047.
2. Store the `.reason` attribute as unicode if it isn't already.
3. In `.raise_for_status` use `%r` as @Lukasa suggests (since exception messages are supposed to be bytestrings).
",erydo,Lukasa
3009,2016-03-06 20:04:57,"@erydo Changing the decoding of the reason phrase is beyond the scope of requests: that is handled entirely by httplib, and we can't change it. Generally speaking that works in a way that is acceptable: it misbehaves only in a few situations on Python 2.
",Lukasa,erydo
3009,2016-03-06 20:23:25,"@Lukasa I would expect that RFC obsolescence to apply more to developers of new servers rather than consumers of them.

Apache Tomcat, for example, sends down ~~unicode~~ `latin1` reason phrases for non-en locales, and that's not a rarely-encountered server.
",erydo,Lukasa
3009,2016-03-07 04:25:48,"> It conforms correctly to RFC2616

Please stop trying to justify the wrong behaviour by saying it conforms to an obsoleted specification. You're not convincing us of anything as we've been working towards conforming with the new set of HTTP/1.1 specifications as best we can while dealing with `httplib`.

I also don't understand why you're referring to sections in the RFC when the ABNF clearly states (as @Lukasa has already shown) that the reason phrase should be `HTAB`, `SP`, `VCHAR`, and `obs-text`.

> I believe RFC2047's recommendation ... is a bug in the specification.

Then please file errata against the RFC to address that. Please also note that 2047 is not the latest RFC on the topic.

---

Regardless, I have to agree with @Lukasa that the reason string should be treated as opaque bytes. I don't understand why you think your decision to run an internally patched would sway us, since you can decode the raw reason bytes before using the reason string.
",sigmavirus24,Lukasa
3009,2016-08-05 00:10:41,"You're right. Thanks @olivierlefloch 
",sigmavirus24,olivierlefloch
3008,2016-02-26 17:08:27,"@junkblocker I'm sorry about that but we're not going to rewrite history.
",sigmavirus24,junkblocker
3008,2016-02-26 17:30:55,"@junkblocker perhaps you can do a shallow clone instead?
",kennethreitz,junkblocker
3007,2016-02-12 20:54:55,"@Lukasa is there a way to query presense of this API in runtime? Could you give me some links to read more about it?
",Kentzo,Lukasa
3007,2016-02-12 21:11:37,"@Kentzo: for a discussion about the system cert bundle, see #2966. It is possible to query for this API at runtime because it's only exposed by PyOpenSSL.
",Lukasa,Kentzo
3007,2017-01-13 05:27:57,@Lukasa I think the original issue is orthogonal to #2966 and this issue should be re-opened because of the discovery made by @dsully.,Kentzo,Lukasa
3007,2017-01-13 05:27:57,@Lukasa I think the original issue is orthogonal to #2966 and this issue should be re-opened because of the discovery made by @dsully.,Kentzo,dsully
3006,2016-02-12 04:37:22,"@sigmavirus24 It's just some bug in my own code without anything to do with networking.
I'm not sure whether this is the real cause or just coincidence.

But one thing is for sure, after some point, I'm completely unable to make request to https://www.telegram.org, which I can do right after install request.

Just FYI: #2906
",caizixian,sigmavirus24
3006,2016-02-12 05:22:29,"@sigmavirus24 So this there anything I can do to help you?
",caizixian,sigmavirus24
3006,2016-02-12 09:34:43,"@Lukasa I think since requests is shipped with its own urllib3, I could not import urllib3 alone. And the following result confirms it.


",caizixian,Lukasa
3006,2016-02-12 10:19:14,"@Lukasa 


",caizixian,Lukasa
3006,2016-02-19 15:01:43,"@the-efi could you be more specific about which exception you're seeing?
",sigmavirus24,the-efi
3006,2016-02-19 15:42:26,"@the-efi Do you have `pyopenssl`, `pyasn1`, and `ndg-httpsclient` installed?
",Lukasa,the-efi
3006,2016-02-20 12:54:52,"@Lukasa Any progress with urllib3?
",caizixian,Lukasa
3006,2016-02-20 13:21:11,"@caizixian We're getting there, but we have some problems with our CI testing because Travis CI has a fairly old PyPy image that doesn't behave well with PyOpenSSL at the moment. I'll see if I can get this to work sometime this weekend.
",Lukasa,caizixian
3006,2016-02-21 00:26:22,"@Lukasa @shazow urllib3 always has a home at http://ci.kennethreitz.org, if desired!
",kennethreitz,Lukasa
3006,2016-05-21 08:22:46,"@mindw As best as I know we don't have a good understanding of exactly where and when it's happening. In the backtrace above it's happening during the handshake, which is usually a problem with negotitation: when are you encountering this error?
",Lukasa,mindw
3006,2016-05-22 15:54:52,"@mindw Look at [OP](https://github.com/kennethreitz/requests/issues/3006#issue-133149897) (the original post) where @caizixian imports the backend from `cryptography` and prints the OpenSSL version.
",sigmavirus24,mindw
3006,2016-05-22 15:54:52,"@mindw Look at [OP](https://github.com/kennethreitz/requests/issues/3006#issue-133149897) (the original post) where @caizixian imports the backend from `cryptography` and prints the OpenSSL version.
",sigmavirus24,caizixian
3006,2016-06-12 10:13:22,"@LukeNZ Lots of things. Python version, installed packages, requests version, OpenSSL version, operating system, and the website you're trying to contact.
",Lukasa,LukeNZ
3006,2016-06-16 07:17:26,"@eoco Ok, that's interesting. I can't reproduce this on OS X 10.11.6 Beta, Python 3.5.1 with Requests 2.10.0. Do you encounter the issue consistently or intermittently?
",Lukasa,eoco
3006,2016-06-17 07:40:11,"@eoco Ok, cool.

So the problem here is that your Python 3 is linked against the system OpenSSL, which is ancient. What's in your pip environment? Can you run `python3 -m pip freeze` for me?
",Lukasa,eoco
3006,2016-06-18 13:59:41,"@eoco Hrm. Can you run this for me? `python3 -c ""from requests.packages.urllib3.contrib import pyopenssl; pyopenssl.inject_into_urllib3()""` and tell me what that outputs? Also run `python3 -c ""from cryptography.hazmat.backends.openssl.backend import backend; print(backend.openssl_version_text())""`.
",Lukasa,eoco
3006,2016-06-20 09:53:57,"@eoco So while we're running this fix through the pipeline (see shazow/urllib3#901), your easiest fix will be to install Python 3 using something like Homebrew. This will link your Python 3 against a much newer OpenSSL, which will solve this problem for you both in requests and in any other application that uses the system TLS bindings.
",Lukasa,eoco
3006,2016-06-29 07:26:50,"@shredding Try installing pyopenssl, ndg-httpsclient, and pyasn1. You need all three at this time.
",Lukasa,shredding
3006,2016-12-02 18:37:47,"@Lukasa Thanks for your quick response. The output as you asked is as follows:

Also I am adding the response from OpenSSL 1.0.1f 6 Jan 2014
It appears to be working on this version.



Also I tried with earlier version of openssl and it seems to be working:

",ceprateek,Lukasa
3006,2017-01-09 19:48:27,"@rubendibattista www.celestrack.com has a *terrible* TLS configuration, as you can see [here](https://www.ssllabs.com/ssltest/analyze.html?d=www.celestrak.com&hideResults=on). This means that modern versions of Requests do not support *any* of the cipher suites that the server does, because all of them are weak and broken.

In the first instance, I recommend not contacting this server at all. If that's not possible, you can amend the default cipher suites by doing something [like this](https://github.com/kennethreitz/requests/issues/3774#issuecomment-267871876).",Lukasa,rubendibattista
3006,2017-01-09 20:06:25,@Lukasa Thanks for your support. I contacted the admin of the site to report what you say and try to find a better solution than using the workaround you suggest. ,rubendibattista,Lukasa
3005,2016-02-11 18:34:07,"Hey @scop I was wondering if you could provide detail about why this is desirable and why we should change the existing behaviour as well as a description of when/where monotonic will be available and where/when it won't.
",sigmavirus24,scop
3004,2016-03-02 13:43:49,"@Lukasa thanks. I wasn't sure if `-1` was the error code exactly. Is this error being caused from server end or from client machine? Any help or link that you can share which explains what this error is? Thanks
",grushler,Lukasa
3004,2016-03-23 03:56:04,"@AlanCoding I think you're on the right track with that code. At the least, it seems like using `args[0]` as the original exception isn't going away anytime soon. The code that currently raises an `SSLError` [passes the original argument as the first parameter](https://github.com/kennethreitz/requests/blob/3f8b1fb617cfdd3911602b5c3e668ad30602c64d/requests/adapters.py#L447). If you look in that same file, there are lots of places where the original exception is passed as the first param.

That said, if you're lucky enough to use python3 you can use the `__context__` property to reliably get at the root cause of an exception and (I don't think) it can change despite whatever changes might happen in requests.

I've put together what I think is the best way to get what you're looking for.



And the results:



Thoughts?
",benweatherman,AlanCoding
3003,2016-02-10 18:35:44,"@Lukasa It looks like this could be accomplished using utils.dump by adding a `print_body=True` flag and wrapping the following lines in a conditional statement:

`line 78: bytearr.extend(prefix + _coerce_to_bytes(request.body))`

`line 106: bytearr.extend(response.content)`

This would allow a user to print Request & Response while not flooding their screen with data returned in the response. If it's preferred to keep this out of Requests, I can move the code over to the utils.dump code.
",gitpraetorianlabs,Lukasa
3002,2016-02-09 20:55:19,"So this was probably introduced by shazow/urllib3#708. Looks like we'll have to revert that @shazow.

@pneumaticdeath Why Python version are you using?
",Lukasa,pneumaticdeath
3002,2016-02-10 09:59:51,"So @shazow: the problem we have here is that the connection_from_url would not pass the port explicitly. That's avoided with the pool manager (which in `connection_from_host` defaults the port appropriately) and it's avoided in requests because it uses pool manager's `connection_from_url` which _also_ defaults the port.

I think this suggests a good fix.
",Lukasa,shazow
3002,2016-04-12 09:09:25,"Ok, thank you @Lukasa !
",fsat,Lukasa
3002,2016-04-13 00:10:07,"Thanks @kennethreitz!
",fsat,kennethreitz
3000,2016-02-08 23:26:54,"@victor1969 can you please share the code you are using so we can understand what you're asking?
",kennethreitz,victor1969
3000,2016-02-08 23:33:40,"@victor1969 is this an error you saw while playing a game? Why did you decide to post about it here?
",kennethreitz,victor1969
3000,2016-02-08 23:37:19,"@victor1969 unfortunately, that has nothing to do with project. There's probably somewhere else online for you to report this, though!
",kennethreitz,victor1969
2999,2016-02-07 20:02:19,"@FlyingLotus1983 if you want a `1` there, you should provide a `1` :)
",kennethreitz,FlyingLotus1983
2998,2016-02-10 13:52:30,"@sigmavirus24 That's the workaround I intend to use, although it's a little troublesome since I'm using a library which is itself using `requests`, and it doesn't expose `trust_env`, so I'm having to patch it in. It's also troublesome because it disables proxy configuration via the environment.

Thanks though, I didn't see the other issue!
",jcmcken,sigmavirus24
2997,2016-02-05 22:51:49,"@yarikoptic thanks for the help, this should now be fixed.
",kennethreitz,yarikoptic
2996,2016-02-05 13:19:26,"Pinging @kennethreitz again for review on test code.

Thanks for this @Stranger6667!
",Lukasa,Stranger6667
2996,2016-02-05 20:34:51,"@Stranger6667 more tests for `utils` are more than welcome!
",kennethreitz,Stranger6667
2996,2016-02-05 20:58:24,"Thank you, @Lukasa @kennethreitz ! :)
",Stranger6667,kennethreitz
2996,2016-02-05 20:58:24,"Thank you, @Lukasa @kennethreitz ! :)
",Stranger6667,Lukasa
2993,2016-02-03 18:10:24,"@angelcaido19 Sorry, but this includes changes in urllib3. You'll need to propose this patch over there first, I'm afraid. =(
",Lukasa,angelcaido19
2993,2016-02-03 18:16:12,"@Lukasa Hey thanks. Help me with this: Is this ""https://github.com/shazow/urllib3"" the oficial urllib3 repo?
Thanks again!!
",angelcaido19,Lukasa
2992,2016-02-04 12:13:25,"@Lukasa @schlamar I agree I'd need to make a custom cache function. Do you think it is appropriate to make it a standalone package and vendor it under `packages`?
",lazywei,schlamar
2992,2016-02-04 12:13:25,"@Lukasa @schlamar I agree I'd need to make a custom cache function. Do you think it is appropriate to make it a standalone package and vendor it under `packages`?
",lazywei,Lukasa
2992,2016-04-16 09:00:30,"@kennethreitz @Lukasa Sorry for taking so long. I implemented a naive TTLCache for caching the `proxy_bypass` function call. Please have a look at it and let me know your thoughts.
If there is no big problem with the implementation, I'll add some documentation and testing if necessary later.

Thanks.
",lazywei,kennethreitz
2992,2016-04-16 09:00:30,"@kennethreitz @Lukasa Sorry for taking so long. I implemented a naive TTLCache for caching the `proxy_bypass` function call. Please have a look at it and let me know your thoughts.
If there is no big problem with the implementation, I'll add some documentation and testing if necessary later.

Thanks.
",lazywei,Lukasa
2991,2016-02-02 20:41:44,"- The rewrite of `test_requests.py` looks great to me. I originally wanted our test suite to be fully compatible with unittest's test runner (and it originally was), but over time this slowly became less and less the case. 
- Jython is a very rarely used Python implementation. If someone fixes a Jython-specific bug in a way that doesn't complicate the Requests codebase, I'm happy for us to accept the patch. But, our level of support should (and does) stop there. 
- I don't think having `tox.ini` is very useful, as we won't be using it in any official capacity. Use of `tox.ini` requires that every specified Python installation be available in order to be used effectively, which is quite difficult to do properly on most systems. This project shipped `tox.ini` long ago, but it was removed for a number of reasons. Tox is a wonderful local testing tool, and anyone is free to use it with Requests — that doesn't mean we need to provide the file, though. It adds (yet) another file to the root of the repository, which I would like to keep as clean as possible.
- I have little opinion regarding coverage, except to echo @Lukasa's `coverage.py` statement. If we can get rid of the `.coveragerc` file, that would be great. Again, I'd like to keep those files to a minimum.
",kennethreitz,Lukasa
2991,2016-02-02 20:52:32,"Regarding Travis: we used to utilize Travis, but I quickly grew frustrated with it reporting false negatives for our test suite on a regular basis. I was also frustrated by them randomly removing a few older versions of Python one day, as well. I was also frustrated by all the email notifications it sent me (many regarding forks of kennethreitz/requests). I'm also not a fan of its UI. :)

That was a long time ago, and I'm sure it's improved quite a bit since then. 

Our current CI integration with my Jenkins server seems to be suiting our needs perfectly fine. I personally quite enjoy having a private Jenkins server. However, I wouldn't be against considering a switch back to Travis if there was an obvious benefit to myself, @Lukasa, or @sigmavirus24. I'm not aware of any. Are there any?
",kennethreitz,Lukasa
2990,2016-02-01 21:00:14,"Yeah, this would be a 3.0.0 change. 

@sigmavirus24 I was just trying to use it directly while debugging an infinitely redirecting URL, and I had to look at the source code to figure out how to do so. I just really need to spend some time figuring out why I wrote it that way, there must have been a reason.
",kennethreitz,sigmavirus24
2989,2016-02-01 20:12:43,"@danechitoaie Let's split this into two questions.

First, the problem with OpenSSL. If you can use a third-party library, you should be able to install `PyOpenSSL`, `ndg-httpsclient`, and `pyasn1`. If the `cryptography` library that backs PyOpenSSL is linked against a newer version of OpenSSL than the bundled one (or if you're on Windows or OS X, where `cryptography` is statically linked against a modern OpenSSL).

Using oscrypto instead is extremely tricky. Requests cannot add support for this in the mainline. You could in principle add support for this in urllib3, but that won't happen quickly. PyOpenSSL is probably your best bet here.
",Lukasa,danechitoaie
2989,2016-02-01 20:35:40,"@danechitoaie PyOpenSSL and oscrypto both use the exact same backing library. PyOpenSSL has not required compilation for more than a year now.
",Lukasa,danechitoaie
2988,2016-02-03 14:27:59,"@lazywei That could definitely work. The issue there is that the cache gets busted after every _n_ calls regardless of how fast they happen or how broadly distributed they are. That seems problematic.

Another option would be to try getting a unix timestamp, dividing that by some interval (e.g. 60 seconds), and then using that as the 'counter'.
",Lukasa,lazywei
2988,2016-02-03 14:59:28,"@Lukasa Oh, thanks for your advice! Using timestamp is brilliant!
I have made the first attempt and opened the PR!

Thanks!
",lazywei,Lukasa
2988,2016-03-16 09:12:56,"@TetraEtc The pull-request is still open, I'm afraid.
",Lukasa,TetraEtc
2988,2016-06-08 17:17:33,"@Lukasa : Thanks! That´s exactly what I thought of. I tried to implement your suggestion into [robotframework-requests](https://github.com/bulkan/robotframework-requests) (see [PR 117](https://github.com/bulkan/robotframework-requests/pull/117))  but was not successful - still have a delay of approx. 5 seconds. But I am just starting out with Python and programming at all, so I am very sure that I missed something :(
",Tset-Noitamotua,Lukasa
2988,2016-06-09 11:27:13,"@Lukasa : I knew I missed somethink :). Thanks again!  I fixed it now but still see a delay :( (more details [here](https://github.com/bulkan/robotframework-requests/issues/116#issuecomment-224866699))
",Tset-Noitamotua,Lukasa
2988,2016-06-09 14:55:24,"@Tset-Noitamotua And you're entirely confident that the session is being used?
",Lukasa,Tset-Noitamotua
2988,2016-06-10 06:30:29,"Hi @Lukasa, 

We are using the [requests.Session](https://github.com/bulkan/robotframework-requests/blob/master/src/RequestsLibrary/RequestsKeywords.py#L78)

[Get request](https://github.com/bulkan/robotframework-requests/blob/master/src/RequestsLibrary/RequestsKeywords.py#L840)
",bulkan,Lukasa
2988,2016-06-10 09:26:26,"@bulkan Sorry, my question wasn't clear. I meant: are we really sure that the request that is being delayed is going through the `Session` constructed with `trust_env=False`?
",Lukasa,bulkan
2988,2016-06-23 11:24:57,"Hi @Lukasa, @bulkan 
how can I help so that we make progress on this issue. Please give me some noob friendly instructions how to verify whether the used session is the expected one :-)
",Tset-Noitamotua,Lukasa
2988,2016-06-23 11:24:57,"Hi @Lukasa, @bulkan 
how can I help so that we make progress on this issue. Please give me some noob friendly instructions how to verify whether the used session is the expected one :-)
",Tset-Noitamotua,bulkan
2988,2016-06-23 15:39:30,"@Tset-Noitamotua You'll need some way to evaluate the value of `trust_env` while the request is hanging, or about to hang. 
",Lukasa,Tset-Noitamotua
2988,2016-07-04 11:43:31,"@Lukasa  Thank you! Did as you said and it came out value of `trust_env` was string represantation  of `False` ... thus it was `'False'` (If you interessted [here](https://github.com/bulkan/robotframework-requests/issues/116#issuecomment-230270721) are more details).
",Tset-Noitamotua,Lukasa
2988,2017-02-21 19:27:58,@Lukasa Is this issue still open? Would be willing to take a shot at it using the `@functools.lru_cache` and time-stamping approach.,davidfontenot,Lukasa
2988,2017-02-22 04:22:25,@schlamar would you be able to review?,davidfontenot,schlamar
2986,2016-01-30 19:14:16,"Thanks @untitaker!
",Lukasa,untitaker
2983,2016-01-29 17:24:46,"No worries @amdei 
",sigmavirus24,amdei
2982,2016-01-27 22:46:47,"@TetraEtc that's overly general. While curl and browsers detect the `.` as the sole element of the path, they don't do anything fancy for `https://github.com/foo.` which yields a 404.

Further Requests is an HTTP client, not an oracle or a browser. We can't know what URLs to meaningfully trim/munge/etc. unless a specification tells us to do so. Even then I'd air on the side of not doing this behind the scenes for the user.
",sigmavirus24,TetraEtc
2982,2016-01-27 23:50:07,"@Lol4t0 
From the RFC you referenced (which is btw deprecated, the new one says the [same](http://tools.ietf.org/html/rfc3986#section-3.3)):



`only considered special when resolving a relative-path...`. requests does not resolve a path, it uses is as is.
",t-8ch,Lol4t0
2982,2016-01-27 23:54:08,"It have to resolve at least to perform redirects
28 янв. 2016 г. 2:51 пользователь ""Thomas Weißschuh"" <
notifications@github.com> написал:

> @Lol4t0 https://github.com/Lol4t0
> From the RFC you referenced (which is btw deprecated, the new one says the
> same http://tools.ietf.org/html/rfc3986#section-3.3):
> 
>   Although this is
>    very similar to their use within Unix-based filesystems to indicate
>    directory levels, these path components are only considered special
>    when resolving a relative-path reference to its absolute form
> 
> only considered special when resolving a relative-path.... requests does
> not resolve a path, it uses is as is.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/2982#issuecomment-175914604
> .
",Lol4t0,Lol4t0
2982,2016-01-27 23:54:49,"@Lol4t0 as @t-8ch points out, relative-references are not what requests accepts (we only accept absolute references which is what you passed us).

> It have to resolve at least to perform redirects

What?
",sigmavirus24,t-8ch
2982,2016-01-27 23:54:49,"@Lol4t0 as @t-8ch points out, relative-references are not what requests accepts (we only accept absolute references which is what you passed us).

> It have to resolve at least to perform redirects

What?
",sigmavirus24,Lol4t0
2982,2016-01-27 23:56:59,"@Lol4t0 those links work in your browser (reminder, requests is not a browser)



They don't work with curl though.
",sigmavirus24,Lol4t0
2982,2016-01-27 23:57:43,"Server can return you a relative URL as redirect target with 302 for
example. How are you going to handle that?
L
28 янв. 2016 г. 2:55 пользователь ""Ian Cordasco"" notifications@github.com
написал:

> @Lol4t0 https://github.com/Lol4t0 as @t-8ch https://github.com/t-8ch
> points out, relative-references are not what requests accepts (we only
> accept absolute references which is what you passed us).
> 
> It have to resolve at least to perform redirects
> 
> What?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/2982#issuecomment-175915919
> .
",Lol4t0,t-8ch
2982,2016-01-27 23:57:43,"Server can return you a relative URL as redirect target with 302 for
example. How are you going to handle that?
L
28 янв. 2016 г. 2:55 пользователь ""Ian Cordasco"" notifications@github.com
написал:

> @Lol4t0 https://github.com/Lol4t0 as @t-8ch https://github.com/t-8ch
> points out, relative-references are not what requests accepts (we only
> accept absolute references which is what you passed us).
> 
> It have to resolve at least to perform redirects
> 
> What?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/2982#issuecomment-175915919
> .
",Lol4t0,Lol4t0
2982,2016-01-27 23:59:25,"@Lol4t0 relative Location headers are different. We already handle relative Location headers although I don't think we've tried handling any with a `.` or `..` in them. You're changing the bug report though. User-input will not be modified. We'll investigate our handling of relative Location headers with `.` or `..` in them.
",sigmavirus24,Lol4t0
2982,2016-01-28 09:27:11,"This is a remarkably tricky issue, but in general I'm sympathetic to the idea that requests should normalise the URL as much as possible when provided by the user or when received in the Location header.

I appreciate that generally speaking we try not to manipulate the URLs provided by users _too_ extensively, but we do still play with them: we try to urlencode partially encoded URLs and generally just try to make sense of what the remote party has sent us. For that reason, I also think we should take a similar attitude to partial paths.

However, there are some reasons to be tentative here. The first is that this amounts to a major change, which would mean it has to go in to 3.0.0. The second is that if we're determined to start handling URLs ""properly"", we should aim to handle them _really_ properly. I believe @glyph is working on a URL handling library, but generally speaking I think we should be aiming to bring in something that lets us work with URLs more effectively so that we can do something that amounts to appropriate behaviour.

The upshot means that, if @sigmavirus24 agrees with me, we'll need to put this on the backburner until we get a library that can let us work with URLs more effectively.
",Lukasa,sigmavirus24
2982,2016-01-28 10:19:03,"Thanks @glyph. I'll try to play around with this at some point and see if it's useful for requests.
",Lukasa,glyph
2982,2016-01-28 10:29:28,"> Thanks @glyph. I'll try to play around with this at some point and see if it's useful for requests.

Thanks for considering this code to solve this problem, @Lukasa.  I'm optimistic you'll like it; we've been screwing up URLs real bad for a real long time in Twisted, and the design on this one really clicked together nicely.  The one part that might seem a little odd to Python programmers at first is the totally functional interface, but being able to treat URLs as immutable like the strings they came from actually ends up being really handy.
",glyph,glyph
2982,2016-01-28 10:29:28,"> Thanks @glyph. I'll try to play around with this at some point and see if it's useful for requests.

Thanks for considering this code to solve this problem, @Lukasa.  I'm optimistic you'll like it; we've been screwing up URLs real bad for a real long time in Twisted, and the design on this one really clicked together nicely.  The one part that might seem a little odd to Python programmers at first is the totally functional interface, but being able to treat URLs as immutable like the strings they came from actually ends up being really handy.
",glyph,Lukasa
2982,2016-01-28 10:40:45,"@Lukasa `URL.normalizePath() → URL with no ""."" or "".."" segments` work as a signature? Any other options which might be required? I could probably have that landed on trunk by Friday.
",glyph,Lukasa
2982,2016-01-28 10:43:08,"I think strictly we want to follow section 5 of RFC 3986. Note also that @sigmavirus24 has a library with exactly that name that may also end up a good fit here, so you may not want to leap immediately into writing that code: we need to work out exactly what we need here!
",Lukasa,sigmavirus24
2982,2016-01-28 13:35:16,"Testing rfc3986, shows that this already works there @lukasa and we had been talking about using that in the past. @glyph maybe we should try to converge our libraries
",sigmavirus24,glyph
2982,2016-01-28 23:24:26,"@sigmavirus24 - Yeah. It would be nice to decide on one or the other. However, they both have a pretty clear internal idea of what a URI is already, which would make it difficult to merge them together much.
",glyph,sigmavirus24
2980,2016-01-25 08:06:50,"@rfyiamcool No. The first case does this: `xxx?id=111`. The second case does this: `xxx?%7B%22id%22:%20111%7D`.

In future, please ask questions on Stack Overflow: the GitHub issues page is strictly for bugs.
",Lukasa,rfyiamcool
2976,2016-01-22 04:49:26,"@sigmavirus24 

Ok, I'll update.
",shoma,sigmavirus24
2975,2016-01-21 13:29:33,"@demonguy That's not how retries work by default. Generally speaking, retries only affect actual connection issues that prevent us from sending the request or retrieving the response altogether. Normally, we do not issue retries on 500 errors. You can change that behaviour by initializing your `Retry` object with `status_forcelist=range(500, 600)`, as documented on the [Retry object API document](https://urllib3.readthedocs.org/en/latest/helpers.html#urllib3.util.retry.Retry).
",Lukasa,demonguy
2975,2016-01-21 13:40:40,"@demonguy in the future **questions** are to be asked on [StackOverflow](https://stackoverflow.com/questions/tagged/python-requests). Please do not ask a question in an issue again.
",sigmavirus24,demonguy
2972,2016-01-20 09:04:14,"@sigmavirus24 We shouldn't leak sockets here: the redirect logic consumes all the data from the connection, which should return it back to the connection pool, and that happens before we raise `TooManyRedirects`.
",Lukasa,sigmavirus24
2972,2016-01-20 18:59:33,"> Got a few small notes, but this looks really good! :cake:

Yay!  Thanks a lot for the fast response! @sigmavirus24 @Lukasa 
",munro,Lukasa
2972,2016-01-20 18:59:33,"> Got a few small notes, but this looks really good! :cake:

Yay!  Thanks a lot for the fast response! @sigmavirus24 @Lukasa 
",munro,sigmavirus24
2972,2016-01-21 22:22:24,"LGTM. @Lukasa thoughts?
",sigmavirus24,Lukasa
2969,2016-01-14 15:06:22,"@ralphbean At some point, sure. We weren't generally planning on racing to new urllib3 versions, especially as really the only change in that urllib3 release is the SOCKS proxy support.

Is there a particular rush to get urllib3 1.14 into Fedora?
",Lukasa,ralphbean
2968,2016-01-13 08:16:17,"@vinsia At this time requests can only pass X509 certificates to OpenSSL from the filesystem: either a file of PEM-encoded certificates or a directory of them. If you serialize your keystore file out to PEM-encoding, you should be able to use the `verify` argument to use them.
",Lukasa,vinsia
2966,2016-01-13 06:19:34,"@glyph I think you may have hit the nail on the head there. That is a significant reason (and addresses the other issues outlined) to use a more centralized location for the cert store.
",morganfainberg,glyph
2966,2016-01-13 12:47:09,"Debian / Ubuntu have more issues. They ignore CKT_NSS_MUST_VERIFY_TRUST flag and throw all trust anchors in one PEM file. The flag overrides extended key usage (EKU) for root certs, e.g. only trust a root cert for S/MIME but not for TLS server auth.

https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=721976
https://bugs.launchpad.net/ubuntu/+source/ca-certificates/+bug/1207004

Apple, Microsoft and Red Hat/Fedora have means to disable certs for a purpose. Red Hat only puts trust anchors for TLS server auth in the default cert bundle. With FreeDesktop.org's p11kit and PKCS#11 bindings the policies can even be modified by user. This doesn't work for OpenSSL yet because OpenSSL uses a static PEM file instead of PKCS#11 to fetch trust anchors.

@glyph Does MS trust their own cert for all EKUs or just for some EKUs like code signing?
",tiran,glyph
2966,2016-01-13 18:52:47,"@alex would something like



Be a sufficiently good API?
",sigmavirus24,alex
2966,2016-01-19 23:55:18,"Echoing the Chrome policy that @dstufft referenced above, I think that maybe the right thing to do here is (for now) to trust the Windows and OS X trust stores (inasmuch as we can) since Microsoft and Apple have demonstrated a baseline level of trustworthiness in their management of certificates.

It seems like neither Debian nor Red Hat is similarly responsible, at least, at this time.

@dstufft asks: what is different about trusting the OS's cert stores than trusting the OS to properly configure OpenSSL? In the Microsoft and Apple case, they knew better than to ship OpenSSL, because it's not actually a proper transport security solution: it's part of a construction kit for building your own transport security.  They built their own, more complete TLS implementations, which have interfaces that let you do things like select trust roots in a sensible way.  In a perfect world, IMHO, we'd be using `cryptography.tls`, which would actually back-end to SChannel or Secure Transport on those platforms, and bundle its own OpenSSL only for linux.  Given that we're a long way away from that world (just in terms of the amount of work it would take), bundling OpenSSL for the protocol implementation on all platforms but trusting the platform trust store on those platforms which seem trustworthy, which hopefully _eventually_ will be all of them.
",glyph,dstufft
2966,2016-01-20 00:13:23,"@glyph well, except AFAIK Windows doesn't provide a way to either enumerate the SSL Certificates in a way that you actually get them all, not just the ones you've seen. Perhaps you an use SChannel for that (is that a thing you can do? last time I saw this someone was trying to replace urllib3 with WinHTTP to get requests to trust the windows trust store), but then if you're relying on SChannel you're also relying on Microsoft for some other things which might be undesirable. It'd mean that there's no more SNI for Windows XP users, no ECDHE+AESGCM for anyone not on Windows 10, 

As far as I know, the only way to get ideal security right now on any platform in a way that isn't tied to the absolute latest versions of that platform is to bring your own TLS library and your own certificates.
",dstufft,glyph
2966,2016-01-22 01:13:44,"Debian and CentOS aren't broken. On both systems the configuration works fine with SSL_CTX_set_default_verify_paths(). Only one of both CAfile and CApath must work for SSL_CTX_set_default_verify_paths(). The patch tried to outsmart the system and work around the default API. Also requests can't handle CAfile and CApath at the same time. The combination caused the bug.

I understand @dstufft motivation for the heuristic. OpenSSL doesn't report if SSL_CTX_set_default_verify_paths() has loaded any certs or has found a CApath with valid certs. For CAfile SSLContext.get_ca_certs() helps but for CApath you are lost.

certifi isn't an option. At best it's a matter of last resort for pip. 
",tiran,dstufft
2966,2016-01-22 08:17:39,"@glyph Certitude can get this right by ignoring what the distro OpenSSL tells us and doing what curl does instead, which is to hardcode the list of paths and walk down them, one-by-one, until we find the bundle.
",Lukasa,glyph
2966,2016-01-28 13:43:17,"@shypike Let's clear some things up. =)

Firstly, requests can and does verify certificates out of the box, and has been doing so for years: much longer than the standard library, and much more effectively. The question is not ""should we verify certificates"", it's ""how should we verify certificates"".

Secondly, no, this problem is not solved in Python 2.7.10+. In those versions of Python, the standard library modules use the `SSLContext` object returned from `create_default_context`. That function calls [`load_default_certs`](https://docs.python.org/2/library/ssl.html#ssl.SSLContext.load_default_certs).

Unfortunately, it turns out that `load_default_certs` is a relatively deficient method. It suffers from some flaws.

Firstly, it only handles two cases: Windows, and all other platforms. On Windows it uses the `enum_certificates` method, which ends up calling into `CertEnumCertificatesInStore`. It grabs those certificates and passes them into OpenSSL to do the verification. This has two problems: firstly, it means that the certificate verification logic is _different_ between Python and, for example, Microsoft Edge, because a different library is used to construct the certificate chain and validate the certificates. Secondly, not all root certificates will be available to Python when this method is called, because SChannel occasionally _dynamically fetches_ root or intermediate certificates on-demand. This leads to unexpected, tricky behaviour.

One of the ""other platforms"" that Python ignores is Mac OS X. On this platform, everything is sad again, because Mac OS X has a complex keychain system that allows substantial editing of trust. However, OpenSSL pays absolutely no attention to this distinction, which means that a user will be in one of three cases:
1. They'll be using the built-in OpenSSL, OpenSSL 0.9.8zg. This version calls into the OS X cert store to do the validation, so it's the most likely to get the right answer, but it's also unsupported and by-default insecure and should never be used.
2. They'll be using an OpenSSL shipped with Python. If they do this, it's not clear where that OpenSSL will get its certificates from: probably nowhere.
3. They'll be using an OpenSSL installed from Homebrew or somewhere equivalent. These places usually copy all the certificates out of the keychain _at the time of install_, but then do no refreshing of the cert store. This means that if OS X subsequently pushes out an update removing one of the root certificates from the store, this change will not propagate to the verifier, which will continue to trust the cert.

Additionally, as in the Windows case, OS X's Security.framework has a very complex and different validation logic than OpenSSL has. For example, users can mark certificates as trusted for only specific websites, or trusted only for specific uses: OpenSSL will not respect those distinctions.

As a result, the only platforms on which this is ""solved"" by the standard library are ones that ship OpenSSL by default, compile it such that `load_default_verify_locations` points to the right places, and that appropriately configure their trust stores. That amounts to meaning some Linuxes, and some other Unix-based OSes. That is officially not good enough.

Right now requests is doing better than the standard library because it carefully polices trust on _all_ platforms, not just the ones the standard library deigns to support. This issue is about whether we can do _even better_ by allowing the platform-specific logic to take over on Windows/OS X, and then as a result transitioning to use the default certificate stores on all platforms.

To be clear: we will _never_ take a decision here that leads to a preference for Unix-like operating systems. That kind of parochial thinking is what leads to Windows users feeling like second-class citizens in the Python community. My current thinking here is that I'd like to emulate Chrome: use OpenSSL to perform the actual TLS, but have the platform-native libraries do the validation if possible. That gets us the closest to ""native"" behaviour on the platform in question.
",Lukasa,shypike
2966,2016-01-28 14:45:29,"@untitaker Yes, we can, and so I'm not worried about Linux. Linux is the easy case here. It's everything else that is tricky.
",Lukasa,untitaker
2966,2016-01-28 16:20:27,"@Lukasa Thanks for your explanation. SSL and certificates are even more of a minefield than I already thought.
My ""simple"" problem is that _requests_ rejects quite a few valid certificates, which are accepted by Python-urllib2, Firefox, IE and Chrome. Installing _certifi_ doesn't resolve this issue.
",shypike,Lukasa
2966,2016-01-28 16:24:52,"@shypike My psychic powers tell me that your OpenSSL is pretty old. Take a read through of the discussion on certifi/python-certifi#26.
",Lukasa,shypike
2966,2016-01-28 16:37:06,"@Lukasa Maybe. Using 2.7.11 on Windows and forced to use eGenix.com's pyopenssl binaries (because those are the only ones that don't crash on me). They are at OpenSSL level 1.0.1q
Even so, I think further discussion of my specific problem doesn't contribute much to the larger issue being discussed here. Thanks for your time.
",shypike,Lukasa
2966,2016-01-29 14:47:29,"On Twitter, @Lukasa responded to my suggestion that requests might punt on this problem by using libcurl as a transport, presumably via pycurl. @Lukasa's concern is that depending on libcurl and pycurl would make requests more difficult to install. Using libcurl is probably off the table anyway; @glyph was right to point out the inherent security risk in such a complex C library. Since @Lukasa pointed out that pycurl is only distributed as source and as a .exe installer for Windows, and not as a wheel for Windows and OS X, I think we can eliminate it from discussion.

I like @Lukasa's idea of using the native APIs on Windows and OS X to do the certificate verification, and configuring OpenSSL to use those APIs through a callback. Does pyOpenSSL support this yet? Of course, to use the appropriate APIs on Windows and OS X, it would be necessary to use ctypes, cffi, or a C extension module. Given that the latter is problematic with PyPy, we can probably eliminate that option. pyOpenSSL already uses cffi (indirectly, via cryptography), so that's probably the best option. So would we create bindings for the certificate verification APIs of Windows and OS X as separate Python packages, and add those packages to requests' extras_require list?
",mwcampbell,glyph
2966,2016-01-29 14:47:29,"On Twitter, @Lukasa responded to my suggestion that requests might punt on this problem by using libcurl as a transport, presumably via pycurl. @Lukasa's concern is that depending on libcurl and pycurl would make requests more difficult to install. Using libcurl is probably off the table anyway; @glyph was right to point out the inherent security risk in such a complex C library. Since @Lukasa pointed out that pycurl is only distributed as source and as a .exe installer for Windows, and not as a wheel for Windows and OS X, I think we can eliminate it from discussion.

I like @Lukasa's idea of using the native APIs on Windows and OS X to do the certificate verification, and configuring OpenSSL to use those APIs through a callback. Does pyOpenSSL support this yet? Of course, to use the appropriate APIs on Windows and OS X, it would be necessary to use ctypes, cffi, or a C extension module. Given that the latter is problematic with PyPy, we can probably eliminate that option. pyOpenSSL already uses cffi (indirectly, via cryptography), so that's probably the best option. So would we create bindings for the certificate verification APIs of Windows and OS X as separate Python packages, and add those packages to requests' extras_require list?
",mwcampbell,Lukasa
2966,2016-01-29 14:58:56,"@mwcampbell PyOpenSSL does _not_ support this, and never will, because PyOpenSSL is a thin wrapper library around OpenSSL, and so doesn't support the relevant APIs.

However, I recently got merged into cryptography the relevant bindings for OS X (pyca/cryptography#2683), and I believe that a cryptography with that change in it has been released now. I've also used those bindings to successfully use OS X to validate a certificate chain. A similar approach can probably be used on Windows and the cryptography developers have expressed a willingness to bind the appropriate functions. Given that PyOpenSSL depends on cryptography, this is far and away the simplest route.

My current proposal is to add the relevant functionality into the urllib3 PyOpenSSL shim. I've briefly discussed this with @shazow, who was open to the idea. Then, urllib3 would allow `urllib3.contrib.pyopenssl.inject_into_urllib3()` to take a parameter (`system_trust=True`, defaulting to `False`) that will automatically use the system trust store instead of OpenSSL on the relevant platforms.

The question then would become whether the requests project can come to consensus on setting that parameter to `True`. =) We should burn that bridge when we get to it: for now, I'd like to get the building blocks in place.

In an ideal world we'd actually pull the OS-specific logic out into its own library, so that it can be meaningfully tested, but that's a pretty tricky goal. On the other hand, if we can pull it off then we have both provided a really useful service to the Python community in general _and_ potentially helped move towards #2118 by providing a PyOpenSSL `SSLContext` equivalent. This I think would be my preferred outcome.
",Lukasa,mwcampbell
2966,2016-01-29 20:22:39,"@mwcampbell because that is a requirement of this library. C compilation is rarely a seamless experience and is the #1 source of end-user confusion/frustration when it comes to package installation. It would come up very often, especially for a library as popular as Requests. You shouldn't need to compile C code to make HTTP Requests properly. 

No dependencies is simply a design decision. One that could be rethought if needed, but again, it's about providing a simple and carefully crafted user experience, beginning to end. Of course, Pip works extremely well, and dependencies are very standard fare, but, Requests aims to be better than the status quo. You shouldn't need 5 dependencies to make HTTP requests — you should need 1. :)
",kennethreitz,mwcampbell
2966,2016-01-29 20:25:08,"@mwcampbell External dependencies could result in cases of version conflicts. One application could require `pyOpenSSL` in a specific version, while `requests` needs another. In some cases, (mostly enterprise) you're locked to older versions due to dependent code. Reworking that code for a newer version could be tedious or impossible, depending on what else depends on that version. (Especially where OpenSSL is involved)
",smiley,mwcampbell
2966,2016-01-29 22:28:13,"I also agree with @kennethreitz.  This is a laudable goal for any library, but given that `requests` is the transport for how you _get_ other libraries, it's particularly important here.  However, ""no compilation"" does not need to mean ""no C code"" any more, with the advent of the https://github.com/manylinux organization, and the possibility of allowing of linux wheels on PyPI.

Compilation on OS X requires installation of the Xcode command line tools, which requires interactive user acceptance of an EULA, and the error messages that users receive when these tools are _not_ present are totally inscrutable.  Part of the problem here rests with `pip`, of course; it should determine if a package needs compilation and just straight up tell you `""run xcode-select --install""` and not barf out a traceback about a missing executable.
",glyph,kennethreitz
2966,2016-01-30 00:16:35,"@smiley - If you have extracted this information from Chrome, perhaps you could comment on [my Stack Overflow question](https://stackoverflow.com/questions/34732586/is-there-an-api-to-pre-retrieve-the-list-of-trusted-root-certificates-on-windows), for posterity?
",glyph,smiley
2966,2016-01-30 08:38:17,"@smiley With OS X I plan to hook into the urllib3.contrib.pyopenssl module and interfere with `ssl_wrap_socket`. My PoC just waited until after the handshake to do the verification, so you can do that too and then provide us with the PoC code. In the proper code it would be better to hook into the verify callback so that we can fail the handshake, but that is a bit trickier.
",Lukasa,smiley
2966,2016-02-10 01:30:31,"@glyph an excellent decision :) (although, an unfortunate one)
",kennethreitz,glyph
2966,2016-04-25 13:08:02,"Finally got around to doing the Windows implementation, and it looks like the configurable callback in PyOpenSSL's [`Context.set_verify(mode, callback)`](https://pyopenssl.readthedocs.org/en/latest/api/ssl.html#OpenSSL.SSL.Context.set_verify) will be a good place to implement this. (as @Lukasa suggested) The only downside is that this will force Windows users to install `ndg-httpsclient`, but a simple `pip install ndg-httpsclient` worked perfectly, so that might not be so bad.

However, I noticed the PR in urllib3 and certitude. Should I implement it there somehow, or directly in Requests?

**EDIT:** I just noticed the native part of Certitude. Guess there's already an implementation. Is this still needed or just waiting on `urllib3` to merge?
",smiley,Lukasa
2966,2016-04-25 15:06:46,"@smiley It's waiting on a few other things. The native part of certitude needs a setup for wheel builders, which I haven't gotten around to providing yet.
",Lukasa,smiley
2966,2016-04-25 15:50:23,"@Lukasa It's on my list! (But you should remind me on occasion)
",reaperhulk,Lukasa
2966,2016-04-25 18:31:09,"@piotrjurkiewicz It won't require compilation on install, we'll distribute wheels. =)

And generally speaking ctypes doesn't behave well on PyPy (it's very slow), and PyPy is a first-class supported platform for requests. =)
",Lukasa,piotrjurkiewicz
2966,2016-04-26 22:20:47,"@glyph 

> This issue has been really illuminating for me. But what do you mean by ""disastrous""? It sounds like you think the implementation will just fail to work somehow?

I just don't like anything about it, and it raises every red flag in my book. It will take a currently very simple implementation of very reliable behavior, and replace it with very a complicated implementation (perhaps just complex — I personally avoid this layer at all costs, hence the simplicity of the current implementation, so this all seems quite complicated to me) with behavior that I fear may be far less reliable, in the name of security.

So ensuring the same level of reliability is my number one concern. Zero user-facing changes to installation behaviors are my second concern (NO compilation acceptable, and external dependencies should also be absent). My third concern, which is a large one, is that Requests will now behave differently on different systems — avoiding this was an explicit design decision when I designed it.

So, my reservations are large, and many. But, I'm open to it, tentatively. Everyone else cares about security far more than I do. I want to support that, but not at any expense to the above mentioned aspects of this project. 
",kennethreitz,glyph
2966,2016-04-27 02:21:39,"@dstufft if that's true, perhaps we can rig it up similar to the current auto-use of PyOpenSSL, if available. 

Taken further, could be a package like `requests-systemcerts`, included in `requests[security]`.
",kennethreitz,dstufft
2966,2016-05-20 19:48:23,"@dstufft how does this sound to you?
",Lukasa,dstufft
2966,2016-05-20 19:56:28,"@tmehlinger build-time/install-time dependency is not a actual runtime dependency
",sigmavirus24,tmehlinger
2966,2016-05-20 19:59:52,"@tmehlinger I don't want to sound rude, but your assumptions don't hold. Your suggestion works under very limited conditions: OpenSSL on Linux/BSD that have single CA cert PEM file that contains only trust anchors for TLS web server authentication. Some operating systems have a cert directory, Windows and OSX have different sources for trust anchors. Certificates can come from other sources like PKCS#11 providers or a NSS database. You also must ensure that the CA file contains _only_ trust anchors for TLS server auth. Otherwise you will run into big trouble.
",tiran,tmehlinger
2966,2016-05-20 20:01:26,"@sigmavirus24 Eh, for that I'd defer to [Ten Immutable Laws Of Security (Version 2.0)](https://technet.microsoft.com/en-us/library/hh278941.aspx), particularly:
- Law #1: If a bad guy can persuade you to run his program on your computer, it's not solely your computer anymore.
- Law #2: If a bad guy can alter the operating system on your computer, it's not your computer anymore.
- Law #3: If a bad guy has unrestricted physical access to your computer, it's not your computer anymore.

Once you install someone's software it's not solely your computer anymore.
",dstufft,sigmavirus24
2966,2016-05-20 20:05:50,"@tiran, I don't think you're rude, but I don't see how that matters. The current implementation already assumes OpenSSL on Linux or BSD by virtue of return the path to a PEM bundle. While I concede that I don't have complete solution, I think it is an _improvement_.
",tmehlinger,tiran
2966,2016-05-20 20:11:54,"@tmehlinger if a dependency adds a dependency on one of these plugins (for whatever reason) and that dependency is hacked, then so are you. Besides, how do determine which of a potential myriad of these types of plugins you use? Just always pick the first?
",sigmavirus24,tmehlinger
2966,2016-05-20 20:18:48,"I think you're responding to the comment I pulled because I second-guessed myself. :)

Regardless--I agree with @dstufft and I'd argue that it's the consumer's job to ensure they're installing sane packages. I don't think we're any less safe without this change; someone could just as easily upload their own fork of requests (many of which already exist) with a malicious PEM bundle and unsuspecting users would be compromised.

I don't deny that it's an attack vector. I also argue that it's no worse than the current situation, which includes a multitude of system administrators like me forced to forego certificate validation just to make everything will work.
",tmehlinger,dstufft
2966,2016-05-20 20:24:02,"@tiran I like that
",tmehlinger,tiran
2966,2016-05-20 20:46:01,"@coderanger I think that's correct.
",dstufft,coderanger
2966,2016-05-20 20:57:01,"@coderanger @dstufft meh, you are right. It's good enough for the simple cases.

For proper SChannel support we would have to provide a SSLContext-like object that wraps SChannel.
",tiran,coderanger
2966,2016-05-20 20:57:01,"@coderanger @dstufft meh, you are right. It's good enough for the simple cases.

For proper SChannel support we would have to provide a SSLContext-like object that wraps SChannel.
",tiran,dstufft
2966,2016-05-20 21:10:46,"@dstufft yeah, I've been planning to propose such an ABC for some time. I'll talk about it at the language summit

@coderanger There is another option. OpenSSL has an internal API to look up certs by subject or issuer. For example a builtin X509_LOOKUP loads certs for the CA directory and CA file. I believe that it is possible to write a Windows X509_LOOKUP for crypt32.dll CAPI to fetch certs from the system store, convert it to an OpenSSL X509 object and inject it into the X590_STORE of the current SSL_CTX. Additional trust flags can be put in the AUX section of the cert. I haven't figured out if CertFindCertificateInStore() triggers a download of missing trust anchors.
",tiran,coderanger
2966,2016-05-20 21:10:46,"@dstufft yeah, I've been planning to propose such an ABC for some time. I'll talk about it at the language summit

@coderanger There is another option. OpenSSL has an internal API to look up certs by subject or issuer. For example a builtin X509_LOOKUP loads certs for the CA directory and CA file. I believe that it is possible to write a Windows X509_LOOKUP for crypt32.dll CAPI to fetch certs from the system store, convert it to an OpenSSL X509 object and inject it into the X590_STORE of the current SSL_CTX. Additional trust flags can be put in the AUX section of the cert. I haven't figured out if CertFindCertificateInStore() triggers a download of missing trust anchors.
",tiran,dstufft
2966,2016-12-06 17:33:17,@tiran I'm under the impression it will. It's a far more reliable way to handle certificates on Windows than what Python standard library ssl uses.,nanonyme,tiran
2966,2016-12-26 01:24:57,"Talking to @dstufft  about something else, it occurred to me that PEP 493 describes a  `/etc/python/cert-verification.cfg` file that can be used to opt-in to stdlib cert validation as the default on older Python 2.7 releases. Since a large part of the Linux problem here is the difficulty of reliably detecting where the system cert stores are located (even when using a custom `SSLContext`, perhaps it would make sense to define an addition to that file that lets the distro tell Python applications definitively where to find them?",ncoghlan,dstufft
2966,2016-12-28 08:48:50,"@Lukasa Now the question would be *how* to define such an extension (since that file is currently a relatively ad hoc solution to the PEP 476 backport problem).

@tiran has talked about extending verified-by-default support to protocols other than HTTPS in the past, so perhaps it would make sense to pursue as a stdlib config format definition for Python 3.7+ first, and then folks could look at making it available earlier for the benefit of third party libraries?",ncoghlan,tiran
2966,2016-12-28 08:48:50,"@Lukasa Now the question would be *how* to define such an extension (since that file is currently a relatively ad hoc solution to the PEP 476 backport problem).

@tiran has talked about extending verified-by-default support to protocols other than HTTPS in the past, so perhaps it would make sense to pursue as a stdlib config format definition for Python 3.7+ first, and then folks could look at making it available earlier for the benefit of third party libraries?",ncoghlan,Lukasa
2966,2016-12-30 13:18:10,"@djmattyg007 That's pretty standard for all distro-provided packages, but downstream patches don't have any effect on the behaviour of upstream components installed with pip (whether to the user site-packages, into a currently active virtual environment, or into the virtual environments implicitly created by tools like `pipsi`)",ncoghlan,djmattyg007
2965,2016-01-11 12:13:39,"@swmerko You have to route your request through the Session object. In your code example you create a Session object `s`, but then call `requests.get`. That's not how Session objects work. Change your last line to `s.get()` instead of `requests.get()` and your problem should be solved.
",Lukasa,swmerko
2963,2016-01-10 02:21:11,"@anarcat why should we track an issue in a dependency?
",sigmavirus24,anarcat
2963,2016-01-10 10:50:54,"@sigmavirus24 is right. There is an issue open tracking this problem. We have zero code change required to use that fix, all we have to do is update our dependency (which we'll do in the next 2.X release just as part of the release process). The correct issue to track is the one on urllib3.
",Lukasa,sigmavirus24
2962,2016-01-06 22:23:56,"@Djokx are you using the requests library in your project? This is a repository for a Python library called requests, not a repository to request help for arbitrary problems. If you are using this library, could you share some code so we can help you debug whatever problem it is that you're running into? It's not apparent what this has to do with this library at the moment.
",sigmavirus24,Djokx
2961,2016-01-05 17:23:19,"\o/ Thanks @whit537! :sparkles: :cake: :sparkles:
",Lukasa,whit537
2961,2016-01-05 17:26:55,"Huzzah, thanks! :-)

@Lukasa++
",whit537,Lukasa
2961,2016-01-05 17:37:52,"@Lukasa please add release notes for this. I think it may be a good idea to have some shim for this as people may be catching `InvalidSchema` or `MissingSchema` exceptions and they won't be able to catch those anymore in 3.0
",sigmavirus24,Lukasa
2961,2016-01-06 18:22:07,"@whit537 yeah, it isn't _necessary_ for 3.0 though and there's no way to tell people to move to those new messages unless we forcibly break things for them though.

What we could do is add `InvalidScheme` and `MissingScheme` as sub-classes of their now defunct relatives so that people can start (with 2.10.0) using those exceptions so they're future proofed for 3.0.0. I'm not sure if @Lukasa would agree that there is value in that though.
",sigmavirus24,Lukasa
2961,2016-01-06 18:22:07,"@whit537 yeah, it isn't _necessary_ for 3.0 though and there's no way to tell people to move to those new messages unless we forcibly break things for them though.

What we could do is add `InvalidScheme` and `MissingScheme` as sub-classes of their now defunct relatives so that people can start (with 2.10.0) using those exceptions so they're future proofed for 3.0.0. I'm not sure if @Lukasa would agree that there is value in that though.
",sigmavirus24,whit537
2960,2016-01-05 17:04:04,"Thanks @whit537. Unfortunately, this is a breaking change so we can't accept it until 3.0.0: can you adjust this PR to point to the proposed/3.0.0 branch instead please?
",Lukasa,whit537
2958,2016-01-04 16:55:46,"> Does equality not fall back on identity? Is it not possible to simply confirm that it was called with a specific auth handler?

@Lukasa you're correct. That said it's not exactly simple. Consider a poorly written API wrapper where someone does something like:



Obviously if ^ were better written it'd be easier to test the way you describe, but we know nothing about @Malizor's case (or anyone else's) and we always get push back for encouraging better design/testing practices.

> the python 2.7 documentation states ...

Ah, I forgot that the always ancient Python 2 has silly things like this.
",sigmavirus24,Malizor
2958,2016-01-04 16:55:46,"> Does equality not fall back on identity? Is it not possible to simply confirm that it was called with a specific auth handler?

@Lukasa you're correct. That said it's not exactly simple. Consider a poorly written API wrapper where someone does something like:



Obviously if ^ were better written it'd be easier to test the way you describe, but we know nothing about @Malizor's case (or anyone else's) and we always get push back for encouraging better design/testing practices.

> the python 2.7 documentation states ...

Ah, I forgot that the always ancient Python 2 has silly things like this.
",sigmavirus24,Lukasa
2958,2016-01-04 17:13:00,"Well my specific use case is similar to your example @sigmavirus24. It is a wrapper except it actually does some things before (build params depending on args…) and after (error handling, result formatting).

@Lukasa : HTTPBasicAuth and HTTPProxyAuth just add a constant header on each request, so equality should not cause any problem here.

HTTPDigestAuth is indeed more complicated with it's hook system. Perhaps I should only compare the username and password?
",Malizor,Lukasa
2958,2016-01-04 17:13:00,"Well my specific use case is similar to your example @sigmavirus24. It is a wrapper except it actually does some things before (build params depending on args…) and after (error handling, result formatting).

@Lukasa : HTTPBasicAuth and HTTPProxyAuth just add a constant header on each request, so equality should not cause any problem here.

HTTPDigestAuth is indeed more complicated with it's hook system. Perhaps I should only compare the username and password?
",Malizor,sigmavirus24
2958,2016-01-04 17:20:23,"@Malizor you could just do



And then


",sigmavirus24,Malizor
2958,2016-01-30 03:08:24,"@Malizor this seems like a fine usecase to me. I'm going to merge this in, then simplify the digest check to simply verify the parameters: username, password. 

In other news, I hadn't looked at that digest code in a long time! I had a blast writing that long ago. So many memories. 
",kennethreitz,Malizor
2956,2015-12-31 18:16:37,"@aewhatley requests is an _HTTP_ client and has nothing to do with _HTML_. POST requests are made to URLs, not HTML elements. As such, we would not be adding any such methods because they wouldn't make sense as part of our API.

Thanks for the suggestion but I think you're a little confused about how requests works.
",sigmavirus24,aewhatley
2955,2015-12-30 12:27:17,"@yhorian1 Aha, there we go!

So, the problem here is almost certainly that your OpenSSL is too old. PHP is likely succeeding because it doesn't verify certificates at all or because it's not using OpenSSL.

OpenSSL versions less than 1.0.2 don't backtrack when trying to build up certificate chains, which means that if it encounters a cross-signed certificate it is unable to check whether or not it has a non-cross-signed version of that certificate in the trust store. That's what's happening in this case I suspect: Let's Encrypt is currently a cross-signed CA.

Do you mind providing a specific URL so I can validate my suspicion?
",Lukasa,yhorian1
2955,2016-05-27 15:50:49,"@Lukasa You are absolutely right, server config was broken. I fixed it and now it works fine - thanks!
",msztolcman,Lukasa
2954,2015-12-30 12:04:52,"@yhorian1 Your bug is in httplib2. This is not httplib2.
",Lukasa,yhorian1
2953,2015-12-30 23:21:28,"@Lukasa what do you think about having a `socks` extra for requests that installs `PySocks` and whatever else?
",sigmavirus24,Lukasa
2953,2016-04-05 10:04:39,"@Lukasa is this was merged or not? Btw, how to configure the socks proxy?
",loretoparisi,Lukasa
2953,2016-04-05 10:23:01,"@Lukasa so it should work like


",loretoparisi,Lukasa
2953,2016-04-05 12:05:05,"@Lukasa uhm, so supposed to use on google appengine, I have to wait the `devappserver2` python sdk update...
",loretoparisi,Lukasa
2953,2016-04-06 19:41:56,"@kennethreitz I'm away from my laptop, so if you're in a merging mood you can land the urllib3 update and then merge this.

Please note that it makes everyone's life easier if you merge in urllib3 directly from the brand new tag (1.15). Just a suggestion!
",Lukasa,kennethreitz
2951,2015-12-29 17:35:31,"Hey @vkosuri what is the library providing ""RequestsLibrary/RequestsKeywords.py"" in your traceback? It seems like you're using something else that calls requests. If you have a link to that source, I'd like to inspect that as well.
",sigmavirus24,vkosuri
2950,2015-12-29 11:35:27,"@yoyoprs Sure it is.


",Lukasa,yoyoprs
2950,2015-12-29 11:56:39,"@yoyoprs That's because that's the defined behaviour of requests. This is why I have suggested using file objects directly, or via the wrapping proxy defined above.
",Lukasa,yoyoprs
2950,2015-12-29 12:20:30,"@Lukasa I do not understand the usefulness of the wrapper (except for progression).

My transfer is not streamed, (except if I control the sending of blocks in the proxy object, we agree) but the server does not receive header that tells it that the transfer is chunked. Should I force the header before send the request? Without this header, it will be complicated for components like uwsgi to understand the request and to avoid content buffering (although in my case for now).

It is not stated anywhere in the documentation it is not possible to stream with post method the contents of a file (using a generator or not) and I still do not understand why Flask (without nginx and uwsgi) can not read the stream
",yoyoprs,Lukasa
2950,2015-12-29 12:37:11,"@yoyoprs I don't understand what you're talking about.

A file object passed to the `data` argument _is_ streamed, as the documentation [clearly states](http://docs.python-requests.org/en/latest/user/advanced/#streaming-uploads). The exact chunk of code you posted above streams the file.

In this context, what we mean by _streamed_ is that Requests does not load the file entirely into memory: instead, we loop over the file sending 8kB worth of data with each send to the socket.

What you're talking about is not streaming, it is _chunked encoding_. Chunked encoding is a different notion: it refers to the way the data is constructed and sent on the wire. Once again, it is possible to do this: sending a generator does that exactly. This is why I believe your problem is not with requests: I believe that someone in your _server_ stack is adding in a content-length header. Requests most certainly is not. To prove it, let me use the following script to upload some data to http2bin:



I captured the data I sent using Wireshark, which records _exactly_ the bytes on the wire, without any application processing. This says my request looked like this:



This is a chunked transfer encoded body, as shown by the headers. There is no content length. The body is correctly encoded.

Requests is _not_ getting this wrong. This is why I asked about your headers.

Note, however, that WSGI provides no specific guidance on how a WSGI server should handle chunked uploads. Therefore, I recommend you look at the interface between uWSGI and Flask.
",Lukasa,yoyoprs
2950,2015-12-29 13:22:25,"@Lukasa sorry for my broken English, on the flask side issue I mention ""chunked"" and not on this one, my mistake.
What interests me is to successfully write directly content of chunked transfer on Flask side and so now I know it is not ""requests"" the problem (i was redirected here ...)

There is a good chance that Flask adds this header, but it does not explain why without uwsgi interface (with it works), it is impossible to read the stream (Flask would require the Content-Length to start streaming ? Maybe ...)

I will make several tests (without/with uwsgi interface) and try this: http://uwsgi-docs.readthedocs.org/en/latest/Transformations.html#streaming-vs-buffering

Thank you for your patience really.
",yoyoprs,Lukasa
2950,2015-12-29 13:32:39,"@yoyoprs do I understand correctly that we can close this now?
",sigmavirus24,yoyoprs
2950,2015-12-29 13:33:17,"@sigmavirus24 yes you can :)
",yoyoprs,sigmavirus24
2950,2015-12-29 13:35:40,"Okay @yoyoprs. I'll be in #python-requests on chat.freenode.net (IRC network) if you want to some one to chat with about this while you're debugging.
",sigmavirus24,yoyoprs
2950,2015-12-30 12:15:31,"@Lukasa Just a question off issue ... with this method:



Do you have a bug when the file size is 0 byte ?
",yoyoprs,Lukasa
2950,2015-12-30 13:08:40,"@yoyoprs This is _not_ normal. The way to signal the end of a chunked body is with a zero-length chunk (specifically, the bytes `0\r\n\r\n`). Requests has done that: the body is signaled to be complete, and it just happens to have zero length.

nginx _must_ cope with this, it's a specification violation for it not to.
",Lukasa,yoyoprs
2950,2015-12-31 13:42:57,"@Lukasa I read that it was not possible (without generator) to define the size of sent blocks during upload (due to a httplib limitation). Have I understood correctly or is there a way via a wrapper to do this?
",yoyoprs,Lukasa
2950,2015-12-31 14:25:59,"@yoyoprs a couple things:
1. Issues are for defect tracking, not general Q&A fora. If you have questions on how to use this project, please ask them on [StackOverflow](https://stackoverflow.com/questions/tagged/python-requests)
2. httplib only reads files in sizes of 8192 **bytes** and sends the data it did read to the server. If you wish to force httplib to read larger amounts, you have to either monkey-patch httplib or create an object that ignores the size of the read. Neither option is very good.
3. I'm locking this issue to prevent further Q&A type conversation on this thread
",sigmavirus24,yoyoprs
2949,2016-06-14 07:21:32,"@jtherrmann If this is an auth header, the easiest way to work around the problem is to set a session-level auth handler that simply always puts the header you want on the request.
",Lukasa,jtherrmann
2949,2016-07-22 14:08:02,"@ethanroy No additional consideration beyond my suggestion of using a session-level auth handler, in the comment directly above yours.
",Lukasa,ethanroy
2949,2016-09-08 12:02:13,"@GregBakker Yes, ish. It's a confluence of intended behaviours. However, this bug notes that the original 403 shouldn't happen.
",Lukasa,GregBakker
2949,2016-09-14 21:51:51,"@Lukasa when you say ""the easiest way to work around the problem is to set a session-level auth handler,"" is that something that works today?  Based on what I'm seeing in the code, the answer is no but your wording makes me wonder if I'm missing something.  You're talking about setting the Session auth attribute, right?
",reteptilian,Lukasa
2949,2016-10-25 23:22:30,"@jwineinger so how did you end up getting around this problem? it still seems to behave the same.
",eatm1,jwineinger
2947,2015-12-28 18:47:14,"So @sigmavirus24, are you thinking that we should attempt to release 3.0.0 sometime in January including a fix for this issue?
",Lukasa,sigmavirus24
2947,2016-09-06 00:07:58,"Thanks for fixing this @nateprewitt!
",sigmavirus24,nateprewitt
2946,2015-12-26 01:25:41,"@Lukasa I would prefer not to but if you under the API var put your email address you should hit the same issue I am having as long as you are not in any of the breach lists
",L1ghtn1ng,Lukasa
2946,2015-12-26 11:30:11,"@L1ghtn1ng So in both Python 3 and Python 2 I find that the response has no body if there have been no breaches. As a result the JSON decoders fail to decode correctly. You should check the status code (`res.status_code`) before using `r.json()`.
",Lukasa,L1ghtn1ng
2942,2015-12-21 09:49:25,"Thank you @Lukasa . My mistake, I thought requests has to read it all into memory before send to network.In fact requests just ""stream"" it, which is cool.Thank you!
",jchluo,Lukasa
2942,2015-12-21 14:24:38,"@jchluo in the future, please do **not** use the **issue** tracker for **questions**. Use [StackOverflow](https://stackoverflow.com/questions/tagged/python-requests).
",sigmavirus24,jchluo
2941,2015-12-20 10:15:40,"Hi @alison985!

Currently our relatively minor code of conduct is [defined here](http://docs.python-requests.org/en/latest/dev/contributing/). There is interest amongst some of the maintainers in moving to something based on the [Contributor Covenant](http://contributor-covenant.org/), given that both myself and @sigmavirus24 maintain personal projects that use it, but I'm unwilling to change that by fiat: I'd want consensus from the whole team.

Regardless, I believe that the Contributor Covenant _implicitly_ applies to this project, and certainly that's the standard to which the maintainers hold themselves.
",Lukasa,alison985
2940,2015-12-19 20:36:07,"@vitorbaptista Out of interest, why do you care if the response object is closed?
",Lukasa,vitorbaptista
2939,2015-12-19 16:05:51,"@L2501 To be clear, we _could_ handle this case. I just don't think we should. The server is very wrong, and it is not our job to work around the failures of that server. In this instance there is a clear acceptable solution, but the general case is not solve able. We can handle this, and we're looking at how best to do that, but if you want to find someone to blame (and your tone suggests you do) then I recommend directing your blame at the website operator whose website is broken, rather than us.
",Lukasa,L2501
2939,2015-12-19 16:39:33,"Yes, @L2501. Please continue to act aggressively towards us for not having encountered such a poorly configured server. Please don't take any action to correct that server.

---

@Lukasa There was a proposed version of the Header Dict that ignored multiple headers for anything other Set-Cookie headers. We could fix this faster in requests by using `.getlist` on the original Header Dict from urllib3 and throwing an exception if it returns more than one value.
",sigmavirus24,L2501
2939,2015-12-19 16:39:33,"Yes, @L2501. Please continue to act aggressively towards us for not having encountered such a poorly configured server. Please don't take any action to correct that server.

---

@Lukasa There was a proposed version of the Header Dict that ignored multiple headers for anything other Set-Cookie headers. We could fix this faster in requests by using `.getlist` on the original Header Dict from urllib3 and throwing an exception if it returns more than one value.
",sigmavirus24,Lukasa
2939,2015-12-19 16:42:49,"@L2501 That doesn't prevent CRLF injection attacks, it just keeps functioning in the face of them. My proposed solution above would actually avoid _some portion_ CRLF injection attacks by throwing an exception when an invalid header block is received. Of course, we cannot _generally_ prevent CRLF injection attacks and also re-use TCP connections, so any client that _does_ re-use TCP connections is vulnerable to a well-crafted CRLF injection attack. Generally speaking, defending against CRLF injection attacks is the responsibility of the server operator, not the client.

@sigmavirus24 The better long-term fix might be to promote urllib3's `HeaderDict` to use in requests. The risk with doing it your way is that we assume that the `raw` object exists and is a `urllib3` one. I suppose we could have a conditional path that _attempts_ to use urllib3's but falls back to using ours.

We could in fact do that, and then plan in 3.0.0 to promote urllib3's `HeaderDict` to use in requests which would allow us to remove the conditional code path in 3.0.0.
",Lukasa,L2501
2939,2015-12-19 16:42:49,"@L2501 That doesn't prevent CRLF injection attacks, it just keeps functioning in the face of them. My proposed solution above would actually avoid _some portion_ CRLF injection attacks by throwing an exception when an invalid header block is received. Of course, we cannot _generally_ prevent CRLF injection attacks and also re-use TCP connections, so any client that _does_ re-use TCP connections is vulnerable to a well-crafted CRLF injection attack. Generally speaking, defending against CRLF injection attacks is the responsibility of the server operator, not the client.

@sigmavirus24 The better long-term fix might be to promote urllib3's `HeaderDict` to use in requests. The risk with doing it your way is that we assume that the `raw` object exists and is a `urllib3` one. I suppose we could have a conditional path that _attempts_ to use urllib3's but falls back to using ours.

We could in fact do that, and then plan in 3.0.0 to promote urllib3's `HeaderDict` to use in requests which would allow us to remove the conditional code path in 3.0.0.
",Lukasa,sigmavirus24
2939,2015-12-19 16:46:02,"@sigmavirus24 No, I'm thinking of test libraries or anything else that returns a `Response` object from a `HTTPAdapter` subclass that is not actually backed by a urllib3 response object.
",Lukasa,sigmavirus24
2939,2015-12-19 16:49:55,"@Lukasa I'm not as concerned about them. Any test library that hasn't been half-written and thrown up on PyPI will handle this just fine (requests-mock, betamax, vcrpy, all do just fine).
",sigmavirus24,Lukasa
2939,2015-12-19 16:51:45,"@sigmavirus24 Be that as it may, in general our code assumes that [`.raw` may not come from urllib3](https://github.com/kennethreitz/requests/blob/fc8fa1aa265bb14d59c68eb68a179bce17953967/requests/models.py#L657-L667), and we should endeavour to maintain that if possible.
",Lukasa,sigmavirus24
2939,2015-12-19 17:02:18,"@sigmavirus24 The common situation is where the response object is `StringIO` or something similar. I think going out of our way to avoid breaking that is a good idea. If we were going straight to 3.0.0 I'd be fine with breaking it, but ideally we'd deal with this sooner.
",Lukasa,sigmavirus24
2939,2016-01-04 02:51:55,"@sigmavirus24

> There was a proposed version of the Header Dict that ignored multiple headers for anything other Set-Cookie headers. We could fix this faster in requests by using `.getlist` on the original Header Dict from urllib3 and throwing an exception if it returns more than one value.

This would break the list-like headers though? The ones defined as `#(values)`, which RFC 7230 [permits](https://tools.ietf.org/html/rfc7230#section-3.2.2) to split into multiple fields.
",vfaronov,sigmavirus24
2939,2016-01-04 09:23:21,"@vfaronov That isn't being proposed _in general_, it's being proposed purely for the `Location` header.
",Lukasa,vfaronov
2939,2016-01-27 13:30:18,"@foxx I appreciate that you were trying to be helpful, but please read the discussion on this issue before commenting. If you look at the first comment in this issue you'll see that it begins with:

> A web server MUST NOT respond with multiple location header fields.

The proposed action in this thread is that requests should error on this condition, rather than doing what it currently does which is to attempt to parse the multiple `Location` fields as though they were one. Note also that that behaviour means that requests already _does_ support multiple headers.

Finally, RFC 2616 is deprecated. RFC 7230 and RFC 7231 have replaced it. =)
",Lukasa,foxx
2939,2016-01-27 14:40:45,"That would be a 3.0 change but I'm amenable to that. @Lukasa, what about you?
",sigmavirus24,Lukasa
2939,2016-01-31 22:20:12,"@kennethreitz Yup agreed
",foxx,kennethreitz
2937,2015-12-18 14:18:56,"@Lukasa are you thinking about making an exception to [our release document](https://github.com/kennethreitz/requests/blob/master/docs/community/release-process.rst#hotfix-releases) in this case?
",sigmavirus24,Lukasa
2937,2015-12-18 14:19:59,"@sigmavirus24 Yeah, good question, but I think I am, given that the urllib3 release would itself be a patch release. Any concerns there? @ralphbean @eriol?
",Lukasa,sigmavirus24
2937,2015-12-18 14:39:36,"@Lukasa Could do. Could try to sneak in SOCKS support too, if you wanna.

Also I noticed some of your changelist includes an update note to the urllib3 version, some don't. Dunno if that's intentional.
",shazow,Lukasa
2937,2015-12-18 14:40:46,"@shazow I'd rather leave SOCKS out for now: makes 1.13.1 very clearly a simple bugfix release.

Yeah, the missing changelist notes are in error, we should try to fix it.
",Lukasa,shazow
2937,2015-12-18 15:03:58,"@Lukasa yeah, if it's just a couple small bug-fixes, that's fine by me. I think we've been bitten in the past by urllib3 having a lot more than just bug fixes when we pulled it in.
",sigmavirus24,Lukasa
2937,2015-12-18 18:04:23,"@ralphbean Frankly I don't recommend releasing 2.9.0, it had enough nasty bugs that going straight to 2.9.1 is going to be the better approach.
",Lukasa,ralphbean
2937,2015-12-19 08:39:50,"\o/ Thanks @shazow!
",Lukasa,shazow
2937,2015-12-21 14:43:20,"@Lukasa is this still a WIP? I believe you wanted to cut this today
",sigmavirus24,Lukasa
2936,2015-12-19 17:34:31,"Woot! :+1: 

Thanks @Lukasa (also YAY OUR CI IS WORKING AGAIN)
",sigmavirus24,Lukasa
2935,2015-12-18 08:59:59,"@Kronuz What version of requests are you using, please?
",Lukasa,Kronuz
2935,2015-12-18 14:25:38,"@Kronuz can you give us an idea of what other packages are installed on your system? Something else may be monkey patching the stdlib and causing this issue.
",sigmavirus24,Kronuz
2935,2015-12-18 15:10:58,"@Kronuz things like eventlet and gevent may be used in one of your dependencies and they tend to monkey-patch Queue. This is why we asked for your dependencies. Many packages do things without telling the end-user.

Further, it's absolutely bizarre that you're getting a `KeyError` when the error raised is `Queue.Empty`.
",sigmavirus24,Kronuz
2935,2015-12-24 08:41:48,"@fqroot Are you using gevent or eventlet? Or anything that monkeypatches the `Queue` module?
",Lukasa,fqroot
2935,2015-12-24 08:52:26,"@fqroot So, to be clear, you are _not_ using gevent? Because if anything monkeypatches the `Queue` module to change its idea of what `Empty` is then this code is at risk of failing.
",Lukasa,fqroot
2935,2015-12-24 12:29:42,"@fqroot, I haven't had the chance to try it myself yet, but why don't you catch `Exception as exc` And then inspect `exc` to see what the type of it is and where it's source code has been defined.
",Kronuz,fqroot
2935,2015-12-24 13:36:37,"@Kronuz OK, I will debug it tommrow.  maybe there is a relationship with the module ""importlib"", I think
",fqroot,Kronuz
2935,2015-12-25 02:21:44,"@Kronuz   



output



and I have found where the err code lead to my problem. 
",fqroot,Kronuz
2935,2015-12-25 09:46:42,"@fqroot, where? ...Wait, what? how?

It's really weird... why doesn't the `except Empty` catches it then?
",Kronuz,fqroot
2935,2015-12-25 15:03:56,"@Kronuz   I noticed that there is an error code in other part of my project
simple like



the handle function was created by module importlib
so Queue.Empty is not  `<type ""type"">` any longer and become a instance of Queue.Empty
This explanation is my guess.  
",fqroot,Kronuz
2935,2015-12-25 16:19:25,"@fqroot, I still don't get it :/

Could you please elaborate a bit more? I'm getting `KeyError` instead of `Empty` (or so the log in Sentry says)... What where you getting there?
",Kronuz,fqroot
2935,2015-12-25 22:40:29,"@Kronuz My situation may not suit you.
what type exception you get in that close function？
",fqroot,Kronuz
2935,2015-12-26 00:02:37,"@fqroot, I'm getting `KeyError((1, True))` ...for some reason
",Kronuz,fqroot
2935,2015-12-26 10:07:15,"@Kronuz I don't know how to help you...
",fqroot,Kronuz
2935,2015-12-27 16:18:50,"To anyone who happens upon this thread in the future, a few things:
1. @Kronuz and @fqroot are having similar (on the face) but very different problems
2. @fqroot is using Python 2 in which you could do
   
   
   
   when they meant to use
   
   
   
   this is why `Queue.Empty` was no longer of type `type` in @fqroot's case and why he was seeing the `Queue.Empty` exception as being unhandled by requests/urllib3.
3. @Kronuz is getting an entirely different exception from code that should only ever raise a `Queue.Empty` exception. At the time of this writing, it is still unclear how this is either happening or even possible.
",sigmavirus24,fqroot
2935,2015-12-27 16:18:50,"To anyone who happens upon this thread in the future, a few things:
1. @Kronuz and @fqroot are having similar (on the face) but very different problems
2. @fqroot is using Python 2 in which you could do
   
   
   
   when they meant to use
   
   
   
   this is why `Queue.Empty` was no longer of type `type` in @fqroot's case and why he was seeing the `Queue.Empty` exception as being unhandled by requests/urllib3.
3. @Kronuz is getting an entirely different exception from code that should only ever raise a `Queue.Empty` exception. At the time of this writing, it is still unclear how this is either happening or even possible.
",sigmavirus24,Kronuz
2935,2016-02-26 12:34:06,"@Kronuz My best guess is that something is monkeypatching the `Queue.Empty` import, either in the `Queue` module or in `requests.packages.urllib3.connectionpool`. That's the only way I can see that we'd hit a problem where the implicit `isinstance` check in `except` fails to work.
",Lukasa,Kronuz
2935,2016-02-26 12:51:31,"@Lukasa, but even if `Queue.Empty` is monkeyparched, how the message ends up being `KeyError: (1, True)` ?? And _if_ the method `LifoQueue.get()` or `LifoQueue` was monkeyparched, the traceback would come with the monkeyparched version of it, wouldn't it? This is weird!
",Kronuz,Lukasa
2935,2016-02-26 12:52:43,"@Kronuz Yeah, this is definitely one of the weirdest things I've seen. You're probably going to want to try to find a consistent reproduction scenario and then shrink it down to as little code as you can.
",Lukasa,Kronuz
2935,2016-02-26 13:33:03,"@Lukasa, Sentry provides the full raw log as well, it's the one I pasted above... But I'll try to log more things ...the bad news is it's not always happening either :/
",Kronuz,Lukasa
2935,2016-02-26 13:41:10,"@Kronuz I haven't actually seen a full, Python-formatted traceback from you in this thread, which is why I asked. I'm just worried that somewhere along the way we're losing information.
",Lukasa,Kronuz
2935,2016-02-26 14:01:38,"I would think the first regexp would be the better one to use. @Kronuz, are you still on 2.8.1 or have you changed versions of requests?
",sigmavirus24,Kronuz
2935,2016-02-26 14:22:33,"@sigmavirus24, I'm still in 2.8.1

The search across all my files with those regexps didn't yield anything even remotely interesting...
",Kronuz,sigmavirus24
2935,2016-03-09 08:17:03,"Hah, very nicely done @Kronuz! And a very subtle bug it was too.
",Lukasa,Kronuz
2935,2016-03-09 21:28:17,":cake: for @Kronuz and @Yhg1s
",sigmavirus24,Kronuz
2934,2015-12-17 16:32:11,"Thank you @Lukasa .  I guess something else went wrong for me originally (when I thought I did use basic auth), and I did misread wget output (which first ""senses"" which authentication mechanism to use).  Sorry about the noise
",yarikoptic,Lukasa
2931,2015-12-16 20:22:08,"@nfnty we've planned the next release for Monday.
",sigmavirus24,nfnty
2929,2015-12-16 18:09:42,"Closing as the bug is actually over at https://github.com/sigmavirus24/requests-toolbelt/issues/117. Thanks for bringing this to our attention @nicorevin ! :cake: 
",sigmavirus24,nicorevin
2928,2015-12-15 16:35:50,"@heni We've been using a tuple here since [literally the start of the project](https://github.com/kennethreitz/requests/commit/0477018761c67152cdcc0b83d56f27e701e65b9e), which _used_ distutils. I do not believe this can possibly be the problem you're experiencing.
",Lukasa,heni
2927,2015-12-15 15:22:00,"@sigmavirus24 I intentionally did not do that, so that we have a single commit entitled `v2.9.0`, rather than a merge commit that hits it. =D
",Lukasa,sigmavirus24
2926,2015-12-11 19:51:10,"Hey @bsamek! Thanks for the PR.

I don't think we want to change the default parameters to the `request` method. We want to be able to discern between something that's a session setting and a request setting which we cannot do if we change that signature. We use [`merge_setting`](https://github.com/bsamek/requests/blob/master/requests/sessions.py#L49..L53) to make that determination and I don't think we want to break this.

Please revert that as the docstring changes _are_ desirable.
",sigmavirus24,bsamek
2926,2015-12-11 22:14:12,"Thanks again @bsamek !
",sigmavirus24,bsamek
2926,2015-12-12 00:43:29,"Ohhhhhhhhh I see. Thanks @sigmavirus24 !
",bsamek,sigmavirus24
2926,2015-12-12 01:05:42,"Anytime @bsamek :) Thanks for the PR!
",sigmavirus24,bsamek
2925,2015-12-11 20:29:26,"> Why are you acquiring the import lock?

@guojh are you acquiring the lock manually in other code or are you positing that the behaviour you're seeing is caused by the import locks?
",sigmavirus24,guojh
2925,2015-12-11 20:49:31,"@sigmavirus24 No, no manually acquired lock. The bash script I pasted is all.
It easy to position the stuck by adding print statements around that line:
https://github.com/kennethreitz/requests/blob/2d91365cba9ddca25ddd28f5c0ecd9497b8a2e24/requests/utils.py#L95

It seems that import lock is automatically acquired by CPython interpreter during import.

Call stack of CPython 2.7 __import__ should look like this (not verified):

builtin___import__
https://github.com/python/cpython/blob/61ee2ece38e609a444575dda0302907fa7a2752f/Python/bltinmodule.c#L36

PyImport_ImportModuleLevel
https://github.com/python/cpython/blob/61ee2ece38e609a444575dda0302907fa7a2752f/Python/import.c#L2287

_PyImport_AcquireLock
https://github.com/python/cpython/blob/61ee2ece38e609a444575dda0302907fa7a2752f/Python/import.c#L292
(reenterable lock)
",guojh,sigmavirus24
2925,2015-12-11 21:25:15,"@Lukasa It seems to have been resolved in CPython 3.3. (per-module import lock instead of global import lock)
https://github.com/python/cpython/blob/d5adb7f65d30afd00921e6c22e9e2b8c323c058d/Doc/whatsnew/3.3.rst#a-finer-grained-import-lock
",guojh,Lukasa
2925,2015-12-11 23:34:08,"`get_netrc_auth` probably isn't the only such function though and there may be other places where a delayed import is necessary.

That said, @Lukasa, @guojh's point is that using `str.encode` and `str.decode` will cause this too because they also use delayed imports apparently (the last test module actually also works with a valid encoding that does exist too).

That said, the other work around is to defer the work in the module such that it doesn't happen at import time.
",sigmavirus24,guojh
2925,2015-12-11 23:34:08,"`get_netrc_auth` probably isn't the only such function though and there may be other places where a delayed import is necessary.

That said, @Lukasa, @guojh's point is that using `str.encode` and `str.decode` will cause this too because they also use delayed imports apparently (the last test module actually also works with a valid encoding that does exist too).

That said, the other work around is to defer the work in the module such that it doesn't happen at import time.
",sigmavirus24,Lukasa
2923,2015-12-09 16:33:02,"Thanks @bsamek! :sparkles: :cake: :sparkles:
",Lukasa,bsamek
2922,2015-12-09 12:47:23,"Thanks for this request @eric-tucker!

Unfortunately, Requests is in feature-freeze for the foreseeable future, which means that we're generally not implementing new features unless they represent a really compelling loss of function. Given that all the information you require is present, and that a helper function to build this information is exactly one line long, I'm afraid I don't think this feature is a priority for us at this time.

Sorry we can't be more help!
",Lukasa,eric-tucker
2920,2015-12-07 22:52:57,"That's interesting @JKulakofsky. A forbidden means that you're not allowed to access that. Are you sure your constructing your request correctly? Is there a reason you can't show us exactly the code you're using?
",sigmavirus24,JKulakofsky
2920,2015-12-07 23:08:33,"@sigmavirus24 The main reason I was avoiding posting the whole code was because it's long and dirty, but here you go:


",JKulakofsky,sigmavirus24
2920,2015-12-07 23:44:40,"@JKulakofsky instead of using split across a url string, why not use the module urlparse (i think the relevant method is urlparse.urlsplit)?
",ueg1990,JKulakofsky
2920,2015-12-07 23:55:31,"@ueg1990 you're misunderstanding the problem.
",sigmavirus24,ueg1990
2915,2015-12-04 12:10:19,"@Lukasa: Thanks for spotting this problem! I've added back the pre-existing ad trackers in `/_templates/layout.html` (Click on ""Files Changed"" link at the top). [Reference](https://stackoverflow.com/questions/5585250/how-can-i-add-a-custom-footer-to-sphinx-documentation-restructuredtext)
",ArcTanSusan,Lukasa
2915,2015-12-08 21:36:23,"@kennethreitz so do you want to fix these concerns up first or merge and then fix these up?
",sigmavirus24,kennethreitz
2915,2015-12-11 21:41:38,"@kennethreitz thoughts?
",sigmavirus24,kennethreitz
2915,2015-12-27 19:07:11,"@onceuponatimeforever I think what @kennethreitz wants is for the column beneath it to be aligned with the `R` in `Requests` under the turtle logo. (At least that's roughly what it looks like where it was from the before picture)
",sigmavirus24,kennethreitz
2913,2015-12-02 08:08:58,"Thanks for this @vkosuri!

This is expected behaviour. Requests attempts to ""connection pool"", which means that it attempts to reuse sockets as frequently as possible. Sometimes, requests will come to try to re-use a socket and find that the underlying TCP connection got closed while it's away. When that happens, it'll reset the connection. We mention this in the logs because it's potentially important to debug very subtle issues.

The important things to know are:
- this is a log message, not a warning or error message
- it's logged at INFO level, which is fairly low

If you want to make the message go away, simply set the log level higher than INFO or disable logging for the `requests.packages.urllib3` logger.
",Lukasa,vkosuri
2913,2016-04-20 10:05:57,"@Lukasa How to disable Retrying warning messages?



I am getting thousands of Warning messages in my testcase, due to this effect the log file size was increasing drastically. Is there any possibility to remove/disable warning message when NewConnectionError ?
",vkosuri,Lukasa
2913,2016-04-20 10:08:54,"@vkosuri That warning is raised by urllib3. If you want to disable that message, either handle retries yourself (stop passing the parameter into urllib3), or raise the log level for urllib3, or disable logging from urllib3 entirely. The logger you want to disable is the one at `requests.packages.urllib3.connectionpool`.
",Lukasa,vkosuri
2913,2016-04-20 10:26:39,"Thank you @Lukasa I am using [Robotframework-Requests](https://github.com/bulkan/robotframework-requests). I have added fallowing lines code to [Requests.py](https://github.com/bulkan/robotframework-requests/blob/master/src/RequestsLibrary/RequestsKeywords.py)

Still it is not working? Am i missed anything? Could you please suggest me?


",vkosuri,Lukasa
2913,2016-04-20 11:53:28,"Thanks @Lukasa It's working


",vkosuri,Lukasa
2912,2015-12-02 12:01:20,"@Lukasa I see. Yes, I'm concerned about the _standard library_ URL parser, but also curious what is a common practice in other parsers. So, thanks for the clarifications so far.

From your earlier response:

> This API design is super fragile, and I guarantee that it'll be prone to breaking in mysterious ways.

That is exactly what worries me, so I'm trying to understand what are the _super fragile_ traits exactly and what are the ways the API may fails.
",mloskot,Lukasa
2912,2015-12-02 13:56:36,"@Lukasa I think we strip it but I can imagine a way to avoid that.
",sigmavirus24,Lukasa
2912,2015-12-02 14:03:13,"@Lukasa Thanks very much. I sympathise with your critique.
",mloskot,Lukasa
2912,2015-12-10 16:23:50,"@mloskot Sadly, lots of people make bad decisions. =(
",Lukasa,mloskot
2912,2015-12-10 16:25:26,"@mloskot constantly posting information here will not get this fixed any faster. You're merely spamming 781 people (- the people who have already unsubscribed).
",sigmavirus24,mloskot
2912,2015-12-10 16:45:59,"@Lukasa Yes. That example surprised me and I thought I will share it as an example of bad design..

@sigmavirus24 

> constantly posting information here will not get this fixed any faster. 

You are misreading my intention - I'm not trying to press anything at all.
I'm simply sharing some further details which I believe are relevant to this issue.
I'm sorry if I caused any annoyance.
",mloskot,Lukasa
2912,2015-12-10 16:45:59,"@Lukasa Yes. That example surprised me and I thought I will share it as an example of bad design..

@sigmavirus24 

> constantly posting information here will not get this fixed any faster. 

You are misreading my intention - I'm not trying to press anything at all.
I'm simply sharing some further details which I believe are relevant to this issue.
I'm sorry if I caused any annoyance.
",mloskot,sigmavirus24
2912,2016-04-08 07:49:23,"@internaught What code do you have that causes requests to insert a question-mark?
",Lukasa,internaught
2912,2016-04-08 07:55:54,"@Lukasa I'm simply doing this:



Thanks for the quick reply!
",internaught,Lukasa
2912,2016-04-08 07:58:15,"@internaught You have a comma between the two parts of your URL. That means your second string is being passed to the `params` parameter of the requests.get call, which means requests assumes it's a query string. Replace the comma with a + and you'll be fine.
",Lukasa,internaught
2912,2016-04-08 07:59:50,"@Lukasa thanks for the help, working great now. Simple mistake, thanks!
",internaught,Lukasa
2910,2015-12-01 20:24:45,"@spookylukey what do you want the installation instructions to say? ""Make sure you use an operating system that ships versions of Python and OpenSSL that have these features or satisfy these minimum requirements""?
",sigmavirus24,spookylukey
2910,2015-12-01 20:35:41,"@sigmavirus24 - something like that, preferably with some information about the additional packages that are necessary, or under what (known) conditions you'll need to do more work. For a library that is explicitly ""HTTP for humans"", and has ""Browser-style SSL Verification"" high up on its list of features, this seems fairly important. The choice to rely on Python/OS for some of its functionality is a choice made by this library, and the user shouldn't be expected to know all the details.
",spookylukey,sigmavirus24
2910,2015-12-01 20:46:51,"the python module ssl has a flag, ssl.HAS_SNI, that can be used to warn
during installation/runtime.

On 2015-12-01 20:25, Ian Cordasco wrote:

> @spookylukey https://github.com/spookylukey what do you want the
> installation instructions to say? ""Make sure you use an operating
> system that ships versions of Python and OpenSSL that have these
> features or satisfy these minimum requirements""?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/2910#issuecomment-161085173.
",FirefighterBlu3,spookylukey
2910,2015-12-01 22:24:22,"Warning during installation is not possible: wheels don't run code at install time, and Python packaging strongly prefers them. 

We can warn at runtime, but I'm nervous about adding too many more warnings: users have complained bitterly about the ones we already have, that serve a security purpose. 

@spookylukey Note that the specific failure mode you encountered here was extremely uncommon, specifically because it was a _success_ mode. `badssl.com` served you a _valid_ certificate for the request you made! This is the rarest kind of website: one that uses SNI to deliberately _break_ your connection. Arguably, `badssl.com` needs a feature request to ensure that connections without SNI _fail_ outright!

Regardless, for the _common_ failure mode we have a [section of our FAQ](http://docs.python-requests.org/en/latest/community/faq/). When it comes to adding additional information to our install instructions, anything that requires manual user intervention is likely to be a non-starter. The most effective course would be a warning if SNI is not available on the platform, which I'm open to adding to urllib3.
",Lukasa,spookylukey
2909,2015-12-01 11:28:04,"Thanks @jwilk! :sparkles: :cake: :sparkles:
",Lukasa,jwilk
2908,2015-11-30 13:26:46,"@rjschwei Where did you install requests from? urllib3 (and by extension requests) have _never_ called `set_default_verify_paths` in any version we shipped, at least according to a quick search of our git history. The only place we've ever called it is inside the PyOpenSSL contrib module, which of course could not produce the traceback you've provided.
",Lukasa,rjschwei
2908,2015-11-30 13:37:56,"Thanks for the apology @rjschwei, it happens to all of us. =) No harm done, and I hope you get the problem resolved!
",Lukasa,rjschwei
2907,2015-11-30 10:32:56,"@rohithpr What would a `None` value be turned into?
",Lukasa,rohithpr
2907,2015-11-30 14:01:33,"@Lukasa - `null` perhaps?


",rohithpr,Lukasa
2907,2015-11-30 14:03:25,"@rohithpr The only place we turn dictionaries into JSON is the `json=` keyword argument, and that is not put into the URL's query string. Query strings in URLs are not JSON, they're urlencoded key-value pairs.
",Lukasa,rohithpr
2907,2015-11-30 14:15:41,"I see.

So, the use of `null` in this fashion is specific to your server: it is not _generally_ true about the URL query string. Requests tries to restrict itself to the subset of behaviour that is universally understood in the query string. Implementation-specific variances have to handle that themselves: otherwise, requests will grow a million different feature flags that subtly change its behaviour in all sorts of weird ways. This kind of combinatorial expansion of the API and the possible configurations of requests would make it a nightmare to use, maintain, and debug.

@sigmavirus24 has provided you a code snippet that should do what you're looking for (with some small edits). I hope you find that helpful!
",Lukasa,sigmavirus24
2907,2015-11-30 14:19:37,"@Lukasa @sigmavirus24 - Thanks! :)
",rohithpr,Lukasa
2907,2015-11-30 14:19:37,"@Lukasa @sigmavirus24 - Thanks! :)
",rohithpr,sigmavirus24
2906,2015-11-28 13:56:26,"@Lukasa So maybe I need to install a newer version of OpenSSL using homebrew and use it to replace the old OpenSSL shipped with OS.
",caizixian,Lukasa
2906,2015-11-28 14:05:13,"@caizixian It's an option, but if you `pip install -U requests[security]` that will grab PyOpenSSL, which includes its own copy of OpenSSL on OS X.
",Lukasa,caizixian
2906,2015-11-29 08:09:26,"@Lukasa I had following errors when installing requests[security], both related to cffi.
1. `c/_cffi_backend.c:14:10: fatal error: 'ffi.h' file not found`
   
   so I `brew install pkg-config libffi`, which solved this problem.
2. After that, I tried to install requests[security] again, and I got `Failed building wheel for cffi` . 
   
   Despite this error, pip said that all things were installed. So I checked `ssl.OPENSSL_VERSION` again, still the same.

FYI, my Xcode is 7.1.1
",caizixian,Lukasa
2903,2015-11-27 09:02:14,"At a certain point this will get clearer if we use a for loop, but for now I think we're still ok. Thanks @thoger!
",Lukasa,thoger
2903,2016-02-26 16:26:07,"@standaloneSA This has been tagged for the 3.0.0 release. =)
",Lukasa,standaloneSA
2903,2016-02-26 16:28:02,"All the +1s. You rock @Lukasa thanks! 
",standaloneSA,Lukasa
2903,2016-04-06 19:18:40,"@Lukasa why is this being delayed until 3.x? It doesn't appear to be break any backwards compatibility to me. 
",kennethreitz,Lukasa
2903,2016-04-07 17:22:11,"@Lukasa ah, think that'll be during the sprints? If so, I might try to stay for them then (was planning on not attending sprints this year).
",kennethreitz,Lukasa
2903,2016-06-26 16:27:47,"@jakirkham The problem is that that call can also accidentally have some unexpected results from there. Ideally we'd come up with something a bit more comprehensive. Especially as that, for example, does not work on Python 2.6 (no such method), older Python 2.7s (pre 2.7.9), or Python 3.3.
",Lukasa,jakirkham
2900,2015-11-26 14:54:55,"@LinusU Unfortunately, `httplib` (upon which requests builds) does not expose this functionality. It converts the socket into a buffered file-like object which has a blocking `read` method, rather than a socket-like `recv` method. You could _in principle_ reach down into the socket below httplib, but in practice I think that will only rarely work because `httplib` itself uses the blocking read logic to get the headers, which means there may be information inside the `httplib` buffer you'd need to grab.
",Lukasa,LinusU
2899,2015-12-07 10:32:01,"@Keops92 The work has been done, sadly: see #2903.
",Lukasa,Keops92
2899,2016-06-10 14:50:18,"@luv Currently the expectation is that a future urllib3 release will allow the passing of custom SSLContext objects to urllib3. That will allow construction of adapters that let you use the system certificate store on Linux. For OS X and Windows we have the beginnings of some code to do it, but it's a lot of very subtle work that needs a lot of bugs ironed out.
",Lukasa,luv
2899,2016-06-26 09:13:02,"@Lukasa `/etc/ssl/cert.pem` is now by default linked by the `ca_root_nss` port.
",michael-o,Lukasa
2899,2016-11-18 19:54:07,"As @luv suggested, that will work.
",Lukasa,luv
2899,2016-11-20 22:19:44,"@luv Thanks, I can confirm that if I do `export REQUESTS_CA_BUNDLE=$SSL_CERT_FILE` i would be able to use requests the same way as other libraries. I dont really know why requests needs another environment variable instead of re-using the same one but I guess there were some reasons for that.

While I did this I observed something that looks like a bug: `requests.certs.where()` returns the same location even when you define `REQUESTS_CA_BUNDLE`, even if the library is using the ones from `REQUESTS_CA_BUNDLE`. Should I open a bug for this or is not really a bug?
",ssbarnea,luv
2898,2015-11-25 07:03:30,"Thanks @lndbrg!
",Lukasa,lndbrg
2897,2015-11-26 09:01:10,"Yup, that seems right. They don't consistently hang, but they do sometimes hang. Are you able to run the tests locally @BraulioVM?
",Lukasa,BraulioVM
2897,2015-11-26 09:01:25,"PS @sigmavirus24 I don't seem to be able to stop the builds: can you do it?
",Lukasa,sigmavirus24
2897,2015-11-29 15:35:37,"@Lukasa I ran the tests locally on python2.7 and know why they hang. I have run the tests for the last version both on 2.7 and 3.4, and they are fine. Is there any easy way to replicate the build environment of the CI server? 

I am sorry for breaking it, I never thought there wasn't going to be a timeout or something like that.
",BraulioVM,Lukasa
2897,2015-12-07 21:30:16,"@BraulioVM So, that test doesn't fail for me, it passes, but more importantly it does run!

Having played with this quickly on my local machine, this seems generally good: the test runs, the structure of the function mostly seems to be good, so I'm happy to have this as the basis of a set of more specific tests. I would like us to add some more helpers, but we don't need to do that right now: we can definitely tackle it later.

Well done @BraulioVM! Did you have more work you wanted to do before you proposed this for merging?
",Lukasa,BraulioVM
2897,2015-12-08 14:44:11,"Thank you @Lukasa! I am happy this is finally going to be useful. 

I don't mind adding those helpers you talk about, but as I am not totally sure about how this server will be used I don't know what helpers should be implemented. I guess some logic for parsing the chunked content must be implemented in order to fully test the library, however, I don't think that logic should be built into TestServer.

There's no more work I was planning on doing. I was hoping you would tell some other things you wanted this PR to have (such as those helpers).
",BraulioVM,Lukasa
2897,2015-12-26 11:51:26,"Not right now @BraulioVM, we're pretty quiet over the holiday period so we probably won't find time to focus on reviewing this until the new year.
",Lukasa,BraulioVM
2897,2016-03-01 08:29:25,"I believe this is generally going in the right direction, but now that @kennethreitz is more active he may want to see if this is something he wants. @sigmavirus24 and I are definitely :+1: on this for improving our testing, but it's up to Kenneth.
",Lukasa,sigmavirus24
2897,2016-03-02 15:51:29,"Ok @BraulioVM I've left some notes in the diff. =)

Unfortunately, while you've been away we moved the tests to a `test` subdirectory, so you'll need to move this change onto that directory structure. Sorry!
",Lukasa,BraulioVM
2897,2016-03-04 09:54:27,"Ok, we're making some good progress here @BraulioVM! We'll have a few more rounds of code review while we get this stuff settled, and right now our Jenkins runs are hanging (I'll investigate why).

Did you lose `test_chunked_upload` entirely? It doesn't seem to be in the diff at all: commit f17ef753d2c1f4db0d7f5aec51261da1db20d611 just removed it, without moving it _to_ anywhere.
",Lukasa,BraulioVM
2897,2016-03-04 20:06:34,"@BraulioVM I can't stress this enough: you don't need to apologise for code review. Code review is a healthy part of the development process, and it doesn't matter how good you are: you _will_ get code review, and it's a good thing. You should see some of my patches on Twisted, which have had more than 100 code-review comments on them. ;)
",Lukasa,BraulioVM
2897,2016-03-14 18:28:28,"@BraulioVM I think I'm happy with this as-is at this point, but I'm also a bit too close to it now to be objective. I'd like to ask @sigmavirus24 to do a review as well now, if he has time: just to make sure the code is as good as it can be. Otherwise, I think this is very close to being ready to merge.
",Lukasa,BraulioVM
2897,2016-03-14 18:28:28,"@BraulioVM I think I'm happy with this as-is at this point, but I'm also a bit too close to it now to be objective. I'd like to ask @sigmavirus24 to do a review as well now, if he has time: just to make sure the code is as good as it can be. Otherwise, I think this is very close to being ready to merge.
",Lukasa,sigmavirus24
2897,2016-04-08 10:58:22,"@sigmavirus24, what are the odds that you'll have time in the next 7 days to do a review?
",Lukasa,sigmavirus24
2896,2015-11-23 20:09:35,"@BraulioVM This is a good first pass! I have a draft of this fix sitting around on my laptop, so when I get home I'll show you how I approached it and let you take the ideas you like best. 
",Lukasa,BraulioVM
2896,2015-11-23 20:20:09,"@Lukasa Thanks!

I have just seen #2866, which won't allow me to implement the tests I want, so I might as well think of a solution for that issue.
",BraulioVM,Lukasa
2896,2015-11-24 12:46:04,"@BraulioVM That's actually intentional: sending a chunked zero-length stream is fine, there won't be a problem there. That way, the code is clearer. =)

So, this change looks good to me, but I'll let @sigmavirus24 review it.
",Lukasa,BraulioVM
2896,2015-12-02 08:09:20,"@BraulioVM Nope, just waiting for @sigmavirus24 to get enough time to swing by and take a look at it. =)
",Lukasa,BraulioVM
2896,2015-12-02 14:35:36,"Woops! Sorry. I lost track of this with recent goings on IRL. This looks great to me! Thanks @BraulioVM 
",sigmavirus24,BraulioVM
2890,2015-11-19 07:58:14,"@Bekt The problem with the header dict is that `Set-Cookie` _cannot_ be simply joined by commas in the general case, because its values _contain_ commas. This is some truly terrible design, but we're stuck with it now. This is why you need to use @sigmavirus24's approach.
",Lukasa,sigmavirus24
2890,2015-11-19 07:58:14,"@Bekt The problem with the header dict is that `Set-Cookie` _cannot_ be simply joined by commas in the general case, because its values _contain_ commas. This is some truly terrible design, but we're stuck with it now. This is why you need to use @sigmavirus24's approach.
",Lukasa,Bekt
2887,2016-04-13 20:48:08,"@suoto It's not the same bug. =)

Your bug is a simple timing issue: the requests.post call executes before the server has started running, which means it isn't yet listening on the socket. If you add a `sleep(1)` call before the call to `requests.post()` you'll find it works fine. =)
",Lukasa,suoto
2884,2015-11-19 15:46:14,"@AraHaan this is absolutely not a bug in requests. Please look into using [data files](http://www.py2exe.org/index.cgi/data_files) with py2exe and [StackOverflow](https://stackoverflow.com) for further help with py2exe.
",sigmavirus24,AraHaan
2879,2015-11-12 16:29:09,"@datarup No need. Can you run this from your Python shell?


",Lukasa,datarup
2879,2015-11-12 18:40:42,"@Lukasa The capture was not possible due to IT restrictions on Wireshark. Anyway I have tried curl and it works. the site has 'SSL connection using TLS_RSA_WITH_RC4_128_SHA'   and I tried the suggestion [on Stack Overflow](http://stackoverflow.com/questions/32650984/why-does-python-requests-ignore-the-verify-parameter) but still get the same error
",datarup,Lukasa
2877,2015-11-12 13:22:01,"@kiddick You're entirely right: that import of the `json` module is from an old version of the documentation. Would you like to open a pull request to remove it?
",Lukasa,kiddick
2877,2015-11-12 13:34:26,"@Lukasa Thanks for reply. Yep I'll open pull request!
",kiddick,Lukasa
2877,2015-11-12 14:11:11,"Thanks for fixing this @kiddick 
",sigmavirus24,kiddick
2876,2015-11-12 08:37:40,"@KenKundert That's sadly a bit of a tricky business. Because of the nature of Python code the total range of exceptions that may be thrown out of requests code is actually very nearly unbounded: our dependencies may throw a really quite dramatic range.

While Timeout errors potentially have nice user-friendly messages, not all the others do, or they represent a painfully large range of problems (SSLError is a particular culprit here, due to the fact that there's only one exception used and it uses OpenSSL's extremely inscrutable error messages to generate its text.

I think the best we could do is enhance the API documentation to indicate how to get good error messages, or to indicate what the error messages could/should be for each type of exception.
",Lukasa,KenKundert
2876,2015-11-12 14:16:20,"@KenKundert I think there's a disconnect here. requests is meant to be powerful and simple for _developers_ (of varying experience with HTTP). Developers tend to prefer more detailed information when something fails than they prefer vague end-user friendly messages. We give developers what they need to debug failure cases. It's up to those developers to make something end-user friendly.

Failure scenarios also aren't uniform across all servers, applications, etc. The developer creating something for the end-user should strive to anticipate as many of those failure cases as possible and handle them in a way that appropriately and accurately describes the problem to those users.
",sigmavirus24,KenKundert
2876,2015-11-12 22:18:09,"I agree that the information should be available, but currently it is very difficult to access. Presumably it is accessible through the exception object itself, but how to access it is undocumented and seems to differ from exception to exception. The only thing I reliably have access to is the exception cast to a string, and there if I want to use it I have to parse the string. Preferably the components of that string would be available through attributes or methods of the exception.

In my experience, casting the exception to a string generally results is a reasonably easy to read summary of the error in a form that could be passed to the end user, and the components of that message are also available so that a tailored response is possible. For example, an IOError contains filename, strerror, errno, etc. to make it possible to roll your own message or craft a response.

The problem I am having with requests at the moment is that I cannot produce an error message suitable for end users. Casting the exception to a string produces something too complex, and I cannot find the things I would need to use to create an informative message. I am reduced to an error message that simply parrots the name of the exception:



My fear is that not only is the information I need not documented, but getting it is complicated and highly exception specific. This seems to be what @Lukasa is suggesting. Then, to produce good responses to exceptions I will have to understand all of those exceptions in detail and craft code that handles all the cases that could occur. And of course, most users would have to do the same thing, meaning that this code will be written over and over. Perhaps someday someone will create a requests-like package for interpreting the exceptions produced by requests. I guess I am suggesting that it would be good to build that into requests itself, or it it already exists, then give some examples of how to use it in the documentation.
",KenKundert,Lukasa
2875,2015-11-11 23:06:05,"Hey @patjones80,

Can you try doing



To see what happens?
",sigmavirus24,patjones80
2875,2015-11-12 08:39:25,"@patjones80 It seems like you have some environment variables set up here. In your deployed environment, can you run `echo ""$HTTP_SERVER""` for me please?

Requests will normally look for environment variables that tell processes to use proxies. It seems like you have one set here.
",Lukasa,patjones80
2875,2015-11-12 14:11:55,"Thanks for updating us @patjones80 
",sigmavirus24,patjones80
2875,2016-05-20 21:13:54,"@patjones80 could you please tell me which views.py? I am new to this and I am getting a similar error MaxRetryError: HTTPConnectionPool(host='w.x.y.z', port=5000):
",wjdan94,patjones80
2875,2016-05-20 21:35:18,"@wjdan94 I'm not sure that I understand your question. If your problem is the same as mine, then your port number is either incorrect or just doesn't need to be specified. In my case, it did not need to be specified since I was working in my local development environment.
",patjones80,wjdan94
2874,2015-11-11 14:35:15,"@yupbank Sorry, I didn't fully understand this message. Can you post some code that demonstrates the problem?
",Lukasa,yupbank
2872,2015-11-10 23:04:03,"@sigmavirus24: Are you opposed to setting the `Content-Length` to `s.len - s.tell()`?

To be clear, my expectation was that the post request would have an empty body precisely for the reason that you state. The surprise was that the request hung, ie. the `Content-Length` is set to the full size, rather than what's remaining.
",braincore,sigmavirus24
2872,2015-11-11 01:26:03,"As @braincore stated, it's more the fact that the request will hang due to the mismatch between the `Content-Length` and the actual content that is being sent. The misuse of the `StringIO` API was the reason this was discovered, and not problem to be solved.

> It will be more burdensome for developers to get around your forced seek(0) than for developers to add a seek(0) themselves when needed.

I couldn't agree more, especially due to the rather simple fact that the forced seek would make explicit partial file uploads more difficult than they need to be as mentioned by @sigmavirus24.
",jperras,braincore
2872,2015-11-11 01:26:03,"As @braincore stated, it's more the fact that the request will hang due to the mismatch between the `Content-Length` and the actual content that is being sent. The misuse of the `StringIO` API was the reason this was discovered, and not problem to be solved.

> It will be more burdensome for developers to get around your forced seek(0) than for developers to add a seek(0) themselves when needed.

I couldn't agree more, especially due to the rather simple fact that the forced seek would make explicit partial file uploads more difficult than they need to be as mentioned by @sigmavirus24.
",jperras,sigmavirus24
2871,2015-11-10 15:28:17,"@Lukasa let's just remove them. This is a hassle and we've had .... 4 bugs about this? It's worthless and it doesn't provide real value.
",sigmavirus24,Lukasa
2871,2015-11-10 15:54:03,":heart: @sigmavirus24 

Thanks for the report @edmorley!
",Lukasa,edmorley
2871,2015-11-10 15:54:03,":heart: @sigmavirus24 

Thanks for the report @edmorley!
",Lukasa,sigmavirus24
2870,2015-11-10 14:42:00,"My recollection is that @fasaxc was using 2.7.0 from Ubuntu 14.04 trusty-liberty, but I may be mistaken.
",Lukasa,fasaxc
2870,2015-11-10 15:32:36,"@untitaker I can understand that, but I don't believe we can really discuss doing that unless we get the whole team together, which realistically means PyCon 2016.
",Lukasa,untitaker
2870,2015-11-10 15:58:40,"@untitaker He is, correct. However, of the three maintainers he's the one _opposed_ to unbundling.

Ian and I are both unanimous that we'd like to unbundle. However, we have a duty of care over this library, and that includes not changing things that Kenneth cares a great deal about without his explicit say-so.

I don't want to have a detailed discussion with Kenneth about this remotely, because this issue is close to all our hearts and we should discuss it in an environment that makes it easy for us to treat each other like humans, with respect and kindness. Those two qualities are often lost online, and that would be a tragedy: our working relationships are more important than that.
",Lukasa,untitaker
2870,2015-11-11 09:11:32,"@Lukasa I'm sad to confirm but yes, the problem actually exists in Debian testing (with the new import machinery).
I tried the same setup of https://github.com/kennethreitz/requests/issues/2867 using only system packages (so I took python-etcd from sid) and I can confirm it.
I looked at python-etcd code, but the problem was already explained  by @fasaxc and is about exceptions.

On Debian this is what I got:



I'm real sorry I did not noticed when we discussed https://github.com/kennethreitz/requests/pull/2567.

I'm not neither an import logic expert, but your proposal seems the only way to fix this, without unvendoring. Maybe we can ask to @brettcannon if he can take a look at this: I can volunteer to recap all the story so far.

Also, I just want to say thanks to you, @sigmavirus24 and @kennethreitz because although you can just mark this as wontfix, we started working together talking without forget that we are all human beings. 
So, thank you.
",eriol,fasaxc
2870,2015-11-11 09:11:32,"@Lukasa I'm sad to confirm but yes, the problem actually exists in Debian testing (with the new import machinery).
I tried the same setup of https://github.com/kennethreitz/requests/issues/2867 using only system packages (so I took python-etcd from sid) and I can confirm it.
I looked at python-etcd code, but the problem was already explained  by @fasaxc and is about exceptions.

On Debian this is what I got:



I'm real sorry I did not noticed when we discussed https://github.com/kennethreitz/requests/pull/2567.

I'm not neither an import logic expert, but your proposal seems the only way to fix this, without unvendoring. Maybe we can ask to @brettcannon if he can take a look at this: I can volunteer to recap all the story so far.

Also, I just want to say thanks to you, @sigmavirus24 and @kennethreitz because although you can just mark this as wontfix, we started working together talking without forget that we are all human beings. 
So, thank you.
",eriol,sigmavirus24
2870,2015-11-11 09:11:32,"@Lukasa I'm sad to confirm but yes, the problem actually exists in Debian testing (with the new import machinery).
I tried the same setup of https://github.com/kennethreitz/requests/issues/2867 using only system packages (so I took python-etcd from sid) and I can confirm it.
I looked at python-etcd code, but the problem was already explained  by @fasaxc and is about exceptions.

On Debian this is what I got:



I'm real sorry I did not noticed when we discussed https://github.com/kennethreitz/requests/pull/2567.

I'm not neither an import logic expert, but your proposal seems the only way to fix this, without unvendoring. Maybe we can ask to @brettcannon if he can take a look at this: I can volunteer to recap all the story so far.

Also, I just want to say thanks to you, @sigmavirus24 and @kennethreitz because although you can just mark this as wontfix, we started working together talking without forget that we are all human beings. 
So, thank you.
",eriol,Lukasa
2870,2015-11-11 09:32:34,"@eriol Thanks for investigating. =) And thanks for working with us on this. Again, I can't reiterate enough, it's really important to us to get to a place where everyone's getting a good experience. We're not there yet, but we'll keep trying.

I'd be delighted to see if someone who knows the import machinery really well can propose a better solution to this problem. If you're willing to do the legwork on ramping up @brettcannon that would be even better @eriol.

Otherwise, we should aim to take that boring manual step or see if we can avoid the problem in requests in some other way.
",Lukasa,eriol
2870,2015-11-16 23:29:16,"Basically @fasaxc is right in his analysis of the problem. The tricky bit is that this is all influenced by how you actually import something. You have to realize each of the following statements use slightly different logic to get you what you want:



If you use the last approach then that will definitely trip you up in this instance because import will do an explicit import for `urllib3.exceptions` _and then_ look for the `HTTPError` attribute. This means that since you are not patching `urllib3.exceptions` in `sys.modules` you end up having `from requests.packages.urllib3 import HTTPError` needing to import `requests.packages.urllib3` directly and since that isn't in `sys.modules` it leads to a fresh import. Now you have two separate modules with two separate classes and thus they won't succeed in a `issubclass()` check in an `except` clause.

Now to solve this, @Lukasa was right and you can just patch every module in `urllib3`. Another option is a custom importer that handles just this case, but I don't know if you want such a magical solution. A third option is to insert an object into `sys.modules` who uses `__getattribute__` to direct to the proper module and handle all magical tweaks to `sys.modules`. And lastly, the simplest solution is either let Debian handle their own desire to monkeypatch vendored code or stop vendoring stuff entirely.
",brettcannon,fasaxc
2870,2015-11-16 23:29:16,"Basically @fasaxc is right in his analysis of the problem. The tricky bit is that this is all influenced by how you actually import something. You have to realize each of the following statements use slightly different logic to get you what you want:



If you use the last approach then that will definitely trip you up in this instance because import will do an explicit import for `urllib3.exceptions` _and then_ look for the `HTTPError` attribute. This means that since you are not patching `urllib3.exceptions` in `sys.modules` you end up having `from requests.packages.urllib3 import HTTPError` needing to import `requests.packages.urllib3` directly and since that isn't in `sys.modules` it leads to a fresh import. Now you have two separate modules with two separate classes and thus they won't succeed in a `issubclass()` check in an `except` clause.

Now to solve this, @Lukasa was right and you can just patch every module in `urllib3`. Another option is a custom importer that handles just this case, but I don't know if you want such a magical solution. A third option is to insert an object into `sys.modules` who uses `__getattribute__` to direct to the proper module and handle all magical tweaks to `sys.modules`. And lastly, the simplest solution is either let Debian handle their own desire to monkeypatch vendored code or stop vendoring stuff entirely.
",brettcannon,Lukasa
2870,2015-11-17 10:00:34,"Thanks so much @brettcannon. :cake: is owed. =)

Ok folks, given the set of solutions proposed, does anyone have preferences?
",Lukasa,brettcannon
2870,2015-11-17 13:59:11,"Thank you @brettcannon!
",sigmavirus24,brettcannon
2870,2015-11-17 14:14:16,"So I'm enumerating the options and asking questions to make sure I understand correctly.

> you can just patch every module in `urllib3`

As in, ensuring that `sys.modules` has both `urllib3.submodule` and `requests.packages.urllib3.submodule`? I think this is the least magic way and given that @Lukasa and I are both core developers of urllib3, we will catch any new submodules that need to be added to requests' patching logic. I'm most strongly in favor of this one.

> Another option is a custom importer that handles just this case, but I don't know if you want such a magical solution.

We had that option and it broke many a thing. I'm not in favor of going back down that route.

> insert an object into `sys.modules` who uses `__getattribute__` to direct to the proper module and handle all magical tweaks to `sys.modules`. 

Hm. This sounds like something in the vein of option 2 but a little less magic-y. It still feels like some abuse of the module system and like it might cause us problems. If we come up with something like this, I would appreciate it if @fasaxc and @eriol would commit to testing this with python-etcd and making sure things still aren't broken before shipping a release with it.

> And lastly, the simplest solution is either let Debian handle their own desire to monkeypatch vendored code or stop vendoring stuff entirely.

Breaking this into two sub-suggestions:
- ""let Debian handle their own desire to monkey patch vendored code"" No. I don't want @eriol to have to figure that out. Vendoring is a position of this project. As much as I dislike downstreams unvendoring things, it isn't the individual maintainer's fault or decision and I don't want to push this work onto their backs.
- ""stop vendoring stuff entirely"" Many people constantly badger (or even, at times, yell) this at us (I know you're not Brett) but it simply just won't happen like that (if it happens at all).

---

My 2¢:

We can take a short term solution of the first option for a `2.8.2` release and get that updated patch to @eriol, @warsaw, @ralphbean, etc. so distros can fix their packages while taking a look at the feasibility and reliability of option 3. I still prefer the first option to the third, but the third may have benefits I'm not seeing due to the bad taste left in my mouth by the `VendorAlias` (a.k.a., option 2).
",sigmavirus24,fasaxc
2870,2015-11-17 14:14:16,"So I'm enumerating the options and asking questions to make sure I understand correctly.

> you can just patch every module in `urllib3`

As in, ensuring that `sys.modules` has both `urllib3.submodule` and `requests.packages.urllib3.submodule`? I think this is the least magic way and given that @Lukasa and I are both core developers of urllib3, we will catch any new submodules that need to be added to requests' patching logic. I'm most strongly in favor of this one.

> Another option is a custom importer that handles just this case, but I don't know if you want such a magical solution.

We had that option and it broke many a thing. I'm not in favor of going back down that route.

> insert an object into `sys.modules` who uses `__getattribute__` to direct to the proper module and handle all magical tweaks to `sys.modules`. 

Hm. This sounds like something in the vein of option 2 but a little less magic-y. It still feels like some abuse of the module system and like it might cause us problems. If we come up with something like this, I would appreciate it if @fasaxc and @eriol would commit to testing this with python-etcd and making sure things still aren't broken before shipping a release with it.

> And lastly, the simplest solution is either let Debian handle their own desire to monkeypatch vendored code or stop vendoring stuff entirely.

Breaking this into two sub-suggestions:
- ""let Debian handle their own desire to monkey patch vendored code"" No. I don't want @eriol to have to figure that out. Vendoring is a position of this project. As much as I dislike downstreams unvendoring things, it isn't the individual maintainer's fault or decision and I don't want to push this work onto their backs.
- ""stop vendoring stuff entirely"" Many people constantly badger (or even, at times, yell) this at us (I know you're not Brett) but it simply just won't happen like that (if it happens at all).

---

My 2¢:

We can take a short term solution of the first option for a `2.8.2` release and get that updated patch to @eriol, @warsaw, @ralphbean, etc. so distros can fix their packages while taking a look at the feasibility and reliability of option 3. I still prefer the first option to the third, but the third may have benefits I'm not seeing due to the bad taste left in my mouth by the `VendorAlias` (a.k.a., option 2).
",sigmavirus24,eriol
2870,2015-11-17 14:14:16,"So I'm enumerating the options and asking questions to make sure I understand correctly.

> you can just patch every module in `urllib3`

As in, ensuring that `sys.modules` has both `urllib3.submodule` and `requests.packages.urllib3.submodule`? I think this is the least magic way and given that @Lukasa and I are both core developers of urllib3, we will catch any new submodules that need to be added to requests' patching logic. I'm most strongly in favor of this one.

> Another option is a custom importer that handles just this case, but I don't know if you want such a magical solution.

We had that option and it broke many a thing. I'm not in favor of going back down that route.

> insert an object into `sys.modules` who uses `__getattribute__` to direct to the proper module and handle all magical tweaks to `sys.modules`. 

Hm. This sounds like something in the vein of option 2 but a little less magic-y. It still feels like some abuse of the module system and like it might cause us problems. If we come up with something like this, I would appreciate it if @fasaxc and @eriol would commit to testing this with python-etcd and making sure things still aren't broken before shipping a release with it.

> And lastly, the simplest solution is either let Debian handle their own desire to monkeypatch vendored code or stop vendoring stuff entirely.

Breaking this into two sub-suggestions:
- ""let Debian handle their own desire to monkey patch vendored code"" No. I don't want @eriol to have to figure that out. Vendoring is a position of this project. As much as I dislike downstreams unvendoring things, it isn't the individual maintainer's fault or decision and I don't want to push this work onto their backs.
- ""stop vendoring stuff entirely"" Many people constantly badger (or even, at times, yell) this at us (I know you're not Brett) but it simply just won't happen like that (if it happens at all).

---

My 2¢:

We can take a short term solution of the first option for a `2.8.2` release and get that updated patch to @eriol, @warsaw, @ralphbean, etc. so distros can fix their packages while taking a look at the feasibility and reliability of option 3. I still prefer the first option to the third, but the third may have benefits I'm not seeing due to the bad taste left in my mouth by the `VendorAlias` (a.k.a., option 2).
",sigmavirus24,warsaw
2870,2015-11-17 14:14:16,"So I'm enumerating the options and asking questions to make sure I understand correctly.

> you can just patch every module in `urllib3`

As in, ensuring that `sys.modules` has both `urllib3.submodule` and `requests.packages.urllib3.submodule`? I think this is the least magic way and given that @Lukasa and I are both core developers of urllib3, we will catch any new submodules that need to be added to requests' patching logic. I'm most strongly in favor of this one.

> Another option is a custom importer that handles just this case, but I don't know if you want such a magical solution.

We had that option and it broke many a thing. I'm not in favor of going back down that route.

> insert an object into `sys.modules` who uses `__getattribute__` to direct to the proper module and handle all magical tweaks to `sys.modules`. 

Hm. This sounds like something in the vein of option 2 but a little less magic-y. It still feels like some abuse of the module system and like it might cause us problems. If we come up with something like this, I would appreciate it if @fasaxc and @eriol would commit to testing this with python-etcd and making sure things still aren't broken before shipping a release with it.

> And lastly, the simplest solution is either let Debian handle their own desire to monkeypatch vendored code or stop vendoring stuff entirely.

Breaking this into two sub-suggestions:
- ""let Debian handle their own desire to monkey patch vendored code"" No. I don't want @eriol to have to figure that out. Vendoring is a position of this project. As much as I dislike downstreams unvendoring things, it isn't the individual maintainer's fault or decision and I don't want to push this work onto their backs.
- ""stop vendoring stuff entirely"" Many people constantly badger (or even, at times, yell) this at us (I know you're not Brett) but it simply just won't happen like that (if it happens at all).

---

My 2¢:

We can take a short term solution of the first option for a `2.8.2` release and get that updated patch to @eriol, @warsaw, @ralphbean, etc. so distros can fix their packages while taking a look at the feasibility and reliability of option 3. I still prefer the first option to the third, but the third may have benefits I'm not seeing due to the bad taste left in my mouth by the `VendorAlias` (a.k.a., option 2).
",sigmavirus24,Lukasa
2870,2015-11-17 14:51:54,"@brettcannon many thanks!

I'm also in favour of the less magic solution.
I will be happy to test before the release python-etcd and all the dependants of requests that can be affected by this: using codesearch.debian.net I noticed that also cinder and proliantutils import urllib3's exceptions using `requests.packages.urllib3`, so I will check also them.

@sigmavirus24 I really appreciate your words, thanks!
",eriol,brettcannon
2870,2015-11-17 14:51:54,"@brettcannon many thanks!

I'm also in favour of the less magic solution.
I will be happy to test before the release python-etcd and all the dependants of requests that can be affected by this: using codesearch.debian.net I noticed that also cinder and proliantutils import urllib3's exceptions using `requests.packages.urllib3`, so I will check also them.

@sigmavirus24 I really appreciate your words, thanks!
",eriol,sigmavirus24
2870,2015-11-17 15:36:29,"On Nov 17, 2015, at 06:14 AM, Ian Cordasco wrote:

> As in, ensuring that `sys.modules` has both `urllib3.submodule` and
> `requests.packages.urllib3.submodule`? I think this is the least magic way
> and given that @Lukasa and I are both core developers of urllib3, we will
> catch any new submodules that need to be added to requests' patching
> logic. I'm most strongly in favor of this one.

It's mildly disconcerting that a library would fiddle with another library's
sys.modules namespace, but I agree that this is probably the least magical
(and thus most likely to work) way.  Maybe we can convince @brettcannon to
build a nicer foolproof <wink> module alias system for 3.6. :)

> We can take a short term solution of the first option for a `2.8.2` release
> and get that updated patch to @eriol, @warsaw, @ralphbean, etc. so distros
> can fix their packages while taking a look at the feasibility and reliability
> of option 3. I still prefer the first option to the third, but the third may
> have benefits I'm not seeing due to the bad taste left in my mouth by the
> `VendorAlias` (a.k.a., option 2).

Yep, I suspect that #1 may be the best approach, and that #3 would be
difficult to debug if things go south.  It may be that no solution is perfect,
given Python's current import semantics and implementation, in which case
doing the best you can with the least magic (i.e. most discoverable and
debuggable) would suck less.

Thanks!
",warsaw,brettcannon
2870,2015-11-17 15:36:29,"On Nov 17, 2015, at 06:14 AM, Ian Cordasco wrote:

> As in, ensuring that `sys.modules` has both `urllib3.submodule` and
> `requests.packages.urllib3.submodule`? I think this is the least magic way
> and given that @Lukasa and I are both core developers of urllib3, we will
> catch any new submodules that need to be added to requests' patching
> logic. I'm most strongly in favor of this one.

It's mildly disconcerting that a library would fiddle with another library's
sys.modules namespace, but I agree that this is probably the least magical
(and thus most likely to work) way.  Maybe we can convince @brettcannon to
build a nicer foolproof <wink> module alias system for 3.6. :)

> We can take a short term solution of the first option for a `2.8.2` release
> and get that updated patch to @eriol, @warsaw, @ralphbean, etc. so distros
> can fix their packages while taking a look at the feasibility and reliability
> of option 3. I still prefer the first option to the third, but the third may
> have benefits I'm not seeing due to the bad taste left in my mouth by the
> `VendorAlias` (a.k.a., option 2).

Yep, I suspect that #1 may be the best approach, and that #3 would be
difficult to debug if things go south.  It may be that no solution is perfect,
given Python's current import semantics and implementation, in which case
doing the best you can with the least magic (i.e. most discoverable and
debuggable) would suck less.

Thanks!
",warsaw,eriol
2870,2015-11-17 15:36:29,"On Nov 17, 2015, at 06:14 AM, Ian Cordasco wrote:

> As in, ensuring that `sys.modules` has both `urllib3.submodule` and
> `requests.packages.urllib3.submodule`? I think this is the least magic way
> and given that @Lukasa and I are both core developers of urllib3, we will
> catch any new submodules that need to be added to requests' patching
> logic. I'm most strongly in favor of this one.

It's mildly disconcerting that a library would fiddle with another library's
sys.modules namespace, but I agree that this is probably the least magical
(and thus most likely to work) way.  Maybe we can convince @brettcannon to
build a nicer foolproof <wink> module alias system for 3.6. :)

> We can take a short term solution of the first option for a `2.8.2` release
> and get that updated patch to @eriol, @warsaw, @ralphbean, etc. so distros
> can fix their packages while taking a look at the feasibility and reliability
> of option 3. I still prefer the first option to the third, but the third may
> have benefits I'm not seeing due to the bad taste left in my mouth by the
> `VendorAlias` (a.k.a., option 2).

Yep, I suspect that #1 may be the best approach, and that #3 would be
difficult to debug if things go south.  It may be that no solution is perfect,
given Python's current import semantics and implementation, in which case
doing the best you can with the least magic (i.e. most discoverable and
debuggable) would suck less.

Thanks!
",warsaw,warsaw
2870,2015-11-17 15:36:29,"On Nov 17, 2015, at 06:14 AM, Ian Cordasco wrote:

> As in, ensuring that `sys.modules` has both `urllib3.submodule` and
> `requests.packages.urllib3.submodule`? I think this is the least magic way
> and given that @Lukasa and I are both core developers of urllib3, we will
> catch any new submodules that need to be added to requests' patching
> logic. I'm most strongly in favor of this one.

It's mildly disconcerting that a library would fiddle with another library's
sys.modules namespace, but I agree that this is probably the least magical
(and thus most likely to work) way.  Maybe we can convince @brettcannon to
build a nicer foolproof <wink> module alias system for 3.6. :)

> We can take a short term solution of the first option for a `2.8.2` release
> and get that updated patch to @eriol, @warsaw, @ralphbean, etc. so distros
> can fix their packages while taking a look at the feasibility and reliability
> of option 3. I still prefer the first option to the third, but the third may
> have benefits I'm not seeing due to the bad taste left in my mouth by the
> `VendorAlias` (a.k.a., option 2).

Yep, I suspect that #1 may be the best approach, and that #3 would be
difficult to debug if things go south.  It may be that no solution is perfect,
given Python's current import semantics and implementation, in which case
doing the best you can with the least magic (i.e. most discoverable and
debuggable) would suck less.

Thanks!
",warsaw,Lukasa
2870,2015-11-17 17:11:32,"I created http://bugs.python.org/issue25649 for the aliasing idea that @warsaw had (I wouldn't count on it getting a blessed solution, but you never know :) .

And just to toss in my opinion, the explicit aliasing is the safest, most compatible way to do things (it's still not fool-proof since `__name__`, the spec object, etc. will still be the other name, but it's better than nothing). Getting a custom importer right is hard and using a non-module object is actually done in the wild, but some people make assumptions as to the kind of objects they get back from an import so it really depends on your user base whether it will work or not.
",brettcannon,warsaw
2869,2015-11-10 12:51:02,"How about now @Lukasa ? I rewrote the third `if` statement as well, for consistency.
",miroli,Lukasa
2868,2015-11-09 15:06:41,"If I know @jamielennox, they're working on SAML auth plugin. Part of (some of) the SAML authentication flow(s) includes cookies being sent that aren't meant to be persisted on the session.

I think what @jamielennox wants is for those cookies that are meant to be part of an intermediate step in authentication not to be persisted. After a series of redirects, we process everything and update the Session cookie jar with the cookies from individual responses in the history. I suspect @jamielennox was removing the cookies individually from the intermediate responses hoping that would prevent them from being persisted on the Session but that isn't working because we don't use those attributes when extracting cookies to a session cookie jar. (Further, this is likely a plugin for OpenStack.)

I don't think there is necessarily a bug here. We're doing the right thing for the 99% case.

@jamielennox I suspect the cookie names are predictable and you could have a CookieJar that ignores those cookie values (along the lines of a far more selective ForgetfulCookieJar which @ceaess is adding to the requests-toolbelt).
",sigmavirus24,jamielennox
2868,2015-11-10 15:18:45,"@jamielennox I'm currently writing tests for the ForgetfulCookieJar and I'm confident it should be available in requests-toolbelt in the next week or so.
",ceaess,jamielennox
2868,2015-11-17 00:57:35,"@Lukasa Yes that's all correct, it's just that the SAML case is the first time i've run across cookies being used at all in OpenStack. I need to look into a way of managing cookies seperate to what the requests.Session is doing. For almost all cases what i need now is just drop all cookies.
",jamielennox,Lukasa
2868,2015-11-17 15:49:49,"@jamielennox so you can lift the couple lines of code that constitute the [ForgetfulCookieJar](https://github.com/sigmavirus24/requests-toolbelt/blob/master/requests_toolbelt/cookies/forgetful.py#L5) or you can wait for/help with requests-toolbelt 0.5 so you can add it to OpenStack.
",sigmavirus24,jamielennox
2867,2015-11-09 02:49:43,"@sigmavirus24 No problem - the pip versions should be good though?
",sjmh,sigmavirus24
2867,2015-11-09 02:54:58,"@sjmh should be!
",sigmavirus24,sjmh
2867,2015-11-09 03:05:09,"@sigmavirus24 Alrighty - thanks, just confirmed those versions work! Do you know if there's any way to get around the issue with those RPM versions?  If not, is there a minimum version that this issue would be fixed in ( Looks like newest from pip is 2.8.1 ) or is this a general thing for all downstream redistributors in any version?

Thanks for the help.
",sjmh,sigmavirus24
2866,2016-02-02 10:37:48,"@ml31415 Given that we're going to try to move the chunked encoding logic to urllib3, that's probably true. However, the time investment here is still useful, because this kind of focused socket-based testing is better than using httpbin. =)
",Lukasa,ml31415
2866,2016-02-02 13:09:42,"@ml31415 If urllib3 switches away from Tornado we'd like to switch to a socket-level test approach as you saw with the chunked encoding tests. That's something we're interested in doing, but it's hard to find the time to do the migration. If you're volunteering, then go for it!
",Lukasa,ml31415
2865,2015-11-07 18:02:11,"@Lukasa that won't work. We expect `data` to be a dictionary (or list of tuples with length two) and `files` to be the same. That said, without a description of what the endpoint you're talking to is actually expecting, we can't provide more help than we already have.

This is probably a problem better suited for [StackOverflow](https://stackoverflow.com/questions/tagged/python-requests).
",sigmavirus24,Lukasa
2864,2015-11-07 21:41:13,"Thanks @aartur 
",sigmavirus24,aartur
2863,2015-11-26 10:47:51,"@catlee My gift to you is the first part of the work required for this: shazow/urllib3#751.
",Lukasa,catlee
2863,2016-03-22 21:24:54,"Yeah, essentially that's the plan. Last I checked, I think @shazow and I were unclear on exactly the direction that change should take.
",Lukasa,shazow
2862,2015-11-06 15:51:06,"This issue can now be followed as [PyPy issue #2183](https://bitbucket.org/pypy/pypy/issues/2183/pypy-400-ssl-module-appears-to-leak-memory). Thanks for the report @aartur!
",Lukasa,aartur
2861,2015-11-07 14:30:23,"@sigmavirus24 This should be good to merge.
",Lukasa,sigmavirus24
2861,2015-11-07 17:50:48,"@Lukasa I'm :+1: but I think we should work on fixing our test suite. If Kenneth comes along and decimates it again, then we at least had test coverage for some period of time there.
",sigmavirus24,Lukasa
2859,2015-11-05 15:37:35,"Thanks @Lukasa!
",sigmavirus24,Lukasa
2856,2015-11-03 22:31:14,"Hi @bodenr thanks for using requests and stopping by with this feature request.

The library strives to make very specific divides in what is appropriate to add where. Timeout values being specified on a session do not make very much sense to the project although it's plausible for you to add some code to do this (like [pip](/pypa/pip)) has. 

In the future please search for closed and open issues discussing your topics. A quick search immediately brought up https://github.com/kennethreitz/requests/issues/2011 which was closed over a year ago. I'm sure there are others, but you can find them too with a quick search.
",sigmavirus24,bodenr
2855,2015-11-03 22:36:23,"Thanks @ZuluPro but we already use an external Jenkins server that we maintain. Travis proved to be unstable for our needs.

Cheers!
",sigmavirus24,ZuluPro
2852,2015-10-29 00:17:41,"This is actually a new bug, it's not the same as shazow/urllib3#729. urllib3 should be wrapping `SysCallError` in this case. @steveoh, can you raise this issue on urllib3?
",Lukasa,steveoh
2852,2015-10-29 22:19:17,"So @Lukasa, how does that little change affect us here? How will requests act differently with the wrapped error?
",steveoh,Lukasa
2852,2015-10-30 00:42:38,"@steveoh urllib3 should now see this as a standard socket error, and so wrap it, and requests will then wrap _that_. I'd expect a standard requests error to be seen.
",Lukasa,steveoh
2852,2015-10-30 01:03:56,"@Lukasa Any idea why the socket is throwing the error in the first place?
",steveoh,Lukasa
2852,2015-10-30 06:56:09,"@steveoh Sure. The error is clear if you happen to speak Windows error codes (and I do). ;)

The error code WSAECONNRESET means that the TCP connection was forcefully closed by the remote peer. This means something about this request the server doesn't like. It'd be interesting if you checked what the state of your system was when it happened so you could see what the request was.
",Lukasa,steveoh
2852,2015-11-02 19:07:40,"@Lukasa could it be that there's a load balancer terminating TLS and that's timing out the connection?
",sigmavirus24,Lukasa
2851,2015-10-27 12:53:10,"@djchou In this context, `user` and `pass` are placeholder strings to be replaced by your own Github username and password.
",Lukasa,djchou
2851,2015-10-27 13:04:25,"Yes, I replaced them with my user pass and still had the same result. 

> On Oct 27, 2015, at 5:53 AM, Cory Benfield notifications@github.com wrote:
> 
> @djchou In this context, user and pass are placeholder strings to be replaced by your own Github username and password.
> 
> —
> Reply to this email directly or view it on GitHub.
",djchou,djchou
2851,2015-10-27 13:07:27,"@djchou What is the result of:


",Lukasa,djchou
2851,2015-10-27 13:28:04,"ah, its the two factor auth that is causing it to fail.

'{""message"":""Must specify two-factor authentication OTP
code."",""documentation_url"":""https://developer.github.com/v3/auth#working-with-two-factor-authentication""}'

On Tue, Oct 27, 2015 at 6:07 AM, Cory Benfield notifications@github.com
wrote:

> @djchou https://github.com/djchou What is the result of:
> 
> import requests
> r = requests.get('https://api.github.com', auth=('user', 'pass'))print(r.content)
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/2851#issuecomment-151489049
> .
",djchou,djchou
2850,2015-10-26 22:36:11,"@pcsysen Can you also print the headers on the response, as observed by requests?
",Lukasa,pcsysen
2850,2015-10-26 23:42:29,"@Lukasa I am sorry that I don't understand requests well enough. Could you show me how to do it? Thanks.
",pcsysen,Lukasa
2850,2015-10-27 00:04:00,"@pcsysen Sure:


",Lukasa,pcsysen
2850,2015-10-27 00:48:34,"@Lukasa Are you talking about the normal result? When it is working it is like this



I don't know if it tells anything. I was thinking maybe you want to see the header when the error happens, which I don't know and I failed to learn it from the example you posted.
",pcsysen,Lukasa
2850,2015-10-27 01:47:28,"@pcsysen Can you guarantee it'll happen _eventually_ if you put it in a `while True` loop?
",Lukasa,pcsysen
2850,2015-10-27 13:50:13,"@pcsysen is `response.content` always bytes? If not is it possible that the requests where you saw this had multi-byte characters in them? (To be clear, I'm fairly certain this isn't caused by requests at this point.)
",sigmavirus24,pcsysen
2850,2015-10-28 22:50:44,"@sigmavirus24 I don't quite understand what you mean by ""always bytes"". The response should be all ASCII characters.
",pcsysen,sigmavirus24
2850,2015-10-29 00:07:46,"@pcsysen It _should_ always be ASCII, but _is_ it?
",Lukasa,pcsysen
2850,2015-11-05 09:59:40,"@pcsysen Ping =)
",Lukasa,pcsysen
2849,2015-10-26 15:57:59,"Thanks @annp89 !
",sigmavirus24,annp89
2848,2015-10-26 20:48:37,"@flupke The urllib3 and requests projects work together closely: we're entirely aware of the bug that you're encountering, but we won't be able to fix it until a new urllib3 release comes out.
",Lukasa,flupke
2847,2015-10-26 13:48:45,"Please do @annp89. I think it was just an error code that we happened to miss. I don't think there was any reason or concern we had with adding it.

Cheers!
",sigmavirus24,annp89
2844,2015-12-16 14:48:23,"@untitaker please open a new issue.
",sigmavirus24,untitaker
2844,2015-12-17 10:05:37,"@touilleMan We're planning to release 2.9.1 on Monday.
",Lukasa,touilleMan
2840,2015-10-21 11:37:40,"@frispete That test is supposed to fail: it is marked `pytest.xfail`. You need to run the tests using py.test.
",Lukasa,frispete
2840,2015-10-21 11:49:27,"@frispete How did you run the test? Because I clearly see it marked as `xfail`.
",Lukasa,frispete
2840,2015-10-21 11:54:19,"@Lukasa I build distribution packages for openSUSE. Since the tests will fail in Build Service due to ""no network"" policy during build, I test these packages in a local installation at least. 

cd requests-2.8.1
./test_requests.py

BTW, I considered to add a feature request to have an --local test option, e.g. using vcrpy.
",frispete,Lukasa
2840,2015-10-21 11:56:39,"@frispete We have a plan to swap to using pytest-httpbin, we just haven't done that yet.
",Lukasa,frispete
2840,2015-10-21 11:57:42,"@Lukasa Does that allow offline tests? Scratch that, I read the purpose: sounds great.
",frispete,Lukasa
2840,2015-10-24 15:42:29,"@frispete we have a similar policy in Debian and I follow the same workflow, testing requests locally.

I'm testing calling py.test (and py.test-3: on Debian we had to version the name, I know it's ugly...) directly and all the tests are fine (with one xfailed):



@Lukasa it's a very good news the plan about switching to pytest-httpbin! I would like to help, is there a ticket about this?
",eriol,frispete
2840,2015-10-24 15:42:29,"@frispete we have a similar policy in Debian and I follow the same workflow, testing requests locally.

I'm testing calling py.test (and py.test-3: on Debian we had to version the name, I know it's ugly...) directly and all the tests are fine (with one xfailed):



@Lukasa it's a very good news the plan about switching to pytest-httpbin! I would like to help, is there a ticket about this?
",eriol,Lukasa
2840,2015-10-24 15:47:34,"@eriol Yup, see #2184. We're very close now, just got problems with one threaded test.  
",Lukasa,eriol
2840,2015-10-28 12:22:03,"@Lukasa Confirming this issue too (requests 2.8.1). The invocation method is setup.py test, which runs py.test directly. I was a bit confused too when I saw the mark.xfail, but it's definitely making the tests fail:



pytest version is 2.7.1.
",koobs,Lukasa
2840,2015-10-28 12:30:52,"@koobs out of interest, what happens if you use a newer py.test?
",Lukasa,koobs
2840,2015-10-28 12:50:25,"@Lukasa It's on my list of ports to update, will report back post-update :)
",koobs,Lukasa
2840,2015-10-28 14:04:26,"@Lukasa Same failure with pytest 2.8.2 (latest py as well)

Edit: Fails with setuptools test command. Direct py.test passes (that test), but fails on another :)


",koobs,Lukasa
2840,2015-10-29 00:19:16,"@koobs So this suggests there's some problem with the py.test to `setup.py test` integration (given that xfail doesn't work appropriately).
",Lukasa,koobs
2840,2015-10-29 02:59:53,"@Lukasa I would concur given the tests/evidence so far. Perhaps @hpk42 could shed some light on it
",koobs,Lukasa
2840,2015-10-29 09:57:03,"@hpk42 Thanks for responding :) TLDR is: tests marked xfail seem to be reported as failures (not expected failures, resulting in a OK on the test suite) for `setup.py test` and `python -m pytest` invocations of py.test (py.test direct works fine)
",koobs,hpk42
2840,2015-10-29 15:00:08,"@koobs i can't reproduce. Both ""python -m pytest"" and ""setup.py test"" work fine for me for an xfailing test when using the standard integration https://pytest.org/latest/goodpractises.html#integration-with-setuptools-test-commands  (using pytest-2.8.2 but i think it worked before). 
",hpk42,koobs
2840,2015-11-04 05:45:56,"@hpk42 Interesting. I'm out of ideas for isolating/debugging it from here, any thoughts? Just for clarity I was just +1 on the original report by @frispete 
",koobs,frispete
2840,2015-11-04 05:45:56,"@hpk42 Interesting. I'm out of ideas for isolating/debugging it from here, any thoughts? Just for clarity I was just +1 on the original report by @frispete 
",koobs,hpk42
2837,2015-10-20 08:24:32,"@ueg1990 This is occurring because you're using this on an old version of Python which does not have a fully-featured `ssl` module. You can fix it either by upgrading to 2.7.9 or later, or by installing `pyopenssl`, `ndg-httpsclient`, and `pyasn1`.
",Lukasa,ueg1990
2836,2015-10-20 08:26:58,"@causton81 What evidence do you have that `Session.verify` is ignored? This works fine for me:


",Lukasa,causton81
2836,2015-10-20 14:46:59,"Ah, yes, I see the problem now.

We have a similar issue with proxy settings, where proxies set in the environment can override proxies set on the session. I'm beginning to wonder if we can fix two problems in one go here, by taking the `trust_env` block of `merge_environment_settings` and moving it below the `merge_setting` lines in that function.

@causton81 This is definitely a real bug, but we need a bit of time to work out what the fix is and what branch it'll go on to (while this is strictly a bug fix, it's been in the product long enough that it's also an API compatibility break and needs to be dealt with cautiously).

@sigmavirus24 What do you think about my proposed rearrangement of `merge_environment_settings`?
",Lukasa,causton81
2836,2015-10-20 16:19:58,"@Lukasa the rearrangement is something we had discussed for 3.0.0 because it's fundamentally backwards incompatible. I'm in favor of it though because I think the order of precendence should be (in order of least to most important):
- Environment
- Session settings
- Per-call settings

(For those features to which this applies)
",sigmavirus24,Lukasa
2835,2015-10-20 08:30:20,"@trcarden Our intention is that we will forbid it: it's simply not a sensible thing to pass to this library. Generally speaking, the better logic for a 'fire-and-forget' request is to set `stream=True` and simply close the request afterwards. This will ensure that the server really did receive the request, but won't block on reading any of the response body.
",Lukasa,trcarden
2835,2015-10-21 17:57:50,"Ok, I am trying to be practical here. I don't want to wait on the response coming back from the server but i do want to obligate the system to send the request. I could use UDP sockets but that requires a new server and a bunch more overhead. I completely understand TCP/IP and HTTP aren't really built for this but i think i can get 90%+ of what i need just setting a small value on the read-timeout. I did read in a couple places that setting a timeout of zero doesn't ""obligate"" the system to send the message (might only make it to buffers without getting transmitted) depending on the local implementation so i see why my problem might have been occurring. 

@sigmavirus24 thank you for the note on the headers. Emulating a ""best effort"" protocol on top of TCP/IP and HTTP with a upper bound on total wait time (limitations acknowledged) seems to work better with low read-timeout settings than enabling streaming responses. Eventually I think we will enable a UDP server.
",trcarden,sigmavirus24
2833,2015-10-17 08:40:29,"Iiiinteresting. So @causton81, I think you're right. Diving into the `httplib` source code shows this:



So, I think you're right and this doesn't work. I think this would be best implemented in `urllib3`, frankly: in urllib3 they can take advantage of the `HTTPResponse.length` parameter that `httplib` maintains (but is weirdly refusing to use) to make sure this works in almost all cases. @shazow does that sound reasonable to you?
",Lukasa,causton81
2831,2015-10-16 12:21:43,"Thanks for this @shagunsodhani! :sparkles: :cake: :sparkles:
",Lukasa,shagunsodhani
2830,2015-10-16 14:12:49,"No worries @tribals. If you read my blog post about retries with requests I probably failed to mention this. Cheers!
",sigmavirus24,tribals
2829,2015-10-15 14:37:15,"Thanks for this @dblia! :cake: :sparkles: :cake:
",Lukasa,dblia
2829,2015-10-15 14:38:23,"Thanks for the immediate response @Lukasa and @sigmavirus24!
",dblia,Lukasa
2829,2015-10-15 14:38:23,"Thanks for the immediate response @Lukasa and @sigmavirus24!
",dblia,sigmavirus24
2828,2015-10-15 13:41:12,"@czerwe 
1. I don't understand your question
2. The issue tracker is not a support forum. Please use [StackOverflow](https://stackoverflow.com/questions/tagged/python-requests)
",sigmavirus24,czerwe
2828,2015-10-15 13:44:50,"It is not an error in the Json handler. The json handler does not accept a _string_, it accepts the Python object directly. The difference between what you provided and what @sigmavirus24 gave is this:


",Lukasa,sigmavirus24
2827,2015-10-15 13:01:10,"Thanks for this @daniel-mueller!
",Lukasa,daniel-mueller
2826,2015-10-16 11:36:37,"@Lukasa I have opened a pull request for this.
",shagunsodhani,Lukasa
2826,2015-10-19 09:48:10,"Thanks @shagunsodhani. I got busy and didn't notice this.
",TorKlingberg,shagunsodhani
2825,2015-10-14 13:29:40,"@Lukasa do we want to add that warning in `super_len` and then make it a hard exception in 3.0.0?
",sigmavirus24,Lukasa
2825,2015-10-14 13:35:01,"@sigmavirus24 Yeah, probably.
",Lukasa,sigmavirus24
2825,2015-10-14 15:58:40,"&hearts; thanks brosephs @sigmavirus24 @Lukasa 
",chrismattmann,Lukasa
2825,2015-10-14 15:58:40,"&hearts; thanks brosephs @sigmavirus24 @Lukasa 
",chrismattmann,sigmavirus24
2825,2015-10-24 04:41:24,"@Lukasa do you want me to send that separately or do you want to do that as part of this PR?
",sigmavirus24,Lukasa
2825,2015-10-24 14:37:28,"Excellent work @Lukasa . Thanks!
",sigmavirus24,Lukasa
2819,2015-10-11 14:32:28,"@asieira Thanks for this! This is a duplicate of #2813, and should be resolved in 2.8.1.
",Lukasa,asieira
2818,2015-10-14 18:58:49,"@fitnr It's going to be really important to have a traceback/use-case. The problem will be mixing unicode/bytes, but we really need to see where the unicode is getting into our system. =)
",Lukasa,fitnr
2818,2015-10-14 19:04:58,"@Lukasa I understand, but I don't have time to crack open Tweepy at the moment. The POST payload did contain unicode, I can tell you that.
",fitnr,Lukasa
2818,2015-10-14 19:54:06,"@jorilallo care to elaborate? Any details you can provide? Anything to help us help you?
",sigmavirus24,jorilallo
2818,2015-10-15 10:48:48,"oh I see. Makes sense.
I guess the problem goes back to @jorilallo , this is a rather simple fix in the coinbase python library.
Thank you for checking this out!
",netanelkl,jorilallo
2818,2015-10-20 00:58:51,"@Lukasa It appears that example works with requests 2.7.0 and a default encoding of 'utf-8'. The issue we are encountering is that upgrading requests from 2.7.0 now breaks all requests that could have utf-8 data being sent in the data field, without prior warning. Would there be a way to avoid this breaking change?
",vinodc,Lukasa
2818,2015-10-20 07:52:19,"@vinodc To be clear, the error you're encountering is a known bug in urllib3 that has been fixed (see shazow/urllib3#719). However, that fix will not be pulled into the released version of requests until urllib3 ships a new release and then requests ships 2.9.0. Until such time, you could apply the diff from the linked patch to your installation or, alternatively, simply encode the data yourself.

> It appears that example works with requests 2.7.0 and a default encoding of 'utf-8'.

The reason I encourage you to encode the data yourself is that the example only works _by sheer good luck_. There are many ways this could fail, and it could fail in other environments. Relying on Python 2.7's implicit encoding/decoding is extremely dangerous, and I strongly encourage you to resolve this problem.
",Lukasa,vinodc
2818,2015-10-20 08:02:04,"@Lukasa Got it. Thanks for the detailed response.
",vinodc,Lukasa
2817,2016-09-07 08:50:52,"@JohnVillalovos Generally that patch is fine. It'd better to pass the `no_proxy` entry around directly rather than the entire proxies dictionary, so I'd suggest refactoring to do that.

Feel free to open a PR with that patch and we can make suggestions about testing.
",Lukasa,JohnVillalovos
2817,2016-09-08 09:21:36,"@JohnVillalovos We shouldn't even get to there if `no_proxy` has been set by the user. That function has short-circuit returns, so if `no_proxy` was set by the user we should be able to get out of the function well before we ever reach it.
",Lukasa,JohnVillalovos
2817,2016-09-08 17:17:58,"@Lukasa Do you think that on https://github.com/JohnVillalovos/requests/blob/a3c5b0325f74517707ddda91de38d55071c9c76a/requests/utils.py#L613

It should maybe just be:


",JohnVillalovos,Lukasa
2817,2016-09-08 17:27:08,"@JohnVillalovos Arg, further reading suggests that maybe it _is_ necessary. Sorry!
",Lukasa,JohnVillalovos
2817,2016-09-08 17:31:39,"@Lukasa Arg, I just removed it!  heh.

Let me know if I should put it back, or if you think it will work.
",JohnVillalovos,Lukasa
2816,2015-10-09 18:50:26,"@warsaw thanks to make this happen! :smile: 
I started reading debian-python@l.d.o after my reply here!
@Lukasa if you can point me to the OpenStack issue I will include it into the description of the patch Debian will use.
Thanks!
",eriol,warsaw
2816,2015-10-09 18:50:26,"@warsaw thanks to make this happen! :smile: 
I started reading debian-python@l.d.o after my reply here!
@Lukasa if you can point me to the OpenStack issue I will include it into the description of the patch Debian will use.
Thanks!
",eriol,Lukasa
2816,2015-10-10 08:18:20,"@eriol OpenStack issue is [here](https://review.openstack.org/#/c/213310/0).
",Lukasa,eriol
2816,2015-10-11 15:20:26,"Thank you @ralphbean and @eriol for working with us on this. :)
",sigmavirus24,ralphbean
2816,2015-10-11 15:20:26,"Thank you @ralphbean and @eriol for working with us on this. :)
",sigmavirus24,eriol
2816,2015-10-11 15:21:27,":heart: Indeed, thanks so much (and thanks @warsaw and @dstufft as well!).
",Lukasa,warsaw
2816,2015-10-23 14:10:41,"@ralphbean Theories?
",Lukasa,ralphbean
2816,2015-10-23 14:25:02,"@vvro At this point there is no evidence that requests did anything to introduce this bug: it was almost certainly introduced downstream. It is _possible_ that this issue caused it, but that remains a Fedora problem. I highly recommend centralising the discussion of this problem on the Red Hat bugzilla, rather than here.
",Lukasa,vvro
2816,2015-11-05 10:00:58,"Alright, that's Debian and Fedora handled. Thanks so much @ralphbean @warsaw @eriol, we :heart: you greatly.
",Lukasa,ralphbean
2816,2015-11-05 10:00:58,"Alright, that's Debian and Fedora handled. Thanks so much @ralphbean @warsaw @eriol, we :heart: you greatly.
",Lukasa,warsaw
2816,2015-11-05 10:00:58,"Alright, that's Debian and Fedora handled. Thanks so much @ralphbean @warsaw @eriol, we :heart: you greatly.
",Lukasa,eriol
2816,2015-11-05 17:20:27,"Thanks @eriol and I'm always happy to help sponsor.  One thing we need to keep in mind is that as we update requests' dependencies, we have to make sure to also update requests setup.py.  I don't think we have any mechanisms in Debian to ensure this but we can probably rely on Ubuntu's -proposed migration to keep track of mismatches: http://people.canonical.com/~ubuntu-archive/proposed-migration/update_excuses.html

The other thing we may want to think about is DEP-8 tests to make these verifications.  Then at least we'll see problems on qa.debian.org.  A README.Debian is a good idea too, but they are easy to miss :(
",warsaw,eriol
2816,2015-11-06 00:03:12,"@warsaw it's a pleasure work with you! Also reading your hints! I had the same thought on README.Debian, but I will add it in the next upload.

Ubuntu's -proposed migration is very interesting! I will keep an eye on it for my packages.

After the git migration I updated my workflow and I discovered some useful tools.
Before the last requests' upload I tried to use [ratt](https://github.com/Debian/ratt) on my laptop to rebuild all the reverse dependencies: it was a bit troublesome since one of the rdeps is blender and it take more than 1 hour to build, so I did not rebuild all of them but only some.
To ""fix"" my building problems I was suggested to use debomatic: https://debomatic.github.io/ :smile: I'm reading the documentation right now.
The only problem is that it's not automated, but I will start to use it for all my packages before uploading them on the archive.

What about a more draconian approach in addiction to DEP-8 test? I'm thinking about testing at build time and let the package fail to build if the setup.py was not correctly updated.

Do you have any hints about safely parse a setup.py? Thanks!
",eriol,warsaw
2815,2015-10-09 14:41:38,"@Lukasa yes, you are right,this is may be a issue of python 3.5,I will close this issue:)
",gengjiawen,Lukasa
2811,2015-10-08 12:50:55,"@Lukasa if the OpenStack gate is indicative of anything, it's between 1.10.4 and 1.11
",sigmavirus24,Lukasa
2811,2015-10-08 12:52:51,"@Lukasa : yes looks like new NewConnectionError raised here: https://github.com/kennethreitz/requests/blob/master/requests/packages/urllib3/connection.py#L142
",aarefiev22,Lukasa
2809,2015-10-08 07:08:34,"I think the core issue here is that requests' certificate store is somewhat out of date. I propose we ship a 2.8.1 containing a cert store update, specifically the `weak.pem` bundle that ships with certifi.

@hombremuchacho Someone I presume was your colleague or another team member (@aehlke) suggested that certifi was missing a certificate you needed, so you couldn't use it as a drop-in replacement. Did you try using `certifi.old_where()`?
",Lukasa,hombremuchacho
2809,2015-10-08 16:37:21,"@Lukasa Thanks, I'll test this now.
",hombremuchacho,Lukasa
2808,2015-10-07 21:37:48,"@Lukasa @sigmavirus24 Thanks for clarifying how this works. I'll open an issue. We use the requests library as part of our project and are missing a certain certificate -- this certificate is present in certifi; however, it hasn't been added to requests. https://github.com/certifi/python-certifi/blob/master/certifi/cacert.pem#L5026
",hombremuchacho,Lukasa
2808,2015-10-07 21:37:48,"@Lukasa @sigmavirus24 Thanks for clarifying how this works. I'll open an issue. We use the requests library as part of our project and are missing a certain certificate -- this certificate is present in certifi; however, it hasn't been added to requests. https://github.com/certifi/python-certifi/blob/master/certifi/cacert.pem#L5026
",hombremuchacho,sigmavirus24
2807,2015-10-10 14:59:30,"Alright, thanks for the clarification.

Then how should I use merge_environment_setting in order to merge only HTTP proxies var ?

Is this correct ?



@sigmavirus24 : I agree, the existing behavior is fine, It just lacks some precision in the documentation though.
",mxjeff,sigmavirus24
2807,2015-10-10 17:36:21,"@mxjeff right, that's why I said we probably need to document it better.

Also your code sample is correct.
",sigmavirus24,mxjeff
2803,2015-10-05 14:32:31,"@asieira No, I think I recommend adding it to your local `.git/info/exclude` file. Text-editor specific config really shouldn't go into the project repository, the world just gets really complex really fast that way.
",Lukasa,asieira
2803,2015-10-05 14:40:22,"LGTM. @sigmavirus24?
",Lukasa,sigmavirus24
2803,2015-10-05 14:54:31,"@Lukasa will you handle the release notes for this or should I do it later tonight?
",sigmavirus24,Lukasa
2803,2015-10-05 14:56:21,"@sigmavirus24 I'll do it.

@asieira Thanks so much for this change! :sparkles: :cake: :sparkles:
",Lukasa,sigmavirus24
2803,2015-10-05 14:56:21,"@sigmavirus24 I'll do it.

@asieira Thanks so much for this change! :sparkles: :cake: :sparkles:
",Lukasa,asieira
2802,2015-10-04 00:14:59,"Hi @sezginriggs thanks for taking the time to open this. We've discussed this previously on https://github.com/kennethreitz/requests/issues/2158 and decided against this. We're currently in a feature freeze (and will be for the rest of the project's life time in all likelihood) so we're not adding new features.

In the future, please search all issues using GitHub before opening new feature requests.

Cheers,
Ian
",sigmavirus24,sezginriggs
2802,2016-01-19 05:16:23,"@sezginriggs use `stream=True`, grab the IP before you read the response body.

More here https://github.com/kennethreitz/requests/issues/2158
",est,sezginriggs
2801,2015-10-03 07:30:28,"@screendriver At the current time there are no plans to support async and await. This is not because they aren't a good idea: they are. It's because to use them requires quite substantial code changes.

Right now requests is a purely synchronous library that, at the bottom of its stack, uses `httplib` to send and receive data. We cannot move to an async model unless we replace httplib. The best we could do is provide a shorthand to run a request in a thread, but asyncio already has just such a shorthand, so I don't believe it would be valuable.

Right now I am quietly looking at whether we can rewrite requests to work just as well in a synchronous environment as in an async one. However, the reality is that doing so will be a lot of work, involving rewriting a lot of our stack, and may not happen for many years, if ever.
",Lukasa,screendriver
2800,2015-10-03 16:32:38,"@Lukasa would you like to revert these changes, or should I?
",sigmavirus24,Lukasa
2800,2015-10-03 17:34:05,"Thanks so much @hosamaly!
",Lukasa,hosamaly
2800,2015-10-03 17:49:21,"You're welcome @Lukasa . Thanks for accepting it.
",hosamaly,Lukasa
2797,2015-10-02 09:08:22,"Thanks @sumitbinnani! :sparkles: :cake: :sparkles:
",Lukasa,sumitbinnani
2795,2015-10-02 07:13:59,"Sorry @sumitbinnani, but there's already an open fix for this in #2763 that I think we're likely to merge. Thanks so much for your contribution though! :sparkles: :cake: :sparkles:
",Lukasa,sumitbinnani
2793,2015-10-03 20:30:12,"@Lukasa You are most certainly correct.  God only knows when I did it.
",MonsieurCactus,Lukasa
2791,2015-09-29 03:12:48,"@sigmavirus24 
Thanks for take care the case.
More details are:

## Environment

1)  A basic ElementaryOS without any configurations, which means a total clean installation from DVD or ISO. (Actually I used 3 different Ubuntu)
- ElementaryOS Freya, based on Ubuntu 14.04 TLS
- ElementaryOS Luna, based on Ubuntu 12.02 TLS
- Docker official python 2.7.10 image which is based on Debian.

2)  A basic CentOS 6.5 without any configuration.
- Python version: 2.6.6
- >>> import ssl
- >>> import requests
- >>> import urllib3

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ImportError: No module named urllib3

## Problem

I wrote a script to require Nuance ASR service, see below code. When I run this script on ElementaryOS or Debian, it get a 500 error from Nuance ASR. And Nuance ASR returns correct 200 http response with recognized text content when I run the script on CentOS.

<pre ><code>
import os
import requests
class Nuance_api():
        """"""Nuance ASR Service API""""""
        def __init__(self, appid, appkey, n_id):
                self.appid = appid
                self.appkey = appkey
                self.id = n_id
        def recogenize(self, language, voicefilename):
                nuance_url = 'https://dictation.nuancemobility.net/NMDPAsrCmdServlet/dictation'
                n_params = {'appId':self.appid, 'appKey':self.appkey, 'id':self.id}
                voice_file = {'file': open(voicefilename, 'rb')}
                n_headers = {'Accept-Language':language, 'Content-Type':'audio/amr;codec=amr;bit=4.75;rate=8000', 'Accept':'text/plain', 'Accept-Topic':'Dictation'}
                r = requests.post(url=nuance_url, params=n_params, headers=n_headers, files=voice_file, verify=False)
                if r.status_code == 200 :
                        return r.text
</code></pre>

## Additional problem

When I use `openssl s_client -connect dictation.nuancemobility.net:443` on Ubuntu evn (whatever which one), it gives an error: 

<pre><code>
SSL handshake has read 4032 bytes and written 609 bytes
---
New, TLSv1/SSLv3, Cipher is RC4-SHA
Server public key is 2048 bit
Secure Renegotiation IS supported
Compression: NONE
Expansion: NONE
SSL-Session:
    Protocol  : TLSv1.2
    Cipher    : RC4-SHA
    Session-ID: 9986FC667995B0BEF3325136811A0D8C4E14901627854D8DC042F9920F341903
    Session-ID-ctx: 
    Master-Key: 15199FB75D47C6B7C1CA86D12010C63C34E7908EBC4902EB5AC91E39A6C965121F9906719857CFDCBC319BEA5824C356
    Key-Arg   : None
    PSK identity: None
    PSK identity hint: None
    SRP username: None
    Start Time: 1443494743
    Timeout   : 300 (sec)
    Verify return code: 20 (unable to get local issuer certificate)
---
</code></pre>

When I use `openssl s_client -connect dictation.nuancemobility.net:443` on CentOS env or `openssl s_client -connect dictation.nuancemobility.net:443 -CApath /etc/ssl/certs` on Ubuntu or Debian, it gives correct response:

<pre><code>
---
New, TLSv1/SSLv3, Cipher is RC4-SHA
Server public key is 2048 bit
Secure Renegotiation IS supported
Compression: NONE
Expansion: NONE
SSL-Session:
    Protocol  : TLSv1.2
    Cipher    : RC4-SHA
    Session-ID: 9986FC667994C5F4F3325136811A0C8C0F5A769166086DE7C042F9920F341B05
    Session-ID-ctx: 
    Master-Key: 0B893DBEF29821430934D7AB665AF2E24E6322E7990EB77415A6715D6196F73E503DC0395DDDFF209D491A144371F5D5
    Key-Arg   : None
    Krb5 Principal: None
    PSK identity: None
    PSK identity hint: None
    Start Time: 1443495250
    Timeout   : 300 (sec)
    Verify return code: 0 (ok)
---
</code></pre>

## Reproduce the fault

for debug purpose, I would suggest the easiest way that to create Docker env and run official python 2.7.10 image, then test Request in it. The instructions for these:

<pre><code>
# Docker installation
$ sudo sh -c ""echo deb https:/get.docker.io/ubuntu docker main > /etc/apt/sources.list.d/docker.list""
$ curl -s https://get.docker.io/gpg | sudo apt-key add -
$ sudo apt-get update
$ sudo apt-get install lxc-docker
# run docker image
$ sudo docker run -i -t python:2.7.10 /bin/bash
# then the current bash is in python 2.7.10 image, and user is root.
# pip install requests
# python MY_SCRIPT_WITH_NECESSARY_PARAMETERS'_VALUE or just to connect dictation.nuancemobility.net:443
</code></pre>
",adrianzhang,sigmavirus24
2791,2015-09-29 09:41:13,"@sigmavirus24 When using Requests, it gets below response.

<pre><code>
/usr/local/lib/python2.7/dist-packages/requests-2.6.0-py2.7.egg/requests/packages/urllib3/connectionpool.py:769: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html
  InsecureRequestWarning)
<html>
<head>
<meta http-equiv=""Content-Type"" content=""text/html; charset=UTF-8""/>
<title>Error 500 x-nuance-sessionid71887ede-d26d-448d-b154-256eecd3dda0
Received QueryRetry: 1 AUDIO_INFO</title>
</head>
<body><h2>HTTP ERROR 500</h2>
<p>Problem accessing /NMDPAsrCmdServlet/dictation. Reason:
<pre>    x-nuance-sessionid71887ede-d26d-448d-b154-256eecd3dda0
Received QueryRetry: 1 AUDIO_INFO</pre></p>
</body>
</html>
</code></pre>

There are tons discussions of the openssl behavior on Debian/Ubuntu. It is quite clear that every ssl client software on Debian/Ubuntu MUST define CA bundle with some parameters by themselves. The sample is how I use `openssl s_client`. To Requests, I tried `export REQUEST_CA_BUNDLE=/etc/ssl/certs/ca-certificates.crt` but still got 500. 

If Nuance ASR service is hard for troubleshooting. My question can be replaced by ""How to make Requests working to access https://www.google.com via TLS v1.2 on Debian/Ubuntu?""

BTW, in order to identify where the problem is, I also tried to revise my code below to use TLS v1, but also got 500. Can you also let me know the usage of Adapter is correct or not please? 

<pre><code>
import os
import requests
from requests_toolbelt.adapters.ssl import SSLAdapter
import ssl
s = requests.Session()
s.mount('https://dictation.nuancemobility.net/', SSLAdapter(ssl.PROTOCOL_TLSv1))
class Nuance_api():
        """"""Nuance ASR Service API""""""
        def __init__(self, appid, appkey, n_id):
                self.appid = appid
                self.appkey = appkey
                self.id = n_id
        def recogenize(self, language, voicefile):
                nuance_url = 'https://dictation.nuancemobility.net/NMDPAsrCmdServlet/dictation'
                n_params = {'appId':self.appid, 'appKey':self.appkey, 'id':self.id}
                voice_file = {'file': open(voicefile, 'rb')}
                n_headers = {'Accept-Language':language, 'Content-Type':'audio/amr;codec=amr;bit=4.75;rate=8000', 'Accept':'text/plain', 'Accept-Topic':'Dictation'}
                r = requests.post(url=nuance_url, params=n_params, headers=n_headers, files=voice_file, verify=False)
                if r.status_code == 200 :
                        return r.text
</code></pre>
",adrianzhang,sigmavirus24
2791,2015-09-30 16:00:34,"@adrianzhang We're really going to need a working repro scenario to help you here. Can you give us some code that fails in the way you observe where curl succeeds?
",Lukasa,adrianzhang
2791,2015-09-30 16:09:22,"@Lukasa 
First of all, I am really appreciate of your support. I did not post the full url because there are credentials in it. Can I send  to you and sigmavirus24 via email? 
",adrianzhang,Lukasa
2791,2015-09-30 16:10:37,"@adrianzhang Of course. If you are concerned about security, my GPG key is available at [Keybase](https://keybase.io/lukasa).
",Lukasa,adrianzhang
2791,2015-09-30 16:28:17,"@Lukasa Thank you so much, email sent to cory@lukasa.co.uk. Any more details you want to know, feel free and let me know please.
",adrianzhang,Lukasa
2786,2016-02-18 15:53:18,"@swistakm We have merged chunked upload support into urllib3. We have not updated to the version of urllib3 that contains that fix, but when we do we'll also remove our own support for timeouts.

However, requests should _never_ lock on sending a socket: that suggests the connection is still up but Jetty is not consuming any data from the socket, leading to the TCP stream being forced open.
",Lukasa,swistakm
2785,2015-09-30 17:36:36,"For the record I agree completely with @Lukasa, his proposal matches perfectly my original suggestion of using `python-requests/<version> (http://http://www.python-requests.org/)` as the default UA.
",asieira,Lukasa
2785,2015-10-05 01:25:25,"@asieira that is not @Lukasa's proposal. Please don't misrepresent people's comments.
",sigmavirus24,Lukasa
2785,2015-10-05 01:25:25,"@asieira that is not @Lukasa's proposal. Please don't misrepresent people's comments.
",sigmavirus24,asieira
2785,2015-10-05 12:11:35,"If I understand @Lukasa 's comments correctly, he agrees that a) removing the kernel version is a good idea, and that b) the Python version is not a vital information to include in the UA, so it could be removed provided this doesn't break pip (which was subsequently confirmed by @dstufft). 

So that would leave us with a default UA with no kernel or Python version, and only contains the requests version. Which is precisely what I had originally proposed.

I'm sorry if I misunderstood any of that. I don't want to start a flame war here or anything, but now I'm genuinely curious as to what part of @Lukasa 's comment you believe I misinterpreted or misrepresented.
",asieira,Lukasa
2785,2015-10-05 12:11:35,"If I understand @Lukasa 's comments correctly, he agrees that a) removing the kernel version is a good idea, and that b) the Python version is not a vital information to include in the UA, so it could be removed provided this doesn't break pip (which was subsequently confirmed by @dstufft). 

So that would leave us with a default UA with no kernel or Python version, and only contains the requests version. Which is precisely what I had originally proposed.

I'm sorry if I misunderstood any of that. I don't want to start a flame war here or anything, but now I'm genuinely curious as to what part of @Lukasa 's comment you believe I misinterpreted or misrepresented.
",asieira,dstufft
2785,2015-10-05 13:17:18,"Damnit @Lukasa you always post a comment ~30s before I can.

@asieira I didn't mean to put you on the offensive. I simply didn't want someone coming along and implementing the wrong thing based on your comment. Clarity and correctness is important.

Frankly, I'm -0 on removing this information but I wouldn't block a PR removing it.
",sigmavirus24,Lukasa
2785,2015-10-05 13:17:18,"Damnit @Lukasa you always post a comment ~30s before I can.

@asieira I didn't mean to put you on the offensive. I simply didn't want someone coming along and implementing the wrong thing based on your comment. Clarity and correctness is important.

Frankly, I'm -0 on removing this information but I wouldn't block a PR removing it.
",sigmavirus24,asieira
2785,2015-10-05 13:23:59,"@Lukasa and @sigmavirus24 thank you for clarifying that. The important part for me is removing the sensitive information. Not at all married to the specific format or having the requests URL there at all, sorry I didn't make that clearer before. You are absolutely right that clarity and correctness are important.

Do we all agree with `requests/<version>` as the UA string, then?

I can submit a PR for this, if you wish, as soon as we agree on the exact format.
",asieira,Lukasa
2785,2015-10-05 13:23:59,"@Lukasa and @sigmavirus24 thank you for clarifying that. The important part for me is removing the sensitive information. Not at all married to the specific format or having the requests URL there at all, sorry I didn't make that clearer before. You are absolutely right that clarity and correctness are important.

Do we all agree with `requests/<version>` as the UA string, then?

I can submit a PR for this, if you wish, as soon as we agree on the exact format.
",asieira,sigmavirus24
2782,2015-09-22 02:26:50,"@tao12345666333 could you explain this? Why is this necessary? Did you encounter a bug? We don't just accept code thrown over the wall without an explanation.
",sigmavirus24,tao12345666333
2782,2015-09-22 03:38:43,"@sigmavirus24 Thanks for your reply.

When I use requests to post json data, the json data will be escaped by default. 
My code is here:



The api server can't parse data correctly, `if include \\uxxxx will create fail!` (The api server is a third party)

Because of requests's default json dumps, all Chinese character will be escaped `\\uxxx`, so I want to change default json.dumps() by it's `ensure_ascii` options.
",tao12345666333,sigmavirus24
2782,2015-09-27 13:10:56,"Hi， @sigmavirus24  I have modified the code before, does not change the original behavior, but to provide a keyword argument. Using this because escape does not an action of `prepare_body()` function.
Therefore, don't use explicit arguments.
",tao12345666333,sigmavirus24
2782,2016-01-30 03:29:55,"@tao12345666333 in this case, you should simply encode the json data yourself and pass it through the `data` parameter. Thanks for sharing, though! I see where you're coming from. 
",kennethreitz,tao12345666333
2782,2016-01-30 07:02:10,"@kennethreitz Thanks! I will try again. :-)
",tao12345666333,kennethreitz
2781,2015-09-21 12:41:48,"@kissgyorgy Thanks for this!

We already do this for DNS names, as you can see in [this code](https://github.com/kennethreitz/requests/blob/master/requests/packages/urllib3/packages/ssl_match_hostname/_implementation.py#L67-L105). The reason we don't do it for IP addresses in SANs is because we currently don't support authenticating IP addresses in SANs.

This is a bit of a flaw in urllib3, and we have an open issue for it in shazow/urllib3#258. Fixing that issue would automatically fix this problem, so I recommend focusing on that.
",Lukasa,kissgyorgy
2780,2015-09-20 12:51:17,"@sigmavirus24 As per my further debugging, the issue is not related to requests. I am closing this issue.
Thanks @sigmavirus24 
",muhammad-ammar,sigmavirus24
2779,2015-12-04 08:25:03,"@kennethreitz: Is this what you have in mind? https://github.com/kennethreitz/requests/pull/2915 How do I confirm that ad tracking codes are still enabled and nothing broke?
",ArcTanSusan,kennethreitz
2778,2015-09-18 16:11:07,"So I took a quick look



So using something like what @Lukasa has suggested should work. Note: the adapter that @Lukasa references in his blog post is also available in the [requests-toolbelt](https://toolbelt.readthedocs.org/en/latest/adapters.html#ssladapter).
",sigmavirus24,Lukasa
2778,2015-09-18 22:41:17,"@Lukasa @sigmavirus24 Thanks a bunch for the advice! I've used the adapters, and:
- Python code works on Ubuntu machines but fails with `SSLV3_ALERT_HANDSHAKE_FAILURE` on OSX
- `openssl s_client -connect service.isracard.co.il:443 -no_ssl2 -no_ssl3 -no_tls1 -no_tls1_1` works on the OSX machine, with `OpenSSL 1.0.2d 9 Jul 2015`

It's probably out of the  `requests` scope, but any idea how to fix this on OSX? The obvious solution seems to be upgrading openSSL version, but it's already upgraded to a pretty new version. Is Python using the same OpenSSL binaries as the command line tools, or is there a way to upgrade the libraries to a working version?
",adamatan,Lukasa
2778,2015-09-18 22:41:17,"@Lukasa @sigmavirus24 Thanks a bunch for the advice! I've used the adapters, and:
- Python code works on Ubuntu machines but fails with `SSLV3_ALERT_HANDSHAKE_FAILURE` on OSX
- `openssl s_client -connect service.isracard.co.il:443 -no_ssl2 -no_ssl3 -no_tls1 -no_tls1_1` works on the OSX machine, with `OpenSSL 1.0.2d 9 Jul 2015`

It's probably out of the  `requests` scope, but any idea how to fix this on OSX? The obvious solution seems to be upgrading openSSL version, but it's already upgraded to a pretty new version. Is Python using the same OpenSSL binaries as the command line tools, or is there a way to upgrade the libraries to a working version?
",adamatan,sigmavirus24
2778,2015-09-18 23:54:56,"@adamatan well that depends on how you've installed Python on OSX. If you used brew and installed that after installing OpenSSL 1.0.2d then Python _should_ be using that. You can check that with 



Since that's the case, I'm going to close this. Feel free to pop into IRC for better help debugging your OpenSSL problems.
",sigmavirus24,adamatan
2775,2015-09-13 12:26:03,"Thanks @ueg1990! :cake:
",Lukasa,ueg1990
2773,2015-09-12 18:35:48,"@jwilk This is certainly a thing we could do. However, it's a bizarrely application specific fix that will not actually help in a lot of cases because applications that use requests will need to opt-in to that functionality. This means they need to know enough to do that, which is not likely.

Really from a security perspective we should switch to disable netrc auth by default (a change that would need to wait until a 3.0.0 release because it's backwards incompatible, though potentially something worth doing).

In the short term, you will get more security either by not using `~/.netrc` files at all (thereby removing the source of the vulnerability altogether) or by constructing AppArmor profiles that limit access to the file to those applications you have pre-authorized to use it.

@sigmavirus24 For the longer term, I'm open to swapping our default here, which is arguably somewhat insecure, though I also just think people shouldn't be writing their passwords down anywhere at all, at least not in plaintext.
",Lukasa,jwilk
2773,2015-09-12 19:05:56,"@Lukasa isn't there already an open issue for turning auto-loading of netrc off by default for 3.0.0? I thought we were in agreement on this already. I've never once thought this was a good idea but we haven't had opportunity to break this behaviour previously.

I'd also be okay moving `get_netrc_auth` (or whatever the function is actually called) into the `auth` module and documenting it publicly.
",sigmavirus24,Lukasa
2773,2016-01-24 17:04:50,"@anarcat That works fine as a user-specific workaround, but we won't ship it in requests itself.
",Lukasa,anarcat
2772,2015-09-11 19:22:07,"Thanks @jimbrowne! :cake: :sparkles: :cake:
",Lukasa,jimbrowne
2771,2015-11-05 22:30:25,"@Lukasa it's getting late by you, want me to tackle this tonight?
",sigmavirus24,Lukasa
2769,2015-09-10 21:14:05,"Thanks @jwilk! :cake: :sparkles: :cake:
",Lukasa,jwilk
2768,2015-09-10 22:01:59,"Actually `__class__` and `self.__class__` are a whole different level of magic that will thoroughly break this for any subclass. Sorry @ueg1990 this is not something I will allow in requests.
",sigmavirus24,ueg1990
2767,2015-09-10 21:12:57,"Thanks @ueg1990! :cake: :sparkles: :cake:
",Lukasa,ueg1990
2765,2015-09-10 11:29:42,"Thanks @ueg1990! I think you're right: feel free to provide a pull request. =)
",Lukasa,ueg1990
2765,2015-09-10 13:36:31,"@ueg1990 It doesn't exist in Python 3. =) Given that this is a one-time operation, we don't care too much about the one-off memory allocation for the temporary list on Python 2.
",Lukasa,ueg1990
2764,2015-09-09 01:13:32,"Dictionary comprehensions were introduced in Python 2.7. Prior to that they are invalid syntax, e.g., for Python 2.5 and 2.6, both of which are supported versions for ~~pep8~~ requests. 

Thanks for contributing though @suhussai!

Perhaps you could look through the bugs and find one that doesn't have a linked PR to try to work on.

Cheers,
Ian!

**Edit** I said pep8 but I meant requests. I clearly can't keep my projects straight anymore. :(
",sigmavirus24,suhussai
2763,2015-10-02 07:30:53,"@sumitbinnani I'd happily accept a pull request that adds only that documentation change.
",Lukasa,sumitbinnani
2763,2015-10-02 13:05:21,"This looks okay to me. How about you @Lukasa? We should probably not have this pending after having merged the documentation for next week.
",sigmavirus24,Lukasa
2758,2015-09-05 21:01:37,"Fantastic @mhils! Thanks! :cake: :sparkles: :cake:
",Lukasa,mhils
2757,2015-09-05 20:59:40,"Thanks @keyan! From the issue:

> On a prepared request, the only thing I think can hurt us is the case when the body attribute is a file object. Is there a specific hook that we're concerned about not being pickleable?

So the concerns seem to be body file objects and hooks.
",Lukasa,keyan
2757,2015-09-20 02:28:18,"@keyan do you have intentions to update this with the tests we were looking to add?
",sigmavirus24,keyan
2757,2015-09-24 01:46:51,"Hey @sigmavirus24, I could use some advice on how to add a file object in the test environment. I just added a commit with a failing test containing a hook. Not sure how to address the actual issue though.
",keyan,sigmavirus24
2757,2015-09-30 12:55:44,"@keyan It is often easiest to just create a file handle to the file under test: `open(__file__)`.
",Lukasa,keyan
2755,2015-09-04 02:39:51,"@drmaples then make a function that handles that ""globally"" for yourself. We won't be introducing this.
",sigmavirus24,drmaples
2754,2015-09-18 22:16:12,"@kennethreitz You're doing an admirable job. ;)
",Lukasa,kennethreitz
2754,2015-10-01 09:29:25,"Hurrah! All tests green. Over to you @sigmavirus24.
",Lukasa,sigmavirus24
2751,2015-08-29 12:21:12,"Thanks @carlosvargas 
",sigmavirus24,carlosvargas
2749,2015-08-31 11:12:00,"@sigmavirus24 sorry.

urlencode in python 2 encode unicode to ?

this is a problem
",pynixwang,sigmavirus24
2748,2015-08-27 00:51:42,"@HeartUnchange are you specifying your own `Host` header? If so, why?
",sigmavirus24,HeartUnchange
2742,2015-08-25 01:45:00,"Thanks @everett-toews ! :sparkles: :cake: :sparkles: 
",sigmavirus24,everett-toews
2741,2015-08-25 14:39:13,"@Lukasa - that's what I intended, and what I'm sure the code does.  Fall back to the scheme lookup if the scheme+hostname lookup fails.  Would you rather I make it 5 lines of code to make that clearer?
",jasongrout,Lukasa
2741,2015-08-25 14:40:09,"@sigmavirus24 - this is my first contribution to requests.  How does testing work in this package?
",jasongrout,sigmavirus24
2741,2015-08-25 18:29:17,"@sigmavirus24, I didn't see any tests in test_requests.py for the current proxies argument to request methods.  I saw a few places where it is set from the urllib getproxies function, presumably to do some actual network calls.  I also saw two tests for the no_proxy environment variable.  Is there a test for the proxies argument to a request that I missed?  I'm happy to write one, just wanted to make sure I didn't miss it (and I was looking for a model for how to write a test that is consistent with your framework).
",jasongrout,sigmavirus24
2741,2015-08-27 18:29:35,"@Lukasa, I've done that now.  I'm not very clear on how this plays with the environment proxies, especially in rebuild_proxies() in the sessions code (https://github.com/kennethreitz/requests/blob/master/requests/sessions.py#L228).  Any help from someone that already knows the call flow and how the proxies are changed at each step is appreciated.  I think I don't need to do anything, but I'm not absolutely sure.
",jasongrout,Lukasa
2741,2015-09-04 19:41:56,"@Lukasa - a friendly ping about reviewing this.  Thanks again for helping this along!
",jasongrout,Lukasa
2741,2015-09-05 04:01:38,"Ok, I'm officially happy with this. @sigmavirus24, can I get you to take a quick look?
",Lukasa,sigmavirus24
2741,2015-09-06 02:17:59,"Looks good to me. Thanks for your hard work and diligence @jasongrout 
",sigmavirus24,jasongrout
2741,2015-09-06 02:26:29,"Hurrah! @sigmavirus24 do we want to merge this before 2.8.0 or hold off until the release after?
",Lukasa,sigmavirus24
2741,2015-09-06 02:45:56,"May as well pull it into 2.8.0. Seems like @jasongrout needs this sooner rather than later.
",sigmavirus24,jasongrout
2741,2015-09-14 16:10:40,"@Lukasa - just curious, is there a planned release date for 2.8.0?
",jasongrout,Lukasa
2740,2015-08-24 12:43:00,"Thanks @qingyunha! :cake:
",Lukasa,qingyunha
2738,2015-08-21 12:21:31,"@komakino has already filed a bug there. Closing to centralize discussion where it belongs.
",sigmavirus24,komakino
2737,2015-08-21 08:08:17,"Yup, this looks right to me. Thanks @mjpieters!
",Lukasa,mjpieters
2735,2015-08-20 21:52:00,"@RuudBurger the next person to report this, please ping me on the bug on your end
",sigmavirus24,RuudBurger
2730,2015-08-15 20:02:04,":sparkles: :cake: :sparkles: Thanks @bmispelon!
",sigmavirus24,bmispelon
2725,2015-08-15 02:31:40,"Hi @Lukasa - what's weird though is that this works fine on *Nix and Mac. Tika Python is a Python library  that uses (at its lowest level) requests to talk to the [Tika JAX RS Server](http://wiki.apache.org/tika/TikaJAXRS). It posts to the /rmeta endpoint. On Linux, calls like `parser.from_file()` work fine - on Windows, they block and block and then finally timeout. I'm not sure how the answer above has to do with that behavior but perhaps I didn't explain it well enough. Any ideas?
",chrismattmann,Lukasa
2725,2015-08-15 03:20:33,"@chrismattmann so there's (as you could guess) a significant difference in behaviour between those two test cases.

When you just give us the raw data as a string, we write it all at once. When you give us an open file descriptor, we pass it down to `httplib`. What httplib then does with it is read 8192 bytes (yes 8 KB) at a time and write it to the socket to stream it. **Note** this is not the same as a chunked upload.

I suspect that Tika server doesn't like getting such small amounts over a period of time.
",sigmavirus24,chrismattmann
2725,2015-08-15 03:25:12,"@sigmavirus24 thanks for the insight. I'll take a look at https://github.com/sigmavirus24/requests-toolbelt/pull/84
",chrismattmann,sigmavirus24
2725,2015-08-15 03:26:05,"One thing too @sigmavirus24 that is kind of odd though - this same behavior with e.g., data=open(filename, 'r') works fine on Linux, e.g., with Tika-server running on Linux and Tika Python running on Linux. It only seems to fail on Windows.
",chrismattmann,sigmavirus24
2725,2015-08-15 03:29:12,"@chrismattmann please be aware that while you debug this in the comments on a closed issue, you're emailing potentially over 700 people subscribed to issues (creation, comments, etc.)
",sigmavirus24,chrismattmann
2725,2015-08-17 05:47:45,"thanks @Lukasa and @sigmavirus24 . The odd thing is this is an extremely small file (default win.ini) and the behavior is that i make the post - then it waits for like 2-3 minutes, then times out. Just odd.
",chrismattmann,Lukasa
2725,2015-08-17 05:47:45,"thanks @Lukasa and @sigmavirus24 . The odd thing is this is an extremely small file (default win.ini) and the behavior is that i make the post - then it waits for like 2-3 minutes, then times out. Just odd.
",chrismattmann,sigmavirus24
2725,2015-08-17 07:13:02,"@chrismattmann That was not clear to me previously, and makes the whole thing substantially more interesting.

I think I've seen something like this on hyper, and couldn't work out what was going on: I was getting situations where I was failing to read data I knew was in the socket buffer. See also: lukasa/hyper#142.
",Lukasa,chrismattmann
2725,2015-08-17 13:49:45,"@chrismattmann that would have been helpful to know at the beginning of this thread. That description seems a bit more consistent with



Note that those are from what I believe are three separate tracebacks, but my Java debugging is several years old and quite rusty.

Could you share _exactly_ the file that you used? Further, what happens if you open the file as



(or the equivalent in your code, the important part is opening the file with the `b` flag).

Let's also try some other permutations:
1. Tika Server running on a non-Windows operating system, with Tika-Python running on Windows
2. Tika Server running on Windows, with Tika-Python running on a non-Windows OS.

I'm fairly confident that this is a bug very far down the stack from us (which is why I'm not reopening this), but I'm interested in helping you find out enough information to report it to the appropriate place.
",sigmavirus24,chrismattmann
2725,2015-08-18 01:13:54,"thanks @Lukasa  and @sigmavirus24 . Here is the file I was using (C:\Windows\win.ini)



I tried with the 'b' flag as well, and there was no difference. 

As for:

> 1. Tika Server running on a non-Windows operating system, with Tika-Python running on Windows
> 2. Tika Server running on Windows, with Tika-Python running on a non-Windows OS.

Got it. I will try both and report back. Thanks for helping.
",chrismattmann,Lukasa
2725,2015-08-18 01:13:54,"thanks @Lukasa  and @sigmavirus24 . Here is the file I was using (C:\Windows\win.ini)



I tried with the 'b' flag as well, and there was no difference. 

As for:

> 1. Tika Server running on a non-Windows operating system, with Tika-Python running on Windows
> 2. Tika Server running on Windows, with Tika-Python running on a non-Windows OS.

Got it. I will try both and report back. Thanks for helping.
",chrismattmann,sigmavirus24
2725,2015-08-31 19:55:43,"any comments here @sigmavirus24 @Lukasa ?
",chrismattmann,Lukasa
2725,2015-08-31 19:55:43,"any comments here @sigmavirus24 @Lukasa ?
",chrismattmann,sigmavirus24
2725,2015-09-03 16:13:37,"hi @Lukasa nope I haven't used it. I did try and find some programs that would do this after I found out Wireshark won't capture localhost on Windows (or it will if you hack some networking routes, etc., but I tried and none of the approaches worked). Any advice here would be appreciated.
",chrismattmann,Lukasa
2725,2015-09-04 19:37:29,"hey @Lukasa yep I tried to use RawCap unfortunately the frickin' page download redirects to an non existent URL and I can't download the tool :( I also couldn't find it on the internet, sadly.
",chrismattmann,Lukasa
2725,2015-09-05 07:04:51,"Hi @Lukasa well see that's the thing - this error only manifests on Windows. There are not any issues from Linux to Linux; from Linux to Windows; but there are errors Windows to Windows.
",chrismattmann,Lukasa
2725,2015-09-06 01:25:29,"@chrismattmann did I misunderstand your earlier comments? I thought we only needed the client to be on Windows to reproduce this.
",sigmavirus24,chrismattmann
2725,2015-09-07 02:05:52,"have either of you @sigmavirus24 and @Lukasa been able to replicate https://github.com/kennethreitz/requests/issues/2725#issuecomment-132040020 and https://github.com/kennethreitz/requests/issues/2725#issuecomment-132092182 on your own machines? That would help I think and I did do the work there hoping you guys would try to replicate.
",chrismattmann,Lukasa
2725,2015-09-07 02:05:52,"have either of you @sigmavirus24 and @Lukasa been able to replicate https://github.com/kennethreitz/requests/issues/2725#issuecomment-132040020 and https://github.com/kennethreitz/requests/issues/2725#issuecomment-132092182 on your own machines? That would help I think and I did do the work there hoping you guys would try to replicate.
",chrismattmann,sigmavirus24
2725,2015-09-07 15:36:36,"@chrismattmann I'm trying to get a hold of a friend who can reproduce this since I don't have access to Windows VMs at the moment. I've been traveling a lot lately so finding time to borrow a friend's laptop hasn't worked out well.
",sigmavirus24,chrismattmann
2725,2015-09-18 05:18:39,"hey guys @Lukasa @sigmavirus24 either of you get a chance to try this or replicate?
",chrismattmann,Lukasa
2725,2015-09-18 05:18:39,"hey guys @Lukasa @sigmavirus24 either of you get a chance to try this or replicate?
",chrismattmann,sigmavirus24
2725,2015-10-13 03:23:44,"guys any progress @Lukasa @sigmavirus24 ?
",chrismattmann,Lukasa
2725,2015-10-13 03:23:44,"guys any progress @Lukasa @sigmavirus24 ?
",chrismattmann,sigmavirus24
2725,2015-10-14 11:07:58,"So, @chrismattmann: right now I can't reproduce because tika-python on my Windows box falls at the first hurdle:


",Lukasa,chrismattmann
2725,2015-10-14 12:18:48,"Interestingly, none of our documentation has a single example of `open` used _without_ binary mode. I'll just provide a docs update that warns about this, then.

@sigmavirus24 files opened in text mode is a thing we can detect in code pretty easily: is it worth us lobbing out a warning about it?
",Lukasa,sigmavirus24
2725,2015-10-14 16:00:14,"thanks @Lukasa and @sigmavirus24 #brosephs &hearts; 
",chrismattmann,Lukasa
2725,2015-10-14 16:00:14,"thanks @Lukasa and @sigmavirus24 #brosephs &hearts; 
",chrismattmann,sigmavirus24
2722,2015-08-19 16:17:46,"Sadly @jasongrout the project has a low communication bandwidth at the moment: I'm moving flat and so am without fixed-line internet, and the other two maintainers are taking well-needed rests. This hasn't been lost, I promise!
",Lukasa,jasongrout
2717,2015-08-12 15:39:11,"@Lukasa that's inaccurate. The traceback comes from an SSL wrapped socket. This has nothing to do with httplib from what I can see. `OverflowErrors` are raised when a value is larger than the underlying C integer size allowed. This can be seen if you call `len(something_larger_than_four_gb)` on a 32 bit system.
",sigmavirus24,Lukasa
2717,2016-10-11 13:55:52,"@eriktews can you share how you're doing the upload? There are ways to stream uploads (like [Lukasa's comment](https://github.com/kennethreitz/requests/issues/2717#issuecomment-130343655) shows). Is there a reason you cannot do that (if you are not already trying that)? Also, can you provide your actual traceback?
",sigmavirus24,eriktews
2717,2017-02-15 22:24:25,"@Lukasa 's method does not work - even with httplib off the signing still happens for the transport itself.  In my case I have a 2GB+ POST request (not a file, just POST data).  This is for an elasticsearch bulk update.  The endpoint only has HTTPS so I'm working through other solutions now.

",adamn,Lukasa
2717,2017-02-16 16:15:23,"@adamn That was not my proposed solution. My proposed solution was to not read the file in manually at all. You are bumping into the same error as before, which is that we are sending a single gigantic string to httplib.

This is a behaviour we can fix: if we spot someone uploading a *gigantic* single string via Python then we can resolve it. But at this point I *strongly* recommend you use an intermediary file object: either one on disk, or by doing the urlencoding yourself and wrapping the result in a `BytesIO`.",Lukasa,adamn
2717,2017-02-16 16:42:10,"@adamn No, that's not necessary. TLS uses stream encryption, it does not need the entire payload at once.

What you're missing is that when given a file object, requests will automatically stream it in smaller chunks (specifically, 8192 kb chunks). Those cause no problem.",Lukasa,adamn
2716,2015-08-28 17:36:50,"@lukasgraf RFC 6265 addresses the issue of empty cookie values in both the `Set-Cookie` grammar in [section 4.1.1](http://tools.ietf.org/html/rfc6265#section-4.1.1) and its interpretation by the client in [section 5.2](http://tools.ietf.org/html/rfc6265#section-5.2) wherein nil values are permitted. Therefore a user agent should return an empty value, subject to constraints imposed when the cookie was set (lifetime, path, etc.), in future requests.
As you remark, an equals symbol is required in the relevant headers, which `cookielib` fails to include.

As a historical note, the grammar in [section 3.2.2 of RFC 2965](https://tools.ietf.org/html/rfc2965#section-3.2) (and [RFC 2901](https://tools.ietf.org/html/rfc2109#section-4.2.2) before it) forbade nil values by specifying cookie values to be tokens (see [section 2.2 of RFC 2616](https://tools.ietf.org/html/rfc2616#section-2.2)) or quoted strings, though the latter could be empty. The original [Netscape proposal](http://curl.haxx.se/rfc/cookie_spec.html) was silent on the matter.
",MikeWinter,lukasgraf
2715,2015-08-08 20:06:01,"+100 to what @Lukasa said. If you want normalized responses, you can use a mapping of status codes to whatever you'd like them to represent.
",sigmavirus24,Lukasa
2714,2015-08-09 15:05:23,"Yeah @sigmavirus24, I'd like to do that too.
",Lukasa,sigmavirus24
2714,2015-08-11 02:51:55,"@Lukasa so this breaks some of our tests around cookies. Particularly because HTTPbin doesn't set the `domain` attribute. So we at least know this works as intended. I'll see how difficult it would be to fix this in httpbin
",sigmavirus24,Lukasa
2714,2015-08-15 15:34:46,"So this isn't breaking tests the way I thought it was. When I looked at which tests are failing, I noticed what was happening. The tests in question all do roughly the following:



In other words, we're expecting a `Cookie` header to be sent that looks like `Cookie: foo=bar` to that URL. This fails because those cookies are naively added to a Cookie Jar and that cookie has no domain associated with it which causes it to not match the request host.

This makes me ask some questions (since I rarely use cookies like this):
- Do users actually use cookies like this?
- Can we safely assume that cookies like this are always meant for the host, e.g., unless the cookie is parsed to have a domain, we forcibly set it to our request host for that request?

Further this also affects users who do something like:



This, however, is far more nebulous. There is no good way to know what domains to send that cookie for. Previously we did something that was arguably really really awful (send it for all domains, I suspect). I think this begs, then, for a helper to create a cookie to be used here. Thoughts @Lukasa?
",sigmavirus24,Lukasa
2714,2015-08-15 19:45:22,"Okay, so @Lukasa and I discussed this in [IRC](https://botbot.me/freenode/python-requests/2015-08-15/?msg=47298372&page=1). The result was the following:
1. Continue with this as it is
2. Update `create_cookie` helper to **require** a domain attribute
3. Disallow string-ish types (str, unicode, bytes, etc.) when assigning to a CookieJar (e.g., the case where you update a session's cookies by doing `s.cookies['foo'] = 'bar'`). This will mean only accepting Cookie objects as the value. (We may also want to validate that said cookie has a domain attribute, or at least issue a warning if it does not.)
4. When passing `cookies=` to a request method, **assume** that if a domain isn't present, it is explicitly for the request domain.

What does this mean for the backport to the `requests-toolbelt` and 2.x:
1. The toolbelt will start to carry the re-implementation of the CookieJar from this request when we move this functionality there.
2. The toolbelt will also carry the implementation of `create_cookie` that **requires** a domain
3. requests 2.x will start issuing a `DeprecationWarning` when doing `s.cookies['foo'] = 'bar'` to warn people of the change coming in 3.0
4. requests 2.x will carry this policy function that's already written so people can start using it and seeing how their code may break but only if they opt in by creating a Policy and a new Cookie Jar with that policy.
",sigmavirus24,Lukasa
2714,2016-03-13 02:59:59,"@Lukasa @sigmavirus24 what is the status of this? I was working a bit in the `RequestsCookieJar` code for #3028, and I'd be willing to work on this a bit if it's still something you think should move forward.
",davidsoncasey,Lukasa
2714,2016-03-13 02:59:59,"@Lukasa @sigmavirus24 what is the status of this? I was working a bit in the `RequestsCookieJar` code for #3028, and I'd be willing to work on this a bit if it's still something you think should move forward.
",davidsoncasey,sigmavirus24
2714,2016-03-13 15:07:30,"@davidsoncasey this is not a simple change. I have a lot of work locally that I'm not yet ready to push.
",sigmavirus24,davidsoncasey
2713,2015-08-07 23:52:35,"@sigmavirus24 got it, I'll remove all the ones except the one for the cookies section. Should I stick with the `api-` prefix or drop that as well?
",lukasgraf,sigmavirus24
2713,2015-08-07 23:57:40,"@sigmavirus24 updated
",lukasgraf,sigmavirus24
2713,2015-08-08 01:18:18,"Thanks @lukasgraf! :sparkles: :cake: :sparkles: 
",sigmavirus24,lukasgraf
2711,2015-08-31 06:52:47,"Sorry @fgimian, it looks like this got lost!

Generally I don't believe automating a single function call is a particularly good idea. The cost of doing that is that it makes it hard to tell from reading code exactly what exceptions a given call into Requests might raise, because they depend on the `Session` object elsewhere in the code.

Given that the only advantage is to save you a single function call, I think this is probably not worthwhile. Sorry!
",Lukasa,fgimian
2711,2015-08-31 07:27:47,"Thanks for your reply @Lukasa, I understand where you're coming from.  Really appreciate you considering my idea :smile: 
",fgimian,Lukasa
2709,2015-08-05 12:57:15,"@sigmavirus24  I understand that.
but why works perfeclty with pycurl?

import pycurl
c = pycurl.Curl()
c.setopt(c.URL,  'https://server.com/server.jsp?login')
postfields = 'USUARIO=username&PASSWORD=contrase%F1a15'
c.setopt(c.VERBOSE, True)
c.setopt(c.POSTFIELDS, postfields)
c.perform()
c.close()

I think maybe is some bug with the encoding.

Because if I try the same with requests, this dont work.
",Wu4m4n,sigmavirus24
2709,2015-08-05 13:36:27,"Thanks for the reply, I hope this could help to another people with the same Issue.
Thanks @sigmavirus24 
",Wu4m4n,sigmavirus24
2706,2015-08-05 05:41:20,"@sigmavirus24 @Lukasa This bug was found when we generate signature based on the order of parameters (signature would be a hash of the url, for example), while on the server side, the signature is validated using the same process based on the query string passed to the server. In this case, the order of the parameters were changed by requests library underneath.

On the other hand, the approach #1921 took is not recommended either, because the order of keys even in normal dictionaries depends on when the insertion and deletion happens, so it is better not to reconstruct the dictionary, do a `del` instead. See the example below:


",ak1r4,Lukasa
2706,2015-08-05 05:41:20,"@sigmavirus24 @Lukasa This bug was found when we generate signature based on the order of parameters (signature would be a hash of the url, for example), while on the server side, the signature is validated using the same process based on the query string passed to the server. In this case, the order of the parameters were changed by requests library underneath.

On the other hand, the approach #1921 took is not recommended either, because the order of keys even in normal dictionaries depends on when the insertion and deletion happens, so it is better not to reconstruct the dictionary, do a `del` instead. See the example below:


",ak1r4,sigmavirus24
2706,2015-08-06 21:19:26,"@sigmavirus24 @Lukasa Any thoughts on this PR?
",ak1r4,Lukasa
2706,2015-08-06 21:19:26,"@sigmavirus24 @Lukasa Any thoughts on this PR?
",ak1r4,sigmavirus24
2706,2015-08-06 22:31:46,"I'm :+1:: @sigmavirus24?
",Lukasa,sigmavirus24
2706,2015-08-06 23:02:55,"Alright, let's try to land this in 2.8.0. @sigmavirus24 I'll go ahead and merge it to the 2.8.0 branch and update the changelog appropriately.
",Lukasa,sigmavirus24
2703,2015-08-04 20:07:36,"@sigmavirus24 

While I don't disagree with you it may be a good idea to update the documentation to state that the quote_plus (or equivalent) is required if passwords are going to contain those characters. This may be the simplest way to resolve this issue. That way future users don't get confused.
",kharmalord,sigmavirus24
2699,2015-07-30 21:24:43,":clap: @Lukasa 
",sigmavirus24,Lukasa
2699,2015-07-30 21:28:40,"@sigmavirus24 Sounds good to me.

Separately, urllib3 may want a fix for this, because it is _also_ affected. /cc @shazow
",Lukasa,sigmavirus24
2699,2015-07-30 21:31:16,"@sigmavirus24 Actually, now that I think about it that's overly aggressive. Generally, things that are `float`-y or `int`-y should be totally acceptable. I think we just explicitly want to forbid `True`/`False`, because they happen to be `int`-y but really shouldn't be.
",Lukasa,sigmavirus24
2699,2015-07-30 21:33:06,"@shazow Not according to the docstring for the `Timeout` class, which is where this happens:



Now, admittedly, the docstring does not allow `True`/`False`, but those can be integer or float if evaluated in that context, so you may want to police them. I can kinda see that `timeout=False` putting the socket in non-blocking mode might make sense in some kind of weird world, but I cannot see how `timeout=True` should put the socket into one-second-timeout mode.
",Lukasa,shazow
2699,2015-07-30 21:36:43,"@offbyone You'd have thought so, right? Except again, the Python `socket` library is all stupid:

>  s.settimeout(`None`) is equivalent to s.setblocking(1)

So...yeah.
",Lukasa,offbyone
2699,2015-07-30 21:37:24,"@Lukasa, this is working now that I'm policing the timeout on my side. For my end, this is fixed. Feel free to leave this open if you want to track the core issue, or close & make a separate one.

Thanks!
",offbyone,Lukasa
2699,2015-07-30 21:37:52,"@offbyone Happy to help: this may be my new favourite bug report on this library. =D

I think we should do some policing of this on our end, so I'm keeping it open but renaming the issue.
",Lukasa,offbyone
2699,2015-07-30 22:51:55,"@Lukasa I was thinking along the lines of



If people have something that should behave like an int/float/tuple they can cast it to that before giving it to us IMO. That said, if we want to eventually allow `Timeout` objects then that might be problematic, but we've never had a desire to allow that so I don't see that being a problem.
",sigmavirus24,Lukasa
2699,2015-08-31 07:00:46,"@sigmavirus24 Ah, crap, that doesn't work. Observe the majesty of Python:



I wonder if we should explicitly disallow anything that compares numerically equal to zero, because setting the socket to non-blocking mode is a wacky and bizarre thing to want to do. Alternatively, we could coerce anything that compares numerically equal to zero to `None` instead, which makes the concrete assertion that when you said `timeout=0` or `timeout=False` what you meant was ""don't time out"".

@shazow @kevinburke It's my assertion that we shouldn't let anyone set `0` as their timeout for either connection or read timeouts, because it doesn't mean what they think it means. We should either coerce to `None` or reject outright. Do either of you disagree?
",Lukasa,shazow
2699,2015-08-31 07:00:46,"@sigmavirus24 Ah, crap, that doesn't work. Observe the majesty of Python:



I wonder if we should explicitly disallow anything that compares numerically equal to zero, because setting the socket to non-blocking mode is a wacky and bizarre thing to want to do. Alternatively, we could coerce anything that compares numerically equal to zero to `None` instead, which makes the concrete assertion that when you said `timeout=0` or `timeout=False` what you meant was ""don't time out"".

@shazow @kevinburke It's my assertion that we shouldn't let anyone set `0` as their timeout for either connection or read timeouts, because it doesn't mean what they think it means. We should either coerce to `None` or reject outright. Do either of you disagree?
",Lukasa,sigmavirus24
2699,2015-09-01 01:32:51,"@Lukasa we could also just explicitly check for boolean before hand so we know we've handled that case. Yes, it is ugly, but we're trying to hide complexity as it is (which is what @offbyone is explaining by thinking aloud through this problem) so we can probably continue to do that.
",sigmavirus24,Lukasa
2699,2015-09-01 01:32:51,"@Lukasa we could also just explicitly check for boolean before hand so we know we've handled that case. Yes, it is ugly, but we're trying to hide complexity as it is (which is what @offbyone is explaining by thinking aloud through this problem) so we can probably continue to do that.
",sigmavirus24,offbyone
2699,2015-09-01 07:15:09,"Only blocking sockets are allowed in requests: non-blocking sockets are explicitly not supported. That means @offbyone is suggesting we reject the value (`ValueError`, probably): does anyone think we should coerce instead?
",Lukasa,offbyone
2699,2015-09-03 03:32:55,"@kevinburke There is one test (`test.with_dummyserver.test_connectionpool.TestConnectionPool.test_total_timeout`) that appears to expect a read timeout of 0 to work (specifically, to raise EAGAIN). This makes me think that maybe you wanted timeout of 0 to work for read. Is that right, or were you just asserting the actual behaviour of setting the timeout in that way?
",Lukasa,kevinburke
2699,2015-11-05 13:26:37,"Ping again @kevinburke.
",Lukasa,kevinburke
2699,2016-02-04 11:28:09,"@kolis What would a zero timeout mean, semantically speaking?
",Lukasa,kolis
2699,2016-07-14 20:05:42,"@Lukasa @sigmavirus24 Is this something we still want patched? [Here](https://github.com/nateprewitt/requests/commit/6b7dd6ae4265cc308e6a7cdbc17fe4812e6caa67) is a quick pass at it, but I'm not sure if this should be PR'd against master or 3.0. There's also a minor side issue with urllib3 that it raises a `ValueError` for values < 0 stating that timeout can be set to 0 or greater.
",nateprewitt,Lukasa
2699,2016-07-14 20:05:42,"@Lukasa @sigmavirus24 Is this something we still want patched? [Here](https://github.com/nateprewitt/requests/commit/6b7dd6ae4265cc308e6a7cdbc17fe4812e6caa67) is a quick pass at it, but I'm not sure if this should be PR'd against master or 3.0. There's also a minor side issue with urllib3 that it raises a `ValueError` for values < 0 stating that timeout can be set to 0 or greater.
",nateprewitt,sigmavirus24
2698,2015-07-30 15:53:36,"We should be using `:ref:` in our docs, so sphinx will handle this for us. Thanks for catching this @nilya 
",sigmavirus24,nilya
2693,2015-07-31 08:09:24,"@Lukasa I wanna ask though, how is `requests` sending the POST requests? My friend has a C code that sends with me to the same table and I just noticed that in one situation, he was sending while my code suddenly stopped sending for around a minute. I am guessing that my code got stuck in one POST request for that minute before it went through. We are both connected to the same network. 

In this case, is there a way to make this more efficient? If I put a timeout, it will just create connections during that minute and then all of them will be received at the same time!
",itsHaddad,Lukasa
2693,2015-08-03 02:24:53,"@itsHaddad you can use a timeout certainly. It looks like it would result in a connection error because connecting would timeout (perhaps). Please don't open an issue for how to properly use a timeout. StackOverflow can help you with that.
",sigmavirus24,itsHaddad
2690,2015-07-24 18:21:45,"> You appear to be the first person to have checked this and found it to be an issue.

Oh, really? I thought I'm just too lazy to find an old and closed issue.

I just happen to have ""transfer.fsckObjects = true"" in my ~/.gitignore...

@sigmavirus24, yep, this looks like a very popular repo. Forking just for one commit would be a solution, but... Ok, I'll just disable the checks in .git/config after the checkout.
",anatolyborodin,sigmavirus24
2690,2015-08-12 23:39:16,"@anatolyborodin Recent versions of git (`master`, but not yet in any released version) have support for configurable fsck checks. So you will be able to set `receive.fsck.badtimezone = warn`, for example, if you would like the usual checks, but would prefer to ignore this particular (minor) problem.
",peff,anatolyborodin
2688,2015-07-26 01:40:19,"Oops, @tiran already confirmed that OpenSSL only accepts 8-bit paths, so there's no need to experiment to determine that.

The issues addressed by win_unicode_console are still a potentially confounding factor in some of the experiments above.

At this point, I'd say the suggestion of force encoding the cert path to UTF-8 (or UTF-8-BOM?) to try to trigger OpenSSL's encoding detection may be worth trying, but it will likely only work in this case if that actually prompts OpenSSL to switch to using the UTF-16-LE Windows APIs.
",ncoghlan,tiran
2688,2015-07-27 23:05:43,"@yyjdelete we have a solution for Python3.4 (always encode as utf-8) but do we have a working solution for Python2? Can you run these tests on Python2:


",alanhamlett,yyjdelete
2688,2015-07-28 02:49:41,"I made an mistake. In python2, `os.getcwdu()` and unicode string should be used instead of normal version, so we can still get the path.

@alanhamlett 
It's hard to test this case, for old python which doesn't has `ssl.SSLContext`(like python 2.7), wrapper in `urllib3/util/ssl_.py` is used instead, which doesn't crash with `load_verify_locations`

---

BTW: Is `os.path.supports_unicode_filenames` should be considered for the case?

Sorry for my poor English, and I'm not an python programmer.
",yyjdelete,alanhamlett
2688,2015-08-23 02:17:14,"@yyjdelete what is the value returned from `os.path.supports_unicode_filenames` on your machine?
",alanhamlett,yyjdelete
2688,2015-08-24 05:37:31,"@alanhamlett
`os.path.supports_unicode_filenames` is `True` in both  py2 and py3
",yyjdelete,alanhamlett
2686,2015-07-22 08:20:28,"@melbic Yeah, sorry, that documentation was written before the `json` parameter was added.

Would you like to propose a pull request with some new wording?
",Lukasa,melbic
2684,2015-07-22 05:59:26,"@sigmavirus24 the link you posted is interesting, but I don't see that it addresses:

> For instance, we can no longer return the response to the caller then inspect status_code.

The linked issue had an insightful explanation by @jvanasco of why this feature is needed, then no response... The issue was closed.

If `requests` means to live up to **http for humans**, then... (quoting the first post):

In keeping with simple is beautiful, it would be terrific if we could do:
`requests.get(url, max_response_size=1024*1024)`
",boolbag,sigmavirus24
2684,2015-07-22 20:53:49,"@Lukasa 

Thank you very much for your extraordinarily detailed and cogent reply. 

I fully get your point.

I'm more than happy to impose checks on `requests` from outside. I just thought that what I needed was not possible because I had misunderstood how to do it.

> For instance, we can no longer return the response to the caller then inspect status_code.

What I probably should have said there is that you can't return the response and inspect the text. I now see that I had misunderstood how to use `stream=True`. My understanding now is that `response.text` gets consumed while we stream, so we have to rebuild it separately, but the other attributes of the response are still available. So instead of returning the response, we can return a tuple with the response and the rebuilt text. Is that right?

Your idea of posting a recipe is a good one, as the few lines currently in the docs are not enough for a hobbyist like me.

The following seems to work. It follows a [recipe by Martijn Pieters](http://stackoverflow.com/q/23514256/), except that in Python 3 it only worked for me once I set `content` to a byte string.

Is this how you suggest it should be done?

To work with unicode inside instead of bytes, do I just `content.decode(r.encoding)` at the end? By the way for that page, it doesn't work. I thought `r.encoding` was the same as `chardet`, but I get something different and end up having to `content.decode(chardet.detect(content)['encoding'])` Maybe that detection discrepancy is specific to streams?



Thank you again for taking the time to explain the interface design.
",boolbag,Lukasa
2681,2015-07-21 13:12:27,"@gsakkis feel free to try out https://github.com/kennethreitz/requests/pull/2678 to make sure it fixes the issue for you
",sigmavirus24,gsakkis
2680,2015-07-19 09:56:59,"Yeah, this is _mostly_ true. I'm going to merge this as-is @peplin, but I'd really love it if you'd follow up with another pull request that adds a paragraph about _why_ you might want to add the `[]` (namely, because many servers expect that notation).

Thanks so much for the contribution! :cake: :sparkles: :cake:
",Lukasa,peplin
2678,2015-07-22 19:30:12,"@kennethreitz #2567 is already running on Debian (and maybe ubuntu?) against Requests 2.7.0 and has yet to produce any bug reports there. It's been running there for _at least_ a month. Also iirc, Cramer signed off on it and pip has been using something similar to #2567 for a while now.
",sigmavirus24,kennethreitz
2678,2015-09-09 04:12:35,"@sigmavirus24 IIRC Ubuntu either pulls from Debian testing or unstable during a new OS release cycle, so if #2567 was put in to one of those branches it will be in Ubuntu by now. Also you would be able to check the patches that are in the source for packaging as well. just my 2 cents
",L1ghtn1ng,sigmavirus24
2676,2015-07-17 11:31:10,"Thanks for the review @Lukasa and @shazow :) I've updated the PR now. 
https://github.com/kennethreitz/requests/blob/master/test_requests.py#L1275 fails for me on Python 3 because the keys are not in order, but looking at the implementation the keys are not expected to be in order? 
",sunu,shazow
2676,2015-07-17 11:31:10,"Thanks for the review @Lukasa and @shazow :) I've updated the PR now. 
https://github.com/kennethreitz/requests/blob/master/test_requests.py#L1275 fails for me on Python 3 because the keys are not in order, but looking at the implementation the keys are not expected to be in order? 
",sunu,Lukasa
2676,2015-07-17 12:13:37,"@Lukasa I've updated the PR.
",sunu,Lukasa
2675,2015-07-17 09:44:35,"@Lukasa Great! I'll submit a PR. Thanks for the quick response :)
",sunu,Lukasa
2675,2015-07-18 15:24:21,"Hmm .. I guess this makes sense. I'll handle this in the application level then. 
Closing the issue. Thanks for your time and input @Lukasa and @sigmavirus24 :)
",sunu,Lukasa
2675,2015-07-18 15:24:21,"Hmm .. I guess this makes sense. I'll handle this in the application level then. 
Closing the issue. Thanks for your time and input @Lukasa and @sigmavirus24 :)
",sunu,sigmavirus24
2672,2015-07-16 06:19:17,"Thanks @petedmarsh, I missed this when I changed the example from content-type to user-agent!
",Lukasa,petedmarsh
2666,2015-07-14 06:11:56,"@Lukasa I believe I've addressed your concern for that :)
",gvangool,Lukasa
2662,2015-07-08 07:23:32,"Thanks for this @mudongliang!

Unfortunately, neither of those is actually incorrect, they're just informal English.

> (or branch off of it)

To ""branch off"" of something is to create a new branch from it. Thus, to ""branch off of it [master]"" is to create a new branch from the master branch.

> Send a pull request and bug the maintainer until it gets merged or published.

In this case, ""bug"" does not mean the noun ""bug"" (a defect or imperfection), it means the verb ""to bug"" (to bother, to annoy, to pester). The sentence therefore means ""Send a pull request and annoy the maintainer until it gets merged or published"".

Sorry we can't merge this, but thanks for opening it! :cake:
",Lukasa,mudongliang
2660,2015-07-09 17:37:28,"Fixed. =) Thanks @gbishop!
",Lukasa,gbishop
2656,2015-06-29 03:24:18,"@sigmavirus24 thanks for the feedback.  Added a new commit to change the logic as suggested.
",dpursehouse,sigmavirus24
2656,2015-06-29 04:02:25,"Cool. LGTM. I'll let @Lukasa weigh in.

Thanks @dpursehouse 
",sigmavirus24,dpursehouse
2655,2015-09-01 08:32:21,"Ok, I've fixed the broken tests. I agree with you though @sigmavirus24, I think this is scary enough to want to go into 3.0.0, so I'm closing this PR and opening a new one with the correct target.
",Lukasa,sigmavirus24
2653,2015-06-27 19:08:29,"Hello, thanks for your answer.

@Lukasa 
This is both from response.text and from response.content:



@sigmavirus24 



Maybe it is also a problem that the Apache server sends the header in ISO-8859-1? But this would likely be a problem of the shared hoster setup, I guess?



I am sorry that I have obfuscated my original domain. Can I somehow privately send it to you? I would not really like to have it here on Github for all eternity, but of course I have no problem sending it directly to you so that you can check it out.
",TheHellstorm,Lukasa
2653,2015-06-27 19:08:29,"Hello, thanks for your answer.

@Lukasa 
This is both from response.text and from response.content:



@sigmavirus24 



Maybe it is also a problem that the Apache server sends the header in ISO-8859-1? But this would likely be a problem of the shared hoster setup, I guess?



I am sorry that I have obfuscated my original domain. Can I somehow privately send it to you? I would not really like to have it here on Github for all eternity, but of course I have no problem sending it directly to you so that you can check it out.
",TheHellstorm,sigmavirus24
2653,2015-06-28 15:59:01,"Ok, I believe #2655 contains a fix for this issue @TheHellstorm. Feel free to check. =)
",Lukasa,TheHellstorm
2652,2015-06-25 11:38:45,"@nmgeek You may want to take a look at http://docs.python-requests.org/en/latest/user/advanced/#ssl-cert-verification where it is documented: 'You can pass verify the path to a CA_BUNDLE file with certificates of trusted CAs.'

This really could use an example though.
",t-8ch,nmgeek
2652,2015-06-25 12:17:16,"@t-8ch is right, this _is_ documented, just not very well. As to your point of `verify` vs `cert`, that's covered in the same section of the prose documentation that @t-8ch linked to.
",Lukasa,t-8ch
2651,2015-06-25 16:09:04,"Give @piotr-dobrogost's input, @agilevic have you tried using the empty string as the value there? Does it work with your API server?
",Lukasa,piotr-dobrogost
2651,2015-06-25 16:09:04,"Give @piotr-dobrogost's input, @agilevic have you tried using the empty string as the value there? Does it work with your API server?
",Lukasa,agilevic
2651,2015-07-28 15:49:09,"@agilevic What would your proposed API design for this feature be?
",Lukasa,agilevic
2651,2016-09-17 05:46:50,"@frnhr So the reason that doesn't work with our API is that setting a key to `None` is the signal for ""please remove this key from the map"". We have that signal because some parameters can be persisted on a Session object itself, and users occasionally want to be able to suppress those parameters on a per-request basis.
",Lukasa,frnhr
2651,2016-09-17 08:31:33,"I'm afraid I don't see what the session object has to do with this bit of API, sincerely. But ok, maybe`False` then, or even some `SpecialImportableObject` instead of `None`?

> On 17 Sep 2016, at 07:47, Cory Benfield notifications@github.com wrote:
> 
> @frnhr So the reason that doesn't work with our API is that setting a key to None is the signal for ""please remove this key from the map"". We have that signal because some parameters can be persisted on a Session object itself, and users occasionally want to be able to suppress those parameters on a per-request basis.
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub, or mute the thread.
",frnhr,frnhr
2651,2016-09-17 08:46:00,"@frnhr The `Session` API is relevant because the `requests.` API is built on top of the `Session` API: it's a subset of that functionality, a convenience wrapper.

So we certainly could do it, but I'm not sure to what extent it's worthwhile. When not using key-value mapping, you should just pass a string to the `params` field: `params=""foo""`.
",Lukasa,frnhr
2649,2015-06-23 10:13:42,"@Lukasa I would like to provide traceback but how can I do that? I'm getting lots of exceptions because it's uses 200 threads. Generally like these but not in particular order.. And after it stucks and does nothing.

---

('Connection aborted.', gaierror(-2, 'Name or service not known'))
ERROR OCCURED:
karawanghosting.net
('Connection aborted.', gaierror(-2, 'Name or service not known'))
ERROR OCCURED:
bjjzk.net
HTTPConnectionPool(host='bjjzk.net', port=80): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<requests.packages.urllib3.connection.HTTPConnection object at 0x7f97dc0c1ad0>, 'Connection to bjjzk.net timed out. (connect timeout=3)'))

---
",sezginriggs,Lukasa
2649,2015-10-01 20:10:49,"@eltermann Can you please add timeouts to your requests call and see if the problem persists?
",Lukasa,eltermann
2649,2015-10-01 20:31:05,"@Lukasa, this snippet doesn't have timeout, but my call does. Edited the snippet anyway.
",eltermann,Lukasa
2649,2015-10-01 20:32:26,"@eltermann Interesting. It would be really insightful to try to get stacks from those threads.
",Lukasa,eltermann
2649,2015-10-01 20:39:33,"@Lukasa, what do you recommend to print an useful stack? And where to place it?
",eltermann,Lukasa
2649,2016-08-26 07:41:21,"@sezginriggs so have you resolve the problem ? I am stuck with it also.
",metrue,sezginriggs
2649,2016-08-26 13:01:39,"@metrue, I changed my approach to use processes instead of threads -- also, because I found that [python does not really parallelize threads execution because of GIL](https://www.google.com.br/search?q=python+does+not+really+parallelize+threads+execution+because+of+GIL).

You have two choices:
1. you deal with multiprocesses yourself in your python code -- I recommend looking at how [scrapy](https://github.com/scrapy/scrapy) does the parallelization (even though it uses twisted under the hood and not requests)
2. (and that's what I did) you write a simple ""stream-consumer"" python program and let something else do the parallelization (something like Kafka or Storm) -- then, you start multiple processes for your ""stream-consumer"" and voilá
",eltermann,metrue
2649,2016-08-27 12:38:15,"@eltermann , I do know 'python does not really parallelize threads execution because of GIL'. So I am using ProcessPoolExecutor instead of ThreadPoolExecutor, But Still, requests also stucks.
",metrue,eltermann
2649,2016-08-27 15:39:48,"@Lukasa 

Right, I realized that, I am not a Python expert,  but I wonder what's the best practice of  sharing data (let's say a global task queue) between those process ?
",metrue,Lukasa
2649,2016-08-27 16:29:07,"@metrue to maintain a thread-safe/multiprocess-safe queue, you can use the standard library's `Queue` implementation. If you're on Python 2



if you're on Python 3


",sigmavirus24,metrue
2649,2016-12-01 14:50:31,@antongulikov We have not. We are still missing a bug chunk of debugging data as discussed earlier in the thread.,Lukasa,antongulikov
2647,2015-06-22 07:20:29,":joy: Thanks  @Lukasa 
",neosab,Lukasa
2646,2015-06-22 06:12:32,"Thanks @sigmavirus24  
",neosab,sigmavirus24
2645,2015-06-22 18:45:10,"@sigmavirus24 that sounds great to me. 

`if response.status_code in requests.codes.success`
",kennethreitz,sigmavirus24
2645,2016-05-16 04:02:01,"The solution proposed is to add a success attribute listing all the status codes that are considered ok (200, 201, 202, ..., 226). Then, we compare the status code of the response with the ones listed in success.

What happens is that the response for a 301 redirect, for example, is a status code 200. The same happens for other status codes of the 3xx family. @sigmavirus24 came with the idea that one of the reasons for that change is to narrow the definition of Response.ok. If that's the case, wouldn't it be better to rethink the outcome of those 3xx cases?
",AlexPHorta,sigmavirus24
2645,2016-05-16 07:10:21,"I think @sigmavirus24's point is just that 3XX status codes should probably not be considered ""ok"": they aren't. But requests _always_ follows redirects, so the only way to _end up_ at a 3XX status code is if the 3XX code is not an unambiguous redirect: i.e., it does not have a `Location` header.

The reason your testing showed no following for 304 and friends is because 304 is not a defined HTTP status code, so httpbin was not generating a Location header for it. That means requests ends up with the 304 code and cannot follow it any further. @sigmavirus24 rightly proposes that that response should not be ""ok"": it is almost certainly not.
",Lukasa,sigmavirus24
2645,2016-05-16 13:58:46,"I see, if a 301 redirect ends up on a successful retrieval of some
resource, then the final status is a 200 code.

Alright, my attempt is probably right. Tonight I'll make the PR, hope
everything is ok!

Thanks!
Am 16.05.2016 04:10 schrieb ""Cory Benfield"" notifications@github.com:

> I think @sigmavirus24 https://github.com/sigmavirus24's point is just
> that 3XX status codes should probably not be considered ""ok"": they aren't.
> But requests _always_ follows redirects, so the only way to _end up_ at a
> 3XX status code is if the 3XX code is not an unambiguous redirect: i.e., it
> does not have a Location header.
> 
> The reason your testing showed no following for 304 and friends is because
> 304 is not a defined HTTP status code, so httpbin was not generating a
> Location header for it. That means requests ends up with the 304 code and
> cannot follow it any further. @sigmavirus24
> https://github.com/sigmavirus24 right proposes that that response
> should not be ""ok"": it is almost certainly not.
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/2645#issuecomment-219363390
",AlexPHorta,sigmavirus24
2645,2016-06-15 08:12:30,"@Lukasa when designed, 'ok' meant 'not an error'. 300 is not an error. I still think should be the case. 

e.g. '.ok' isnt OMG ALL IS PERFECT, it is NOTHING APPEARS IMMEDIATELY BROKEN
",kennethreitz,Lukasa
2642,2015-06-15 15:41:28,"`BadStatusLine` is extremely unclear. It can occur in a number of situations, and makes no guarantees about whether the request made it through. It just fundamentally means httplib failed to parse the header line.

In _this_ instance it's unlikely that the server ""sent back the empty string"" because that's not a thing you can meaningfully do in TCP (I guess you could try emitting a zero length packet? not sure if that's valid TCP). What this means is that the connection got closed, likely by the shutdown of the server, without any data left in the send buffer. That will have caused a zero-length read.

A good example of why we _shouldn't_ blindly retry in this case is that plenty of servers will reject requests they don't like by just closing the connection, which will cause exactly this error. This kind of thing tells you nothing about whether the request succeeded. This means @kevinburke's advice is right (only retry idempotent requests), but not quite for the right reason.

</pedantry>
",Lukasa,kevinburke
2642,2015-06-18 12:27:49,"@Lukasa  thank you very much! I was not aware of the ""zero means any port"" method.

For the unix-sockets way I found this:

https://pypi.python.org/pypi/requests-unixsocket/
",guettli,Lukasa
2641,2015-06-15 06:39:01,"Thanks for this @duanhongyi!

However, I'm :-1: on this idea. My primary objection is that this adds implicit global state. Generally speaking I don't think libraries should maintain any internal state at all except what is truly necessary for their function. Wherever possible we should provide 'state objects' to users that the user has control over, ensuring that users own the lifetimes of objects: we already do this.

I think having a pool of sessions in the background is a bad idea. It'll lead to bug reports from users who aren't expecting it and it'll lead to complaints from users who don't like the memory profile it causes. Most importantly, it makes it _very very difficult_ to obtain reproducible behaviour out of the top-level API, because what exactly happens on a given request is actually dependent on the entire lifetime of the program up until that point. `requests.get()` may or may not work depending on whether there are cookies present in the particular `Session` you're using. It may or may not work depending on whether the remote server supports keepalive connections (a surprising number don't handle it well).

I'll let @sigmavirus24 express an opinion as well, but I'm sorry, I doubt we'll merge this.
",Lukasa,duanhongyi
2641,2015-06-15 09:21:23,"@Lukasa 

I agree with you, but the global shared socket connection, which is common in programming languages such as Java, is also practical. and perhaps adapters.py is more appropriate for doing this.
",duanhongyi,Lukasa
2641,2015-06-15 15:41:06,"I agree with @Lukasa. This will also likely negatively affect things like grequests which build on the api module's design. I also don't think we want this in `requests/adapters.py`.
",sigmavirus24,Lukasa
2636,2015-06-13 09:49:13,"@Lukasa I did always like future-you better than past-you.
",shazow,Lukasa
2634,2015-06-10 14:13:34,"I have to agree with @Lukasa. This is not on the roadmap for future versions of requests at this point.
",sigmavirus24,Lukasa
2633,2015-06-09 13:05:24,"@kaizengliu Please do not use the _bug_ tracker to ask questions. Our question forum is located [here](https://stackoverflow.com/questions/tagged/python-requests).
",sigmavirus24,kaizengliu
2631,2015-06-21 14:23:00,"Thanks @neosab 
",sigmavirus24,neosab
2630,2015-06-05 04:48:55,"Hey @koobs, thanks for the PR. Can I ask why you're adding this?
",sigmavirus24,koobs
2630,2015-06-05 05:35:02,"@sigmavirus24

TLDR: Best practice python packaging.

Longer version:

The 'test' command is the canonical method for running tests in Python land (ala make check for autotools projects) independent to how or what tests are run, or the tools that get used.

It also makes for a great downstream (users and os packaging) experience so we/they can QA, orthogonally to how an upstream might do 'development' testing (read: tox, travis, whatever)

For me (and FreeBSD), I have the following in the port for requests, so that I can QA updates and commit changes quicker with confidence.


",koobs,sigmavirus24
2630,2015-06-05 13:24:34,"@Lukasa that command class is definitely what I had in mind. Without using py.test to _run_ the tests, only a fraction of them will be run since we have test functions that aren't attached to any test class that `unittest` could discover.

Also, regarding unverified TLS, I hope that setuptools has improved it but I'd be skeptical. It'd have to load certificates from somewhere and I doubt that it is also willing to vendor them. (The problem may be mostly alleviated on 2.7.9+ or 3.x, but that still leaves tons of people on systems without those versions in a lurch.) That said, I like being friendly to our downstream redistributors, so I'm happy to accept work to make `python setup.py test` work, but I think we need to actively discourage contributors from using it given the security concerns.
",sigmavirus24,Lukasa
2630,2016-01-30 05:34:25,"@kennethreitz Where is the full test suite and how is it run?
",koobs,kennethreitz
2630,2016-01-31 03:34:41,"@koobs it is all contained in `test_requests.py`, but it does not all use the unittest framework, instead relying on py.test to automatically pick up some functions and run them as tests. 

This should be fixed, in my opinion. I see no reason to be lazy just because we're using py.test.
",kennethreitz,koobs
2625,2015-06-15 03:28:33,"@sigmavirus24 Pardon my poor English, but how could  `elapsed` be widely used and you weren't keen to expose it?

My rationale is like this, since `elapsed` is widely used, and it involves a system call to get start request time, so it'd better to directly expose the start request time as a property directly, so other developers can make more use of this measure without wrapping yet another get current time system calls. As you can see people use `requests` often involve with time based constraints and there would be a ton duplicated code.
",est,sigmavirus24
2624,2015-06-02 02:14:47,"@kvarga sorry, are you using the requests library? You seem to not be using it at all unless I'm misunderstanding your bug.
",sigmavirus24,kvarga
2622,2015-06-01 21:38:44,"@t-8ch has diagnosed this correctly. Sadly there isn't much we can do about this on our side beyond what has been described here.
",Lukasa,t-8ch
2622,2015-06-01 22:07:40,"@mgdelmonte this isn't our fault. This is httplib's fault. Raise the bug there and eventually if it's ever fixed it'll trickle down to here.
",sigmavirus24,mgdelmonte
2622,2015-06-01 22:12:39,"@mgdelmonte That's extremely disingenuous, and I think you know that.

Chrome, Firefox, and Internet Explorer are browsers whose development is backed by the paid, full-time efforts of a team of developers. They own the entirety of their software stack, do not have backwards compatibility concerns with other libraries, and are beholden only to themselves for release schedules.

Requests is a tool whose development is funded by zero people. Three core developers devote more than 40 cumulative hours a week to this project, _for free_, with ancillary and extremely important contributions from people like @t-8ch which total probably another 10 hours, also _for free_. We do not control our entire software stack: many of the tools we build on top are are in the python standard library, like `httplib`, `cookielib`, `ssl`, and more. The fact that this project exists _at all_ is entirely down to the goodwill of a series of developers whose names you will never know and who received exactly no compensation for their work other than, if they are extremely lucky, the thanks of someone else.

As @sigmavirus24 points out, the bug is in httplib, which we rely on to do our HTTP parsing. Even then, I wouldn't call it a bug: there is a very clear specification for HTTP headers which is being violated by the server in question. At some point we need to stop tolerating the mistakes of others and say that something is simply not HTTP. This is particularly true of [Merrick Bank](http://www.merrickbank.com), a presumably financial institution that actually _did_ pay someone to build their website.

If you would like to get this fixed, please file a bug against httplib. More than that, please submit a patch. The last time we had one of these bugs was in #1804. For that bug I actually did provide a fix for `httplib`, which was merged more than one year after it was proposed. For that reason, I do not have particularly high hopes for this being fixed any time soon.

@mgdelmonte Please consider the way you act towards purely volunteer run projects. We are doing the best we can, but we need to pick our battles. If you'd like to pay one of us some money to address this bug, I'm sure we can come to some arrangement. Otherwise, we will pursue this in terms of the priorities of the requests project, and in those cases we promise nothing when the bug is upstream of us.
",Lukasa,mgdelmonte
2622,2015-06-01 22:12:39,"@mgdelmonte That's extremely disingenuous, and I think you know that.

Chrome, Firefox, and Internet Explorer are browsers whose development is backed by the paid, full-time efforts of a team of developers. They own the entirety of their software stack, do not have backwards compatibility concerns with other libraries, and are beholden only to themselves for release schedules.

Requests is a tool whose development is funded by zero people. Three core developers devote more than 40 cumulative hours a week to this project, _for free_, with ancillary and extremely important contributions from people like @t-8ch which total probably another 10 hours, also _for free_. We do not control our entire software stack: many of the tools we build on top are are in the python standard library, like `httplib`, `cookielib`, `ssl`, and more. The fact that this project exists _at all_ is entirely down to the goodwill of a series of developers whose names you will never know and who received exactly no compensation for their work other than, if they are extremely lucky, the thanks of someone else.

As @sigmavirus24 points out, the bug is in httplib, which we rely on to do our HTTP parsing. Even then, I wouldn't call it a bug: there is a very clear specification for HTTP headers which is being violated by the server in question. At some point we need to stop tolerating the mistakes of others and say that something is simply not HTTP. This is particularly true of [Merrick Bank](http://www.merrickbank.com), a presumably financial institution that actually _did_ pay someone to build their website.

If you would like to get this fixed, please file a bug against httplib. More than that, please submit a patch. The last time we had one of these bugs was in #1804. For that bug I actually did provide a fix for `httplib`, which was merged more than one year after it was proposed. For that reason, I do not have particularly high hopes for this being fixed any time soon.

@mgdelmonte Please consider the way you act towards purely volunteer run projects. We are doing the best we can, but we need to pick our battles. If you'd like to pay one of us some money to address this bug, I'm sure we can come to some arrangement. Otherwise, we will pursue this in terms of the priorities of the requests project, and in those cases we promise nothing when the bug is upstream of us.
",Lukasa,t-8ch
2622,2015-06-01 22:12:39,"@mgdelmonte That's extremely disingenuous, and I think you know that.

Chrome, Firefox, and Internet Explorer are browsers whose development is backed by the paid, full-time efforts of a team of developers. They own the entirety of their software stack, do not have backwards compatibility concerns with other libraries, and are beholden only to themselves for release schedules.

Requests is a tool whose development is funded by zero people. Three core developers devote more than 40 cumulative hours a week to this project, _for free_, with ancillary and extremely important contributions from people like @t-8ch which total probably another 10 hours, also _for free_. We do not control our entire software stack: many of the tools we build on top are are in the python standard library, like `httplib`, `cookielib`, `ssl`, and more. The fact that this project exists _at all_ is entirely down to the goodwill of a series of developers whose names you will never know and who received exactly no compensation for their work other than, if they are extremely lucky, the thanks of someone else.

As @sigmavirus24 points out, the bug is in httplib, which we rely on to do our HTTP parsing. Even then, I wouldn't call it a bug: there is a very clear specification for HTTP headers which is being violated by the server in question. At some point we need to stop tolerating the mistakes of others and say that something is simply not HTTP. This is particularly true of [Merrick Bank](http://www.merrickbank.com), a presumably financial institution that actually _did_ pay someone to build their website.

If you would like to get this fixed, please file a bug against httplib. More than that, please submit a patch. The last time we had one of these bugs was in #1804. For that bug I actually did provide a fix for `httplib`, which was merged more than one year after it was proposed. For that reason, I do not have particularly high hopes for this being fixed any time soon.

@mgdelmonte Please consider the way you act towards purely volunteer run projects. We are doing the best we can, but we need to pick our battles. If you'd like to pay one of us some money to address this bug, I'm sure we can come to some arrangement. Otherwise, we will pursue this in terms of the priorities of the requests project, and in those cases we promise nothing when the bug is upstream of us.
",Lukasa,sigmavirus24
2622,2015-06-01 22:39:02,"@Lukasa my sincere apologies, and I do appreciate the value of the requests lib.  My point was more to mock Internet Explorer (which frankly surprises me if it handles anything less than perfectly clean data) than to deride your efforts.

I'll file a bug report with httplib, which appears to be the source.  It's hard to foresee someone breaking spec like Merrick has, but nevertheless it seems like an easy accommodation to make with no penalties elsewhere, to ignore a broken HTTP header line.
",mgdelmonte,Lukasa
2622,2015-06-01 22:44:31,"@mgdelmonte Sincerely, thankyou for the apology. =) Miscommunications on the Internet are easy, and I'm sure you meant no ill-will. Your apology was extremely graceful.

When you file your httplib bug, please link it here. I'll make sure to keep an eye on it, and if I find time tomorrow I may even try to flesh out a patch. We'll see if this one merges quicker. ;)
",Lukasa,mgdelmonte
2622,2015-06-02 15:12:50,"@Lukasa no problem, we're all pulling in the same direction (usually).  I submitted an issue for httplib here

http://bugs.python.org/issue24363

This is my first time submitting an issue through that system so I was a little mystified and wasn't sure how to submit a patch, so just recommended my fix.  I'm amazed there aren't more problems with readheaders() as it seems to completely ignore the requirement of a blank line to terminate the header.  Yes, you shouldn't have malformed header lines, either; but certainly it's easier to skip them and accommodate the mistakes of others than to terminate parsing altogether, especially when the specification has a clear and (mostly) unambiguous terminator.

Anyway, hopefully this clears it up for good.  Famous last words, right?
",mgdelmonte,Lukasa
2621,2015-06-02 08:11:47,"Oooof. Thanks @tiran. For the moment I'll leave the stdlib in your hands, I think we should update the algorithm we've back ported to add this support. I'll take a look at this today and see if I can extend it.
",Lukasa,tiran
2621,2015-06-02 12:59:37,"@tiran the use case is only for private CAs. Can't imagine a public CA would sign an IP, yes.
",itamarst,tiran
2621,2015-06-02 13:43:33,"@Lukasa do you think cURL handles this? Should we give Daniel a ping?
",sigmavirus24,Lukasa
2621,2015-06-03 08:10:59,"@bagder thanks so much. :cake:

Ok, with Daniel's point 1 in mind, I still think we should aim to do this _if possible_. I don't believe we need to do it on both of our SSL backends (stdlib and PyOpenSSL), but if we can make it possible in our PyOpenSSL backend I think we should. That will allow users who desperately need this function to achieve it, but we won't bend over backwards to make this available to all our users.
",Lukasa,bagder
2620,2015-06-02 18:56:53,"Ah thanks @jongiddy. It's been long enough (over a year, wow) that I completely forgot that was open. Would you mind if we closed this since it's a duplicate?
",sigmavirus24,jongiddy
2619,2015-06-02 08:45:47,"@RubenvWyk Thanks for the follow-up! :cake:
",Lukasa,RubenvWyk
2618,2015-06-01 06:19:36,"@gengjiawen This URL redirects to a 4GB big ISO image. So the download takes a little bit.
You probably don't want to read this into memory anyways.
Try `requests.get(url, stream=True)` and then iterate over the content.
",t-8ch,gengjiawen
2618,2015-06-02 01:08:05,"@t-8ch Thanks, your solution also works.
",gengjiawen,t-8ch
2617,2015-05-30 14:38:19,"@Lukasa thoughts on the last commit that I pushed?
",sigmavirus24,Lukasa
2617,2015-06-02 19:21:07,"@Lukasa updated with that approach.
",sigmavirus24,Lukasa
2616,2015-06-08 15:04:19,"@mieciu thanks for the input. That said, my recommendation is to use homebrew. :wink: 
",sigmavirus24,mieciu
2616,2016-09-22 08:05:26,"@biplab-dholey This error can occur for a number of reasons. The retries count is unrelated. This error results from your TLS configuration, and without more information it's very difficult for us to help.
",Lukasa,biplab-dholey
2615,2015-05-26 14:12:46,"Thanks @awiddersheim! :sparkles: :cake: :sparkles: 
",sigmavirus24,awiddersheim
2614,2015-05-26 19:56:18,"@colindickson Thanks for this! Rather than merge two identically named commits and a merge commit I rebased them down into fd31453aa25f80f04e1ce36de9abff0460bd136a, and merged it in 8b5e457b756b2ab4c02473f7a42c2e0201ecc7e9.

Thanks again! :cake: :sparkles:
",Lukasa,colindickson
2613,2015-05-26 13:10:02,"@bboe it's in your best interest to copy and paste `to_native_string` out of requests though. It's an undocumented function that's effectively meant to be internal to requests. If we move it around or change something in it, it could cause compatibility problems for you and there's no guarantee of backwards compatibility for that function as it isn't a defined member of the API.

That said, @Lukasa and I agree that it's highly unlikely to break, change, or disappear. So, while I'd prefer you to copy and paste it out, there's nothing I can do to enforce that. ;)
",sigmavirus24,bboe
2613,2015-05-26 13:10:02,"@bboe it's in your best interest to copy and paste `to_native_string` out of requests though. It's an undocumented function that's effectively meant to be internal to requests. If we move it around or change something in it, it could cause compatibility problems for you and there's no guarantee of backwards compatibility for that function as it isn't a defined member of the API.

That said, @Lukasa and I agree that it's highly unlikely to break, change, or disappear. So, while I'd prefer you to copy and paste it out, there's nothing I can do to enforce that. ;)
",sigmavirus24,Lukasa
2612,2015-05-25 14:56:45,"Hey @Lukasa !

I use this command. (http://justniffer.sourceforge.net/)
sudo justniffer -i eth0 -r > log_socket.txt

Here is the log related with request:
http://paste2.org/mhZCj2t7

and here is the log related with socket.

http://paste2.org/8ztZbD7K

what I see is, socket make two request. and requests just make one.

the important value for me is:
Set-Cookie: LOGIN_USERNAME_COOKIE=<ID_USERNAME>
Set-Cookie: WWV_CUSTOM-F_1279122985171442_102=8DA9C805E89B5CAE; path=/

because with that value I create another requests to get the content in the system.

I hope this help!

And thanks Lukasa!
",Wu4m4n,Lukasa
2609,2015-05-22 16:33:31,"@Lukasa  It is very difficult to using `objgraph` in my code.
Because the memory leak is slow and `objgraph` can't track every `requests` instance,
 it will create a lot of pictures, Which picture Useful?

Thank You for your help.
If I find the code of memory leaks, I will contact you again.
",030io,Lukasa
2609,2015-05-25 11:46:16,"@sigmavirus24 
I use `http.client` instead of `requests`. And the memory leak is gone.
",030io,sigmavirus24
2608,2015-05-21 08:16:19,"\o/ Thanks @radarhere! :cake: :sparkles:
",Lukasa,radarhere
2604,2015-05-17 20:48:29,"@Lukasa it's probably best to use print() for instructions like that in case the end user is on a distro which has changed the sym link for python to point to python 3 instead of 2.

As a side note, I generally recompile python after installing a new version of OpenSSL, so I was a little surprised to see my workstation here reporting the version as 1.0.2 instead of 1.0.2a and it turns out that the cryptography module requires a force reinstall to detect that change in addition to whatever is done to the python installation (which normally includes byte compiling installed modules), it would be quite easy for anyone to overlook that even after upgrading OpenSSL.
",Hasimir,Lukasa
2604,2015-05-17 20:58:14,"> it's probably best to use print() for instructions like that in case the end user is on a distro which has changed the sym link for python to point to python 3 instead of 2.

That's not really relevant. =) The number of users using Python 3 instead of Python 2 is very low, and I trust that most users capable of reporting bugs on this bug tracker are also capable of translating this extremely common line of code into the correct syntax for their platform. Given that `print()` won't work on Python 2 without an appropriate **future** import, I'm choosing to optimise for the more likely case.

Regardless, I think we can close this. @sergedroz, your OpenSSL is too old. If you're using pyOpenSSL, please reinstall cryptography linked against a more modern OpenSSL. If you're not, please upgrade to a newer version of OpenSSL and then rebuild Python.
",Lukasa,sergedroz
2602,2015-05-15 09:42:55,"LGTM, but I'm going to wait for @sigmavirus24 on this one.
",Lukasa,sigmavirus24
2602,2015-05-15 15:14:56,"Hi @ly0 

I pulled down your PR and squashed those three commits with the same exact message. The fix is on master as of https://github.com/kennethreitz/requests/commit/ab1f493c8b6f82cbf80f8554b5fbbd02f2b2a363

Thanks!

:sparkles: :cake: :sparkles: 
",sigmavirus24,ly0
2598,2015-05-14 19:59:00,"ping @sigmavirus24 
",Lukasa,sigmavirus24
2595,2015-05-11 21:26:40,"@Lukasa The problem happens in requests 2.7.0 as well -- all released versions after 2.6.0. The problem is in urllib3 versions before https://github.com/shazow/urllib3/commit/22a9713fab2ed831204711906a974c3beba3319e, so would be fixed by merging in a more recent urllib3.
",agfor,Lukasa
2595,2015-05-11 22:21:40,"@agfor https://github.com/shazow/urllib3/commit/22a9713fab2ed831204711906a974c3beba3319e should be in 2.7.0.
",sigmavirus24,agfor
2595,2015-05-11 23:02:02,"@sigmavirus24 @Lukasa  It looks like there are actually two different problems, and that commit only fixed one. With requests 2.6.2 / urllib3 before https://github.com/shazow/urllib3/commit/22a9713fab2ed831204711906a974c3beba3319e it blows up inside requests:



but after that commit / in 2.7.0, we blow up later inside the Braintree library. This appears to be because requests has tried to convert gzipped data to Unicode as if it had already been un-gzipped, and so replaced most of it with the unicode replacement character:



example unicode code points for the response body:



Versions 2.6.0 and below work.
",agfor,Lukasa
2595,2015-05-11 23:02:02,"@sigmavirus24 @Lukasa  It looks like there are actually two different problems, and that commit only fixed one. With requests 2.6.2 / urllib3 before https://github.com/shazow/urllib3/commit/22a9713fab2ed831204711906a974c3beba3319e it blows up inside requests:



but after that commit / in 2.7.0, we blow up later inside the Braintree library. This appears to be because requests has tried to convert gzipped data to Unicode as if it had already been un-gzipped, and so replaced most of it with the unicode replacement character:



example unicode code points for the response body:



Versions 2.6.0 and below work.
",agfor,sigmavirus24
2595,2015-05-12 15:20:53,"@sigmavirus24 @Lukasa I haven't been able to reproduce off of Google App Engine, so here is a minimal GAE repo you can test with the development environment that exhibits the problem: https://github.com/agfor/requests-2.7-appengine-fail

The core of it is:



It will dump the still-gzipped response body, and the un-gzipped response body, to the logs, then blow up on the `response.text.encode('ascii')` line, since `text` is producing gibberish full of unicode replacement characters from trying to encode gzipped data as unicode.
",agfor,Lukasa
2595,2015-05-12 15:20:53,"@sigmavirus24 @Lukasa I haven't been able to reproduce off of Google App Engine, so here is a minimal GAE repo you can test with the development environment that exhibits the problem: https://github.com/agfor/requests-2.7-appengine-fail

The core of it is:



It will dump the still-gzipped response body, and the un-gzipped response body, to the logs, then blow up on the `response.text.encode('ascii')` line, since `text` is producing gibberish full of unicode replacement characters from trying to encode gzipped data as unicode.
",agfor,sigmavirus24
2595,2015-05-12 17:06:39,"@shazow Just to close the loop, I was able to bisect the issue back to https://github.com/shazow/urllib3/commit/f21c2a2b73e4256ba2787f8470dbee6872987d2d specifically.
",agfor,shazow
2595,2015-05-12 17:25:25,"@agfor Ah good find, thanks.

Freakin' chunked encoding, breaking errything.

Does this mean GAE still works for non-chunked/non-streaming?

@sigmavirus24 Lovely idea, any interest in reaching out to the GAE team and asking if they'll fund this? :P
",shazow,agfor
2595,2015-05-12 17:25:25,"@agfor Ah good find, thanks.

Freakin' chunked encoding, breaking errything.

Does this mean GAE still works for non-chunked/non-streaming?

@sigmavirus24 Lovely idea, any interest in reaching out to the GAE team and asking if they'll fund this? :P
",shazow,sigmavirus24
2594,2015-05-13 06:56:30,"Thanks for your response.
This may be a solution.  Close this issue. I will come back to you if I got some useful error message with your method.

PS: It looks like our proxy problem. The code ran smoothly last night. Thanks, @Lukasa 
",fivejjs,Lukasa
2592,2015-05-05 08:33:31,"Thanks for the feature request @cornelinux!

This seems like the kind of thing that could easily be built on top of requests. =) If a `Session` object is used, it shouldn't be hard to write a little wrapper around it that does exactly that.

However, I don't think we'll be adding this feature to requests core, as we're in an indefinitely feature freeze. Sorry!
",Lukasa,cornelinux
2590,2016-02-02 08:16:32,"@borbamartin I believe so, yes. I think really _only_ the logic we use with POST is wrong here.
",Lukasa,borbamartin
2590,2016-02-02 12:44:54,"Thanks @Lukasa !
",borbamartin,Lukasa
2586,2015-05-03 14:55:58,"@sigmavirus24 Any objections to me merging this PR (manually without a merge commit, because we don't need it) and pushing the release?
",Lukasa,sigmavirus24
2585,2015-05-02 11:33:30,"@sigmavirus24 I'm beginning to wonder if we should have a 3.0.0 feature branch, and should start landing these things we keep deferring to 3.0.0.
",Lukasa,sigmavirus24
2585,2015-05-02 14:49:12,"@Lukasa I've been thinking of starting just such a branch in my fork. I wasn't sure if we would want to keep it here or elsewhere. While having people test it would be nice, having bug reports here about it might become confusing to some.
",sigmavirus24,Lukasa
2583,2015-05-01 14:35:07,"@Lukasa Thanks for the quick reply. I can confirm the error is gone in 2.6.1.
",htgoebel,Lukasa
2582,2015-05-01 05:59:45,"Thanks for this @mhavard999, it looks great! :cake:
",Lukasa,mhavard999
2580,2015-04-29 21:20:00,"@sigmavirus24 Thanks for the quick response with examples. I'll try these and report back...
",Microserf,sigmavirus24
2576,2015-04-29 06:10:51,"@sigmavirus24 Can users not provide a stricter policy by replacing the cookiejar on the `Session` object?
",Lukasa,sigmavirus24
2576,2015-04-29 14:37:22,"I'm pretty sure we can use the script in @henadzit's original report to test browsers and curl both. It's well done.
",sigmavirus24,henadzit
2575,2015-04-28 15:26:27,"@t-8ch This is an odd dupe of shazow/urllib3#601 then, but I suppose it's possible. Definitely seems like the relevant smoking gun for now, so I'm closing to centralise there.
",Lukasa,t-8ch
2575,2015-04-28 15:44:09,"@Lukasa ofc you are right :-)



It gets another redirect with a content-length and then seems to wait for the body



Yields no redirect for me and also hangs.

cc @bagder
",t-8ch,Lukasa
2575,2015-04-28 16:00:34,"@t-8ch I'm sure @bagder will tell you you're not the first person to make this mistake, and you won't be the last. ;)

Yeah, that's the bug.
",Lukasa,t-8ch
2573,2015-04-26 12:04:05,"@Nelluk Can you provide more information on what causes this? Which server are you accessing, etc?

At the moment, the most likely change in that delta is the fact that we slightly changed our default SSL cipher list:



It's possible this is causing a problem, though it's tricky to see how: knowing what server you're contacting would help me diagnose in that regard.
",Lukasa,Nelluk
2571,2015-04-25 10:33:38,"@Lukasa - there is no redirection in my php code. this is what the myapp index.php file looks like 


",bmutinda,Lukasa
2571,2015-04-25 10:49:50,"@Lukasa realized the same thing too. Just seen a note from this link http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html that 



The redirection might be in the server configuration if the fallback is not done in the requests lib..
",bmutinda,Lukasa
2571,2015-04-25 13:19:03,"@Lukasa do you have an alternative fix for this that would force the post request to be treated as a post no matter what?? Its an app that I am building that would require developers to put their urls so that I can ping their url with post params only. 
",bmutinda,Lukasa
2568,2015-04-28 16:14:38,"@rfortress I'm afraid not. We're working on this, there are a number of other problems with the chunked change and we'd like to land them at all once if we possibly can.

I'm genuinely sorry for how long this is taking. =(
",Lukasa,rfortress
2567,2015-04-24 13:12:42,"@untitaker why not just add it while I'm testing it?
",sigmavirus24,untitaker
2567,2015-05-03 23:15:47,"@untitaker thanks for working on this. Just for reference, here is what it's used in Debian at the moment:
https://anonscm.debian.org/viewvc/python-modules/packages/requests/trunk/debian/patches/04_make-requests.packages.urllib3-same-as-urllib3.patch?revision=32576&view=markup

Yes, I'm exporting only urllib3 since importing chardet from requests.packages seems not used, at least no one complained about this.

I choosed to not cherry pick  #2375 due the problems emerged: I don't want Debian and Ubuntu users to have a system version of requests not in the best shape: it will give only more problems.
",eriol,untitaker
2567,2015-05-04 07:31:21,"Seems good @eriol. Would you adopt a solution offered by requests though, to avoid Distro-specific breakage?

BTW @sigmavirus24 this is ready for review.
",untitaker,eriol
2567,2015-05-04 07:31:21,"Seems good @eriol. Would you adopt a solution offered by requests though, to avoid Distro-specific breakage?

BTW @sigmavirus24 this is ready for review.
",untitaker,sigmavirus24
2567,2015-05-04 18:26:05,"@untitaker of course I will like to drop a custom patch! :smile:
I had to write more to explain better my opinion, sorry for this. My opinion is that less divergence from upstream is the better, and it's for this reason that I forward all modification non Distro-specific to upstream.
Furthermore the Debian patch is not perfect, see [Debian Bug #771349](https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=771349).
So to recap I will be extremely happy to drop 04_make-requests.packages.urllib3-same-as-urllib3.patch in favor of an upstream solution.
Thanks!
",eriol,untitaker
2567,2015-05-09 19:55:46,"This works well for me. Objections to merging @Lukasa?
",sigmavirus24,Lukasa
2567,2015-05-10 12:31:44,"@sigmavirus24 I'm super reluctant to rush this out, given the way stuff previously went. Can we try to tap people who've had problems/objections in the past and get them to confirm that this works better for them?
",Lukasa,sigmavirus24
2567,2015-05-10 15:43:41,"@untitaker did you mean @mitsuhiko? I would hope a direct ping would suffice.

---

@Lukasa I would argue this is probably, on the whole, better than our previous meta_path hackery for several reasons:
1. It provides the same functionality
2. It's significantly simpler (simple is better than complex)
3. It allows for all of this to work in the case where requests is vendored without its vendored dependencies (see @untitaker's use of `__name__` when adding the alias to `sys.modules`)
4. It doesn't mess with other meta_path plugins (e.g., PyInstaller, the hack that @dcramer and @mitsuhiko are using, etc.)
5. It falls back in the correct order
6. It's far easier to explain to someone
7. When urllib3 is not vendored, the following works as we'd like it to:
   
   

I have yet to test this with PyInstaller, but the root of the problem there was the fact that our meta_path plugin was in the wrong place relative to the multiple plugins that PyInstaller uses. So with that removed, this should just work. I'm also confident that if @mitsuhiko and @dcramer test this with their code that hacks the meta_path, then they'll not see any problems.
",sigmavirus24,Lukasa
2567,2015-05-10 15:43:41,"@untitaker did you mean @mitsuhiko? I would hope a direct ping would suffice.

---

@Lukasa I would argue this is probably, on the whole, better than our previous meta_path hackery for several reasons:
1. It provides the same functionality
2. It's significantly simpler (simple is better than complex)
3. It allows for all of this to work in the case where requests is vendored without its vendored dependencies (see @untitaker's use of `__name__` when adding the alias to `sys.modules`)
4. It doesn't mess with other meta_path plugins (e.g., PyInstaller, the hack that @dcramer and @mitsuhiko are using, etc.)
5. It falls back in the correct order
6. It's far easier to explain to someone
7. When urllib3 is not vendored, the following works as we'd like it to:
   
   

I have yet to test this with PyInstaller, but the root of the problem there was the fact that our meta_path plugin was in the wrong place relative to the multiple plugins that PyInstaller uses. So with that removed, this should just work. I'm also confident that if @mitsuhiko and @dcramer test this with their code that hacks the meta_path, then they'll not see any problems.
",sigmavirus24,dcramer
2567,2015-05-10 15:43:41,"@untitaker did you mean @mitsuhiko? I would hope a direct ping would suffice.

---

@Lukasa I would argue this is probably, on the whole, better than our previous meta_path hackery for several reasons:
1. It provides the same functionality
2. It's significantly simpler (simple is better than complex)
3. It allows for all of this to work in the case where requests is vendored without its vendored dependencies (see @untitaker's use of `__name__` when adding the alias to `sys.modules`)
4. It doesn't mess with other meta_path plugins (e.g., PyInstaller, the hack that @dcramer and @mitsuhiko are using, etc.)
5. It falls back in the correct order
6. It's far easier to explain to someone
7. When urllib3 is not vendored, the following works as we'd like it to:
   
   

I have yet to test this with PyInstaller, but the root of the problem there was the fact that our meta_path plugin was in the wrong place relative to the multiple plugins that PyInstaller uses. So with that removed, this should just work. I'm also confident that if @mitsuhiko and @dcramer test this with their code that hacks the meta_path, then they'll not see any problems.
",sigmavirus24,untitaker
2567,2015-05-10 16:04:52,"@sigmavirus24

> @untitaker did you mean @mitsuhiko? I would hope a direct ping would suffice.

Unfortunately I don't think so. I haven't heard back from him either, perhaps
@dcramer can ping him about that?

---

@eriol BTW this should also remove the need to rewrite every import statement
inside requests itself.
",untitaker,eriol
2567,2015-05-10 16:04:52,"@sigmavirus24

> @untitaker did you mean @mitsuhiko? I would hope a direct ping would suffice.

Unfortunately I don't think so. I haven't heard back from him either, perhaps
@dcramer can ping him about that?

---

@eriol BTW this should also remove the need to rewrite every import statement
inside requests itself.
",untitaker,dcramer
2567,2015-05-10 16:04:52,"@sigmavirus24

> @untitaker did you mean @mitsuhiko? I would hope a direct ping would suffice.

Unfortunately I don't think so. I haven't heard back from him either, perhaps
@dcramer can ping him about that?

---

@eriol BTW this should also remove the need to rewrite every import statement
inside requests itself.
",untitaker,sigmavirus24
2567,2015-05-10 16:04:52,"@sigmavirus24

> @untitaker did you mean @mitsuhiko? I would hope a direct ping would suffice.

Unfortunately I don't think so. I haven't heard back from him either, perhaps
@dcramer can ping him about that?

---

@eriol BTW this should also remove the need to rewrite every import statement
inside requests itself.
",untitaker,untitaker
2567,2015-05-10 17:34:27,"@sigmavirus24 I'm not disputing better at all. =) What I'd like to do is to take all reasonable precautions to reduce the risk of deploying this fix. For example, can @eriol and @ralphbean confirm that their package building functions correctly with this patch?

Basically, rushing helps nobody, and I'd like to try to begin a run of stable requests releases if at all possible. The last run of four-or-five broken releases in a row is bad, and we need to not get in the habit of doing that.
",Lukasa,eriol
2567,2015-05-10 17:34:27,"@sigmavirus24 I'm not disputing better at all. =) What I'd like to do is to take all reasonable precautions to reduce the risk of deploying this fix. For example, can @eriol and @ralphbean confirm that their package building functions correctly with this patch?

Basically, rushing helps nobody, and I'd like to try to begin a run of stable requests releases if at all possible. The last run of four-or-five broken releases in a row is bad, and we need to not get in the habit of doing that.
",Lukasa,sigmavirus24
2567,2015-05-10 18:21:15,"@Lukasa Note that I'm not arguing for this to be merged with a release to follow immediately after. We could even defer this to 3.0 if you want to not rush it (which I don't think we're doing frankly). The current state of affairs only makes end users' lives more difficult at this point.
",sigmavirus24,Lukasa
2567,2015-05-12 11:11:10,"@Lukasa if is it possible I would like to have a bit more of time: right now I'm checking reverse dependencies of requests in Debian to not break stuff, see: https://lists.debian.org/debian-python/2015/05/msg00021.html (@sigmavirus24 thanks for details, I did not have time to reply on the list, but your mail was appreciated :smile:).
I should finish during this week, so should be able to cherry pick and test this immediately after.
",eriol,Lukasa
2567,2015-05-12 11:11:10,"@Lukasa if is it possible I would like to have a bit more of time: right now I'm checking reverse dependencies of requests in Debian to not break stuff, see: https://lists.debian.org/debian-python/2015/05/msg00021.html (@sigmavirus24 thanks for details, I did not have time to reply on the list, but your mail was appreciated :smile:).
I should finish during this week, so should be able to cherry pick and test this immediately after.
",eriol,sigmavirus24
2567,2015-05-12 13:10:31,"@eriol always happy to help. We don't have a release planned so we don't have a timeline for when this would be merged. Take your time. Don't stress out. We'll be here and so will the patch. =D
",sigmavirus24,eriol
2567,2015-05-19 08:52:15,"@wangshaochen you can just add "".patch"" to PR URL:
https://github.com/kennethreitz/requests/pull/2567.patch

HTH!
",eriol,wangshaochen
2567,2015-05-19 09:05:07,"@eriol  Ths!  I yet don't know. with command_line? 
",wangshaochen,eriol
2567,2015-05-27 12:50:35,"@Lukasa @sigmavirus24 any updates on this?
",untitaker,Lukasa
2567,2015-05-27 12:50:35,"@Lukasa @sigmavirus24 any updates on this?
",untitaker,sigmavirus24
2567,2015-05-27 13:02:45,"We're still hoping to have @eriol and/or @ralphbean take a swing at this code and confirm it's working for them.
",Lukasa,eriol
2567,2015-06-09 14:50:32,"Sorry for the delay! I have tested it and all seem fine! Many thanks @untitaker!
",eriol,untitaker
2567,2015-06-10 20:14:50,"I've done a fair bit of testing of this patch for Debian and I think it does exactly what we want it to do.  @eriol I've been able to remove the 02_user-system-chardet-and-urllib3.patch and 04_make-requests.packages.urllib3-same-as-urllib3.patch quilt patches and replace it with one that essentially does the PR submitted here.  AFAICT, it all looks pretty good.  I have a diff against the package that makes it all work, and I'd be inclined to upload a 2.7.0-3 with this patch, even if upstream requests puts it off until 2.8.  I'll open a Debian bug with additional details.

+1 for this PR from me.
",warsaw,eriol
2567,2015-06-10 20:31:43,"@warsaw thanks for the debdiff, I forgot to commit my changes when I tested it. I agree with you, I want to remove Debian patches in favor of this and I was already working on 2.7.0-3. :smile: 
",eriol,warsaw
2567,2015-06-30 16:00:56,"@untitaker to be honest, there are some differences. @dstufft mentioned that his patch includes something that handles `from pip._vendor import requests` as well as `import pip._vendor.requests` but I'm not sure we need that because I think ours just works as it's written. Perhaps @dstufft could explain more about what he found to be necessary in his patch that didn't work with this (assuming he tried this one in pip)
",sigmavirus24,untitaker
2567,2015-09-01 07:16:47,"I don't think we _can_ tell people not to import from urllib3: there are too many documents etc. around the web that indicate that we expected people to do that. It'll be really painful trying to discourage that behaviour in future, especially as we can't necessarily very easily deprecate/remove it.

I'm mostly interested to see if @kennethreitz is actually open to considering unvendoring. If he's not, this is the best solution we have so far and we should roll with it.
",Lukasa,kennethreitz
2567,2015-09-03 13:06:54,"@dstufft : +1
",warsaw,dstufft
2567,2015-09-15 19:21:45,"@rschoon yes, the problems with `__name__` have already been mentioned by mitsuhiko before.

I'd just opt for unbundling all those dependencies, and optionally providing a separate tarball that contains all dependencies (all as top-level package each, not inside the requests package).
",untitaker,rschoon
2567,2015-09-16 20:24:22,"@untitaker yes, it's included in Debian (and Ubuntu) since 2.7.0-3. It was uploaded on 11 Jun 2015. I did not receive complains related to it yet.
",eriol,untitaker
2565,2015-05-01 17:31:01,"@wbolster All responses must do one of the following: have a content-length, have a chunked transfer encoding, or close the connection when they're done. We know we have a bug with chunked, but when that's resolved there should be no problems. =)
",Lukasa,wbolster
2562,2015-04-23 14:13:46,"@ikus060 Many people have proposed it and even begun work: see #2425, #2156, #1982, shazow/urllib3#486, shazow/urllib3#284, shazow/urllib3#68, and shazow/urllib3#44.

You are welcome to attempt to do the work. The initial implementation should be over in urllib3: I recommend that you start work there.

If you were simply _requesting_ the feature, the core team does not believe it's an important enough priority to justify our development time. In practice, the absence of SOCKS support does not appear to be limiting the success of the library. For that reason, unless someone does the work for the implementation themselves, I do not believe we'll be implementing it ourselves.
",Lukasa,ikus060
2561,2015-04-23 16:04:31,"@JonathonReinhart no joke. Feel free to [Contact](https://github.com/contact) them though and tell them that. I know I have in the past and the more people who ask for it, the better our chances of getting it (I would hope).
",sigmavirus24,JonathonReinhart
2560,2015-04-23 06:20:45,"@kennethreitz In that case, what do you think of [this doc](https://hyper.readthedocs.org/en/latest/contributing.html).
",Lukasa,kennethreitz
2560,2015-04-23 16:08:20,"@Lukasa wonderful!
",kennethreitz,Lukasa
2559,2015-04-27 16:50:22,"@missingdink whenevs :)
",kennethreitz,missingdink
2559,2015-04-27 23:48:54,"@missingdink :cake: 
",sigmavirus24,missingdink
2559,2015-07-18 04:12:31,"@missingdink actually came through @hickford. We just need to revamp our stuff with the new logo.
",sigmavirus24,hickford
2559,2015-07-18 04:12:31,"@missingdink actually came through @hickford. We just need to revamp our stuff with the new logo.
",sigmavirus24,missingdink
2558,2015-04-22 19:34:55,"@dcramer This is interesting timing, as we're about to release 2.6.1. We believe 2.6.1 contains some fixes in this area. Do you mind trying the current master to see if that solves your problems?
",Lukasa,dcramer
2558,2015-04-22 19:37:06,"@Lukasa I did try the referenced sha (which was master an hour or so ago) and it still exhibited the same issue. It's possible this is an issue in Logan, but the code there isn't very hacky (and I'm mostly certain its correct).
",dcramer,Lukasa
2558,2015-04-22 19:52:55,"@dcramer The easiest way to verify that this is the source of the problem is to replace `requests/packages/__init__.py` with an empty file and see if the problem still appears. (I'm assuming you've already verified that the problem doesn't appear with some earlier version of Python.)
",Lukasa,dcramer
2557,2015-04-22 22:43:12,"Also, chardet is working on fixing these issues. If you'd like to see them fixed faster you too can help @dcramer 
",sigmavirus24,dcramer
2556,2015-04-22 16:37:42,"Ah okay. I think we need to update our release process then to pull from the release branch unless we specifically need to pull from master. @Lukasa I think you have write access to my fork, would you mind updating our vendored copy of requests to use urllib3 from the ""release"" branch? If not, I'll do it over lunch or after work today.
",sigmavirus24,Lukasa
2556,2015-04-22 17:42:43,"Sadly, I'm not a collab on your repo @sigmavirus24. =( I opened a pull request instead: sigmavirus24/requests#3.
",Lukasa,sigmavirus24
2556,2015-04-22 21:33:09,"Alright @sigmavirus24, this all LGTM except that the changelog probably needs an update to say that we aren't fixing the machinery, we're just removing it wholesale.
",Lukasa,sigmavirus24
2556,2015-04-22 21:53:02,"> Alright @sigmavirus24, this all LGTM except that the changelog probably needs an update to say that we aren't fixing the machinery, we're just removing it wholesale.

:heavy_check_mark: 
",sigmavirus24,sigmavirus24
2556,2015-04-22 22:00:36,"Make it happen, @sigmavirus24. :shipit: 
",Lukasa,sigmavirus24
2556,2015-04-22 22:09:19,"Many thanks for the update, I will include it as a patch, and I will be very happy to help on making it robust, thread-safe... I will also put a note on README.Debian explaining why in Debian VendorAlias patch is used, but I don't know how many users look at /usr/share/doc: I will try to monitor all issues here but please ping me on a issue related to Debian. @sigmavirus24 many thanks for your offer, it's highly appreciated! :smile:

I will update requests for Debian during the weekend. Hopefully Debian Jessie will be released on saturday, so I will be able to backport 2.6.1 for Jessie as soon as it will migrate to Debian Testing.
",eriol,sigmavirus24
2554,2015-06-05 15:59:50,"Apologies for the slow response on this. After trying several different approaches, it seems the most flexible and cleanest approach is subclassing.

Here's an example that meets our particular needs of prepending a URL;



As for documentation, I'm thinking perhaps a ""recipes"" page of patterns and user contributed subclasses for achieving common goals, which aren't appropriate for inclusion into the core.

Thoughts @kennethreitz / @sigmavirus24 ?
",foxx,sigmavirus24
2554,2015-07-28 19:39:15,"@foxx Did you ever end up submitting a PR with this implementation? I wanted this feature today.
",jaraco,foxx
2554,2015-07-28 20:23:51,"@jaraco double checking the toolbelt, no it does not seem as if @foxx ever did.
",sigmavirus24,jaraco
2554,2015-07-28 20:23:51,"@jaraco double checking the toolbelt, no it does not seem as if @foxx ever did.
",sigmavirus24,foxx
2550,2015-04-17 12:44:36,"@sigmavirus24 send another pr with that :)
",kennethreitz,sigmavirus24
2547,2015-04-11 03:02:46,"Thanks @sh1buy! :sparkles: :cake: :sparkles:
",sigmavirus24,sh1buy
2543,2015-04-21 05:58:54,"Again, to be clear, `Connection reset by peer` means that the remote end closed the connection while we were expecting to read data from it. In your case @JohnCC330, we're trying to read a chunked body and the chunk is not complete.

You should be trying to investigate why the modem closed the connection in a non-graceful manner.
",Lukasa,JohnCC330
2543,2015-04-25 07:41:45,"@JohnCC330 How urgent is a fix for this? Are you ok to wait until we have a proposed fix for #2568?
",Lukasa,JohnCC330
2543,2015-07-06 16:37:21,"@edsu what version of Python is installed and what version of requests is in use?
",sigmavirus24,edsu
2543,2015-07-06 21:58:47,"@remagio let us know if 2.7.0 resolves your issue. I suspect it will
",sigmavirus24,remagio
2543,2015-07-07 13:08:47,"I agree with @Lukasa, this is something requests is expecting urllib3 to catch, wrap, and raise for us to do the same.
",sigmavirus24,Lukasa
2538,2015-04-07 13:48:59,"So actually this looks like @francesco1119 is using python brew to install things, which does not work (I would guess) with aptitude/dpkg. That would be your problem @francesco1119. You can't mix python brew with system packages. You should look for `/usr/bin/python` (or something like that) to determine:
1. If you have Python installed
2. What version of Python _is_ installed.
",sigmavirus24,francesco1119
2538,2015-04-08 00:28:24,"@francesco1119 You're clearly in a whole world of dependency pain.

The reason pulseaudio can't satisfy its requests requirement is because there is no requests package installed, because apt-get ignores anything installed by pip. Your inability to install requests is a much bigger problem, and clearly your system requires real work to get into a good state again.
",Lukasa,francesco1119
2538,2015-04-08 00:29:17,"Hello, thanks for the ping for the Debian related stuff!

@francesco1119 I don't know python brew but it does not create a _Debian package_ (for this you can use [stdeb](https://github.com/astraw/stdeb)) so this is why when installing pulseaudio-dlna_0.3.4_all.deb you get that dependency is not satisfiable.
If you want to use pulseaudio-dlna_0.3.4_all.deb (it's not an official Debian package, did you package it by yourself?) the best thing to do is to backport requests from Jessie. Keep in mind that you have to backport python-urllib3 before backporting requests since, in Debian, requests uses the system-wide urllib3.
To backporting a package you can follow https://wiki.debian.org/SimpleBackportCreation

Otherwise you can use stdeb and package requests from PyPI (you don't need to follow Debian Policy for a package builded by yourself), and then install pulseaudio-dlna_0.3.4_all.deb. Or since you seems at ease with python brew, you can see if you can install pulseaudio-dlna using it.

HTH
",eriol,francesco1119
2537,2015-04-09 17:15:41,"@kevinburke thanks for your response too. I'm actually already using timeout in my code, but I think that also controls how long the request can take. We have some requests that take quite a while like 25 seconds (not great form) based on some app requirements we have.

I'll see how hard it might be to add a connection_timeout option and maybe put in a request or pull request for that. 

:update looking at the code I immediately see something about connection timeout in a tuple of some sort.) So hopefully that works.

Thanks!!!!
",twiggy,kevinburke
2532,2015-04-06 23:02:22,"I have no remarks about `prepare_cookies` specifically, however...

Generally speaking, I found the adapter API for sessions and cookie handling difficult to work with, as you can tell by some of the odd things I'm doing in my own code. Using the private-sounding `_cookies` attribute feels icky, and having to pass the cookie from the first response so I can artificially merge it into the headers of the second response (rather than a simple `self.cookies.set(...)` or the like) is irritating.

There seem to be some disconnects between the adapter API and the concept of a Requests session (or even just an HTTP session). It's very possible such a connection was never intended in the first place, but it is frustrating nonetheless. If there is a way of refactoring my code to make it more idiomatic I would greatly appreciate any suggestions and advice.

@smiley I might be interested in merging this functionality into cloudflare-scrape. I assume this is for a competing CDN like Incapsula. Feel free to email me.
",Anorov,smiley
2529,2015-04-05 22:33:06,"@hartwork Are you using a new virtualenv for each test? Mostly importing pyopenssl takes a while because cffi does an implicit C compile at import time in some cases, but that should only happen the first time.
",Lukasa,hartwork
2529,2017-01-26 14:20:19,@afcady Please report the issue to the appropriate upstream.,Lukasa,afcady
2528,2015-04-06 00:19:14,"@hartwork as @Lukasa has already kindly explained, PyPI is the source of truth. Not GitHub. You might notice that there are _several_ releases missing notes on GitHub.
",sigmavirus24,hartwork
2528,2015-04-06 00:19:14,"@hartwork as @Lukasa has already kindly explained, PyPI is the source of truth. Not GitHub. You might notice that there are _several_ releases missing notes on GitHub.
",sigmavirus24,Lukasa
2528,2015-04-06 01:46:59,"@hartwork it is not trivial.
",sigmavirus24,hartwork
2527,2015-04-05 18:16:28,"Hi @sigmavirus24,
Thanks for quick response.  This situation is a little tricky that the base CookieJar class does not supply the 'copy' method but the RequestsCookieJar does. However, in the method 'prepare_cookies' of PreparedRequests, you permit all the instances of the subclass of CookieJar preserved as its original form(which may not contain 'copy' method). As I suppose, one uniformed way is to remove the copy method of  RequestsCookieJar and use the python copy function when copy is needed(This may need changes of other places). The other is more conservative, that is, restrict the self._cookies of PreparedRequest to be only the instance of RequestsCookieJar or else check whether having copy method before calling this method of a CookieJar instance.


",zhaoguixu,sigmavirus24
2527,2015-04-06 02:29:39,"@zhaoguixu check out #2534 
",sigmavirus24,zhaoguixu
2527,2015-04-06 02:50:07,"Thanks for catching that @zhaoguixu. I updated the PR to account for that.
",sigmavirus24,zhaoguixu
2526,2015-04-04 06:50:25,"@justintime32 Thanks for the feature request!

It should be entirely possible to write a short authentication handler that does exactly what you need: a quick Google showed [this](http://stackoverflow.com/questions/13506455/how-to-pass-proxy-authentication-requires-digest-auth-by-using-python-requests) as the top result. Because of the ease of adding such a thing yourself, and because it's relatively infrequently used, we don't believe there's much advantage in bringing Proxy Digest Auth into the core library.
",Lukasa,justintime32
2526,2015-04-04 06:53:49,"Hi @Lukasa, I actually started by writing an auth handler. The main issue I ran into was that it only works for non-SSL requests. SSL requests through a proxy are made through a CONNECT tunnel, and the CONNECT request must be authenticated. The auth handler appears to be unable to hook into the proxy tunnel creation step, and therefore SSL requests will always fail.

Are there other ways to add this authentication without modifying the core? Maybe some more hooks around the proxy tunnel creation?
",justintime32,Lukasa
2525,2015-04-03 15:57:47,"\o/ This is beautiful @benjaminran! Thanks so much! :cake: :sparkles:
",Lukasa,benjaminran
2524,2015-04-02 19:30:59,"@Lukasa Thanks for fast reply!
",mktums,Lukasa
2524,2016-10-12 11:17:29,"@gdonzy Please read the above issue, which links to a related tracking issue.
",Lukasa,gdonzy
2523,2015-04-03 12:22:18,"@tardyp : You are correct. This needs further improvement, just as you point out there are other attributes currently shared accross threads that will lead to auth failures in certain conditions.

I'll extend the per-thread approach of num_401_calls to the remaining state-tracking attributes and update the PR for review.

Regarding the drawback you refer to I think it comes as a reasonable compromise:
- This approach won't hurt the currently working single-thread case.
- It guarantees correct authentication in a multi-threaded case (which requests claims to support).
- Simple and easy to understand code change.
",exvito,tardyp
2523,2015-04-03 13:15:09,"@exvito : @vincentxb and I are on the same team, and have story on our sprint to resolve this issue. I was working on it this morning, and am at the point of creating a unit test.
",tardyp,exvito
2523,2015-04-03 13:29:35,"Updated with commit e8d9bc5. Highlights:
- All state now in thread local storage.
- Factored out state initialization to `init_per_thread_state()` which must be called from `__init__()` for the regular, single-threaded case and, eventually, from `__call__()` in the cases where threads that did not create the auth handler invoke it.
- Maybe the `self.tl` name can be better: I opted for short, project leaders may prefer `self.thread_local` or some variation.
- Maybe the test for thread local state initialization in `__call__()` can take a different approach.

Here's a contribution. Thanks for your feedback.
PS: @tardyp does that mean you'll work from this PR or you'll take a different approach and suggest I drop this PR?
",exvito,tardyp
2523,2015-04-03 13:45:26,"@exvito, we worked on the same approach at the same time. 

I opted for the name thd for the local variable, and did the same ini_per_thread call. I think it is not necessery in **init** however, only in _call_

 :+1:  

I have a unit test on a separate commit:

https://github.com/tardyp/requests/commit/e15ed2c46823183b73250503fc44146f979eb891

Please cherry-pick it!
",tardyp,exvito
2523,2015-04-21 12:52:50,"@kennethreitz The unit test for it is, but the product code doesn't. It's just to make it safe when it's called from multiple threads.
",Lukasa,kennethreitz
2519,2015-03-30 17:24:44,"Hi @tdussa,

In the future, please ask _questions_ on [StackOverflow](https://stackoverflow.com/questions/tagged/python-requests). Quoting from [the docs](http://docs.python-requests.org/en/latest/api/?highlight=cert).

> cert – (optional) if String, path to ssl client cert file (.pem). If Tuple, (‘cert’, ‘key’) pair.

So if you have the key file, you can use that. @t-8ch can correct me if I'm wrong, but I don't believe we (or urllib3) support sending a passphrase. I think the lack of support is specifically a limitation of the way the SSL module [loads verification data](https://docs.python.org/3/library/ssl.html?highlight=ssl#ssl.SSLContext.load_verify_locations).
",sigmavirus24,tdussa
2519,2015-04-28 09:48:03,"@tdussa Not at this time I'm afraid. =(
",Lukasa,tdussa
2519,2015-11-17 15:43:42,"@traut There isn't a good one, really. You can attempt to use the TransportAdapter's `init_poolmanager` method to pass objects into urllib3.
",Lukasa,traut
2519,2016-09-14 15:24:15,"@traut Yes. We're a few releases away from users being able to use TransportAdapters to provide SSLContext objects to urllib3, which will resolve this issue.
",Lukasa,traut
2519,2016-09-14 15:29:49,"nice! thanks for the update, @Lukasa 
",traut,Lukasa
2519,2017-03-08 21:11:31,"@amiralia Right now the answer is ""not easily"". The Python standard library `ssl` module exposes no way to load client certs/keys from bytes: only from files. This is despite the fact that OpenSSL itself does provide these tools.

One way around this is to ensure you're using Requests PyOpenSSL support. If you do that, you can get an `SSLContext` object that uses PyOpenSSL instead by calling `requests.packages.urllib3.util.ssl_.create_urllib3_context()`. That will let you call PyOpenSSL methods like [`use_certificate`](https://pyopenssl.readthedocs.io/en/stable/api/ssl.html#OpenSSL.SSL.Context.use_certificate), which will work with files loaded from memory.

Unfortunately, until the stdlib changes its API, that is the only option.",Lukasa,amiralia
2518,2015-03-27 00:15:58,">  I think using the dollar and thus requiring the ""version"" line ends with a newline is a bad idea : it might end with a whitespace or a comment, it's valid.

No one asked for you to add anything other than the `^`.

Thanks for fixing this up. Any further feedback @Lukasa?
",sigmavirus24,Lukasa
2518,2015-03-27 12:49:49,"I misspoke twice then! Sorry for the confusion @deronnax. Thanks for the contribution! :cake: 
",sigmavirus24,deronnax
2517,2015-03-24 15:52:27,"No worries @chrj. Hopefully now that there's a bug here, people will see it and not open a new one. =D It's still appreciated. :D :+1: 
",sigmavirus24,chrj
2516,2015-03-24 14:12:32,"@sigmavirus24 Which people would rather use simplejson? For what reason?
",ionelmc,sigmavirus24
2516,2015-03-24 14:56:57,"@ionelmc it was an oft-requested feature. simplejson supposedly has better performance. As for ""which people"" my job is not to catalogue all users of requests relying on _X_ feature. The people using this are likely easy to find in the issue history of requests. Searching ""simplejson"" should probably get you 98% of the way there.
",sigmavirus24,ionelmc
2516,2015-03-24 17:13:42,"@sigmavirus24 Seems your simplejson tests were incomplete, see https://github.com/simplejson/simplejson/issues/114 for details.

Victor Stinner [seemed to be able to reproduce the issue](https://gist.github.com/ionelmc/811bbf8ceed66ca83a2a). But alas, cpython devs aren't interested in fixing simplejson. Neither am I.
",ionelmc,sigmavirus24
2516,2015-04-02 18:48:48,"@kennethreitz they are in fact slightly different, the builtin json receiving serveral bugfixes simplejson didn't.
",ionelmc,kennethreitz
2513,2015-03-23 15:13:56,"@Montycarlo please see #2514. This should have been opened at [urllib3](/shazow/urllib3) and the change needs to be much better since this will introduce a breaking change to urllib3.
",sigmavirus24,Montycarlo
2513,2015-03-23 15:14:07,"Sorry @Montycarlo! I should have caught that earlier. =(
",Lukasa,Montycarlo
2511,2015-03-21 16:48:54,"Actually @Lukasa that's not exactly correct. See https://github.com/kennethreitz/requests/pull/2209 for more details.
",sigmavirus24,Lukasa
2511,2015-03-21 17:10:48,"To be clear, it seems to me that `elapsed` serves almost no purpose at all if it does not work as @Lukasa describes - measuring the total time taken is trivial to do in the application, but measuring the ""time to first byte"" is impossible if Requests does not provide that information itself.
",jribbens,Lukasa
2511,2015-03-22 13:00:57,"This sort of confusion is why I thought it important that the expected behaviour be documented ;-)
TL;DR for the below: it looks like the behaviour changed between versions of Requests and @sigmavirus24 used to be correct but now @Lukasa is correct...

I've set up two URLs. `redirect.php`:



`delay.php`:



With Requests 2.2.1, this is what happens:



With Requests 2.6.0, this happens:



So with `stream=True` the behaviour is consistent between versions, and matches what @Lukasa describes. With `stream=False`, in 2.2.1 `elapsed` changes from ""time to first byte"" to ""time to last byte"", but in Requests 2.6.0 there is no difference.

Can I humbly suggest that this change be regarded as a ""bug fix"" rather than a ""bug to fix""? The new behaviour is more logical, consistent and useful, and to change it would be to remove a feature (i.e. the ability to get TTFB with `stream=False`).
",jribbens,Lukasa
2511,2015-03-22 13:00:57,"This sort of confusion is why I thought it important that the expected behaviour be documented ;-)
TL;DR for the below: it looks like the behaviour changed between versions of Requests and @sigmavirus24 used to be correct but now @Lukasa is correct...

I've set up two URLs. `redirect.php`:



`delay.php`:



With Requests 2.2.1, this is what happens:



With Requests 2.6.0, this happens:



So with `stream=True` the behaviour is consistent between versions, and matches what @Lukasa describes. With `stream=False`, in 2.2.1 `elapsed` changes from ""time to first byte"" to ""time to last byte"", but in Requests 2.6.0 there is no difference.

Can I humbly suggest that this change be regarded as a ""bug fix"" rather than a ""bug to fix""? The new behaviour is more logical, consistent and useful, and to change it would be to remove a feature (i.e. the ability to get TTFB with `stream=False`).
",jribbens,sigmavirus24
2511,2015-03-22 20:19:30,"@sigmavirus24 you misread my comment. I said that time to _last_ byte may not be available, not that `elapsed` (time to _first_ byte) may not be available. And yes, that `sum` expression is exactly what I am using in my application to determine TTFB :-)
",jribbens,sigmavirus24
2511,2015-03-22 20:23:45,"Final question on this - @Lukasa's documentation change specifies that the DNS-lookup time and socket connect() time are excluded from `elapsed`, which seems peculiar if true. Is it true?
",jribbens,Lukasa
2510,2015-03-21 16:21:19,"Sorry @jribbens, but you didn't read #1289 or #1915 clearly enough. I will reproduce my statements from those issues again, in the hopes that this will be the last time I have to write this down. =)

This is an unforeseen problem to do with how exception tracebacks are being reported in Python 3. PEP 3134 introduced this 'chaining exceptions' reporting that you can see in the traceback. The purpose of this error reporting is to highlight that some exceptions occur in except blocks, and to work out what chain of exceptions was hit. This is potentially very useful: for instance, you can hit an exception after destroying a resource and then attempt to use that resource in the except block, which hits another exception. It's helpful to be able to see both exceptions at once.

The key is that the `TypeError` raised as the first exception is **unrelated** to the subsequent ones. In fact, that's the standard control flow in urllib3. This means that the real exception that's being raised here is the request.exceptions.ReadTimeout exception that wraps the urllib3.exceptions.ReadTimeoutError exception being raised in urllib3.

The exception that actually bubbled up to the user code is the **last** one, not the first one. The way you read that traceback is as follows:
- I hit a `TypeError` in this method.
- In the `except` block that caught the `TypeError`, I raised a `socket.timeout` (in practice, this is well down the call stack, in a totally standard control flow)
- In the `except` block that caught the `socket.timeout`, I raised a `requests.packages.urllib3.exceptions.ReadTimeoutError`
- In the `except` block that caught the `requests.packages.urllib3.exceptions.ReadTimeoutError`, I raised a `requests.exceptions.ReadTimeout`.
- This exception was never caught.

To be clear, I have stated quite publicly that I think Python 3 screwed users on this 'feature'. I think it's a misfeature that discourages the Easier to Ask Forgiveness than Permission convention in the Python community because it leads to repeated bug reports like this one, even by people who honestly believe that they have read and understood the previous bug reports on this topic.

I promise you, the `ReadTimeout` is exactly what was raised here.
",Lukasa,jribbens
2510,2015-11-06 21:00:42,"@Grazfather No, it's a syntax error: Python 2 cannot even parse the file with that in it
",Lukasa,Grazfather
2504,2015-03-24 15:16:37,"> The one thing I'm thinking about is whether we want to allow for forward compatibility by actually storing these kwargs in a dictionary for resolve_redirects

Could you expand on that @Lukasa? I'm not quite certain what you mean.
",sigmavirus24,Lukasa
2504,2015-04-16 03:21:59,"I'm okay with this if you are @Lukasa.
",sigmavirus24,Lukasa
2499,2015-03-16 15:45:03,"@nanonyme Currently we can't achieve this: we just don't have the primitives. At the very least we would need shazow/urllib3#507 to get merged before this is even possible.

Additionally, the stdlib SSL module only allows this for some versions: specifically, those with SSLContext objects. That limits it to 2.7.9 and later, which is problematic.

Finally, of course, this doesn't solve the Windows problem since Windows users can't point OpenSSL at a file _period_, they need another solution.

Altogether, I think this is just too niche a request to satisfy at the moment. It seems like it would make life easier for a tiny minority of users, but that minority would be just as well served by a merge of sshazow/urllib3#507 and then playing with SSLContext objects.

Thanks for the suggestion, though! Please keep them coming. =)
",Lukasa,nanonyme
2498,2015-03-15 22:50:39,"@ttomoday By the way, while I did answer your questions, please note that this issue tracker is a place for reporting bugs and discussing improvements, not for general questions about library usage or about HTTP more widely. I recommend using Stack Overflow to ask further questions of that nature. =)
",Lukasa,ttomoday
2497,2015-03-16 16:00:55,"Some lead time on the changes in 3bd8afbff would have been nice, yes.  @alex's recommendations look quite good to me as a 'maximal effort'.  If even portions of those suggestions could be adopted, it would be an improvement.  Thanks for the effort on this!
",ralphbean,alex
2497,2015-03-16 16:09:54,"I agree with @ralphbean and I too would like to thank your effort on this!
",eriol,ralphbean
2497,2015-03-16 20:12:13,"I'm entirely happy to adopt almost all of @alex's suggestions. I'll take a look at writing up a procedure at some point shortly and pass it around for consideration.
",Lukasa,alex
2497,2015-03-16 20:15:10,"FWIW, if you need those in a more prose version, you're welcome to any text
from http://cryptography.readthedocs.org/en/latest/security/

On Mon, Mar 16, 2015 at 4:12 PM, Cory Benfield notifications@github.com
wrote:

> I'm entirely happy to adopt almost all of @alex https://github.com/alex's
> suggestions. I'll take a look at writing up a procedure at some point
> shortly and pass it around for consideration.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/2497#issuecomment-81910438
> .

## 

""I disapprove of what you say, but I will defend to the death your right to
say it."" -- Evelyn Beatrice Hall (summarizing Voltaire)
""The people's good is the highest law."" -- Cicero
GPG Key fingerprint: 125F 5C67 DFE9 4084
",alex,alex
2497,2015-03-18 17:49:03,"@ralphbean Ah, yes, that was something I intended but clearly failed to make explicit in the document. I'll extend it to include it.
",Lukasa,ralphbean
2493,2015-03-15 18:44:00,"@yasoob Feel free to open a pull request adding yourself to AUTHORS.rst!
",Lukasa,yasoob
2490,2015-03-14 12:40:29,"With that detail, we can definitely write a socket-level test. Thank you @ben-crowhurst 
",sigmavirus24,ben-crowhurst
2490,2015-03-14 12:41:39,"@sigmavirus24 FYI, if you're planning to take this, I think the fix will actually go into urllib3 (or maybe even httplib, if it turns out httplib can't handle it).
",Lukasa,sigmavirus24
2490,2015-03-21 16:46:13,"@Lukasa you're correct.
",sigmavirus24,Lukasa
2487,2015-03-12 13:21:10,":cake: :star: :cookie: Thanks for this @ulope! Want to add yourself to AUTHORS.rst while we're here?
",Lukasa,ulope
2487,2015-03-14 11:19:03,"Beautiful! :cookie: :sparkles: :cake:

Thanks so much @ulope!
",Lukasa,ulope
2486,2015-03-13 19:49:59,"@p-p-m right, so the only way to reproduce is during install from an install_requires. I've got that too, I'll just have to push this commit up to pypi under a new project name and test that way.
",wardi,p-p-m
2486,2015-03-13 20:51:20,"@wardi with this info I have an idea how to test this without making a new package on PyPI. Give me a couple hours to experiment. Thanks.
",sigmavirus24,wardi
2486,2015-03-13 22:09:03,"So here's something that's interesting. I followed @p-p-m's instructions and had 0 problems. (Granted I've done it once so this may be non-deterministic behaviour and may require repetition.)

That said, I tried it on setuptools 3.6. I would hope that Travis is on an even more recent version.
",sigmavirus24,p-p-m
2486,2015-03-15 02:20:21,"@johnnykv 2.6.0 was released earlier today (2015 March 14)
",sigmavirus24,johnnykv
2481,2015-03-10 14:22:12,"@gsakkis Another option is to try passing the username and password to the proxy URL, and hope that HAProxy's HTTP engine is too stupid to strip the Proxy-Authorization header off. That is, change your command line to:


",Lukasa,gsakkis
2481,2015-03-10 14:36:47,"@bagder Thanks for looking in. =) I suspected that was going to be the reasoning for Proxy-Connection, but did just want to clarify it. I doubt we'll add it.

Glad we were able to help find a regression, even if that regression made it look like you were working and we weren't! ;)

Given that feedback, I'm closing this issue because it's not a requests bug, it's a problem with @gsakkis' HAProxy usage/configuration. @gsakkis, please note that it's definitely possible to configure HAProxy to do what you want it to do, you just need to make some configuration changes or change the way you make the requests.
",Lukasa,bagder
2481,2015-03-10 14:36:47,"@bagder Thanks for looking in. =) I suspected that was going to be the reasoning for Proxy-Connection, but did just want to clarify it. I doubt we'll add it.

Glad we were able to help find a regression, even if that regression made it look like you were working and we weren't! ;)

Given that feedback, I'm closing this issue because it's not a requests bug, it's a problem with @gsakkis' HAProxy usage/configuration. @gsakkis, please note that it's definitely possible to configure HAProxy to do what you want it to do, you just need to make some configuration changes or change the way you make the requests.
",Lukasa,gsakkis
2481,2015-03-10 14:39:39,"@Lukasa passing credentials to the proxy url does work indeed! I'd never figure it out on my own, thank you so much for the investigation!

PS: I linked to this ticket on the selected ServerFault answer in case someone stumbles on it too.
",gsakkis,Lukasa
2481,2015-03-10 15:12:01,"@gsakkis Happy to help, I'm glad we were able to resolve the situation. =)
",Lukasa,gsakkis
2480,2015-03-09 14:19:57,"@rubentorresbonet if you choose to modify httplib like that, that's your decision. It's not up to our discretion nor is it our place to recommend it.

I think modifying a standard library is a poor way of handling this, but if that's what you choose to do, there's nothing for anyone to do about it.

Also @Lukasa didn't tell you to do _everything_ with sockets, only portions dealing with ICY.
",sigmavirus24,rubentorresbonet
2480,2015-03-09 14:19:57,"@rubentorresbonet if you choose to modify httplib like that, that's your decision. It's not up to our discretion nor is it our place to recommend it.

I think modifying a standard library is a poor way of handling this, but if that's what you choose to do, there's nothing for anyone to do about it.

Also @Lukasa didn't tell you to do _everything_ with sockets, only portions dealing with ICY.
",sigmavirus24,Lukasa
2479,2015-03-08 18:01:51,"@manojgudi can you give us more information. We have absolutely nothing to debug this with otherwise and will be forced to close the issue.
",sigmavirus24,manojgudi
2479,2015-03-08 18:06:43,"@sigmavirus24 Thanks for prompt reply! Lemme write a script to reproduce this issue.. Gimme a day or so..

Thanks
",manojgudi,sigmavirus24
2478,2015-03-07 09:50:52,"@plaes This is a great PR. One small note however, we don't use `unittest`, we use `pytest`. This is why your test suite failed: `unittest.expectedFailure` doesn't exist in Python 2.6. Instead, use [`pytest.mark.xfail`](http://pytest.org/latest/skipping.html#mark-a-test-function-as-expected-to-fail).
",Lukasa,plaes
2478,2015-03-07 10:03:41,"Hurrah! Thanks so much for this @plaes!
",Lukasa,plaes
2478,2015-03-07 14:01:11,"Thanks @plaes!
",sigmavirus24,plaes
2476,2015-03-06 18:17:51,"So I took a quick look at the implementation of [`Response.iter_lines`](https://github.com/kennethreitz/requests/blob/master/requests/models.py#L693)

We read a chunk from the response, and then find the line within that chunk and return it. We store the remainder locally in the function and not on a the response object itself. I think an update to the docs like @Lukasa described is warranted. 
",sigmavirus24,Lukasa
2474,2015-03-05 12:27:29,"Thanks @dieterv, this is a known issue, see shazow/urllib3#561. We aim to push out a fix for this very shortly.
",Lukasa,dieterv
2468,2015-03-03 20:01:38,"Thanks @scholer! :cake: 
",sigmavirus24,scholer
2468,2015-03-03 20:37:13,"Thank you for accepting my pull request, @sigmavirus24  :)
",scholer,sigmavirus24
2467,2015-03-03 16:12:47,"@sigmavirus24 is entirely right. Testing against http2bin.org (which correctly returns a content-length) does not reveal this bug.

Focusing further on this behaviour, the problem is very, very deep. So far I've tracked it as low as line 371 of socket.py, in the `SocketIO.readinto` method, which looks like this:



This method calls `socket.recv_into` for non-SSL methods, and `SSLSocket.recv_into` for SSL. This is where the problem seems to lie. That recv_into method seems to block for quite a long time in the SSL case, presumably waiting on an underlying socket timeout.
",Lukasa,sigmavirus24
2467,2015-03-03 16:36:32,"So at this point, it would seen this is a stdlib/language-level bug, if I'm following your research correctly @Lukasa. In that case, we should continue this on bugs.python.org. With that in mind, @zsalzbank you absolutely will not be better served by waiting for requests to cut a release because there's nothing the library can do (I don't think) that will allow us to work around this for you given the level it seems to be at. If you'd like to wait longer, you can but I think you'll see better results in general by writing RFC compliant servers.
",sigmavirus24,Lukasa
2467,2015-03-03 16:36:32,"So at this point, it would seen this is a stdlib/language-level bug, if I'm following your research correctly @Lukasa. In that case, we should continue this on bugs.python.org. With that in mind, @zsalzbank you absolutely will not be better served by waiting for requests to cut a release because there's nothing the library can do (I don't think) that will allow us to work around this for you given the level it seems to be at. If you'd like to wait longer, you can but I think you'll see better results in general by writing RFC compliant servers.
",sigmavirus24,zsalzbank
2467,2015-03-03 17:01:35,"@zsalzbank Sadly, you're looking at the wrong RFC. RFC 2616 got superseded by RFCs 732[0-5]. From [RFC 7320](https://tools.ietf.org/html/rfc7230) section 3.3:

> The presence of a message body in a request is signaled by a Content-Length or Transfer-Encoding header field.  Request message framing is independent of method semantics, even if the method does not define any use for a message body.

The follow-on logic is in section 3.3.3, that says:

>    `7.  Otherwise, this is a response message without a declared message
>        body length, so the message body length is determined by the
>        number of octets received prior to the server closing the
>        connection.`

Given that the server is _not_ closing the connection in this HTTPS case, the server is still at fault here.

Requests is really doing the best it can with the information it has.
",Lukasa,zsalzbank
2467,2015-03-03 17:11:15,"@zsalzbank Really? I don't. Exact same code (fixing the imports up) shows no delay on Python 2.
",Lukasa,zsalzbank
2467,2015-03-03 17:22:16,"@zsalzbank To be clear, the _fundamental_ bug is absolutely with the server: it just turns out that mostly Python handles it pretty well. The server should do one of three things:
1. Use Transfer-Encoding: chunked, and send the empty chunk to terminate the data stream. We can then see that the server sent Connection: close, and close the connection.
2. Use Content-Length, and we can then close once we've read that content.
3. Send neither, and the server has to close the connection.

None of the above is happening.
",Lukasa,zsalzbank
2467,2015-03-03 17:23:42,"@zsalzbank Using Python 2.7.8 and requests 2.5.3 I do not see the delay. What platform are you on?
",Lukasa,zsalzbank
2467,2015-03-03 17:24:50,"@zsalzbank I also think you're misreading that text. Look closely (emphasis mine):

> Additionally, while HTTP/1.1 requests and responses are expected to be keep-alive by default, if the initial **request** had an explicit connection: close header from the router to the dyno[...]

Your 'response' has Connection: close, but your request does not.
",Lukasa,zsalzbank
2465,2015-03-02 15:49:34,"@gRanger915 We agree, there's a candidate fix in #2466.
",Lukasa,gRanger915
2465,2015-03-29 14:39:26,"@sigmavirus24 Seems that we're still having some trouble here with our new vendoring logic. Care to take a look?
",Lukasa,sigmavirus24
2465,2015-03-29 15:04:13,"@aanand if you can remove as many variables from your reproduction case as possible that'd be appreciated (e.g., just PyInstaller and Requests, no docker or anything else)
",sigmavirus24,aanand
2465,2015-03-29 16:03:03,"Sorry, meant to reopen this after @aanand commented but never did.

So here's the minimal number of steps to reproduce this:
1. Create a virtualenv
2. `pip install pyinstaller requests`
3. Create a file (e.g., `foo.py`) that imports requests (and optionally uses it)
4. Run `pyinstaller -F <filename>` (e.g., `pyinstaller -F foo.py`)
5. Run the created single-file executable (e.g., `./dist/foo`)

Here's the slightly more entertaining thing. In a plain 2.7 environment, if I do this:



So this is, in a large part, a wart in how the import machinery in Python 2.x works. If PyInstaller weren't experimental on Python 3, I'd spend time seeing if I could reproduce it there, but I suspect it wouldn't happen. I have a hunch for how to fix this though (and it doesn't involve needing to toy with the custom import machinery any further).
",sigmavirus24,aanand
2463,2015-02-27 02:35:23,"@GAZ082 the following should work:



In the future though, questions like this will be answered quickly on [StackOverflow](https://stackoverflow.com/questions/tagged/python-requests)
",sigmavirus24,GAZ082
2462,2015-03-06 07:14:24,"@sigmavirus24 What's the odds this is related to our import machinery?
",Lukasa,sigmavirus24
2462,2015-03-11 14:58:58,"@sigmavirus24 You've got more experience in this area than me. Thoughts?
",Lukasa,sigmavirus24
2462,2015-03-12 01:56:34,"@wardi did you test that solution or would you be willing to test it if I put it on a branch?
",sigmavirus24,wardi
2460,2015-02-24 22:53:16,"To be clear, @Lukasa is talking about 



I'm rather surprised though that that would even be used. I thought that absolute imports on Python 3 would be preferring the stdlib's copy module.
",sigmavirus24,Lukasa
2460,2015-02-25 08:32:40,"@Lukasa Yes, this was a second python script, that used request module. Importing request library worked well in first file.
",theasder,Lukasa
2460,2015-02-25 14:05:59,"Specifically the import of tempfile never completes, so when urllib tries to import it, we go back through that look. @theasder can you try importing requests on its own from a different directory or renaming `grabber/copy.py` to `grabber/_copy.py`?
",sigmavirus24,theasder
2460,2015-02-25 18:47:17,"@sigmavirus24 hooray, it works, thank you!
",theasder,sigmavirus24
2460,2015-02-25 19:43:08,"Excellent! Thanks for the report @theasder.
",Lukasa,theasder
2456,2015-02-24 07:12:01,"Yeah, I'm seeing three Thawte certificates in our bundle and three Thawte certificates in Mozilla's bundle. Which certificate exactly are you expecting to see, @jimrollenhagen?
",Lukasa,jimrollenhagen
2455,2015-02-24 07:07:11,"@sigmavirus24 I would rather blame the new ciphersuite or cert bundle (not tested)
",t-8ch,sigmavirus24
2455,2015-02-24 07:15:02,"I'm assuming that you hand-typed that code @rmcgibbo? The reason I ask is because you're accessing HTTP urls there.
",Lukasa,rmcgibbo
2455,2015-02-24 07:21:11,"@Lukasa I can now reproduce it, the URLs are right, there seems to be an redirect
",t-8ch,Lukasa
2455,2015-02-24 07:29:37,"@Lukasa Are you running from a git checkout? It works for me there to, but not from a wheel, will try a sdist
",t-8ch,Lukasa
2455,2015-02-24 07:42:18,"@t-8ch What makes you say that?
",Lukasa,t-8ch
2455,2015-02-24 09:55:55,"@t-8ch To verify: does your bundle succeed with `s_client`?
",Lukasa,t-8ch
2455,2015-02-24 15:39:05,"Ok, @dstufft just realised his 2.7.9 success is a red herring, so the summary is this:
- The bundle works fine when used from the command line with openssl s_client
- It does not work when used from within Python
",Lukasa,dstufft
2455,2015-02-24 17:26:51,"@asmeurer We already have. =) 2.5.3 is out that resolves this bug by reverting the offending change from 2.5.2, while leaving all the rest of the changes in place.
",Lukasa,asmeurer
2455,2015-03-01 12:31:07,"@Lukasa 




",t-8ch,Lukasa
2455,2015-03-01 15:07:22,"I followed the same steps as @t-8ch (sans the checkout of 2.5.2) and got the same problem on OSX 10.10 with 1.0.1l
",sigmavirus24,t-8ch
2455,2015-03-01 16:56:51,"@dstufft There's some extra stuff here. In particular, we need a way to identify which certificates were removed because they're 1024 bit certs and which were removed for other reasons. Mozilla doesn't really publish that information, which makes it trickier than I'd like to do this in an automated way.

As for dropping the 1024-bit roots, I'm extremely nervous about doing that. We'll fall into a trap of ""this works in my browser but not with requests"" and I don't think ""It's OpenSSL's fault"" is a sufficiently good answer.
",Lukasa,dstufft
2455,2015-03-01 17:09:11,"@Lukasa yea, it could catch certificates removed for other reasons, though maybe the timing would mean that it's unlikey? Did all the 1024 bit roots get removed in one commit maybe?
",dstufft,Lukasa
2455,2015-03-01 18:31:12,"@dstufft Is there even a way to inspect the certificate chain? `getpeercert()` only gives you the leaf certificate.
",t-8ch,dstufft
2455,2015-03-01 19:04:34,"@asmeurer while you're certainly not alone in that, you're also not clearly in the majority. I can think of several very large communities that will just place an upper cap on the version of requests they use. That said, I agree with you entirely that the people who complain to the services that cause breakage will be entirely in the minority and receive absolutely no help from the operators.
",sigmavirus24,asmeurer
2455,2015-03-01 19:48:19,"@sigmavirus24 has a better relationship with the distros than I do.
",Lukasa,sigmavirus24
2455,2015-03-02 08:42:48,"Thanks @bagder.

Just to update based on where we got to last night in IRC:

@alex's patch would fix the problem in 2.7.10 and 3.5 (assuming it gets merged), as well as possibly a later version of 3.4.

We can do this more generally by ORing in `X509_V_FLAG_TRUSTED_FIRST` numeric constant into the `SSLContext.verify_flags` field:



This will only work on Python versions that a) have an SSL context (2.7.9, 3.2 and onwards), and b) are using OpenSSL 1.0.2 or later. This satisfies a large number of users, but lots of users don't have access to either or both of those things.
",Lukasa,bagder
2455,2015-04-06 14:02:00,"@Lukasa actually, I like that far better. I think we'll need to really work hard to publicize the decision properly. Should we plan the strong bundle for something like 2.8/3.0 so we have at least 2.7 as a release to use so we can really publicize the notion that 2.8/3.0 will break stuff unless they use certifi to work around it?
",sigmavirus24,Lukasa
2455,2015-04-06 20:20:41,"@sigmavirus24 Yeah, I think that would be a good idea. 

@asmeurer We can probably write a tool from the CLI, but ideally I'd add warnings into requests and I don't think we can easily do that.
",Lukasa,sigmavirus24
2455,2015-04-06 20:20:41,"@sigmavirus24 Yeah, I think that would be a good idea. 

@asmeurer We can probably write a tool from the CLI, but ideally I'd add warnings into requests and I don't think we can easily do that.
",Lukasa,asmeurer
2455,2015-04-06 20:21:22,"@asmeurer I wasn't clear enough: I think we _should_ write such a tool.
",Lukasa,asmeurer
2455,2015-04-07 16:16:57,"@Glandos I'm not certain the issue you're having is the same as is being discussed here.
",sigmavirus24,Glandos
2455,2015-04-23 18:01:11,"Alright, fuck that. Here's my new plan.

The following are the Mozilla issues that track the removal of 1024 bit certs that I could find:
- [X] [856718](https://bugzilla.mozilla.org/show_bug.cgi?id=856718)
- [x] [881553](https://bugzilla.mozilla.org/show_bug.cgi?id=881553)
- [x] [936105](https://bugzilla.mozilla.org/show_bug.cgi?id=936105)
- [x] [936304](https://bugzilla.mozilla.org/show_bug.cgi?id=936304)
- [x] [986005](https://bugzilla.mozilla.org/show_bug.cgi?id=986005)
- [x] [986014](https://bugzilla.mozilla.org/show_bug.cgi?id=986014)
- [x] [986019](https://bugzilla.mozilla.org/show_bug.cgi?id=986019)
- [x] [1155279](https://bugzilla.mozilla.org/show_bug.cgi?id=1155279)

I plan to identify every certificate that was removed by these purges and check whether any of them are in the current requests cert bundle. If they are, I plan to restore them to the new certifi bundle.

Does anyone think this plan is bad? @reaperhulk @dstufft @alex @sigmavirus24
",Lukasa,sigmavirus24
2455,2015-04-23 18:01:11,"Alright, fuck that. Here's my new plan.

The following are the Mozilla issues that track the removal of 1024 bit certs that I could find:
- [X] [856718](https://bugzilla.mozilla.org/show_bug.cgi?id=856718)
- [x] [881553](https://bugzilla.mozilla.org/show_bug.cgi?id=881553)
- [x] [936105](https://bugzilla.mozilla.org/show_bug.cgi?id=936105)
- [x] [936304](https://bugzilla.mozilla.org/show_bug.cgi?id=936304)
- [x] [986005](https://bugzilla.mozilla.org/show_bug.cgi?id=986005)
- [x] [986014](https://bugzilla.mozilla.org/show_bug.cgi?id=986014)
- [x] [986019](https://bugzilla.mozilla.org/show_bug.cgi?id=986019)
- [x] [1155279](https://bugzilla.mozilla.org/show_bug.cgi?id=1155279)

I plan to identify every certificate that was removed by these purges and check whether any of them are in the current requests cert bundle. If they are, I plan to restore them to the new certifi bundle.

Does anyone think this plan is bad? @reaperhulk @dstufft @alex @sigmavirus24
",Lukasa,alex
2455,2015-04-23 18:01:11,"Alright, fuck that. Here's my new plan.

The following are the Mozilla issues that track the removal of 1024 bit certs that I could find:
- [X] [856718](https://bugzilla.mozilla.org/show_bug.cgi?id=856718)
- [x] [881553](https://bugzilla.mozilla.org/show_bug.cgi?id=881553)
- [x] [936105](https://bugzilla.mozilla.org/show_bug.cgi?id=936105)
- [x] [936304](https://bugzilla.mozilla.org/show_bug.cgi?id=936304)
- [x] [986005](https://bugzilla.mozilla.org/show_bug.cgi?id=986005)
- [x] [986014](https://bugzilla.mozilla.org/show_bug.cgi?id=986014)
- [x] [986019](https://bugzilla.mozilla.org/show_bug.cgi?id=986019)
- [x] [1155279](https://bugzilla.mozilla.org/show_bug.cgi?id=1155279)

I plan to identify every certificate that was removed by these purges and check whether any of them are in the current requests cert bundle. If they are, I plan to restore them to the new certifi bundle.

Does anyone think this plan is bad? @reaperhulk @dstufft @alex @sigmavirus24
",Lukasa,dstufft
2455,2015-04-23 18:42:28,"(Oh, /cc @t-8ch as well).
",Lukasa,t-8ch
2454,2015-02-22 12:40:37,"@RuudBurger The certificate chain is sent with each SSL handshake, which is performed for each new connection. If you use a `requests` Session it will use a connectionpool, which reuses connections for each host.
As you are talking to the same host only one handshake will be performed.
",t-8ch,RuudBurger
2454,2015-02-22 12:44:31,"@Lukasa Instead of trusting the intermediate, would it not be easier then to use a single self-signed cert and thrust it directly or just pin the fingerprint?
",t-8ch,Lukasa
2454,2015-02-22 12:46:43,"@RuudBurger I assume the 7kb you measured are the PEM-encoded files. This will be less when transmitted over the wire.
",t-8ch,RuudBurger
2450,2015-02-20 02:02:07,"Thanks @iKevinY !
",sigmavirus24,iKevinY
2449,2015-03-15 10:31:39,"@Lukasa Since it's in urllib3 now, how we can take advantage of it?
",TomasTomecek,Lukasa
2448,2015-02-17 23:27:55,"So I think the fundamental problem here is that neither requests nor urllib3 have any way of knowing what's happening with the actually underlying TCP connection. urllib3 at its best wraps sockets to enable TLS but doesn't really directly manage sockets beyond that very thoroughly.

It isn't as if urllib3 or requests is acknowledging the FIN that it receives. That's generally how the socket behaves without us having to do that. I think a way around this is to enable TCP Keep-Alive but I'm not entirely convinced this is a solution that needs to live in requests proper.

I think @Lukasa is on vacation or just generally taking a break so I'll have to look into this more tonight. Thanks for the very very detailed issue @pddenhar!
",sigmavirus24,pddenhar
2448,2015-02-17 23:29:11,"BTW, @pddenhar I'm also in MSN. =D
",sigmavirus24,pddenhar
2448,2015-02-18 03:35:59,"@pddenhar I wonder if the `TCPKeepAliveAdapter` in [this PR](https://github.com/sigmavirus24/requests-toolbelt/pull/59) helps any with your problem.
",sigmavirus24,pddenhar
2447,2015-02-15 15:53:00,"Hey @hoodja I'm glad you like the library.

We've had several (suggestions|feature requests) to add various different (and usually of little popularity) keyword arguments to `HTTPAdapter.__init__`. We've been consistently resistant because the ideology of the project author and its maintainers is to serve the 95% (or greater) use case. I personally added the ability to configure socket options to urllib3 and you're the third person I've ever found who has needed to set socket options through requests.  So we're certainly not likely to add a socket options keyword argument to `HTTPAdapter.__init__`. The [toolbelt](/sigmavirus24/requests-toolbelt) already has one or two adapters for less common but non-trivial use-cases and we can discuss there whether a `CustomSocketOptionsAdapter` would fit your needs there and what that API would look like.

Cheers!
Ian
",sigmavirus24,hoodja
2447,2015-02-18 03:35:24,"Hey @hoodja I added both a socket options adapter to the toolbelt and a TCP Keep-Alive adapter: https://github.com/sigmavirus24/requests-toolbelt/pull/59
",sigmavirus24,hoodja
2447,2015-02-19 19:18:49,"Thanks Ian! I'll definitely give that a spin.

On Tue, Feb 17, 2015 at 9:35 PM, Ian Cordasco notifications@github.com
wrote:

> Hey @hoodja https://github.com/hoodja I added both a socket options
> adapter to the toolbelt and a TCP Keep-Alive adapter:
> sigmavirus24/requests-toolbelt#59
> https://github.com/sigmavirus24/requests-toolbelt/pull/59
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/2447#issuecomment-74805834
> .
",hoodja,hoodja
2446,2015-10-14 20:04:25,"@Lukasa:
I'm wondering if it's possible to use Requests to decode a gzip-encoded file (e.g. not-for-transport), or would that not be in the spirit of the library (file provenance and all)?

I've got a large number of gzipped text file to download then parse (that would be amazing to just stream), and if it was the server doing the encoding, it seems like Requests could handle it no problem, but if the file's already in that form, would it work?
",riordan,Lukasa
2446,2015-10-14 20:26:01,"@riordan we will not decode something that does not have a gzip (or compress) Content-Encoding. The gzip module in Python should do this for you though and it might be possible to create that with `response.raw` as a file object to sort of stream i.
",sigmavirus24,riordan
2446,2015-10-16 17:40:03,"@sigmavirus24 @Lukasa This is _awesome_ thank you. That's exactly what I wound up doing.
",riordan,Lukasa
2446,2015-10-16 17:40:03,"@sigmavirus24 @Lukasa This is _awesome_ thank you. That's exactly what I wound up doing.
",riordan,sigmavirus24
2444,2015-02-11 19:07:03,"@sigmavirus24 sounds good- I updated to HEAD, but feel free to drop this PR in favor of another one if this is in flight elsewhere.
",Yasumoto,sigmavirus24
2444,2015-02-24 15:27:40,"Thanks @sigmavirus24 !
",Yasumoto,sigmavirus24
2441,2015-02-08 15:32:45,"Thanks for the PR @andreif!

I'm mostly -0.5 on this. I think the right approach is to fix the documentation, since that's clearly what's lacking here (in fact, I could find no examples of using the json parameter actually, and I take full blame for that).

That said, the API for multipart requests seems confusing enough that I think some really good documentation around the `data`, `json`, and `files` parameters is really necessary.

Further, I think if we are going to raise an exception here, it should be a `ValueError`. `NotImplementedError` is usually used for abstract methods that need to be overridden by a subclass. That's not what we expect people to do here. What actually happens here, the problem is that we technically received to many values and can't possibly know what the users want us to do.
",sigmavirus24,andreif
2441,2015-02-08 16:05:46,"@sigmavirus24 Well, I wonder if we could take both files and json. It's a common case to upload files with json meta data, so maybe it makes sense to have a nice api for this.
",andreif,sigmavirus24
2441,2015-02-08 16:24:19,"@andreif That's a good idea, but sadly the API doesn't render out well. For instance, what happens if instead of uploading one file you're uploading two? And how do you associate them together?

Sadly, JSON in mutlipart/form-data messages is not standardised in any sense, so there's no way we can extend our API for that to be consistently useful. In that situation I'd want to resist the temptation to guess.

However, I'm +0 on throwing exceptions when mutually exclusive parameters are passed, because I don't really think we should fail silently. That said, it's backwards incompatible, and so we need to hold it for at least a minor release.
",Lukasa,andreif
2441,2015-02-08 16:50:09,"@Lukasa Alright. Should I change to `ValueError` in this case?
",andreif,Lukasa
2441,2015-02-10 14:11:19,"@andreif please do change it to a ValueError
",sigmavirus24,andreif
2441,2015-02-10 14:17:43,"@sigmavirus24 I took existing raise as an example. Should I change all of them to `ValueError` or should I keep the original as it is?


",andreif,sigmavirus24
2439,2015-02-05 08:58:59,"@zaitcev The closing statement was: ""Given that this is mostly in urllib3 and would rely on acceptance there, **I'm closing this until progress has been made there**."" (Emphasis mine.)
",Lukasa,zaitcev
2439,2015-02-06 00:19:34,"@zaitcev `CERT_NONE`, `CERT_REQUIRED` and `CERT_OPTIONAL` are all defined on the `ssl` module and have a real meaning in the context of performing validation. `CERT_EXPLICITLY_NONE` means absolutely nothing in the same context.
",sigmavirus24,zaitcev
2434,2015-02-02 19:57:56,"@shazow Maybe not?  Looking at the code in `urllib3.response`, `self._fp` is set from `body`, which could be a string type instead of a file pointer.

...oh wait, that's a lie, it couldn't be a string type in that case, but it only checks for a `read` method so it might not have `fileno()`, I guess.
",larsks,shazow
2431,2016-04-06 19:25:35,"This PR hasn't seen any love in a long time. @ianepperson, would mind performing a rebase to make this once-again mergable? 

@sigmavirus24 @Lukasa +1? -1? I want to get this merged or closed out. 
",kennethreitz,sigmavirus24
2431,2016-04-06 19:25:35,"This PR hasn't seen any love in a long time. @ianepperson, would mind performing a rebase to make this once-again mergable? 

@sigmavirus24 @Lukasa +1? -1? I want to get this merged or closed out. 
",kennethreitz,ianepperson
2429,2015-01-28 14:59:10,"Added you @Lukasa and added a bit more detail about vulnerability reporting.
",sigmavirus24,Lukasa
2429,2015-03-04 13:51:18,"@Lukasa ping ;)
",sigmavirus24,Lukasa
2429,2015-03-04 14:03:30,"@sigmavirus24 Is it worth adding GPG key fingerprints for you and I?
",Lukasa,sigmavirus24
2427,2015-01-26 05:54:36,"I haven't read this yet, and I'm certain it really is this simple, but I want to re read the RFC about this regardless to make sure this applies to all cases. I suppose we've never had trouble with this because people were using URIs that had paths when authenticating. It doesn't hurt to be safe though. =)

Thanks for your patience @luozhaoyu 
",sigmavirus24,luozhaoyu
2427,2015-01-26 07:00:17,"@sigmavirus24 Sure! It is rare because I could only reproduce it when use a proxy and encounter a redirect...
",luozhaoyu,sigmavirus24
2427,2015-04-06 03:32:08,"I'm comfortable with this. Sorry for the delay @luozhaoyu. I lost track of this.

Thoughts @Lukasa?
",sigmavirus24,luozhaoyu
2427,2015-04-06 15:19:21,"Thanks @luozhaoyu 
",sigmavirus24,luozhaoyu
2427,2015-04-06 15:27:07,"@sigmavirus24 You're welcome!
",luozhaoyu,sigmavirus24
2425,2016-10-23 18:34:42,"@loretoparisi make sure you have the latest version of `requests` and also install `PySocks`. here is an example of using Tor (via a socks5 proxy) with requests: https://github.com/AlJohri/python-tor-examples
",AlJohri,loretoparisi
2424,2015-01-25 18:11:17,"> I think the reality is that if this module enters the standard library the current core team will move on from it. I certainly have little interest following it into the quagmire that is core dev. The most likely to steward requests in the stdlib is @sigmavirus24, and he's just one man. That loss of direction will inevitably lead to an erosion of the library's interface over time, and I think that would be a tragic thing.

I would wander into the stdlib to try to help, but given the fact that exactly one of I don't know how many previous patchsets I've submitted has been accepted and one other _reviewed_ makes me wary of wanting to bother with that process. I know the core devs are entirely swamped by more important things. I also know someone else has decided randomly that they want to maintain httplib/http but they're clearly not suited for the job (yet) and I don't have the patience to work on httplib when patches that both @Lukasa and I sit around, unreviewed, and not cared about (when they fix pressing issues with the library).

I'd probably end up just forking requests to continue using it.

> requests is absolutely unsuitable for stdlib inclusion for the many reasons stated before me. The urllib3 dependency alone is a complete showstopper; we don’t want it to got to die in the stdlib.

It's always been a contention of @kennethreitz (and therefore, the project as a whole) that urllib3 is an implementation detail. Many of requests' biggest features are handled entirely by urllib3, but it doesn't mean they couldn't be reimplemented with care into truly dependency-less library.

Regarding the chardet dependency: it's been nothing but a headache to us (and to me specifically). It used to have separate codebases for py2 and py3 until I got it into a single codebase library (which has only in the last several months been merged back into chardet proper). The library is slow and a huge memory hog (which angers many people to the point of yelling at us here on the issue tracker). It's not entirely accurate and Mozilla's universalchardet that it is modeled after has all but been abandoned by Mozilla. So removing chardet would probably be a net positive anyway.

Regarding whether we should do this or not, I'm frankly unconcerned. Whatever would be in the stdlib would end up being requests in API only. The Python 3 adoption rate is slow enough that I don't think people will be meaningfully affected by this for the next N years (where N is the globally unknown number of years for 3.5 to be used in production by corporations).

And like I said, I'd probably end up just forking requests or using urllib3 directly at that point.
",sigmavirus24,kennethreitz
2424,2015-01-25 18:11:17,"> I think the reality is that if this module enters the standard library the current core team will move on from it. I certainly have little interest following it into the quagmire that is core dev. The most likely to steward requests in the stdlib is @sigmavirus24, and he's just one man. That loss of direction will inevitably lead to an erosion of the library's interface over time, and I think that would be a tragic thing.

I would wander into the stdlib to try to help, but given the fact that exactly one of I don't know how many previous patchsets I've submitted has been accepted and one other _reviewed_ makes me wary of wanting to bother with that process. I know the core devs are entirely swamped by more important things. I also know someone else has decided randomly that they want to maintain httplib/http but they're clearly not suited for the job (yet) and I don't have the patience to work on httplib when patches that both @Lukasa and I sit around, unreviewed, and not cared about (when they fix pressing issues with the library).

I'd probably end up just forking requests to continue using it.

> requests is absolutely unsuitable for stdlib inclusion for the many reasons stated before me. The urllib3 dependency alone is a complete showstopper; we don’t want it to got to die in the stdlib.

It's always been a contention of @kennethreitz (and therefore, the project as a whole) that urllib3 is an implementation detail. Many of requests' biggest features are handled entirely by urllib3, but it doesn't mean they couldn't be reimplemented with care into truly dependency-less library.

Regarding the chardet dependency: it's been nothing but a headache to us (and to me specifically). It used to have separate codebases for py2 and py3 until I got it into a single codebase library (which has only in the last several months been merged back into chardet proper). The library is slow and a huge memory hog (which angers many people to the point of yelling at us here on the issue tracker). It's not entirely accurate and Mozilla's universalchardet that it is modeled after has all but been abandoned by Mozilla. So removing chardet would probably be a net positive anyway.

Regarding whether we should do this or not, I'm frankly unconcerned. Whatever would be in the stdlib would end up being requests in API only. The Python 3 adoption rate is slow enough that I don't think people will be meaningfully affected by this for the next N years (where N is the globally unknown number of years for 3.5 to be used in production by corporations).

And like I said, I'd probably end up just forking requests or using urllib3 directly at that point.
",sigmavirus24,Lukasa
2424,2015-01-25 18:14:13,"I discussed this at length with Guido the other day — chardet would have to be included first. I think that urllib3 and requests could be included into the http package together. 

However, I'm very inclined to agree with @hynek and @dstufft. Perhaps requests is fine just the way it is :)
",kennethreitz,hynek
2424,2015-01-25 18:14:13,"I discussed this at length with Guido the other day — chardet would have to be included first. I think that urllib3 and requests could be included into the http package together. 

However, I'm very inclined to agree with @hynek and @dstufft. Perhaps requests is fine just the way it is :)
",kennethreitz,dstufft
2424,2015-01-25 21:51:04,"I too have spoken with Guido about tossing urllib3 into the stdlib some years ago with the conclusion that it's not a great idea, but I'm fairly neutral about it at this point.

urllib3's API has been mostly-stable and pretty much completely backwards compatible for several years now. Its' pace is possibly even slower than that of the stdlib today, with the vast majority of changes being minor fixes or security improvements (with occasional backwards-compatible feature additions like granular timeouts/retries). If somebody really wanted to try and get urllib3 into the standard library, I don't think it's a terrible idea—it's just not the _best_ idea.

(I'm not speaking for requests, as it moves at a very different pace with different goals than urllib3.)

The best idea, in my opinion, would be for the PSF to hire (or maybe Kickstart or something) 1-3 developers to build out a brand new http library on top of asyncio with HTTP/2 support with heavy inspiration from urllib3, requests, and hyper. I'd be happy to see as much code taken verbatim as possible but laid out in a consistent, modular, and reusable manner. Ideally target Python 4 or something, and get rid of all the urllibs and httplibs. I expect this would be 6-9mo of hard work, but possibly more.

The very worst part about urllib3, which I'd love to see replaced if somebody attempts to rewrite it per @sigmavirus24's suggestion, is that it depends on httplib. urllib3's functionality is substantially limited with lots of code spent working around shortcomings of httplib. Though if HTTP/2 support would be taken seriously in this goal, then the scope of re-implementing HTTP/1.1 would be a very comforting fraction of the work required.

Many PyCons ago, a bunch of us met up and whiteboarded a layout of a brand new http library that refactors all the pieces into the ideal arrangement we could imagine at the time. I'd be happy to dig up these notes if anyone is going to attempt this.
",shazow,sigmavirus24
2424,2015-01-25 23:12:20,"+1 @shazow

Again, if anyone finds themselves with the time and inclination to take on that fairly large project, I've sketched out a putative API design that might make a good starting point.
",Lukasa,shazow
2424,2015-01-26 02:00:17,"@dstufft this is in projects that generally don't, where everyone can't be bothered to figure out how to use urllib. (people aren't adding it as a dep because of ssl/etc generally, but out of laziness)
",dcramer,dstufft
2424,2015-01-26 02:01:13,"@dstufft also multi-version deps basically make it hard to use things in libraries. You probably want to use requests in your project and if we require it then there's a potential for a world of hurt if API changes happen in versions.
",dcramer,dstufft
2424,2015-01-26 02:38:44,"@sigmavirus24 I disagree. requests has had its API change in the past. APIs change, thats why we have versioning, thats why dependencies are complex. This is a perfect case for that discussion because requests is a dependency in a lot of projects.

If you move into the stdlib the API must be stable.
",dcramer,sigmavirus24
2424,2015-01-26 02:51:26,"@dcramer the API broke exactly once, in 1.0. APIs do change, but requests' API isn't planning any changes either. The only change we've had is adding the `json` parameter which doesn't break anything. You can keep trying to accuse us of breaking the API too much, but when projects like OpenStack have had requirements defined as `>=1.2.3` for a long time, I think that says a lot about the stability of requests. The API has been stable, precisely because after we cut 1.0 we rejected all new additions to the API (with the obvious recent exception of adding a `json` param) and we've been very strict about not breaking the API. If you're not a consumer of requests, you wouldn't know this though. So I don't take your ignorance personally.
",sigmavirus24,dcramer
2424,2015-01-26 03:02:46,"@sigmavirus24 you're now purely turning this into an API debate. I was just pointing out the reason I dont include it is because it can change, and everyone uses it, and everyone uses different versions. It's great that you guys never change your API but I have no desire or time to follow it or assume thats true.
",dcramer,sigmavirus24
2424,2015-01-26 04:25:28,"> @dcramer 
> Not sure what you're all bitching about.

Very appropriate language for this debate. When legitimate counters are made to your position, you use a slur meant to demean women. Par for the course though. Moving on,

---

@ncoghlan 

Re point 1: I think the documentation would be drastically simplified with requests (requests-alike) in the stdlib. One of the first things I do when learning a new language is figure out how do HTTP. Having that featured is something the guide would benefit from regardless.

Re point 2: There's a difference between the API and the library being de facto vs. de jure. The API could easily be provided by the standard library. I think your concern about support would be more aimed towards requests (the code) being included.

Re points 3 & 4: I'm not sure that's something to be discussed here. Maybe python-ideas would be better.

> As far as the idea of PSF managed development work goes, I'd be heavily against that, as we don't have the management infrastructure in place to handle that kind of thing.

That's interesting. I didn't think it was a probability but it'd be great to have something better than http(lib). 

> With the standard library, we generally don't have that concern - while redistributors occasionally break things, in a commercial context, vendors breaking stuff that works upstream gives you quite a lot of leverage to get the offending vendor to fix things.

I'm not sure what leverage you're talking about. I've seen ensurepip, venv, and other things broken by Debian and other redistributors in CPython. That's tangential to this discussion though.

> Oh, one other key question to answer before volunteering to actually maintain stuff in the standard library: are you prepared to accept the responsibility of shipping software that helps power half the world's stock exchanges, is one of the most popular languages for corporate infrastructure, one of the most popular languages for scientific programming (including being used for trajectory planning on inter-planetary space missions), one of the most popular languages for web development, and one of the key languages being employed in new computer literacy initiatives in educational institutions?

As already mentioned, most of the people currently involved wouldn't continue with the project. I'd probably be the only person but given my track record with upstream CPython development, it wouldn't be productive leaving that burden to the existing (and other future) core developers.

> That post-chasm portion of the technology adoption curve are the ones we reach when we say ""yes, this approach is now sufficiently mature that we can push it, or something based on it, into the standard library to help make it a truly universal assumption"".

The reality is that those people will never catch up though, no? People are still running software on Python 2.4 and 2.5. F5's load balancers still only support Python 2.5. 2.7 will be in use probably until the end of my natural life (which I hope will be quite long). Are these really the people this decision will affect most strongly? Those same people you describe may never make the leap to Python 3. And currently, it still is a _leap_. Maybe by the time they've decided to consider it, Python 3.8 or 3.9 or 4.2 will be out and will be much less of a hassle for them.
",sigmavirus24,dcramer
2424,2015-01-26 04:25:28,"> @dcramer 
> Not sure what you're all bitching about.

Very appropriate language for this debate. When legitimate counters are made to your position, you use a slur meant to demean women. Par for the course though. Moving on,

---

@ncoghlan 

Re point 1: I think the documentation would be drastically simplified with requests (requests-alike) in the stdlib. One of the first things I do when learning a new language is figure out how do HTTP. Having that featured is something the guide would benefit from regardless.

Re point 2: There's a difference between the API and the library being de facto vs. de jure. The API could easily be provided by the standard library. I think your concern about support would be more aimed towards requests (the code) being included.

Re points 3 & 4: I'm not sure that's something to be discussed here. Maybe python-ideas would be better.

> As far as the idea of PSF managed development work goes, I'd be heavily against that, as we don't have the management infrastructure in place to handle that kind of thing.

That's interesting. I didn't think it was a probability but it'd be great to have something better than http(lib). 

> With the standard library, we generally don't have that concern - while redistributors occasionally break things, in a commercial context, vendors breaking stuff that works upstream gives you quite a lot of leverage to get the offending vendor to fix things.

I'm not sure what leverage you're talking about. I've seen ensurepip, venv, and other things broken by Debian and other redistributors in CPython. That's tangential to this discussion though.

> Oh, one other key question to answer before volunteering to actually maintain stuff in the standard library: are you prepared to accept the responsibility of shipping software that helps power half the world's stock exchanges, is one of the most popular languages for corporate infrastructure, one of the most popular languages for scientific programming (including being used for trajectory planning on inter-planetary space missions), one of the most popular languages for web development, and one of the key languages being employed in new computer literacy initiatives in educational institutions?

As already mentioned, most of the people currently involved wouldn't continue with the project. I'd probably be the only person but given my track record with upstream CPython development, it wouldn't be productive leaving that burden to the existing (and other future) core developers.

> That post-chasm portion of the technology adoption curve are the ones we reach when we say ""yes, this approach is now sufficiently mature that we can push it, or something based on it, into the standard library to help make it a truly universal assumption"".

The reality is that those people will never catch up though, no? People are still running software on Python 2.4 and 2.5. F5's load balancers still only support Python 2.5. 2.7 will be in use probably until the end of my natural life (which I hope will be quite long). Are these really the people this decision will affect most strongly? Those same people you describe may never make the leap to Python 3. And currently, it still is a _leap_. Maybe by the time they've decided to consider it, Python 3.8 or 3.9 or 4.2 will be out and will be much less of a hassle for them.
",sigmavirus24,ncoghlan
2424,2015-01-26 16:29:30,"@dcramer thank you for giving us your time — it's really appreciated :heart:
",kennethreitz,dcramer
2424,2015-01-26 16:33:15,"@sigmavirus24 all is well :)
",kennethreitz,sigmavirus24
2423,2015-01-24 14:56:38,"Yea what @sigmavirus24 said. Some systems unbundle requests from pip.
",dstufft,sigmavirus24
2423,2015-01-24 14:59:55,"@dstufft is this maybe related to debian preventing pip from removing packages installed by the system package manager?
",sigmavirus24,dstufft
2422,2015-03-23 00:32:47,"@barseghyanartur perhaps you can give us more details than @cloverstd did, for example, what code will reproduce this?

I have a suspicion as to the cause but I haven't been able to reproduce this yet.
",sigmavirus24,cloverstd
2422,2015-03-23 00:32:47,"@barseghyanartur perhaps you can give us more details than @cloverstd did, for example, what code will reproduce this?

I have a suspicion as to the cause but I haven't been able to reproduce this yet.
",sigmavirus24,barseghyanartur
2422,2015-03-23 01:10:40,"@sigmavirus24 

I have gone a bit further in research and this is what I have found.

Basically, it's this:



Environment details:
- Python==2.7.3
- requests==2.5.1

I managed to reproduce the problem exactly. In my case, the endpoint is a multi-lingual Django site. Requests to URL without the language prefix (like ""http://example.com/foo/endpoint/"") do get redirected to the same URL with the language prefix (like ""http://example.com/en/foo/endpoint/"").

If `some_url` is ""http://example.com/foo/endpoint/"" I get an error:

('Connection aborted.', error(32, 'Broken pipe'))

If `some_url` is ""http://example.com/en/foo/endpoint/"" it all goes fine. Likely, `requests` doesn't handle large post requests to endpoints which do redirect them to some other endpoints.

I have just tried it with requests==2.6.0 and I do get the same problem.
",barseghyanartur,sigmavirus24
2422,2015-03-23 01:18:52,"@barseghyanartur can you give us the status code from the redirect?
",sigmavirus24,barseghyanartur
2422,2015-03-23 14:39:35,"@Lukasa or if we change the method (i.e., in a case like 7231 Section 6.6.4) without stripping the body/content-length. (or strip the body and not the content-length).
",sigmavirus24,Lukasa
2422,2015-03-23 14:39:56,"Really any information @barseghyanartur can give us about the redirect response and the subsequent request will be helpful
",sigmavirus24,barseghyanartur
2422,2015-03-23 15:03:32,"@sigmavirus24 

Hey, thanks for getting back on this. I'll post it here quite soon after I get home. :) That's gonna be in about 5 hours.
",barseghyanartur,sigmavirus24
2422,2015-03-23 15:14:37,"@barseghyanartur no rush. I'd like to fix this once and for all. That can be today or next week. As long as it's fixed, I'll be happy.
",sigmavirus24,barseghyanartur
2422,2015-03-24 22:13:19,"@sigmavirus24 

I don't get any response back, since it fails hard.



And this is the code:


",barseghyanartur,sigmavirus24
2422,2015-03-25 16:52:07,"@barseghyanartur you can determine what the redirect codes, etc. are with `requests.post(url, data=data, files=files, allow_redirects=False)`
",sigmavirus24,barseghyanartur
2422,2015-03-25 16:58:14,"@sigmavirus24 

Ah, sorry, I didn't realise you were asking for that. It was 301, as far as I remember.
",barseghyanartur,sigmavirus24
2422,2015-04-07 23:31:47,"@jazzfan thanks for doing that. I'll look tonight and I suspect that @Lukasa will look when he wakes up
",sigmavirus24,Lukasa
2422,2015-04-08 00:24:36,"@sigmavirus24 You forget that I am in NYC at the minute, and so am (relatively) awake!

As I've said elsewhere, EPIPE almost always comes when we attempt to send on a connection that has been remote-closed.

I think it would be most interesting if you could run a patch in requests' vendored copy of urllib3 that would throw the socket object up with the `ConnectionError`, and then read from it. The exact patch to apply here is a bit unclear and overlaps with some other stuff we've been thinking about when it comes to EPIPE.

Alternatively, a non-SSL repro would work!
",Lukasa,sigmavirus24
2422,2015-04-08 00:39:46,"@Lukasa, does this mean that the SSL tcpdump file is essentially useless?  Also, I'm afraid I'm too much of a newbie to be able to figure this patch you're describing, especially if it's unclear to you.  

I guess I'll rig up some kind of little test HTTP server on my localhost and see if I can reproduce the error using that.
",JazzFan,Lukasa
2422,2015-04-08 00:56:07,"@JazzFan Sadly, it's not very easy to tell from it whether or not a response was sent. That said, we can definitely see that the remote end tore the connection down at 15:17:24.141410 (sending a TCP FIN). Shortly thereafter we emitted another packet, which caused the remote end to send a barrage of TCP RSTs (though most of these appear to be retransmits, and in practice appear to be in response to the other TCP packets that were in flight when the FIN was emitted (as shown by the sequence/acknowledgement numbers).

What we really want to understand is what triggered the connection teardown.
",Lukasa,JazzFan
2422,2015-04-08 06:50:31,"@Lukasa 

Well, one thing is clear - it happens only when large amounts of data (files) and transferred.
",barseghyanartur,Lukasa
2422,2015-04-08 12:17:13,"@barseghyanartur Yup, but that doesn't necessarily mean much. You can only get an EPIPE if the remote end forcibly closes the connection, which is only likely to happen for large data files.
",Lukasa,barseghyanartur
2422,2015-04-08 12:24:10,"@Lukasa 

Are you interested to know what happens if 20 Mb plain text (textarea) is posted?
",barseghyanartur,Lukasa
2422,2015-04-09 11:04:21,"@JazzFan In addition to what you sent me I need the actual packet data. There's a 40-byte TCP payload coming from your server in frame 60 that Wireshark is claiming is HTTP data, but it hasn't broken out the data itself so I can't see it.

What tcpdump command are you running?
",Lukasa,JazzFan
2422,2015-04-09 16:08:11,"Oops, I forgot to include @Lukasa in my previous comments.
",JazzFan,Lukasa
2422,2015-04-10 20:19:58,"@JazzFan I haven't tested this yet (I'm at PyCon so I'm short on time), but my suspicion is that your test server causes this by immediately writing a response and then going out of scope without draining the POST data. This means that the server will attempt to close the socket, sending a TCP FIN, and then causing the Broken Pipe error.
",Lukasa,JazzFan
2422,2015-04-11 02:09:18,"@Lukasa, please don't let me take you away from PyCon!

I think you are right about needing to drain the POST data.  When I change my test server's `do_POST` method from this:



to this, by adding code to read all of the input stream for this post request:



then the error goes away; `requests` does not terminate in a stack trace.  

So based on my evidence, when `requests` abnormally terminates, it is due to a problem in the server implementation, not in `requests` itself.  Maybe other people have evidence that it's a problem in `requests`, but on the face of it, I don't.   Sorry to have troubled you!
",JazzFan,Lukasa
2422,2015-04-11 02:58:50,"No problem @JazzFan! I think we still would like to surface responses in this case, but it's another issue. 
",Lukasa,JazzFan
2422,2015-07-30 15:57:04,"@Lukasa I am seeing a similar issue, but the cause is from nginx immediately returning a `413 Request Entity Too Large`. RFC2616 says this about 413:

> The server _MAY close the connection_ to prevent the client from continuing the request. 

The server closing the connection is the behaviour I am seeing, and requests (actually, I think the issue is in Python's `httplib`) is not handling that case.

I have a server on EC2 that can be used for testing against. nginx is configured with a `max_client_body_size` of `1k`, and the Python server behind nginx is the example posted above in this issue (with the modification to consume the whole body). The catch is that nginx doesn't proxy the request to the Python server due to the `413` error. You can POST to `http://52.18.181.108/`.
In my case, when uploading a file larger than roughly 75MB I see this issue.

Using wireshark, I see the below behaviour, where nginx responds upfront with a 413, but Python httplib doesn't handle the response until the very end of the request (happily uploading the whole file while the server has already rejected the request). If the file is large enough, nginx will terminate the connection before httplib/urllib3 handles the response, resulting in either a `Broken Pipe`, or a `Connection reset by peer`.



cc @cournape
",sjagoe,Lukasa
2422,2016-02-15 00:43:30,"@sjagoe Thanks for this info! 

> Using wireshark, I see the below behaviour, where nginx responds upfront with a 413, but Python httplib doesn't handle the response until the very end of the request (happily uploading the whole file while the server has already rejected the request). If the file is large enough, nginx will terminate the connection before httplib/urllib3 handles the response, resulting in either a `Broken Pipe`, or a `Connection reset by peer`.

This indeed seem to be the case. 

I have the same exact trouble for a completely different server (VirusTotal). The easy way to check this behavior is by creating a file slightly bigger than 64Mib and another slightly smaller with:



Submitting this gives the `(104, 'ECONNRESET')` exception, indicating that the foreign server dropped the connection. (This happens fast.)

The smaller file was created with:



Indeed results in the correct server response of `413`, exceeding the 32 MB API limit. (This process is slow.)

What is weird is the speed of the returned result, which is opposite of what is expected. 
",E3V3A,sjagoe
2422,2016-06-29 10:48:49,"@chenziliang There is not at this time.
",Lukasa,chenziliang
2422,2016-06-29 13:56:19,"Thanks @Lukasa ! May i ask if there is workaround ? I didn't run into broken pipe issue by using httplib2 by issuing the same request with the same payload.
",chenziliang,Lukasa
2422,2016-06-29 14:09:29,"@chenziliang Are you sure they were the same? httplib2 and requests handle connections differently and set different headers, so it would be worth using something like Wireshark to confirm that the behaviours really are the same.
",Lukasa,chenziliang
2422,2016-11-04 08:29:32,"@huntc The ultimate problem is that Requests currently has no way to detect that a response has arrived and abort processing of the request. httplib just doesn't let us in at that low a level. The only fix to your problem is to remove httplib from the equation, and while that's a thing we're working on it represents a really pretty major change to urllib3 and to Requests. It'll take a while to happen.
",Lukasa,huntc
2422,2016-11-30 09:12:03,"@DavidMcLaughlin We know it's a problem, we know it affects lots of people, and we know that the amount of work that is required to fix it is really *very* substantial. There is no amount of reprioritisation that will get this moved up the list because the work required is O(months) of time. I am currently working on it in and amongst my many other maintenance duties.

As I noted in my above comment, I warned that tickets like this are unlikely to be of much value because of a hoard of ""me too"" comments. Now that they have begun, I am immediately locking this issue to further discussion.",Lukasa,DavidMcLaughlin
2417,2015-01-20 18:08:44,"@Lukasa : I am following a well known practice that is detailed in Idiomatic Python author:
http://www.jeffknupp.com/blog/2013/11/15/supercharge-your-python-developers/
Also detailed in Google coding standards.
FWIW:  pylint has caught many errors in my code and others in the past.
While we may differ in our views, not placing value judgements.  Just a data point.
Like I said in my prior post, my intent is to weight differing approaches.  Request at the top of that list.

Was hoping that this kind ask would be more open to possible changes... looks like closed for discussion already.

If you guys are open, I'd be open to making changes once I had more time.
But then that would be un-welcomed based on your statement too.
my 2 cents
",DavidHwu,Lukasa
2417,2015-01-23 13:53:41,"@Lukasa 
Are you open to annotating source with comments disabling pylint warnings? Given that one [can](http://docs.pylint.org/faq.html#do-i-have-to-remember-all-these-numbers) use symbolic names of warnings/errors such annotation would serve as documentation at the same time which should be useful.
",piotr-dobrogost,Lukasa
2417,2015-01-23 13:57:03,"@piotr-dobrogost Nope. =)

My concern is the potentially unbounded number of linters that we may be asked to support annotations or dotfiles for. Just because someone created a tool that they love doesn't mean we should add metadata to our codebase to support it. If the requests project uses a tool for its development, we'll add that metadata. Otherwise, we won't.
",Lukasa,piotr-dobrogost
2417,2015-01-23 14:46:51,"@piotr-dobrogost Are you open to determining how many of those are false positives and constitute absolutely useless feedback from a linter? The majority of the errors are just plain **wrong** and any changes belong in pylint.
",sigmavirus24,piotr-dobrogost
2416,2015-01-20 14:56:05,"Apologies @sigmavirus24, definitely short on info:
- Redirect is append-slash like (`dev.local/api/test` -> `dev.local/api/test/`)
- Both served over HTTP
- Cookie is set in the headers dict passed to `requests.get`, no jar/session

Let me know if you need anything else.
",Fizzadar,sigmavirus24
2413,2015-01-19 13:35:11,"@Lukasa This is what I get for working on this way too late last night.

@arthurdarcet thanks. Done.
",sigmavirus24,Lukasa
2413,2015-01-21 02:47:17,"@Lukasa fwiw, I edited the commit message on my commit and retitled the PR to be more accurate descriptions of what's happening. The contents of the PR didn't change though.
",sigmavirus24,Lukasa
2411,2015-01-18 19:34:53,"@sjagoe I have a question: if you provide us a unicode filename, what encoding should we render that with when transmitting that over the wire?
",Lukasa,sjagoe
2411,2015-01-18 19:40:09,"@Lukasa Admittedly, that is an issue. There is no way for you to know. I guess in the past is just encoded to `ascii`?

In which case, this issue is still two things: 
- v2.5.1 broke compatibility with v2.5.0.
- I have just verified that under Python 3, if the filename _is_ encoded beforehand, then the original issue I reported under Python 2 rears its head. That is, requests does not send the filename and my server sees the filename simply as `'file'`, instead of the original.

EDIT: To clarify, under Python 3, a Unicode filename is _required_ (any encoded filename, e.g. `ascii`, `utf-8`, etc, is not sent to the server) and under Python 2, and encoded filename is _required_.
",sjagoe,Lukasa
2411,2015-04-11 14:45:36,"@abbeycode Can you run the following snippet for me please?


",Lukasa,abbeycode
2411,2015-04-11 14:50:44,"Yeah, suspect you're right @sigmavirus24. Note that this is a fairly widely-deployed standard, and the HTTPBis is working on bringing it up to date in the draft for [RFC 5987bis](https://tools.ietf.org/html/draft-reschke-rfc5987bis-07), so servers and frameworks that don't support it really should.
",Lukasa,sigmavirus24
2411,2015-04-11 15:12:01,"@Lukasa it seems that 5987bis is expired =( (Expires Jan 3, 2015)
",sigmavirus24,Lukasa
2411,2015-04-11 20:25:40,"@Lukasa this is what I get:


",abbeycode,Lukasa
2411,2015-04-11 21:56:52,"@Lukasa thanks for all the help, but I'm getting this:

> TypeError: Type str doesn't support the buffer API

on this line:



in `urllib3/fields.py` (line 34). This seems like a Python 3 issue, perhaps?
",abbeycode,Lukasa
2411,2015-04-11 22:02:14,"Oh wait, you mean from using @Lukasa's example using encode? Yeah. I think you're better bet is to annoy Slack until it works. I'll have to look into your existing problem first though.
",sigmavirus24,Lukasa
2411,2015-04-12 14:55:01,"I sent Slack an email describing what's happening, so hopefully they'll respond and fix the issues. Thanks @Lukasa and @sigmavirus24 for all the help!
",abbeycode,Lukasa
2411,2015-04-12 14:55:01,"I sent Slack an email describing what's happening, so hopefully they'll respond and fix the issues. Thanks @Lukasa and @sigmavirus24 for all the help!
",abbeycode,sigmavirus24
2409,2015-01-15 21:08:15,"Ugh, FFS, I've found it.

The problem is that we handle the redirect cache in `Session.send` with this logic:



However, this is well _after_ we've decided what the cookies should be (in `Session.prepare_request`). This means that the cookies are chosen based on the original URL, but the actual URL hit is potentially redirected dramatically.

This is a definite bug introduced by #2095. Apologies for my earlier comment @RossLote, you're quite right that this is a bug.
",Lukasa,RossLote
2409,2015-01-15 21:10:28,"@RossLote Until this gets fixed, feel free to use the workaround I posted above. It has no negative side-effects aside from occasionally slightly slowing your code down.
",Lukasa,RossLote
2409,2015-06-25 16:25:57,"@dpursehouse What's in the response history for each response?
",Lukasa,dpursehouse
2409,2015-06-26 02:02:37,"@Lukasa the history seems to be empty for both responses


",dpursehouse,Lukasa
2409,2015-06-26 08:03:53,"@dpursehouse That strongly suggests one of two things:
1. The connection is setting a cookie that it then does not expect to receive back. Unlikely.
2. The remote server doesn't like connection reuse. _Way_ more likely. Particularly if it's behind HAProxy, which can get all kinds of sad here.

Can you try mounting a HTTPAdapter with a poolsize of 1?


",Lukasa,dpursehouse
2409,2015-06-29 12:16:32,"@dpursehouse Yeah, that can work, though hopefully you'll be able to put it back with the above code and get the benefits of connection pooling.
",Lukasa,dpursehouse
2409,2015-06-30 12:25:53,"@Lukasa maybe we should add that to the toolbelt :)
",kennethreitz,Lukasa
2409,2015-06-30 13:02:51,"@kennethreitz Issue created. =) I'm leaving it on the vine as something that'd be a great ""first issue"" for someone. @sigmavirus24 may even have someone in mind he could pair with for it.
",Lukasa,kennethreitz
2409,2015-06-30 13:02:51,"@kennethreitz Issue created. =) I'm leaving it on the vine as something that'd be a great ""first issue"" for someone. @sigmavirus24 may even have someone in mind he could pair with for it.
",Lukasa,sigmavirus24
2408,2015-01-14 15:49:44,"@berndschultze Sorry, I was insufficiently clear. I agree. What I meant was pulling that line into the `auth` block _and_ changing it to always specifically use `auth`.
",Lukasa,berndschultze
2408,2015-01-14 17:16:00,"@Lukasa You are right, I misunderstood. Thanks again.
",berndschultze,Lukasa
2408,2015-01-20 00:52:07,"@berndschultze if you want to test out #2415 and let us know how that goes, that'd be awesome.
",sigmavirus24,berndschultze
2406,2015-01-13 09:00:11,"@ducu Thanks for doing that investigation, it'll be helpful (I don't have any machines with Redis installed atm).

While we're waiting on that I can infodump a few things. Firstly, requests has little-to-no global state. We certainly don't persist any of our own objects at the top-level, though I can't guarantee right now that none of the import machinery has anything to do with anything.

Garbage collection _should_ ensure that the socket you created initially is cleaned up because you aren't using a Session object. This is good, as it means that you shouldn't be accidentally re-using an invalid file handle (I think).

Unfortunately, right now I'm all out of ideas, so I'd like to see if we can get a consistent repro on multiple platforms to exclude single-platform weirdness before I dive in further. We may need to get some serious debugging tools onto this task.
",Lukasa,ducu
2406,2015-01-13 13:05:57,"@Lukasa Sure, I hope we can get to the bottom of this.

For now I'm running the gist on an AWS instance with ubuntu-trusty-14.04-amd64-server-20140927 (ami-f0b11187) via SSH. This means I cannot disable the network adapter (can't do `sudo ifconfig en0 down`), so I thought to use iptables to block outgoing http traffic (`sudo iptables -A OUTPUT -p tcp --dport 80 -j REJECT`) which is not exactly the same.. 

By doing this I cannot reproduce the issue, `requests` behaves properly -- it raises ConnectionError: ('Connection aborted.', error(111, 'Connection refused')) when traffic is rejected by the firewall, and recovers to running normally when the rule is deleted (`sudo iptables -D OUTPUT -p tcp --dport 80 -j REJECT`).

Python 2.7.6 (default, Mar 22 2014, 22:59:56) [GCC 4.8.2] on linux2

Please let me know if I'm off-track here..
I will try doing the same iptables trick on my Mac tonight and see if there's any difference.
",ducu,Lukasa
2406,2015-01-13 14:57:24,"@sigmavirus24 I suspected it's because of forking.

As I mentioned previously, having a single process with a simple loop to perform `count_words_at_url` doesn't cause this issue.. as `requests` recovers after the network gets back up.
",ducu,sigmavirus24
2406,2015-01-13 15:03:06,"@sigmavirus24 But even using the `rq`..
It also recovers if `requests.get` is **not** called before forking -- comment that [line](https://gist.github.com/ducu/ee8c0b1028775df6c72e#file-_test_rq_requests-py-L23) and there's no issue.
",ducu,sigmavirus24
2406,2015-01-13 16:58:32,"@ducu Is this a personal mac machine or one running OS X server?
",Lukasa,ducu
2406,2015-01-15 09:50:52,"Ok guys, I managed to test this on Ubuntu via VirtualBox and no issue there..

The exception being raised when disabling the network is

> ConnectionError: ('Connection aborted.', gaierror(-2, 'Name or service not known'))

different than what I get on the Mac

> ConnectionError: ('Connection aborted.', gaierror(8, 'nodename nor servname provided, or not known'))

The script goes on properly after the network gets back up, so `requests` is getting responses. I guess I can close this ticket, may be what @Lukasa is suggesting, something to do with the OS X DNS service.

Thanks a lot for your support, cheers
",ducu,Lukasa
2406,2015-01-15 10:42:06,"@ducu No problem! I hope this does get resolved, though it looks like discoveryd is ruining a lot of people's days at the minute.
",Lukasa,ducu
2406,2016-01-01 20:47:59,"@erdillon What specific behaviour is that? There is a quite lengthy debugging discussion in this issue: how much of that have you seen?
",Lukasa,erdillon
2406,2016-01-01 22:21:23,"@Lukasa I'm currently chasing rq forked child processes that die on my without notice on a larger POST request. No exception, nothing to be found. I've tracked it down to the actual POST. 
",erdillon,Lukasa
2406,2016-02-23 06:54:56,"@erdillon I believe I'm seeing the same sort of problem -- did you ever figure it out?
",jjwon0,erdillon
2406,2016-02-24 00:42:33,"@jjwon0 I did not figure it out. But in the meantime i've seen similar behavior go away after upgrading to the latest versions on the requests package. Are you up2date on all the libs?
",erdillon,jjwon0
2406,2016-07-21 17:16:12,"just reposting an issue I've been having related to this:

Hey guys, @selwin @ducu  @Lukasa @erdillon 

I've got an issue related to this (i think). But it's actually really easy to replicate. When making a request from a worker fails, the worker just dies, but no errors are reported nor is the job sent to the failed queue. 

my example:



When I queue `make_request` the process just quits when making the request. Whereas a traceback is thrown when I execute the script directs. Took me a while to track this down, is there a fix or something I may be doing wrong?
",dflatow,erdillon
2406,2016-07-21 17:16:12,"just reposting an issue I've been having related to this:

Hey guys, @selwin @ducu  @Lukasa @erdillon 

I've got an issue related to this (i think). But it's actually really easy to replicate. When making a request from a worker fails, the worker just dies, but no errors are reported nor is the job sent to the failed queue. 

my example:



When I queue `make_request` the process just quits when making the request. Whereas a traceback is thrown when I execute the script directs. Took me a while to track this down, is there a fix or something I may be doing wrong?
",dflatow,ducu
2406,2016-07-21 17:16:12,"just reposting an issue I've been having related to this:

Hey guys, @selwin @ducu  @Lukasa @erdillon 

I've got an issue related to this (i think). But it's actually really easy to replicate. When making a request from a worker fails, the worker just dies, but no errors are reported nor is the job sent to the failed queue. 

my example:



When I queue `make_request` the process just quits when making the request. Whereas a traceback is thrown when I execute the script directs. Took me a while to track this down, is there a fix or something I may be doing wrong?
",dflatow,Lukasa
2406,2016-07-21 17:32:25,"@dflatow See @jjwon0. However, the fact that it was causing a traceback in the main thread suggests the issue is with RQ, not requests. 
",Lukasa,dflatow
2406,2016-07-21 17:32:25,"@dflatow See @jjwon0. However, the fact that it was causing a traceback in the main thread suggests the issue is with RQ, not requests. 
",Lukasa,jjwon0
2406,2016-07-21 17:53:38,"@erdillon see nvie/rq#702 and nvie/rq#710 for the issue of rq not properly handling the killing of the python process of  a worker.
",spiliopoulos,erdillon
2406,2016-07-21 18:17:44,"thanks @jjwon0. @Lukasa Yeah agree, will see what the RQ folks say.
",dflatow,jjwon0
2406,2016-07-21 18:17:44,"thanks @jjwon0. @Lukasa Yeah agree, will see what the RQ folks say.
",dflatow,Lukasa
2406,2016-07-21 23:42:41,"@jjwon0 to be clear, RQ doesn't actively try to aggressively killing processes. It may be a side effect caused by `os.fork()` or the way we [run the job](https://github.com/nvie/rq/blob/master/rq/worker.py#L587) on a separate process but we don't do anything advanced like killing idle processes.

If anyone can tell me how we can change our code so requests can run smoothly without any workarounds, I'm more than happy to consider it :).
",selwin,jjwon0
2406,2016-09-06 07:24:29,"facing exactly the same issue as reported by @dflatow 
I am on OSX 10.11.3 with python 3.5.1. The process just dies without the job being put in the failed queue. 

However on our production server running Ubuntu 14.04.4 with python 3.4.3, it seems to be working just fine. I am at a loss trying to figure out if this is an rq problem or an issue with requests. Both the prod server and my dev box have the same version of both rq and requests.
",ujj,dflatow
2404,2015-01-10 13:40:40,"@NikolaiT just to give you some more detail about this:

The URI structure (that URLs follow) is of the form `{scheme}://{authority}{/path}{?query}{#fragment}`. (On Python 3) The `urllib.parse` module gives us the `urlparse` function to look at these components of a URL.



According to this module (and the RFC that defines handling of URIs) these two URLs are equivalent. This can be seen by visiting both in your browser. In essence, if we didn't modify the URL it would be as valid as our current approach is. We do normalize URLs though because servers (and sometimes users) give us some rather bizarre URLs that will only ""just work"" when normalized. As core developers of a library whose core design goal is to make users' lives better, we need to take this approach to satisfy that goal.

I hope that helps give you a deeper understanding of what's happening, why it is okay, and why it _should_ happen.
",sigmavirus24,NikolaiT
2402,2015-01-09 00:38:39,"Thanks for the quick response @sigmavirus24. That makes a lot of sense. I was thinking that a verbose flag would avoid the need for the try/except structure, but it is simple enough to a make a little helper function that processes the request and does that. Thanks again! Please go ahead and close this out.
",mrname,sigmavirus24
2401,2015-07-17 21:00:52,"Hi @tesande , were you able to resolve the issue. I have a similar case where I have  a server accepting tlsv1 and client using a request adapter with tlsv1. 
My gunicorn server  config 
keyfile = '/path/to/keyfile'
certfile = '/path/to/certfile'
ca_certs = '/path/to/ca/cert'
cert_reqs = ssl.CERT_OPTIONAL
ssl_version = ssl.PROTOCOL_TLSv1
do_handshake_on_connect = True

My client code looks like this:
requests_session = requests.Session()
requests_session.mount('https://', TLSAdapter)
resp = requests.request(request.http_method, request.full_uri, data=request.data, headers=request_headers, verify=verify, cert=(cert, key))

// tlsv1 adapter
class TLSAdapter(HTTPAdapter):
  def init_poolmanager(self, connections, maxsize, block=False):
    self.poolmanager = PoolManager(num_pools=connections,
                                   maxsize=maxsize,
                                   block=block,
                                   ssl_version=ssl.PROTOCOL_TLSv1)

I get this
TransportException: TransportException : <urlopen error [Errno 1] _ssl.c:493: error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown protocol>

Going into server logs, I also find these:
 _ssl.c:493: error:1408F10B:SSL routines:SSL3_GET_RECORD:wrong version number
",pankit,tesande
2401,2015-07-24 18:13:55,"Hi!

My web server version was from 2003. After updating, things worked fine.

However, if you don't need certification validation, you can turn that off. That might solve your problem

Date: Fri, 17 Jul 2015 14:01:21 -0700
From: notifications@github.com
To: requests@noreply.github.com
CC: tekvisle@hotmail.com
Subject: Re: [requests] Problems in SSL communication (#2401)

Hi @tesande , were you able to resolve the issue. I have a similar case where I have  a server accepting tlsv1 and client using a request adapter with tlsv1. 

My gunicorn server  config 

keyfile = '/path/to/keyfile'

certfile = '/path/to/certfile'

ca_certs = '/path/to/ca/cert'

cert_reqs = ssl.CERT_OPTIONAL

ssl_version = ssl.PROTOCOL_TLSv1

do_handshake_on_connect = True

My client code looks like this:

requests_session = requests.Session()

requests_session.mount('https://', TLSAdapter)

resp = requests.request(request.http_method, request.full_uri, data=request.data, headers=request_headers, verify=verify, cert=(cert, key))

// tlsv1 adapter

class TLSAdapter(HTTPAdapter):

  def init_poolmanager(self, connections, maxsize, block=False):



I get this

TransportException: TransportException : 

Going into server logs, I also find these:

 _ssl.c:493: error:1408F10B:SSL routines:SSL3_GET_RECORD:wrong version number

—
Reply to this email directly or view it on GitHub.
",tesande,tesande
2400,2015-01-06 03:30:52,"@sigmavirus24 Thanks for the suggestion. Will look it up.
Does using grequests allow connection pooling? I briefly looked at it right now and it does not look to be as feature rich as the requests library.
We make a ton of HTTP requests and connection pooling is important for us.
",sinank,sigmavirus24
2399,2015-01-06 01:09:05,"Thank you for helping with this!

Trying to clarify a bit what's happening..
As @sigmavirus24 mentioned, I'm using [`rq`](https://github.com/nvie/rq/) and [`summary`](https://github.com/svven/summary/) to process a bunch of URLs. I start several simple (forking) [workers](http://python-rq.org/docs/workers/) so they are doing one URL per job in totally separate processes on the same machine. It all goes fine for a while and suddenly, for no apparent reason, all workers hang at the same time at this line:

https://github.com/svven/summary/blob/master/summary/__init__.py#L193

By hanging I mean suspending the running job and leaving it in the _started_ state. There's no exception whatsoever and it doesn't get passed that line so the job is not finished properly. Then it gets to the next job and does the same thing going on in this ""strike"" mode until the main worker process is killed. All running workers behave this way. Newly started workers perform properly though.

It's a very peculiar issue and I haven't been able to reproduce it. It just happens after a while - few hours or even days, probably depending on the number of URLs that had been processed.

Running out of available sockets may be the cause of the problem, I think it's a good assumption but I didn't verify it yet. What puzzles me is the fact that I get no exception whatsoever, the child (forked) process doing the job simply dies at that line.
",ducu,sigmavirus24
2399,2015-01-06 11:01:24,"I wonder if calling `select` and checking if the socket is marked readable is a good idea. If it is we can pre-emptively close it, because either there's still data on it (and so it hasn't been exhausted) or it was closed remotely (and so closing it here is a no-op). @sigmavirus24?
",Lukasa,sigmavirus24
2399,2015-01-06 15:49:24,"@sigmavirus24: Ok I understand there's no easy way to access the socket..
But in my opinion, reading the whole content of the response when `stream=True` defies the purpose of having the streaming feature, isn't it?

Anyway if `requests` follows the redirects there's no problem with `summary`. It actually follows http_equiv_refresh ""redirects"" as well: https://github.com/svven/summary/blob/master/summary/__init__.py#L209

Getting back to the actual issue..
I'm trying to reproduce the problem like this: https://gist.github.com/ducu/a684c1e96afdaf2c5657

Do you think that [`psutil.Process.connections`](https://pythonhosted.org/psutil/#psutil.Process.connections) would show leaking sockets? Didn't try it properly yet but seen it running on Windows and Ubuntu, interesting how they behave differently, but no signs of leaking sockets for ~300 URLs.
I'll try it out on OS X tonight where I experienced the ""worker strike"" issue, and I'll feed it more URLs.
Am I on the right track here?
",ducu,sigmavirus24
2393,2015-01-18 18:13:47,"Added two tests and gave the strings slightly better names. Not sure we need to spell out every last RFC referenced in the comments though since they're all pretty well known. @Lukasa I'd appreciate some review.
",sigmavirus24,Lukasa
2393,2015-01-19 13:36:21,"@Lukasa forgot to mention this was updated to address your feedback.
",sigmavirus24,Lukasa
2389,2014-12-23 16:51:12,"@Lukasa I'm just waiting for @Jaypipes to verify this fixes their issues.
",sigmavirus24,Lukasa
2389,2014-12-23 17:39:02,"@sigmavirus24 I have now verified that this fixes the issue I was seeing. Thanks very much! :)
",jaypipes,sigmavirus24
2389,2014-12-23 17:40:32,"Cool. @Lukasa I'm going to cut 2.5.1 with this because it's a rather serious bug. (I have to wonder if it could be considered a CVE because someone could perform a denial of service on a client if it knows the clients behaviour and when to expire the nonce.)
",sigmavirus24,Lukasa
2388,2015-06-04 12:51:17,"@neosab can you resubmit this against the `proposed/3.0` branch?
",sigmavirus24,neosab
2388,2015-06-05 06:09:05,"@sigmavirus24 Resubmitted this against proposed/3.0.0 branch in https://github.com/kennethreitz/requests/pull/2631
",neosab,sigmavirus24
2388,2015-06-05 12:33:56,"Thanks @neosab! I'm closing this as a duplicate of #2631. Cheers! :sparkles: 
",sigmavirus24,neosab
2386,2014-12-25 16:12:55,"@looperhacks can you tell us what kind of proxy this is? Also, I doubt it will help, but have you attempted to use the `proxies` argument to `get`?
",sigmavirus24,looperhacks
2386,2015-01-27 12:57:27,"Ah, thanks for the reminder @schlamar, I had forgotten that we regressed/removed this.
",Lukasa,schlamar
2385,2014-12-21 15:41:52,"@sigmavirus24 You are right. That was because of `print(r.text)` call. Thank you!
",burdiyan,sigmavirus24
2385,2014-12-21 19:29:03,"@burdiyan the next time you have _questions_ and need help, please use our official Q&A section on [StackOverflow](https://stackoverflow.com/questions/tagged/python-requests). You'll get an answer much faster.
",sigmavirus24,burdiyan
2381,2014-12-15 15:22:58,"I would make a joke about how omitting random letters is cool, but it wouldn't be funy. 

Thanks @namlede 
",sigmavirus24,namlede
2380,2014-12-14 17:55:50,"Thanks for this @syndbg! Unfortunately, we're in feature freeze, so we won't be accepting this patch. Sorry, and thanks for the work!
",Lukasa,syndbg
2380,2014-12-14 18:13:28,"No problems, @Lukasa !

Can I help you somehow? I'm assuming a feature freeze is applied to fix bugs. 
",syndbg,Lukasa
2379,2014-12-12 16:28:05,"@arthurdarcet have you reported this as a bug to CherryPy?
",sigmavirus24,arthurdarcet
2379,2014-12-13 00:39:37,"@arthurdarcet I respectfully disagree that this isn't a bug on their side.
",sigmavirus24,arthurdarcet
2379,2014-12-13 19:49:01,"I agree @arthurdarcet. I just did not want the cherrypy issue to go unreported.
",sigmavirus24,arthurdarcet
2379,2015-01-18 19:14:04,"@sjagoe come again?
",sigmavirus24,sjagoe
2377,2014-12-11 15:59:25,"This is a direct consequence of supportiing https://github.com/kennethreitz/requests/issues/1638. You should remove your `Host` header that you specify and _then_ this should just work @antoniofelleca 
",sigmavirus24,antoniofelleca
2377,2014-12-16 20:51:22,"@antoniofelleca any updates?
",sigmavirus24,antoniofelleca
2376,2014-12-10 14:31:57,"@reddypdl can you do `python -c 'import platform; print(platform.python_implementation())'`, if not can you tell us how you built this version of Python? The `platform` module is a standard library module and `platform.python_implementation()` has been around since Python 2.3 so if this is a real version of Python 2.7, then that function should be present. Also is it possible you have a `platform.py` file in the same directory? If so, please rename it.
",sigmavirus24,reddypdl
2375,2014-12-09 13:53:23,"@Lukasa because it's faster if we fix it and provide a little extra flexibility at ~~no~~ little extra cost.
",sigmavirus24,Lukasa
2375,2014-12-09 14:27:43,"Hello,
I can understand how @lukasa is annoyed by unvendoring made downstream, but I can also understand why this is done. For example, in Debian, Python SSLv3 support was removed and I only had to patch (I forwared my patch upstream of course[¹]) urllib3 to fix all packages depending on urllib3.

So, yes, this is a downstream problem, but as in past (even when this issue arosed on Debian Bug Tracker) I will never add something without asking first if you, upstream developers, are ok with a downstream change. 
 As I said a lot of time before being the Debian maintainer of requests I'm one of its users: I want requests to be in the best shape in Debian. :)

I agree with @dstufft and @sigmavirus24 but, if you don't agree, I can also replace the currently used patch in Debian with this so, at least, Debian, Ubuntu and pip will use the same code.
IMHO cooperating we will arrive to the best solution for all.

[¹] http://anonscm.debian.org/viewvc/python-modules/packages/python-urllib3/tags/1.9.1-3/debian/patches/06_do-not-make-SSLv3-mandatory.patch?view=markup Yes, next time I will use a PR, fortunately @dstufft forwarded it properly! :)
",eriol,sigmavirus24
2375,2014-12-09 14:27:43,"Hello,
I can understand how @lukasa is annoyed by unvendoring made downstream, but I can also understand why this is done. For example, in Debian, Python SSLv3 support was removed and I only had to patch (I forwared my patch upstream of course[¹]) urllib3 to fix all packages depending on urllib3.

So, yes, this is a downstream problem, but as in past (even when this issue arosed on Debian Bug Tracker) I will never add something without asking first if you, upstream developers, are ok with a downstream change. 
 As I said a lot of time before being the Debian maintainer of requests I'm one of its users: I want requests to be in the best shape in Debian. :)

I agree with @dstufft and @sigmavirus24 but, if you don't agree, I can also replace the currently used patch in Debian with this so, at least, Debian, Ubuntu and pip will use the same code.
IMHO cooperating we will arrive to the best solution for all.

[¹] http://anonscm.debian.org/viewvc/python-modules/packages/python-urllib3/tags/1.9.1-3/debian/patches/06_do-not-make-SSLv3-mandatory.patch?view=markup Yes, next time I will use a PR, fortunately @dstufft forwarded it properly! :)
",eriol,dstufft
2375,2014-12-09 14:40:29,"@eriolv question: Since the symlink is in place (it may just be Fedora that has this in place), is there any chance of the imports that import from `.packages` inside of requests could not be rewritten? 

If not, can the imports not be rewritten after we ship this patch? (After we've updated it to give proper attribution to @dstufft and pypa/pip) The crux of this issue is that sys.modules is incorrectly populated and needs to work a certain way for users to not run into surprises like this.

[/Edit - I submitted my comment too soon]
And I appreciate your collaboration @eriolv. That's why I pinged you immediately. I wanted to make you aware of this from the start and get your feedback as well as @Lukasa's and @ralphbean's
",sigmavirus24,Lukasa
2375,2014-12-09 14:40:29,"@eriolv question: Since the symlink is in place (it may just be Fedora that has this in place), is there any chance of the imports that import from `.packages` inside of requests could not be rewritten? 

If not, can the imports not be rewritten after we ship this patch? (After we've updated it to give proper attribution to @dstufft and pypa/pip) The crux of this issue is that sys.modules is incorrectly populated and needs to work a certain way for users to not run into surprises like this.

[/Edit - I submitted my comment too soon]
And I appreciate your collaboration @eriolv. That's why I pinged you immediately. I wanted to make you aware of this from the start and get your feedback as well as @Lukasa's and @ralphbean's
",sigmavirus24,dstufft
2375,2014-12-09 16:40:17,"My snarkiness was mostly the result of me waking up late and being late for work and having not had coffee, apologies all.

In reality I'm +0 on this. I don't like that we have to do it, and the unvendoring zealots have hardened my opinion towards the idea of doing them any favours on any issue whatsoever. (I don't include you in that group @eriolv, you have not displayed any zealotry that I'm aware of :wink: )

However, I acknowledge the Catch-22 of the fact that _we_ will get blamed for the zealots decision to unbundle us breaking their code. For that reason I have no intention of blocking this patch: punishing users is unacceptable.

However, I'd like _someone_ to test the change, as neither @sigmavirus24 nor @dstufft appear to have. Ideally I'd like some form of automated testing for it as well: having that would raise me to +0.5.
",Lukasa,sigmavirus24
2375,2014-12-09 16:40:17,"My snarkiness was mostly the result of me waking up late and being late for work and having not had coffee, apologies all.

In reality I'm +0 on this. I don't like that we have to do it, and the unvendoring zealots have hardened my opinion towards the idea of doing them any favours on any issue whatsoever. (I don't include you in that group @eriolv, you have not displayed any zealotry that I'm aware of :wink: )

However, I acknowledge the Catch-22 of the fact that _we_ will get blamed for the zealots decision to unbundle us breaking their code. For that reason I have no intention of blocking this patch: punishing users is unacceptable.

However, I'd like _someone_ to test the change, as neither @sigmavirus24 nor @dstufft appear to have. Ideally I'd like some form of automated testing for it as well: having that would raise me to +0.5.
",Lukasa,dstufft
2375,2014-12-09 16:58:46,"@dstufft Indeed, and that's what's important. I want to make sure that if we're doing this we do it right the first time.

Note that it obviously won't do that on vendored systems. =)
",Lukasa,dstufft
2375,2014-12-10 15:27:05,"Thanks @ralphbean. Will Fedora 21 be able to generate a new build with this change at least?

Also, to everyone commenting on the fact that this file still references pip, I pulled this in and committed it to start a discussion. This will not be merged as is.
",sigmavirus24,ralphbean
2375,2014-12-24 20:06:33,"@Lukasa thoughts?
",sigmavirus24,Lukasa
2375,2014-12-24 20:07:19,"I'm with @kennethreitz. I don't like it, but I think we have to do it.
",Lukasa,kennethreitz
2375,2014-12-31 14:57:27,"@kennethreitz @Lukasa either of you have time to give this another once over? This will vastly improve user experience when using downstream distributed modules. Believe it, or not, it will also reduce the number of changes downstream re-distributors need to make to requests itself as well. Finally, it will make requests far more stable for our users on those systems who do not have the ability to ""Just use pip"". I still prefer to use pip myself (because I like to have a better control over what I use) but that isn't a luxury everyone has who wants to (and needs to) use requests. We shouldn't begrudge them for having a different set of constraints than we typically have.
",sigmavirus24,kennethreitz
2375,2014-12-31 14:57:27,"@kennethreitz @Lukasa either of you have time to give this another once over? This will vastly improve user experience when using downstream distributed modules. Believe it, or not, it will also reduce the number of changes downstream re-distributors need to make to requests itself as well. Finally, it will make requests far more stable for our users on those systems who do not have the ability to ""Just use pip"". I still prefer to use pip myself (because I like to have a better control over what I use) but that isn't a luxury everyone has who wants to (and needs to) use requests. We shouldn't begrudge them for having a different set of constraints than we typically have.
",sigmavirus24,Lukasa
2375,2014-12-31 15:28:27,"The code looks good to me, as it did originally, though I'm hardly an expert on Python's import machinery.

I still resent the requirement to add this code. I accept the benefits and acknowledge the need for it, I just refuse to be happy about it. ;) Basically, I feel the same way about this as I do about cleaning the house.

I'm +1 on merging, I'm sure I'll get over the pain. Want @kennethreitz to sign off though.
",Lukasa,kennethreitz
2375,2015-01-09 19:41:44,"@kennethreitz you have approximately a zero chance of convincing them to do that. I spent months on it for pip and basically gave up. As far as I can tell your options are either provide the tooling for unbundling to happen in a way that you control, or they are going to do it however each individual distro feels like doing it with varying levels of support for thinks like requests-toolbelt needing to import from urllib3.

Ubuntu 14.04 for instance ships with a completely broken ensurepip and a broken by default venv module, and they knew it was broken at the time, because ensurepip bundles pip and they didn't have a a solution for handling the bundling in a way that they considered acceptable. From what I can tell a lot of distros are perfectly happen shipping broken things as long as it's not broken for their particular use cases. This sucks but I don't think it's going to change.

Personally in pip I decided that grabbing my nose to deal with the stink and getting at least consistent de-bundling was worth the somewhat gross code when the alternative was that end users would suffer.
",dstufft,kennethreitz
2375,2015-01-09 19:44:28,"Cool. While we wait for you to do that @kennethreitz our users can continue to suffer. 
",sigmavirus24,kennethreitz
2375,2015-01-09 20:33:46,"This is officially @dstufft's decision. :)
",kennethreitz,dstufft
2375,2015-01-09 20:38:45,"yay delegation

@dstufft gets a cookie :cookie:
",kennethreitz,dstufft
2375,2015-01-09 20:47:49,"@kennethreitz et al, many thanks for this! :)
",eriol,kennethreitz
2375,2015-01-25 18:55:03,"@legrostdg maybe
",sigmavirus24,legrostdg
2374,2014-12-08 23:46:14,"Thanks @krvc 
",sigmavirus24,krvc
2373,2014-12-08 18:40:42,"reStructuredText on GitHub is so weird. Thanks @frewsxcv 
",sigmavirus24,frewsxcv
2371,2014-12-05 17:58:06,"@alex Interestingly, neither `env PYTHONUNBUFFERED=` or `-u` has the same effect on Python 2. Results from my machine incoming.
",Lukasa,alex
2371,2014-12-05 18:06:31,"FWIW, I just merged adding `buffering=True` from @kevinburke, do your runs
include that?

On Fri Dec 05 2014 at 12:04:40 PM Cory Benfield notifications@github.com
wrote:

> Alright, the below data comes from a machine that is doing nothing else
> but running these tests. The last test was run with the Python -u flag
> set, and as you can see that flag has no effect.
> 
> Python 2.7.6
> go version go1.2.1 linux/amd64
> BENCH SOCKET:
>    8GiB 0:00:16 [ 500MiB/s] [================================>] 100%
> BENCH HTTPLIB:
>    8GiB 0:01:32 [88.6MiB/s] [================================>] 100%
> BENCH URLLIB3:
>    8GiB 0:01:20 [ 101MiB/s] [================================>] 100%
> BENCH REQUESTS
>    8GiB 0:01:21 [ 100MiB/s] [================================>] 100%
> BENCH GO HTTP
>    8GiB 0:00:21 [ 385MiB/s] [================================>] 100%
> 
> Python 2.7.6
> go version go1.2.1 linux/amd64
> BENCH SOCKET:
>    8GiB 0:00:16 [ 503MiB/s] [================================>] 100%
> BENCH HTTPLIB:
>    8GiB 0:01:33 [87.8MiB/s] [================================>] 100%
> BENCH URLLIB3:
>    8GiB 0:01:20 [ 101MiB/s] [================================>] 100%
> BENCH REQUESTS
>    8GiB 0:01:22 [99.3MiB/s] [================================>] 100%
> BENCH GO HTTP
>    8GiB 0:00:20 [ 391MiB/s] [================================>] 100%
> 
> Python 2.7.6
> go version go1.2.1 linux/amd64
> BENCH SOCKET:
>    8GiB 0:00:16 [ 506MiB/s] [================================>] 100%
> BENCH HTTPLIB:
>    8GiB 0:01:31 [89.1MiB/s] [================================>] 100%
> BENCH URLLIB3:
>    8GiB 0:01:20 [ 101MiB/s] [================================>] 100%
> BENCH REQUESTS
>    8GiB 0:01:20 [ 101MiB/s] [================================>] 100%
> BENCH GO HTTP
>    8GiB 0:00:21 [ 389MiB/s] [================================>] 100%
> 
> These numbers are extremely stable, and show the following features:
> 1. Raw socket reads are fast (duh).
> 2. Go is about 80% the speed of a raw socket read.
> 3. urllib3 is about 20% the speed of a raw socket read.
> 4. requests is slightly slower than urllib3, which makes sense as we
>    add a couple of stack frames for the data to pass through.
> 5. httplib is slower than requests/urllib3. That's just impossible,
>    and I suspect that we must be configuring httplib or the sockets library in
>    a way that httplib is not.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/2371#issuecomment-65829335
> .
",alex,kevinburke
2371,2014-12-08 17:04:52,"Cake for @alex for being super helpful :cake:
",kennethreitz,alex
2371,2014-12-08 17:10:48,"@nelhage did some stracing of the various examples (in the transfer
encoding: chunked case) https://gist.github.com/nelhage/dd6490fbc5cfb815f762
are the results. It looks like there's a bug in httplib which results in it
not always reading a full chunk off the socket.

On Mon Dec 08 2014 at 9:05:14 AM Kenneth Reitz notifications@github.com
wrote:

> Cake for @alex https://github.com/alex for being super helpful [image:
> :cake:]
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/2371#issuecomment-66147998
> .
",alex,alex
2371,2014-12-08 17:13:42,"So what we have here is a bug in a standard library that no one is really maintaining? (@Lukasa has at least 2 patch sets that  have been open for >1 year.) Maybe I'll raise a stink on a list somewhere tonight
",sigmavirus24,Lukasa
2371,2014-12-08 17:15:06,"Someone (I might get to it, unclear) probably needs to drill down with pdb
or something and figure out what exact code is generating those 20-byte
reads so we can put together a good bug report.

On Mon Dec 08 2014 at 9:14:09 AM Ian Cordasco notifications@github.com
wrote:

> So what we have here is a bug in a standard library that no one is really
> maintaining? (@Lukasa https://github.com/Lukasa has at least 2 patch
> sets that have been open for >1 year.) Maybe I'll raise a stink on a list
> somewhere tonight
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/2371#issuecomment-66149522
> .
",alex,Lukasa
2371,2014-12-20 17:37:37,"@kislyuk Not as far as I'm aware. Hopefully I'll have some time to chase it down this christmas holiday.
",Lukasa,kislyuk
2371,2014-12-22 18:55:06,"Thanks @Lukasa. I'm dealing with a performance issue where download speed on a chunked response using urllib3/requests is much slower than with curl and other libraries, and trying to understand if this is the culprit.
",kislyuk,Lukasa
2371,2015-03-07 19:24:05,"@gardenia I haven't had a chance to absorb all of this, but thank you so much for your effort and work here. @shazow perhaps you'd find @gardenia's research interesting.
",sigmavirus24,gardenia
2371,2015-03-07 19:28:35,":+1: thanks @gardenia. Incidentally, my own research into performance in my use case uncovered that in my case the responses are not chunked, but urllib3 performs 20+% faster than requests, so there is some overhead being introduced that I want to characterize. Still in line with the title of this issue, but different root cause.
",kislyuk,gardenia
2371,2015-03-07 19:29:41,"Fascinating, thanks for sharing! :)

Seems like a great goal for @Lukasa's Hyper to address, too.
",shazow,Lukasa
2371,2015-03-07 20:56:00,"@alex - I toyed around a little with the urllib3 vs requests non-chunked performance issue you mentioned.  I think I see a similar 20% drop in requests.

In requests I speculatively tried replacing the call to self.raw.stream with the inlined implementation of stream() (from urllib3).  It seemed to bring the throughput a lot closer between requests and urllib3, at least on my machine:



Maybe you could try the same on your machine to see if it makes a difference for you too.

(Note yes I know the call to is_fp_closed is encapsulation busting, it isn't meant as a serious patch just a data point)
",gardenia,alex
2371,2015-03-07 22:29:53,"@shazow It's my hope that the `BufferedSocket` that hyper uses should address a lot of that inefficiency, by essentially preventing small reads. I wonder if `httplib` on Py3 has this problem, because it uses `io.BufferedReader` extensively, which should provide roughly the same kind of benefit as the `BufferedSocket`.

Certainly, however, when `hyper` grows enough HTTP/1.1 functionality to be useful we should try to benchmark it alongside these other implementations and make efforts to make `hyper` as fast as possible.
",Lukasa,shazow
2370,2014-12-04 21:32:23,"Hey @wangcc I  expect you resolved whatever problem you had. In the future, please don't delete the description of the issue, even if you filed it on the wrong project. Issues that have had their content edited out are confusing and unhelpful to most people.
",sigmavirus24,wangcc
2369,2014-12-04 02:57:41,"@kevinburke Wrong repo. ;) Merge to urllib3 first, then we'll take the fix.
",Lukasa,kevinburke
2368,2014-12-04 05:21:59,"So that's also my first instinct @Lukasa but I don't think `getpeercert` would remove a wildcard `subjectAltName` entry randomly. Still, I'd like to know from @EthanBlackburn or @buttscicles what happens on Python 3 (preferably 3.3 or 3.4).
",sigmavirus24,Lukasa
2368,2014-12-04 05:21:59,"So that's also my first instinct @Lukasa but I don't think `getpeercert` would remove a wildcard `subjectAltName` entry randomly. Still, I'd like to know from @EthanBlackburn or @buttscicles what happens on Python 3 (preferably 3.3 or 3.4).
",sigmavirus24,EthanBlackburn
2368,2014-12-04 14:40:17,"I beat @Lukasa to commenting on an issue! Holy smokes!
",sigmavirus24,Lukasa
2366,2014-12-07 18:58:22,"Yeah. I agree with @Lukasa. By specifying a custom header like this, you're basically asserting that you know what you want to happen and if something unexpected happens then you have to remedy it, even if that means having to manually handle redirects
",sigmavirus24,Lukasa
2366,2014-12-18 08:17:38,"@Lukasa ok, i got it.  you could try with curl -H""Host: xxx.yyy.com"" url.  (curl/7.19.7)
the behavior of curl is also use new-host header, if a 302 return, not user set one.
",hakulat,Lukasa
2366,2015-03-13 03:44:07,"@dieselmachine it's working as intended because users are not supposed to set the Host header and if they do then they need to understand that there are going to be big differences in behaviour in certain cases.
",sigmavirus24,dieselmachine
2364,2014-12-02 16:00:39,"@kevinburke it usually takes about a minute to fail.

I've attached a few screenshots from wireshark. Let me know if a screenshot of something else might be more helpful.

![initial](https://cloud.githubusercontent.com/assets/410872/5265689/49539ce0-7a12-11e4-9fea-fde29204fe43.png)

![exchange](https://cloud.githubusercontent.com/assets/410872/5265696/5426e35c-7a12-11e4-85ee-e26cfc64c9db.png)

Thanks guys
",greedo,kevinburke
2364,2014-12-02 16:34:36,"Are there any machine or proxies sittin in between the server and the
client? does the client machine have a default socket timeout configured?

On Tuesday, December 2, 2014, Joe Cabrera notifications@github.com wrote:

> @kevinburke https://github.com/kevinburke it usually takes about a
> minute to fail.
> 
> I've attached a few screenshots from wireshark. Let me know if a
> screenshot of something else might be more helpful.
> 
> [image: initial]
> https://cloud.githubusercontent.com/assets/410872/5265689/49539ce0-7a12-11e4-9fea-fde29204fe43.png
> 
> [image: exchange]
> https://cloud.githubusercontent.com/assets/410872/5265696/5426e35c-7a12-11e4-85ee-e26cfc64c9db.png
> 
> Thanks guys
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/2364#issuecomment-65253593
> .

## 

## 

Kevin Burke
phone: 925.271.7005 | twentymilliseconds.com
",kevinburke,kevinburke
2364,2014-12-02 16:51:18,"@kevinburke there is a NAT sitting between the machine and the server, but no proxy.

I am using a `(1,30)` timeout and my understanding is `requests` overrides whatever the client timeout defaults are.
",greedo,kevinburke
2364,2014-12-02 17:01:07,"That's correct. Curious why it's taking a minute to time out if you set the
timeout to 30 seconds.

On Tuesday, December 2, 2014, Joe Cabrera notifications@github.com wrote:

> @kevinburke https://github.com/kevinburke there is a NAT sitting
> between the machine and the server, but no proxy.
> 
> I am using a (1,30) timeout and my understanding is requests overrides
> whatever the client timeout defaults are.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/2364#issuecomment-65262796
> .

## 

## 

Kevin Burke
phone: 925.271.7005 | twentymilliseconds.com
",kevinburke,kevinburke
2364,2014-12-02 19:34:44,"@kevinburke it could be closer to 30 seconds, however I also make more than 1 request
",greedo,kevinburke
2364,2015-07-20 14:55:06,"@greedo Python 3.5 has not been released yet and this is not something we can just _replace_. http.client on Python 3.5 will raise RemoteDisconnected now instead of BadStatusLine (presumably  in only _some_ cases) and that will be wrapped the same way that we currently wrap BadStatusLine.
",sigmavirus24,greedo
2364,2016-05-24 10:32:26,"Thanks @Lukasa. **`data={'cmd': 'date +%Y%m%d'}`** fails to receive command results, ex:


",nixawk,Lukasa
2364,2016-05-24 10:39:47,"@Lukasa  I've tested this against **`D-LINK DIR-600`** routers

## requests



## curl   -- OK


",nixawk,Lukasa
2364,2017-03-12 12:02:12,"@sigmavirus24 The reason `RemoteDisconnected` exception was added, is so that HTTP clients can safely retry (see [issue 3566](https://bugs.python.org/issue3566)).
Don't you think the right behavior here is adding an automatic retry?",shoham-stratoscale,sigmavirus24
2364,2017-03-12 18:55:51,"@shoham-stratoscale presuming that the ""right behaviour"" is an automatic retry, that should be added to urllib3 (which actually handles the error), not requests.",sigmavirus24,shoham-stratoscale
2363,2014-12-01 22:30:08,"@alex those tests that were using https didn't seem to have any necessity to rely on https. They were likely written by me and I just write `'https://httpbin.org'` everytime I want to talk to httpbin by force of habit.
",sigmavirus24,alex
2362,2014-12-01 19:56:28,"@kevinburke hmm, i suppose so . 
",kennethreitz,kevinburke
2362,2014-12-01 20:05:35,"@kevinburke if someone tells me what command to type in, i'll do it lol
",kennethreitz,kevinburke
2362,2014-12-03 02:25:06,"@sigmavirus24 now has access to dnsimple. @Lukasa, can you tell me your dnsimple email address? 
",kennethreitz,sigmavirus24
2362,2014-12-03 07:12:31,"@kennethreitz Sorry, used a different one: lukasaoz@gmail.com.

Always best to have 8 million email addresses.
",Lukasa,kennethreitz
2362,2014-12-08 17:10:08,"Yeah. I'm curious why that's no longer building PRs or anything. If you want, I can set up ci.sigmavir.us to do CI here for now, but I'll need access to settings (or will need to sync up with @Lukasa about adding it)
",sigmavirus24,Lukasa
2359,2014-11-28 02:35:48,"We can't vendor anything that uses C extensions, so no @rsnair2 we can't use cChardet
",sigmavirus24,rsnair2
2359,2014-11-28 11:37:40,"Couldn't it be the solution in @marcocova's case though? If instead of letting requests determine the charset, he could use cChardet and tell requests what charset to expect.
",Terr,marcocova
2359,2014-11-29 00:30:06,"@Terr the solution of setting the encoding manually is well documented, so I would hope that @marcocova had considered that.
",sigmavirus24,Terr
2359,2014-11-29 00:30:06,"@Terr the solution of setting the encoding manually is well documented, so I would hope that @marcocova had considered that.
",sigmavirus24,marcocova
2359,2014-12-02 14:02:19,"@sigmavirus24, yes, workarounds are well understood. I was just concerned that the default behavior leaves the user exposed to this issue (with no warnings in the docs - that I could find at least).
",marcocova,sigmavirus24
2359,2015-07-01 09:24:50,"@marcocova I had the same issue, then i found this: https://pypi.python.org/pypi/cchardet/
From 5 seconds (chardet), I got 1 milisecond :)
",simion,marcocova
2357,2014-11-26 02:23:35,"Hey @netjunki,

We're not expanding the API any longer. Requests is under a feature freeze unless Kenneth blesses it for inclusion (which is rare and highly unlikely). You could probably subclass the `MultipartEncoder` from the requests-toolbelt and achieve what you're looking for here pretty easily.

Thanks for the pull request
",sigmavirus24,netjunki
2356,2014-12-25 16:22:45,"So, looking at this again, I did tried the following:



I assume this is something along the lines of what @suhaasprasad is seeing. I'm going to see if following @Lukasa's idea will work for this.
",sigmavirus24,Lukasa
2356,2014-12-25 16:22:45,"So, looking at this again, I did tried the following:



I assume this is something along the lines of what @suhaasprasad is seeing. I'm going to see if following @Lukasa's idea will work for this.
",sigmavirus24,suhaasprasad
2355,2014-11-25 01:00:14,"Very nice @msabramo 
",nuxlli,msabramo
2355,2014-11-25 02:26:18,"@sigmavirus24: Thanks for the detailed explanation!
",msabramo,sigmavirus24
2352,2014-11-23 13:51:20,"@Lukasa All good points. From the perspective of a new user who isn't aware of the architecture of requests, they just want a simple mechanism that will allow easy debugging without having to use wireshark filters, passive burp proxy etc. For example, when using `curl` you can use the `-v` option to dump all request data, something like that for requests would be more than sufficient. 

I'm wondering if it would be more appropriate to raise a feature request in `urllib3` for a simple session method, such as `verbose(True/False)` which then implements the necessary recipe to dump traffic data in a similar format as `curl -v`. If accepted, then we could implement a method on `requests.Session` that called the urllib3 verbose method.

Does that sound like a good way forward?
",foxx,Lukasa
2352,2014-11-23 21:11:08,"@sigmavirus24 Normally I don't bother commenting on rudeness, but I felt compelled to on this occasion because of my prior background with requests/urllib3. I really feel like your response was overly aggressive and borderline rude. I've never had this issue with any of the other core maintainers in the past, and I spent time specifically replying because they have always been polite and reasonable, even when rejecting my code or requests. This interaction has left me feeling like I no longer want to bother contributing towards either of those projects, your reaction/approach was completely unnecessary and similar in tone with the Django core developers poor bed side manner.

Sometimes things get rejected, and that's fine, but there's no need to be rude about it.
",foxx,sigmavirus24
2352,2014-11-23 23:16:34,"@foxx I'm sorry you felt that way. It's never our intention to create a hostile atmosphere, and I apologise unreservedly if that's what we did.

This is another of my 'wish list' features for a rewrite of `httplib`. If I ever find the time to do it, this will be a key tentpole of that rewrite.
",Lukasa,foxx
2352,2014-11-24 00:02:27,"@Lukasa Thank you for the follow up, it's appreciated. Based on comments in those other issues and a quick inspection of the source, additional debugging support would _probably_ have to rely on monkey patching. Given that's not a great approach, I'd agree that a rewrite of `httplib` is sounding like the cleaner option. 

Out of curiosity, were you planning on keeping the `httplib` rewrite backwards compatible, or would it be `httplib2`?
",foxx,Lukasa
2352,2014-11-24 02:31:10,"@foxx I wasn't trying to be rude just honest. If there's anything that annoys me most it's when people aren't fully honest in a way that would help me prevent wasting my time. If I'm mistaken about which comment offended you, let me know. It's no excuse but I'm typically in a rush and replying from my phone. I don't often have the luxury of re-reading things to make sure they won't come across as rude. I do hope you continue to contribute because your input is valuable. 
",sigmavirus24,foxx
2352,2014-11-24 02:38:25,"@sigmavirus24 I appreciate the follow up, thank you, and no hard feelings :)
",foxx,sigmavirus24
2352,2014-11-24 08:31:27,"@foxx It would not be backwards compatible, but neither would it be `httplib2` (that already exists!). `httplib`'s API is _bad_, and it limits what can be done by libraries that build on top of it. I have ideas for where to go next, but again, no time. =(
",Lukasa,foxx
2345,2014-11-21 18:55:26,"@mattrobenolt Not your fault, our CI was busted so you had no way to know. =)
",Lukasa,mattrobenolt
2344,2014-11-16 15:43:28,"@Lukasa it's a test failure on master apparently. Besides that, I'm -1 on this. A `LocationParseError` happens long before a connection is even attempted. This should be an `InvalidURL` error
",sigmavirus24,Lukasa
2344,2014-11-17 05:39:00,"@sigmavirus24 If I've understood you correctly, I have changed the ConnectionError to an InvalidURL error.
",ContinuousFunction,sigmavirus24
2344,2014-11-18 03:54:43,"@ContinuousFunction could you amend your commit messages? Two commits in a row with the same message is incredibly unhelpful when looking at the log.
",sigmavirus24,ContinuousFunction
2344,2014-11-18 04:11:25,"@sigmavirus24 sorry about that. I've changed the message.
",ContinuousFunction,sigmavirus24
2344,2014-12-15 17:41:30,"@ContinuousFunction can you rebase this, uncomment the test that you commented, and ensure the tests pass. Then force-push to this same branch?
",sigmavirus24,ContinuousFunction
2344,2014-12-15 19:05:32,"@sigmavirus24 I believe I have made the appropriate changes.
",ContinuousFunction,sigmavirus24
2344,2014-12-17 04:14:20,"Thanks @ContinuousFunction I'll run these tests now and make sure they pass before merging.
",sigmavirus24,ContinuousFunction
2344,2014-12-17 04:50:48,"There was a test failure on Python 3 because `urllib3.exceptions.LocationParseError` doesn't have a `message` attribute. That's fixed in bd3cf95e34aa49c8d764c899672048df107e0d70.

Thanks @ContinuousFunction !
",sigmavirus24,ContinuousFunction
2344,2014-12-17 16:50:52,"@sigmavirus24 My pleasure!
",ContinuousFunction,sigmavirus24
2342,2014-11-14 22:12:59,"No worries @ivankoster 
",sigmavirus24,ivankoster
2340,2014-11-15 14:20:39,"As discussed on your [StackOverflow question](https://stackoverflow.com/questions/26928672/importerror-cannot-import-name-certs?noredirect=1#comment42433284_26928672), we concluded this is not a bug. Thanks @davyria 
",sigmavirus24,davyria
2340,2014-11-18 13:15:57,"@blazaid it sounds more like an issue with the environment to me. Without a Windows VM to test with I can't provide much help. Your best bet is to describe the series of events that led to your broken installation and posting to StackOverflow with more detail than @davyria 
",sigmavirus24,blazaid
2340,2014-11-18 13:15:57,"@blazaid it sounds more like an issue with the environment to me. Without a Windows VM to test with I can't provide much help. Your best bet is to describe the series of events that led to your broken installation and posting to StackOverflow with more detail than @davyria 
",sigmavirus24,davyria
2339,2014-11-14 14:20:48,"I agree. This belongs in another library. If that library chooses to interact with requests that'd be great and they could probably take a similar approach to [betamax](/sigmavirus24/betamax). I'm happy to advise, but this isn't a bug and it's at this point a rejected feature.

Thanks @recrm for starting this discussion.
",sigmavirus24,recrm
2338,2014-11-13 18:13:49,"@xxx Oh, that's gotta be the most annoying handle on GitHub. Sorry you got accidentally summoned!
",Lukasa,xxx
2338,2014-11-17 04:43:52,"@Lukasa can this be closed?
",sigmavirus24,Lukasa
2336,2014-12-03 15:03:39,"We haven't shipped the fix for this yet. Ping @shazow and @sigmavirus24

On Wednesday, December 3, 2014, Zhang Yibin notifications@github.com
wrote:

> I also encoutered a deadlock problem caused by this similar timeout issue.
> 
> What I did is sending a GET request to some site (like http://jsonip.com)
> using an HTTP proxy to get proxy's public ip address, the code is like:
> 
> s = requests.Session()
> s.proxies = {'http': 'http://xx.xx.xx.xx:xxx', 'https': 'http://xx.xx.xx.xx:xxx'}
> r = s.get('http://jsonip.com', timeout=5)
> r.raise_for_status()
> ...
> 
> After deadlock, the stack trace of the problmatic thread is (other threads
> are waiting for its completion):
> 
> File: ""C:\Python27\lib\threading.py"", line 783, in __bootstrap
>   self.__bootstrap_inner()
> File: ""C:\Python27\lib\threading.py"", line 810, in __bootstrap_inner
>   self.run()
> File: ""C:\Python27\lib\threading.py"", line 763, in run
>   self.__target(_self.__args, *_self.__kwargs)
> File: ""C:\Python27\lib\site-packages\concurrent\futures\thread.py"", line 73, in _worker
>   work_item.run()
> File: ""C:\Python27\lib\site-packages\concurrent\futures\thread.py"", line 61, in run
>   result = self.fn(_self.args, *_self.kwargs)
> File: ""E:\zbb\projects\crawler\new\zbb-crawler\zbb_proxymanager\app\proxy_filter.py"", line 45, in test
>   ip = self.public_ip.get(host)
> File: ""E:\zbb\projects\crawler\new\zbb-crawler\zbb_proxymanager\utils.py"", line 91, in get
>   ip = func(session, timeout)
> File: ""E:\zbb\projects\crawler\new\zbb-crawler\zbb_proxymanager\get_ip.py"", line 43, in get_ip_4
>   r = session.get('http://jsonip.com', timeout=timeout)
> File: ""C:\Python27\lib\site-packages\requests\sessions.py"", line 473, in get
>   return self.request('GET', url, *_kwargs)
> File: ""C:\Python27\lib\site-packages\requests\sessions.py"", line 461, in request
>   resp = self.send(prep, *_send_kwargs)
> File: ""C:\Python27\lib\site-packages\requests\sessions.py"", line 599, in send
>   history = [resp for resp in gen] if allow_redirects else []
> File: ""C:\Python27\lib\site-packages\requests\sessions.py"", line 192, in resolve_redirects
>   allow_redirects=False,
> File: ""C:\Python27\lib\site-packages\requests\sessions.py"", line 573, in send
>   r = adapter.send(request, **kwargs)
> File: ""C:\Python27\lib\site-packages\requests\adapters.py"", line 370, in send
>   timeout=timeout
> File: ""C:\Python27\lib\site-packages\requests\packages\urllib3\connectionpool.py"", line 513, in urlopen
>   conn = self._get_conn(timeout=pool_timeout)
> File: ""C:\Python27\lib\site-packages\requests\packages\urllib3\connectionpool.py"", line 231, in _get_conn
>   return conn or self._new_conn()
> File: ""C:\Python27\lib\site-packages\requests\packages\urllib3\connectionpool.py"", line 717, in _new_conn
>   return self._prepare_conn(conn)
> File: ""C:\Python27\lib\site-packages\requests\packages\urllib3\connectionpool.py"", line 690, in _prepare_conn
>   conn.connect()
> File: ""C:\Python27\lib\site-packages\requests\packages\urllib3\connection.py"", line 217, in connect
>   self._tunnel()
> File: ""C:\Python27\lib\httplib.py"", line 752, in _tunnel
>   (version, code, message) = response._read_status()
> File: ""C:\Python27\lib\httplib.py"", line 365, in _read_status
>   line = self.fp.readline(_MAXLINE + 1)
> File: ""C:\Python27\lib\socket.py"", line 476, in readline
>   data = self._sock.recv(self._rbufsize)
> 
> I think it's the same bug discussed in this issue, right? The version of
> Requests is 2.5.0.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/2336#issuecomment-65412519
> .

## 

## 

Kevin Burke
phone: 925.271.7005 | twentymilliseconds.com
",kevinburke,sigmavirus24
2336,2015-01-06 21:00:33,"I think we need to pull urllib3 in before the next release to fix this. 2.5.1 was released mostly with the intention of fixing a bug with HTTPDigestAuth. @glaslos if we pull in a more modern copy of urllib3, can you test master for us to see if it fixes the problem? If it does I'm comfortable cutting 2.5.2
",sigmavirus24,glaslos
2336,2015-01-07 00:40:43,"Master has an up-to-date copy of urllib3 @glaslos. Test away
",sigmavirus24,glaslos
2336,2015-01-07 11:41:32,"@kevinburke could you confirm that your changes to urllib3 are in the requests master branch?
",glaslos,kevinburke
2336,2016-07-23 13:13:49,"@Petelin The code in both cases is identical: there is no difference. Please note, the timeout is _per socket event_: it is possible that you are hanging elsewhere in the stack.
",Lukasa,Petelin
2336,2016-07-23 13:31:19,"@Lukasa my bad 



use a fake https proxie to access real https://baidu.com,it will just hang there.

i do this in ipython with no context.
",Petelin,Lukasa
2336,2016-07-24 20:35:09,"@Lukasa many thanks for pointing me to this!
@Petelin If I understand correctly you were using a backported system package of requests, since on Trusty I see requests 2.2.1 as system package. Is this right?
I'm asking because I see an instant timeout too using requests (from the system package) 2.10.0 on Debian testing. I don't have at the moment an Ubuntu trusty VM, but I will be happy to indvestigate if you can give me more details. Thanks! 
",eriol,Lukasa
2336,2016-07-24 20:35:09,"@Lukasa many thanks for pointing me to this!
@Petelin If I understand correctly you were using a backported system package of requests, since on Trusty I see requests 2.2.1 as system package. Is this right?
I'm asking because I see an instant timeout too using requests (from the system package) 2.10.0 on Debian testing. I don't have at the moment an Ubuntu trusty VM, but I will be happy to indvestigate if you can give me more details. Thanks! 
",eriol,Petelin
2334,2015-04-02 10:59:42,"Update, correcting myself:
- Went reading RFC2617.
- Figured out that my understanding about Digest auth was wrong: Digest auth does not authenticate connections (contrarily to NTLM), it does authenticate requests.
- Given this, please disregard my considerations above concerning HTTPAdapters, Pools, etc.
- Digged deeper into this particular case:
  - The culprit seems to be the fact that `HTTPDigestAuth.handle_401` only produces the `Authorization` header if it has been called less than 2 times (see ...`and num_401_calls < 2:` around line 172 in auth.py)
  - With two near-simultaneous threads, the first one entering `handle_401` will correctly produce the necessary auth headers, the second one will not.
- My hack test:
  - Commented out the `and num_401_calls < 2` in auth.py.
  - Ran a variation of the test above successfully with 2, 20 and 200 threads (with the original code, I would only get 50% of the requests authenticated).
- Possible fix:
  - The underlying `and num_401_calls < 2` seems to stem from #547 which, rightfully so, tries to prevent infinite auth failure loops. This means throwing the 401 call count away would lead to regressions.
  - A primitive approach would count 401 calls on a per-thread basis: not sure if feasible, not sure if elegant.

Here are my 2c.

PS: @Lukasa, given this, your comment about the (agreed, really nasty) possibility of pulling the raw `httplib` connection object would not apply here, but maybe feasible (at least for quick-hack tests) in https://github.com/requests/requests-ntlm. However, I'm not sure how to grab that object: any pointer? (lest's move this discussion away from this issue, shall we?) :)

More PS: For completeness, I dump my test code (easier to raise up thread count)


",exvito,Lukasa
2334,2015-04-02 13:38:50,"Update:
- I submited PR #2523 with a possible solution based on a per-thread num_401_calls count.
- @achorrath , @vincentxb : Can you give it a run and confirm correct multi-threaded operation?
",exvito,achorrath
2334,2015-04-02 13:38:50,"Update:
- I submited PR #2523 with a possible solution based on a per-thread num_401_calls count.
- @achorrath , @vincentxb : Can you give it a run and confirm correct multi-threaded operation?
",exvito,vincentxb
2334,2015-04-03 09:24:57,"Greetings all. Thanks for taking care of this @exvito. My only concern is about nonce and nonce_count values in a multi-threaded context. More generally, how do you deal with two threads being simultaneously in the handle_401() method?
",vincentxb,exvito
2334,2015-04-03 12:18:42,"@vincentxb : thanks for your feedback. You are correct. @tardyp made the same comment in #2523. I'll go ahead and extend the per-thread storage to the other state attributes.
",exvito,vincentxb
2333,2014-11-12 15:53:45,":sparkles: :cake: :sparkles: Thanks @akitada !!!
",sigmavirus24,akitada
2327,2014-11-07 18:26:17,"This is an interesting proposal, that I think suffers from being for an extremely specific use-case that is relatively uncommon. Let's take it in two parts.

First, `max_connectiontime`. `time` in this context is underspecified. What do we mean? CPU time? On a heavily loaded CPU you may not get scheduled and so this can turn into an extremely long wall-clock duration. Wall-clock time? This is more likely, but it still allows for us to be resource starved out, and to achieve it correctly requires that we register for signals from the OS, which is an extremely unpleasant thing for a library to do. For us to pick which of these the user meant is extremely presumptuous, especially when, as @kevinburke says, it's not that hard for the user to do themselves.

The max file-size is an interesting idea, but fundamentally starts adding some weirdness to the API. Firstly, what does `requests.send(url, stream=False, max_filesize=80)` mean? It should never be possible to write a mutually contradictory line of code in this manner. I'd be _more_ open to having this be a property on a `Response` object, realistically I think I'm pretty happy with the situation as is. I'd be interested to hear from the other maintainers though.
",Lukasa,kevinburke
2327,2014-11-07 19:49:19,"""Why does there need to be a maximum filesize enforced by requests. Why aren't you using streaming and detecting this yourself?""

I recently implemented streaming and chunking.  It wasn't difficult for me to implement.

However:
- This appears to be a question/need that often occurs on StackOverflow and in this issue tracker
- Because there are no limits to filesize in the requests library, it is easy for a user of the library to request a large file or endless stream of data and not realize this is happening. This can lead to issues with memory and disk usage. 
- Dealing with a chunked read is a bit of an advanced topic as it deals with manipulating the `raw` file-like object.  While it is something that is old and familiar for people with a few years of Python experience, it's not something that is easy for junior developers.  Some sort `max_filesize` argument or a `safe_read` function could encapsulate this logic and make it accessible to all.

""You've described a ""content stash"" that has no basis in anything this library does and did not define it.""
- The response object has a `.content` property/method (https://github.com/kennethreitz/requests/blob/master/requests/models.py#L714-L736)
- Said property iterates through the `.iter_content()` filelike object (https://github.com/kennethreitz/requests/blob/master/requests/models.py#L639-L683) 
- The read content is then stored in an internal stash named `._content` (https://github.com/kennethreitz/requests/blob/master/requests/models.py#L539-L540)

I think I was wrong about placement, and it would better function as a method in Response as @Lukasa noted.  What I'm essentially suggesting, is extending the library to have a safeguard around the call to `.iter_content`.  The data would be read in chunks, and stored in the object's `._content` stash.  If the entirety of the data is read, everything functions as normal.  If the filesize is exceeded, an exception is raised.  It would make a defensive programming concept relatively accessible.

This is in line with stdlib functions like `file.read` and `httlib.read` (and consquently urllib) that all support a form of ranged reads without having to loop over chunks.  It is possible to do a ranged read on the `raw` property of the response object, but it's a looped chunk read and it appears to possibly have the potential for unintentionally changing some internal flags on the response object and create issues with future calls to `.content`.

I think features/recipes like this could work in requests-toolbelt if there were an ability/hook to register consumed content into the response object so that the API doesn't change for end users.  For example, if I were to read everything by chunks into a variable, I could them call ""Response._register_content(content)"" which would set  `Response._content`, and `Response._content_consumed`.  
",jvanasco,Lukasa
2326,2014-11-07 17:00:47,"@Lukasa it may be CPython being overly cautious but it will be a problem on PyPy. Stuff like [this](https://bugs.launchpad.net/dateutil/+bug/1376343) can cause an application to crash because there are too many file descriptors open. Others can explain why this is important to PyPy better than I can, but I'm very confident this is a worth while change.

My only qualm is worrying about users who may be accessing `r.raw._pool`. Granted they shouldn't be, but I'm worried about an influx of users who will complain that we broke X.
",sigmavirus24,Lukasa
2324,2014-11-07 14:11:20,"@tcalmant the `exec_` method is used in `six` though, which is used by `urllib3`. The latest version of six [still uses `sys._getframe`](https://bitbucket.org/gutworth/six/src/c17477e81e482d34bf3cda043b2eca643084e5fd/six.py?at=default#cl-647). You should raise this bug there and then **if you are successful** in getting it fixed, create a bug on [`urllib3`](/shazow/urllib3) to update the copy of six it vendors.
",sigmavirus24,tcalmant
2315,2014-11-18 05:02:26,"@jkroening what DNS do you have configured? Could you try using Google's DNS (or someone else's)?
",sigmavirus24,jkroening
2315,2014-11-18 05:09:13,"yeah, you're right to think that's the issue. I contacted my ISP about this because I don't run into the slow requests speed on a VPN. seems to be a problem on their end. thanks for checking up on this. it can be closed.

## 

Jonathan Kroening  ::  jonathankroening.com

On Mon, Nov 17, 2014 at 9:02 PM -0800, ""Ian Cordasco"" notifications@github.com wrote:

@jkroening what DNS do you have configured? Could you try using Google's DNS (or someone else's)?

—
Reply to this email directly or view it on GitHub.
",jkroening,jkroening
2315,2014-11-18 05:13:16,"Thanks @jkroening . Hope you have some luck with your ISP.
",sigmavirus24,jkroening
2314,2014-11-05 01:00:14,"@raulcd I agree.
",sigmavirus24,raulcd
2313,2015-01-12 17:08:20,"And @kampde thanks searching for prior issues and for not opening a new issue.
",sigmavirus24,kampde
2313,2015-01-13 14:22:04,"@kampde after a quick skim, that is the correct RFC. As you can see it is 18 years old.
",sigmavirus24,kampde
2313,2015-07-10 12:55:20,"@zhangchunlin What does 'most servers' mean? Which servers? Which versions of those servers? Why don't they implement RFC 2231?
",Lukasa,zhangchunlin
2313,2015-07-10 13:16:20,"@zhangchunlin if those servers do not implement a standard that is 18 years old, I fail to see why we should be forced to violate the standard.
",sigmavirus24,zhangchunlin
2313,2015-07-10 14:44:35,"@Lukasa  OK, I didn't test so much, my statement maybe wrong.
I just found that the behavior of requests wasn't same as browser(for example chrome), what  I thought is that the method chrome using is workable.

@sigmavirus24 I will try to make clear and submit issue to those server if needed.
",zhangchunlin,Lukasa
2313,2015-07-10 14:44:35,"@Lukasa  OK, I didn't test so much, my statement maybe wrong.
I just found that the behavior of requests wasn't same as browser(for example chrome), what  I thought is that the method chrome using is workable.

@sigmavirus24 I will try to make clear and submit issue to those server if needed.
",zhangchunlin,sigmavirus24
2313,2015-10-26 14:38:17,"@WishCow I'm not certain what result you expect to see if you're filing a PHP bug against another project. It seems frameworks in Perl, Ruby, and Python all appropriately support RFC 2231. If PHP 5.6.14 doesn't support an 18 year old standard, you should file a bug with PHP.
",sigmavirus24,WishCow
2313,2015-10-26 14:44:45,"@WishCow you'll probably have a better time putting together some minimal bit of PHP code and [filing a bug with PHP](https://bugs.php.net/). This comment will help others, but filing a bug to get this fixed in PHP would help a lot more people.
",sigmavirus24,WishCow
2313,2015-10-26 15:41:23,"@WishCow right, that's what I meant (instead of using curl use PHP).
",sigmavirus24,WishCow
2312,2016-01-27 11:33:23,"@skvsn Is your key encrypted? That is, does it have a passphrase?
",Lukasa,skvsn
2312,2016-01-27 11:53:27,"@Lukasa Thanks a lot. It works perfectly with unencrypted key. Probably worth mentioning this in the docs.
",skvsn,Lukasa
2309,2014-10-28 22:06:09,"So, @kennethreitz could you give me a bit more direction regarding what you want me to do with this? Should I default disable the warning everywhere and then provide a function for users to enable them? 
",sigmavirus24,kennethreitz
2307,2014-10-26 22:51:43,"This was extremely subtle and something none of us caught. I'm sorry for the inconvenience @itaiin.

Thanks for figuring this out @kevinburke. I agree with your reasoning about not retrying a POST request for those exact reasons.
",sigmavirus24,kevinburke
2307,2014-10-26 22:51:43,"This was extremely subtle and something none of us caught. I'm sorry for the inconvenience @itaiin.

Thanks for figuring this out @kevinburke. I agree with your reasoning about not retrying a POST request for those exact reasons.
",sigmavirus24,itaiin
2307,2014-10-28 09:51:40,"@kevinburke thanks for all the effort!
",itaiin,kevinburke
2304,2014-10-25 12:10:27,"@sigmavirus24 I think this is different than SNI.
@Fuzion24 wants, that requests accepts certificates for `*.example.com` when requesting `a.b.c.example.com`. The standard defines, that `*` is _not_ recursive and only matches one level.

The problem with the example is, that with SNI we get a certificate which is valid for `*.clients.google.com` which is correct according to the standard.
Without SNI get one which is only valid under the requested validation rules.

Personally I am -1 on this.
",t-8ch,Fuzion24
2304,2014-10-25 12:10:27,"@sigmavirus24 I think this is different than SNI.
@Fuzion24 wants, that requests accepts certificates for `*.example.com` when requesting `a.b.c.example.com`. The standard defines, that `*` is _not_ recursive and only matches one level.

The problem with the example is, that with SNI we get a certificate which is valid for `*.clients.google.com` which is correct according to the standard.
Without SNI get one which is only valid under the requested validation rules.

Personally I am -1 on this.
",t-8ch,sigmavirus24
2304,2014-10-26 01:28:54,"Yeah, @t-8ch, we're in agreement here. =D
",sigmavirus24,t-8ch
2303,2016-04-08 15:26:21,"@vvolkman it seems as though this is ""Not a bug"" to the upstream CPython develoeprs. Their recommendation is for people to [stop using `__del__`](https://bugs.python.org/msg240246) and instead use `weakref.finalize`. I'm going to check to ensure this doesn't actually affect requests.
",sigmavirus24,vvolkman
2301,2014-10-24 14:29:53,"@Lukasa I just ran the tests and it's arising from `test_prepare_unicode_url` because it's creating a `PreparedRequest` and then calling `prepare` without passing any hooks. This is a poorly written test, not something that should be fixed here.
",sigmavirus24,Lukasa
2299,2014-10-24 14:52:45,"@Lukasa I'd really like to see something like `urllib3`s LRUCache used here as well.
",sigmavirus24,Lukasa
2299,2014-10-24 14:54:05,"Also thank you so very much for this @mattrobenolt 
",sigmavirus24,mattrobenolt
2299,2014-10-24 14:57:39,"I'll switch this up in a bit. I wasn't aware there was an LRUCache as a part of urllib3 already.

@sigmavirus24 do you have an opinion on the size? Is 10k alright? That number is completely arbitrary. 
",mattrobenolt,sigmavirus24
2299,2014-10-27 00:34:55,"@mattrobenolt why not cap it a bit lower? Or is 10k already ""low""?
",sigmavirus24,mattrobenolt
2299,2014-10-27 16:23:57,"@sigmavirus24 sorry for the delay. Updated to use the `RecentlyUsedContainer` instead and I dropped the cache size down to 1000. We could also make this configurable on the `Session`.
",mattrobenolt,sigmavirus24
2299,2014-10-27 16:32:22,"> We could also make this configurable on the Session

I'd rather we didn't.

@Lukasa this looks good to me. How do you like it?
",sigmavirus24,Lukasa
2299,2014-11-07 04:44:40,"LGTM. @Lukasa ?
",sigmavirus24,Lukasa
2299,2014-11-07 08:08:15,"@kevinburke it's an LRU (least recently used) algorithm. So that means that the least used objects get evicted first. Active/hot objects stay in. Similar to how memcached works. So there is no error. Objects are just evicted based on LRU.
",mattrobenolt,kevinburke
2299,2014-11-08 04:22:27,"Thank you @mattrobenolt !
",sigmavirus24,mattrobenolt
2297,2014-10-22 20:36:20,"> Would it be useful to add a keywork argument specifying if the url is encode, unencoded, or current behavior of unknown?

No. In spite of RFC 3986 there wouldn't be a good way for requests to always encode a URL. As it is now, you should be encoding this yourself as @Lukasa already pointed out. We will not be adding extra keyword arguments and we will not guess this for you.
",sigmavirus24,Lukasa
2296,2014-10-22 16:16:59,"@tgs thanks. We haven't had the reason to do a new release yet. When we do, we'll pull this in.

Until we do, if you know the encoding you can simply do `response.encoding = 'utf8'`
",sigmavirus24,tgs
2293,2014-10-21 12:28:28,"That fix is unnecessary. =)

If you simply install pyopenssl, pyasn1 and ndg-httpsclient, requests will automatically perform SNI for you. Alternatively, pip install requests with the 'security' optional dependencies: `pip install requests[security]`.

Thanks for your help @KoesterD!
",Lukasa,KoesterD
2291,2014-10-21 04:22:40,"@sigmavirus24 I am asking about a pyhton-requests functionality. Can you please give me a simple yes or no answer if this is possible?
",HulaHoopWhonix,sigmavirus24
2291,2014-10-21 05:39:17,"@HulaHoopWhonix We know what you're doing, we're just asking you to take the question to the appropriate location. GitHub issues are for feature requests, bug reports and code enhancements. They are not a forum for questions.
",Lukasa,HulaHoopWhonix
2291,2014-10-21 12:23:51,"@Lukasa Ok I see what you mean. Any input for this question on stackoverflow would be appreciated. I posted it here:

https://stackoverflow.com/questions/26479039/python-requests-direct-pem-pinning-with-self-signed-cert
",HulaHoopWhonix,Lukasa
2290,2014-10-19 09:03:32,"@syedsuhail Quick question, why'd you change the width?
",Lukasa,syedsuhail
2289,2014-10-19 07:56:35,"Thanks for this @syedsuhail! :cake:
",Lukasa,syedsuhail
2289,2014-10-19 09:03:59,"@syedsuhail I'm glad you chose to contribute to requests!
",Lukasa,syedsuhail
2283,2014-10-15 07:09:11,"@t-8ch I haven't read up on POODLE yet: are cookies enough?
",Lukasa,t-8ch
2282,2014-10-14 15:26:26,"Thanks @morganthrapp! :cake: 
",sigmavirus24,morganthrapp
2278,2015-03-09 18:25:48,"@dav009 two options:
1. Use cChardet instead of `apparent_encoding`
2. Set a custom encoding if you know it to prevent requests from needing to use `apparent_encoding`
",sigmavirus24,dav009
2278,2015-03-09 18:26:13,"@dav009 one other alternative, help chardet fix the bug on their side.
",sigmavirus24,dav009
2277,2014-10-13 02:24:53,"@CarlFK what would that even return?
",sigmavirus24,CarlFK
2276,2014-10-13 02:15:48,"@mikecool1000 why did you close this?
",sigmavirus24,mikecool1000
2276,2014-10-13 18:00:57,"I just didn't want try to submit work I am unsure of

Sent from my iPhone

> On Oct 12, 2014, at 7:16 PM, Ian Cordasco notifications@github.com wrote:
> 
> @mikecool1000 why did you close this?
> 
> —
> Reply to this email directly or view it on GitHub.
",mikecool1000,mikecool1000
2275,2014-10-12 17:41:08,"Hi @squareproton 

Let me start off by saying that we've had this requested in the past (see: #1938) so you would do well to read that discussion to understand why we will not ever add content-length checking to requests. (This is also vaguely related to #2255 and https://github.com/sigmavirus24/requests-toolbelt/issues/41.)

If you read the thread that resulted from the last time this was suggested, you'll see that we do not add arbitrary or niche features to requests. If there's a high likelihood of users needed it (something greater than 90% of our current userbase) then we'll add it. You might understand how this makes requests far more maintainable, especially given we don't do much in the way of header checks.

What I am going to address here is the following:

> Requests is already doing some checks on Content-Length. When a response body is larger than a Content-Length header describes requests silently truncates the body.

I'm fairly certain you're misunderstanding what you're receiving. Of course, if you can provide an example that reliably reproduces that, this would be most appreciated. I have never personally seen this behaviour, and I'd be surprised to hear that we're truncating large bodies. If that's happening anywhere, it might be httplib but I doubt it happens there either.
",sigmavirus24,squareproton
2275,2014-10-12 18:18:55,"@Lukasa I was under the same impression frankly
",sigmavirus24,Lukasa
2272,2014-10-11 14:46:04,"So actually, the catch here is import/monkey patch order.

I'm willing to be the following will fix this:



@ilovenwd if you don't `patch_all` before import requests, it probably won't replace any usage of `RLock` used in the `LifoQueue` which is imported from `threading`.
",sigmavirus24,ilovenwd
2264,2014-10-05 01:27:30,"@sigmavirus24 Thanks for giving me pointers for troubleshooting. The file I was originally using was haphazardly called 'urllib.py'. Once I ran Requests from a different directory it worked fine.
",evanemolo,sigmavirus24
2264,2014-10-05 01:29:42,"@evanemolo it was a lucky guess. :)
",sigmavirus24,evanemolo
2264,2014-10-05 01:43:05,"The error message for this is really bad. I wish they'd make it more clear
that you're shadowing a module name.

On Saturday, October 4, 2014, Ian Cordasco notifications@github.com wrote:

> @evanemolo https://github.com/evanemolo it was a lucky guess. :)
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/2264#issuecomment-57923709
> .

## 

## 

Kevin Burke
phone: 925.271.7005 | twentymilliseconds.com
",kevinburke,evanemolo
2264,2014-10-05 01:51:44,"@kevinburke it's doubtful that this will be updated in 2.7 and in 3.x it's a non-issue. I tried adding `from __future__ import absolute_import` and that doesn't fix it either. There's not much we can do here.
",sigmavirus24,kevinburke
2263,2014-10-03 18:50:43,"This looks great @daftshady, thanks! :cake: I'll let @sigmavirus24 review, but I'm :+1:.
",Lukasa,daftshady
2263,2014-10-05 07:07:09,"@sigmavirus24 I agree with you. It's fairly reasonable to use a set here. I added another commit.
",daftshady,sigmavirus24
2263,2014-10-05 16:52:28,"Looks great now! Thanks @daftshady!
",sigmavirus24,daftshady
2262,2014-10-03 13:50:01,"This should be fixed in chardet now (I just remembered why this seemed familiar, it's because of https://github.com/chardet/chardet/issues/30). That said, this really is a chardet issue, as we've already said and as @Lukasa said, we do the right thing when you tell us you want JSON from the body. Other than that, we have to assume it falls under the domain of HTTP and that explicitly requires a `charset` parameter in `Content-Type`. We have no intention of every doing anything more than using that or trying to guess the encoding with chardet. We already do not introspect the body automatically (although we provide that as a convenience function) for meta tags or anything else that would hint the character encoding and we provide you with the ability to tell us what encoding to use.

There's enough degrees of freedom to do what you want to do. Further, if you're only ever using a session to talk to your API, you could (if you desired) do something like:



If you're not using a session, you can do:


",sigmavirus24,Lukasa
2262,2014-10-03 14:10:38,"Good to know it's fixed in chardet, thanks @sigmavirus24. Yeah, I have ways of detecting the encoding in my client code. I'll close this now.

Thanks for your time, guys.
",noseworthy,sigmavirus24
2261,2014-10-02 19:43:29,"Here I was writing a nice response and @Lukasa said it faster and better... again. ;)

Thanks for the suggestion @dequis and taking the time to file the bug. That said, you said one thing that @Lukasa didn't address and which bothered me. 

We should take the following conversation off this bug because it's entirely tangential. _Why are you copying recipes from ActiveState to do `multipart/form-data` file uploads?_ Requests provides that for you and the requests-toolbelt ensures you don't need to think about it in the case where your file is too big to be loaded into memory all at once. Feel free to email me and reply to this. 
",sigmavirus24,Lukasa
2261,2014-10-02 19:43:29,"Here I was writing a nice response and @Lukasa said it faster and better... again. ;)

Thanks for the suggestion @dequis and taking the time to file the bug. That said, you said one thing that @Lukasa didn't address and which bothered me. 

We should take the following conversation off this bug because it's entirely tangential. _Why are you copying recipes from ActiveState to do `multipart/form-data` file uploads?_ Requests provides that for you and the requests-toolbelt ensures you don't need to think about it in the case where your file is too big to be loaded into memory all at once. Feel free to email me and reply to this. 
",sigmavirus24,dequis
2260,2014-10-02 12:49:03,"@Hasimir is correct. Please consult the documentation before raising bugs. =)
",Lukasa,Hasimir
2259,2014-10-01 10:15:45,"@tarzanjw The short answer is because it doesn't know which header to send until it gets the 401 challenge. It's not _necessary_ that it do this, it could send Basic by default.
",Lukasa,tarzanjw
2258,2014-10-05 16:49:29,"@willingc thank you so much for your hard work on this. :cake: (Those emoji from @kennethreitz are for you ;))
",sigmavirus24,kennethreitz
2258,2014-10-05 17:50:15,"Thank you @sigmavirus24 for your support and mentoring :guitar: :sunrise: :evergreen_tree: , @lukasa for the review :cat: :cat2: :smile_cat:  , and @kennethreitz for the community atmosphere :camera: :v: :guitar:
",willingc,kennethreitz
2258,2014-10-05 17:50:15,"Thank you @sigmavirus24 for your support and mentoring :guitar: :sunrise: :evergreen_tree: , @lukasa for the review :cat: :cat2: :smile_cat:  , and @kennethreitz for the community atmosphere :camera: :v: :guitar:
",willingc,sigmavirus24
2258,2014-10-05 18:42:33,"@willingc thank YOU, and you're welcome :)
",kennethreitz,willingc
2257,2014-09-30 16:00:54,"@gagoman someone had been working on this for a while and stalled. I've opened a PR on their behalf. Did you read the entirety of the issue?
",sigmavirus24,gagoman
2257,2014-09-30 16:08:53,"@sigmavirus24 reopen if it is still required.
",gagoman,sigmavirus24
2257,2014-09-30 16:12:16,"@gagoman the discussion said the work would be slow but that two people were owning the work. The proper etiquette would have been to ask if they were still working in it. Thanks for your contribution, but we'll be continuing forward with their work.
",sigmavirus24,gagoman
2257,2014-09-30 16:17:19,"@sigmavirus24 got it.
",gagoman,sigmavirus24
2253,2014-09-27 19:58:41,"Hey @yossigo thanks for the pull request!

While you're here, could you fix something that I think is a bug? On [this line](https://github.com/yossigo/requests/commit/c28da22e9c42e22b303bb07da434ce65e10c0cb2#diff-9f3a95293a5d26032b1c588167760362R193) could you change it to



We still want to check to prevent users from being caught in an endless challenge state with a server.

Right now, I'm trying to decide if I think `handle_302` should check the response code before resetting the attribute because it will be called regardless of whether a 302 was received or not.

Also, @yossigo what do you think about setting `num_401_calls` to 1 in `handle_302` instead of deleting the attribute? It would be much simpler. Also, there's no need to return `r` from `handle_302`.
",sigmavirus24,yossigo
2253,2014-10-25 14:10:06,"I think this is ready to merge. Sorry I didn't see your update @yossigo 

Thanks for this!
",sigmavirus24,yossigo
2252,2014-09-27 19:20:15,"Hi @kostyll thanks for the change! Unfortunately this is a change that belongs upstream in [urllib3](/shazow/urllib3).
",sigmavirus24,kostyll
2251,2014-09-27 19:19:47,"Hey @kostyll can you tell us what version of Python you're using?
",sigmavirus24,kostyll
2251,2014-09-29 01:46:25,"This is not a bug in requests or urllib3. If you need help with this @kostyll you should see if there are any ST3 users who can help you with this. Better yet, I would advise not using the vendored copy of Python with ST3.
",sigmavirus24,kostyll
2250,2014-09-26 20:21:09,"@hariharshankar if you can solve the sole issue here without also addressing #1980 that will be fine. If you can fix the parsing so extensions of 5988 to handle case [c], then that's great. As you can tell from #1980 cases [a] and [b] are not cases we're willing to tackle as a core part of the library.
",sigmavirus24,hariharshankar
2249,2014-09-26 16:12:45,"@t-8ch that's roughly `rfc3986`'s API, except that I don't think I call it `replace` but maybe I do.... I'll double check later.
",sigmavirus24,t-8ch
2249,2014-09-26 16:31:19,"@sigmavirus24 Please open the specific issue with urllib3's url parser, no need to be backhanded. :)
",shazow,sigmavirus24
2249,2014-09-26 16:35:36,"Sorry @shazow, it wasn't meant to be back handed. I'll pull together the list of things the object is missing and make an issue with it tonight.
",sigmavirus24,shazow
2249,2014-10-05 16:48:51,"@kennethreitz :+1: 
",sigmavirus24,kennethreitz
2247,2014-09-25 15:44:10,"

Note that this doesn't seem to be reproducable. @cool-RR can you share the code and requests version that caused this?
",sigmavirus24,cool-RR
2247,2014-09-25 15:45:51,"Ian, can you please check using something like Fiddler the true Host of the
request?

On Thu, Sep 25, 2014 at 6:44 PM, Ian Cordasco notifications@github.com
wrote:

> > > > import requests>>> r = requests.get('https://username:foo@httpbin.org/get')>>> r.json()['headers']{'Accept': '_/_', 'Accept-Encoding': 'gzip, deflate', 'User-Agent': 'python-requests/2.4.1 CPython/3.3.2 Darwin/13.3.0', 'Authorization': 'Basic dXNlcm5hbWU6Zm9v', 'X-Request-Id': 'fb06edf3-14c8-42ad-b3c5-fefa801ddd9a', 'Connection': 'close', 'Host': 'httpbin.org'}>>> r.request.headers{'Authorization': 'Basic dXNlcm5hbWU6Zm9v', 'Accept': '_/_', 'Accept-Encoding': 'gzip, deflate', 'Connection': 'keep-alive', 'User-Agent': 'python-requests/2.4.1 CPython/3.3.2 Darwin/13.3.0'}
> 
> Note that this doesn't seem to be reproducable. @cool-RR
> https://github.com/cool-RR can you share the code and requests version
> that caused this?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/2247#issuecomment-56838438
> .
",cool-RR,cool-RR
2247,2014-09-25 15:47:22,"@cool-RR I'll do that when you can give me the version and the code. The true host received by httpbin is what is printed on the 4th line. Also, it would look as though we're stripping the auth out and creating an Authorization header as we should be, before we even pass the URL on to urllib3.
",sigmavirus24,cool-RR
2247,2014-09-25 15:51:33,"Python 2.7.6 (default, Nov 10 2013, 19:24:24) [MSC v.1500 64 bit (AMD64)]
on win32
Type ""copyright"", ""credits"" or ""license()"" for more information.
DreamPie 1.2.1

> > > import requests
> > > requests.**version**
> > > 0: '2.4.1'
> > > requests.get('http://yo:pw@google.com')
> > > 1: <Response [400]>

Request in Fiddler raw view:

GET http://yo:pw@google.com/ HTTP/1.1
Host: yo:pw@google.com
Connection: keep-alive
Accept: _/_
Accept-Encoding: gzip, deflate
Authorization: Basic eW86cHc=
User-Agent: python-requests/2.4.1 CPython/2.7.6 Windows/7

Also Fiddler shows another warning about the port not being specified. Also
should the credentials show in the first line too?

On Thu, Sep 25, 2014 at 6:47 PM, Ian Cordasco notifications@github.com
wrote:

> @cool-RR https://github.com/cool-RR I'll do that when you can give me
> the version and the code. The true host received by httpbin is what is
> printed on the 4th line. Also, it would look as though we're stripping the
> auth out and creating an Authorization header as we should be, before we
> even pass the URL on to urllib3.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/2247#issuecomment-56838948
> .
",cool-RR,cool-RR
2247,2014-09-25 16:26:42,"The full request URL is only sent when sent to a proxy. I presume you have your `$HTTP_PROXY` environment variable set to Fiddler. We've detected that and used our proxy-specific handling. This is why @sigmavirus24 couldn't reproduce your behaviour. 
",Lukasa,sigmavirus24
2244,2014-09-23 14:52:33,"@RuudBurger both @Lukasa and I have our emails available on our GitHub profiles. If you'd rather use PGP, you can find my PGP key (and associated email address) by searching https://pgp.mit.edu/ for my real name.
",sigmavirus24,Lukasa
2244,2014-09-23 14:52:33,"@RuudBurger both @Lukasa and I have our emails available on our GitHub profiles. If you'd rather use PGP, you can find my PGP key (and associated email address) by searching https://pgp.mit.edu/ for my real name.
",sigmavirus24,RuudBurger
2244,2014-09-24 08:39:10,"In my opinion an exception needs to be raised and `RedirectLoopError` is always going to be a `TooManyRedirects` at the end, if you want to subclass it is fine but maybe unnecessary.

I like @sigmavirus24 set() solution, maybe you want to pack it a bit more:


",fcosantos,sigmavirus24
2244,2014-09-24 13:08:48,"> RedirectLoopError is always going to be a TooManyRedirects at the end

@fcosantos I don't quite understand what you mean. Could you clarify this for me?
",sigmavirus24,fcosantos
2244,2014-09-24 15:25:20,"@Lukasa I can think of a couple ways to make it influence the max_redirects count.
1. Move the logic into `Session#resolve_redirects`. This unfortunately would mean actually using the network for the first (possibly permanent) redirect.
2. Keep count along with the set of how many times we go through that loop and add a new optional parameter to `Session#resolve_redirects` along the lines of `redirects_already_followed=0`. We can then initialize `i` in `Session#resolve_redirects` with that and that will affect the max number of redirects possible (including using the cache).

Option 2 seems most practical, but I just loathe adding more arguments (that could confuse a user) to a public API like this.

Also, I think there's value in using a subclass of `TooManyRedirects`. I can see an instance where this might cause confusion because of a case like @RuudBurger has. In a browser, it might very well work just fine, but because of oddities in the usage of requests the loop is caused by the redirect cache. I think providing users a way to disambiguate where the error is actually coming from is very useful (and a better experience).
",sigmavirus24,Lukasa
2244,2014-09-24 15:25:20,"@Lukasa I can think of a couple ways to make it influence the max_redirects count.
1. Move the logic into `Session#resolve_redirects`. This unfortunately would mean actually using the network for the first (possibly permanent) redirect.
2. Keep count along with the set of how many times we go through that loop and add a new optional parameter to `Session#resolve_redirects` along the lines of `redirects_already_followed=0`. We can then initialize `i` in `Session#resolve_redirects` with that and that will affect the max number of redirects possible (including using the cache).

Option 2 seems most practical, but I just loathe adding more arguments (that could confuse a user) to a public API like this.

Also, I think there's value in using a subclass of `TooManyRedirects`. I can see an instance where this might cause confusion because of a case like @RuudBurger has. In a browser, it might very well work just fine, but because of oddities in the usage of requests the loop is caused by the redirect cache. I think providing users a way to disambiguate where the error is actually coming from is very useful (and a better experience).
",sigmavirus24,RuudBurger
2244,2014-09-24 16:15:38,"@Lukasa good point. I'm not sure we can actually reconstruct the history accurately unless we also cache responses in the redirect cache. In other words, we'd have a cache something like:



And a big problem with that would be timestamps in headers and such (e.g., cookies). All of this which makes me wonder exactly how good an idea it is to keep the redirect cache around.
",sigmavirus24,Lukasa
2242,2015-04-22 18:54:36,"@xnulinu would you mind linking the feature request here?
",Estevo-Aleixo,xnulinu
2241,2014-09-22 19:12:25,"@tijko so could you update this PR to add the described exception to `requests/exceptions.py` and then use it here where you're raising the `RuntimeError`? Thanks in advance
",sigmavirus24,tijko
2241,2014-10-05 16:47:25,"@sigmavirus24 looks like @tijko updated this as requested
",kennethreitz,sigmavirus24
2241,2014-10-05 16:47:25,"@sigmavirus24 looks like @tijko updated this as requested
",kennethreitz,tijko
2241,2014-10-05 16:48:31,"@kennethreitz looks good to me
",sigmavirus24,kennethreitz
2239,2014-10-20 22:02:48,"@sigmavirus24 is right. We are still arguing about whether or not that was ok.
",Lukasa,sigmavirus24
2238,2014-09-20 23:37:55,"I agree with @Lukasa that this is the behaviour we want. We absolutely want `unicode` urls because on Python 3, we can handle IRIs. We would need to adopt something that implements RFC 3987 to support it on Python 2, but once we did, we would be able to support them there too. For example, http://☃.net should be supported. (Your browser, and Python 3 should properly ""encode"" that to http://xn--n3h.net/.)
",sigmavirus24,Lukasa
2238,2014-09-30 18:49:08,":heart: @buttscicles 

@Lukasa this looks okay to me. Thoughts?
",sigmavirus24,Lukasa
2236,2014-09-19 12:52:30,"As @sigmavirus24 says, lots and lots of projects hook into requests and we very definitely don't want to include them in requests. =)
",Lukasa,sigmavirus24
2235,2014-09-21 11:58:16,"UTF8 is the most reasonable default.
Besides, python3 string defaults to unicode, many data read from db/http is auto convert to unicode(default utf8).
so, why not accept utf8 as default unicode encoding?
The python standard library ALREADY AUTO convert unicode to utf8 when write to socket.
(that why the chunk size is wrong, but the chunk body is ok)

@sigmavirus24 this bug only appears when using generator as data (chunked encoding)
post data=unicode works because 

> The python standard library ALREADY AUTO convert unicode to utf8 when write to socket.
",ilovenwd,sigmavirus24
2235,2014-09-21 14:00:31,"> Getting weird server errors is worse than us blowing up and saying ""you have to give us binary data!""

Yeah I'm surprised we haven't had more bug reports about this frankly. Like I said, I think we should follow a deprecation pattern for this behaviour for 2.5 and 2.6, then make it default in 2.7 (or 3.0).
- I think we should issue a `DeprecationWarning` when we receive `data` whose type is not `bytes`. We should then immediately try to encode the data for the user.
- For the case that @ilovenwd is encountering (using a generator) we should issue _1_ deprecation warning after the first chunk and then encode the data for the user.
- In the case of the user passing a file(-like) object to `data`, we should check the mode to ensure it was opened with `'b'` or is an instance/subclass of `BytesIO`. This case is tougher because some portions of it may be handled by the generator case (i.e., some users don't define `__len__` on `BytesIO` subclasses and so they're treated as generators.).

Once we have a `json` parameter, we can confidently handle that ourselves, for the user.
",sigmavirus24,ilovenwd
2234,2014-09-19 01:49:52,"@vdanen we remove `None` before passing the headers on to `urllib3`. `urllib3` does not do the same processing that we do on headers before passing them on to `httplib`/`http.client` (as it shouldn't). There's no way to pass this on to urllib3 in such a way that it can also tell `httplib` that. Is there any `Accept-Encoding` value that this server won't balk at?
",sigmavirus24,vdanen
2233,2016-01-26 01:33:36,"Thanks @jonathan-s. Those of us interested in this were already aware of requests 3.0 drawing close.
",sigmavirus24,jonathan-s
2230,2014-09-18 06:09:32,"Sorry @kevinburke, it's not clear to me what the bug is here.
",Lukasa,kevinburke
2230,2014-09-18 06:45:43,"A ConnectionError implies that the server never received/began acting on
data. This way all ConnectionErrors are safe to retry.

imagine you send a POST request to a server and then wait for a response
for 30 seconds, at which point the server closes the connection without
sending data, headers, anything. Requests will raise a ConnectionError. Is
that appropriate?

On Wednesday, September 17, 2014, Cory Benfield notifications@github.com
wrote:

> Sorry @kevinburke https://github.com/kevinburke, it's not clear to me
> what the bug is here.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/2230#issuecomment-55998776
> .

## 

## 

Kevin Burke
phone: 925.271.7005 | twentymilliseconds.com
",kevinburke,kevinburke
2229,2015-04-07 08:27:31,"Here's what I was able to come up with.

According to [RFC 7230](https://tools.ietf.org/html/rfc7230#section-5.3.2):

> When making a request to a proxy, other than a CONNECT or server-wide OPTIONS request (as detailed below), a client MUST send the target URI in absolute-form as the request-target.... An example absolute-form of request-line would be:
> GET http://www.example.org/pub/WWW/TheProject.html HTTP/1.1

But the `HTTPAdapter.proxy_manager_for` method uses urllib3's `ProxyManager`, which uses CONNECT for https requests. [A paragraph down in RFC 7230](https://tools.ietf.org/html/rfc7230#section-5.3.3):

> When making a CONNECT request to establish a tunnel through one or more proxies, a client MUST send only the target URI's authority component (excluding any userinfo and its ""@"" delimiter) as the request-target.  For example,
> CONNECT www.example.com:80 HTTP/1.1

Therefore, it looks to me like https requests should always only pass the authority-form of the request target (i.e. the path, not the full url). Then @newsham is seeing a bug in the https requests are handled. Am I missing anything?

However, I wasn't able to reproduce the problem. On my own installation of BURP, the demonstration script created two requests, both of which were logged by BURP as only including the path (correct behavior). Furthermore the method was listed as GET for the https request, but I was expecting CONNECT. Could this be my unfamiliarity with BURP?
",benjaminran,newsham
2229,2015-04-07 11:33:11,"@benjaminran Nope, it's a corner case in how requests handles proxies.

If you pass a proxy whose proxy url scheme is 'https://', we don't use CONNECT to tunnel through it, because what's the point? Instead, we connect directly to it over TLS and then treat it like a plaintext proxy, which explains the behaviour you're seeing.
",Lukasa,benjaminran
2228,2014-09-16 15:38:11,"@Lukasa I'm surprised that we don't use the default socket timeout. I'm more surprised that the ""default"" timeout is not automatically applied to a socket. That said, I don't think there's a good (or easy way) to handle adding this at this juncture. We could consider it for 3.0 but if we truly have no timeout set on sockets by default (which I'm prepared to believe), then we can't change it easily and it _should_ be documented for now.

The thing we should determine is whether we want that behaviour at all. The sad thing is that more often than not, not being able to put a timeout on `socket.getaddrinfo` has affected me more than a connection not timing out appropriately. 
",sigmavirus24,Lukasa
2228,2014-09-16 16:44:24,"I disagree that it is sufficient as timeouts  are only mentioned at the
very bottom of the page.  Users often base their solutions off of examples
and don't read all the documentation, only what they need to get a result.
 While examples should be brief, they should not lead a user to writing
code that has a pretty strong potential to lock up.

The default socket timeout is used by urllib2 when a timeout is not
specified. This further compounds the above problem for people moving from
that library.  To have no timeout specified I can see either None or 0
being passed as the timeout value.  Explicit is better than implicit.

On Tue, Sep 16, 2014 at 9:26 AM, Cory Benfield notifications@github.com
wrote:

> Thanks for the suggestion! However, I disagree.
> 
> In many cases, users find it valuable to add a timeout to their calls.
> This is entirely reasonable. However, the quickstart code is not an example
> of 'general use', it's intended to be illustrative and educational. We
> should not obscure the pedagogical point in the pursuit of some form of
> pure correctness.
> 
> There is a timeout section
> http://docs.python-requests.org/en/latest/user/quickstart/#timeouts in
> the quickstart docs, which we believe to be sufficient. Do you disagree?
> 
> As for mentioning the default socket timeout, I hadn't spotted that. That
> may actually be a bug. @sigmavirus24 https://github.com/sigmavirus24,
> what do you think: should we use the default socket timeout when no value
> is provided? If so, how do users specify they want no timeout at all? Am I
> overthinking the whole thing?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/2228#issuecomment-55760567
> .
",mountainpaul,sigmavirus24
2228,2014-09-16 17:25:50,"> @Lukasa I'm surprised that we don't use the default socket timeout. I'm more surprised that the ""default"" timeout is not automatically applied to a socket.

Ordinarily it would be: this is a consequence of the way our library interacts with urllib3. We build a Timeout object with `None, None` as its values and send that down to urllib3. This looks for a sentinel object: if it finds it it'll set the timeout to the default. We aren't sending that sentinel object, we're sending `None, None`, which sets the socket timeout to `None`, which means it never times out. A bit sad really.

> Users often base their solutions off of examples and don't read all the documentation, only what they need to get a result.

Yes, they do. However, this is not a justification for all examples being bulletproof. Our examples don't catch all exceptions, they don't list all possible failure cases, and they don't explain some of the ins-and-outs of weirder parts of the library. I'd argue they shouldn't. That overwhelms beginners by giving them far too many things to focus on at once, which prevents them from taking away the key lesson.

Our position is that documentation should not be optimised for copy-and-paste coding sessions. You can see this by the fact that our quickstart code examples usually aren't code blocks, but representations of an interactive session. They are intended as a guide, not an example of how to build a production-ready requests application.

The correct way to handle this is not to litter our code examples with irrelevant noise to avoid a footgun in production code, but to remove the footgun entirely. Users should not be subject to the TCP timeout (often very long) in the _default_ case, they should be subject to something vaguely reasonable (in this case the default socket timeout).

Note also that the timeouts are not a panacea. The timeouts apply to each connection event, but if `getaddrinfo` returns many many IP addresses it can take a _very_ long time to time them all out. This is not a _hang_, but it's frequently very close to one (and indeed we've seen it reported that way in the past).

> Explicit is better than implicit.

This is a point that is extremely unclear with the Python community. Let me show you something that is unarguably explicit:



_Nothing_ happens automatically here, everything is stated explicitly. Realm, the URL to apply to, everything. Here's what we do:



There is so much that is implicit here. The auth type is implicit (Basic), the realm it applies to is not specified (because _no-one cares_, the realm is basically irrelevant on Basic auth), the URL it applies to is inferred by the library (anything on `http://www.example.com/`). But here's the thing: people love our way, and hate urllib2's way.

The Zen of Python is great, but every line of it needs to be followed by asterisks for qualification. It is not the dogma of Python. You cannot wave it at a solution you do not like to force compliance. And for every clause in it that supports your point, I can find plenty that don't:

> Beautiful is better than ugly.
> Simple is better than complex.
> Readability counts.

We have timeouts, and we mention them in their own section in the quickstart guide. Users who care will find it. And if they don't find it, they clearly don't care about errors at all because the ""Errors and Exceptions"" section is _below_ the timeouts section!

Our documentation cannot accommodate all cases, so we optimise. The quickstart guide is just that: a quickstart guide. We cannot prevent users doing something else with it, but that shouldn't change what _we_ do with it.

**tl;dr**: I think the documentation isn't the problem, the default behaviour is the problem.
",Lukasa,Lukasa
2228,2014-09-16 18:17:47,"@Lukasa hit every point on the head. Moving forward we can repurpose this issue to discuss setting the default socket timeout _or_ we can close this and open a new issue to discuss that.
",sigmavirus24,Lukasa
2228,2014-09-16 20:32:29,"On Tue, Sep 16, 2014 at 11:26 AM, Cory Benfield notifications@github.com
wrote:

> @Lukasa https://github.com/Lukasa I'm surprised that we don't use the
> default socket timeout. I'm more surprised that the ""default"" timeout is
> not automatically applied to a socket.
> 
> Ordinarily it would be: this is a consequence of the way our library
> interacts with urllib3. We build a Timeout object with None, None as its
> values and send that down to urllib3. This looks for a sentinel object: if
> it finds it it'll set the timeout to the default. We aren't sending that
> sentinel object, we're sending None, None, which sets the socket timeout
> to None, which means it never times out. A bit sad really.
> 
> Users often base their solutions off of examples and don't read all the
> documentation, only what they need to get a result.
> 
> Yes, they do. However, this is not a justification for all examples being
> bulletproof. Our examples don't catch all exceptions, they don't list all
> possible failure cases, and they don't explain some of the ins-and-outs of
> weirder parts of the library. I'd argue they shouldn't. That overwhelms
> beginners by giving them far too many things to focus on at once, which
> prevents them from taking away the key lesson.
> 
> Our position is that documentation should not be optimised for
> copy-and-paste coding sessions. You can see this by the fact that our
> quickstart code examples usually aren't code blocks, but representations of
> an interactive session. They are intended as a guide, not an example of how
> to build a production-ready requests application.
> 
> The correct way to handle this is not to litter our code examples with
> irrelevant noise to avoid a footgun in production code, but to remove the
> footgun entirely. Users should not be subject to the TCP timeout (often
> very long) in the _default_ case, they should be subject to something
> vaguely reasonable (in this case the default socket timeout).
> 
> I completely agree with this, my suggestion was only for if you decided to
> leave the ""footgun"" in place.   I would not consider adding the one
> parameter, littering the code with irrelevant noise, but this maybe a
> matter of taste as to where to draw the line and can appreciate your point
> of view.
> 
> Note also that the timeouts are not a panacea. The timeouts apply to each
> connection event, but if getaddrinfo returns many many IP addresses it
> can take a _very_ long time to time them all out. This is not a _hang_,
> but it's frequently very close to one (and indeed we've seen it reported
> that way in the past).
> 
> Thanks for bringing that to my attention, and that is an interesting point.
> 
>  Explicit is better than implicit.
> 
> This is a point that is extremely unclear with the Python community. Let
> me show you something that is unarguably explicit:
> 
> Good point. Although I still believe it is better to be explicit when the
> behavior differs from the expected (using the default socket timeout).
> 
> import urllib2auth_handler = urllib2.HTTPBasicAuthHandler()auth_handler.add_password(realm='PDQ Application',
>                           uri='https://mahler:8092/site-updates.py',
>                           user='klem',
>                           passwd='kadidd!ehopper')opener = urllib2.build_opener(auth_handler)urllib2.install_opener(opener)urllib2.urlopen('http://www.example.com/login.html')
> 
> _Nothing_ happens automatically here, everything is stated explicitly.
> Realm, the URL to apply to, everything. Here's what we do:
> 
> import requestsr = requests.get('http://www.example.com/login.html', auth=('klem', 'kadidd!ehopper')
> 
> There is so much that is implicit here. The auth type is implicit (Basic),
> the realm it applies to is not specified (because _no-one cares_, the
> realm is basically irrelevant on Basic auth), the URL it applies to is
> inferred by the library (anything on http://www.example.com/). But here's
> the thing: people love our way, and hate urllib2's way.
> 
> The Zen of Python is great, but every line of it needs to be followed by
> asterisks for qualification. It is not the dogma of Python. You cannot wave
> it at a solution you do not like to force compliance. And for every clause
> in it that supports your point, I can find plenty that don't:
> 
> Beautiful is better than ugly.
> Simple is better than complex.
> Readability counts.
> 
> We have timeouts, and we mention them in their own section in the
> quickstart guide. Users who care will find it. And if they don't find it,
> they clearly don't care about errors at all because the ""Errors and
> Exceptions"" section is _below_ the timeouts section!
> 
> Our documentation cannot accommodate all cases, so we optimise. The
> quickstart guide is just that: a quickstart guide. We cannot prevent users
> doing something else with it, but that shouldn't change what _we_ do with
> it.
> 
> _tl;dr_: I think the documentation isn't the problem, the default
> behaviour is the problem.
> 
> AGREED! I just didn't know if you would be amenable to the change your
> proposing and I whole heartily agree with.

Thank you one and all for your attention to this matter.  It is greatly
appreciated

> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/2228#issuecomment-55779615
> .
",mountainpaul,Lukasa
2228,2014-09-16 20:35:33,"Excellent, I think we're all on the same page! Now we just have to work out whether we can make this change before 3.0 or whether it's likely to break too many people's code. @sigmavirus24, you suggested we'd have to wait until 3.0: what makes you say that?
",Lukasa,sigmavirus24
2228,2016-08-14 15:11:00,"Hi,

sorry if I'm missing the obvious here, I'm rather new to Python (and to requests as well, of course) and I was thinking of picking this up. I noticed that @Lukasa did some changes in July 2015 after the last comment (https://github.com/kennethreitz/requests/issues/commit/e9a1e35f8989691afe99407d9b65a67684aa8bac). After looking at the code I'm not even sure if I'm getting the point here. Is this one still open? What is the desired behavior?
",Brausepaul,Lukasa
2228,2016-08-16 20:14:07,"@Brausepaul I would be delighted to merge a PR that adds a note in the documentation to clarify this.

Hopefully in the longer term we'll get rid of httplib and get to a place where we can set timeouts in a substantially more sensible manner than we currently do.
",Lukasa,Brausepaul
2227,2014-09-16 13:10:21,"@Lukasa I'm -0 about the reference in the quickstart guide to the advanced section. How do you feel about it?
",sigmavirus24,Lukasa
2225,2014-09-15 14:49:19,"@temi4 the following works for me:



Could you describe how that is failing to work? Could you maybe show more code than what you have already shown? Is the URL one that you can share?
",sigmavirus24,temi4
2225,2014-09-15 18:29:45,"@sigmavirus24, thanks
I send this 



at answer I get



how I may look send or not data at request? 

Then I print header



I look this
http://c2n.me/iUQk1B
",temi4,sigmavirus24
2224,2014-09-12 15:47:27,"@blueyed thanks but we do not follow `pep8` in requests. 
",sigmavirus24,blueyed
2224,2014-09-12 16:09:19,"@sigmavirus24 
It's not about pep8, but comes from frosted (and maybe other type (not style) checkers).
",blueyed,sigmavirus24
2223,2014-09-12 16:09:46,"Look good to me. I'll just make sure Jenkins is happy with it and them I'll merge it. Thanks @blueyed 
",sigmavirus24,blueyed
2222,2014-09-12 02:46:45,"@blueyed I elaborated on this change in the issue you opened on shazow/urllib3. This allows for third-party developers to create adapters using the `file` protocol and has the side-effect of allowing `data` URLs as well even though requests will not handle them.
",sigmavirus24,blueyed
2222,2014-09-12 03:05:02,"I see.

So it's probably expected and sane to get a `InvalidSchema: No connection adapters were found for 'localhost:5432'` exception instead of `MissingSchema: Invalid URL 'localhost:5432': No schema supplied. Perhaps you meant http://localhost?` then.

I've thought about adding a regex that would let it fall through, but it doesn't change much in the end.

The expected outcome should get added as test however.

@jvantuyl 

> the error you're getting (although as an AssertionError, which is probably bad form). Perhaps you could replace that assertion with a different error (say UnrecognizedScheme), check for it, then implement the fallback behavior?

I do not understand this. It appears to be a proper exception already (`InvalidSchema`).
Where do you mean to add the check and fallback?

I came to this issue through pip and the handling of its proxy setting initially (IIRC, and unrelated).
",blueyed,jvantuyl
2221,2014-09-12 06:19:58,"My objection to this functional change is simpler than @sigmavirus24's. URLs are not strings, and treating them like strings is how mistakes get made. I think the correct approach would be to try to use `urllib3`'s URL utilities for this.
",Lukasa,sigmavirus24
2221,2014-09-12 13:39:06,"Thanks for your feedback.

@Lukasa 
I've initially used urllib3's `Url` (https://github.com/blueyed/requests/commit/1bcc46e50661a68e08723571ac42d8069d2b4063 - was amended), but then noticed that I would have to handle `auth` explicitly, in the same way it gets done in PrepareRequest (IIRC), and ""simplified"" it.

If urllib3 had a method to ""urlunparse"" (proposed in https://github.com/shazow/urllib3/pull/394), this could be used here. But then it seems that urllib3 would also fail for `//localhost:80`, because my simplification was inspired by its scheme detection: https://github.com/shazow/urllib3/blob/master/urllib3/util/url.py#L111-L113:



For now, I propose to only change the documentation to state that the default scheme is ""http"".
What about the documentation at https://github.com/kennethreitz/requests/blame/master/docs/api.rst#L238-249 ?

As for the approach to simplify it, it could get expanded to also check for `//` at the beginning of url.

Given that the function is meant to only prepend a scheme, it should be enough to detect if a scheme exists already, and that does not require to parse all of the url in the end.

Side note: about the validity/usefulness of `//localhost`: would/could this be interpreted to be `https://localhost` for `https_proxy` and `http://localhost` for `http_proxy`? (relative to the protocol that is being accessed).
([RFC](http://tools.ietf.org/html/rfc3986#page-26)).
",blueyed,Lukasa
2221,2014-09-12 14:38:01,"@Lukasa if you build a good parser for RFC 3986 then it will. [rfc3986](/sigmavirus24/rfc3986) for example parses the examples here with ease:



The only hiccup is that RFC 3986 does not define every URI to have the structure `scheme://authority{/path}{?query}{#fragment}`. A valid URI, for example is: `mailto:user@domain.com:port` and in this case `mailto` is the scheme, so the following happens:



Sadly this isn't a bug and attempting to special case it will lead only to trouble in all likelihood.
",sigmavirus24,Lukasa
2218,2014-09-11 12:37:17,"Thanks for thinking of us anyway @singingwolfboy and say hi to Ned for me. =D
",sigmavirus24,singingwolfboy
2217,2014-09-10 18:37:56,"@Lukasa this is a duplicate of https://github.com/kennethreitz/requests/issues/2117
",sigmavirus24,Lukasa
2216,2014-09-10 16:27:28,"@kevinburke always read the source ;)
",sigmavirus24,kevinburke
2216,2014-09-10 20:02:47,"@kevinburke how does ""If you need to provide granular detail about what to retry and how many times to retry it, import urllib3's `Retry` class and pass that instead."" sound?
",sigmavirus24,kevinburke
2216,2014-09-10 20:16:08,"Good! Id maybe pass a link to the object or the urllib3 docs too

On Wednesday, September 10, 2014, Ian Cordasco notifications@github.com
wrote:

> @kevinburke https://github.com/kevinburke how does ""If you need to
> provide granular detail about what to retry and how many times to retry it,
> import urllib3's Retry class and pass that instead."" sound?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/pull/2216#issuecomment-55174200
> .

## 

## 

Kevin Burke
phone: 925.271.7005 | twentymilliseconds.com
",kevinburke,kevinburke
2216,2014-09-11 17:35:01,"@kevinburke I would guess that `MaxRetryError` inherits from `ProtocolError` somewhere. While I have no strict opinions about the exception hierarchy in urllib3, this could simply mean that we need to re-order our except clauses in requests.
",sigmavirus24,kevinburke
2216,2014-10-25 03:24:02,"@kennethreitz I removed the sentinel object and added a quick test. @kevinburke also pointed out that it makes sense for this to be able to raise a new exception since people will not run into this if they're not using the new logic.
",sigmavirus24,kennethreitz
2216,2014-10-25 03:24:02,"@kennethreitz I removed the sentinel object and added a quick test. @kevinburke also pointed out that it makes sense for this to be able to raise a new exception since people will not run into this if they're not using the new logic.
",sigmavirus24,kevinburke
2216,2014-10-25 04:47:59,"LGTM!

(_pretends to know how to add a cake emoji_)

On Friday, October 24, 2014, Ian Cordasco notifications@github.com wrote:

> @kennethreitz https://github.com/kennethreitz I removed the sentinel
> object and added a quick test. @kevinburke https://github.com/kevinburke
> also pointed out that it makes sense for this to be able to raise a new
> exception since people will not run into this if they're not using the new
> logic.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/pull/2216#issuecomment-60470082
> .

## 

## 

Kevin Burke
phone: 925.271.7005 | twentymilliseconds.com
",kevinburke,kennethreitz
2216,2014-10-25 04:47:59,"LGTM!

(_pretends to know how to add a cake emoji_)

On Friday, October 24, 2014, Ian Cordasco notifications@github.com wrote:

> @kennethreitz https://github.com/kennethreitz I removed the sentinel
> object and added a quick test. @kevinburke https://github.com/kevinburke
> also pointed out that it makes sense for this to be able to raise a new
> exception since people will not run into this if they're not using the new
> logic.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/pull/2216#issuecomment-60470082
> .

## 

## 

Kevin Burke
phone: 925.271.7005 | twentymilliseconds.com
",kevinburke,kevinburke
2216,2014-11-01 14:38:14,"Ping @kennethreitz ;)
",sigmavirus24,kennethreitz
2216,2014-11-12 19:58:21,"@kennethreitz I've rebased this.
",sigmavirus24,kennethreitz
2216,2014-12-24 03:40:39,"@dawncold the API is in feature freeze. Please read up on the current status of the project. We're not expanding or adding new parameters nor will we.
",sigmavirus24,dawncold
2216,2014-12-25 14:52:02,"@sigmavirus24 Sorry, I have searched some places on documents, but found nothing, can you provide me an URL?
",dawncold,sigmavirus24
2216,2014-12-25 16:07:42,"@dawncold searching for ""Feature Freeze"" in the documentation quickly reveals [this section](http://docs.python-requests.org/en/latest/dev/todo/?highlight=freeze#feature-freeze) and searching open and closed issues for (feature freeze) bring up #1165 and #1195.
",sigmavirus24,dawncold
2216,2015-02-06 00:45:42,"@sigmavirus24 no need to be smug :)
",kennethreitz,sigmavirus24
2215,2014-09-10 13:07:14,"Ok @sigmavirus24 , I didn't know your policy about that but that sounds consistent.

> That said, I'm +0 on the proposal of not returning anything that is non-positive from `super_len`.

I totally agree with you. This is definitely not acceptable, `super_len` has specific behavior depending the object it gets. So testing the non-positive value should be considered _only_ when using st_size of the stat of the fileno.

> Sending a chunked upload of zero length is a bit weird, but not the end of the world, and it works fine.

Maybe that's the best solution after all...

...or not guessing the size of a file descriptor no matter what (and let the user deal with it)? This super power may looks handy but at the end this is not an accurate behavior.
",cecton,sigmavirus24
2215,2014-09-11 10:00:52,"No there is no way it would enter in memory... Actually they are database dumps and it seems, some of them are huge like 13GB.

But you're right, it doesn't look complicated to implement. I will think of it if I want to implement it for the server. On werkzeug I can use `request.input_stream` to access the raw body, so it's definitely possible, but I prefer to keep the code clean from very specific handling like this.

But thank you very much for your help @Lukasa here and on urllib3. (^_^)/
",cecton,Lukasa
2215,2014-09-11 11:26:04,"@cecton is there any way that you can determine the size in advance? If so, I may have something you can use without having to implement anything yourself.
",sigmavirus24,cecton
2215,2014-09-12 16:55:20,"@cecton are you okay with me making the name of this bug a bit more clear/descriptive?
",sigmavirus24,cecton
2215,2014-09-12 16:56:28,"Sure!

On Fri, Sep 12, 2014 at 6:55 PM, Ian Cordasco notifications@github.com
wrote:

> @cecton https://github.com/cecton are you okay with me making the name
> of this bug a bit more clear/descriptive?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/2215#issuecomment-55431222
> .
",cecton,cecton
2215,2014-09-12 18:38:49,"Thanks @cecton 
",sigmavirus24,cecton
2214,2014-09-09 22:24:09,"At this point it's fairly obvious that @Lukasa and I are -1 on this feature. @kennethreitz @shazow any opinions?
",sigmavirus24,Lukasa
2214,2014-09-09 22:25:29,"To some extent, I agree that the warnings are important, but I think that there are multiple factors that might need to be considered.

From a developer perspective, know that I know about this, I can just turn this off if I feel inclined. I'm newer to packages, so it when I read the docs, in the warning, that solution didn't actually work. I like the idea that @Lukasa presented about making something that 's specific to `requests`. 

From a user perspective, I installed `pyvmomi` with `pip`today, which uses `requests` in its internals. It's really a non-transparent error that gets emitted back to the user in a case where `requests` is a silent supporting library.
",invisiblethreat,Lukasa
2214,2014-09-11 12:39:01,"@invisiblethreat feel free to hop into IRC if you have questions
",sigmavirus24,invisiblethreat
2214,2014-09-24 12:55:13,"@macterra I'm not sure I understand. Are you looking for alternative strategies for disabling the warning or ...?
",sigmavirus24,macterra
2214,2014-10-07 14:08:44,"I agree with @kankri, for the most part. That was the original design intention.

I propose something — that we disable by default but have our own function for re-enabling it, or documenting how to turn it on. I don't want out users to have to go out of the way to use the code as intended. `verify=False` is a feature, albeit not a best-practice one. That's none of our business. 
",kennethreitz,kankri
2214,2014-10-09 07:25:30,"Just wanted to say I agree with @kankri and that @kennethreitz's remark

> verify=False is a feature, albeit not a best-practice one. That's none of our business. 

sums it up well.
",piotr-dobrogost,kennethreitz
2214,2014-10-09 07:25:30,"Just wanted to say I agree with @kankri and that @kennethreitz's remark

> verify=False is a feature, albeit not a best-practice one. That's none of our business. 

sums it up well.
",piotr-dobrogost,kankri
2214,2015-01-29 12:17:23,"@Lukasa 

> Or simply do this:
> 
> 

Except that I find no mention of this function in the requests manual.

While it is far from everybody who knows about it, I would argue that the `warnings` module _is_ the standard tool a Python programmer should look to when he wants to disable warnings. It is part of the standard library and is well documented.

I suggest putting a reference to `warnings` into the `requests` documentation — or to the convenience `disable_warnings` function if you like, as long as there is a corresponding `enable_warnings` function (it [seems that there isn't such a function](https://urllib3.readthedocs.org/en/latest/security.html#insecurerequestwarning)).
",mgeisler,Lukasa
2214,2015-02-04 21:19:59,"@zaitcev At the risk of repeating myself:


",Lukasa,zaitcev
2214,2015-02-04 21:26:25,"Finally, a note, @zaitcev: you will find that taking the exasperated tone you just did wins you no favours at all. Remember that we are all volunteers, and that we have a limited amount of our time to give to build you things. Please try to treat us in the manner you would like to be treated.
",Lukasa,zaitcev
2214,2015-02-09 13:24:57,"@zaitcev It doesn't look like this will change in the requests module itself, but I hope you can use the code I put in [my other comment](https://github.com/kennethreitz/requests/issues/2214#issuecomment-72006954). That should allow you to selectively disable the warnings emitted by urllib3.
",mgeisler,zaitcev
2214,2015-04-03 21:47:43,"@zaitcev Pulling all the previous suggestions together, you can do something like this:


",utkonos,zaitcev
2214,2015-06-09 14:06:44,"@utkonos That would leave warnings disabled for all subsequent requests.

Putting other examples together, I extended the default `Session` (as `requests.get` and other shortcuts create a temporary `Session`, anyway):


",tuukkamustonen,utkonos
2214,2015-08-13 14:25:54,"> This was my principal objection: they could've made it work right, I even provided a patch in a pull request. But they are in denial that the problem exists and tell all the users to come up with terrible workarounds like ""except Exception"" or ""from requests.packages.urllib3 import exceptions"". At this point someone have to admit they were wrong all along and so we're stuck.

@zaitcev Once again, let me remind you that this is a volunteer community doing the best they can. We have left this issue free for discussion, we have not attempted to lock it or prevent further discussion. We are _listening to you_. What we aren't doing is immediately agreeing with your assessment of the situation. Please consider the possibility that we care about more use cases than simply yours, and that we need to balance the needs of all of them.

As to your pull request, it was rejected for a _very specific reason_ that you are constantly ignoring! Let me [quote myself](https://github.com/kennethreitz/requests/pull/2439#issuecomment-73013989) quoting [Ian](https://github.com/kennethreitz/requests/pull/2439#issuecomment-72994669):

> The closing statement was: ""Given that this is mostly in urllib3 and would rely on acceptance there, **I'm closing this until progress has been made there.**"" (Emphasis mine.)

As of today I still see no associated pull request or issue for this problem in urllib3. No-one from this project has stood in your way or prevented this work from occurring, we simply haven't chosen to do it ourselves because _we do not currently agree with you_.

However, at the risk of going down this rabbit hole again, let me reiterate:

> This was my principal objection: they could've made it work right.

**I do not believe your patch makes this work ""right""**. As I have said many times in this thread, I consider the current behaviour to be desirable. Doing insecure TLS requests is a bad idea, and users should be admonished against doing so.

My position is that a user _deserves to know_ when they are making a TLS request that is not adequately secured, especially in any system that is handling their passwords.

There is **agreement in this thread** that we should consider having a requests-level hook to disable these warnings. On the other hand, currently no-one but you believes that some previously-nonexistent distinction between `verify=False` and `verify=None` should be added in order to implicitly silence these warnings. You will find it much easier to do the former than the latter.
",Lukasa,zaitcev
2214,2015-10-06 13:18:02,"I have a situation similar to @jamie-sparked. 

I understand the point of Lukasa on enforcing security, but I think you should let YOUR user decide what's best for them. 
Requests is a library, not an end-user application. IMO you should consider developers as your users. 
Application developers should be liable for security mistakes, if they decide to turn off the cert verification (i.e. verify=False)

As developer, I value flexibility over a library trying to dictate what I should do. 

BTW as others said, I find requests _excellent_ and I appreciate all your effort. Thanks.
",thalesac,jamie-sparked
2214,2015-10-06 14:01:46,"@thalesac We _do_ let developers decide. As discussed _many_ times in this thread, it is quite possible to turn this warning off. However, we don't have one switch that turns all the warnings off: you need to manually do each one. This is an attempt to make our users _consciously_ remove each safety guard.

Think of it as defense-in-depth. To use a footgun analogy, we're handing you a gun with the safety on and no bullets in it, and a magazine. If we had `verify=False` disable all the warnings, that would be the equivalent of having a gun that, when a magazine was inserted, automatically disabled the safety and chambered a round. Convenient? Sure. Dangerous? You bet.
",Lukasa,thalesac
2214,2016-02-06 07:25:18,"Hi @Lukasa,

I put the breakpoint after the if. In the end I stopped using debian testing as I came across too many issues, and this may very well be one of them. I would just ignore my comment, I am not sure what happened but it is likely not something that will affect a lot of people.

Thanks!
Pedr
",droope,Lukasa
2211,2014-09-08 18:19:32,"@feus4177 Can you provide the entire traceback, including the full text of the exception?
",Lukasa,feus4177
2209,2014-09-07 15:27:38,"Final comment of the morning (as I have other things I have to do), I'm not sure there is anything preventing us from moving that consumption back to `HTTPAdapter#send`. I'm genuinely interested in @schlamar's reason for moving it though if they can remember it.

Also, @heyman thank you for changing the test style. We may not end up merging this right away, but your cooperation and bug report are much appreciated.
",sigmavirus24,heyman
2209,2014-09-07 15:42:40,"@sigmavirus24 Thanks for your thorough response!

Just a quick note. I tried to move the consumption of r.content back into HTTPAdapter.send. As expected it fixes the test I've supplied, but it also makes the `test_redirect_with_wrong_gzipped_header` test (https://github.com/kennethreitz/requests/blob/359659cf4b9dbeeef1ed832501dc1f99b0f0beac/test_requests.py#L978) fail with a ContentDecodingError.
",heyman,sigmavirus24
2209,2014-09-07 19:00:18,"The redirect issue is exactly why we moved it. We were encountering problems on redirect where we'd barf on invalid response bodies, even though we didn't really need them to proceed. We judged that to be a bad idea, so we moved the decode to ensure that it would happen in a guarded block for redirects, but still fail hard for non-redirected requests.

As @sigmavirus24 has observed, this bug is an accidental consequence of that change. Thanks for the fix, I'll review it shortly.
",Lukasa,sigmavirus24
2209,2014-09-07 19:01:27,"Ah, I misunderstood: this PR is the test, but not the fix. @heyman were you planning to provide the fix as well? Were you looking for guidance in how to do that?
",Lukasa,heyman
2209,2014-09-08 02:00:19,"I've been thinking about this for most of today and I'm not certain that there's a _good_ way to solve this. I had a similar idea to yours @heyman but one of the catches with that is the use case where the user decides to handle redirects themselves, e.g.,



In that case, `r`'s content will not be consumed. Prior to this, it was guaranteed to be consumed (unless the user specifies `stream=True`). In this case, we're breaking that guarantee for the user.
",sigmavirus24,heyman
2209,2014-09-09 10:01:15,"@heyman Yes. That's considered to be OK: with allow_redirects set to false you really need to handle all the redirect logic yourself. We simply don't know what you want to do.
",Lukasa,heyman
2209,2015-04-02 18:44:21,"@sigmavirus24 and I talked about doing a big issue and pull request cleanup at PyCon.

> On 2 Apr 2015, at 19:38, Kenneth Reitz notifications@github.com wrote:
> 
> Okay, we currently have 10 pull requests open for this project, many of which are labelled ""do not merge"".
> 
> Let's get this cleaned up.
> 
> —
> Reply to this email directly or view it on GitHub.
",Lukasa,sigmavirus24
2209,2015-04-05 16:03:15,"I think our documentation now very clearly illustrates that the elapsed time is always the time to the first byte of the body. Which means that this can be closed as we no longer expect this behaviour. @Lukasa or @kennethreitz should re-open this if they disagree.
",sigmavirus24,kennethreitz
2209,2015-04-05 16:03:15,"I think our documentation now very clearly illustrates that the elapsed time is always the time to the first byte of the body. Which means that this can be closed as we no longer expect this behaviour. @Lukasa or @kennethreitz should re-open this if they disagree.
",sigmavirus24,Lukasa
2208,2014-09-07 19:09:16,"@mlissner it's above all of our paygrades ;)
",sigmavirus24,mlissner
2208,2014-09-07 19:21:08,"@mlissner Trust me, this is one of those situations where you'd really rather not know. Get out, get out now. This way lies madness. Dive not into the murky world of HTTP, for monsters lurk in the depths.
",Lukasa,mlissner
2207,2014-09-06 16:49:07,"Don't be too happy @Lukasa. =P
",sigmavirus24,Lukasa
2206,2014-09-05 21:23:43,"Thanks for the thorough reply, @sigmavirus24.

> There is a good reason not to do it on the module scope. That function is so rarely used by 98% of requests users that triggering the compile when they do import requests will hurt their performance for no real benefit.

A typical solution is to compile them lazily (on first use) then, no? That way when you don't need to use the function you don't have to pay for it, and when you do, you're not punished for it on every call.

That said, out of curiosity I measured the performance impact of adding



to an otherwise empty module, and the difference was indistinguishable from noise. Did I measure wrong?

> That said, how would you determine which was the first encoding in the body? You could do something like...

(I was even thinking something simpler, something like:



)

> But that seems kind of arbitrary. On the one hand, returning all of them is also arbitrary but if we had written it the way you want, someone else would have wanted us to write it the way it is currently written.

Hm. Having a function that returned the first match rather than all matches for all three patterns makes the use case of getting the charset declared in the head of a large html document much more efficient. That seems like a primary use case for this function, in light of `get_unicode_from_response`'s docstring (though see #2205). So it's not arbitrary. At least, an additional `get_first_encoding_from_content` function could be offered to facilitate this use case.

I did a github code search for `get_encodings_from_content` to see how people are using this and so far most are using it like [this](https://github.com/ackwell/ninjabot/blob/68163212380a1cd82fda4e0c6e0d1b09a7c5a64e/plugins/web/linkinfo.py#L49..L51):



I'm not sure if you've gotten any requests for it to be the way it's currently written, but the way I'm proposing seems more efficient for the use cases I'm finding.

Thanks for the pointer to #2086. I'll keep an eye on that and also look forward to others' feedback on this.
",jab,sigmavirus24
2206,2014-09-05 23:42:21,"> A typical solution is to compile them lazily (on first use) then, no? That way when you don't need to use the function you don't have to pay for it, and when you do, you're not punished for it on every call.

Yeah, I'm not adverse to a pattern like this:



> That said, out of curiosity I measured the performance impact [...] Did I measure wrong?

You didn't show us how you measured it.

> At least, an additional get_first_encoding_from_content function could be offered to facilitate this use case.

I'm not sure it's necessary though. I understand there are a lot of people using this just like you describe but I also found plenty of people using the entire list. 

> I'm not sure if you've gotten any requests for it to be the way it's currently written, but the way I'm proposing seems more efficient for the use cases I'm finding.

I wouldn't be surprised if it were. That said, I'd like to reiterate @Lukasa's assertion that requests is fundamentally a HTTP library. I have not so quietly been an advocate of not having this in `requests.utils` at all. Further a function as you describe is something I don't feel comfortable including. If we include the first of the three that match, then someone will want to reorder the list of regular expressions we use. That will only lead to pain for @Lukasa and myself. The whole soup-y mess that consistently results in the `get_encodings_from_content` function is enough to make me want to push to remove it yet again.

Few people can agree on how it should behave, and some of the decisions have been quite arbitrary.
",sigmavirus24,Lukasa
2206,2014-09-06 08:52:50,"My position here is basically a less negative version of @sigmavirus24's. I simple _don't care_ about these functions. They used to be used by requests pre-1.0 and aren't now, but we left them in because some people found them and used them. They aren't documented and the maintainers don't advertise them, but ordinarily they don't represent a maintenance cost to us.

However, fundamentally, all of these functions are _trivial_ utilities. My recommendation is that, rather than change behaviour or add yet another function I'll studiously ignore, people should just copy the function out of `utils.py` and change it to do what they need it to do. It should take all of 10 seconds. =)
",Lukasa,sigmavirus24
2206,2014-09-11 16:57:28,"I think we've come to the conclusion that we will not be fixing this then. @Lukasa are you okay with my closing this?
",sigmavirus24,Lukasa
2205,2014-12-07 18:56:03,"Good catch @ibnIrshad 
",sigmavirus24,ibnIrshad
2204,2014-09-04 19:15:35,"@Lukasa `retries=False` does not raise `MaxRetryError`s, not sure if the same is true for `retries=0`.
",sigmavirus24,Lukasa
2204,2014-09-04 19:21:53,"@Lukasa I think this is fixed in #2193 since this is very related to #2192.
",sigmavirus24,Lukasa
2201,2014-09-04 16:50:16,"@Lukasa still doesn't work:


",stantonk,Lukasa
2201,2014-09-04 16:51:59,"@t-8ch system time is correct.

Oddly, requests made with the oauth2 lib works when I use certify:


",stantonk,t-8ch
2201,2014-09-05 17:57:43,"@Lukasa i did try it with cerifi above, it still fails


",stantonk,Lukasa
2201,2014-09-05 18:02:44,"@t-8ch NICE! That exposed the issue. The problem is that we were statically routing in /etc/hosts through an Nginx proxy with a self-signed cert :) Closing as this is not a requests issue.

Thanks for all your help guys!
",stantonk,t-8ch
2201,2014-11-26 04:23:27,"@stantonk Hi, how did you solve the issue specifically. I've ran into the same particular issue. I'm using a self signed certificate served via gunicorn
",komuW,stantonk
2199,2014-09-03 19:00:03,"@sigmavirus24 Why do we think it might?
",Lukasa,sigmavirus24
2196,2014-09-01 13:41:27,"Thanks for reporting this @whereskenneth 
",sigmavirus24,whereskenneth
2195,2014-08-30 08:02:08,"I agree that it won't hurt us to have, but I agree with @t-8ch that we should avoid the slightly prejudicial name. Maybe just call it `pyopenssl`?
",Lukasa,t-8ch
2195,2014-09-05 15:15:30,"@t-8ch false
",kennethreitz,t-8ch
2195,2014-09-05 15:44:02,"@t-8ch I don't understand your concerns in this statement:

> In requests < 2.4.0 The syntax error will even bubble up and just crash everything.

The extra does not ship with anything `<= 2.4.0`. How does this affect those people?
",sigmavirus24,t-8ch
2194,2014-08-29 21:40:12,"@kevinburke I'll take care of that as I pull this into my PR. Thanks
",sigmavirus24,kevinburke
2193,2014-08-29 21:25:46,"@kevinburke are you okay with me pulling #2194 in here to keep things simple?
",sigmavirus24,kevinburke
2193,2014-08-29 21:29:20,"Yep

## 

Kevin Burke
phone: 925.271.7005 | twentymilliseconds.com

On Fri, Aug 29, 2014 at 2:26 PM, Ian Cordasco notifications@github.com
wrote:

> @kevinburke https://github.com/kevinburke are you okay with me pulling
> #2194 https://github.com/kennethreitz/requests/pull/2194 in here to
> keep things simple?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/pull/2193#issuecomment-53931737
> .
",kevinburke,kevinburke
2193,2014-09-05 23:42:44,"@asmeurer hopefully 2.4.1 will be out in a relatively short period of time.
",sigmavirus24,asmeurer
2193,2014-09-07 16:50:37,"@sigmavirus24 should we cut it?
",kennethreitz,sigmavirus24
2193,2014-09-07 16:56:33,"@kennethreitz I think we should. This also forced OpenStack to pin to 2.3.0 in some (if not all) projects.
",sigmavirus24,kennethreitz
2193,2014-09-07 16:57:40,"This also is preventing requests upgrade in pip

On Sunday, September 7, 2014, Ian Cordasco notifications@github.com wrote:

> @kennethreitz https://github.com/kennethreitz I think we should. This
> also forced OpenStack to pin to 2.3.0 in some (if not all) projects.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/pull/2193#issuecomment-54752715
> .

## 

## 

Kevin Burke
phone: 925.271.7005 | twentymilliseconds.com
",kevinburke,kennethreitz
2192,2014-08-29 19:37:25,"Looking at it right now @Lukasa 
",sigmavirus24,Lukasa
2184,2014-09-14 22:55:49,"@kennethreitz yeah, pytest makes this a bit awkward, so I just released a new version of pytest-httpbin to make this easier.  Check out this section of the readme: https://github.com/kevin1024/pytest-httpbin#using-pytest-httpbin-with-unittest-style-test-cases

Here is the example usage:


",kevin1024,kennethreitz
2184,2015-11-08 09:28:34,"Thanks for the work @kevin1024! I did see the message, but the scary test output was a bigger problem. ;) I think it'd be better to come up with a different way to test this scenario.

Thanks for your library!
",Lukasa,kevin1024
2184,2015-11-10 15:40:02,"To be clear, @kennethreitz, the current test suite has been ported to use pytest-httpbin. =D At PyCon 2016 we'll probably want to have a discussion about the longer-term approach to testing in requests, but for now this is good enough. =)
",Lukasa,kennethreitz
2183,2014-09-20 14:01:03,"@myint No, I just missed them. This should be fixed in master now. Thanks for reporting it!
",Lukasa,myint
2181,2014-08-26 18:58:37,"@sigmavirus24 Nope, that's a graph of network throughput (IIRC). We should be able to get this at least as fast, even if it means working out what the hell httplib is doing.
",Lukasa,sigmavirus24
2181,2014-08-26 19:16:27,"@sigmavirus24  we should document this wealth of information you have in your brain :)
",kennethreitz,sigmavirus24
2181,2014-08-26 19:31:01,"@sigmavirus24 The graph is from the networking tab of Windows Task Manger. It is indeed showing network throughput. 

Here is the code for `requests` using `MultipartEncoder`. 



And here is the code that is just using `requests`



And lastly, the command for `cURL`



Here is the output from `cURL`



It does not seem to make a difference whether `MultipartEncoder` is in use. Both uploads using `requests` take ~16.5 seconds. 
",Tech356,sigmavirus24
2181,2016-03-01 20:55:56,"@lifeofdave you could try to adapt something like https://github.com/sigmavirus24/requests-toolbelt/pull/84 to your needs. That code is not merged into the toolbelt and not really well tested. That said, it forces the a larger read than what httplib asks for meaning it will send more data than httplib intends. That said, it's probably not a good long-term solution.
",sigmavirus24,lifeofdave
2176,2014-08-29 14:28:02,"@sigmavirus24 that can be 2.4.1 :)
",kennethreitz,sigmavirus24
2176,2014-08-29 14:28:13,"@kevinburke want to add some docs? :)
",kennethreitz,kevinburke
2176,2014-08-29 17:59:30,"@kennethreitz I added this section on timeouts:
http://docs.python-requests.org/en/latest/user/advanced/#timeouts

Just noticed I missed some method signatures, will add the docs for those
now.

## 

Kevin Burke
phone: 925.271.7005 | twentymilliseconds.com

On Fri, Aug 29, 2014 at 7:28 AM, Kenneth Reitz notifications@github.com
wrote:

> @kevinburke https://github.com/kevinburke want to add some docs? :)
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/pull/2176#issuecomment-53882868
> .
",kevinburke,kennethreitz
2176,2014-08-29 17:59:30,"@kennethreitz I added this section on timeouts:
http://docs.python-requests.org/en/latest/user/advanced/#timeouts

Just noticed I missed some method signatures, will add the docs for those
now.

## 

Kevin Burke
phone: 925.271.7005 | twentymilliseconds.com

On Fri, Aug 29, 2014 at 7:28 AM, Kenneth Reitz notifications@github.com
wrote:

> @kevinburke https://github.com/kevinburke want to add some docs? :)
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/pull/2176#issuecomment-53882868
> .
",kevinburke,kevinburke
2173,2014-08-22 11:24:35,"Thank you @tychotatitscheff!! :sparkles: :cake: :sparkles: 
",sigmavirus24,tychotatitscheff
2169,2014-08-26 21:37:35,"> 1. all the browser (and also curl) 

@tychotatitscheff requests is not a browser nor is it curl. Some of our behaviour is informed by them because how they behave is the de facto way servers expect all user-agents to behave. Regardless, it's a moot point because we all seem to be in agreement that this should be set by default.

The discussion that should have happened here (not where it did happen) is how adding this now will affect users of requests. The fact that this likely disappeared in 1.0 means that no one has really missed this significantly in almost a year and a half (if not 2 years). That said, the consequences of forcing it in something like 2.3.1 or 2.4.0 are unclear right now. It's possible this will affect larger consumers of requests like RedHat, Rackspace, OpenStack, and Runscope. For now, the solution is fairly simple: use a `session` object on which you set `session.headers['Connection'] = 'keep-alive'`.
",sigmavirus24,tychotatitscheff
2169,2014-08-27 05:36:26,"@kevinburke Interesting! I didn't have wireshark working at the time so I didn't check it. You're right though, we just aren't emitting it at all. Not quite so bad. =)

I'm with @sigmavirus24 on this: we should definitely set it by default, but we should hold off until at least 2.4.0 and possible until 3.0.
",Lukasa,kevinburke
2169,2014-08-27 05:36:26,"@kevinburke Interesting! I didn't have wireshark working at the time so I didn't check it. You're right though, we just aren't emitting it at all. Not quite so bad. =)

I'm with @sigmavirus24 on this: we should definitely set it by default, but we should hold off until at least 2.4.0 and possible until 3.0.
",Lukasa,sigmavirus24
2169,2014-08-27 14:54:57,"@tychotatitscheff I believe that's closer to what we want to do, yeah. We'll see, it depends a bit on what @kennethreitz wants.
",Lukasa,kennethreitz
2169,2014-08-27 14:54:57,"@tychotatitscheff I believe that's closer to what we want to do, yeah. We'll see, it depends a bit on what @kennethreitz wants.
",Lukasa,tychotatitscheff
2169,2014-08-28 17:02:47,"@sigmavirus24 what were the results of your investigation? I intend to cut 2.4.0 shortly, which will add the proper header in the defaults. 
",kennethreitz,sigmavirus24
2169,2014-08-28 17:06:28,"They were inconclusive. From what I can tell I don't think there will be problems, but I can't be 100% certain. The alternative to @Lukasa's suggestion of using `'close'` is also to set the value to `None`.
",sigmavirus24,Lukasa
2168,2014-08-19 07:37:37,"@Lukasa I will try to come up with a better way to test.
",ContinuousFunction,Lukasa
2168,2014-08-21 17:10:46,"@Lukasa I believe I have made the appropriate changes.
",ContinuousFunction,Lukasa
2165,2014-08-12 17:44:54,"OK, got familiar with tcpdump .  It's so much easier than Wireshark for analysis.  Issue is not with requests.  In case (b) and (d) from original post, server was returning HTTP response with content length of 132 (as expected).  In case (b), the JSON response was included to fulfill the length of 132.  In case (d), the JSON response was not included so I guess client is waiting for the JSON response that never comes and then times out.  Weird thing is, once I started debug printing out the request content on the server-side flask app, I started getting the JSON response for L>3487 and things are working perfectly now.  But anyway, thanks @Lukasa for your help.
",ekw,Lukasa
2164,2014-08-27 23:23:27,"@msrajan when you do, we'll reopen the issue if it's truly a bug in requests.
",sigmavirus24,msrajan
2164,2015-12-08 16:35:35,"@mobcdi this issue was opened after @msrajan was told to open it on StackOverflow. That said, no one has been able to reproduce this or provide sufficient information for us to debug it. Further, please do not revive old issues unless you have something to add in the way of information.
",sigmavirus24,msrajan
2164,2015-12-08 16:35:35,"@mobcdi this issue was opened after @msrajan was told to open it on StackOverflow. That said, no one has been able to reproduce this or provide sufficient information for us to debug it. Further, please do not revive old issues unless you have something to add in the way of information.
",sigmavirus24,mobcdi
2164,2016-11-02 11:32:51,"@kagupta28 Are you talking to a proprietary server, or one that I can reach on the open web?

This issue is almost certainly caused by the combination of the request that we emit and the response we receive, and if I can't reproduce or see those it is very difficult for me to assist you.
",Lukasa,kagupta28
2164,2016-11-02 11:42:10,"@Lukasa understand that... talking to a proprietary server. Just thinking what can be done to help investigate this issue further.
",kagupta28,Lukasa
2163,2014-08-12 12:44:06,"@m-novikov we need more information.
1. What kind of proxy are you using?
2. What is the full traceback?
3. What is the exact version of python you're using (e.g., 3.4.1)
4. What is the version of requests you're using?
5. Can you still reproduce this with the version of requests present in `master`?
",sigmavirus24,m-novikov
2163,2014-08-12 15:41:18,"@Lukasa that was why I asked question 5. I just didn't want to jump the gun on the association.
",sigmavirus24,Lukasa
2163,2014-08-27 16:08:54,"@sigmavirus24 
Sorry for late answer.
- Random https proxies from public lists
- Traceback


- Python 2.7.6
- tested on requests 2.2.1, requests 2.3.0
- Looks like it's fixed in master.
",m-novikov,sigmavirus24
2162,2014-08-12 02:10:44,"@fenume they're not ""lost"". See @lukasa's [comment](https://github.com/kennethreitz/requests/issues/1446#issuecomment-20658864) on #1446. This was intentional because a single response's CookieJar should represent the cookies sent by the server in _that_ response. If you want what used to work in version 1.2.0, I would advise you start using a session.


",sigmavirus24,fenume
2159,2014-08-02 12:37:04,"Hi, thanks for raising this issue!

This is a known bug in the Python standard library, [issue 17849](http://bugs.python.org/issue17849). I have submitted a patch for this bug upstream, but it hasn't been merged at this time (despite being submitted in January). The best way to get this properly fixed is to try to agitate to get that patch merged.

We _may_ be able to fix this by doing #1869. @sigmavirus24, do you think it's worth it?
",Lukasa,sigmavirus24
2159,2014-09-07 18:46:56,"@bmcorser could you elaborate?
",sigmavirus24,bmcorser
2159,2014-09-07 18:53:36,"@bmcorser As discussed, this limitation is not in requests and there's really nothing we can do to speed things up. I recommend doing a code review of the patch I submitted to make it easier for the CPython maintainers to get it merged.
",Lukasa,bmcorser
2159,2015-01-18 20:21:18,"@Lukasa didn't we merge a PR that fixes this?
",sigmavirus24,Lukasa
2158,2014-08-01 18:20:47,"Hey @zapman449!

Thanks for your suggestion! We really appreciate the fact that you took the time to file this. Unfortunately we're not adding anything new to the requests API at this time.

I'll wait for @Lukasa to chime in before closing this though.
",sigmavirus24,zapman449
2158,2016-01-18 09:16:02,"This solution right here simply must be added to the docs, brilliant work @est.
",Hasimir,est
2158,2016-04-06 14:22:48,"@Lukasa what are your thoughts/feelings on a `Response.origin`? Seems like it would be a nice addition to me. 
",kennethreitz,Lukasa
2157,2014-08-01 11:19:35,"@atmb4u Sadly there isn't. That is built directly off a git tag, which is a static reference to a single commit, we can't update it.
",Lukasa,atmb4u
2155,2014-08-01 13:48:51,"@sigmavirus24 Thanks, that's definitely an elegant solution to the problem I outlined above!

I would recommend adding that to requests' documentation, e.g. in the FAQ: http://docs.python-requests.org/en/latest/community/faq/#encoded-data  
Currently, the statement ""Requests automatically decompresses gzip-encoded responses"" is not correct for the `stream=True` case and can lead to surprises.

As for my problem, as you've read on the [urllib3 issue](https://github.com/shazow/urllib3/issues/437), the urllib3 implementation of the gzip decompression has its own little quirks I have to work around in my code, but that is no longer a problem for requests.
",hheimbuerger,sigmavirus24
2155,2014-08-01 14:05:43,"@sigmavirus24 I believe it should be documented, as the current documentation is incorrect.

But if you disagree with that, yes, close away!
",hheimbuerger,sigmavirus24
2153,2014-07-30 03:42:48,":heart: Thanks @jschneier !

LGTM! :+1: 
",sigmavirus24,jschneier
2152,2014-07-27 16:52:19,"@Lukasa Because it is very convenient, can save lots of code (let's just say a project uses request. This project uses about 100 times a request. So, it _could_ save 200 lines of code, and prevent typos because I would have to write the exception 100 times (in case I don't use a method for raising an exception), and it unifies exceptions/errors (another API may use requests and raise a custom error in case of an invalid status code. I like it better if it throws the same exception as requests does). 

@sigmavirus24  Sorry, I didn't know this method exists. You should mention that  an HTTPError can be raised by raise_for_status in Error and Exceptions section in the docs. An HTTPError currently claims only to be raised ""In the rare event of an invalid HTTP response"".  
You could also consider implementing some of my ideas into my raise_for_status so that it can allow other status codes, too. 
",Matt3o12,Lukasa
2152,2014-07-27 16:52:19,"@Lukasa Because it is very convenient, can save lots of code (let's just say a project uses request. This project uses about 100 times a request. So, it _could_ save 200 lines of code, and prevent typos because I would have to write the exception 100 times (in case I don't use a method for raising an exception), and it unifies exceptions/errors (another API may use requests and raise a custom error in case of an invalid status code. I like it better if it throws the same exception as requests does). 

@sigmavirus24  Sorry, I didn't know this method exists. You should mention that  an HTTPError can be raised by raise_for_status in Error and Exceptions section in the docs. An HTTPError currently claims only to be raised ""In the rare event of an invalid HTTP response"".  
You could also consider implementing some of my ideas into my raise_for_status so that it can allow other status codes, too. 
",Matt3o12,sigmavirus24
2150,2014-07-25 14:19:37,"@s0rg you didn't tell us which version of requests you're using so we can't even test this to ensure that it's a problem.
",sigmavirus24,s0rg
2150,2014-07-25 14:56:18,"@s0rg if you upgrade to Python 3 this should be fixed http://bugs.python.org/issue1660009
",sigmavirus24,s0rg
2148,2014-07-24 21:31:13,"@sigmavirus24 @alex Fixed. Not sure what I was thinking, there.
",romanlevin,sigmavirus24
2145,2014-07-22 17:10:57,"@sigmavirus24 I've updated the wording - comments?
",TkTech,sigmavirus24
2145,2014-07-22 20:20:36,"@TkTech I completely agree! A pull requests that added that would be accepted :)
",kennethreitz,TkTech
2143,2014-07-22 13:34:46,"@jeffg-vsn the problem is that I'm not even confident that `urllib3` supports this behaviour, which makes requests support of it that much more difficult.
",sigmavirus24,jeffg-vsn
2143,2014-09-12 15:58:06,"@Lukasa yeah. I'm pretty sure we can't really do anything about this sadly. Closing until that changes.
",sigmavirus24,Lukasa
2136,2014-07-16 12:14:49,"That's embarrassing. Thanks @dpursehouse !
",sigmavirus24,dpursehouse
2134,2014-07-15 17:13:06,"Thanks @nonZero !
",sigmavirus24,nonZero
2133,2014-07-15 17:12:54,"Thanks @esparta !
",sigmavirus24,esparta
2129,2014-07-15 02:14:15,"Hi @jamielennox I haven't looked at this yet because the tests are failing. Please check the failures and fix them. Thank you
",sigmavirus24,jamielennox
2129,2014-07-15 02:42:42,"@sigmavirus24 fixed, thanks.
",jamielennox,sigmavirus24
2129,2014-07-22 20:23:04,"There are some nice changes in here. Looking forward to hearing what @sigmavirus24 and @Lukasa have to say :)
",kennethreitz,sigmavirus24
2129,2014-07-23 00:39:10,"I agree with @Lukasa and want to stress that it's a very subtle way in which it affects the API. Most people who use `Session#send` are not expecting environment based data to affect their request. If they are using prepared requests to avoid this altogether, we're making this basically impossible for them now. We might need to wait for Requests 3.0 to merge this.
",sigmavirus24,Lukasa
2129,2014-07-23 04:41:16,"@jamielennox We don't need the `trust_env` keyword argument (in fact I don't think we want it). What we're getting at is that we've moved the location where a specific action occurs, so that previously people who didn't need to worry about the `trust_env` value now do. The fact that it can easily be turned off is not really relevant, we still need to be careful with when we merge the change. I'm happy for us to do it, we should just exercise caution.
",Lukasa,jamielennox
2129,2014-07-23 04:54:46,"@Lukasa Yea, i understand that. I was thinking by doing it this way we didn't change the existing behaviour for anyone and people who wanted to use the environment variables could opt-in. I don't know what the timeframe is but that's preferable to me than waiting for 3.0. 

Do you want me to revert back to the older commit? (wish github had a better way of managing this). 
",jamielennox,Lukasa
2129,2014-08-12 19:06:55,"@kennethreitz it seems @jamielennox has abandoned this. I'll pull their commit(s) into a branch of my own and update it with our decision tonight.
",sigmavirus24,kennethreitz
2129,2014-08-12 19:06:55,"@kennethreitz it seems @jamielennox has abandoned this. I'll pull their commit(s) into a branch of my own and update it with our decision tonight.
",sigmavirus24,jamielennox
2129,2014-08-12 21:26:59,"Sorry everyone, i had let this slip off my list.

This is at least an example of extracting that information to a function, there is a question there as to whether the trust_env check should go inside the apply_environ() function or not, i've gone back and forth. 

I'm not sure if this is what you were looking for - it doesn't feel as 'clean' as the last patches for some reason. 

@sigmavirus24 don't be worried about stepping me through the review process, i'd prefer to get this feature in than worry about who contributed it. If you have something in mind that satisfies the case then merge it and close this out. 
",jamielennox,sigmavirus24
2129,2014-08-22 13:05:32,"@sigmavirus24 want to own this?
",kennethreitz,sigmavirus24
2128,2014-07-14 14:39:17,"Thanks @tshepang !
",sigmavirus24,tshepang
2127,2014-07-14 01:41:26,"@Fire30 that traceback doesn't appear to have anything to do with requests. Can you explain the relationship?
",sigmavirus24,Fire30
2124,2014-09-04 12:08:24,"That's a risk. If we take the policy that our new dependence on certifi means we no longer need to update the build-in bundle we've exposed our users to risk.

I have no particular objection, so I'll let @kennethreitz make the call: should the hard dependency on certifi be removed?
",Lukasa,kennethreitz
2120,2014-07-04 17:20:17,"@jmandreoli I'm certain that is not the entire of your traceback and it is likely related to #1915 and #1289. Your issue is not that, because we were handling that exception when a different one occurred (with certainty). You need to give us the version of requests and what the _entire_ traceback is.
",sigmavirus24,jmandreoli
2120,2014-07-07 08:43:39,"On 04/07/14 19:20, Ian Cordasco wrote:

> @jmandreoli https://github.com/jmandreoli I'm certain that is not the
> entire of your traceback and it is likely related to #1915
> https://github.com/kennethreitz/requests/issues/1915 and #1289
> https://github.com/kennethreitz/requests/issues/1289. Your issue is
> not that, because we were handling that exception when a different one
> occurred (with certainty). You need to give us the version of requests
> and what the /entire/ traceback is.

I send you enclosed the full traceback.
- For me, the ""unexpected keyword argument 'buffering'"" error may not be 
  the main cause of the problem, but it must still be a bug (invoking a 
  method with a keyword argument which does not exist).
- The version of python/requests is

> Python 3.3.5 |Continuum Analytics, Inc.| (default, Mar 10 2014, 11:19:31)
> [GCC 4.1.2 20080704 (Red Hat 4.1.2-54)] on linux
> Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
> 
> > > > import requests
> > > > requests.**version**
> > > > '2.3.0'
- I must also point out that the error does not always happen.
- If it can help, I also send you enclosed the relevant section of the 
  code. Note that the generator ""egxmonitor"" is enumerated only at low 
  frequency (once every 2 secs), so it is not like the server is bombarded 
  with requests in the ""while True"" loop.

Thanks for looking into this issue anyway, and if you find something, 
please let me know.



Traceback (most recent call last):
  File ""/.../anaconda/envs/py3k/lib/python3.3/site-packages/requests/packages/urllib3/connectionpool.py"", line 319, in _make_request
    httplib_response = conn.getresponse(buffering=True)
TypeError: getresponse() got an unexpected keyword argument 'buffering'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/.../anaconda/envs/py3k/lib/python3.3/site-packages/requests/packages/urllib3/connectionpool.py"", line 493, in urlopen
    body=body, headers=headers)
  File ""/.../anaconda/envs/py3k/lib/python3.3/site-packages/requests/packages/urllib3/connectionpool.py"", line 321, in _make_request
    httplib_response = conn.getresponse()
  File ""/.../anaconda/envs/py3k/lib/python3.3/http/client.py"", line 1147, in getresponse
    response.begin()
  File ""/.../anaconda/envs/py3k/lib/python3.3/http/client.py"", line 358, in begin
    version, status, reason = self._read_status()
  File ""/.../anaconda/envs/py3k/lib/python3.3/http/client.py"", line 328, in _read_status
    raise BadStatusLine(line)
http.client.BadStatusLine: ''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/.../anaconda/envs/py3k/lib/python3.3/site-packages/requests/adapters.py"", line 327, in send
    timeout=timeout
  File ""/.../anaconda/envs/py3k/lib/python3.3/site-packages/requests/packages/urllib3/connectionpool.py"", line 543, in urlopen
    raise MaxRetryError(self, url, e)
requests.packages.urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='13.202.222.220', port=80): Max retries exceeded with url: /UE/Post__PL__Data (Caused by <class 'http.client.BadStatusLine'>: '')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/.../chronorec.py"", line 161, in newsession
    for itern,x in enumerate(source,1):
  File ""/.../chronorec.py"", line 526, in atinterval
    for x in it:
  File ""/.../src/pwr/egx.py"", line 30, in egxmonitor
    r = session.post(url,auth=login,data=data)
  File ""/.../anaconda/envs/py3k/lib/python3.3/site-packages/requests/sessions.py"", line 498, in post
    return self.request('POST', url, data=data, *_kwargs)
  File ""/.../anaconda/envs/py3k/lib/python3.3/site-packages/requests/sessions.py"", line 456, in request
    resp = self.send(prep, *_send_kwargs)
  File ""/.../anaconda/envs/py3k/lib/python3.3/site-packages/requests/sessions.py"", line 559, in send
    r = adapter.send(request, **kwargs)
  File ""/.../anaconda/envs/py3k/lib/python3.3/site-packages/requests/adapters.py"", line 375, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='13.202.222.220', port=80): Max retries exceeded with url: /UE/Post__PL__Data (Caused by <class 'http.client.BadStatusLine'>: '')
",jmandreoli,jmandreoli
2120,2014-07-07 09:53:38,"@jmandreoli It is not a bug. The relevant code is reproduced here in full (it lives [here](https://github.com/kennethreitz/requests/blob/master/requests/packages/urllib3/connectionpool.py#L318-L321) for context):



This code block exists because we want to use the `buffering` kwarg if we can on platforms that have it, but it isn't present on Python 2.6 or some versions of Python 3 (where it's the default). The style (attempt to use it, fail if we hit the exception and try without it) is a programming style usually summarised as ""it's easier to ask forgiveness than permission"". It's a very common Python programming style.

_Unfortunately_, the chained tracebacks feature in Python 3.X trips people up when using this style because you frequently see exceptions that are of that form. In this case, the TypeError is totally expected.

The _relevant_ exception is the second one: `BadStatusLine`. The subsequent two exceptions are just wrapper exceptions and can be ignored. Looks like you're receiving a malformed response: you should use Wireshark or tcpdump to examine what is actually being received when you're hitting this error.

`<digression>`

I am not a fan of Python 3's chained exceptions being on by default. They're a very useful debugging feature, but they're also a classic example of an expert-level interface. If you don't expect to see them they're hugely difficult to read.

This is the perfect example: you posted four tracebacks. It's ridiculous to ask a non-expert requests user to work out which the hell one of those is the exception that matters. All you can know is that the last one is the one you'd have had to catch.

_I_ know which one matters, but I've worked on this library for three years. This means that when I debug I can tell the difference between things that should happen and things that shouldn't, and so the extra context is helpful. For people who can't tell the difference (i.e. non-experts), the extra context is noise that makes debugging harder. On Python 2, you'd have seen this:



That exception is much more immediate: we had an error during connection (`ConnectionError`), which is the result of hitting our retry limit with a specific URL (`Max retries exceeded`), and the things that caused us to retry were `BadStatusLine` errors.

For all of Python 3's moves to make things easier for non-experts, this one made it harder for them. I find that inconsistency frustrating.

`</digression>`
",Lukasa,jmandreoli
2118,2014-07-02 16:15:20,"Can we step back for a moment?

@brandon-rhodes I'm really not clear about what you mean by requests sprouting more and more keywords. We have two: `verify` and `cert`. Anything else is done at the `HTTPAdapter` layer on a case-by-case basis, which allows exactly what you've described.

What have I missed? What parameters are you identifying that I don't see?
",Lukasa,brandon-rhodes
2118,2014-07-02 16:24:05,"Reading your post again @brandon-rhodes, I realise that your concern is not actually that we _currently_ have too many parameters, but that we might _end up_ having too many parameters. In this I agree with you.

However, I think requests should continue its proud tradition of solving the 80% use-case in the primary API and allowing the Transport Adapter API to help the remaining 20%. For this reason, I'd never accept adding a `ciphers` kwarg to the main API: it's just too niche, _especially_ if we require it to look anything like OpenSSL's terrible cipher format.

Instead, let's look at providing something like the `ssl_version` support in `urllib3`. This would allow us to write something like the [SSLAdapter](http://toolbelt.readthedocs.org/en/latest/user.html#ssladapter), but for ciphers (or indeed just to extend the SSLAdapter). That allows people who genuinely do care to plug something in, while allowing requests to continue to support the best-possible use-case.
",Lukasa,brandon-rhodes
2118,2014-07-02 16:39:48,"> Further, we're ignoring whether or not @kennethreitz would even want to add this keyword argument.

By **no means** did I intend to ignore @kennethreitz’s wishes! Alas. Opening this issue was, I had thought, precisely the means by which Mr. Reitz’s wishes could become known. Would there have been a more appropriate forum for asking my question?

> In this I agree with you.

Thank you, @Lukasa! Sorry if I worded the issue awkwardly and it required multiple read-throughs.

>  I think requests should continue its proud tradition of solving the 80% use-case … I'd never accept adding a `ciphers` kwarg to the main API … This would allow us to write something like the `SSLAdapter`

So the keyword arguments to `get()` and the other functions are not “kitchen sink” collections of everything that _could_ be specified, but a smaller collection of settings, and users are intended to step back and create adapters for more difficult cases? Then you are correct that what I probably need is an adapter that accepts an `SSLContext` for building connections!

The `urllib3` library accepts an `ssl_version` keyword? That, I fear, is a first step towards insanity, and a course which can be stopped by turning to `SSLContext`. Because the next logical step after `ssl_version` (which is really setting what OpenSSL calls the “protocol” version, from what I can see?) is adding a `ciphers` keyword, and then a `dh_params` keyword, and then `ecdh_curve`, and so forth.

In fact, what I really probably want is an adapter that does not even know that `SSLContext` exists, but simply accepts that I have gotten ready an object with a `wrap_socket()` method that it can use when it is ready to negotiate an encrypted connection. That way, as long as an SSL library that I want to use in the future also offers a `wrap_socket()` method (think of PyOpenSSL, or that new `cryptography` project), then it would automatically work if dropped in to the transport object.
",brandon-rhodes,Lukasa
2118,2014-07-02 16:49:28,"> Would there have been a more appropriate forum for asking my question?

No. I just like to couch my positive replies in ""But Kenneth might not want this, so don't get your hopes up"". It might take him a while to get around to reading or replying though.

> So the keyword arguments to get() and the other functions are not “kitchen sink” collections of everything that could be specified, but a smaller collection of settings, and users are intended to step back and create adapters for more difficult cases?

That is correct. There are currently a few good examples of this (like the SSLAdapter @Lukasa linked above).

> Then you are correct that what I probably need is an adapter that accepts an SSLContext for building connections!

Good news! @Lukasa has a blog post about building adapters for requests and we can both help you with building one of these. Further, I'd be interested in it if only to add it to the [`toolbelt`](https://github.com/sigmavirus24/requests-toolbelt) if only to help it be discovered more easily.

> In fact, what I really probably want is an adapter that does not even know that SSLContext exists, but simply accepts that I have gotten ready an object with a wrap_socket() method that it can use when it is ready to negotiate an encrypted connection.

You _could_ do this, but then you would probably doing a lot of `urllib3`'s work since it doesn't (if I remember correctly) provide a way for you to pass in a socket to use (and I don't think it should start accepting sockets to use either).

It might be more helpful to acquaint yourself further with the adapters that currently exist in requests, to understand how they work. They're not terribly difficult to understand either.
",sigmavirus24,Lukasa
2118,2014-07-02 17:23:36,"Hooray for three person issue conversations! They're always so easy to follow. =) @brandon-rhodes, you've provided lots of great options for us. I'd like to try to enumerate them as I understand them, and then provide feedback on each of them. Please step in if you think I've left anything out or misunderstood something.
1. Make it possible to pass urllib3 a `socket`.
   
   I don't like this idea much. In principle it's do-able, but it violates the abstraction layer that urllib3 provides. Our favourite feature of urllib3 is that it performs connection pooling, and for it to do that it needs to be able to transparently create new connections (to conserve resources). This means we can't say ""here, use this connection object"" because urllib3 owns all the connection objects. This idea is sadly unworkable.
2. Provide urllib3 with an `SSLContext`.
   
   This is workable, as urllib3 can use it as a kind of connection factory. However, urllib3 right now does an excellent job of transparently interworking between PyOpenSSL and the standard library's `ssl` module. This transparent interworking is only going to get stronger as we move toward [hyper](http://hyper.rtfd.org/en/development/) and HTTP/2. This means being able to provide an SSL context is something of a footgun: if you provide a stdlib `SSLContext` to `hyper`, it won't be able to make an HTTP/2 connection through it, it needs PyOpenSSL's.
   
   Additionally, the PyOpenSSL `Context` does not have the same API as the `ssl` module's one. This is incredibly annoying, and @alekstorm has done awesome work by writing a compatibility module ([backports.ssl](https://github.com/alekstorm/backports.ssl)) to paper over the differences. Again, however, I note that working around the myriad user inputs is a logistical nightmare.
3. More SSL keyword arguments.
   
   Brandon, you've expressed a discomfort with the many keyword arguments potentially required. I am sympathetic to that argument. I have no good alternative to using either a **ssl_kwargs or to have a ssl_args dictionary argument, neither of which is great. I guess a NamedTuple?
",Lukasa,brandon-rhodes
2118,2014-07-20 11:44:19,"@EnTeQuAk I continue to be in favour of my option 2. =)
",Lukasa,EnTeQuAk
2118,2014-10-16 20:44:40,"I second @alex's suggestion. Constructing it once and not having to use those 4 lines each time I need a slight variation is much nicer. Immutability will also be pleasant so the options cannot be changed under anyone's feet.
",sigmavirus24,alex
2118,2014-10-16 21:25:20,"@shazow it's lived almost entirely here so let's just finish it here?
",sigmavirus24,shazow
2118,2015-12-03 20:46:21,"@aadamowski You wouldn't need that. Generally speaking, if you can provide an SSLContext you should just use PyOpenSSL and provide Context with a custom verify callback. That would allow you to achieve your goal.
",Lukasa,aadamowski
2118,2015-12-03 21:47:14,"@Lukasa , is the plan for `requests` to handle both Python `ssl`'s [`SSLContext`](https://docs.python.org/3/library/ssl.html#ssl.SSLContext) and PyOpenSSL's [`Context`](http://pyopenssl.sourceforge.net/pyOpenSSL.html/openssl-context.html) objects equivalently?

Despite some similarities, these classes don't have compatible APIs and aren't related.
",aadamowski,Lukasa
2118,2017-01-13 15:56:19,@Lukasa Are there docs on doing so? I'm not finding anything.,dsully,Lukasa
2118,2017-01-13 15:57:21,"@dsully Not at this time, though I have written examples [a few times](https://github.com/kennethreitz/requests/issues/3774#issuecomment-267871876).",Lukasa,dsully
2118,2017-01-13 16:03:23,"@Lukasa Thanks, I'll start playing around with that. FWIW, the reason I need this is adding our internal CA to the trust store in addition to locking down the cipher suite.",dsully,Lukasa
2117,2014-07-03 14:17:16,"@homm What is your proposal for requests? The best option I've seen so far is to send both `filename` and `filename*` in that order. If that happens, what should we put in `filename`?
",Lukasa,homm
2117,2014-07-03 14:19:44,"@homm you're inherently wrong. Rack _claims_ to support RFC 2231, this is clearly a failure to properly do so given how clear 2231 (and 5987) are in their grammar. You claim earlier versions of requests do not cause the problem, the solution to your problem then is to clearly use old versions since you seem to think broken behaviour is correct.
",sigmavirus24,homm
2117,2014-07-03 14:21:31,"@sigmavirus24 I am open to sending both `filename` and `filename*`, this should be supported, but I don't know how we'd populate `filename`.
",Lukasa,sigmavirus24
2117,2014-07-03 14:24:40,"@Lukasa
1) whatever is enough for file recognition for not compliant server and does not brake others (because ""specification suggests that a parameter using the extended syntax takes precedence"").
2) it can be value without any non-ascii chars and ""unknown"" as fallback.
3) should be a way to send encoded `filename` if I exactly know how server handles encoded values.
",homm,Lukasa
2117,2014-07-03 14:26:38,"@sigmavirus24 I can't use old requests version because I need SNI on python 2 :(
",homm,sigmavirus24
2117,2014-07-03 14:28:05,"@Lukasa ostensibly through the toolbelt since that's the only place where users have complete control over the fields but even so, this would probably need to be worked on in urllib3 because I'm fairly certain it doesn't currently provide a way to do this.

Further RFC 2231 is the authority on this since 5987 (as I've re-read it) is for HTTP Headers (not MIME/multipart headers) and 2231 does not allow for multiple parameters (on quick glance over it)

---

@homm 

>  2) it can be value without any non-ascii chars and ""unknown"" as fallback.

This is not an accurate representation in the slightest. At best it will introduce utter confusion.

>  3) should be a way to send encoded filename if I exactly know how server handles encoded values.

You can never know exactly how any server handles these values unless you've written it from scratch yourself.
",sigmavirus24,homm
2117,2014-07-03 14:28:05,"@Lukasa ostensibly through the toolbelt since that's the only place where users have complete control over the fields but even so, this would probably need to be worked on in urllib3 because I'm fairly certain it doesn't currently provide a way to do this.

Further RFC 2231 is the authority on this since 5987 (as I've re-read it) is for HTTP Headers (not MIME/multipart headers) and 2231 does not allow for multiple parameters (on quick glance over it)

---

@homm 

>  2) it can be value without any non-ascii chars and ""unknown"" as fallback.

This is not an accurate representation in the slightest. At best it will introduce utter confusion.

>  3) should be a way to send encoded filename if I exactly know how server handles encoded values.

You can never know exactly how any server handles these values unless you've written it from scratch yourself.
",sigmavirus24,Lukasa
2117,2014-07-03 14:33:11,"Let's be clear on our thinking here @homm. What we need is something that satisfies criteria 1) and 2) in an unambiguous way while providing a pleasant API. I contend that no such solution exists, because criteria 1) and 2) are ambiguous and resolving that ambiguity dirties up the API.

Requests does not need to be able to generate every valid HTTP request under the sun, only a subset of them. We've decided how we're approaching this, and we're backed up by the specs. We've also provided you with a workaround. That workaround reveals a bug in urllib3 that we're going to pursue fixing.

Unless you can design a pleasant, non-ambiguous API, we're not going to be too worried about this.
",Lukasa,homm
2115,2014-07-03 12:19:40,"@kennethreitz Disagree: verify may be set to a string (path to the cert bundle) and we shouldn't overwrite it.
",Lukasa,kennethreitz
2114,2014-07-01 20:08:35,"@manizzle you're linking to an ancient version of the documentation (you can see it in the URL `v0.10.6`). The current documentation is fine: http://docs.python-requests.org/en/latest/user/advanced/#session-objects
",sigmavirus24,manizzle
2113,2014-07-01 18:57:14,"@joeshaw do you mean to say this works without unicode headers with pyOpenSSL?
",sigmavirus24,joeshaw
2113,2014-07-01 18:58:05,"@sigmavirus24 Both without unicode headers against HTTPS and with unicode headers against HTTP.  In my particular case the data I am sending is always ASCII, it just happens to be a unicode object because i got it from a different library that always returns unicode objects.
",joeshaw,sigmavirus24
2113,2014-07-01 18:59:33,"@joeshaw True, but as you've pointed out, it will break lots of currently working code. Even though that code is in error, I'm reluctant to break it because it does _happen_ to work. This is the reason Python 3 happened in the first place. =)

As for assuming UTF-8 is valid, you would be wrong. =) There is no clear ruling on the valid textual encoding for headers. Generally, ASCII is all you can _guarantee_ to be safe. Everything from then on is difficult to know. Better to make the user do this.
",Lukasa,joeshaw
2113,2014-07-01 19:03:30,"@joeshaw the fix is to call `string.encode` on each string that library gives you. You also seem to know the encoding (something requests should never guess at), so you can get the string from that library and then simply call `s.encode('utf-8')` on it.

As @Lukasa the only totally safe header encoding is ASCII. The next safest would be Latin-1 (ISO-8859-1). Other than that, there's not much we can do for you.
",sigmavirus24,joeshaw
2113,2014-07-01 19:03:30,"@joeshaw the fix is to call `string.encode` on each string that library gives you. You also seem to know the encoding (something requests should never guess at), so you can get the string from that library and then simply call `s.encode('utf-8')` on it.

As @Lukasa the only totally safe header encoding is ASCII. The next safest would be Latin-1 (ISO-8859-1). Other than that, there's not much we can do for you.
",sigmavirus24,Lukasa
2111,2014-06-26 16:27:30,"@saulshanabrook all of this isn't to say that your request isn't a reasonable one, it's just that there's no practical way to implement it. This is something most of us would _love_ to have. The problem is that there is no way to do this that will be consistent, reliable, and reasonable. This kind of introspection/inference is totally out of the scope of requests unfortunately.
",sigmavirus24,saulshanabrook
2111,2014-06-26 18:28:22,"@saulshanabrook the docs aren't meant to be comprehensive, so no this isn't really worth mentioning in the docs. It would be worth writing a blog post about if you feel so inclined. With that indexed by Google, you'll be helping other people with the blog post though. (And sometimes the best documentation is third-party documentation written by people who have experienced problems and are passionate about explaining their solutions or discoveries.)
",sigmavirus24,saulshanabrook
2111,2014-06-26 18:41:01,"@sigmavirus24 I just took you up on your suggestion http://www.saulshanabrook.com/requests-timeout-and-odd-traceback/
",saulshanabrook,sigmavirus24
2111,2014-06-26 19:00:41,"I'll be tweeting it shortly @saulshanabrook looks like a good post. :+1: 
",sigmavirus24,saulshanabrook
2108,2014-06-23 09:05:41,"@Lukasa: anyway it's urllib3 related issue
",vitek,Lukasa
2106,2014-06-23 12:33:33,"@xiongxoy can you apply the commit in the PR that @Lukasa referenced (#1860) and see if that fixes the test for you?
",sigmavirus24,Lukasa
2106,2014-06-23 12:33:33,"@xiongxoy can you apply the commit in the PR that @Lukasa referenced (#1860) and see if that fixes the test for you?
",sigmavirus24,xiongxoy
2105,2014-06-21 01:00:08,"@robvdl it does not say that it ""does not support Python 3.4"" it is merely an omission that we do in fact support 3.4. I'd like to wait for @kennethreitz to add a Python 3.4 builder to the Jenkins server before adding it however.
",sigmavirus24,robvdl
2105,2014-08-28 17:11:00,"@sigmavirus24 you're too abrasive :)
",kennethreitz,sigmavirus24
2105,2014-08-28 17:11:08,"@robvdl many thanks! fixed by d22b8d8e7e8fcb8d16efba6841977dc81ac2c935
",kennethreitz,robvdl
2104,2014-06-21 05:50:07,"I'm generally in agreement with @sigmavirus24. More generally, if you want redirects on (or off), you should explicitly turn them on (or off). That's the only way to write code that is never going to change under your feet.
",Lukasa,sigmavirus24
2099,2014-06-18 15:30:44,"@q4478842 Do not test whatever you're doing against this repository.
",sigmavirus24,q4478842
2095,2014-06-12 13:14:46,"@sigmavirus24 @dstufft Both good points that I hadn't considered. I'm moving back down to -0, waiting to hear from some of the other big guns. =)
",Lukasa,sigmavirus24
2095,2014-06-12 13:14:46,"@sigmavirus24 @dstufft Both good points that I hadn't considered. I'm moving back down to -0, waiting to hear from some of the other big guns. =)
",Lukasa,dstufft
2095,2014-06-12 17:38:04,"@ericfrederich It does control whether we follow redirects. `resolve_redirects` returns a generator, so if `allow_redirects` is false that generator doesn't get consumed and no redirects get followed.

The key thing to note is that requests is not, in fact, a browser, and there are times users are going to want more control. With that said, the bigger problem is that we're strongly resistant to adding more fields to the `Session`. That's why I wanted @kennethreitz's insight, just to see if he thinks this is a reasonable exception.
",Lukasa,ericfrederich
2095,2014-06-12 18:01:56,"requests is obviously not a ""browser,"" but it is an HTTP client, and HTTP/1.1 10.3.2 reads:

> The requested resource has been assigned a new permanent URI and any future references to this resource SHOULD use one of the returned URIs.

If you're opposed to requests following recommendations in the HTTP specification by default, please justify why the recommendation should not guide the default behavior of requests.

@ericfrederich: caching is a first class citizen in HTTP, so I agree it should be a first class citizen in a requests session.
",dolph,ericfrederich
2095,2014-06-12 18:06:22,"@dolph We've had this discussion many times. The specifications apply to _user agents_, and I've argued that requests by itself is not a whole user agent. There are some actions we delegate to the user of requests.

Now, to be clear, I have not taken a strong position on this one way or another, aside from to suggest that we should err on the side of back-compatibility and respect the requests feature freeze. At this stage I'm keeping an open mind as to both arguments.

Note, finally, that the normative language in that section is SHOULD, not MUST. =)
",Lukasa,dolph
2095,2014-06-12 18:09:16,"Interesting @dolph .
@Lukasa , I caught the ""SHOULD"" as well, but @dolph  did state it is only recommendation.

It seems this comes down to whether requests is just a http library or a http client.
Perhaps a http library shouldn't do caching but a http client should?
So, the question of the day: is a requests.Session object a client?... or is it just some percentage of a client?
",ericfrederich,dolph
2095,2014-06-12 18:09:16,"Interesting @dolph .
@Lukasa , I caught the ""SHOULD"" as well, but @dolph  did state it is only recommendation.

It seems this comes down to whether requests is just a http library or a http client.
Perhaps a http library shouldn't do caching but a http client should?
So, the question of the day: is a requests.Session object a client?... or is it just some percentage of a client?
",ericfrederich,Lukasa
2095,2014-06-12 18:11:39,"@Lukasa My apologies for not being aware of prior discussions. Is there a documented guideline on where requests draws the line between the actions delegated to users of the library and requests' own responsibilities?
",dolph,Lukasa
2095,2014-06-12 18:15:04,"@dolph No need to apologise. =) Just wanted to make you aware of the context.

The short answer is no, we tend to do it by feel. Makes deciding these issues interesting. =P
",Lukasa,dolph
2095,2014-06-12 23:17:05,"@johnsheehan you're ready to jump ship already?
I hope your solution isn't going after the head of some github repo.
I'm all for having an opt-in / opt-out option.
You could do it yourself or I might be able to get to it sometime.
I just contributed 2 patches out of the blue while experimenting with a new design.
",ericfrederich,johnsheehan
2095,2014-06-13 00:40:29,"@johnsheehan You can pin requests to 2.3.0 until we refine this feature.
",sigmavirus24,johnsheehan
2095,2014-06-13 01:11:01,"I'm not taking it out immediately as there are mitigations like pinning and this isn't shipped yet, but I've made a note to our team to plan accordingly since this would have a direct impact on our customers' expected product experience. My opinion is that this shouldn't have been merged without an explicit opt in/out but I respect the  decision of the maintainers recognizing that our situation is likely unique.On Thu, Jun 12, 2014 at 5:41 PM -0700, ""Ian Cordasco"" notifications@github.com wrote:

@johnsheehan You can pin requests to 2.3.0 until we refine this feature.

—Reply to this email directly or view it on GitHub.
",johnsheehan,johnsheehan
2095,2014-06-13 05:33:21,"@johnsheehan This is very easily disabled.


",Lukasa,johnsheehan
2094,2014-06-12 13:09:49,"Thanks for all the effort you put in @moliware ! Even though we didn't accept the work, we appreciate your effort to make requests better :)
",sigmavirus24,moliware
2093,2014-07-29 18:06:49,"I'm sincerely sorry if my response came across as argumentative @kevin1024. That really wasn't my intention. As for whether or not Betamax should use pytest-httpbin we should probably discuss that on an issue in Betamax. :)
",sigmavirus24,kevin1024
2092,2014-06-11 13:12:35,"I second @Lukasa's far more explicit check for `None`.
",sigmavirus24,Lukasa
2090,2014-06-11 21:33:46,"Thanks @ericfrederich !
",sigmavirus24,ericfrederich
2088,2014-06-10 13:19:29,"@jeffknupp In general this is great and I'm sure we'll be happy to have it. However, please note that we view PEP8 more as guidelines than actual rules (see #1899), and there's some things that Kenneth actively dislikes, so don't get too invested by changing other parts of the code. =)
",Lukasa,jeffknupp
2086,2015-11-24 01:13:02,"@Lukasa  @kennethreitz 
Hey guys, for the time being, I don't think we have a obvious solution yet, but can we at least make this `ISO-8859-1` optional? 



This looks way too brutal. Some parameters like `Session(auto_encoding=False)` would be nice. What do you guys think?
",est,kennethreitz
2086,2015-11-24 01:13:02,"@Lukasa  @kennethreitz 
Hey guys, for the time being, I don't think we have a obvious solution yet, but can we at least make this `ISO-8859-1` optional? 



This looks way too brutal. Some parameters like `Session(auto_encoding=False)` would be nice. What do you guys think?
",est,Lukasa
2086,2015-11-24 08:03:31,"@est ISO-8859-1 _is_ optional, you can simply set `response.encoding` yourself. It's a one-line change. =)
",Lukasa,est
2086,2015-12-15 17:07:37,"@Lukasa But you can't determine whether the initial `response.encoding` came from the `Content-Type` header or whether it's the `ISO-8859-1` fallback, which means if you want to avoid the fallback you have to start parsing the `Content-Type` header yourself, and that's quite a lot of extra complexity all of a sudden.
",gsnedders,Lukasa
2086,2015-12-15 17:10:11,"@gsnedders Sure you can. `if 'charset' in response.headers['Content-Type']`.
",Lukasa,gsnedders
2086,2015-12-15 17:21:18,"@gsnedders Try `if 'charset=' in response.headers['Content-Type']`. At this point we're out of 'crazy' and into 'invalid formatting'.
",Lukasa,gsnedders
2086,2015-12-15 17:32:28,"@Lukasa uh, I thought there was whitespace (or rather what 2616 had as implicit *LWS) allowed around the equals, apparently not.

The grammar appears to be:



So I guess the only issue here is something like `text/html; foo=""charset=bar""`.

FWIW, html5lib's API changes are going to make the correct behaviour with requests require something like:


",gsnedders,Lukasa
2086,2016-01-09 14:50:11,"@dentarg That's not really a counter example: it's an example of why this system works.

The guidance from the IETF is that for all text/\* encodings, one of the following things MUST be true:
- the peer MUST send a `charset` in the content-type
- the content MUST carry an encoding specifier in it (HTML, XML)

Requests' default behaviour here works well: in the case of XML and HTML, ISO-8859-1 is guaranteed to safely decode the text well enough to let you see the `<meta>` tag. Anyone working with HTML really should go looking for that tag, because servers almost never correctly advertise the content type of the HTML they deliver, but the HTML itself is usually (though sadly not always) right. 

The behaviour requests has right now is probably less prone to failure with HTML than the one proposed for 3.0.0, and part of me does wonder if we should try to solve this more by documentation than by code change.
",Lukasa,dentarg
2086,2016-03-29 08:04:43,"@gsnedders I'm not sure what your point is. ISO-8859-1 will decode incorrectly, but you can still search for the initial `<meta>` tag. It's not _trivial_ but it is not enormously difficult either.
",Lukasa,gsnedders
2086,2016-03-29 09:12:23,"@Lukasa My point is that you're not necessarily just looking for a `<meta>` tag, you could also be wanting to see if the opening bytes are 00 3C 00 3F (which obviously you can do as they'll just be U+0000 U+003C U+0000 U+003F), and then start decoding as UTF-16 until you find the inline encoding declaration. There's a lot more complexity (esp. in the XML case) than just looking for one ASCII string. ISO-8859-1 isn't that bad _insofar_ as it causes no data loss, but it doesn't really make searching any easier.
",gsnedders,Lukasa
2083,2014-06-03 14:25:33,"@Lukasa Makes sense, thankyou.
",JakeAustwick,Lukasa
2080,2014-06-02 13:01:55,"@sylvinus 100% of that level of cookie handling is handled by the standard library (cookielib). We simply implement a few extra methods on top of the standard Cookie Jar.

A quick test shows that this does work as expected:



Do you have a problem with some other website?
",sigmavirus24,sylvinus
2080,2014-06-03 11:52:12,"@sylvinus what you seem to need is to either use a lower level solution so you can pass the proper class to the cookielib methods or to write a custom cookie jar that will be able to use a Response instance from requests retrieve all of the information it needs. If you need further help with this, I would suggest asking a question on [StackOverflow](https://stackoverflow.com/questions/tagged/python-requests).

Regarding adding a test I'm -0, because the tests are already slow enough.
",sigmavirus24,sylvinus
2073,2014-05-28 14:18:34,"@edrevo That's a great idea. Unfortunately, it's quite challenging to do, as we don't normally read from the file ourselves, we pass it down to the lower layers. An alternative might be to adjust [this section of the docs](http://docs.python-requests.org/en/latest/user/advanced/#streaming-uploads) to use `'rb'` as well, to reduce the risk of people getting this wrong.
",Lukasa,edrevo
2072,2014-05-28 13:17:35,"@Lukasa fixed in 5ab79e2
",sigmavirus24,Lukasa
2071,2014-05-28 01:31:51,"#2072 fixes this issue. Thanks for reporting it @miracle2k! :cake: 
",sigmavirus24,miracle2k
2068,2014-05-26 13:45:00,"I agree entirely with @Lukasa. You may want to investigate how (if at all) other libraries do this @taku-pl like [httplib2](https://github.com/jcgregorio/httplib2) or [httpplus](https://code.google.com/p/httpplus/) and use them instead if possible.
",sigmavirus24,taku-pl
2068,2014-05-26 13:45:00,"I agree entirely with @Lukasa. You may want to investigate how (if at all) other libraries do this @taku-pl like [httplib2](https://github.com/jcgregorio/httplib2) or [httpplus](https://code.google.com/p/httpplus/) and use them instead if possible.
",sigmavirus24,Lukasa
2068,2014-05-28 13:18:39,"Then you can probably do this with pyCurl @taku-pl. I always forget it's an option because of its terrible API. =P
",sigmavirus24,taku-pl
2066,2014-05-26 15:27:08,"Thanks. @Lukasa, can `response.request` be added to the [Response documentation](http://docs.python-requests.org/en/latest/api/#requests.Response)?
",migurski,Lukasa
2066,2014-05-26 15:42:04,"@migurski Good idea, and done. See #2069.
",Lukasa,migurski
2065,2014-05-27 15:24:40,"@sigmavirus24 noted.
",kennethreitz,sigmavirus24
2064,2014-05-26 13:37:14,"I could be wrong, but I think I remember @dstufft saying that pip support for 3.2 wasn't a high priority. That aside, I'm not quite sure why we support using simplejson. If @mgeisler hadn't linked to #710 I would have guessed it was to support Python 2.5 (which we no longer do).
",sigmavirus24,mgeisler
2063,2014-05-27 20:55:21,"@asmeurer is conda looking for proxies on its own? If so I would guess that they're getting the same proxies we are (which is why it would work on the first try -- they come out of the ) and then they're trying to update them but it still pulls from the env
",sigmavirus24,asmeurer
2063,2014-05-28 00:32:41,"@asmeurer I'm not sure what the issue linked has to do with the proxy problems. That said, I see that you're setting the `proxies` attributes on the Session instance [here](https://github.com/conda/conda/blob/96aaccefddce58850b07f414dce95192e6b1a697/conda/connection.py#L60). The easy solution to this bug is to simply also do `self.trust_env = False`. That will prevent us from loading from the environment each time. You're already loading proxies from the environment in `get_proxy_servers`. The only things you'll lose by setting `trust_env` to `False` are:
- setting `verify` based on `REQUESTS_CA_BUNDLE` or `CURL_CA_BUNDLE`
- using authentication defined in `.netrc`
",sigmavirus24,asmeurer
2063,2014-05-28 01:36:56,"> Oh d'oh. That would be me not copying the url correctly. Try the updated comment.

No worries. I wasn't sure if I was missing something =P

> .netrc seems potentially important

It can be. That's why I mentioned it as a downside :)

>  I just opened this issue because it was confusing and I don't think I should have to do it. 

I agree. I don't think you should have to either. It seems, however, that this is a duplicate of #2018 after all. @Lukasa should we close this in deference to #2018?
",sigmavirus24,Lukasa
2063,2014-05-28 08:06:33,"Agreed, I think us fixing our proxy source priorities should fix your problem, @asmeurer. =)
",Lukasa,asmeurer
2062,2014-05-24 18:41:55,"Hey @joe42 There are several headers we don't expect users to set on their own because they really shouldn't be. If you want to prevent us from overwriting the header you should use the `auth=` parameter to your request. By default it takes a two-tuple (username, and password) or any other Authentication Handler.
",sigmavirus24,joe42
2062,2014-05-24 21:10:04,"@sigmavirus24 Sorry, I should have mentioned that I already circumvented the problem by writing an authentication handler that does not modify the request. My post was meant as a suggestion to improve requests, as I suppose other users might run into the same problem.

> Hey @joe42 There are several headers we don't expect users to set on their own because they really shouldn't be.

I see. I do set several headers directly. Can you point me to a resource explaining which headers should not be set and why? Setting the headers directly seemed to be the way to go after reading the documentation's ""Custom Headers"" section.

Thx,
Joe
",joe42,joe42
2062,2014-05-24 21:10:04,"@sigmavirus24 Sorry, I should have mentioned that I already circumvented the problem by writing an authentication handler that does not modify the request. My post was meant as a suggestion to improve requests, as I suppose other users might run into the same problem.

> Hey @joe42 There are several headers we don't expect users to set on their own because they really shouldn't be.

I see. I do set several headers directly. Can you point me to a resource explaining which headers should not be set and why? Setting the headers directly seemed to be the way to go after reading the documentation's ""Custom Headers"" section.

Thx,
Joe
",joe42,sigmavirus24
2062,2014-05-25 10:21:57,"@Lukasa : I can understand why you choose to design it this way. But I think that the result of the following request with default credentials in .netrc  is not obvious at all. The point I do still not get is that I cannot imagine a case where a user would actually set a specific header (like in your example) and not expect it to be used by requests. Why should he bother doing this? So it might be good to either make it obvious (by documentation/warning/error) or to respect the users explicit settings.


",joe42,Lukasa
2062,2014-05-25 18:13:28,"> So it might be good to either make it obvious (by documentation/warning/error) or to respect the users explicit settings.

I appreciate that you've shared your opinion @joe42. You're right that `explicit is better than implicit` and in this case you can explicitly turn off requests using your `.netrc` file. With it off, you wouldn't have this trouble. That said, the documentation should be improved to explain which headers users should refrain from setting themselves. We will not try to internally keep track of which headers were provided by the users and which we have generated or which ones are generated by urllib3 or which ones are provided by httplib. It is certainly possible to do, but it is not something we will do. Further, if we allow users to set their own headers in cases like this (and others) it will likely lead to far more cases where the user is surprised by requests' behaviour. We're doing the right thing here.
",sigmavirus24,joe42
2062,2014-05-26 15:44:26,"I had a related problem in #2066.

My issue arose using Heroku’s API, which has two confusing factors relevant to this issue. The toolbelt [quietly creates a .netrc file](https://devcenter.heroku.com/articles/heroku-command#logging-in) (mine was a year old; I didn't know it existed) and Heroku asks users to [create a custom Authorization header](https://devcenter.heroku.com/articles/oauth#web-application-authorization):

> For example, given the access token _01234567-89ab-cdef-0123-456789abcdef_, request headers should be set to _Authorization: Bearer 01234567-89ab-cdef-0123-456789abcdef_.

I am now using `Session.trust_env` to prevent this issue thanks to @Lukasa, but I found the silent overwrite of the `Authorization` header to be a nasty surprise. I’m in agreement with @joe42 on this: specifically providing a header ought to overrule environmental factors. Alternatively, could the `.netrc` behavior be documented [under API](http://docs.python-requests.org/en/latest/api/#requests.request), [custom authentication](http://docs.python-requests.org/en/latest/user/advanced/#custom-authentication), and [quickstart](http://docs.python-requests.org/en/latest/user/quickstart/#custom-headers)? My searches for “auth” in the requests documentation should have mentioned this.
",migurski,joe42
2062,2014-05-26 15:44:26,"I had a related problem in #2066.

My issue arose using Heroku’s API, which has two confusing factors relevant to this issue. The toolbelt [quietly creates a .netrc file](https://devcenter.heroku.com/articles/heroku-command#logging-in) (mine was a year old; I didn't know it existed) and Heroku asks users to [create a custom Authorization header](https://devcenter.heroku.com/articles/oauth#web-application-authorization):

> For example, given the access token _01234567-89ab-cdef-0123-456789abcdef_, request headers should be set to _Authorization: Bearer 01234567-89ab-cdef-0123-456789abcdef_.

I am now using `Session.trust_env` to prevent this issue thanks to @Lukasa, but I found the silent overwrite of the `Authorization` header to be a nasty surprise. I’m in agreement with @joe42 on this: specifically providing a header ought to overrule environmental factors. Alternatively, could the `.netrc` behavior be documented [under API](http://docs.python-requests.org/en/latest/api/#requests.request), [custom authentication](http://docs.python-requests.org/en/latest/user/advanced/#custom-authentication), and [quickstart](http://docs.python-requests.org/en/latest/user/quickstart/#custom-headers)? My searches for “auth” in the requests documentation should have mentioned this.
",migurski,Lukasa
2062,2014-05-26 16:08:59,"@migurski Whew, I was worried we had a bigger discoverability problem. Our docs are pretty large, sadly, which makes it easy to miss things. =(
",Lukasa,migurski
2062,2014-05-26 16:20:04,"@Lukasa: Thanks for your detailed answer. 

> headers are the lowest source of truth, and we feel quite happy to replace many kinds of headers that the user sets. Some examples:
> 
>    We will replace Authorization headers when an alternative auth source is found.
>     We will remove Authorization headers when you get redirected off-host.
>     We will replace Proxy-Authorization headers when you provide proxy credentials in the URL.
>     We will replace Content-Length headers when we can determine the length of the content.
> 
> We do this for very good reason: a headers-based API is utterly terrible. Affecting the library behaviour based on arbitrary key-value pairs set in the headers dictionary is asking for all kinds of terrible bugs

It would be great if you could add this sentence to the ""Custom Headers"" section, after it states:

> If you’d like to add HTTP headers to a request, simply pass in a dict to the headers parameter.
",joe42,Lukasa
2062,2014-05-26 17:08:22,"@sigmavirus24 Do you feel like this would be a good addition to the docs?
",Lukasa,sigmavirus24
2062,2015-04-03 13:54:01,"@benjaminran Agreed, that looks like extremely useful documentation to me. If you open a pull request that adds it, I'll happily merge it. =)
",Lukasa,benjaminran
2062,2015-04-03 14:45:49,"@benjaminran one thing: documentation shouldn't be overly verbose but they also shouldn't be too concise. The difficulty in writing documentation is finding the right balance such that everyone can understand what's going on without reading a novel. ;)
",sigmavirus24,benjaminran
2062,2015-12-21 02:45:22,"I was about to suggest that we close this issue, since the documentation given by @benjaminran is now merged into the master docs (in Quickstart->Custom Headers). However I have one more question: what happens when the `auth=` parameter is used, AND the `.netrc` environment config is set? It is not clear from the docs which one takes precedence. And as @migurski pointed out, we have a definite case where a hosting provider (Heroku) quietly created a `.netrc` file, so it would be important to know this. 

I suspect that `auth=` should override `.netrc`, but I wanted to confirm before creating a PR.
",ibnIrshad,benjaminran
2062,2015-12-21 02:45:22,"I was about to suggest that we close this issue, since the documentation given by @benjaminran is now merged into the master docs (in Quickstart->Custom Headers). However I have one more question: what happens when the `auth=` parameter is used, AND the `.netrc` environment config is set? It is not clear from the docs which one takes precedence. And as @migurski pointed out, we have a definite case where a hosting provider (Heroku) quietly created a `.netrc` file, so it would be important to know this. 

I suspect that `auth=` should override `.netrc`, but I wanted to confirm before creating a PR.
",ibnIrshad,migurski
2062,2015-12-21 14:44:49,"@ibnIrshad please do not create a PR. That would be a breaking change that we will not accept until 3.0.0
",sigmavirus24,ibnIrshad
2061,2014-05-24 14:08:44,"> Uh...as far as I can see on a quick code read, we absolutely pick up the proxy settings from the Session. What makes you think we're not?

My guess would be that once we have made a request with a proxy, e.g., we've used the `http` proxy, then if @asmeurer changes the list of proxies it isn't fixed because we're using the same `HTTPAdapter` which hits [lines 209 through 215](https://github.com/kennethreitz/requests/blob/a7c218480d7acf1e866e07fde0627d05fb77fbc1/requests/adapters.py#L209..L215). Notice that if `http` is already in `self.proxy_manager` we don't do anything. Since it is, the new proxy is ignored. I haven't attempted to test if that's in fact the case, but that's my first guess as to what might be causing the behaviour that @asmeurer is seeing.

> As for 'prompting' for proxy auth, that's not going to happen, it's simply not requests' job.

I agree.
",sigmavirus24,asmeurer
2061,2014-05-25 18:34:08,"So I want to be sure I'm understanding everyone's concerns here properly:

@asmeurer when you say you'd like requests to prompt for proxy auth, do you mean you'd like us to literally use `raw_input` (or `input` in the case of Python 3)? I'm pretty sure that's not what you want and that's not something we'll ever support. Further, I'm not quite certain how we would properly implement a callback system for this particular case since the only other system like it in requests relies on having a response which we don't have in this case.

That said, we've had a troubled history (which @Lukasa knows much better than I) dealing with HTTPS Proxy Authentication. If there were a better way of handling them, we would have already implemented it (I'd like to think).

This discussion should remain in this issue. Your other problem @asmeurer (in which mutating a Session's proxies does not affect subsequent requests should be a separate issue). I'm trying to think of a good way to handle that case since I think I've located the problem above.
",sigmavirus24,Lukasa
2061,2014-05-25 18:34:08,"So I want to be sure I'm understanding everyone's concerns here properly:

@asmeurer when you say you'd like requests to prompt for proxy auth, do you mean you'd like us to literally use `raw_input` (or `input` in the case of Python 3)? I'm pretty sure that's not what you want and that's not something we'll ever support. Further, I'm not quite certain how we would properly implement a callback system for this particular case since the only other system like it in requests relies on having a response which we don't have in this case.

That said, we've had a troubled history (which @Lukasa knows much better than I) dealing with HTTPS Proxy Authentication. If there were a better way of handling them, we would have already implemented it (I'd like to think).

This discussion should remain in this issue. Your other problem @asmeurer (in which mutating a Session's proxies does not affect subsequent requests should be a separate issue). I'm trying to think of a good way to handle that case since I think I've located the problem above.
",sigmavirus24,asmeurer
2061,2014-05-27 17:26:05,"Not sure if this discussion is resolved or not, but please ping me again if I still have an action item/question. I will be home tomorrow for more in-depth reading. :)

@Lukasa If this is still an open question, handling special-handling 407 in urllib3 sounds sensible if we can do it in a low-impact way.
",shazow,Lukasa
2057,2014-05-22 18:28:11,"The purpose of `dict_class` is stated in the docstring:

> If a setting is a dictionary, they will be merged together using `dict_class`.

This means that `dict_class` is intended to control the merging logic. An example here is that of `MultiDict`, which will treat merging settings fundamentally very differently to the way a `dict` or `OrderedDict` would.

_With that said_, I have no particular objection to having `merge_setting` return the `dict_class`. Clearly @sigmavirus24 doesn't agree, but I'll try to have a chat with him and see why we're on different pages. I suspect he's seeing something I'm not. =)
",Lukasa,sigmavirus24
2054,2014-05-21 00:15:46,"It is not the most common behaviour ever but it is in fact a very deliberate design decision. We wrap all errors provided to us by urllib3 (and even some that bubble up from underneath urllib3) and present them to the user in a predictable way. However, for the more advanced user, being able to inspect those lower level error messages (and in fact when trying to help users it is very useful for us as well) is invaluable. You very easily ensure that `e.message` is a string by calling `str` on it if you can manage that yourself. That aside, I consider it your error reporting tools fault for not being more paranoid about the data it collects from user's code.

I appreciate that you took the time to raise this issue @fletom but I don't think we're going to change how we handle exceptions.

Cheers!
",sigmavirus24,fletom
2053,2014-05-20 16:56:58,"@Lukasa as you might see in its source code, it applies gevent.monkeys.patch_all, which is known to break various things and is a generally dangerous thing to do.  (worse, it does that on import-)
",HoverHell,Lukasa
2053,2014-05-20 17:02:24,"What specifically does it break for you @HoverHell?
",sigmavirus24,HoverHell
2053,2014-05-20 17:36:04,"@sigmavirus24 I think that's mostly out of the issue. Regardless, it threw an “this call would block forever” exception (on a code which otherwise works normally) which is enough to be suspicious; additionally (but not critically), it is known to have some problems with celery, raven, possibly uwsgi, and various other things. Not quite something I would want to deal with in an actively used django application.
",HoverHell,sigmavirus24
2053,2014-05-24 09:11:56,"@HoverHell grequests isn't exactly an alternative implementation; it's a little more like an example script or basic wrapper with some convenience methods (like mapping).

Some slightly less ham-handed monkeypatching might accomplish what you want, perhaps by doing `httplib.socket = gevent.socket; urllib3.socket = gevent.socket; requests.socket = gevent.socket`. Otherwise, there is no real way of doing it ""the right way"" due to requests' dependence on urllib3, and urllib3's dependence on httplib from the standard lib. Your geventhttpclient adapter would likely be the overall safest bet in that regard (and would probably give you some speed increase as well, due to geventhttpclient being written specifically for performance with gevent), but of course it's going to bring its own problems as you say.

For what it's worth, though, I've been using `gevent.monkey.patch_all()` in many of my applications for a long time with utterly no issues. I can see how it might cause issues with other big network-based third party libraries, but I think it's worth trying the monkey patching in new code before looking at alternatives. I can confirm that global monkey patching seems to work just fine with requests at least, without any need for grequests.

> it threw an “this call would block forever” exception (on a code which otherwise works normally) which is enough to be suspicious

It is rare to see such an exception when using gevent as intended, I believe.
",Anorov,HoverHell
2053,2014-05-25 06:33:54,"@Anorov 

> Your geventhttpclient adapter would likely be the overall safest bet in that regard
> Certainly; my question is: can it be done in a less hack-ish / more future-proof way, perhaps by integrating it more with the requests library e.g. making few minor changes in the methods arhitecture of the requests connection / connectionpool modules?

@sigmavirus24 

> I agree. I don't think I've ever seen that problem before when using gevent correctly.

I wasn't even using gevent at that point – I simply installed grequests, and another imported library (which wasn't being used either) imported grequests; in all the other regards it is simply a django application.

(though, thinking of it, I could guess that it is the late importing that made it problematic; still, I expect other problems regardless)
",HoverHell,sigmavirus24
2053,2014-05-25 06:33:54,"@Anorov 

> Your geventhttpclient adapter would likely be the overall safest bet in that regard
> Certainly; my question is: can it be done in a less hack-ish / more future-proof way, perhaps by integrating it more with the requests library e.g. making few minor changes in the methods arhitecture of the requests connection / connectionpool modules?

@sigmavirus24 

> I agree. I don't think I've ever seen that problem before when using gevent correctly.

I wasn't even using gevent at that point – I simply installed grequests, and another imported library (which wasn't being used either) imported grequests; in all the other regards it is simply a django application.

(though, thinking of it, I could guess that it is the late importing that made it problematic; still, I expect other problems regardless)
",HoverHell,Anorov
2053,2014-05-25 18:19:25,"@HoverHell I'm starting to question whether this issue is even suited for this project. Your trouble seems to arise from that project, not requests, and you've been dismissive of efforts to try to help you in using it. I'm also surprised you're intent on using gevent when you're so convinced that it's going to cause you so many problems.
",sigmavirus24,HoverHell
2053,2014-05-25 18:40:53,"@sigmavirus24 I'm intent on using it (or an alternative, really) without _potentially_ problematic monkeypatching (sometimes even small chances of breaking something are too much); that's why I need more support from the requests.
",HoverHell,sigmavirus24
2053,2014-05-25 19:36:32,"@HoverHell you haven't told us how we can support you further though? There aren't changes that need to be made to requests to support gevent either by monkey patching everything or only a small number of things. We've tried to help but with every suggestion we're told it is insufficient.
",sigmavirus24,HoverHell
2053,2014-05-25 22:34:46,"@HoverHell I believe my suggestion of monkeypatching the `socket` module in `requests` and all of its dependencies are the quickest ""less-hacky"" ways of accomplishing this.

gevent itself was kind of designed as a hack: a way to add green threads into Python without a new interpreter and/or new standard lib. The gevent designer(s) tried to make the monkeypatched modules as backwards compatible as possible, but obviously monkeypatching will never be perfect.

Your absolute safest bet, if you wanted to make a fully compatible clone that defers to geventhttpclient and does no actual monkeypatching, is probably to make an alternative urllib3 implementation that uses geventhttpclient's swap-in httplib as described in [its README](https://github.com/gwik/geventhttpclient/blob/master/README.mdown):



urllib3 doesn't really support adapters, so you'd have to make a new `geurllib3` module I suppose.

You'd basically just change a few of the imports in `connection.py` [here](https://github.com/shazow/urllib3/blob/98c6fbfc27d7d51327ad85a85a80dd4fe096cc79/urllib3/connection.py#L14-L30).

Then you should probably be able to subclass `HTTPAdapter` and use the new `geurllib3` module's classes and functions instead.

This would have to rely on geventhttpclient's version of httplib being 100% compatible with the standard lib's. I'd recommend running through the full urllib3 and requests test suite after making these changes. If you discover a bug, you can open an issue on the geventhttpclient repo. 

Either way, I agree this is out of scope as a requests issue. It's just a personal decision you'll have to make.
",Anorov,HoverHell
2053,2014-05-26 05:19:41,"@sigmavirus24, @Anorov  as you can see by the link in the first message, I've already made a no-monkeypatching adapter through subclassing. However, it is variably hack-ish and incomplete and unreliable.

My question, thus, is: what changes can be made to requests that would make it better? The very least would be making `pool_classes_by_scheme` an attribute of the PoolManager; and, likely, adding more attributes like ConnectionCls.

Basically, my problem is that the whole default _HTTPAdapter_ of the requests is nearly a monolithic untweakable thing if you don't count in monkeypatching.
",HoverHell,Anorov
2053,2014-05-26 05:19:41,"@sigmavirus24, @Anorov  as you can see by the link in the first message, I've already made a no-monkeypatching adapter through subclassing. However, it is variably hack-ish and incomplete and unreliable.

My question, thus, is: what changes can be made to requests that would make it better? The very least would be making `pool_classes_by_scheme` an attribute of the PoolManager; and, likely, adding more attributes like ConnectionCls.

Basically, my problem is that the whole default _HTTPAdapter_ of the requests is nearly a monolithic untweakable thing if you don't count in monkeypatching.
",HoverHell,sigmavirus24
2051,2014-05-19 12:09:43,"No worries @Lawouach ! We appreciate that you took the time to report it
",sigmavirus24,Lawouach
2050,2014-05-18 01:42:48,"This is great work @Hasimir! I really appreciate your effort in putting this together.

Personally, I think this is better suited to a blog post and not the documentation. I'll leave this open, though, until @Lukasa wakes up. :)
",sigmavirus24,Hasimir
2050,2014-05-18 08:03:15,"This is brilliant, @Hasimir! Unfortunately, I don't think it belongs in the documentation. =)

Requests documentation doesn't really contain 'recipes', or pre-canned instructions for how to do specific things. This is for two reasons: firstly, they add greatly to the (already not inconsiderable) size of the docs; and secondly, they then become something that needs to be actively maintained. This is harder for recipes like this one because they depend on external projects, meaning that if those projects change our documentation becomes out of date with no real way for us to spot it.

I agree that a blog post is a good idea. You'll also find that a good blog post providing a tutorial on using Requests in a certain way has a tendency to move up quite high in the Google listings. As two examples from my own blog, the specific search [""force ssl version python requests""](https://www.google.co.uk/search#q=force+ssl+version+python+requests) has my blog post as the top entry, and even the fairly general [""proxies python requests""](https://www.google.co.uk/search#q=proxies+python+requests) has my blog post on the front page. This is really where the information you've provided should belong. =)
",Lukasa,Hasimir
2050,2014-05-18 11:48:12,"@Hasimir Do send me and @sigmavirus24 tweets when you write it and we'll make sure we RT and talk about your blog post lots. =D
",Lukasa,Hasimir
2050,2014-05-18 11:48:12,"@Hasimir Do send me and @sigmavirus24 tweets when you write it and we'll make sure we RT and talk about your blog post lots. =D
",Lukasa,sigmavirus24
2049,2014-05-17 22:44:58,"LGTM. @codedstructure you should checkout [the toolbelt](https://gitlab.com/sigmavirus24/toolbelt). We just moved the `SourceAddressAdapter` there so people can use it without having to maintain their own version.
",sigmavirus24,codedstructure
2049,2014-05-18 08:07:02,"This looks great @codedstructure, but I want to bikeshed here for a moment. I'm wondering whether the right API is actually for `proxy_manager_for` to take an arbitrary keyword argument dict that it passes to `proxy_from_url`. This saves several lines from the `SourceAddressAdapter` because it can now do:



Thoughts?
",Lukasa,codedstructure
2049,2014-05-18 14:30:35,"@Lukasa that sounds like a good idea. The revised PR would look like this then?


",sigmavirus24,Lukasa
2049,2014-05-18 19:18:10,"Thanks both, I'm 100% in favour of anything which makes adapters easier to write. The PR code now looks like @sigmavirus24' code above.

Although while we're at it / (& for consistency), should `init_poolmanager` also have a similar kwargs parameter `pool_kwargs`?
",codedstructure,sigmavirus24
2049,2014-05-18 19:30:07,"Consistency is the hobgoblin of the foolish mind. That aside, I'm in favor of consistency. I just like keeping PRs narrowly focused to one topic. If you're going to make that change, _I_ would prefer it in a separate PR, but @Lukasa should feel free to disagree. 
",sigmavirus24,Lukasa
2049,2014-06-09 14:59:09,"@kennethreitz I think we're good to go at this stage. =)
",Lukasa,kennethreitz
2048,2014-05-17 21:17:37,"And yes I'm :+1:. Go for it @codedstructure 
",sigmavirus24,codedstructure
2046,2014-05-16 21:05:47,"Thanks @alex 
",sigmavirus24,alex
2044,2014-05-15 15:07:53,"@p-clem what's the `filename` of `object.image`, i.e., `object.image.filename`? I'm guessing it has unicode characters which cannot be decoded with `utf-8`.
",sigmavirus24,p-clem
2044,2014-05-19 07:31:43,"@sigmavirus24  You forgot to add the authentification. Without it always worked for me but with authentification it didnt.

Anyway I tried to test it out on an other environment and it worked without problems, even on production! So this bug only appears on my work PC which is frustrating but oh well...

Thank you for your help,
",p-clem,sigmavirus24
2043,2014-08-26 08:46:47,"@naveenjindal I'm afraid the issue tracker is not the place to ask for programming assistance, it's the place to report bugs. I recommend resources like reddit.com/r/learnpython. Alternatively, try googling it: I'm sure you'll find an example.
",Lukasa,naveenjindal
2042,2014-06-12 12:48:01,"@untitaker Way ahead of you, see #2086. =)
",Lukasa,untitaker
2038,2014-05-12 18:59:41,"@Lukasa @sigmavirus24 alright, let's get a release ready. :)
",kennethreitz,Lukasa
2036,2014-05-08 09:19:54,"@Lukasa Yes, I want requests not to explicitly consult a proxy. I use ISA Client for proxy-transparency.

I do not set the environment variables `HTTP_PROXY` or `HTTPS_PROXY`. Other programs work transparently with ISA Server proxy via ISA Client.

Thank you for info about `NO_PROXY=1`. It works! My troubles are ended for ""python networking"". :)
",espdev,Lukasa
2036,2015-10-26 13:44:58,"You may download the tar.gz/zip file and pass it to the pip install
command.
(it's my workaround behind corp proxy)

On Mon, Oct 26, 2015 at 12:43 PM, Fan Du notifications@github.com wrote:

> @espdev https://github.com/espdev Hi, I've been reading all posts about
> this problems by you. Followed your instructions but anyhow couldn't make pip
> install work behind NTLM. :(
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/2036#issuecomment-151121746
> .
",HelioGuilherme66,espdev
2035,2014-05-05 13:46:31,"@paramiao I'm not angry with you voicing your opinion. I was just voicing mine. I'll elaborate a bit more:

Cookies can be set during a redirect. We keep track of those and send them on subsequent requests (if appropriate). We cannot do that regardless of whether you set the prior cookie header or not. We construct the `Cookie` header ourselves when necessary and happily delete it on redirects only to reconstruct it.

If you want the header you set to persist, I suggest you take control of redirects using `allow_redirects=False`. This is the only way requests can and will work the way you expect it to. 
",sigmavirus24,paramiao
2029,2014-05-06 17:56:34,"@GP89 which SSL backend and Python interpreter are you using?
I can't reproduce a SSLError on read timeout, only with a handshake using the stdlib. The handshake with PyOpenSSL never times out...
",t-8ch,GP89
2028,2014-04-30 14:40:54,"@sigmavirus24 @Lukasa I'm using 2.2.1 from pypi...

I saw in #2026 that it was ""fixed"" by installing 2.2.1 from pypi, but mine IS a normal, vanilla version...
",poolski,Lukasa
2028,2014-04-30 14:40:54,"@sigmavirus24 @Lukasa I'm using 2.2.1 from pypi...

I saw in #2026 that it was ""fixed"" by installing 2.2.1 from pypi, but mine IS a normal, vanilla version...
",poolski,sigmavirus24
2028,2016-09-13 11:56:26,"@bochecha Nothing breaks without a change. _Something_ in your environment has changed. There are a few options:
1. Your code has changed. The Python import system is fragile and weirdly global. To work this out, do a binary search through your commits until you can find the breakage. The advantage of this is that it should be easily reproducible on more or less any system.
2. Your dependencies have changed. You appear to have pinned Requests: have you pinned everything else? Check whether any of your dependencies have pushed updates recently.
3. Your OS has changed. Do you control your OS image on Codeship?

Basically: code that is exactly the same doesn't just magically change. Given that the one thing you definitely _haven't_ changed is Requests, I think it's safe to say that Requests should not be the first place you look for problems. =D
",Lukasa,bochecha
2027,2014-04-29 15:51:43,"Hi @gauravp2003,

Much of the documentation around requests, including the [README](https://github.com/kennethreitz/requests#contribute), suggests that the issue tracker is for bugs or feature requests, not questions. If you have questions, the should be asked on [StackOverflow](https://stackoverflow.com/questions/tagged/python-requests). @Lukasa, someone else, or I will answer it there.

Thanks for your interest in requests,
",sigmavirus24,gauravp2003
2025,2014-04-29 09:05:37,"Tagging this as contributor friendly because it's perfect for @sigmavirus24 to pair with a new developer on.
",Lukasa,sigmavirus24
2025,2014-06-17 19:44:25,"@gbromios That's the only way to do it at the moment. Note, however, that the Content-Type is wrong. =)
",Lukasa,gbromios
2023,2014-04-29 00:22:43,"@kennethreitz this will help a great deal when debugging issues where people are running into errors with HTTPS connections.
",sigmavirus24,kennethreitz
2023,2014-05-02 19:20:16,"@kennethreitz I don't think we can do that. We want to know if the user installed the dependencies to make the top level import/execution work.
",sigmavirus24,kennethreitz
2023,2014-05-13 01:55:11,"I agree with @t-8ch that this belongs here, even if it's hidden somewhere
",sigmavirus24,t-8ch
2022,2014-04-26 20:55:00,"@t-8ch Thanks for taking a look at this, I'm a bit confused. OpenSSL makes my life really hard =(
",Lukasa,t-8ch
2022,2014-04-26 20:57:44,"@t-8ch I haven't installed PyOpenSSL if that's what you're asking?

I would have assumed (perhaps incorrectly) that `pip install requests` should give me everything I need to successfully call `requests.get('...')` on an HTTPS page.  Which, of course, it works for the most part, just not for this site for some reason. 
",jaddison,t-8ch
2022,2014-04-26 20:58:30,"@jaddison It _mostly_ does. Unfortunately, Python 2.7s standard library sucks hard and doesn't support some features, such as SNI.

I wonder if this is SNI...
",Lukasa,jaddison
2022,2014-04-26 20:59:25,"@jaddison There are two different codepaths behind the scenes. You shouldn't have to care about those, but it helps to know when debugging.

However I can now reproduce this on ubuntu. But only o Py2. On Py3 everything is fine.
I suspect @Lukasa is right and the server fails when the client is not using SNI.
",t-8ch,Lukasa
2022,2014-04-26 20:59:25,"@jaddison There are two different codepaths behind the scenes. You shouldn't have to care about those, but it helps to know when debugging.

However I can now reproduce this on ubuntu. But only o Py2. On Py3 everything is fine.
I suspect @Lukasa is right and the server fails when the client is not using SNI.
",t-8ch,jaddison
2022,2014-04-26 21:00:58,"@jaddison To test whether this is SNI, you'll need to [install the SNI requirements](https://stackoverflow.com/questions/18578439/using-requests-with-tls-doesnt-give-sni-support/18579484#18579484) for Python 2.
",Lukasa,jaddison
2022,2014-04-26 21:02:05,"@Lukasa was right. Compare:


",t-8ch,Lukasa
2022,2015-08-20 22:48:04,"Thanks, @Microserf. I'm pretty much running the same specs (10.9.5, Python 2.7.6 installed via Homebrew but compiled with system provided OpenSSL 0.9.8zg) and this was my entire process for getting `requests` up and running for Django:



Install `requests` with a bunch of [SNI stuff](https://urllib3.readthedocs.org/en/latest/security.html#openssl-pyopenssl), compiled against our new install of OpenSSL. The `[security]` option simply installs `pyopenssl ndg-httpsclient pyasn1`



And we're good to go:


",brianlittmann,Microserf
2022,2016-03-17 05:05:05,"@lsemel thank you, that just saved me a bunch of time
",wiltzius,lsemel
2022,2016-04-01 21:11:21,"@lsemel Are your sure? I tried it on Ubuntu 15.10 and it still doesn't work with Python 2.7.10.

It works with Python 2.7 on Travis CI:
https://travis-ci.org/playing-se/swish-python
",MadSpindel,lsemel
2022,2016-05-31 23:24:30,"@jvanasco what are you using to install those packages? I assume pip. Why are you installing urllib3 and requests separately?
",sigmavirus24,jvanasco
2022,2016-06-01 01:27:10,"Well I needed urllib3 in the virtualenv... but I installed it to try and get the requirements installed by pip and easy_install.  (I used both)

I have a web indexer and a few urls broke.  I wrote a quick script to try the broken ones, and kept reinstalling/delete+installing the packages in the urllib3 instructions on ssl issues until they worked.

On May 31, 2016, at 7:25 PM, Ian Cordasco notifications@github.com wrote:

@jvanasco what are you using to install those packages? I assume pip. Why are you installing urllib3 and requests separately?

—
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub, or mute the thread.￼
",jvanasco,jvanasco
2022,2016-06-15 01:20:12,"@lukas-gitl frustration will not help you solve the problem. Providing us with information about your environment (preferably some - if not all - of the information that we asked Lekinho above) will help.
",sigmavirus24,lukas-gitl
2022,2016-06-15 16:43:06,"@sigmavirus24 Apologies. I meant to provide more information and then got side tracked (since I had no time for this). I'm using Ubuntu 14.04, python 2.7.6 and the latest requests version on pip. This happens when I try to access as API Gateway endpoint (they might be quite restrictive).

I tried removing the virtualenv and regenerating it but unfortunately that didn't solve it.

Let me know what else you need. I switched to nodejs for the time but would be happy to help with a resolution.
",lukas-gitl,sigmavirus24
2022,2016-06-15 16:44:26,"@lukas-gitl It's highly likely that the server you're contacting requires ciphers you aren't offering, or TLS versions you aren't offering. This can be related to the OpenSSL you have installed. You should also try running `pip install requests[security]`: you may be encountering problems with SNI.
",Lukasa,lukas-gitl
2022,2016-06-25 11:36:56,"@jschwinger23 Can you run `pip install pyopenssl ndg-httpsclient pyasn1` as well please?
",Lukasa,jschwinger23
2022,2016-06-25 11:49:21,"@Lukasa Thanks for your reply. I reconfirmed that I did install them:

`$ pip install pyopenssl ndg-httpsclient pyasn1
Requirement already satisfied (use --upgrade to upgrade): pyopenssl in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python
Requirement already satisfied (use --upgrade to upgrade): ndg-httpsclient in /Library/Python/2.7/site-packages
Requirement already satisfied (use --upgrade to upgrade): pyasn1 in /Library/Python/2.7/site-packages`

but code still down.

Anyway, I figured out that everything goes well in Python3, and I am glad to be able to code in python3.
Thank you very much.
",jschwinger23,Lukasa
2022,2016-07-24 18:56:01,"@rohanpai It is likely that you have either no cipher overlap, or that the remote server is unhappy with the versions you're offering, or that you're expected to provide a client cert and are not. It's hard to give more specific advice. Try [this](https://lukasa.co.uk/2016/01/Debugging_With_Wireshark_TLS/) to investigate the issue. 
",Lukasa,rohanpai
2022,2017-02-17 22:52:51,@markstrefford 's solution also worked for me. ,SethGoodLuckMartin,markstrefford
2020,2017-02-12 18:49:40,"@Lukasa  http/2 may not call it chunked transfers, but it does have DATA frames as mentioned here: [rfc7540](http://httpwg.org/specs/rfc7540.html#HttpSequence). The 'pad length' concept is analogous to chunk size.",electronicsguy,Lukasa
2020,2017-02-12 19:23:35,"@electronicsguy pad length is not analagous to chunk size: it is how much padding is attached to the frame. 

More importantly, again, DATA frames are not semantic and should also not be exposed to users. ",Lukasa,electronicsguy
2020,2017-02-13 06:17:37,"@Lukasa   oh I see. Thanks for clearing that up about lad length. But I don't understand why DATA frames are not analogous to chunking. It is in-fact mentioned in this mailing list that it is the case, at least logically: [http/2-chunking](https://lists.w3.org/Archives/Public/ietf-http-wg/2014JulSep/1676.html). Even in the case of http/1.1, chunked transfer encoding is not visible to the users since it is at the transport layer. So when you said above that chunked encoding is an artefact of http/1.1 and not present in http/2, I'm not able to understand how that is.",electronicsguy,Lukasa
2020,2017-02-13 08:19:50,"@electronicsguy So the short answer is that we're in agreement. Note again what I said:

> Note also that chunked encoding is an artefact of HTTP/1.1 and is not present in HTTP/2. That may be an argument *against* exposing it in requests: it's obviously a transport-level detail.

That sentence can be rephrased as:

> Note that because chunked encoding was removed in HTTP/2, it is clearly not a semantic part of HTTP, as HTTP/2 was required to keep a one-to-one semantic mapping to HTTP/1.1. For this reason, we should probably consider not exposing the chunks: they aren't supposed to have semantic meaning to the user.

This is of course backed up by all the relevant RFCs. It's also the course we're taking in the urllib3 v2 work: the boundaries of data chunks are not being exposed to users.",Lukasa,electronicsguy
2018,2014-04-26 22:50:42,"To be clear for those who aren't sure, the way @ouroborus' suggestion differs from the current logic is that we take the proxies from the request, then apply proxies from the environment, then finally apply proxies from the `Session`.

I'm open to re-ordering the precedence of the priorities. @sigmavirus24, thoughts?
",Lukasa,ouroborus
2018,2014-05-04 17:28:09,"It's been noted that this issue is poorly named. I'm not sure what to rename it or even if it should be now that it has been created. @Lukasa, feel free to rename it if and as you see fit.
",ouroborus,Lukasa
2018,2014-08-13 03:10:02,"@Lukasa wasn't this already fixed?
",sigmavirus24,Lukasa
2018,2014-08-13 06:28:04,"@sigmavirus24 Not that I can see. =)
",Lukasa,sigmavirus24
2013,2014-04-23 01:39:47,"Hey @cheecheeo, thanks for opening this!

Your code is fine and the idea is valid. The problem is that this makes the rest of the library inconsistent and it is _not_ desired behaviour. We have no intent to implement this and we have never intended for the a Request's representation to be used like this.

I'll wait for @Lukasa to weigh in, but I'm pretty sure we will not be accepting this contribution.

Cheers
",sigmavirus24,cheecheeo
2013,2014-04-23 06:24:30,"Yeah, I'm with @sigmavirus24 I'm afraid. This is really nice work, but it's just not the way the requests project tends to work. Sorry! :cake:
",Lukasa,sigmavirus24
2011,2014-04-21 16:04:32,"@ctheiss Hey, thanks for raising this issue!

This is something that's come up before, most recently in #1987 but also in #1130 and #1563 (all this year). Kenneth has generally expressed a lack of interest in making this change, expecting that it'd be done using Transport Adapters. If you're interested in getting some guidance as to how you'd do that, I'm happy to help, but I don't think we'll be adding `timeout` to the `Session`.

Sorry we can't be more helpful!
",Lukasa,ctheiss
2011,2014-11-25 18:00:56,"@staticshock that's an option. Yes
",sigmavirus24,staticshock
2010,2014-04-18 19:20:57,"@kennethreitz Yeah, I'm more interested in the description of responsibilities. =)
",Lukasa,kennethreitz
2008,2014-04-18 07:34:20,"@Lukasa Thanks for your reply. But, what I want is to send http requests with specifying source address. I've no idea how to use the  Transport Adapter abstraction to finish this. Maybe you can give an example how it does. 
",bofortitude,Lukasa
2008,2014-04-18 08:08:57,"@Lukasa Great! That's what I want. After replacing with the new version urllib3, I did it! I hope there will be new version requests which embedded with new urllib3 as soon as possible. 
It's so kind of you to show me this! Thanks you again. 
",bofortitude,Lukasa
2008,2015-09-14 11:25:16,"@Lukasa 

thanks for the above dode
your code above have a small issue.



the source_address should be a pair (host, port).

You pass a string and this will cause an exception when the sock.bind method is called.

The correct way is to pass a tuple like:



cheers
",gosom,Lukasa
2008,2015-09-14 11:32:54,"@gosom You're quite right: the version of the code in requests-toolbelt does not have this problem.
",Lukasa,gosom
2008,2015-09-14 15:57:42,"@Lukasa 
I think the version of the code should not work with the current requrests version.

check here:https://github.com/kennethreitz/requests/blob/master/requests/packages/urllib3/util/connection.py#L77

The socket.bind method it is called and you get an exception if you pass a string.

Ok, this is not a bug but would be nice to be documented that source_address 
should be a (host, post) tuple.
cheers
",gosom,Lukasa
2008,2015-09-14 16:07:55,"@gosom Please read my comment again. I said the version of the code in the requests toolbelt works correctly. That code is available [here](https://github.com/sigmavirus24/requests-toolbelt/blob/master/requests_toolbelt/adapters/source.py).
",Lukasa,gosom
2008,2015-09-14 19:18:46,"@Lukasa 
yes you are right. I was not even aware of the request-toolbelt :+1:  

Thanks for the reply
",gosom,Lukasa
2006,2014-04-16 20:03:11,"This is an interesting idea @untitaker! It does feel like it'd be useful.

I wonder if we should trial it in [the toolbelt](https://github.com/sigmavirus24/requests-toolbelt) first, see if it's useful. @sigmavirus24?
",Lukasa,untitaker
2006,2014-04-16 20:12:30,"@untitaker That's not entirely true. It's what we do for Basic auth, but it's not what we do for Digest. And the fact that we do it pre-emptively for Basic is very deliberate: GitHub's API requires it. =)
",Lukasa,untitaker
2006,2014-04-28 19:55:10,"@sigmavirus24
I had a similar usage to `AuthBase` in mind:



Where GuessAuth is something like this:



I don't think it would be nice to blend into `AuthHandler`, as the auth type
to use can vary within a domain.
",untitaker,sigmavirus24
2004,2014-04-16 10:08:26,"@giampaolo This is an artifact of our connection pooling. The socket objects aren't being explicitly closed because reusing them is cheaper than making new syscalls and reassembling the object. This is tracked in #1882.
",Lukasa,giampaolo
2003,2016-12-16 18:50:48,"@nateprewitt I am in agreement, but I would be unsurprised if Cory changed his mind. Since he's on holiday, why don't you hold off on working on this so you don't waste your time.",sigmavirus24,nateprewitt
2003,2016-12-16 19:01:45,"Yep, will do. Just floating it out there for when you've both got a free moment. Thanks @sigmavirus24 :)",nateprewitt,sigmavirus24
2002,2014-04-14 21:18:37,"Hey @slinkp thanks for opening this!

In this case, what you're looking for is an explicit attribute: `r.ok`

The fact of the matter is that this works on Python 3 because we define `__bool__`. If we define `__nonzero__` on Python 2 this would work there as well. That said, I think you should be using the `r.ok` pattern anyway. It's a far better pattern personally.

@jamesob you're getting exactly what you ask for in that case, sorry to say it. You did get something if `resp` is not `None`. If you want a ""good"" response that also isn't None, you should be explicit about it:



Even if the `__nonzero__` bug is fixed, this is still far more obvious to anyone who is going to come along and read your code that you're not only expecting a non-`None` value but also a `2xx` response.
",sigmavirus24,jamesob
2002,2014-04-14 21:18:37,"Hey @slinkp thanks for opening this!

In this case, what you're looking for is an explicit attribute: `r.ok`

The fact of the matter is that this works on Python 3 because we define `__bool__`. If we define `__nonzero__` on Python 2 this would work there as well. That said, I think you should be using the `r.ok` pattern anyway. It's a far better pattern personally.

@jamesob you're getting exactly what you ask for in that case, sorry to say it. You did get something if `resp` is not `None`. If you want a ""good"" response that also isn't None, you should be explicit about it:



Even if the `__nonzero__` bug is fixed, this is still far more obvious to anyone who is going to come along and read your code that you're not only expecting a non-`None` value but also a `2xx` response.
",sigmavirus24,slinkp
2002,2014-04-14 21:23:05,"@sigmavirus24 heh, appears this issue has just bitten you too. 

> You did get something if resp is not None.

isn't true given the current implementation of `__nonzero__` (which kicks to `ok`), which is the point I was trying to get across.
",jamesob,sigmavirus24
2002,2014-04-14 21:27:12,"@jamesob yeah I asked that because I thought I read that `bool(resp) is True` in the original issue. I've had too much caffeine to deal with the sleep deprivation caused by PyCon. =D
",sigmavirus24,jamesob
2002,2014-04-14 21:28:54,"Yeah I figured this would break backward compatibility... somebody somewhere is surely depending on the current behavior.  I just really dislike it :)

Thanks @sigmavirus24 
",slinkp,sigmavirus24
2002,2014-05-20 22:31:21,"I think this is an irrelevant design decision that should really never be relied on either way. In my opinion neither `__nonzero__` nor `__bool__` should have been implemented for response objects in the first place.

@jamesob In your first code snippet, you shouldn't be using None as a boolean anyways. Write `if resp is None:` instead and that pattern works fine. (Also, just FYI, `except:` should always be `except Exception:`, otherwise you catch things like KeyboardInterrupt which is baaaaaaad.)

The truthiness of an HTTP response is a very ambiguous concept. Instead, just use `response.ok` as was suggested earlier.

> Explicit is better than implicit.
> — Tim Peters
",fletom,jamesob
2002,2014-05-23 00:43:47,"@fletom I'd like to remind you to [be cordial](http://www.kennethreitz.org/essays/be-cordial-or-be-on-your-way). The code snippets provided for this issue likely do no represent code actually copied and pasted from production code. They merely serve as examples to illustrate a point. There's no need to teach anyone about bad practices. All you needed to do was voice an opinion relevant to the discussion of what `bool(response)` would return.
",sigmavirus24,fletom
2002,2014-05-23 01:58:29,"@sigmavirus24 Sorry my comment came across as negative. I only meant to suggest a way that that common pattern can work with requests' current design. As KR suggested, the intention was to be educational/constructive and not insulting.
",fletom,sigmavirus24
1997,2014-04-09 20:50:42,"At the end of that section that @Lukasa linked to, there's a bit about the [requests-toolbelt](https://gitlab.com/sigmavirus24/toolbelt). Using that library would allow you to specify your own boundary without having to craft the header yourself.
",sigmavirus24,Lukasa
1997,2014-04-09 23:30:13,"@Lukasa Yes, I appreciate how simple and clear the document said about posting files. However, from the same page of document it also specified [custom headers](http://docs.python-requests.org/en/latest/user/quickstart/#custom-headers). And some samples from the document also have custom 'Content-Type' header. So it would seem totally OK to do that.
@sigmavirus24 Thanks for point that out. But I've encountered this issue when I'm just playing around with fairly small sized files. So at first glance, I'd think that I don't need streaming and skip that part.

The reason I report this is that when I'm trying requests, there's no warning about custom boundaries in document or stderr ouput. If there were, it'd save me a lot of time trying to figure out why the sent request isn't working.
So I think at least there should be a note like ""Custom boundaries are not supported. Please let requests generate that specific header for you"".
",jcfrank,Lukasa
1997,2014-04-09 23:30:13,"@Lukasa Yes, I appreciate how simple and clear the document said about posting files. However, from the same page of document it also specified [custom headers](http://docs.python-requests.org/en/latest/user/quickstart/#custom-headers). And some samples from the document also have custom 'Content-Type' header. So it would seem totally OK to do that.
@sigmavirus24 Thanks for point that out. But I've encountered this issue when I'm just playing around with fairly small sized files. So at first glance, I'd think that I don't need streaming and skip that part.

The reason I report this is that when I'm trying requests, there's no warning about custom boundaries in document or stderr ouput. If there were, it'd save me a lot of time trying to figure out why the sent request isn't working.
So I think at least there should be a note like ""Custom boundaries are not supported. Please let requests generate that specific header for you"".
",jcfrank,sigmavirus24
1997,2014-04-10 00:31:08,"@jcfrank there's a difference in the documentation (that may be too subtle) between the parts that specify a `Content-Type` header and the Multipart Post part -- The former require you to format the data yourself, specifically when posting JSON data. You're relying on requests to format the multipart request so you should not send the header.

Also, the toolbelt is desired mostly by people who have large file sizes but it even works if you need neither streaming or large file handling. You can do the following:



In this case you do want to set your own header **because** you're formatting the data.
",sigmavirus24,jcfrank
1995,2014-04-26 21:16:29,"@dstufft can we do something like `requests[+PyOpenSSL]` or `requests[+betterssl]`? By which I mean: is the `+` allowed by distutils/setuptools?
",sigmavirus24,dstufft
1995,2014-04-26 21:17:31,"No, a `+` won't parse correctly on the `pip install` side.

On Sat, Apr 26, 2014 at 2:16 PM, Ian Cordasco notifications@github.comwrote:

> @dstufft https://github.com/dstufft can we do something like
> requests[+PyOpenSSL] or requests[+betterssl]? By which I mean: is the +allowed by distutils/setuptools?
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/issues/1995#issuecomment-41481134
> .

## 

""I disapprove of what you say, but I will defend to the death your right to
say it."" -- Evelyn Beatrice Hall (summarizing Voltaire)
""The people's good is the highest law."" -- Cicero
GPG Key fingerprint: 125F 5C67 DFE9 4084
",alex,dstufft
1994,2014-04-05 14:45:03,"@ionelmc Thanks for this! Leave the '$' signs in (we like it stylistically) and I'll happily merge it. =)
",Lukasa,ionelmc
1994,2014-04-05 14:51:00,"@ionelmc regardless of whether you think it is poor style, it is convention in all technical documentation and is a clear signal (to most) that you should be using the text following it on a command line.
",sigmavirus24,ionelmc
1987,2014-04-01 13:43:46,"FWIW I don't mind the code in pip, we have to subclass session anyways for some other stuff and the code to add timeout support isn't very large :) I opened primarily at @Lukasa's prompting :)
",dstufft,Lukasa
1985,2016-03-09 01:46:55,"@steve100 can you test if the `rfc3986` library handles this appropriately?
",sigmavirus24,steve100
1985,2016-08-18 00:02:31,"So I'm going to start working on adding this support to [rfc3986](/sigmavirus24/rfc3986). In reading the specification, there's this:

>   In a URI, a literal IPv6 address is always embedded between ""["" and
>   ""]"".  This document specifies how a <zone_id> can be appended to the
>   address.  According to URI syntax [RFC3986], ""%"" is always treated as
>   an escape character in a URI, so, according to the established URI
>   syntax [RFC3986] any occurrences of literal ""%"" symbols in a URI MUST
>   be percent-encoded and represented in the form ""%25"".  Thus, the
>   scoped address fe80::a%en1 would appear in a URI as
>   http://[fe80::a%25en1].

So the right thing to do is encode the `%` sign it seems.

@Lukasa should we reopen this to track the work that we need to do in urllib3?
",sigmavirus24,Lukasa
1985,2016-10-04 22:42:51,"Hi @Lukasa

Wondering if you could post the related issue number for this on urllib3 so I could track it?  I tried looking for it, but didn't see it (I did see something about proxy handling...).  Thanks in advance

Also, for others that might be wondering - there is a workaround: if your system only has 1 interface with IPv6 addressing, then you do not need to supply the %<interface>, and things work just fine.  Unfortunately, all of my testing involves many networks with different vlans and I need IPv6 support on them all.
",briggr1,Lukasa
1985,2016-10-12 08:14:37,"@briggr1 I don't believe any tracking issue for this was actually opened on urllib3. I'd need @sigmavirus24 to outline exactly what work he believes is needed though: in my initial testing urllib3 seems to be handling this appropriately.
",Lukasa,briggr1
1985,2016-10-12 08:14:37,"@briggr1 I don't believe any tracking issue for this was actually opened on urllib3. I'd need @sigmavirus24 to outline exactly what work he believes is needed though: in my initial testing urllib3 seems to be handling this appropriately.
",Lukasa,sigmavirus24
1981,2014-03-31 14:29:29,"@Lukasa that is not true :)
",kennethreitz,Lukasa
1980,2014-05-04 06:46:01,"@alekstorm Kenneth's hugely busy, and so sizeable pull requests will tend to sit on the pile for a while until he has time to look at them. We're not a project in a rush, so don't worry, this won't be lost, and we won't be pressuring you to make fast changes. =)

Congrats on the new job, btw! 
",Lukasa,alekstorm
1980,2014-05-04 14:01:41,"@alekstorm also there's no rush. If needed, @Lukasa can improve the PR on a branch of our own (using all of your work as a base). And congratulations on the new job! I hope it treats you well :)
",sigmavirus24,Lukasa
1980,2014-05-04 14:01:41,"@alekstorm also there's no rush. If needed, @Lukasa can improve the PR on a branch of our own (using all of your work as a base). And congratulations on the new job! I hope it treats you well :)
",sigmavirus24,alekstorm
1980,2014-05-12 23:24:17,"@alekstorm if you can't finish this up, let me know and I'll take over the work for you.
",sigmavirus24,alekstorm
1979,2014-03-31 15:53:13,"@costinb7 Can you print the following things for me?


",Lukasa,costinb7
1979,2015-11-04 12:36:29,"@amnong We have no specific timetable for this at this time.
",Lukasa,amnong
1979,2016-12-11 20:59:20,"So it looks like this was patched with #2253 but there wasn't a test included at the time.

I've written up a [couple tests](https://gist.github.com/nateprewitt/86b4e076ab5142ede32f09dbddd2ae56) to verify the individual pieces are working as expected, but can't find a way to do a full repro with httpbin.

I'm not sure @amnong's issue was related to this since it's through `HTTPBasicAuth` which should always send the 'Authorization' header.

I'll throw up a PR with the tests if this looks right.",nateprewitt,amnong
1978,2014-03-29 08:58:07,"@hackdna Also, what would 'handling that exception' mean? We don't know what you want to do in that situation.
",Lukasa,hackdna
1977,2014-03-27 18:10:41,"@t-8ch @athoik 

wildcard certs support only one level wildcard-ish. 

But: The SSL certificate on hostname `ia600301.us.archive.org` is issued on the name `*.us.archive.org`. 

The error-message in requests shows another domain name .. 
",syphar,athoik
1977,2014-03-27 18:10:41,"@t-8ch @athoik 

wildcard certs support only one level wildcard-ish. 

But: The SSL certificate on hostname `ia600301.us.archive.org` is issued on the name `*.us.archive.org`. 

The error-message in requests shows another domain name .. 
",syphar,t-8ch
1977,2014-03-27 18:12:22,"@syphar, that is correct! Certificate has `Common name: *.us.archive.org` but in logs there is `hostname 'ia600301.us.archive.org' doesn't match either of '*.archive.org', 'archive.org'`
",athoik,syphar
1977,2014-03-27 18:44:22,"Thanks for the explanation @Lukasa, lets hope that PEP 466 will accepted in Python 2.7. Until then i am afraid that setting `verify=False` is the only way.
",athoik,Lukasa
1976,2014-03-26 13:38:02,"In related news, @kennethreitz is there an email address that the Jenkins server emails when tests fail? If not, could you add @Lukasa and/or me to the notifications? Assuming Jenkins runs on pushes to master (i.e., when you merge a PR) I would have seen this and fixed it sooner.
",sigmavirus24,Lukasa
1976,2014-03-26 15:34:33,"@sigmavirus24 I can set that up if you'd like, but it's pretty damn annoying :)
",kennethreitz,sigmavirus24
1976,2014-03-26 16:23:59,"@kennethreitz It's annoying, but it's the kind of annoying that @sigmavirus24 and I are for. =D
",Lukasa,kennethreitz
1976,2014-03-26 16:23:59,"@kennethreitz It's annoying, but it's the kind of annoying that @sigmavirus24 and I are for. =D
",Lukasa,sigmavirus24
1976,2014-03-26 16:31:24,"What @Lukasa said. Also I like to know when I've broken stuff. (Because this was sort of my fault)
",sigmavirus24,Lukasa
1973,2014-03-24 20:51:13,"@Lukasa try that:



do an 



you should get 60

then ""release"" the connections to the pool:



do the same 



There are still 60 sockets in CLOSE_WAIT state.

I think that something is not as expected.

Can you verify that it is ok?
Thanks
",gosom,Lukasa
1973,2014-03-25 11:57:30,"@gosom The key is that you're storing the responses in the list comprehension. These responses have references to the socket, which prevents it getting garbage collected. We don't aggressively close sockets because there's no requirement to do that, but we do aggressively release them to the pool (where appropriate).
",Lukasa,gosom
1973,2014-03-25 11:57:53,"@Lukasa 

you are right it works correct this way.
But when you explicitly close the connections there should be only MAX_CONNECTION_POOL_SIZE open correct?
",gosom,Lukasa
1972,2014-03-23 16:49:03,":+1: :cake: Thanks. Assigning @kennethreitz since @Lukasa and I agree this is ready to merge.
",sigmavirus24,Lukasa
1972,2014-04-04 17:04:24,"@avidas Kenneth is insanely busy (and probably even more so than usual as PyCon is next week), and so he does these things in batches. He'll get to it. =)
",Lukasa,avidas
1970,2016-03-09 12:23:26,"@mattwilliamson _What specifically_ is happening to you?
",Lukasa,mattwilliamson
1970,2016-03-09 21:28:47,"@Lukasa clearly _it_ is happening. :wink: 
",sigmavirus24,Lukasa
1969,2014-08-03 13:03:37,"@glyph there's a separate issue (#1995) for that. Feel free to ping Kenneth there.
",sigmavirus24,glyph
1968,2014-03-19 22:43:18,"@Lukasa I didn't trigger this but I prefer it this way. Certain things have been fixed/broken in 2.3.0. That behaviour is not available/visible in 2.2.1 and so the documented behaviour is accurately documented. :)
",sigmavirus24,Lukasa
1968,2014-03-20 00:46:38,"Thanks for the info @lukasa.

Do we all agree the current info on the
http://docs.python-requests.org/en/latest/ page is misleading, as it
(a) calls 2.3.0 ""released"", and (b) calls it ""latest"" (in the url)? From
that I'd conclude 2.3.0 was the latest release.

On Wednesday, March 19, 2014, Ian Cordasco
<notifications@github.com<javascript:_e(%7B%7D,'cvml','notifications@github.com');>>
wrote:

> @Lukasa https://github.com/Lukasa I didn't trigger this but I prefer it
> this way. Certain things have been fixed/broken in 2.3.0. That behaviour is
> not available/visible in 2.2.1 and so the documented behaviour is
> accurately documented. :)
> 
> ## 
> 
> Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/issues/1968#issuecomment-38116629
> .
",skivvies,Lukasa
1968,2014-03-20 00:55:32,"If you change the current latest version to something like ""2.3.0-dev"" (and
then update the docs, ideally substituting ""release"" with ""version"" for
docs on not-yet-released versions) it'd be clearer, and more semantic too.

On Wednesday, March 19, 2014, _pants@getlantern.org wrote:

> Thanks for the info @lukasa.
> 
> Do we all agree the current info on the
> http://docs.python-requests.org/en/latest/ page is misleading, as it
> (a) calls 2.3.0 ""released"", and (b) calls it ""latest"" (in the url)? From
> that I'd conclude 2.3.0 was the latest release.
> 
> On Wednesday, March 19, 2014, Ian Cordasco notifications@github.com
> wrote:
> 
> > @Lukasa https://github.com/Lukasa I didn't trigger this but I prefer
> > it this way. Certain things have been fixed/broken in 2.3.0. That behaviour
> > is not available/visible in 2.2.1 and so the documented behaviour is
> > accurately documented. :)
> > 
> > ## 
> > 
> > Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/issues/1968#issuecomment-38116629
> > .
",skivvies,Lukasa
1968,2014-03-20 11:40:51,"@skivvies this isn't standard procedure for requests. Reverting the version bump seems silly at this point and changing it to 2.3.0-dev will only invoke Kenneth's ire. For now I vote we leave this alone. After 2.3.0 this just won't happen. 
",sigmavirus24,skivvies
1967,2014-03-23 11:36:14,"Ok, I think what we need to do is work out what exactly is happening. @jgatkinsn, are you familiar with Wireshark/tcpdump? I'd like to see some packet captures of each of the different permutations so I can get an idea of how the connections are proceeding.
",Lukasa,jgatkinsn
1966,2014-03-18 23:38:14,"I can reproduce this but I have the same question as @Lukasa 
",sigmavirus24,Lukasa
1965,2014-03-24 15:40:29,"Exactly what @Lukasa said — the Mixin is all about readability :)
",kennethreitz,Lukasa
1964,2014-03-16 14:11:14,"@jayanthkoushik also a few notes for future reference (that are independent of this project).
- [pep8](/jcrocholl/pep8) is out of date with the actual PEP-0008 (meaning flake8 is out of date as well). The maximum line length suggested by PEP-0008 is now 100 characters.
- `not` is not a function. Changing `not (expr)` to `not(\nexpr)` doesn't look so great. Leave the space next time ;)
- Chained methods look really awful when you place a line continuation after the `.` (not your fault though)
- Function/method invocations with lots of parameters can have the parameters split. They do not need to be all on one line.
- `\` line continuations are ugly and you can use parentheses to avoid them, e.g.,



Python by default will think of that as one string and act accordingly. It's valid syntax and also valid PEP-0008.

Long arithmetic expressions and imports also benefit from using parentheses. PEP-0008 provides a lot of freedom in writing compliant code but what I've outlined above are conventions in the community that work within PEP-0008 that I've seen when working in projects.
",sigmavirus24,jayanthkoushik
1964,2014-03-16 14:34:09,"@sigmavirus24 : Thank you so much; will keep these things in mind.
",jayanthkoushik,sigmavirus24
1962,2014-03-24 15:44:41,"@sigmavirus24 you seem to really be in a rush lately.
",kennethreitz,sigmavirus24
1961,2014-03-15 16:54:50,"Good catch @Lukasa. There's a [related issue](https://github.com/shazow/urllib3/issues/356) already open on urllib3.

I'm closing this since we pull in urllib3 before releasing each time anyway.
",sigmavirus24,Lukasa
1959,2014-03-14 13:28:38,"@Feng23 thanks in advance. Also thank you for being patient and understanding with @Lukasa and me. It's our job to make sure your code is excellent enough to be merged without Kenneth objecting. Thanks for this contribution. It's excellent. :cake: 
",sigmavirus24,Feng23
1959,2014-03-14 13:28:38,"@Feng23 thanks in advance. Also thank you for being patient and understanding with @Lukasa and me. It's our job to make sure your code is excellent enough to be merged without Kenneth objecting. Thanks for this contribution. It's excellent. :cake: 
",sigmavirus24,Lukasa
1959,2014-03-24 15:46:50,"@Feng23 Can you do a rebase for me? I'll merge once you do :)
",kennethreitz,Feng23
1959,2014-03-26 16:38:59,"@Feng23 one more rebase and your build should pass. :)
",sigmavirus24,Feng23
1956,2014-10-10 21:01:10,"Hey @mikecool1000 welcome to the project. So you can't simply change the function signature like that, here's why.

Right now, someone could be doing this:



If we then change the signature, even one extra positional argument will fail, e.g., try the following:



So you'd want to add `**kwargs` after the existing positional arguments (that have defaults). Maybe this will set you on the right course without giving you too much room to shoot yourself in the foot or giving you the all the answers. :)
",sigmavirus24,mikecool1000
1956,2014-10-10 21:56:51,"Thanks that's very helpful

Sent from my iPhone

> On Oct 10, 2014, at 2:01 PM, Ian Cordasco notifications@github.com wrote:
> 
> Hey @mikecool1000 welcome to the project. So you can't simply change the function signature like that, here's why.
> 
> Right now, someone could be doing this:
> 
> s = requests.Session()
> 
> # ...
> 
> s.resolve_redirects(resp, req, False, 1.0, True, None, {})
> If we then change the signature, even one extra positional argument will fail, e.g., try the following:
> 
> def foo(a, b, c=3):
>     return a, b, c
> 
> foo(1, 2)  # should return (1, 2, 3)
> foo(1, 2, 4)  # should return (1, 2, 4)
> 
> def foo(a, b, **kwargs):
>     c = kwargs.pop('c', 3)
>     return a, b, c
> 
> foo(1, 2)  # should return (1, 2, 3)
> foo(1, 2, 4)  # should raise a TypeError about foo only accepting 2 arguments and receiving 3
> So you'd want to add **kwargs after the existing positional arguments (that have defaults). Maybe this will set you on the right course without giving you too much room to shoot yourself in the foot or giving you the all the answers. :)
> 
> —
> Reply to this email directly or view it on GitHub.
",mikecool1000,mikecool1000
1956,2014-10-10 22:13:29,"@mikecool1000 if you need other help, please hop onto #python-requests on Freenode this weekend. If I'm around, I'll be happy to chat with you about this.
",sigmavirus24,mikecool1000
1956,2014-10-13 02:14:29,"@mikecool1000 how familiar are you with generators? You should try this out at the interpreter:



When you run `g = s.resolve_redirects(r, r.request)` note that it will return immediately. It doesn't talk to the network until you start to consume the generator. There's no unnecessary computation at all :)
",sigmavirus24,mikecool1000
1956,2014-10-13 18:07:30,"No one called you a bad coder @mikecool1000 and you don't need to apologize for not having had prior experience with generators
",sigmavirus24,mikecool1000
1956,2014-10-13 18:58:02,"Thanks for helping me learn then :)

Sent from my iPhone

> On Oct 13, 2014, at 11:07 AM, Ian Cordasco notifications@github.com wrote:
> 
> No one called you a bad coder @mikecool1000 and you don't need to apologize for not having had prior experience with generators
> 
> —
> Reply to this email directly or view it on GitHub.
",mikecool1000,mikecool1000
1955,2014-03-15 17:55:05,"@zackw check out #1963 if you have some time.
",sigmavirus24,zackw
1954,2014-03-14 13:37:07,"I'm with @Lukasa on this one and I'll work on an example. That said, we allow the user to shoot themselves in the foot plenty. There's absolutely no reason to stop them from doing so here. It's our job to provide an elegant API to the user, not to make sure they do everything right.

You're also proposing that we should favor the prepared request on the Response object to the one passed in. Perhaps someone is relying on this specific behaviour for a really good reason. If we change this the way you're suggesting, that person's code will break in an awful and entirely unexpected way. No matter how well you document changes, people will ignore that documentation and expect the same behaviour unless the API is totally changed, i.e., the request parameter is totally removed.
",sigmavirus24,Lukasa
1953,2014-03-14 14:22:43,"Taking @sigmavirus24's concern on board, is there an elegant way we can do this _outside_ of the library, e.g. in the toolbelt?
",Lukasa,sigmavirus24
1953,2014-03-15 17:21:34,"> Taking @sigmavirus24's concern on board

It isn't a very strong concern. It's more of a pattern I've seen develop as of late. People watch the repo for a tiny change and use that change to get their foot in the door for a larger one that is widely unnecessary. It's a tiny concern that's ever present now.

Likewise, I think the toolbelt could easily accomodate this. That said, I'm not convinced it should be either in or outside of the core (i.e., I don't actually know where it belongs).

I've also been thinking along the same lines @zackw, but more geared towards making an eventual refactor a lot easier. I like having a compliment to `prepare_request` sibling. How does `prepare_redirected_request` sound?
",sigmavirus24,zackw
1953,2014-03-15 17:21:34,"> Taking @sigmavirus24's concern on board

It isn't a very strong concern. It's more of a pattern I've seen develop as of late. People watch the repo for a tiny change and use that change to get their foot in the door for a larger one that is widely unnecessary. It's a tiny concern that's ever present now.

Likewise, I think the toolbelt could easily accomodate this. That said, I'm not convinced it should be either in or outside of the core (i.e., I don't actually know where it belongs).

I've also been thinking along the same lines @zackw, but more geared towards making an eventual refactor a lot easier. I like having a compliment to `prepare_request` sibling. How does `prepare_redirected_request` sound?
",sigmavirus24,sigmavirus24
1953,2014-03-16 13:45:56,"I really want to get some work done on the toolbelt and betamax today. If @zackw has the time to throw together an example of `prepare_redirected_request` that'd be great. Otherwise, I'll likely work on it later this week.
",sigmavirus24,zackw
1953,2014-03-16 14:28:47,"@sigmavirus24 Not a problem - it's a simple matter of moving code around.  See #1965.

(I am going to be offline for most of the rest of the day, though.)
",zackw,sigmavirus24
1952,2014-03-13 16:59:41,"@zackw can you please give more detailed steps to reproduce (code example or test case). 
",schlamar,zackw
1951,2014-03-23 14:45:45,"Assigning to @kennethreitz since this looks good to me. :shipit: 
",sigmavirus24,kennethreitz
1949,2014-03-12 18:36:54,"Can I get review from @sigmavirus24 and @ionrock?
",Lukasa,ionrock
1949,2014-03-12 22:37:22,"This seems to fix things in CacheControl. Thank you @Lukasa! 

My reasoning for suggesting the `HTTPResponse` was because then someone doesn't have to check whether or not a response was cached or created outside a normal request. For example, if you used `resp.raw.seek(0)` you'd get an error as `None` obviously doesn't have a `seek` method. 

With that said, `seek` doesn't work anyway! No blood, no foul. 

Thanks for fixing this so quickly. I'll add some docs in CacheControl to help communicate that a cached response's `raw` attribute will be `None`. 
",ionrock,Lukasa
1945,2014-03-11 18:52:29,"Given @kennethreitz's -1 I'm going to close this.
",sigmavirus24,kennethreitz
1945,2014-03-12 20:56:11,"@sigmavirus24 I can close pull requests.
",kennethreitz,sigmavirus24
1945,2014-03-12 20:59:05,"@ttanner thank you so much for this contribution! Unfortunately, we won't be accepting it at this time. Basically, we've went down the route of passing tuples and dicts to a few different APIs, and it got a bit hairy if it wasn't the primary interface.

It may be worth exploring other options (perhaps passing `verify` a class?), if you think these SSL features are important, though. I'd love to learn more about them.
",kennethreitz,ttanner
1945,2014-10-10 14:03:32,"@kennethreitz Here is a use case regarding `assert_hostname`. I want to run a HTTPS web server with a self signed certificate and run some tests with requests against it. As this web server can run on multiple nodes it has no static host/IP. 

If I set the requests CA_BUNDLE to my certificate it fails on `ssl_match_hostname`, because there is no hostname defined in the cert (_requests.exceptions.SSLError: no appropriate commonName or subjectAltName fields were found_ ).

Right now, the only way of making this configuration to work is a) completely disable certificate verification or b) patch urllib3/requests to pass `assert_hostname=False`. Both options are not really nice.

Can you elaborate on your proposal to use a class? I guess I can help with a PR if you are more specific about what you expect.
",schlamar,kennethreitz
1945,2014-10-10 15:16:45,"@schlamar in this case, if I were you, I would do the following:
1. Use @t-8ch's [StackOverflow answer](http://stackoverflow.com/a/22794281/1953283) to create a new Adapter
2. Register the adapter on your session like so:


",sigmavirus24,schlamar
1945,2014-10-10 20:43:07,"@piotr-dobrogost as of this moment 98% (or more) of users can consider urllib3 an implementation detail. The rest of those users will need something from urllib3 and there are examples in the documentation (for the most common cases) which explicitly use urllib3. Regardless your comment is off-topic.
",sigmavirus24,piotr-dobrogost
1944,2014-03-10 14:41:12,"@maxcountryman Does this fix your issue (https://github.com/shazow/urllib3/issues/206)?
",schlamar,maxcountryman
1944,2014-03-10 14:48:09,"@schlamar it looks like it should. I can't test it at the moment. A little later on I'll see if I have time to give it a proper run with Photobucket.
",maxcountryman,schlamar
1944,2014-03-13 15:56:30,"In addition to #1939, I believe this also fixes #1952 (which I just filed - sorry about that).

@schlamar Could you please add some test cases for these bugs as well?  A test case for #1952 can be dug out of the (now-scrapped) pull request #1919.

@sigmavirus24 Regarding ""`except RuntimeError: pass # already decoded`"", I sympathize with your ""should this exception have been thrown at all?"" reaction, but it makes sense for `Response.content` to throw an exception when all the content has already been consumed; it happens that in this case we don't care since we are only accessing `.content` for its side effects.  (Would it make you more comfortable if a more specific exception were thrown?)
",zackw,schlamar
1944,2014-03-13 15:56:30,"In addition to #1939, I believe this also fixes #1952 (which I just filed - sorry about that).

@schlamar Could you please add some test cases for these bugs as well?  A test case for #1952 can be dug out of the (now-scrapped) pull request #1919.

@sigmavirus24 Regarding ""`except RuntimeError: pass # already decoded`"", I sympathize with your ""should this exception have been thrown at all?"" reaction, but it makes sense for `Response.content` to throw an exception when all the content has already been consumed; it happens that in this case we don't care since we are only accessing `.content` for its side effects.  (Would it make you more comfortable if a more specific exception were thrown?)
",zackw,sigmavirus24
1944,2014-03-13 16:39:43,"> Could you please add some test cases for these bugs as well?

Very interesting. I guess this should be possible (while a test case for the original issue is actually hard, we would need a misbehaving web service for that).

> should this exception have been thrown at all

I don't think this was his point. I guess you mixed two things together. My interpretation of @sigmavirus24 comments is:
1. Instead of a RuntimeError there should be a more explicit exception if the content is already consumed
2. Don't handle the RuntimeError separately in resolve_redirects, just do a `r.raw.read(...)` in all exception cases.
",schlamar,sigmavirus24
1944,2014-03-14 13:43:41,"> My interpretation of @sigmavirus24 comments _[snip]_

Your interpretation is correct. I've discussed this with several (far more experienced and knowledgeable Python developers, including core PyPy and CPython developers). `RuntimeError` exceptions should never be raised by any library ever. Yes they're there but that does not mean you should use them.

This also works with your overall goal @zackw: If the error were instead a child of a `RequestException` we could name it well and have:



Work in every case. In the redirect case, before this PR, it wouldn't. I know that there are several ideas of ""good"" Python code that I disagree with others on (including Kenneth), so I don't push those issues normally.

I haven't found a good way to preserve backwards compatibility though for those catching the RuntimeError and allowing for a RequestException. I don't think creating a new one that inherits from both is a good idea in the slightest. I'd love if either of you had the solution. In fact, I'd probably send you :cake: :)
",sigmavirus24,zackw
1944,2014-03-14 13:43:41,"> My interpretation of @sigmavirus24 comments _[snip]_

Your interpretation is correct. I've discussed this with several (far more experienced and knowledgeable Python developers, including core PyPy and CPython developers). `RuntimeError` exceptions should never be raised by any library ever. Yes they're there but that does not mean you should use them.

This also works with your overall goal @zackw: If the error were instead a child of a `RequestException` we could name it well and have:



Work in every case. In the redirect case, before this PR, it wouldn't. I know that there are several ideas of ""good"" Python code that I disagree with others on (including Kenneth), so I don't push those issues normally.

I haven't found a good way to preserve backwards compatibility though for those catching the RuntimeError and allowing for a RequestException. I don't think creating a new one that inherits from both is a good idea in the slightest. I'd love if either of you had the solution. In fact, I'd probably send you :cake: :)
",sigmavirus24,sigmavirus24
1944,2014-03-15 18:01:41,"@schlamar it took me several months to get a feature merged and deployed on HTTPBin in order to test a bug fix. I'm not certain we should bother waiting that long. Do you have sufficient confidence that you could fake out a gzipped response from a redirect? You could take a similar approach to my test in #1963. Unfortunately, when I planned that test code, I was planning for the simplest case (mine). If there's a good way to adapt it to this PR, that would be awesome.
",sigmavirus24,schlamar
1944,2014-03-15 20:35:19,"@sigmavirus24 @zackw Added a test. What do you think?
",schlamar,zackw
1944,2014-03-15 20:35:19,"@sigmavirus24 @zackw Added a test. What do you think?
",schlamar,sigmavirus24
1944,2014-03-16 17:47:13,"@sigmavirus24 Should I change 



to


",schlamar,sigmavirus24
1944,2014-03-18 11:10:26,"> I prefer to do something as opposed to using pass in an except block. 

@sigmavirus24 I guess you'll like the new [`contextlib.suppress`](http://docs.python.org/3.4/library/contextlib.html#contextlib.suppress) in Python 3.4. =)
",schlamar,sigmavirus24
1944,2014-03-24 17:00:25,"@kennethreitz If `.transfer_encoding` is something you're interested in I'll take a crack at it.
",Lukasa,kennethreitz
1944,2014-03-31 14:31:38,"@Lukasa by all means! 
",kennethreitz,Lukasa
1944,2014-03-31 14:32:36,"@schlamar or specifying a transfer encoding other than what the server provided. It'll just give is a more flexible architecture around this, in general.
",kennethreitz,schlamar
1944,2014-05-12 19:22:05,"@schlamar if you perform a rebase this will be merged and released today!
",kennethreitz,schlamar
1942,2014-03-07 13:28:36,"Mm, I agree with @sigmavirus24 here. Something weird should be going on with YCM to cause this. Here's my reasoning:
1. `socket` must be imported because it's unconditionally imported at the top of `adapters.py`: any failure to import would cause an `ImportError`.
2. `socket` must be imported because if the import hadn't run at all we wouldn't get an `AttributeError`, we'd get a `NameError` on `socket`.

These two facts suggest that `requests.adapters.socket` has been monkeypatched to `None`. We never do this in Requests, so I'd look at YCM.
",Lukasa,sigmavirus24
1942,2014-03-07 15:20:34,"Thanks @ross !
",sigmavirus24,ross
1942,2014-03-07 18:38:46,"> Looking into YCM, they use pythonfutures and requests-futures. I wonder if @ross has any insight into this. Granted I'm not sure they actually use requests-futures, but they're at least using requests with concurrent futures and he might be able to help us all out.

YCM uses requests-futures, correct.

> base_request.py seems to import requests-futures and creates a FuturesSession. the code is a bit difficult to follow, but what it is doing with requests-futures seems valid and i don't see any clear way it'd be related to the socket.error problem.

YCM code doesn't do any monkeypatching of anything inside Requests (or any other third-party lib it uses). requests-futures might be doing it though, I'm not sure.

> the fact that unsafe_thread_pool_executor.py has comments about it being safe to kill still working threads and the stacktrace mentions that it's probably happening in shutdown would in my opinion point things at YCM.

Yeah, the `unsafe_thread_pool_executor` is unsafe in general (that's why I named it that way) but safe for YCM workloads. When the user wants to shut down Vim, the YCM plugin that uses Requests to talk to the `ycmd` server might have some requests in flight and we don't care about those anymore, they're (logically) fine to kill at any point of execution. But from a code perspective, it appears it might provoke some tracebacks... _sigh_.

But waiting on the network threads to stop is profoundly not an option; depending on the user's code (that they're trying to get completions for), some of those requests might take several seconds to finish and blocking Vim shutdown to wait for them pisses everyone off. People want Vim to exit quickly, not block and then exit some time in the near future.

I'm open to ideas here.
",Valloric,ross
1942,2014-03-07 19:06:19,"@Valloric Sorry I couldn't get back to you after your first message, but yeah, if you genuinely don't care about what happens to the messages once you've decided to shut down you should just swallow those exceptions. After all, at this point they aren't exceptional, they're expected.
",Lukasa,Valloric
1942,2014-03-07 19:33:20,"Are we certain that this is occurring when @ashemedai is quitting vim? I didn't see him indicate that but perhaps I missed it in the original thread.
",sigmavirus24,ashemedai
1942,2014-03-07 20:13:32,"@sigmavirus24 It happens for me only when I write-quit or quit out of vim and only from time to time. I am fine with it silently swallowing it. The reason I brought it up in the first place with @Valloric was that I thought it was unintended behaviour. :)
",ashemedai,Valloric
1942,2014-03-07 20:13:32,"@sigmavirus24 It happens for me only when I write-quit or quit out of vim and only from time to time. I am fine with it silently swallowing it. The reason I brought it up in the first place with @Valloric was that I thought it was unintended behaviour. :)
",ashemedai,sigmavirus24
1941,2014-03-06 16:48:47,"@Lukasa sounds like a worthwhile addition.
",sigmavirus24,Lukasa
1940,2014-03-08 02:32:36,"> The actual point of raising the exception is here: https://github.com/kennethreitz/requests/blob/master/requests/adapters.py#L391

@schlamar we cannot catch that in `Session#resolve_redirects`. To catch it in the adapter would be a serious change in behaviour. I wonder why we consume all of the content in the adapter though. If there's a decoding error, we don't even attempt to return a Response to the user. That may be fodder for an entirely different issue though. If @Lukasa is satisfied with this as an immediate solution to what was reported, I am too. But I want to investigate that other piece in the adapter as well.
",sigmavirus24,schlamar
1940,2014-03-08 14:53:49,"Reviewing the issue, you're once again correct @schlamar. That aside @mechanical-snail is not wrong that this could in fact also be a problem. Their issue won't be fixed by this but it may fix the case where streaming is used.
",sigmavirus24,schlamar
1939,2014-03-05 06:44:15,"We decode the data because your assertion that it won't be read is false. You may read the response body from any redirect because we save it. Each redirect builds a _full_ response object that can be used exactly like any other. This is a very good thing, and won't be changed. =)

The fix, as @sigmavirus24 has suggested, is simply to catch this error.
",Lukasa,sigmavirus24
1939,2014-03-07 14:14:10,"@schlamar That's very interesting.

However, this isn't a dupe, it's just related. The key is that we shouldn't really care even if we hit a legitimate decoding error when following redirects: we just want to do our best and then move on.
",Lukasa,schlamar
1939,2014-03-07 14:23:03,"@Lukasa hit the nail on the head :)
",sigmavirus24,Lukasa
1939,2014-03-07 14:29:03,"@schlamar So the issues you linked cause the exception, but they aren't the problem being referred to. The key problem in _this_ issue is that if we hit an error decoding the response body of a redirect, we'll stop following redirects. That _shouldn't_ happen: we understood enough of the message to follow the redirect, so there's no reason to stop following them. =)

Fixing the bugs you linked fixes the specific case in question, but not the general one.
",Lukasa,schlamar
1938,2014-03-05 06:48:26,"There are two forms of decoding here: decoding from compressed data to bytes, and then decoding from bytes to unicode. The first is done in urllib3, the second in Requests. Because the first is done in urllib3, Requests never sees the gzipped bytes. This means the only way to achieve what you want _in requests_ is to reimplement the convenience methods that urllib3 already has for decompressing data. That just seems silly, so I recommend following @sigmavirus24's advice and getting this into urllib3.
",Lukasa,sigmavirus24
1935,2014-03-01 01:10:52,"I think I've fixed the styling issues, now @Lukasa - thanks! :) Let me know if I've missed anything. 
",ceaess,Lukasa
1935,2014-03-03 20:25:19,"@vlevit: Good catch, that section of the docs has been changed.

@kennethreitz: For the moment I've made sure it'll be in the changelog. If you want something more drastic I'll whip up a section of the documentation that explains timeouts in more detail. =)
",Lukasa,vlevit
1935,2014-03-03 20:25:19,"@vlevit: Good catch, that section of the docs has been changed.

@kennethreitz: For the moment I've made sure it'll be in the changelog. If you want something more drastic I'll whip up a section of the documentation that explains timeouts in more detail. =)
",Lukasa,kennethreitz
1935,2014-03-12 11:26:59,"@mortoray no. And there shouldn't be a release until we fix the Proxy Authorization exposure
",sigmavirus24,mortoray
1935,2014-03-12 11:33:10,"@mortoray This patch was only made 9 days ago, and we don't have a really rapid release cadence. =)
",Lukasa,mortoray
1935,2014-03-12 18:28:01,"@Lukasa Regarding docs again, I think the code example after the timeout changes explanation is no more relevant:-)
",vlevit,Lukasa
1931,2014-02-26 07:21:04,"@hamish This is a per-release action for us, so the next release should have the newest version of urllib3 in it. =)
",Lukasa,hamish
1930,2014-02-26 14:17:08,"I agree with @Lukasa's comment. Further, `Session#send` should not return either an iterator or a response. It should only return one ever. The 98% use case desires a Response and that's all we should ever return. I will not budge on this.

I haven't reviewed the code at all, but I will probably leave PR review when I do. As this is described right now, I'm :-1: on the entire change with the caveat that I haven't reviewed much beyond your description @zackw 
",sigmavirus24,zackw
1930,2014-02-26 14:17:08,"I agree with @Lukasa's comment. Further, `Session#send` should not return either an iterator or a response. It should only return one ever. The 98% use case desires a Response and that's all we should ever return. I will not budge on this.

I haven't reviewed the code at all, but I will probably leave PR review when I do. As this is described right now, I'm :-1: on the entire change with the caveat that I haven't reviewed much beyond your description @zackw 
",sigmavirus24,Lukasa
1930,2014-02-27 02:44:10,"I agree with all of @Lukasa's feedback. Frankly @zackw you seem to want to throw everything out and rewrite it from scratch. While that isn't always a bad thing, you seem to be taking advantage of our desire to help you out. I for one will not accept that. We have tried to help usher you through the process of making your work as close to perfect as possible and all I have seen in this thread is you fighting us. I understand why you're fighting but I see no genuine attempts at compromise. At best you're not making me anymore sympathetic to your goal.
",sigmavirus24,zackw
1930,2014-02-27 02:44:10,"I agree with all of @Lukasa's feedback. Frankly @zackw you seem to want to throw everything out and rewrite it from scratch. While that isn't always a bad thing, you seem to be taking advantage of our desire to help you out. I for one will not accept that. We have tried to help usher you through the process of making your work as close to perfect as possible and all I have seen in this thread is you fighting us. I understand why you're fighting but I see no genuine attempts at compromise. At best you're not making me anymore sympathetic to your goal.
",sigmavirus24,Lukasa
1930,2014-02-28 12:40:42,"Also, for what it is worth, having a huge change like this in one commit is not helpful to either @Lukasa or me. This [Guide to Git Commits](https://wiki.openstack.org/wiki/GitCommitMessages) for OpenStack should explain why it's harder for us to review one huge commit as opposed to a series of smaller ones which tell a story.
",sigmavirus24,Lukasa
1930,2014-03-12 20:41:36,"@kennethreitz I'm @zackw, not @zachw :-)

Could probably sit down and talk sometime tomorrow.  I'm in US/Eastern and can do either Skype or Google (but Skype is preferred).  I will also find time to file bugs for all the things I was trying to fix, as requested earlier.
",zackw,kennethreitz
1930,2014-03-12 20:41:36,"@kennethreitz I'm @zackw, not @zachw :-)

Could probably sit down and talk sometime tomorrow.  I'm in US/Eastern and can do either Skype or Google (but Skype is preferred).  I will also find time to file bugs for all the things I was trying to fix, as requested earlier.
",zackw,zackw
1929,2014-02-25 22:42:16,"Thanks for this @alex! Assuming #1919 gets cleaned up this should be fixed by that pull request. =)
",Lukasa,alex
1929,2014-08-21 17:43:19,"@alex fixed!
",kennethreitz,alex
1928,2014-02-22 20:49:58,"@Lukasa,

Thank you for the monkey patch. I have it running on a test stream right now. I understand using this patch is on my own recognizance. That you disavow all knowledge of its existence and will deny that you were ever involved. ;-)

There are three main reasons I've moved to using your general purpose stack. First, I'll be able to use this knowledge elsewhere. (The database I use, Couchbase/CouchDB, uses HTTP for control messages.) Second, Requests API is almost as efficient as the Twitter library I am used to. This is a great accomplishment. Third, Requests is more performant. Requests is able to turn gzip on during the stream. This reduces my bandwidth use by a factor of 4 and, apparently, Twitter gives me more tweets. Yay!

Keep up the good work. I'll watch this repo. I'll try to be aware when you've come to a decision. That said, do not hesitate to ping me. As long running timeouts are hard to test, I'll be happy to start testing the fix when you're team is ready.

Anon,
Andrew
",adonoho,Lukasa
1928,2014-02-24 14:30:19,"@Lukasa,

I've had the patch running for two days now and it appears to have properly detected hangups from Twitter. My app then safely restarts the connection.

I'm looking forward to your full featured solution. Rest assured though that it is needed and will be used.

Anon,
Andrew
",adonoho,Lukasa
1928,2014-02-24 19:13:42,"@reticulatingspline Yes, this won't work for your use-case, I'm afraid. The read timeout function is at the scope of an individual socket `recv()` call, so that if the server stops sending data for more than the read timeout we'll abort.

If you really want to set a maximum length you'll need to use `stream=True` and `iter_content()` in small chunks. The read timeout will then apply to each `iter_content()` call. That, plus some judicious use of the `time` module, should get you the behaviour you want.
",Lukasa,reticulatingspline
1928,2014-02-24 19:20:34,"@Lukasa 

I was testing this out with it and it seemed to work:



I'd call get within the try/except after a with time_limit(10):

That seemed to work.

Same concept?

Thanks.
",reticulatingspline,Lukasa
1928,2014-03-12 09:52:04,"@mortoray There is no schedule on this feature. Kenneth has expressed ambivalence about the quality of the API, and we won't implement anything until we've got an API he's happy with.
",Lukasa,mortoray
1926,2014-02-19 17:25:01,"Ugh, this is why I hate HTTP. RFC 2616 has the following things to say on this topic:

Firstly, [the definitions of headers](http://pretty-rfc.herokuapp.com/RFC2616#message.headers):

> 

UTF-8 is necessarily outside the range of tokens and separators, so we need to consider the TEXT BNF rule. Once again, [RFC 2616 to the rescue](http://pretty-rfc.herokuapp.com/RFC2616#basic.rules):

> The TEXT rule is only used for descriptive field contents and values that are not intended to be interpreted by the message parser. Words of *TEXT MAY contain characters from character sets other than ISO-8859-1 Information technology - 8-bit single byte coded graphic - character sets only when encoded according to the rules of RFC 2047 RFC 2047[sic].
> 
> 

Note please that ISO-8859-1 is a synonym of latin-1.

At this stage things get ambiguous. Strictly speaking, UTF-8 can be represented in the `TEXT` field as a string of opaque octets (as none of them will be mistaken for ASCII control characters). However, plain UTF-8 does _not_ meet the RFC 2047 encoding requirements.

Requests is between a rock and a hard place. RFC 2616 makes clear that any complaint implementation will be able to handle ISO-8859-1, and makes no guarantees about supplying non-RFC 2047-encoded UTF-8 header values. In such a world, Requests is always going to choose the most-likely-to-succeed case, fitting in with our goal of satisfying the 90% use-case. In your situation @oinopion, I think the best thing to do is to build the Basic Auth header yourself: it's not very hard. =) You can therefore take control of the encoding yourself and choose the approach you know your target server supports.
",Lukasa,oinopion
1925,2014-02-18 21:12:42,"@t-8ch Correct. =)

@ntucker Thanks for the suggestion! Unfortunately, this is not a direction we're prepared to go. As I said [in the linked issue](https://github.com/kennethreitz/requests/issues/1595#issuecomment-30993198):

> I see no reason for Requests to favour ujson over any other third-party JSON decoder. We do nothing very complicated with JSON decoding, so replacing the decoder we use either via monkeypatching or via doing the decoding yourself is totally safe. With that in mind, there's no good reason to move away from the standard library in Requests proper.
",Lukasa,t-8ch
1925,2014-02-18 21:12:42,"@t-8ch Correct. =)

@ntucker Thanks for the suggestion! Unfortunately, this is not a direction we're prepared to go. As I said [in the linked issue](https://github.com/kennethreitz/requests/issues/1595#issuecomment-30993198):

> I see no reason for Requests to favour ujson over any other third-party JSON decoder. We do nothing very complicated with JSON decoding, so replacing the decoder we use either via monkeypatching or via doing the decoding yourself is totally safe. With that in mind, there's no good reason to move away from the standard library in Requests proper.
",Lukasa,ntucker
1924,2014-03-13 06:45:42,"@Lukasa I assume this stalled until 2.3 is in sight? Any rough ETA?
",schlamar,Lukasa
1924,2014-03-14 13:44:56,"@schlamar I've been doing this for PRs that need to be merged quickly so they don't get lost. I'll assign it and then comment along the lines ""LGTM!"" so that Kenneth gets an email (I'm not sure he has emails turned on for every issue/PR). This is just my way of being certain that he receives a notification. :)
",sigmavirus24,schlamar
1920,2014-02-14 20:31:06,"@sigmavirus24 Sorry, I had the context for this issue already. =)

Basically, we allow you to temporarily unset a header like this:



But if you try to permanently unset a header on a `Session` in an analogous way, you get surprising behaviour:



The question is, should we allow the example above to work, or should we just continue to use the `del` behaviour?
",Lukasa,sigmavirus24
1920,2014-02-14 22:17:09,"@Lukasa I think this is actually a regression in how we used to behave but such are the consequences when less tests are preferred to more tests. =P
",sigmavirus24,Lukasa
1918,2014-02-13 21:07:28,"I've discovered the flow for getting @kennethreitz to merge things faster. /Social Coding Hacking/
",sigmavirus24,kennethreitz
1916,2014-03-06 16:17:44,"> I don't want to be too coupled to urllib3.

@kennethreitz You know that urllib3 is doing the decompression, right?
",schlamar,kennethreitz
1916,2014-03-08 15:37:11,"@kennethreitz updated
",schlamar,kennethreitz
1916,2014-03-10 13:01:54,"@schlamar yes — I wrote the code, believe it or not :)

Just because urllib3 gives us a place to bind to doesn't mean that we should. As a matter of fact, it's often a good place to question ourselves in every way and learn a lot about ourselves.
",kennethreitz,schlamar
1916,2014-03-12 11:15:06,"@kennethreitz updated > just remove compress from accepted encoding.
",schlamar,kennethreitz
1915,2014-02-13 12:50:10,"@jcea did you search other issues on the project? Your ticket reminded me of https://github.com/kennethreitz/requests/issues/1289 but I searched for `getresponse` before I found it. The last activity on it was a month ago.

To update you, there are patches on bugs.python.org which haven't moved anywhere because no core developers have bothered reviewing them. If you want action on this, your best bet is to go find those issues on bugs.python.org and bump them. I have done that myself but perhaps the more people who do so, the faster a response.

Thanks for opening this, but it is a duplicate and it is _not_ a bug in requests.
",sigmavirus24,jcea
1915,2014-05-21 02:50:30,"@rramanadham I edited your comment to make it easier to read (using fenced code blocks).

The problem I'm having is that your traceback shows that the unhandled exception is a `KeyboardInterrupt` exception which means someone pressed Ctrl-C (assuming *nix, otherwise Ctrl-Z if I'm remembering my Windows correctly) which caused the traceback you're seeing. The problem is not passing `buffering=True`, that exception was handled. While it was being handled, someone killed the process.

If you can get me the actual Traceback we can look at this again. If that traceback shows something different, I'd suggest we open a new issues _but only if we determine this isn't someone killing the process_.
",sigmavirus24,rramanadham
1915,2014-08-03 17:08:24,"@awbacker the real problem you're seeing has nothing to do with `getresponse` receiving an extra argument. What's causing this for you is that you're getting a `ConnectionResetError` while making a request.
",sigmavirus24,awbacker
1915,2014-08-03 17:57:11,"@awbacker when the connection is reset the only sane thing to do is throw an exception because the request/response cycle cannot be completed. Returning a `Response` object would be entirely disingenuous and there's no fake status code we could use to indicate anything other than a catastrophic failure.

> forgive my newness at interpreting python stacktraces

No worries. Python 3's stacktraces seem to be a source of frustration for a large number of people. This issue continues to see activity because of that exact reason.

> Should I remove the comment and not muddy the issue?

No need to remove it. If others find this and read the entire comment history, maybe it will give them a hint as to how to find the real problem they're encountering.
",sigmavirus24,awbacker
1915,2014-09-06 16:07:05,"@piyushjajoo No you aren't. Please read this issue carefully. I made [this comment](https://github.com/kennethreitz/requests/issues/1915#issuecomment-34976750), which linked to [this comment](https://github.com/kennethreitz/requests/issues/1289#issuecomment-31294851) from issue #1289. Once again, reproducing the body of the comment:

> The key is that the `TypeError` raised as the first exception is unrelated to the subsequent ones. In fact, that's the standard control flow in `urllib3`. This means that the real exception that's being raised here is the `request.exceptions.ConnectionError` exception that wraps the `urllib3.exceptions.MaxRetryError`exception being raised in `urllib3`.

In your case, the real exception you care about is the _last_ one: the `ReadTimeoutError`. You should pursue that one.
",Lukasa,piyushjajoo
1915,2015-04-28 19:43:34,"@Lukasa : wondering if the issue is related , however i am trying different option as you stated earlier , but none seems to work for me , Please suggest if you have any pointer:

when I am making this api call to fetch the details from iCloud , it thows error 

> > > api.contacts.all()

Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py:768: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html
  InsecureRequestWarning)
/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py:768: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html
  InsecureRequestWarning)
Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py"", line 372, in _make_request
    httplib_response = conn.getresponse(buffering=True)
TypeError: getresponse() got an unexpected keyword argument 'buffering'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py"", line 544, in urlopen
    body=body, headers=headers)
  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py"", line 374, in _make_request
    httplib_response = conn.getresponse()
  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/http/client.py"", line 1162, in getresponse
    raise ResponseNotReady(self.__state)
http.client.ResponseNotReady: Request-sent

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/requests/adapters.py"", line 370, in send
    timeout=timeout
  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py"", line 597, in urlopen
    _stacktrace=sys.exc_info()[2])
  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/requests/packages/urllib3/util/retry.py"", line 245, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/requests/packages/urllib3/packages/six.py"", line 309, in reraise
    raise value.with_traceback(tb)
  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py"", line 544, in urlopen
    body=body, headers=headers)
  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py"", line 374, in _make_request
    httplib_response = conn.getresponse()
  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/http/client.py"", line 1162, in getresponse
    raise ResponseNotReady(self.__state)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection aborted.', ResponseNotReady('Request-sent',))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/gaurav/pyicloud-0.6.2/pyicloud/services/contacts.py"", line 55, in all
    self.refresh_client()
  File ""/Users/gaurav/pyicloud-0.6.2/pyicloud/services/contacts.py"", line 44, in refresh_client
    self.session1.post(self._contacts_changeset_url, params=params_refresh)
  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/requests/sessions.py"", line 508, in post
    return self.request('POST', url, data=data, json=json, *_kwargs)
  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/requests/sessions.py"", line 465, in request
    resp = self.send(prep, *_send_kwargs)
  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/requests/sessions.py"", line 573, in send
    r = adapter.send(request, **kwargs)
  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/requests/adapters.py"", line 415, in send
    raise ConnectionError(err, request=request)
requests.exceptions.ConnectionError: ('Connection aborted.', ResponseNotReady('Request-sent',))

The code which is making this call is 

host = self._service_root.split('//')[1].split(':')[0]
        self.session.headers.update({'host': host})
        params_contacts = dict(self.params)
        params_contacts.update({
            'clientVersion': '2.1',
            'locale': 'en_US',
            'order': 'last,first',
        })
        req = self.session.get(
            self._contacts_refresh_url,
            params=params_contacts
        )
        self.response = req.json()
        params_refresh = dict(self.params)
        params_refresh.update({
            'prefToken': req.json()[""prefToken""],
            'syncToken': req.json()[""syncToken""],
        })
        self.session.post(self._contacts_changeset_url, params=params_refresh)
        req = self.session.get(
            self._contacts_refresh_url,
            params=params_contacts
        )
        self.response = req.json()

Appreciate your support . I am currently using Python 3.4.3 ..
",Gaurav-Ambasta,Lukasa
1915,2015-05-04 16:44:03,"@Lukasa YES! After upgrading, the problems seems to be solved!!! Thank you a lot!  
",1a1a11a,Lukasa
1915,2016-01-12 16:14:29,"@laike9m Please read the issue. As explained above, you are misreading the traceback. The actual exception you're hitting is the last one in the tree. That error is caused by the remote server closing the connection.
",Lukasa,laike9m
1915,2016-01-12 16:16:09,"@Lukasa Alright, understand.
",laike9m,Lukasa
1915,2017-03-16 09:34:59,"@Cedric-Venet No you don't. You are seeing the effect of Python 3's exception chaining. Please read this issue *very carefully*.

In fact, I'm now going to lock this issue because it is an absolute magnet for people chiming in with ""me too"" comments without reading the issue. For anyone who wants to leap in, here is the TL;DR.

This is a problem to do with how exception tracebacks are being reported in Python 3. PEP 3134 introduced 'chaining exceptions' reporting, which means that when an exception bubbles up, all exceptions that were hit on the way to that original exception are shown. The purpose of this error reporting is to highlight that some exceptions occur in except blocks, and to work out what chain of exceptions was hit. This is potentially very useful: for instance, you can hit an exception after destroying a resource and then attempt to use that resource in the except block, which hits another exception. It's helpful to be able to see both exceptions at once.

The key is that the `TypeError` raised as the first exception is unrelated to the subsequent ones. In fact, that's the standard control flow in urllib3. This means that this error is *never* your actual problem. In general, when using Python 3, you should look at the *last* exception in the chain first and treat that as the actual error to solve.",Lukasa,Cedric-Venet
1914,2014-02-13 19:26:54,"@sigmavirus24 -- Thanks for the clarification. We were not relying on your documentation here; we were just leaning on past expectations of convention, which was clearly mistaken.

Thanks the for explanation, and you're right about coercing something into the format you expect it to be in, rather than just trying to marshal it and crossing your fingers.

Thanks the the replies!

--Robin
",hobbeswalsh,sigmavirus24
1914,2014-02-13 21:01:15,"I'm glad we could help @hobbeswalsh 

Please continue sending PRs as you see fit! We don't mind discussing them with you. :)

Cheers! :wine_glass: 
",sigmavirus24,hobbeswalsh
1913,2014-02-11 21:49:37,"@sigmavirus24 re `next(self.resolve_redirects())`, the most significant reason for doing it differently is that `resolve_redirects` doesn't include the very first response in its iterable (and I didn't feel safe changing that).  That means code that needs to look at each response as it comes in is most naturally structured like this:



rather than like this:



which duplicates part of the processing and has some finicky logic after the loop to get hold of the final response.
",zackw,sigmavirus24
1913,2014-02-12 02:17:28,"@zackw Also, as you reply to PR feedback (since there will be so much of it here) can you reply to each comment your fixing with ""Fixed in <sha>"". I won't mind the emails sent and it will help us keep track of what was fixed and when. It also provides more context when performing final reviews.
",sigmavirus24,zackw
1913,2014-02-12 08:49:35,"I've glanced through the code review comments stuff so far, and wanted to talk more generally about how this relates to our API freeze.

@zackw has said in one of his comments that he believes these changes would be welcome extensions to the API for anyone who has to manually handle redirects. That's probably true. However, our API freeze policy does not say that we are freezing the API ""except when it'll be useful for people if it was extended"". By default, the Requests answer to API changes will always be ""no"". The reason this issue is still open and being discussed is because we think there is potentially enough value here that we want to look at it in depth. Please don't assume that we hate your PR, @zackw, we're just starting from a very conservative position.

Next, there is a question about how much affordance we should give to people who circumvent Requests' redirection policy. The general Requests policy on this sort of thing (see also: `PreparedRequest` objects) is that if you don't like the way Requests does it you should do it yourself. We have made concessions here in the past, but not many and always under substantial duress.

Again, I'm going to hold off more dramatic code review until @kennethreitz gives an idea of whether he's likely to want this change at all.
",Lukasa,zackw
1913,2014-02-12 16:04:29,"@zackw can you explain the `Response.is_redirect` reasoning to me? I can understand the thought process, as redirects are a very first-class citizen in the HTTP world, but I'd love to hear your feelings on it :)
",kennethreitz,zackw
1913,2014-02-12 16:17:42,"I gotta get other work done this afternoon, but, would it be useful for me to split this pull request into two? One strictly for bug fixes, and another for anything that changes the API even a little.

@kennethreitz given that you are open to the possibility of some amount of API changes, I'd like to observe that one API change I didn't make, but rather wanted to, was to have `Session.resolve_redirects` return an iterable that _does_ include the very first response.  Such a generator (under another name, for compatibility's sake) would be a workable, perhaps even preferable, alternative to the `resolve_one_redirect` API I did add.
",zackw,kennethreitz
1913,2014-02-12 16:22:21,"@zackw make one for `Response.is_redirect` first. I'll merge it right away :)
",kennethreitz,zackw
1913,2014-02-12 16:23:11,"@zackw perhaps we could give resolve redirects an argument to include the first response? 
",kennethreitz,zackw
1913,2014-02-12 16:34:56,"@zackw let's break each change into a new PR as we discuss them and keep this one open for discussion. Then, we can bite one thing off at a time. 

So, first thing first — open a new PR for `Request.is_redirect` :)
",kennethreitz,zackw
1913,2014-02-12 17:32:22,"+1 on using smaller PRs.

Also @zackw I'm sorry that none of us ever answered you: we use `py.test` to run the tests.
",sigmavirus24,zackw
1913,2014-02-12 21:14:58,"Alright, made some review comments inline. They are a supplement to @sigmavirus24's. =)
",Lukasa,sigmavirus24
1912,2014-02-11 20:08:25,"@zackw actually this is the right direction. On any response that did have history previously we were returning tuples and on any without we were returning an empty list. The fact of the matter is that history on a response should be immutable. For that to be the case, it should be a tuple. I also don't quite understand your arguments. If you're manually processing redirections or using a hook you will now get a tuple after this change, as you should have in the first place. That does raise a good point that hook authors will expect a list though (if they're writing hooks dealing with manual processing of redirecitons).

@Lukasa that makes this (sort of) a backwards incompatible change.
",sigmavirus24,zackw
1912,2014-02-11 20:35:12,"@sigmavirus24 

> The fact of the matter is that history on a response should be immutable. For that to be the case, it should be a tuple.

It is _conceptually_ immutable, but I do not see why the library needs to bother enforcing that. I think it's rather more important for the type of the property to be the same in all contexts.  And since the response-modification hook is allowed to modify history, it needs to be a list then.  Ergo it should always be a list.

Please see pull request #1913 -- I have put a good deal of thought into how this stuff needs to work, based on actual application experience.

> If you're manually processing redirections or using a hook you will now get a tuple after this change, as you should have in the first place.

On reflection, I think that accurately describes the behavior after your patch for manual redirection processing, but _not_ for the response-modification hook, which fires before the conversion to a tuple.
",zackw,sigmavirus24
1912,2014-02-12 16:12:37,"I agree with @zackw on this. Our current tuple-ness is a bit pedantic and we have to dance about it internally for no reason. Let's not do that.
",kennethreitz,zackw
1912,2014-02-13 12:53:36,"The tuple-ness on the contrary is meaningful and it would be no extra work for @zackw to adapt to this in his PR. Since `redirect_once` does the work for him now, he won't need to worry about munging the history (nor should anyone else). And we should keep in mind that people already expect a tuple _when there is history to be had_. Regardless, I agree it isn't worth arguing about.
",sigmavirus24,zackw
1910,2014-02-11 17:15:17,"Hm, I can't reproduce this at the moment.

@e-manuel Could you give more information about your environment:
- Python version
- Operating System
- Using gevent? (or the like)
- Using pyOpenSSL?
- Does it work without multiprocessing?
",t-8ch,e-manuel
1910,2014-02-11 20:27:24,"Yes, I have: `py27-asn1-0.1.4_1,1`, `py27-ndg_httpsclient-0.3.2`, `py-openssl 0.13` but following the FreeBSD ports tree:
- `py27-asn1` is required by `py27-ndg_httpsclient`
- `py27-ndg_httpsclient` is required by `py-urllib3` (mentioned by @t-8ch) and requires to run: `py-openssl` and `py-asn1`
  Of course, I can de-install any of them, but I like to keep full functionality of requests (SSL, cert. etc.). What I should to do - ""to be or not to be, this is the question..."" ;)
",e-manuel,t-8ch
1910,2014-02-11 21:45:13,"So I managed to reproduce it:

Terminal 1:



Code:



It seems this only happens when using PyOpenSSL, timeouts and FreeBSD.
I can't reproduce it any other way, permutating those conditions and on Linux.

@e-manuel Pick your poison :-)
",t-8ch,e-manuel
1910,2014-02-12 08:32:10,"I ranted about this to my housemate last night, by the by. I don't understand why PyOpenSSL busy-waits on this socket instead of calling select like a good person. @t-8ch are you making changes in the urllib3 version, or are you going to submit your patch upstream?
",Lukasa,t-8ch
1910,2014-02-13 01:49:54,"@e-manuel all of the changes are in PyOpenSSL. Since you're using the packages from your distro, you'll need to bother the maintainer of that package when it gets released.

@Lukasa your question is especially relevant since FreeBSD seems to strip out the vendored packages so it should be sent upstream if possible.
",sigmavirus24,e-manuel
1910,2014-02-13 01:49:54,"@e-manuel all of the changes are in PyOpenSSL. Since you're using the packages from your distro, you'll need to bother the maintainer of that package when it gets released.

@Lukasa your question is especially relevant since FreeBSD seems to strip out the vendored packages so it should be sent upstream if possible.
",sigmavirus24,Lukasa
1910,2014-02-13 07:41:46,"Yeah, I think we want an answer on what PyOpenSSL is going to do.

@e-manuel In the meantime, you can patch your copy so this stops happening.
",Lukasa,e-manuel
1910,2014-02-13 13:05:41,"@t-8ch thank You very much - Your patch works great, now when GET starts it uses below 2% of CPU and after few seconds less :)
",e-manuel,t-8ch
1910,2014-02-13 13:21:38,"Hurrah! Let's leave this open until we get an idea of what's happening upstream.

@t-8ch saves the day again! I should buy him a boat.
",Lukasa,t-8ch
1910,2014-02-14 11:26:29,"@pasha-r That's a very neat solution, it didn't even occur to me. That, plus checking the return code from select, should add timeouts back. 
",Lukasa,pasha-r
1910,2014-02-19 09:06:20,"@t-8ch You are great - now timeout works fine in every case - and may CPUs can ""sleep"" calmly ;)
",e-manuel,t-8ch
1908,2014-02-08 19:27:24,"Those conditionals are 100% necessary. `verify` can be (and frequently is) `False` or a path to a certificate bundle. We must conditionally set the values. Consider the paths through this code: 
1. `verify` is `True`: then `cert_loc` will be the default bundle
2. `verify` is `False`: then `cert_loc` will be `False`
3. `verify` is `/path/to/other/bundle`: then `cert_loc` will be `/path/to/other/bundle`

This code breaks a great deal of features that are apparently untested. This is more of an alarm that those features are not tested rather than a reason for removing the conditionals here. Thanks for your help @benediktkr !
",sigmavirus24,benediktkr
1907,2014-02-08 10:45:30,"Hi @Lukasa ,
Today if a user import `requests` from global python, and if this package has been patched (like it is intended and encouraged) the user will get the platform CA and not the bundle CA.
This pull request aim to give to the user to the same behaviour when he uses `requests` from its own virtualenv, not from global python. In this regard, I think, this PR is pretty consistent.

Now if we want to make progress. Do you suggest to go in a way that is more explicit for the user ?



or you completely reject the idea ?

thx
",ticosax,Lukasa
1907,2014-03-26 09:45:46,"_Use by default SSL CA certificate bundle from the platform_ is what OP proposes in this issue and that's something I totally agree with. It feels so natural to me that I find it difficult someone could oppose this, yet alone argue that not doing this is better experience. Yet this is what I see here and that's why I decided to share my opinion with you.
@Lukasa states

> I don't think we should change which certificates we use based on whether or not you install an optional dependency. Our current behaviour WRT pyopenssl is to enable additional features without changing our current behaviour. That's what you should be aiming for here.

This argument is misguided here as pretty much everyone agrees that when you're dealing with security you should **by default** be as secure as you are able to be, given the environment you operate in. This means using system CA certificates by default (and fallback to bundled ones if it's not possible or very hard to do) and not bundled ones. I think one might argue that security begs for policy of graceful 
degradation.

Also @Lukasa states

> Surely you'd want to ensure that all your users have the exact same certs on all platforms, to avoid annoying platform-specific bugs?

Surely, if the only goal is to _avoid annoying platform-specific bugs_ then yes. But if the goal is to make requests secure by default (and this is in line with _HTTP library for humans_ motto) then surely you would want to ensure that all your users have _the best certs available on their systems_.

Additional bonus is that you save packagers from having to patch this (mis)feature.

I'm curious what @dstuff and @t-8ch think.
",piotr-dobrogost,Lukasa
1907,2014-03-26 10:30:27,"@piotr-dobrogost I'm pretty neutral on this, but I suspect Kenneth will be against it. It adds code complexity (we need a fallback path) and project management overhead (extra bugs filed, both when we change which certificates we use to verify and later when people have failures on specific platforms that do not reproduce on others). Given that we're doing a pretty good job of securing the system already, and that we make it possible to use other certificate bundles, you've got a hell of a job proving that this is worth the switch.
",Lukasa,piotr-dobrogost
1907,2014-03-26 13:12:38,"@sigmavirus24, That's the point of this PR, pyopenssl takes care to provide the default platform CA.

https://github.com/shazow/urllib3/commit/5c25a73dfb48e4260c44e19e3a50fb5d46832c52
http://pyopenssl.sourceforge.net/pyOpenSSL.html/openssl-context.html#l2h-132
",ticosax,sigmavirus24
1907,2014-03-26 13:16:45,"@ticosax The section of the documentation you linked to quite literally says 'This method may not work properly on OS X'. Such an admission is tantamount to saying 'This method only sometimes works'. 

Additionally, I can tell you that this doesn't work properly because I've hit exactly this bug in my own project, [hyper](https://github.com/Lukasa/hyper). See issue Lukasa/hyper#9, but in summary, `set_default_verify_paths()` only works on some systems, and otherwise fails _without any way to detect it_, as you can see from [the Python documentation](http://docs.python.org/3.4/library/ssl.html#ssl.SSLContext.set_default_verify_paths). Given that PyOpenSSL is only a thin wrapper around SSL, much like the stdlib version, I will assume that PyOpenSSL has exactly the same problems with its method.

BTW, one of those systems is Windows. =)
",Lukasa,ticosax
1907,2014-03-26 13:17:43,"@Lukasa summed up everything I was about to say in a much nicer way. Count this as a +1 for what he just said.
",sigmavirus24,Lukasa
1907,2014-03-26 14:46:37,"@sigmavirus24 

> These people then sent PRs to extend the list of possible locations to include the specific locations for the different versions of that distribution they happened to use.

Then all you have to do per @ticosax's remark – _(...) pyopenssl takes care to provide the default platform CA._ – is to direct these people to pyopenssl and close issue right away :)

The theme of this project is that you care about what most people do and you try to make their lives easier. Now, I bet most people use only a handful of systems so there's no problem for pyopenssl project to take care of those. When someone uses unpopular system he should not expect that every piece of software on earth would handle his system. In this case telling him he has to point requests to his CA certs is perfectly fine.

@Lukasa 

> The section of the documentation you linked to quite literally says 'This method may not work properly on OS X'. Such an admission is tantamount to saying 'This method only sometimes works'. 

OS X is a popular OS (unfortunately as I don't like Apple at all) so if it really does not work for it then it's unacceptable. However I don't believe OS X actively hides its CA certs :)
",piotr-dobrogost,ticosax
1907,2014-03-26 14:46:37,"@sigmavirus24 

> These people then sent PRs to extend the list of possible locations to include the specific locations for the different versions of that distribution they happened to use.

Then all you have to do per @ticosax's remark – _(...) pyopenssl takes care to provide the default platform CA._ – is to direct these people to pyopenssl and close issue right away :)

The theme of this project is that you care about what most people do and you try to make their lives easier. Now, I bet most people use only a handful of systems so there's no problem for pyopenssl project to take care of those. When someone uses unpopular system he should not expect that every piece of software on earth would handle his system. In this case telling him he has to point requests to his CA certs is perfectly fine.

@Lukasa 

> The section of the documentation you linked to quite literally says 'This method may not work properly on OS X'. Such an admission is tantamount to saying 'This method only sometimes works'. 

OS X is a popular OS (unfortunately as I don't like Apple at all) so if it really does not work for it then it's unacceptable. However I don't believe OS X actively hides its CA certs :)
",piotr-dobrogost,Lukasa
1907,2014-03-26 14:46:37,"@sigmavirus24 

> These people then sent PRs to extend the list of possible locations to include the specific locations for the different versions of that distribution they happened to use.

Then all you have to do per @ticosax's remark – _(...) pyopenssl takes care to provide the default platform CA._ – is to direct these people to pyopenssl and close issue right away :)

The theme of this project is that you care about what most people do and you try to make their lives easier. Now, I bet most people use only a handful of systems so there's no problem for pyopenssl project to take care of those. When someone uses unpopular system he should not expect that every piece of software on earth would handle his system. In this case telling him he has to point requests to his CA certs is perfectly fine.

@Lukasa 

> The section of the documentation you linked to quite literally says 'This method may not work properly on OS X'. Such an admission is tantamount to saying 'This method only sometimes works'. 

OS X is a popular OS (unfortunately as I don't like Apple at all) so if it really does not work for it then it's unacceptable. However I don't believe OS X actively hides its CA certs :)
",piotr-dobrogost,sigmavirus24
1907,2014-03-26 14:49:00,"@piotr-dobrogost Gotta read my whole answer. I have personal evidence that this does not work on Windows (though OS X was actually fine), where I got SSL certificate errors when talking to Twitter's servers. Given that there's no error response from the method, we just can't rely on it.
",Lukasa,piotr-dobrogost
1906,2014-02-08 15:00:52,"@ssbarnea can you at least share the URL so that we can attempt to reproduce it? So far I think we have almost sufficient information, we just need the URL or characteristics of the server to reproduce it:
- Multiprocessing using _10 threads_
- Function that instantiates a Session and makes more than one request (ostensibly to the same server) with it.
",sigmavirus24,ssbarnea
1906,2014-03-06 01:59:37,"@crizCraig I'm not familiar with that project. What is the significance of those values? Are they:
- Configuration file values?
- Environment values?
- Other?
",sigmavirus24,crizCraig
1906,2014-03-23 11:52:06,"@crizCraig Ping. =)
",Lukasa,crizCraig
1906,2014-12-16 06:56:13,"@zvodd Have you tried forcing SSL negotiation at different versions, as per [this article](https://lukasa.co.uk/2013/01/Choosing_SSL_Version_In_Requests/)?
",Lukasa,zvodd
1906,2015-02-12 19:54:54,"@maxcountryman can you provide any of the details we've been asking for?
",sigmavirus24,maxcountryman
1906,2015-02-12 20:01:20,"@sigmavirus24 well, I'm not using Heroku. What other details did you want?
",maxcountryman,sigmavirus24
1906,2015-02-12 20:14:33,"@maxcountryman 
- Your version of python and openssl
- You operatingsystem and version
- A minimal breaking code snippet
- If possible a public URL which triggers the bug (together with the mentioned code snippet)
",t-8ch,maxcountryman
1906,2015-03-17 08:51:34,"Hi @Lukasa @sigmavirus24 @t-8ch ,
Is your article (https://lukasa.co.uk/2013/01/Choosing_SSL_Version_In_Requests/) compatible with Python 3? Is it right that the current issue will be solved by updating python to version 2.7.9?
",ulandj,t-8ch
1906,2015-03-17 08:51:34,"Hi @Lukasa @sigmavirus24 @t-8ch ,
Is your article (https://lukasa.co.uk/2013/01/Choosing_SSL_Version_In_Requests/) compatible with Python 3? Is it right that the current issue will be solved by updating python to version 2.7.9?
",ulandj,Lukasa
1906,2015-03-17 08:51:34,"Hi @Lukasa @sigmavirus24 @t-8ch ,
Is your article (https://lukasa.co.uk/2013/01/Choosing_SSL_Version_In_Requests/) compatible with Python 3? Is it right that the current issue will be solved by updating python to version 2.7.9?
",ulandj,sigmavirus24
1906,2015-03-17 08:57:05,"@ulandj The article should be Python 3 compatible. Upgrading to Python 2.7.9 will solve a lot of problems.
",Lukasa,ulandj
1906,2015-03-17 09:11:45,"@Lukasa i.e. after upgrading to Python 2.7.9 I don't need use your adapter in the article, right? And 



will work with any count of multiprocesses?
",ulandj,Lukasa
1906,2015-03-18 05:10:31,"@Lukasa our customer says that they have the following error: 



Does it say about something?

Using:
python (2.7.9)
requests (2.3.0)
",ulandj,Lukasa
1906,2015-03-18 05:40:31,"@ulandj sounds like you're running into http://stackoverflow.com/a/3724938/1953283
",sigmavirus24,ulandj
1906,2015-03-18 06:03:31,"@sigmavirus24 
you think that if I move this line of code (from constructor of class) - https://github.com/iron-io/iron_core_python/blob/master/iron_core.py#L162  here - https://github.com/iron-io/iron_core_python/blob/master/iron_core.py#L188, then it will work without errors?
",ulandj,sigmavirus24
1906,2015-03-18 14:52:31,"@ulandj fwiw the code that breaks in my application involves that same IronIO wrapper.
",maxcountryman,ulandj
1906,2015-03-18 15:30:39,"@ulandj I've made the [changes you suggested](https://github.com/maxcountryman/iron_core_python/commit/654350539374951bc852c57e0abe92a72cef24fd) and so far things seem to be working fine on my local development machine. I should be able to roll this out to staging soon where the errors are more prevalent.
",maxcountryman,ulandj
1906,2015-03-18 18:20:17,"@ulandj rolled the above changes out to staging and it's been smooth sailing ever since. I doubt they'll be merged upstream since you totally lose the benefit of the session but at any rate it does seem to fix it.
",maxcountryman,ulandj
1906,2015-03-19 04:11:55,"@maxcountryman thanks,
We offered our customer to install iron_core from [ulan-multiprocessing-sslerror](https://github.com/iron-io/iron_core_python/compare/ulan-multiprocessing-sslerror) branche. We will observe it. Hope the move ""requests"" will help to resolve the ssl error in multiprocessing.
",ulandj,maxcountryman
1906,2016-03-25 00:45:43,"@Lukasa simple repro description:
1. Make session in master process
2. Make some requests to an SSL-enabled site
3. Start 2 multiprocessing processes
4. In the first, make some more requests to the same SSL-enabled site (so connection pooling kicks in)
5. In the second, wait a little bit, then make a request to the same SSL-enabled site

I can't post my script that's hitting this, but I'll see if I can make a simple script that reproduces this with httpbin.

Why this happens: The two children processes share the SSL state initially, and are using the same socket for communication, but once one of them makes a request, the state becomes desynchronized, so when the other tries to use it, the SSL decryption fails.

Some possible fixes:
- Throw out SSL connections from the connection pool upon unpickling the session and close their associated socket objects
  - This is fine because closing the socket object only closes that process's file descriptor; the underlying connection will not be closed until all file descriptors pointing to it are closed
  - Do it on unpickling instead of preventing the connections from making it into the pickle so that you can clean up the file descriptors in the child processes
- Throw out the whole connection pool on unpickling
  - Probably the best idea, because things are probably going to break
- Warn/explode on unpickling a session that has SSL/any connections in its connection pool
",dwfreed,Lukasa
1906,2016-03-25 09:36:36,"@Lukasa I'm MITMing the connection using a transparent proxy.  Only 1 connection is ever made.
",dwfreed,Lukasa
1906,2016-03-25 10:52:47,"@dwfreed Session setup costs are very low: sessions don't have to do a whole lot to get set up. In particular, given that you have to wait for the API to return before setting up the children I think you can probably just wrap the setup as part of the function you invoke in the background process and it becomes essentially unnoticeable.

This comes in to why I'm nervous about writing documentation for this: I don't think we can provide much more than general advice which would boil down to: don't share Sessions across processes, consider carefully whether you want to share Sessions across threads, when writing concurrent code try to adopt good concurrent design patterns.

Anything more than that is just too restrictive as far as advice goes.
",Lukasa,dwfreed
1906,2016-11-18 04:28:40,"@jbbarth you're my hero. this error just started happening to me on boto as I added multiprocessing. your workaround isn't working for me (yet). Which requests version are you using?
",fuzzyami,jbbarth
1906,2016-11-20 00:38:47,"@fuzzyami I think it was 2.10, along with boto 2.38 or 2.39. The heuristics in boto may have changed, I recommend you look into their connection pooling mechanism or grep the environment variables I was referring to. Good luck! :)
",jbbarth,fuzzyami
1905,2015-11-24 16:42:58,"Hi @rattrayalex Your patch does not seem to work anymore, in particular to connect to Google IP addresses, as such IP's are blocked in GAE's new sockets API: https://cloud.google.com/appengine/docs/python/sockets/
",jacobg,rattrayalex
1905,2015-11-24 19:29:53,"@Lukasa think that should go into the toolbelt?
",sigmavirus24,Lukasa
1900,2014-02-03 14:09:54,"Thanks @mjpieters! :cake: 
",sigmavirus24,mjpieters
1900,2014-02-11 16:40:36,"@Lukasa for the record, i only got notified when someone directly mentions me ;)
",kennethreitz,Lukasa
1899,2014-02-06 19:23:22,"@rattrayalex sorry?
",sigmavirus24,rattrayalex
1898,2014-02-02 16:49:39,"The exact line that @Lukasa is referring to is [here](https://github.com/kennethreitz/requests/blob/master/requests/sessions.py#L533). Just out-dent that line and you'll be fine. Please revert the change you've made here before making this change. Also please continue working on this PR and do not open a new one @Zopieux 

Please leave a comment when you've updated the PR.
",sigmavirus24,Zopieux
1898,2014-02-02 16:49:39,"The exact line that @Lukasa is referring to is [here](https://github.com/kennethreitz/requests/blob/master/requests/sessions.py#L533). Just out-dent that line and you'll be fine. Please revert the change you've made here before making this change. Also please continue working on this PR and do not open a new one @Zopieux 

Please leave a comment when you've updated the PR.
",sigmavirus24,Lukasa
1896,2014-01-31 14:16:54,"Beyond the security implications that @Lukasa has already outlined, I am 100% opposed to the hard coding of an import statement of a module that is not maintained by one of us. This provides a rich opportunity for someone to make a package on PyPI that subtly hides this and installs a package named `requests_extension`. Let's say I added that to a package I already distribute, then I could, by someone installing an otherwise innocuous package, totally take control of certificate discovery on their installation without their knowledge.

You claim requests is insecure already but this would introduce the largest of backdoors into requests as it exists now.

Thank you for your contribution but we can not accept it in good conscience.

Furthermore, I don't feel there is further need for discussion on this issue.
",sigmavirus24,Lukasa
1896,2014-01-31 14:44:11,"thank you @Lukasa I understand the use case you pointed now,
and agreed it was a bad idea to do it that way.

What about adding built-in operating system flavor instead ?
Most package maintainer of requests for major linux distributions just hardcode the path to the main cert file like `/etc/ssl/certs/ca-certificates.crt` for debian based distributions for instance.

Then `requests` could provide out of the box some of those locations, if we can guess reliably the type of distribution requests is running on.

How does it sounds ?
",ticosax,Lukasa
1896,2014-01-31 15:22:50,"@ticosax we experimented with including a list of popular locations and just kept receiving pull requests to add it for more and more obscure repositories so we removed them entirely and rely on what we have now. It is highly unlikely we'll be moving back to that model
",sigmavirus24,ticosax
1896,2014-01-31 17:45:02,"Thank you for contributing @ticosax 
",sigmavirus24,ticosax
1894,2014-01-30 13:37:21,"Thanks for this @vmalloc! I do not think we'll accept it at this time though. I'm on my phone at the moment but I'll be happy to give a more detailed explanation in a short while.
",sigmavirus24,vmalloc
1894,2014-01-31 17:56:27,"@vmalloc that isn't really the issue. There's a semantic meaning in the name `raise_for_status`. The meaning in it is that you want to raise an Exception for something that is not an okay status code (vaguely some status code < 400). There should be no explicit return from that method. There is an implicit return of `None` because any Python function (or method) written without a `return` returns `None`. The function has one purpose: raise an exception. Returning the response makes the API inconsistent -- which is why it is a breaking API change -- and it makes the meaning of the function nebulous.

I understand why you feel this change is desirable but it is not a change we can accept. We also will not accept an addition of a method to provide this behaviour. It isn't something that fits in with the overall design of requests.

Thanks for contributing though! :cake:
",sigmavirus24,vmalloc
1894,2016-05-12 21:08:15,"i know this PR is quite old, but i really like it!... mostly because IMO it is completely in sync with the `requests` package's main objective of reducing clutter. without this change, the following is required:



isn't @vmalloc's version much cleaner? if the problem is the naming of `raise_for_status`, how about renaming it (with a deprecating alias)? e.g.:



(or whatever you want) or how about a new option to all of the request methods:



btw, i currently always monkey-patch `Response.raise_for_status` to do exactly this. ugly!
",metagriffin,vmalloc
1894,2016-05-13 05:27:35,"I frankly still don't understand the reason this was rejected. I don't
think this qualifies for ""API breakage"" per se, and the utility of this
change is enormous.

I suspect the pattern you mentioned is prevalent in 99% of the use cases.
On יום ו׳, 13 במאי 2016 at 0:08 metagriffin notifications@github.com
wrote:

> i know this PR is quite old, but i really like it!... mostly because IMO
> it is completely in sync with the requests package's main objective of
> reducing clutter. without this change, the following is required:
> 
> result = requests.get(""http://api.server.com/path"")
> result.raise_for_status()
> result = result.json()
> 
> isn't @vmalloc https://github.com/vmalloc's version much cleaner? if
> the problem is the naming of raise_for_status, how about renaming it
> (with a deprecating alias)? e.g.:
> 
> result = requests.get(""http://api.server.com/path"").require_ok().json()
> 
> (or whatever you want) or how about a new option to all of the request
> methods:
> 
> result = requests.get(""http://api.server.com/path"", require_ok=True).json()
> 
> btw, i currently always monkey-patch Response.raise_for_status to do
> exactly this. ugly!
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/pull/1894#issuecomment-218886371
",vmalloc,vmalloc
1894,2017-02-10 13:28:59,@guillp I agree. It's funny to me how this library willy-nilly broke compatibility time and again (e.g. with `json` turning to `json()`) but adding a single return value where there was none is a major backward compatibility issue...,vmalloc,guillp
1894,2017-02-10 13:31:24,"Hi @vmalloc.

*Please* try to extend charity to the maintainers of OSS projects you use. We have not willy-nilly broken compatibility. The move from `json` to `json()` was accompanied by a major version bump and a substantial internal refactoring. Other breakages of compatibility have been considered to be bugs and errors and treated as such.

Ironically, I was previously inclined to look much more warmly on this proposal than my fellow maintainers. Now I'm not. The answer is no.",Lukasa,vmalloc
1892,2014-01-30 20:38:17,"@kennethreitz we absolutely cannot continue reusing authorizations on redirects to sites that are not the same host. With that in mind we almost certainly need to issue a CVE. I'll happily work on that though.

We've been leaking credentials and we need to at least address that. Whether we repopulate the auth after stopping the leak or not is more of a feature decision. I'm sure one of the security experts, like @dstufft would back up @Lukasa and I on that.
",sigmavirus24,kennethreitz
1892,2014-01-30 20:38:17,"@kennethreitz we absolutely cannot continue reusing authorizations on redirects to sites that are not the same host. With that in mind we almost certainly need to issue a CVE. I'll happily work on that though.

We've been leaking credentials and we need to at least address that. Whether we repopulate the auth after stopping the leak or not is more of a feature decision. I'm sure one of the security experts, like @dstufft would back up @Lukasa and I on that.
",sigmavirus24,Lukasa
1892,2014-01-31 07:19:31,"@kennethreitz , call me the outsider but @sigmavirus24 got a point. As the new guy 'round these parts. You have 1.1million downloads. Even if half that still use, we are talking about a **major** security failure in the code base. :no_good: 
",Stephn-R,kennethreitz
1892,2014-01-31 07:19:31,"@kennethreitz , call me the outsider but @sigmavirus24 got a point. As the new guy 'round these parts. You have 1.1million downloads. Even if half that still use, we are talking about a **major** security failure in the code base. :no_good: 
",Stephn-R,sigmavirus24
1892,2014-01-31 07:44:24,"@Lukasa So in that sense, are you suggesting that the session itself may be invalidated and then a restart is required?
",Stephn-R,Lukasa
1892,2014-01-31 16:59:22,"@sigmavirus24 this was an explicit design decision, and it has been stated as such many times before. I've considered implementing a patch much like many times before, and this was a minor concern in the back of my mind when I did ""the big refactor"". As I said, I need to think about it.

Further comments about are neither helpful nor welcome. :)
",kennethreitz,sigmavirus24
1891,2014-02-01 15:28:26,"@kennethreitz I have no strong opinion on this either way frankly. #1890 seemed like a reasonable feature request though. The decision is all yours.
",sigmavirus24,kennethreitz
1891,2014-02-11 17:14:05,"@kennethreitz you assume correctly. And this was a one time idea anyway. It seemed reasonable enough to allow it.
",sigmavirus24,kennethreitz
1885,2014-01-27 19:46:16,"> EDIT: To be clear, we should conditionally delete the Authorization header, only if we're being redirected to a new host.

@Lukasa I agree!
",eriol,Lukasa
1885,2014-01-31 12:38:35,"@sigmavirus24 unfortunately I don't have experience with CVEs, but it's well described here:
http://people.redhat.com/kseifrie/CVE-OpenSource-Request-HOWTO.html

Sending a request to oss-security@lists.openwall.com should be enough,
but maybe security folks at Red Hat made a request for a CVE since they are following the issue too: https://bugzilla.redhat.com/show_bug.cgi?id=1046626

As you can see Endi Sukma Dewata also asked me if there are tests for the Proxy-Authorization case.
I'm going to reply on Red Hat tracker asking if they already sent a request for a CVE.
",eriol,sigmavirus24
1885,2014-09-12 20:03:14,"Good catch @blueyed. This was fixed in [v2.3.0](https://github.com/kennethreitz/requests/releases/tag/v2.3.0).
",sigmavirus24,blueyed
1884,2014-02-08 17:58:53,"@jimbaker correct me if I am misunderstanding you but basically requests should not need to do anything after Jython 2.7 is released to support it. It sounds like the IDNA changes that were made here will be made unnecessary and that ssl will not be an issue. Also, requests does not use unicode string literals in the library at all, only in the tests. Can you ping us when Jython 2.7 is released so we can start testing with it then?
",sigmavirus24,jimbaker
1884,2014-04-02 03:55:28,"@sigmavirus24 As of a few minutes ago, 100% of the unit tests for requests master now pass when run on this branch of Jython, which supports SSL and other goodies: https://bitbucket.org/jimbaker/jython-socket-reboot

Please note that the above branch still needs to be cleaned up (at the very least, remove copious print debugging!), but that's all pretty obvious in the FIXMEs and prints.

I will ping again when we have this merged against Jython trunk, in prep for beta 3 - it would be great for us to have Jython as part of your testing.
",jimbaker,sigmavirus24
1884,2014-05-11 00:15:01,"You're doing great work @jimbaker !
",sigmavirus24,jimbaker
1884,2014-09-04 19:18:43,"@kennethreitz sorry?
",sigmavirus24,kennethreitz
1882,2014-05-06 13:15:34,"Thanks @here 
",sigmavirus24,here
1882,2014-08-15 07:41:36,"@Lukasa: if this is not a bug, then what exactly is it? It seems to be that its complaining that a connection was left open after the unit tests ended, but how can that be if I'm wrapping the requests.post() call with contextlib.closing() like the documentation mentions here? http://docs.python-requests.org/en/latest/user/advanced/#body-content-workflow am I doing something incorrect?
",mgrandi,Lukasa
1882,2014-11-07 02:25:54,"@thejohnfreeman we create a new session every time you make a request with `requests.{get,post,put,delete,etc.}`. See [the relevant code in `requests/api.py`](https://github.com/kennethreitz/requests/blob/master/requests/api.py#L48)
",sigmavirus24,thejohnfreeman
1882,2015-09-02 09:55:43,"@here suppressing the warning with `warnings.catch_warnings()` is not guaranteed to work because the warning is issued when the connection pool is garbage collected which can happen after the context manager has closed.
",siebenschlaefer,here
1882,2015-09-03 03:36:27,"@siebenschlaefer That's correct, which is why we recommend you use explicit `Session` objects. =)
",Lukasa,siebenschlaefer
1879,2014-01-24 22:24:53,"@ibuildthecloud can you provide an example of the URL you were requesting with docker-py?
",sigmavirus24,ibuildthecloud
1879,2014-01-25 20:58:44,"> What are docker-py doing to route over the unix domain socket? At the very 
> least they'll have to be mounting a Transport adapter.

I was thinking the same thing, but the Transport adapter doesn't handle 
params. That can only be handled by a pepared request object. I'm not sure 
they could pass on the params they need to tack on with the adapter in this 
case. Unless, you had another idea how to go about that @Lukasa, I think that 
may be a dead end. I agree though that the parameter handling should only be 
for HTTP(S) URLs since those are the only ones we are really positioned or 
likely to support.

docker-py could use URI templates to bypass having to use the `params` 
parameter to requests, but that would likely introduce a new dependency which 
they might not want.
",sigmavirus24,Lukasa
1879,2014-01-26 00:23:51,"So, the issue is that as @sigmavirus24 mentioned, Requests handles parsing the URL about two layers higher than the Transport Adapter (in the PreparedRequest object). Certainly docker-py _can_ work around this by playing about with that object (either monkeypatching it or using the [explicit PreparedRequest flow](http://docs.python-requests.org/en/latest/user/advanced/#prepared-requests)), but the real meat of this issue is whether they should have to.

I remain as I was before, at -0.5. It feels like we'd be doing the wrong thing. I am, however, open to being convinced on this issue.
",Lukasa,sigmavirus24
1879,2014-01-26 00:34:28,"I took a look to see how simple that would be for them to implement but they're code seems quite full of misdirection. That said, the way they're currently doing everything, I find it hard to believe they'll be up for using the explicit flow @Lukasa linked to.
",sigmavirus24,Lukasa
1879,2014-01-26 14:05:43,"I'm in favor of 3. I just looked at docker-py to see if I could give @ibuildthecloud a hand in preparing a PR. That said, 1 is plenty dangerous in my opinion. Just because someone is using requests to talk to something does not mean we can assume it is not translating a PreparedRequest into some other format. I'm not convinced that it is reasonable (or not totally unreasonable) to assume that they're using `HTTP/1.1`.
",sigmavirus24,ibuildthecloud
1879,2014-01-26 21:42:24,"@ibuildthecloud the short answer is no.

The long answer is this: Transport Adapters _only_ handle the _transmission_ of requests and responses. Prepared Requests are the representation of a request ready to be sent. Sessions encapsulate the logic (and only the logic) of a user-agent session (lightly speaking) including handling of cookies, and determining which transport adapter to use to perform an interaction.

The pattern, is this: Each object only knows what it has to know about. Parameters are related to requests only, transport adapters should care not what parameters are sent. They should concern themselves only with processing a request and generating a response.

Perhaps the best solution for docker-py is to monkeypatch the `PreparedRequest` object's `prepare_url` method to not check the scheme of the URL.  With that in mind, you will be duplicating some code unfortunately.
",sigmavirus24,ibuildthecloud
1879,2014-01-26 21:51:42,"Hmmm...

Who's responsibility is it to generate and write the line ""GET /x?k=b""
line in the HTTP request?  Isn't that the transport that does that?

On Sun, Jan 26, 2014 at 2:42 PM, Ian Cordasco notifications@github.comwrote:

> @ibuildthecloud https://github.com/ibuildthecloud the short answer is
> no.
> 
> The long answer is this: Transport Adapters _only_ handle the
> _transmission_ of requests and responses. Prepared Requests are the
> representation of a request ready to be sent. Sessions encapsulate the
> logic (and only the logic) of a user-agent session (lightly speaking)
> including handling of cookies, and determining which transport adapter to
> use to perform an interaction.
> 
> The pattern, is this: Each object only knows what it has to know about.
> Parameters are related to requests only, transport adapters should care not
> what parameters are sent. They should concern themselves only with
> processing a request and generating a response.
> 
> Perhaps the best solution for docker-py is to monkeypatch the
> PreparedRequest object's prepare_url method to not check the scheme of
> the URL. With that in mind, you will be duplicating some code unfortunately.
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/issues/1879#issuecomment-33331429
> .
",ibuildthecloud,ibuildthecloud
1879,2014-01-27 00:05:23,"> There should be a nice hook to provide custom logic to deal with params.

I think it's fairly obvious that @Lukasa and I disagree with this sentiment. I'm not sure we can say it enough times but requests is an HTTP library. While docker-py is using it to perform HTTP over a unix socket, not every client using it on a Unix socket will be doing HTTP necessarily. There is no reasoning beyond some extraordinary uses for us to enable a hook or any other simpler means when there are already documented ways around this.

Frankly, it is far from impossible to accomplish but just because we make it possible does not mean we should encourage it by making it simple.

I'm glad you've found one way around it and are working on a pull request to docker-py.

Cheers!
",sigmavirus24,Lukasa
1879,2014-01-27 00:59:28,"Thanks for your help and quick replies.  I should have clarified from the
beginning that I was talking about HTTP over a custom transport.  It never
even occurred to me that requests could be used for non-HTTP.

On Sun, Jan 26, 2014 at 5:05 PM, Ian Cordasco notifications@github.comwrote:

> There should be a nice hook to provide custom logic to deal with params.
> 
> I think it's fairly obvious that @Lukasa https://github.com/Lukasa and
> I disagree with this sentiment. I'm not sure we can say it enough times but
> requests is an HTTP library. While docker-py is using it to perform HTTP
> over a unix socket, not every client using it on a Unix socket will be
> doing HTTP necessarily. There is no reasoning beyond some extraordinary
> uses for us to enable a hook or any other simpler means when there are
> already documented ways around this.
> 
> Frankly, it is far from impossible to accomplish but just because we make
> it possible does not mean we should encourage it by making it simple.
> 
> I'm glad you've found one way around it and are working on a pull request
> to docker-py.
> 
> Cheers!
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/issues/1879#issuecomment-33335146
> .
",ibuildthecloud,Lukasa
1879,2014-01-27 02:41:11,"@ibuildthecloud we love to help when possible. 

> It never even occurred to me that requests could be used for non-HTTP.

As @Lukasa mentioned, he wrote an FTP adapter. User's can implement whatever backend they want with Transport Adapters. They do not have to use urllib3. They can also process a Prepared Request however they like. We cannot facilitate all of their needs even when they're performing HTTP over a different transport.
",sigmavirus24,ibuildthecloud
1879,2014-01-27 02:41:11,"@ibuildthecloud we love to help when possible. 

> It never even occurred to me that requests could be used for non-HTTP.

As @Lukasa mentioned, he wrote an FTP adapter. User's can implement whatever backend they want with Transport Adapters. They do not have to use urllib3. They can also process a Prepared Request however they like. We cannot facilitate all of their needs even when they're performing HTTP over a different transport.
",sigmavirus24,Lukasa
1877,2014-01-24 06:52:54,"@sigmavirus24 is exactly right.

The `Response.json()` method has caused us a lot of trouble. In almost every situation we reject features that involve manipulating content on the grounds that requests is an HTTP library, not an ""anything-else"" library, and every time we do that `Response.json()` gets pointed out. The fact is, we only implemented that because it's an overwhelmingly common use-case, which parsing multipart data is not.

That said, I encourage you to to whack it in the toolbelt. That would be an excellent place for it.

Thanks for raising this issue!
",Lukasa,sigmavirus24
1875,2014-01-22 14:29:31,":heart: our release manager @Lukasa 
",sigmavirus24,Lukasa
1871,2015-12-14 15:23:37,"@Lukasa If the issue is limited to the stdlib cookie jar, then using a requests.Session across multiple threads should be fine for APIs (without cookies).

Is this correct?
",pior,Lukasa
1871,2015-12-14 15:24:27,"@pior It's my belief that that should be safe.
",Lukasa,pior
1871,2015-12-15 08:15:27,"@pepijndevos That should not happen. The connection pool is thread-safe, and when a connection is removed from the pool it is owned entirely by the object that withdrew it. As a result, the connection should not be dropped before use. It is _possible_ that there is a TCP FIN packet in flight when the connection is being handled before use, and as a result the connection is torn down: in that situation, a simple retry is a good idea (and something that should be being done anyway).
",Lukasa,pepijndevos
1869,2014-01-31 07:32:36,"@jschneier I'm not so worried about that particular issue: we've never hit it in Requests. I'm just wondering whether there's a good technical reason we shouldn't just limit ourselves to well-formed HTTP/1.1.
",Lukasa,jschneier
1868,2014-01-18 22:16:33,"@creese is it possible that you could share the URL with us as well so we can see if we can reproduce the behaviour? It would also be very helpful if you could answer @Lukasa's questions. Thanks for helping us help you!
",sigmavirus24,creese
1868,2014-01-18 22:16:33,"@creese is it possible that you could share the URL with us as well so we can see if we can reproduce the behaviour? It would also be very helpful if you could answer @Lukasa's questions. Thanks for helping us help you!
",sigmavirus24,Lukasa
1868,2014-01-18 22:31:26,"@sigmavirus24 There was an issue with the form data in the initial request. It's resolved.
",creese,sigmavirus24
1868,2014-01-19 04:27:39,"Thanks for updating us @creese 
",sigmavirus24,creese
1866,2014-01-15 17:08:49,"Hi @oinopion, thanks for raising this issue!

It's an understandable thing to want. However, putting this in Requests by default is a trade-off: each additional authentication option added to Requests is an additional bit of code and maintenance cost. It also adds confusion to the API.

For this reason, Requests undertook to remove some of its authentication helpers in V1.0, as you can see in the [changelog](https://github.com/kennethreitz/requests/blob/1720e4bb87f5db29e9d9f42da1b0cbb1f52a7171/HISTORY.rst#100-2012-12-17). These got moved to helper modules under the requests organisation ([OAuth](https://github.com/requests/requests-oauthlib), [Kerberos](https://github.com/requests/requests-kerberos) and [NTLM](https://github.com/requests/requests-ntlm)), indicating their status as 'blessed' first-party solutions, while reducing the weight and complexity of the main library.

Candidates for the main library need two things: they need to be simple, and they need to be popular. Digest auth for proxies is simple, but it's not popular. I'd argue that it's dramatically less popular than OAuth, and probably less popular than Kerberos. For this reason, I'm disinclined to add support to the main Requests library. Interestingly, because it's so simple, I'm also disinclined to add a whole additional package for it under the requests organisation. I think the best place for it is actually the nascent [requests toolbelt](https://github.com/sigmavirus24/requests-toolbelt), which is the intended home for useful utilities like this kind of thing.

Does that sound like an acceptable approach?
",Lukasa,oinopion
1863,2014-01-13 21:31:36,"Thanks for raising this @kevinburke!

That area is very carefully designed. We aren't reraising exceptions, we're wrapping them. This means we don't want the urllib3 exception to be raised, we want to raise the `requests.exceptions.SSLError`. In principle we could attach the traceback from the previous exception to this one, but that's unfortunately somewhat misleading. I don't really see any particular problem with this traceback, if I'm honest.

Does anyone disagree?
",Lukasa,kevinburke
1863,2014-01-13 22:38:47,"I'm in complete agreement with @Lukasa. FYI @kevinburke there has been a recent push to catch the last few exceptions that we didn't realize were spuriously being raised by urllib3 and bleeding through.

+1 to close this.
",sigmavirus24,kevinburke
1863,2014-01-13 22:38:47,"I'm in complete agreement with @Lukasa. FYI @kevinburke there has been a recent push to catch the last few exceptions that we didn't realize were spuriously being raised by urllib3 and bleeding through.

+1 to close this.
",sigmavirus24,Lukasa
1862,2014-01-13 13:12:53,"thanks @Lukasa sorry I didn't notice your weekend work..!
",sybeck2k,Lukasa
1862,2014-01-13 13:18:16,"@sybeck2k It's really not a problem at all. =) Ego is a dangerous thing in open source so I try to check mine; and besides, you did the very generous thing of merging my changes with yours, so I think you handled this very well. Thankyou. =)
",Lukasa,sybeck2k
1861,2014-01-13 09:00:08,"@Lukasa 

> My only worry is that technically this changes what our multipart requests look like on the wire

I don't think it changed anything. A proper guess of file content type shouldn't break anything (I hope).

ping @sigmavirus24 for a code review.
",lepture,Lukasa
1861,2014-01-13 10:46:17,"@Lukasa Yes, you are definitely right. But it has a chance to do such thing:



I think we are here to discuss which is the best default way to handle file uploading.
",lepture,Lukasa
1861,2014-01-13 12:47:59,"@lepture are you suggesting that in order to achieve the old behaviour (that probably most of our users rely on) they'll have to pass a tuple of `(filename, data, None)`? To me that is entirely backwards incompatible behaviour and would indicate this change would need to wait until 3.0.0.

I share @Lukasa's concern that while this is technically valid and should not break anything that it might. I'm trying to imagine a file type for which `mimetools` could return the wrong `Content-Type` header. All I can imagine is something entirely proprietary that isn't necessary publicly available and that a corporate client is posting to a service they own. It's entirely plausible for them to not use a `Content-Type` header in that case because their server will know how to handle it. In that case, since they control both ends, I can't imagine this would necessarily break anything but it would certainly cause them a headache.

That aside, my larger sticking point is that this is a backwards incompatible change. To achieve the same behaviour as before, users have to pass extra parameters that they didn't before. That's not good.
",sigmavirus24,lepture
1861,2014-01-13 12:47:59,"@lepture are you suggesting that in order to achieve the old behaviour (that probably most of our users rely on) they'll have to pass a tuple of `(filename, data, None)`? To me that is entirely backwards incompatible behaviour and would indicate this change would need to wait until 3.0.0.

I share @Lukasa's concern that while this is technically valid and should not break anything that it might. I'm trying to imagine a file type for which `mimetools` could return the wrong `Content-Type` header. All I can imagine is something entirely proprietary that isn't necessary publicly available and that a corporate client is posting to a service they own. It's entirely plausible for them to not use a `Content-Type` header in that case because their server will know how to handle it. In that case, since they control both ends, I can't imagine this would necessarily break anything but it would certainly cause them a headache.

That aside, my larger sticking point is that this is a backwards incompatible change. To achieve the same behaviour as before, users have to pass extra parameters that they didn't before. That's not good.
",sigmavirus24,Lukasa
1861,2014-01-13 14:44:05,"@sigmavirus24 I am not so sure now. I think it would be better for a smart guessing of content type. Because when I first use this lib for posting files, I thought it should handle smarter.

I had a look at the code. And I found that urllib3 handles smart, but requests not. I thought maybe requests just missed it. I didn't expect that you intended to handle it this way.

However, if you find this patch is meaningless, just ignore it.
",lepture,sigmavirus24
1861,2014-01-14 03:30:55,"@lepture like I said, it _should not_ break anything and I'm sure it will improve some user's experience with requests. The issue is that it is not backwards compatible because to keep the same behaviour a user has to pass extra parameters. The key part is that I **never** said this pull request was ""meaningless"". I think @Lukasa and I both agree that it is an improvement, but it is an improvement we cannot make until we start considering a 3.0.0 release which seems to me to be very far off.
",sigmavirus24,lepture
1861,2014-01-14 03:30:55,"@lepture like I said, it _should not_ break anything and I'm sure it will improve some user's experience with requests. The issue is that it is not backwards compatible because to keep the same behaviour a user has to pass extra parameters. The key part is that I **never** said this pull request was ""meaningless"". I think @Lukasa and I both agree that it is an improvement, but it is an improvement we cannot make until we start considering a 3.0.0 release which seems to me to be very far off.
",sigmavirus24,Lukasa
1861,2014-01-14 22:10:10,"@kennethreitz As accurate as Python's stdlib mimetype guessing. http://docs.python.org/2/library/mimetypes.html#mimetypes.guess_type
",shazow,kennethreitz
1861,2014-01-14 22:13:23,"@Lukasa Time for another Python core patch? :P
",shazow,Lukasa
1861,2014-01-15 09:46:13,"@Lukasa That's weird. I never knew it would be a disaster on Windows.
",lepture,Lukasa
1861,2014-01-15 10:01:07,"@lepture No worries, I'd have been amazed if you did know. I only happen to know because I bumped into this totally by accident [when working on urllib3](https://github.com/shazow/urllib3/issues/256#issuecomment-26619884).
",Lukasa,lepture
1861,2014-01-16 03:10:37,"I'm with @Lukasa until we can drop 2.6, let's leave this feature out. A point of inquiry though - hasn't 2.6 seen the last of it's bug/security releases? In other words, I think 2.6 has reached its end of life. We could start planning 3.0 to abandon 2.6 and introduce this feature.
",sigmavirus24,Lukasa
1861,2014-01-16 07:20:13,"@sigmavirus24 Is python 2.7 the default python on every linux distribution now? If so, I think 2.6 has reached its end of life.
",lepture,sigmavirus24
1860,2014-01-12 20:34:24,"That's a good point @Lukasa. I'm guessing @gazpachoking might have a good idea. I'm just too tired to think this hard right now =P
",sigmavirus24,Lukasa
1860,2014-01-23 23:28:02,"Sorry I never got around to that. @Lukasa how about you?
",sigmavirus24,Lukasa
1860,2014-01-30 12:45:39,"@Lukasa how should we make sure this doesn't mess anything up?
",sigmavirus24,Lukasa
1860,2015-12-16 15:32:50,"Ok, I'm reopening this because I've finally validated that the stdlib does this. That suggests that it's the right thing to do. I'll have to do the merge manually, but I'd like to have this.

@sigmavirus24, one question: could we meaningfully add this to a 2.9.1, or should it wait for 2.10.0?
",Lukasa,sigmavirus24
1859,2014-01-12 20:07:38,"Why the hell is the relevant function [here](http://docs.python.org/2/library/calendar.html#calendar.timegm)?



@sigmavirus24 Can you confirm that works OK for you too?
",Lukasa,sigmavirus24
1858,2014-01-12 14:26:45,"And the other place it's used is here: https://github.com/kennethreitz/requests/blob/ac4e05874a1a983ca126185a0e4d4e74915f792e/requests/models.py#L456 and notice that there's an optional param to that method that is never used if it is ever passed. We should either use it or remove it (not necessarily in this PR though).

Finally, given that the call to `HTTPAdapter#proxy_headers` is wrapped inside an `if proxy:` block, the param it sends to `get_auth_from_url` should never be `None` or `''`. And `prepare_auth` on the `PreparedRequest` is called after `prepare_url` which should blow up if `url` is not a valid `url`. I think we're safe making `get_auth_from_url` a bit less paranoid. As with all of my reviews though, this is totally up to the discretion of @kennethreitz and @Lukasa 
",sigmavirus24,Lukasa
1858,2014-01-12 14:43:30,"Thanks for that @sigmavirus24, that's a really helpful set of information! I'll update this PR to reflect it.
",Lukasa,sigmavirus24
1858,2014-01-13 12:42:17,"@Lukasa @sybeck2k found a problem with this test: https://github.com/kennethreitz/requests/pull/1862/files#diff-56c2d754173a4a158ce8f445834c8fe8R705 can you fix it here too?
",sigmavirus24,Lukasa
1858,2014-01-13 13:08:44,"@sigmavirus24 @Lukasa sorry I didn't notice the issue was tracked also here. I've tried to merge all the changes of Lukasa into my pull request - the main difference is about the test `test_get_auth_from_url_percent_chars` that on my opinion was flawed as the input url was not url-encoded.
",sybeck2k,Lukasa
1858,2014-01-13 13:08:44,"@sigmavirus24 @Lukasa sorry I didn't notice the issue was tracked also here. I've tried to merge all the changes of Lukasa into my pull request - the main difference is about the test `test_get_auth_from_url_percent_chars` that on my opinion was flawed as the input url was not url-encoded.
",sybeck2k,sigmavirus24
1857,2014-01-10 18:54:02,"Fine then. So we just need to work out how to do it. It's clear that @t-8ch has some plans to investigate nginx, so I'm open to doing that for now.
",Lukasa,t-8ch
1857,2014-01-10 18:57:43,"+1 on disabling TLS compression.

On Fri, Jan 10, 2014 at 10:54 AM, Cory Benfield notifications@github.comwrote:

> Fine then. So we just need to work out how to do it. It's clear that
> @t-8ch https://github.com/t-8ch has some plans to investigate nginx, so
> I'm open to doing that for now.
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/issues/1857#issuecomment-32054215
> .

## 

""I disapprove of what you say, but I will defend to the death your right to
say it."" -- Evelyn Beatrice Hall (summarizing Voltaire)
""The people's good is the highest law."" -- Cicero
GPG Key fingerprint: 125F 5C67 DFE9 4084
",alex,t-8ch
1857,2014-01-10 22:39:13,"@jmhodges This is the plan.
@alex I guess your are running it with openssl > 1.0.0. If yes, the last point of my initial list applies.
",t-8ch,alex
1857,2014-01-10 22:39:13,"@jmhodges This is the plan.
@alex I guess your are running it with openssl > 1.0.0. If yes, the last point of my initial list applies.
",t-8ch,jmhodges
1856,2014-01-11 09:43:26,"@Lukasa I think the unquoted `#` is not valid at this position:

[RFC 3986](https://tools.ietf.org/html/rfc3986)



While `#` is in the `gen-delims` group. Encoding it as `%23` makes both parse functions work for me.
",t-8ch,Lukasa
1856,2014-01-11 18:16:40,"@Lukasa Could make urllib3's parser more forgiving if that's what you really want, or just encode things properly. :)
",shazow,Lukasa
1856,2014-01-11 18:23:14,"@shazow something something if you change your parsers to accept bad URLs you're going to have a bad time something something. =P
",sigmavirus24,shazow
1856,2014-01-12 09:40:58,"Nah @shazow, you're fine. We'll always have encoded the URL at the point we call `get_auth_from_url`, so we should never have a hash that doesn't represent the fragment. All is well. =)
",Lukasa,shazow
1855,2014-01-10 13:40:16,"@Lukasa I agree wholeheartedly with everything you said. Your reservations (regarding data loss), however, make me wonder if we shouldn't be attaching the response object to the exceptions we throw. That's a different discussion which should take place on a different issue but I thought I might raise that idea before I forget it.

On the topic of this issue, I can understand that people may want this and that there are corner cases where the Content Length is not exactly a match for the response body (and in those cases it is okay). I can especially see the benefit where users use requests to perform tests on their servers and web apps. With that in mind, would something similar to `raise_for_status` be a reasonable compromise? We won't break backwards compatibility for existing users who do not realize their depending on our decision to not be strict and we give these extraordinary users (that I'm probably completely imagining) a way to reasonably achieve their goal.

One other alternative is to provide this functionality via the toolbelt. This way users have confidence in the implementation and they just need to import one extra thing.
",sigmavirus24,Lukasa
1853,2014-01-09 17:45:58,"Nginx disables SSL compression since 1.2.2 (07/2012).
Looking at the SSL client hello with wireshark requests announces support for `DEFLATE` compression. (openssl version 1.0.1f).

Given the fact, that SSL compression is a security risk (CRIME) we are trying to disable it altogether in urllib3, see the linked issue.
Unfortunately the standard ssl module on older pythons doesn't allow us to do this.

@chmouel @bugsduggan Wouldn't it be useful to disable SSL compression on the openstack-swift server-side as the performance impact would also hit other API clients besides the official one?
",t-8ch,chmouel
1853,2014-01-09 17:45:58,"Nginx disables SSL compression since 1.2.2 (07/2012).
Looking at the SSL client hello with wireshark requests announces support for `DEFLATE` compression. (openssl version 1.0.1f).

Given the fact, that SSL compression is a security risk (CRIME) we are trying to disable it altogether in urllib3, see the linked issue.
Unfortunately the standard ssl module on older pythons doesn't allow us to do this.

@chmouel @bugsduggan Wouldn't it be useful to disable SSL compression on the openstack-swift server-side as the performance impact would also hit other API clients besides the official one?
",t-8ch,bugsduggan
1853,2014-01-09 17:50:18,"@t-8ch Brilliant, if urllib3 disables it altogether that'll make me very happy, as we won't have to worry about the API. =) Give me a shout on that issue if you think I can help.
",Lukasa,t-8ch
1853,2014-01-09 19:25:47,"@t-8ch sorry my answer was a bit short, we could indeed disable SSL comprssion in swift (and openstack in general) but that's not how people usally runs it they would have usually things like (since this is how we advise to run it) pound/nginx doing that for us. 
",chmouel,t-8ch
1851,2014-01-08 20:40:36,"So is the general idea that I'd subclass `HTTPAdapter`, and do something approximately like this?



(And in response to @sigmavirus24 , it's a bit more complicated than just elapsed time, since the profiler is also profiling memory usage and things from `resource.getrusage()`.)
",eklitzke,sigmavirus24
1850,2014-01-07 23:14:19,"Thanks @t-8ch 
",sigmavirus24,t-8ch
1848,2014-02-17 18:33:03,"The primary use-case here is to avoid Kenneth getting spammed with bug reports in his inbox that basically go: ""This website doesn't work for me in Requests"". I'm open to this being a more general service like @shshank suggested, but right now our interest is in getting that crud off Kenneth's plate and onto mine and @sigmavirus24's. =)
",Lukasa,sigmavirus24
1848,2014-02-17 18:33:03,"The primary use-case here is to avoid Kenneth getting spammed with bug reports in his inbox that basically go: ""This website doesn't work for me in Requests"". I'm open to this being a more general service like @shshank suggested, but right now our interest is in getting that crud off Kenneth's plate and onto mine and @sigmavirus24's. =)
",Lukasa,shshank
1847,2014-01-07 20:03:09,"So, @Lukasa what's causing this? 
",kennethreitz,Lukasa
1847,2014-01-07 20:07:23,"@alex In principle I agree, but currently Requests just uses whatever the Python version has as the default in the `ssl` module.

@kennethreitz We _are_ autonegotiating (I'm pretty sure), but the remote server refuses the initial version we try.
",Lukasa,alex
1847,2014-01-07 20:07:23,"@alex In principle I agree, but currently Requests just uses whatever the Python version has as the default in the `ssl` module.

@kennethreitz We _are_ autonegotiating (I'm pretty sure), but the remote server refuses the initial version we try.
",Lukasa,kennethreitz
1847,2014-01-07 20:55:30,"We're really in between a rock and a hard place. Is our current solution of pointing people to [here](https://lukasa.co.uk/2013/01/Choosing_SSL_Version_In_Requests/) really unacceptable?

@kennethreitz I think it's harder than that, I just realised that my Python 3 is using a more recent OpenSSL:




",Lukasa,kennethreitz
1847,2015-07-17 22:49:29,"Hi @Lukasa ,

I am trying to use python requests to send a requests to a gunicorn server that accepts tls_v1 protocol which means that the client will use tls_v1 only. I create an adapter like:
class TLSAdapter(HTTPAdapter):
  def init_poolmanager(self, connections, maxsize, block=False):
    self.poolmanager = PoolManager(num_pools=connections,
                                   maxsize=maxsize,
                                   block=block,
                                   ssl_version=ssl.PROTOCOL_TLSv1)

And use it like:
requests_session = requests.Session()
requests_session.mount('https://', TLSAdapter())

Now when I try to send a request to my server like:
resp = requests_session.put(request.full_uri, data=request.data, headers=request_headers, verify=verify, cert=(cert, key))

Then I get an error like:
requests.exceptions.ConnectionError: ('Connection aborted.', BadStatusLine('',))

Could you please let me know if this issue has been seen before.
I am running server on my mac and sending the request from the same mac.

I am using python 2.6.9
",pankit,Lukasa
1846,2014-01-07 16:38:56,"@Lukasa I took a first pass at a patch in 0b41cec
",acdha,Lukasa
1844,2014-01-07 09:41:28,"Fix is in #1845. Feel free to take a look, @dstufft.
",Lukasa,dstufft
1844,2014-01-08 19:08:09,"Ok @dstufft, I think all the things you needed are merged (though I can't find the comment where you listed them).
",Lukasa,dstufft
1844,2014-01-09 13:08:04,"@Lukasa Looks good to me. I had the reporter of the bugs test it out and he was able to successfully install stuff without error. So once @kennethreitz cuts a new release (hopefully soon!) pip 1.5.1 can vendor that release and close out those bugs.
",dstufft,Lukasa
1842,2014-01-07 13:18:18,"@dstufft I'd greatly appreciate that. :)
",sigmavirus24,dstufft
1840,2014-01-05 02:49:41,"@dvasseur I updated your issue to use code blocks so that it's highlighted properly.

Can you let us know which version of requests you're using?
",sigmavirus24,dvasseur
1840,2014-01-05 13:21:42,"@sigmavirus24 thanks for editing, didn't know how to do it !

I have tested with 2.1.0 (arch version) and installed it again just to be sure

On my machine:


",dvasseur,sigmavirus24
1839,2014-03-21 09:32:42,"@jsullivanlive 
Has someone raised this issue with salesforce.com?
",piotr-dobrogost,jsullivanlive
1839,2014-03-21 14:25:38,"@sigmavirus24 and I have had the same experience, which is also the prevailing impression in #salesforce on freenode.  Oracle, Salesforce, and any other big companies have more concern over their slipping ship dates than their adherence to rfc and test coverage. This is why, after 20 years of development behind me, I touch data as little as possible. :)
",jsullivanlive,sigmavirus24
1837,2014-01-03 07:30:43,"Looks like @daftshady nailed this one down
",sigmavirus24,daftshady
1836,2014-01-03 19:43:05,"@zedxxx I'm following the convention of the project. requests has been around (much) longer than a year, yes, but for as long as I've been around it has never had the year range in the license.
",sigmavirus24,zedxxx
1835,2014-01-01 18:18:12,"@V-E-O the parameters passed to `get`, `post`, `put`, and other VERB methods are parameters that pertain to the request itself, not to the environment and not to the session. `trust_env` pertains to the environment and how the session considers the environment when performing requests. In short, no we will not add `trust_env` to the parameter list of any of those methods.
",sigmavirus24,V-E-O
1835,2014-01-02 17:18:57,"@sigmavirus24 I understand your point, also fine with disabling proxy by setting `trust_env`. Just feeling odd first time I tried to use requests.get(..., proxies=None/{}) to bypass env proxies.
Anyway, it's OK with us, thank you all for reply!
",V-E-O,sigmavirus24
1833,2013-12-28 12:28:19,"@Lukasa, it worked, thanks for your help.
",xjsender,Lukasa
1830,2013-12-27 09:39:03,"@NickGeek This is simply not true. _Sometimes_ Requests must be installed using `sudo`: if you install directly into the site packages and you don't have the correct permissions for them. But if you're using a virtualenv or a copy of Python where you have permission to the site packages, you do not need `sudo`.

For that reason, we assume that people who see errors out of `pip` that relate to lack of permissions will just elevate, running `sudo !!`. Better to leave the `sudo` out of the docs.

Thanks for this though!
",Lukasa,NickGeek
1829,2013-12-26 02:18:41,"@sigmavirus24 Ok, maybe my title confused you, now it is ok.
",douglarek,sigmavirus24
1825,2013-12-23 01:32:08,"This was fixed in version 2.1.0 of requests. 

I, like @Lukasa, see this work properly when tried interactively:


",sigmavirus24,Lukasa
1824,2013-12-20 15:18:27,"@ssbarnea That's interesting, you seem to have contradictory views about these two issues. In the first case, we're continuing in the face of errors, which you deem to be a bug. In the second case, Python aborts in the face of errors, which you deem to be a bug.

Furthermore, if you fixed the second issue (Python does not fail loading netrc if parsing one line fails), Requests will never silently bypass errors in parsing `.netrc` because it will never find out about those errors!

I'm +0 on the idea of no longer catching the `Netrc` exception. We could do it. Some people will find unexpected failures because they've never noticed that they even had anything in `.netrc`, but they'll be able to fix that in time. However, we certainly can't do it until at least 2.2, because we cannot start throwing exceptions where previously we worked just fine.
",Lukasa,ssbarnea
1822,2013-12-20 05:42:41,"@sigmavirus24 , 

I used requests in sublime plugin, if the soap_body in below statement didn't contains any Chinese characters, there will be no exception.

`response = requests.post(self.apex_url, soap_body, verify=False, headers=headers)`
",xjsender,sigmavirus24
1819,2013-12-20 02:40:07,"@ogrishman, @Lukasa explained on #1820 that this is a bug tracker and not a question and answer forum. Further questions you ask here will be closed without answer.
",sigmavirus24,Lukasa
1819,2013-12-20 02:40:07,"@ogrishman, @Lukasa explained on #1820 that this is a bug tracker and not a question and answer forum. Further questions you ask here will be closed without answer.
",sigmavirus24,ogrishman
1816,2013-12-19 09:09:23,"LGTM! Thanks so much @daftshady! :cake:
",Lukasa,daftshady
1814,2013-12-19 03:40:45,"Hey @tg123 this PR belongs on [shazow/urllib3](/shazow/urllib3) as was noted in the [README in the packages directory](https://github.com/tg123/requests/tree/fa34ab816aa03a009ceb86db7e55f16ca6687ba4/requests/packages). Please be more careful next time.
",sigmavirus24,tg123
1813,2013-12-18 22:15:35,"@orokusaki In principle there's no reason. I suggest raising the issue on urllib3 and seeing what @shazow thinks. =)
",Lukasa,orokusaki
1813,2013-12-18 22:29:27,"@shazow - interesting... is there currently a way to manually use a `RequestField` while using `request.post(...)`? If not, that would surely be something worth my contributing when I get a moment.

CC @Lukasa 
",orokusaki,shazow
1813,2013-12-18 22:29:27,"@shazow - interesting... is there currently a way to manually use a `RequestField` while using `request.post(...)`? If not, that would surely be something worth my contributing when I get a moment.

CC @Lukasa 
",orokusaki,Lukasa
1813,2013-12-18 22:31:10,"@orokusaki Yea, you should be able to pass a list of `RequestField` objects same way you pass a list of tuples. They should behave interchangeably. If that's not the case, please open a bug. :)

Might be worth updating the docs accordingly.
",shazow,orokusaki
1813,2013-12-18 22:37:48,"@shazow No docs changes on our side. =) There's no plan to officially document the crazy things you can do with multipart encoding because they make us sad. There's plans afoot to move some multipart stuff outside the main library, which will likely become the preferred way of doing anything non-trivial with multipart encoding.
",Lukasa,shazow
1813,2013-12-18 22:48:50,"@shazow - sorry to pester, but I'm not sure I follow. I normally use `post` like this:



Where would manually creating a `RequestField` fit into that equation, simply replacing the list of `files` with a list of `RequestField` instances? If so, it would seem that the header would still be overwritten per https://github.com/kennethreitz/requests/blob/d88cd02fd86c6e74ef8a2d1928db78b8976ce00f/requests/models.py#L141 - also would result in a likely error due to the lines preceding that line, right?

I might simply be confused as to what you mean though.

CC @Lukasa 
",orokusaki,shazow
1813,2013-12-18 22:48:50,"@shazow - sorry to pester, but I'm not sure I follow. I normally use `post` like this:



Where would manually creating a `RequestField` fit into that equation, simply replacing the list of `files` with a list of `RequestField` instances? If so, it would seem that the header would still be overwritten per https://github.com/kennethreitz/requests/blob/d88cd02fd86c6e74ef8a2d1928db78b8976ce00f/requests/models.py#L141 - also would result in a likely error due to the lines preceding that line, right?

I might simply be confused as to what you mean though.

CC @Lukasa 
",orokusaki,Lukasa
1813,2013-12-18 22:56:48,"Ah, so it sounds like maybe a `requests==3.0.0` type of feature, if it were to be added. Thanks for checking on that for me @shazow.

@Lukasa @kennethreitz If more things were delegated to urllib3, it would simplify the requests codebase even more. If either of you have suggestions pertaining a change like this, I'd be more than happy to take a stab at it.
",orokusaki,shazow
1813,2013-12-18 22:56:48,"Ah, so it sounds like maybe a `requests==3.0.0` type of feature, if it were to be added. Thanks for checking on that for me @shazow.

@Lukasa @kennethreitz If more things were delegated to urllib3, it would simplify the requests codebase even more. If either of you have suggestions pertaining a change like this, I'd be more than happy to take a stab at it.
",orokusaki,Lukasa
1813,2013-12-19 06:27:47,"@orokusaki That's true, but not relevant. In our list of priorities, the simplicity of the API is vastly higher than the simplicity of the codebase. We're already not hugely happy with the interface here, and using `RequestField` classes is not going to make us happier.

As mentioned above, we're more likely to suggest that anyone who needs to do something interesting with multipart encoding use a third-party package that interoperates with Requests. Right now such a package doesn't exist, but there are plans to work on one.
",Lukasa,orokusaki
1812,2013-12-18 16:03:36,"@kennethreitz Any write-ups/blogs you can point to on the reasons?
",semarj,kennethreitz
1812,2014-03-19 19:38:26,"@westurner 
You compiled a nice set of questions there :) Statements from https://github.com/kennethreitz/requests/pull/1812#issuecomment-30871660 neither answer nor invalidate any of these questions to be clear. However that's in theory. In practice, most of the time, bundling just works and is better than all existing alternatives. Sharing dependencies, while sound in theory, is very difficult to do right and that's why people chose alternatives which in practice just work better. I have a strong feeling that sharing dependencies, which brings the need to manage them, once a must due to scarce computer resources (I suppose), nowadays creates more problems than it solves.
I also do not understand why packagers waste their time unbundling bundled packages. Do they also reverse monkey patching :) ? I mean if some piece of software chooses to bundle something then it does it for a reason. It's not like authors just don't know any better.
Returning to your questions; while there are no policies set, from which one could arrive at clear answers you expected, general answer is that contributors make whatever they are able and willing to do to make complaining users (more) happy. You know, because in life, people need working software more than official policies.
",piotr-dobrogost,westurner
1812,2014-03-20 05:04:18,"How many outdated bundled/embedded copies of OpenSSL could there be? Are
there standard mechanisms for ensuring that security updates reach end
users?

Here's this: https://news.ycombinator.com/item?id=7385150

Wes Turner
On Mar 19, 2014 2:38 PM, ""Piotr Dobrogost"" notifications@github.com wrote:

> @westurner https://github.com/westurner
> You compiled a nice set of questions there :) Statements from #1812
> (comment)https://github.com/kennethreitz/requests/pull/1812#issuecomment-30871660neither answer nor invalidate any of these questions to be clear. However
> that's in theory. In practice, most of the time, bundling just works and is
> better than all existing alternatives. Sharing dependencies, while sound in
> theory, is very difficult to do right and that's why people chose
> alternatives which in practice just work better. I have a strong feeling
> that sharing dependencies, which brings the need to manage them, once a
> must due to scarce computer resources (I suppose), nowadays creates more
> problems than it solves.
> I also do not understand why packagers waste their time unbundling bundled
> packages. Do they also reverse monkey patching :) ? I mean if some piece of
> software chooses to bundle something then it does it for a reason. It's not
> like authors just don't know any better.
> Returning to your questions; while there are no policies set, from which
> one could arrive at clear answers you expected, general answer is that
> contributors make whatever they are able and willing to do to make
> complaining users (more) happy. You know, because in life, people need
> working software more than official policies.
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/pull/1812#issuecomment-38097168
> .
",westurner,westurner
1812,2014-03-20 11:46:51,"@westurner while I appreciate your concern, your efforts are for naught. Allow me to relay to you what I recently learned, security is not the foremost concern of requests (not @Lukasa's or my opinion). Its API is the first concern. Trying to support your point by making valid points about security will do nothing. Since little is likely to change, I'm unsubscribing from this issue and would encourage others who value their time to do the same.
",sigmavirus24,westurner
1812,2014-03-20 11:46:51,"@westurner while I appreciate your concern, your efforts are for naught. Allow me to relay to you what I recently learned, security is not the foremost concern of requests (not @Lukasa's or my opinion). Its API is the first concern. Trying to support your point by making valid points about security will do nothing. Since little is likely to change, I'm unsubscribing from this issue and would encourage others who value their time to do the same.
",sigmavirus24,Lukasa
1812,2014-03-31 19:06:14,"> In practice, most of the time, bundling just works and is better than all existing alternatives.

Having said that I agree with @abadger and I see making is easy to use non-bundled dependencies as something maintainers of every software should do.
",piotr-dobrogost,abadger
1812,2014-09-17 19:40:25,"@kennethreitz totally agree with the decisions made here. If anyone has ever tried to ship Python software to be standalone you'll know why it's done like this. 
",whalesalad,kennethreitz
1812,2014-09-18 20:01:52,"@whalesalad exxxactly :)
",kennethreitz,whalesalad
1811,2013-12-18 15:46:43,"@sigmavirus24 The big problem is that distros remove the bundled version anyways, making conflicts when you want to handle exceptions / use urllib3 directly, since requests is using a different version than everything else.
",sontek,sigmavirus24
1808,2013-12-18 07:44:53,"Looks good too! Thanks for fixing it so quickly @sigmavirus24 
",teleyinex,sigmavirus24
1804,2013-12-16 13:12:58,"Thanks @Lukasa - quick and detailed follow up. I'll keep an eye on the httplib bug. 
",ChristopherHackett,Lukasa
1804,2013-12-16 14:02:33,"@Lukasa how can I get some beer to you? Seriously, that was incredibly fast and awesome.
",sigmavirus24,Lukasa
1804,2013-12-16 14:35:05,"Thanks guys. This was actually quite a fun bug to find. =D

@ChristopherHackett I forgot to mention, if you want to be able to work around this, it seems to be affecting only the `gzip` delivery of the page. It'll work fine if you unset the `Accept-Encoding` header we send, like this:


",Lukasa,ChristopherHackett
1803,2013-12-16 14:10:21,"Once again @Lukasa is 100% correct. This isn't an issue or a bug in requests, just in your expectations of how it behaves.
",sigmavirus24,Lukasa
1803,2013-12-18 22:09:10,"@antoineleclair You could use multithreading, or the gevent library.
",Anorov,antoineleclair
1803,2013-12-19 08:24:10,"So, I chatted briefly via email with @sigmavirus24 about this. Actually changing the library so that timeouts apply to streaming data is a trivial change with a negative diff. The problem is that it's a backwards incompatible change.

So I'm in favour of fixing this, but not right now. Maybe as part of 2.2. I'm going to schedule this issue for that release unless the BDFL or @sigmavirus24 complains.
",Lukasa,sigmavirus24
1803,2014-03-20 18:36:14,"@andynemzek We're not on any release schedule, so it'll be released when we next release requests. =)
",Lukasa,andynemzek
1803,2017-02-07 21:31:01,"@julienaubert Sorry, can you provide more explanation about what is going on here? The read timeout should still apply.",Lukasa,julienaubert
1803,2017-02-08 00:56:17,"@Lukasa afaic, the test that was meant to test that timeout works when stream=True, is not correct, as it is not setting stream=True and the default value is stream=False. See: https://github.com/kennethreitz/requests/blob/master/tests/test_requests.py#L1943",julienaubert,Lukasa
1803,2017-02-08 01:14:13,"@julienaubert The reason there isn't an explicit `stream` argument is due to the patch that fixed this (and added this test).  #1935 removed the use of a different timeout object in `send` depending on the value of `stream`. That means Requests now *always* hands the read timeout so the `stream` flag isn't relevant in this regard.

Are you experiencing issues with a current version of Requests?",nateprewitt,julienaubert
1802,2013-12-16 08:51:58,"Thanks for the pull request @kevinburke!

Unfortunately, I'm -1 on this feature. My problems with this feature are as follows:
1. It's misleading. You say in your original description that ""the simplest debugging information I want is the actual HTTP content that was sent over the wire"". This is almost certainly true, and is exactly what any user of this feature would expect it to give. Unfortunately, that's not what this feature provides. Instead, it provides Requests' view of the response, which is not the same thing. Mostly this is stuff that's probably trivial: all the header keys are lower case, their order isn't preserved, etc. However, the fact of the matter is that we're not showing the actual response, we're reconstructing something that matches our view of the response. This means that there's nothing in the textual output you've provided that isn't in the `Response` object already. Any unusual stuff in the HTTP response won't be shown.
   
   This gets even worse if you consider extending this to serialize HTTP requests (which is almost certainly the next step here, being infinitely more useful in debugging weird failures), since Requests cannot possibly reconstruct the HTTP request: we just don't have all the information.
2. It extends the API. We're fighting very hard to avoid extending the API if at all possible: as a result, any extension to the API automatically starts in negative territory, and must provide a very strong justification. =)

Sorry about this: thanks for doing the work, but I don't think we'll merge it. My advice to people who need this debugging functionality is either to get familiar with Wireshark or to consider using a service like [Runscope](https://www.runscope.com/).
",Lukasa,kevinburke
1802,2013-12-16 14:08:23,"I agree with 100% of what @Lukasa said. In fact, even curl would give you far more information to debug with. _Furthermore_, there are headers that we compute that we don't even set on the `PreparedRequest` object so adding this method to that would be useless too quite frankly. Unfortunately your best bet is something like `mitmproxy` or `wireshark` (locally) or a service like `Runscope` or `httpbin`.

With that said, the both of us are in agreement and I'm certain @kennethreitz will agree with both of us, so I'm going to close this.
",sigmavirus24,Lukasa
1801,2013-12-16 08:56:58,"In principle I'm +0.5 on this. It's a nice extension. However, as memory serves, @kennethreitz considered this approach and ended up not taking it. It might be that now that someone has written the code he'll be happy with it, but equally, it might be that he doesn't like the approach.

@sigmavirus24 Yeah, `TimeoutSauce` is used for the urllib3 `Timeout` object, because we have our own `Timeout` object (an exception).
",Lukasa,sigmavirus24
1801,2013-12-16 14:05:32,"@Lukasa As I understood it @kennethreitz was more concerned with the addition (and requirement) of the `Timeout` class from urllib3. And thanks for clearing up the naming, I still think there has to be a better name for it. (I'm shaving a yak, I know)
",sigmavirus24,Lukasa
1801,2013-12-16 14:49:50,"@sigmavirus24 That was my understanding from IRC as well.
",kevinburke,sigmavirus24
1801,2013-12-16 16:08:14,"@kevinburke I discussed it with you on IRC so the likelihood is that you came to that conclusion through me.
",sigmavirus24,kevinburke
1801,2014-01-06 21:04:58,"Hmm. I'd rather not do a `Timeout` class, I'd prefer the optional tuple I think. But hold off until @kennethreitz gets another chance to look at this.
",Lukasa,kennethreitz
1801,2014-04-05 14:40:08,"That's weird, it works fine on Python 2.7. Seems like a Python 3 bug, because I can reproduce your problem in 3.4. @kevinburke, are you aware of any timeout bugs in urllib3?
",Lukasa,kevinburke
1801,2014-05-10 08:11:55,"@kirelagin I see the same behaviour as you, but I believe this to be a socket-level problem. Dumping Wireshark shows that my OS X box makes five independent attempts to connect. Each of _those_ connection attempts only retransmits for five seconds.

I suspect this behaviour comes down to the fact that `httplib` uses `socket.create_connection`, not `socket.connect()`. Python's socket module documentation [has this to say](https://docs.python.org/2/library/socket.html#socket.create_connection) (emphasis mine):

> This is a higher-level function than socket.connect(): if host is a non-numeric hostname, it will try to resolve it for both AF_INET and AF_INET6, and _then try to connect to all possible addresses in turn_ until a connection succeeds.

Closer examination of Wireshark trace shows that we are definitely hitting different addresses: five of them.

If we wanted to 'fix' this behaviour (more on those scare quotes in a minute) it would be incredibly complicated: we'd end up needing to either circumvent `httplib`'s connection logic or use signals or some form of interrupt-processing to return control to us after a given timeout.

More generally, I don't know what 'fix' would mean here. This behaviour is naively surprising (you said we'd time out after 5 seconds but it took 30!), but makes sense once you understand what's happening. I believe that this is an 'expert friendly' interface (thanks to Nick Coghlan for the term), so I'm prepared to believe that we should change it. With that said, if we do change it, how do we give the 'I want to wait 5 seconds for each possible target' behaviour to expert users?
",Lukasa,kirelagin
1801,2014-05-10 08:30:19,"There's a retries branch of urllib3 that would let you specify exactly this.

On Saturday, May 10, 2014, Cory Benfield notifications@github.com wrote:

> @kirelagin https://github.com/kirelagin I see the same behaviour as
> you, but I believe this to be a socket-level problem. Dumping Wireshark
> shows that my OS X box makes five independent attempts to connect. Each of
> _those_ connection attempts only retransmits for five seconds.
> 
> I suspect this behaviour comes down to the fact that httplib uses
> socket.create_connection, not socket.connect(). Python's socket module
> documentation has this to sayhttps://docs.python.org/2/library/socket.html#socket.create_connection(emphasis mine):
> 
> This is a higher-level function than socket.connect(): if host is a
> non-numeric hostname, it will try to resolve it for both AF_INET and
> AF_INET6, and _then try to connect to all possible addresses in turn_until a connection succeeds.
> 
> Closer examination of Wireshark trace shows that we are definitely hitting
> different addresses: five of them.
> 
> If we wanted to 'fix' this behaviour (more on those scare quotes in a
> minute) it would be incredibly complicated: we'd end up needing to either
> circumvent httplib's connection logic or use signals or some form of
> interrupt-processing to return control to us after a given timeout.
> 
> More generally, I don't know what 'fix' would mean here. This behaviour is
> naively surprising (you said we'd time out after 5 seconds but it took
> 30!), but makes sense once you understand what's happening. I believe that
> this is an 'expert friendly' interface (thanks to Nick Coghlan for the
> term), so I'm prepared to believe that we should change it. With that said,
> if we do change it, how do we give the 'I want to wait 5 seconds for each
> possible target' behaviour to expert users?
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/pull/1801#issuecomment-42735878
> .

## 

## 

Kevin Burke | Twilio
phone: 925.271.7005 | kev.inburke.com
",kevinburke,kirelagin
1801,2014-05-10 15:55:38,"@p2 Sadly computing the wall clock time for an HTTP request remains incredibly difficult. Mostly, our tools for determining the amount of time used for system calls like DNS resolution, establishing a socket, and sending data (and then passing this information to the necessary places, and subtracting these from a total amount of time) remains difficult.

My suggestion would be to run your HTTP request in a separate thread, then use a timer to cancel the thread if it takes longer than a stated amount of time to return a value. gevent can be used for this: http://www.gevent.org/gevent.html#timeouts

Sorry I can't be more helpful :(
",kevinburke,p2
1801,2014-05-10 16:05:57,"@kevinburke That is kind of what I do now, I was just wondering if this would make sense as a default approach for _requests_ as well. I personally don't have a need to specify individual timeouts, but that assumption may be too naïve.
",p2,kevinburke
1801,2014-05-10 16:06:54,"@p2 I agree that “human” interpretation is to have a total timeout. And, by the way, that's how `requests` works right now. But there, _still_ might be possibilities when you want a more fine-grained control.
",kirelagin,p2
1801,2014-05-11 08:05:32,"Ok, a lot of conversation happened here, let me deal with it in turn.

> Anyway, I just checked Firefox and it is trying exactly one IPv6 and exactly one IPv4 address. I believe, that multiple DNS records are mostly used for load balancing, not fault-tolerance, so attempting only the first address by default makes most sense.

You checked Firefox against actual google.com then, not on an incorrect port. [Browsers will also fall back to the other addresses if the first doesn't respond](http://webmasters.stackexchange.com/a/12704). This makes sense. Having multiple A records means ""this host is available at these addresses"". If I can't contact that host at one of those addresses, it's nonsensical to say ""welp, clearly the host is down"" when I know several other addresses I might be able to contact them at.

This feature of 'multiple addresses' is widely used for both balancing load _and_ fault tolerance. In fact, if you _really_ want to balance load then DNS SRV is the mechanism to use, not A/AAAA, as it provides better control over how the load is spread.

> Is there need to be able to specify both timeouts independently? When I specify the timeout, I'm thinking of it as ""I don't want this line of code to run longer than x seconds"", I don't care which part of the connection takes how long.

The short answer is 'yes', because of the `stream` keyword argument. If you've set `stream=True` and use `iter_content()` or `iter_lines()`, it's useful to be able to set a timeout for how long those calls can block.

> It seems to me this would be a true ""human"" interpretation and could be implemented without having to rely on urllib3 by an internal timer that kills the request if it hasn't returned within the timeout.

As @kevinburke points out, this isn't as easy as it seems. More importantly, it also leaves us exposed to implementation details. 'Until the request returns' is not a well-defined notion. What does it mean for the request to return? Do I have to download the whole body? Just the headers? Just the status line? Whatever we choose is going to be utterly arbitrary.

> Also, as a human when I'm telling a line of code “go, try to connect to Google with this timeout” I'm not thinking about multiple DNS A-records. I'm thinking of Google as a single entity. 

Agreed.

> 1. Do not attempt multiple IPs. If some library code does this, I consider this code broken. If some OS does this, I consider the OS poor.

Woah, now you go off the rails. If you are thinking of Google as a single entity, then you would expect us to connect to it if it's up. If one time in seven we fail to connect, even though you always connect fine in your browser, you're going to assume requests is bugged as hell.

If a host is up, we must be able to connect you to it.

---

The ideal fix, from my position, would be to take over the logic used in `socket.create_connection()`. This allows us to have fine-grained control over timeouts. Unfortunately, it also complicates the `timeout` logic further, as you'd now have per-host connection attempt timeouts, total connection attempt timeout, and read timeout. That's beginning to become hugely complicated and to expose people to the complexities of TCP and IP in a way that I'm not delighted about.
",Lukasa,kevinburke
1801,2014-08-22 13:06:36,"@kevinburke can you rebase?
",kennethreitz,kevinburke
1799,2013-12-14 20:28:46,"I'm moreso in the half-open interval [-1, -0) on this if only because the warning will likely only be displayed once per detected encoding and not once per response if I remember the way they work correctly.

An example is like so:



Will only produce 3 warnings and will not achieve what you want @martinblech 
",sigmavirus24,martinblech
1799,2013-12-14 20:49:28,"You're right @sigmavirus24, that's not what I want. I didn't know warnings worked like that, thanks for teaching me something new!

How about doing `log.warning()` instead of `warnings.warn()`? Here's how it would look on the REPL:


",martinblech,sigmavirus24
1798,2013-12-14 18:44:04,"Everything @Lukasa said has been correct and as far as I'm concerned this is not a valid issue. Your concerns are duly noted and appreciated. Don't hesitate to open further issues if you think you've found other bugs.
",sigmavirus24,Lukasa
1798,2013-12-15 17:54:30,"@Lukasa @sigmavirus24 the method `prepare_url` from  `PreparedRequest` [already checks](https://github.com/kennethreitz/requests/blob/master/requests/models.py#L375) if the given url contains previous params. I don't understand why `PreparedRequest` shouldn't have a params property as they are serializable into a HTTP req when the method `prepare` is called.
",juanriaza,Lukasa
1798,2013-12-15 17:54:30,"@Lukasa @sigmavirus24 the method `prepare_url` from  `PreparedRequest` [already checks](https://github.com/kennethreitz/requests/blob/master/requests/models.py#L375) if the given url contains previous params. I don't understand why `PreparedRequest` shouldn't have a params property as they are serializable into a HTTP req when the method `prepare` is called.
",juanriaza,sigmavirus24
1797,2013-12-14 16:05:48,"As @daftshady points out, please check the issues tracker before opening a new issue.
",Lukasa,daftshady
1797,2013-12-14 16:55:16,"Thanks for opening this @joepie91 and thank you @daftshady for taking the time to find the already opened issue.
",sigmavirus24,joepie91
1797,2013-12-14 16:55:16,"Thanks for opening this @joepie91 and thank you @daftshady for taking the time to find the already opened issue.
",sigmavirus24,daftshady
1796,2013-12-14 09:41:16,"Thanks for this @martinblech!

Unfortunately, I'm opposed to this change. Not anything to do with the code itself, but I don't think adding this parameter to the `Response` object adds sufficient value to justify including it. If people _really_ want this information they can get it (by running `charade`/`chardet` themselves), and for a significant majority of people this information is simply unnecessary. Requests got where it was today by hiding complexity from users, and I'm in favour of that trend continuing. @sigmavirus24?
",Lukasa,martinblech
1796,2013-12-14 19:14:34,"@Lukasa I understand and I love Requests' simplicity. Still, I think it's not safe for Requests to rely so heavily on charade as default and hide it completely from the user.

Here's some examples of what happens to common German, Spanish, French and Portuguese words after being ""charadized"":


",martinblech,Lukasa
1795,2013-12-13 23:56:02,"@Lukasa Absolutely agree: I wouldn't expect Requests to parse the XML declaration to read the encoding, that completely out of scope for a HTTP library.

I wonder, though, what made Requests autodetect a Thai encoding for an XML document with some Spanish content. It's latin alphabet after all.

If the autodetection feature is not reliable enough, maybe the documentation should discourage the users from using `response.text` at all and use `response.content` instead. Or made an opt-in feature instead of default?
",martinblech,Lukasa
1795,2013-12-15 07:44:19,"@martinblech Point 2 isn't correct. If `response.encoding` is `None`, an automatic charset is used.

The biggest problem here is that Requests can only do so much to protect users. Text encoding is a horrible minefield of pain, and `charade`/`chardet` usually helps us solve _some_ of that pain. However, users who are in a position to know better should take a more active role in choosing text encodings. For instance, on HTML/XML pages, people should look for an `encoding` directive in the content.
",Lukasa,martinblech
1795,2013-12-15 15:09:24,"@Lukasa Got it. It wasn't obvious to me but it's [right there](http://requests.readthedocs.org/en/latest/api/?highlight=text#requests.Response.text) in the docs: ""If Response.encoding is None, encoding will be guessed using charade."" My bad.

My point is, I'm not sure if falling back to `charade`/`chardet` solves more pain than it creates. It didn't for me, but maybe I just hit an unusual number of edge cases that most users of Requests won't. It's hard to estimate how well my experience extrapolates to the average Requests user.

I would love to see a shorter version of this part of your answer in the docs for `Response.text`, just in case:

> […] users who are in a position to know better should take a more active role in choosing text encodings. For instance, on HTML/XML pages, people should look for an encoding directive in the content.
",martinblech,Lukasa
1794,2013-12-13 12:32:44,"My fix initializes those attributes ahead of time in the event that they ever get added to the `__attrs__` list. It seems the most fool-proof way at the moment and far less complex than first checking if it has an attribute. (I don't quite like `hasattr` or how frequently we use it.)

The only issue with both of our pull requests is that the original issue could still come back (sort of) if the for-loop ever initializes `proxy_manager` to `None`. I'm going to pull @erikcw's commit into my branch and work from there.
",sigmavirus24,erikcw
1794,2013-12-13 20:01:56,"In retrospect, I tend to agree with @sigmavirus24 approach of initializing the attributes instead of using `hasattr` in this case.  Since it's before the loop, there are no concerns about clobbering pickled values with an empty dictionary -- while future-proofing the ability to move it into `__attrs__`.

My first approach to solving this was to simply add `proxy_manager` to `__attrs__` -- but that won't work because of the lambdas in `proxy_manager`.  My recommendation would be to initialize the attributes before the loop as suggested by @sigmavirus24 , and to incorporate my code comments to save future contributors from stumbling down the `__attrs__` approach.
",erikcw,sigmavirus24
1794,2013-12-14 04:29:58,"I cherry-picked your commits into my PR. Thanks for your work here @erikcw ! Keep the PRs coming!
",sigmavirus24,erikcw
1790,2013-12-12 06:57:48,"Hey, thanks for thi @kracekumar!

I don't think this is a good idea, however. The key reason is that auto-converting outgoing data is upsettingly magic. As much as possible, what you put in one parameter to the request function (e.g. `headers`) should not affect what happens to something that came in on another parameter (e.g. `data`, or `auth`).

It's also not quite the same as what we do on `Response`s. For a `Response`, we don't automatically convert to JSON unless explicitly asked to by the user: that is, they have to actually call `Response.json()`. That fits the Zen of Python (""Explicit is better than implicit""). For that reason as well, I'd rather that we stick to the current behaviour.

Let's leave this open to see if @sigmavirus24 agrees.
",Lukasa,kracekumar
1790,2013-12-12 12:32:24,"This has been proposed countless times before and every time it has been shot down. We analyze your request to make sure that it appears as correct as we can possibly ensure without restricting what you can actually send.

As @Lukasa said, headers have currently no effect on any other parameter we use and they shouldn't have any effect here. `data`+`files` has an effect because it's the simplest API to making a `multipart/form-data` request especially since sending a file is safest when using that multipart type. Beyond the fact that this is a bad design for an API (it is entirely unintuitive), we would then have people insisting that because we accepted this then we should also accept other `Content-Type` headers and parse the other parameters based upon that. For example, someone might insist that we accept a header like `multipart/form-data; boundary=xyz` and dictionaries for `data` and `files` and then parse out the boundary and create a multipart/form-data` request. That is insanity.

Someone else might want us to turn his dictionaries into XML for him... 

You can see exactly where this rabbit hole goes, just like https://github.com/kennethreitz/requests/pull/1779 led to https://github.com/kennethreitz/requests/pull/1785

Finally, you seem to already understand it, but let me stress:



Will raise an exception as well it should. In both your example, and this one though, calling `r.content` **or** `r.text` will get you the body of the response.

I'm :-1: \* 10 on this (regardless of how convenient it would make developing things like [github3.py](https://github.com/sigmavirus24/github3.py)).
",sigmavirus24,Lukasa
1790,2013-12-12 13:18:18,"Alright then, let's close this. Thanks for the suggestion @kracekumar and please keep them coming! We always say 'no' to more things than we say 'yes' to, but we can't say 'yes' to things people never ask us to do. You're doing good work! Keep it up.

:cake:
",Lukasa,kracekumar
1789,2013-12-11 16:30:35,"Yeah, @t-8ch seems to have the right analysis. In CPython, it takes 29milliseconds per call to `requests.get()`, of which 22ms is `getaddrinfo()`. In PyPy, it takes 47 ms per call to `requests.get()`, of which 35ms is `getaddrinfo()` That accounts for 13 ms of the 18ms difference. I imagine the remaining portion of that time difference is probably because the profiler doesn't play well with PyPy (I seem to recall that being a problem, though @alex will surely be able to correct me if it isn't).
",Lukasa,t-8ch
1789,2013-12-13 18:03:17,"Hmm, I wanted to test this. Running on Windows, I get garbage output when profiling on PyPy, but I wrote a quick test of `getaddrinfo()`:



Output:



This _is_ slower, but only marginally. I don't think this is PyPy's fault, unless @handloomweaver can demonstrate the problem on their hardware/network.
",Lukasa,handloomweaver
1788,2013-12-11 12:35:52,"@anantasty next time, please check the README in the `requests/packages` directory which GitHub kindly displays when you visit [that directory](https://github.com/kennethreitz/requests/tree/master/requests/packages).
",sigmavirus24,anantasty
1787,2013-12-18 17:23:54,"@ThiefMaster , Hi, can you still reproduce this using the requests package compiled from the sources on here?
I burrowed down into the urllib3 and it seems i can't reproduce this issue.
Can you provide me with the server code or give me some hints of what it is or description something to help me diagnose the problem better?
Thanks.
",adaschevici,ThiefMaster
1787,2013-12-19 02:59:20,"@adaschevici I've been experiencing this bug sporadically too. It's been occurring under similar circumstances to @ThiefMaster. I'm just running a script which makes a bunch of GET requests on 3rd party servers. About 1 of every 100 requests which time out throw the bad exception.

I've been attempting to reproduce it consistently, but haven't had any luck.

edit: This seems to be the exact same issue as: kennethreitz/requests/issues/1236. I'm also using 2.1.0
",adamsc,adaschevici
1787,2013-12-19 02:59:20,"@adaschevici I've been experiencing this bug sporadically too. It's been occurring under similar circumstances to @ThiefMaster. I'm just running a script which makes a bunch of GET requests on 3rd party servers. About 1 of every 100 requests which time out throw the bad exception.

I've been attempting to reproduce it consistently, but haven't had any luck.

edit: This seems to be the exact same issue as: kennethreitz/requests/issues/1236. I'm also using 2.1.0
",adamsc,ThiefMaster
1787,2013-12-19 05:57:52,"@ThiefMaster , Thanks. I will try and reproduce this. Regarding the description provided it seems that it will occur on server overload.
When i tried to overload it in ubuntu what i got was a pipe error. I've been trying it on windows but unfortunately the tests are failing before this one.
We might be looking into a platform dependent test?
Or maybe a patch test to provide the coverage?

CrossRef: shazow/urllib3#297
",adaschevici,ThiefMaster
1787,2013-12-19 08:48:51,"@shazow Pfft, as a socket-level test that's easy =D. I'll take a swing at it at somepoint today if someone doesn't beat me to it.
",Lukasa,shazow
1787,2013-12-19 08:57:35,"@Lukasa , where do you mean to catch the exception
In httpresponse.read(), yes?
I have been working with it and trying to get the exception thrown from there but i've had no successful attempts so far.

Anyway...i will retrace my steps and see where i went wrong maybe i will come up with the proper test.
",adaschevici,Lukasa
1787,2014-03-20 18:05:40,"@darshahlu To be clear, are you actually getting a `socket.timeout` exception, or one that has been wrapped? Can I see the full exception text?
",Lukasa,darshahlu
1787,2014-03-20 18:21:58,"@Lukasa Sure, here is the full exception (starting at requests code):

  File ""C:\dev\virtualenvs\py27mft\lib\site-packages\requests\sessions.py"", line 395, in get
    return self.request('GET', url, *_kwargs)
  File ""C:\dev\virtualenvs\py27mft\lib\site-packages\requests\sessions.py"", line 383, in request
    resp = self.send(prep, *_send_kwargs)
  File ""C:\dev\virtualenvs\py27mft\lib\site-packages\requests\sessions.py"", line 486, in send
    r = adapter.send(request, **kwargs)
  File ""C:\dev\virtualenvs\py27mft\lib\site-packages\requests\adapters.py"", line 394, in send
    r.content
  File ""C:\dev\virtualenvs\py27mft\lib\site-packages\requests\models.py"", line 679, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File ""C:\dev\virtualenvs\py27mft\lib\site-packages\requests\models.py"", line 616, in generate
    decode_content=True):
  File ""C:\dev\virtualenvs\py27mft\lib\site-packages\requests\packages\urllib3\response.py"", line 236, in stream
    data = self.read(amt=amt, decode_content=decode_content)
  File ""C:\dev\virtualenvs\py27mft\lib\site-packages\requests\packages\urllib3\response.py"", line 183, in read
    data = self._fp.read(amt)
  File ""C:\Python27\Lib\httplib.py"", line 567, in read
    s = self.fp.read(amt)
  File ""C:\Python27\Lib\socket.py"", line 380, in read
    data = self._sock.recv(left)
socket.timeout: timed out

When I merged the fixed response.py (https://raw.githubusercontent.com/adaschevici/urllib3/296-exception-not-properly-wrapped/urllib3/response.py), the exception now changed to (the retry did not occur as I hoped):

  File ""C:\dev\virtualenvs\py27mft\lib\site-packages\requests\sessions.py"", line 395, in get
    return self.request('GET', url, *_kwargs)
  File ""C:\dev\virtualenvs\py27mft\lib\site-packages\requests\sessions.py"", line 383, in request
    resp = self.send(prep, *_send_kwargs)
  File ""C:\dev\virtualenvs\py27mft\lib\site-packages\requests\sessions.py"", line 486, in send
    r = adapter.send(request, **kwargs)
  File ""C:\dev\virtualenvs\py27mft\lib\site-packages\requests\adapters.py"", line 394, in send
    r.content
  File ""C:\dev\virtualenvs\py27mft\lib\site-packages\requests\models.py"", line 679, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File ""C:\dev\virtualenvs\py27mft\lib\site-packages\requests\models.py"", line 616, in generate
    decode_content=True):
  File ""C:\dev\virtualenvs\py27mft\lib\site-packages\requests\packages\urllib3\response.py"", line 245, in stream
    data = self.read(amt=amt, decode_content=decode_content)
  File ""C:\dev\virtualenvs\py27mft\lib\site-packages\requests\packages\urllib3\response.py"", line 223, in read
    ""Remote connection closed. Read timed out."")
urllib3.exceptions.ReadTimeoutError: <requests.packages.urllib3.response.HTTPResponse object at 0x000000000326BDD8>: Remote connection closed. Read timed out.

By the way: this issue is easily reproduced on this particular embedded device--occurs within seconds.  I took a packet trace and I can see a difference between no-issue and issue--but not sure if it is the client (requests) or the server (my embedded device) that is misbehaving.

Thanks!
Darhsan
",darshahlu,Lukasa
1786,2015-12-28 15:41:13,"I am also interested by this feature. When using an internal CA, the CRL as a file is the easiest path. Far easier than using an OCSP endpoint and more reliable than embedding an HTTP URL into the certificate (no way that the CRL can be unavailable due to a temporary outage).

@Lukasa I don't understand when you say this can be done outside requests. There is no way to preprocess the certfile with the CRL. The certfile is usually a root certificate and the CRL contains the serial numbers of revoked certificates. There is no way to apply a CRL to a root certificate.

With curl, you can specify it with `--crlfile`.
",vincentbernat,Lukasa
1786,2015-12-28 15:58:06,"@vincentbernat Yeah, that's a fair point.

We're aiming to add support for providing an external SSLContext object, which should make this possible, but until that's done I don't think we can do much else.
",Lukasa,vincentbernat
1784,2013-12-06 22:02:19,"@wchang we do not change the size of the file.
",sigmavirus24,wchang
1784,2013-12-09 13:18:26,"@wchang you ignored my question about whether you're behind a proxy or not. That would not cause the Application Error you were seeing with httpbin, but it might explain the bizarre difference in behaviour between putting a 2.0 GB file and a >2.0 GB file.
",sigmavirus24,wchang
1781,2013-12-06 17:02:16,"@kracekumar I know. =) The only reason I did this was because Kenneth got eager to push out a release last night, and I wanted to make sure we didn't slow him down. =) Your PR was a good one, thanks for providing it!
",Lukasa,kracekumar
1781,2013-12-06 17:13:05,"@kracekumar it's no big deal at all :)
",kennethreitz,kracekumar
1781,2013-12-06 17:16:26,"@kennethreitz @Lukasa That is great. 
",kracekumar,kennethreitz
1781,2013-12-06 17:16:26,"@kennethreitz @Lukasa That is great. 
",kracekumar,Lukasa
1780,2013-12-12 07:39:16,"@Lukasa I personally make great use of Requests' file uploading options (very helpful when debugging and auditing web apps), and would like to see it remain in core if possible. It's features like this (being able to specify a filename, a string or file object, and a content type in any multipart upload) that make Requests far more versatile than urllib2.

A potential API change could be to allow each file to be either a tuple as it is now, or a dict, where the dict can specify any number of parameters as in: `{""name"": ""filename.xls"", ""content"": open(""filename.xls""), ""content_type"": ""application/vnd.ms-excel""}`. This is more verbose, but removes the need to remember where to position each element in the tuple (though unfortunately might cause the need to remember the filename is ""name"", etc..).
",Anorov,Lukasa
1780,2013-12-12 09:22:27,"@Anorov I appreciate that the current set of features is highly useful, and I wouldn't dream of stripping it out unless there was a good option for replacing it that had total feature parity. We're a ways away from that right now, so we don't need to worry about it, but your objection is noted. =)

This isn't the kind of decision I get to make anyway, I only get to petition Kenneth.
",Lukasa,Anorov
1779,2013-12-05 10:07:46,"@Lukasa I am wondering why you didn't use `0`. 
",kracekumar,Lukasa
1779,2013-12-05 11:32:27,"I'm surprised that you haven't seen +0 before @kracekumar. We use it excessively to vote on topics and it is used in lots of other projects.

+1 := strongly in favor
+0 := neutral but leaning positively toward the change
-0 := neutral but not exactly a fan of the change
-1 := very much against the change.

I'll respond to the PR itself later today
",sigmavirus24,kracekumar
1779,2013-12-05 11:46:31,"@sigmavirus24 I have seen `+0` in mailing lists and other places. Sure. I am glad to fix the bugs pointed out by @Lukasa if needed. Waiting for the comments to improve. 
",kracekumar,Lukasa
1779,2013-12-05 11:46:31,"@sigmavirus24 I have seen `+0` in mailing lists and other places. Sure. I am glad to fix the bugs pointed out by @Lukasa if needed. Waiting for the comments to improve. 
",kracekumar,sigmavirus24
1779,2013-12-05 18:29:19,"I'm not really sure. It's a small change but we would then have to expose it to the user and that's what I'm not exactly a fan of. Certainly they can find it on their own now but that's not the same as exposing it to them which would be encouraging its use. How commonly do requests users set their own User-Agent string and want it to include all of that information? Frankly I'm not convinced it is all that frequently. That said if @kennethreitz wants this I'm perfectly okay with it granted that it handles strings correctly (as @Lukasa already mentioned).
",sigmavirus24,Lukasa
1777,2013-12-12 16:28:01,"@erikcw Any updates on this issue? I'm really curious about how that happens in normal use case.
",daftshady,erikcw
1777,2013-12-13 00:36:15,"@erikcw There already is related PR :)
https://github.com/kennethreitz/requests/pull/1793
",daftshady,erikcw
1775,2013-12-04 03:27:50,"@sigmavirus24 you're more than welcome to work in the main repo in side-branches :)
",kennethreitz,sigmavirus24
1774,2013-12-03 14:08:30,"This issue has been raised many times in the past (please see #1737, #1604, #1589, #1588, #1546. There are others, but this list should be sufficient). The issue @sigmavirus24 is looking for is #1604.

RFC 2616 is very clear here: if no encoding is declared in the Content-Type header, the encoding for text/html is assumed to be ISO-8859-1. If you know better, you are encouraged to either decode `Response.content` yourself or to set `Response.encoding` to the relevant encoding.
",Lukasa,sigmavirus24
1774,2013-12-04 01:40:21,"As usual @Lukasa is 100% correct.
",sigmavirus24,Lukasa
1774,2013-12-04 02:14:36,"@Lukasa thanks for your explanation! I think not every user knows the detail defined in RFC2616, so should you add some comment on `Response.text`?
",weiqiyiji,Lukasa
1772,2013-12-04 01:51:08,"With the exception that the diff is a bit noisy due to @mdbecker alphabetizing portions (imports and one functions params), this is good to merge in my opinion.

For those who don't care to read :+1: :shipit: 
",sigmavirus24,mdbecker
1772,2013-12-06 02:18:50,"Thanks @kennethreitz. Looks like there is a bug here (either in my test or in the code.) The issue is that `time.mktime` returns a time in localtime which I wasn't accounting for in my test. There are 2 ways to solve this problem:
1. Modify `morsel_to_cookie` to subtract `time.timezone` so that the resulting value represents UTC time. This will result in the value of `expires` being the same on all systems regardless of timezone.
2. Modify `TestMorselToCookieExpires.test_expires_valid_str` to subtract `time.timezone` so that the test always passes.

The former solution seems correct to me, and I can't find anything in the code or RFCs to contradict this. Unfortunately this change would definitely impact users of this module so I'd like to hear others opinions on this.

Thanks!
",mdbecker,kennethreitz
1772,2013-12-06 03:22:05,"@mdbecker the PR currently doesn't merge cleanly, could you rebase off of master?
",sigmavirus24,mdbecker
1772,2013-12-06 15:09:55,"@Lukasa re-pushed
",mdbecker,Lukasa
1772,2013-12-06 16:18:20,"Awesome, this is great work! Thanks @mdbecker! :cake:
",Lukasa,mdbecker
1770,2013-12-03 03:43:49,"I second everything @Lukasa said. If you can, please rebase out everything except f9a48e0
",sigmavirus24,Lukasa
1770,2013-12-04 18:58:27,"@Lukasa don't apologize. Those were good catches that I totally missed because I skimmed over them.

I'm :+1: when @Lukasa's comments are addressed.
",sigmavirus24,Lukasa
1770,2013-12-05 22:29:41,"@Lukasa what feedback do you have? Is it still standing?
",kennethreitz,Lukasa
1767,2013-12-01 17:53:14,"Thanks for reporting this @lerks 
",sigmavirus24,lerks
1765,2013-11-29 16:38:30,"@bicycle1885 Sorry, I didn't spot this was a Pull Request! Do you want to add the test I wrote in #1766 to this PR, and I'll close the other one?
",Lukasa,bicycle1885
1765,2013-11-29 16:45:36,"@Lukasa if you rebase your branch off of his, you can give him credit for the fix, while adding the test yourself. You kill two birds with one stone. You both get credit for the work you did.
",sigmavirus24,Lukasa
1765,2013-11-29 16:51:28,":+1: Thanks for taking care of that @Lukasa 
",sigmavirus24,Lukasa
1765,2013-11-29 17:06:05,"Thanks for your attention @sigmavirus24.
And I can save the cost to write a test code thanks to @Lukasa's job!
",bicycle1885,Lukasa
1765,2013-11-29 17:06:05,"Thanks for your attention @sigmavirus24.
And I can save the cost to write a test code thanks to @Lukasa's job!
",bicycle1885,sigmavirus24
1764,2013-11-29 16:59:48,"Looking over the PRs listed above now:
- #1713 :+1: merge it
- #1657 :+1: merge it
- #1729 :-1: do not merge it (I don't fee it is ready yet. @Lukasa and I left feedback that hasn't been addressed)
- #1628 :-1: do not merge it (neither @kennethreitz nor I are very fond of this)
- #1766 :+1: merge it
- #1743 +0 I don't see anything awful about allowing for separate timeouts, it is just the API under question. I've proposed a different way of handling the same feature. I'm not sure this _has_ to be in 2.1 though 
",sigmavirus24,Lukasa
1764,2013-12-05 22:34:05,"@sigmavirus24 d'oh, sorry guys. I got ahead of myself :) Hoping to do this release tonight. 
",kennethreitz,sigmavirus24
1755,2013-11-27 02:29:41,"@akitada please don't close your own issues. We'll close them when we are ready to.
",sigmavirus24,akitada
1755,2013-11-28 02:02:48,"No need to apologize @akitada 
",sigmavirus24,akitada
1751,2013-11-22 02:18:25,"@skorokithakis it does matter. Frozen does not mean that backwards compatible changes are allowable. Generally speaking a frozen API means that no new features can be made. On the other hand, given a frozen API and semantic versioning a backwards compatible change can be made assuming it doesn't add anything to the API.

The solution is just for the cases you mentioned in your issue report. You can always pass the `stream=True` parameter and still use `r.json()`.
",sigmavirus24,skorokithakis
1751,2014-03-12 10:11:26,"@mortoray The way the `stream` option is intended to work is to have the user (in this case yourself) take control of how to handle data downloads. For example, you've got a use case where you want to limit the size of the downloaded body. You can do something like this:



If you're concerned about being sent infinite header data you have a very separate problem, which is that Requests uses urllib3 which uses httplib for its header parsing. There's an [open Python issue](http://bugs.python.org/issue16037) which tracks the fact that `httplib` has no maximum limit on the number of headers downloaded. This has been fixed in 2.6 and 3.3, but I don't see a fix on my copy of 2.7.6. I'm going to check out the codebase and confirm, but I suspect this fix hasn't been merged. There's nothing Requests can do for you in that situation.
",Lukasa,mortoray
1750,2013-11-22 17:33:33,"@glennbach Linked from that issue is the relevant pull request, which is #1713. It hasn't yet been accepted, but feel free to try it.
",Lukasa,glennbach
1748,2013-11-21 14:23:18,"@guettli Thanks for providing your feedback! It's always appreciated. :cookie:
",Lukasa,guettli
1744,2013-11-18 14:11:16,"I feel fairly confident this is exactly the same issue as https://github.com/kennethreitz/requests/issues/1711 and that @auworkshare did not look at open issues before filing this one.
",sigmavirus24,auworkshare
1744,2013-11-18 15:46:30,"@auworkshare If you're using cookies you got from a successful login, you should really just be using a `Session` to persist your state for you. I'm assuming you're just taking the cookies from the `Response` object, which are in a `CookieJar`. This means that @sigmavirus24 is correct, this is a duplicate of #1711.
",Lukasa,auworkshare
1744,2013-11-18 15:46:30,"@auworkshare If you're using cookies you got from a successful login, you should really just be using a `Session` to persist your state for you. I'm assuming you're just taking the cookies from the `Response` object, which are in a `CookieJar`. This means that @sigmavirus24 is correct, this is a duplicate of #1711.
",Lukasa,sigmavirus24
1743,2013-11-20 09:03:16,"@kevinburke i'll try to chat with you on IRC about this this week. :)
",kennethreitz,kevinburke
1739,2013-11-15 18:51:45,"i also love you, @pengfei-xue :cake:
",kennethreitz,pengfei-xue
1738,2013-11-15 14:24:14,"@Lukasa i created another pull reuqest, please close this out thanks.
",pengfei-xue,Lukasa
1737,2013-11-14 13:00:00,"@Lukasa you were tricked into saying this:

> Stripping all the functionality from Response.text, as you suggest in your last point, seems silly to me. If we're going that far, we should remove Response.text altogether.

This is clearly the agenda of this issue as you can tell by:

> If you're going to keep the .text property

@itsadok clearly wants the `.text` property to disappear because issues have been filed regarding it in the past.

Let me address one other thing that @Lukasa didn't before I add my opinion.

> Additionally, the documentation should contain a warning not to use it for arbitrary web pages, and perhaps a code sample showing the proper way to do it.
1. charade works fairly well for well established codecs. There are new ones that subsume old ones which it doesn't support yet. Why? There aren't publicly available statistics for those encodings and that's what charade relies on. If you disagree with how something is being detected, why not file a bug report on charade?
2. That code sample is **not** the proper way to do it. Using regular expressions on HTML is insanity and is **never** the correct answer.

With that addressed, let me address one more theme of this issue: Because _some negligible percentage_ of all issues have been filed about _x_, _x_ should be (changed|removed).

One thing to note is that all the issues with numbers lower than 1000 were filed before requests 1.0 which is when the API was finalized. If there were legitimate bugs in this attribute prior to that, I would be far from surprised. Also some of those issues are instead about the choice that chardet/charade made. Those are not bugs in requests or `.text` but instead in the underlying support.

Finally, after the release of 1.0 we had a lot of issues about the change from `json` being a property on a Response object to becoming a method. We didn't remove it or change it back for a good reason. It was a deliberate design decision.

The `.text` attribute is quite crucial to this library, especially to the `json` method, and it will likely never be removed. Can it be improved? Almost certainly. You provided a couple of good suggestions, but the overall tone this issue is meant to convince the reader that it should be removed and that will not happen. Without a reasonable guess at the encoding of the text, we cannot provide the user with the `json` method which also will not go away. Simply, the user is not the sole consumer of `.text`.
",sigmavirus24,itsadok
1737,2013-11-14 13:00:00,"@Lukasa you were tricked into saying this:

> Stripping all the functionality from Response.text, as you suggest in your last point, seems silly to me. If we're going that far, we should remove Response.text altogether.

This is clearly the agenda of this issue as you can tell by:

> If you're going to keep the .text property

@itsadok clearly wants the `.text` property to disappear because issues have been filed regarding it in the past.

Let me address one other thing that @Lukasa didn't before I add my opinion.

> Additionally, the documentation should contain a warning not to use it for arbitrary web pages, and perhaps a code sample showing the proper way to do it.
1. charade works fairly well for well established codecs. There are new ones that subsume old ones which it doesn't support yet. Why? There aren't publicly available statistics for those encodings and that's what charade relies on. If you disagree with how something is being detected, why not file a bug report on charade?
2. That code sample is **not** the proper way to do it. Using regular expressions on HTML is insanity and is **never** the correct answer.

With that addressed, let me address one more theme of this issue: Because _some negligible percentage_ of all issues have been filed about _x_, _x_ should be (changed|removed).

One thing to note is that all the issues with numbers lower than 1000 were filed before requests 1.0 which is when the API was finalized. If there were legitimate bugs in this attribute prior to that, I would be far from surprised. Also some of those issues are instead about the choice that chardet/charade made. Those are not bugs in requests or `.text` but instead in the underlying support.

Finally, after the release of 1.0 we had a lot of issues about the change from `json` being a property on a Response object to becoming a method. We didn't remove it or change it back for a good reason. It was a deliberate design decision.

The `.text` attribute is quite crucial to this library, especially to the `json` method, and it will likely never be removed. Can it be improved? Almost certainly. You provided a couple of good suggestions, but the overall tone this issue is meant to convince the reader that it should be removed and that will not happen. Without a reasonable guess at the encoding of the text, we cannot provide the user with the `json` method which also will not go away. Simply, the user is not the sole consumer of `.text`.
",sigmavirus24,Lukasa
1737,2013-11-14 13:20:14,"Sorry about the closing and reopening, that was a mis-click.

@sigmavirus24 I'm sorry if it seems like I have an agenda. I'm honestly just trying to help. I read through the discussions in **all** of the issues I posted. They all really seem to revolve around the same basic confusion, with people expecting Response.text to be an all-encompassing solution where in reality it is not. 

Let me just clarify some misunderstandings in what I wrote.

> > Additionally, the documentation should contain a warning not to use it for arbitrary web pages, and perhaps a code sample showing the proper way to do it.
> > charade works fairly well for well established codecs. There are new ones that subsume old ones which it doesn't support yet. Why? There aren't publicly available statistics for those encodings and that's what charade relies on. If you disagree with how something is being detected, why not file a bug report on charade?

I merely meant that it should be noted that `Response.text` should not be used willy-nilly on arbitrary web page, precisely because it avoids using charade in many places where it can be used.

> That code sample is not the proper way to do it. Using regular expressions on HTML is insanity and is never the correct answer.

The `get_encodings_from_content` function is fully copied from `requests.utils`, with the added comment by me that it doesn't really work in Python 3. In any case the point was that this needs to be clarified, not my specific solution.

> The .text attribute is quite crucial to this library, especially to the json method, and it will likely never be removed. Can it be improved? Almost certainly. You provided a couple of good suggestions, but the overall tone this issue is meant to convince the reader that it should be removed and that will not happen. Without a reasonable guess at the encoding of the text, we cannot provide the user with the json method which also will not go away. Simply, the user is not the sole consumer of .text.

This is a valid point, and perhaps serves to explain the weird nature of .text. Perhaps all that is needed is a note in the documentation.
",itsadok,sigmavirus24
1737,2013-11-14 14:39:45,"> I merely meant that it should be noted that Response.text should not be used willy-nilly on arbitrary web page, precisely because it avoids using charade in many places where it can be used.

Just because something _can_ be used somewhere does not mean it should be. `charade` is slow as a result of its accuracy and painstaking meticulousness. Using it when it _can_ be used as opposed to when it _must_ be used makes the performance difference in the user's eyes.

> The get_encodings_from_content function is fully copied from requests.utils

But we never use it. It is cruft and _should_ be removed. It is the wrong way to do things.

> That means that the cases where automatic charset detection is actually used are pretty rare, and unpredictable for the user. So why keep the charade?

You're assuming everything behaves the same way and that RFCs are followed by servers. They're not. Charade is occasionally used. We keep it because it is essentially part of the API. Response's couldn't have an `apparent_encoding` attribute if we discarded charade. We can break the API if we ever release 3.0 but until then, the API is frozen except for backwards compatible changes. Also, that's an excellent pun.

> It seems like I hit a nerve, which was really not my intention. I'm going to close the issue

I'm going to re-open it. You made valid points as @Lukasa pointed out. It just needs to be clear that this is not any agreement about removing the property.
",sigmavirus24,Lukasa
1737,2013-11-14 14:44:20,"Heh, @sigmavirus24 both have the same reactions. Neither of us thinks this issue should be closed: I reopened it just before he could!

Neither of us is angry, or unhappy about having this feedback. We're both delighted. You just can't tell because of the limitations of textual communication! =D

The reality is that text encoding is hard, everyone is doing the wrong thing from time to time, and we cannot possibly please anyone. For that reason, we do the best we can, and then we expose the `Response.encoding` property for people who don't like the way we do it.

You've identified good issues with `Response.text`, and I plan to fix them (unless someone else does so first). They're not all backward compatible, so we'll need to sit on them for a bit, but they're good.
",Lukasa,sigmavirus24
1736,2013-11-14 14:43:53,"@Lukasa there was a reason I started https://github.com/sigmavirus24/requests-data-schemes. It was a bad name but the intent was to provide a better way of doing complicated multipart uploads (specifically when no files should be sent, which is an **extremely** common use case).
",sigmavirus24,Lukasa
1735,2013-11-15 12:52:32,"Why were you afraid @kennethreitz ?
",sigmavirus24,kennethreitz
1734,2013-11-10 20:36:42,"@Oire can you give more details than that? What kind of file are you trying to post? Any other details? 
",sigmavirus24,Oire
1733,2013-11-10 01:59:14,"If you feel you want to add more tests @ionrock then go ahead but I'm happy with this as it is. :+1:
",sigmavirus24,ionrock
1729,2013-12-04 01:48:49,"@Lukasa if I take over the rest of this PR (to address our feedback) would we be comfortable getting it into 2.1? I feel like this addresses a real concern and I think you were in agreement with @gazpachoking at the end of #1728. Correct me if I'm wrong. 

For reference, I'll pull his commits onto a branch of mine, and just fix up the one minor thing and send a new PR and then close this one. Sound good?
",sigmavirus24,gazpachoking
1729,2013-12-04 01:48:49,"@Lukasa if I take over the rest of this PR (to address our feedback) would we be comfortable getting it into 2.1? I feel like this addresses a real concern and I think you were in agreement with @gazpachoking at the end of #1728. Correct me if I'm wrong. 

For reference, I'll pull his commits onto a branch of mine, and just fix up the one minor thing and send a new PR and then close this one. Sound good?
",sigmavirus24,Lukasa
1729,2013-12-04 12:47:15,"@Lukasa done in #1776 
",sigmavirus24,Lukasa
1728,2013-11-07 14:03:55,"@gazpachoking I agree that for that specific example the correct behaviour is clear. But what about this one?



I continue to believe that from where we stand it seems obvious that the answer should be 'it shouldn't be persisted', but I'm convinced that people will get this wrong. I just want to make sure we're all aware of the various subtleties here before we make a call. =)
",Lukasa,gazpachoking
1727,2013-11-10 16:02:04,"@Lukasa frankly, I don't understand the problem well. Proxies have always been your thing and I never attempted to get enough background on them.

So what Betamax does is replace the adapters that the Session object is using and then use a regular HTTPAdapter to make the actual request if necessary. The reason I'm not certain, is because of [`get_connection`](https://github.com/kennethreitz/requests/blob/master/requests/adapters.py#L187..L211] on the `HTTPAdapter`.

Betamax looks at the Prepared Request that comes through and then matches that against previously recorded (if any) requests to find an appropriate response. Because of how requests handles redirects, even redirects are recorded properly. If the logic you're looking for happens outside of the HTTPAdapter (any of the layers above it), Betamax can probably help so long as the info is on the Prepared Request. If it happens underneath the covers of the HTTPAdapter, then the answer is still ""maybe"" because the request may have the data you're looking for, but I'm not certain.
",sigmavirus24,Lukasa
1727,2014-09-12 13:28:08,"Ah, correct, so it is. Thanks @blueyed!
",Lukasa,blueyed
1726,2013-11-05 03:57:25,"@t-8ch how were you not already in the AUTHORS.rst file? This is an unacceptable oversight and I'm glad you remedied it.
",sigmavirus24,t-8ch
1726,2013-11-05 22:01:57,"@sigmavirus24 To be honest, there are only a handful of contributions codewise (directly to requests not updating urllib3) by me. 
It seems you also took your time when adding yourself to AUTHORS.rst. Happens to the best. :smile: 
",t-8ch,sigmavirus24
1724,2013-11-04 12:55:06,"@daftshady please use pytest's capabilities to add a test that should only ever fail on Python 2.x please.
",sigmavirus24,daftshady
1724,2013-11-04 13:36:15,"@sigmavirus24 I moved logic and added test case. Please review this. Thanks.
",daftshady,sigmavirus24
1724,2013-11-05 03:56:43,":cake: Thanks @daftshady 
",sigmavirus24,daftshady
1723,2013-11-04 15:38:38,"Thanks for the quick response!  I just found some relevant history from 6 months ago:

https://github.com/kennethreitz/requests/issues/1252#issuecomment-17875143

At that time, @Lukasa you didn't consider this worth fixing.  Has anything changed?

In my projects I'm still using Python 2.7 but I would like to migrate to Python 3 someday, so I always use unicode unless I actually need bytes.  This did not seem to be one of those cases; since the `method` parameter can be passed as lowercase and `requests` will convert it to the proper format, I assumed if it needed converting from unicode that `requests` would handle that as well.
",hwkns,Lukasa
1723,2013-11-04 16:05:44,"@hwkns Yes, something has changed. Specifically, we merged #1338. This added the `to_native_string` method that @daftshady is using in the fix.

Previously, we'd have had to special-case this parameter, which seemed insane to me, especially as it's very easy to simply not use it (e.g. call requests.get or requests.post). However, we can now treat this parameter the way we treat header keys, which is to say that people who use this library have all sorts of ways of handling their internal strings, so we should just fix it up as best we can.

In your particular case I think you're overthinking the issue. If the method name can't come from outside your application (and I hope to god it can't), then there's no reason for you not to just use native strings. You gain nothing by forcing them to unicode, and your code will be cleaner. However, I'm still +1 on taking a fix for this, because it's now so easy to do.
",Lukasa,hwkns
1717,2013-10-31 18:33:34,"@sigmavirus24 How's that?
",jvantuyl,sigmavirus24
1717,2013-11-15 09:22:08,"@jvantuyl This isn't Kenneth sitting on the fence, this is Kenneth being super busy. =) Requests takes some time to merge pull requests because Kenneth has the final say, and he's got about a million responsibilities. You'll just have to be patient I'm afraid.

(Also, it would help if you rebased to make it easier for Kenneth to merge if he decides to.)
",Lukasa,jvantuyl
1717,2013-11-15 09:32:42,"@Lukasa you make me sound way more impressive than I am. If I ever need a PR person, remind me to hire you :P
",kennethreitz,Lukasa
1717,2013-11-15 09:33:02,"@jvantuyl :sparkles: :beers: :sparkles: 
",kennethreitz,jvantuyl
1717,2013-11-15 09:38:10,"Yay!  @kennethreitz Look me up next time you're in San Francisco.  Beer, on me.
",jvantuyl,kennethreitz
1714,2013-10-30 11:45:21,"I support absolutely everything @Lukasa has said here.
",sigmavirus24,Lukasa
1713,2013-11-05 05:51:10,"@kennethreitz It's easy to do so. But in my opinion, if `CookieJar` updated by another `CookieJar` happens in `cookiejar_from_dict`, it may harm originality(converting `dict` to `CookieJar`) of `cookiejar_from_dict`. (If that happens, method name may should be changed too)
Do you have any good idea about moving into `cookiejar_from_dict`?
What about moving whole logic into another new method like `merge_cookies`?
",daftshady,kennethreitz
1713,2013-11-05 08:25:12,"@Lukasa Is it good if i update that method not to create new instance?
",daftshady,Lukasa
1713,2013-11-05 08:28:54,"@daftshady Yes, I think so. As you've pointed out, the basic `CookieJar` doesn't have a `.update()` method. But I think that's fine, we can either let the `AttributeError` bubble up or rethrow something nicer, e.g. `requests.exceptions.CookieJarError`.
",Lukasa,daftshady
1713,2013-11-05 14:51:16,"@Lukasa I pushed updated commit. 'merge_cookies' method does not create new `CookieJar` anymore.
",daftshady,Lukasa
1713,2013-11-05 16:27:28,"@Lukasa Thanks for reviewing. 
I think build has been failed because 'httpbin' does not be responded in one test :(
",daftshady,Lukasa
1713,2013-11-06 09:50:45,"@Lukasa I tried to unnecessary re-throw because you pointed out that :). Your `right code` is exactly same as my first version of code. I will attach fixed code. thanks.
",daftshady,Lukasa
1713,2013-11-06 19:18:19,"@daftshady can you do a rebase? I'd love to get this merged in today if @Lukasa or @sigmavirus24 +1
",kennethreitz,Lukasa
1713,2013-11-06 19:18:19,"@daftshady can you do a rebase? I'd love to get this merged in today if @Lukasa or @sigmavirus24 +1
",kennethreitz,sigmavirus24
1713,2013-11-06 19:18:19,"@daftshady can you do a rebase? I'd love to get this merged in today if @Lukasa or @sigmavirus24 +1
",kennethreitz,daftshady
1713,2013-11-06 19:36:25,"I'm definitely +1, though I'd like it if @sigmavirus24 could give his input too. =)
",Lukasa,sigmavirus24
1713,2013-11-07 00:02:22,"I'll look at this when I get to a computer but I trust @gazpachoking 's judgment. Also, Chase can you pinpoint which commit started that? I'll have to see if I remember the context for it.
",sigmavirus24,gazpachoking
1713,2013-11-07 03:12:34,"@kennethreitz Sure, I rebased it.
",daftshady,kennethreitz
1713,2013-11-25 19:41:53,"@daftshady can you rebase? It's been a while :)
",kennethreitz,daftshady
1713,2013-11-26 06:41:58,"@kennethreitz yes it's been a while. I rebased it again :)
",daftshady,kennethreitz
1711,2013-10-30 00:48:26,"@abn can you try out the code here: https://github.com/kennethreitz/requests/pull/1713
",sigmavirus24,abn
1711,2013-10-30 01:56:48,"@sigmavirus24 https://github.com/kennethreitz/requests/pull/1713 works for me.
",abn,sigmavirus24
1711,2013-10-30 02:38:07,"Thanks for dobule checking @abn 
",sigmavirus24,abn
1710,2013-10-29 14:15:42,"@jvantuyl I actually disagree with that part. Requests is fundamentally a HTTP library. It's nice that Transport Adapters make it possible to use whole different protocols (with `file://` and `ftp://` being the only ones I've seen done), but it's not actually what Requests is intended to do. When we don't need to make it harder we shouldn't, but neither should we make Requests protocol-agnostic.
",Lukasa,jvantuyl
1710,2013-10-29 16:33:47,"@Lukasa Hmmmmm, I'm not suggesting it needs to be entirely protocol agnostic; merely that, if we're going to purport to have pluggable protocols at all, we should compartmentalize them as much as reasonable.

@sigmavirus24 Please describe the smell.  Am I looking for cheese or rotting meat?  Seriously though, the file RFC specifies that empty hosts equate to localhost.  file:///yada/yada/yada is perhaps the most common form of file URL that there is.  Refusing to accept that as possible is refusing the file protocol in its entirety.

How do you propose to allow this work?  Back in Pull Request #1109, I was told that this was ""exactly the type of thing that I was hoping people would make with Connection Adapters"" and that ""just doesn't belong in Requests itself, but another module"" and ""it would be awesome"".  Are we making this impossible now?  That would not be awesome.

Then again, one man's cheese is another man's mold?  Indulge me for a minute.  Sample my cheese and see if you like the smell after tasting the flavor.  As it turns out, this eliminates something phenomenally useful.  Let me explain what I actually _do_ with this code, as it all comes down to two very useful things.

Firstly, I like to test my code.  Tested code is great.  Testing code with requests is hard, because it's network-active.  Essentially, you have to mock it out; which, in turn, makes your tests inaccurate.  When requests changes, my mocks don't, and my tests don't reflect the changes.

To get good coverage, this forced me to set up a mock web-API and run it on a small web server.  It made my code have environmental dependencies for testing, and building the test environment was a pain.  As I developed this mock server, I realized that 95% of the handlers were just returning a file.

I thought to myself, ""Why not just use file:/// into the test directory and work with the files? That makes it self-contained, and only requires a different URL for testing, which I already do!""  That's when I implemented the file:/// URL because, well, it was easy.

Secondly, I love documentation.  The more I started developing my code this way, I started to collect a battery of files.  These files were checked in with my code, contained JSON of the actual requests, and turned out to entirely document the expectations I had for the API I was interacting with.  I could even look in the git history to see when I started using a feature, etc.  It made my code much more accessible to other developers because it was so much ""look here, see what it's doing"".

So, here are two very real benefits in very real code that are made possible by this rather modest change.  I feel that they heavily outweigh any small smell that they may cause (of course, I'm biased).  This is a five-line change limiting HTTP-specific behavior to HTTP-specific protocols in a system that is architected to support additional protocols through plugins in order to support one of the other most widely-used RFC-specified protocols on the planet.  Isn't it a smell if a minor, overzealous sanity check prevents this from being attainable?

If that's too much, can I just remove the checkl?  Or can I move it into the HTTP adapter so it happens later?  I don't think either of those make it more maintainable, thought they at least make things possible.
",jvantuyl,Lukasa
1710,2013-10-29 16:33:47,"@Lukasa Hmmmmm, I'm not suggesting it needs to be entirely protocol agnostic; merely that, if we're going to purport to have pluggable protocols at all, we should compartmentalize them as much as reasonable.

@sigmavirus24 Please describe the smell.  Am I looking for cheese or rotting meat?  Seriously though, the file RFC specifies that empty hosts equate to localhost.  file:///yada/yada/yada is perhaps the most common form of file URL that there is.  Refusing to accept that as possible is refusing the file protocol in its entirety.

How do you propose to allow this work?  Back in Pull Request #1109, I was told that this was ""exactly the type of thing that I was hoping people would make with Connection Adapters"" and that ""just doesn't belong in Requests itself, but another module"" and ""it would be awesome"".  Are we making this impossible now?  That would not be awesome.

Then again, one man's cheese is another man's mold?  Indulge me for a minute.  Sample my cheese and see if you like the smell after tasting the flavor.  As it turns out, this eliminates something phenomenally useful.  Let me explain what I actually _do_ with this code, as it all comes down to two very useful things.

Firstly, I like to test my code.  Tested code is great.  Testing code with requests is hard, because it's network-active.  Essentially, you have to mock it out; which, in turn, makes your tests inaccurate.  When requests changes, my mocks don't, and my tests don't reflect the changes.

To get good coverage, this forced me to set up a mock web-API and run it on a small web server.  It made my code have environmental dependencies for testing, and building the test environment was a pain.  As I developed this mock server, I realized that 95% of the handlers were just returning a file.

I thought to myself, ""Why not just use file:/// into the test directory and work with the files? That makes it self-contained, and only requires a different URL for testing, which I already do!""  That's when I implemented the file:/// URL because, well, it was easy.

Secondly, I love documentation.  The more I started developing my code this way, I started to collect a battery of files.  These files were checked in with my code, contained JSON of the actual requests, and turned out to entirely document the expectations I had for the API I was interacting with.  I could even look in the git history to see when I started using a feature, etc.  It made my code much more accessible to other developers because it was so much ""look here, see what it's doing"".

So, here are two very real benefits in very real code that are made possible by this rather modest change.  I feel that they heavily outweigh any small smell that they may cause (of course, I'm biased).  This is a five-line change limiting HTTP-specific behavior to HTTP-specific protocols in a system that is architected to support additional protocols through plugins in order to support one of the other most widely-used RFC-specified protocols on the planet.  Isn't it a smell if a minor, overzealous sanity check prevents this from being attainable?

If that's too much, can I just remove the checkl?  Or can I move it into the HTTP adapter so it happens later?  I don't think either of those make it more maintainable, thought they at least make things possible.
",jvantuyl,sigmavirus24
1710,2013-10-29 16:40:44,"@jvantuyl In principle, sure. In practice, Requests only knows about HTTP. Everything about the Transport Adapter is totally about HTTP. If we were moving protocol-specific features to the Transport Adapter, we'd just have `RequestsAdapter`. =D

Don't get me wrong. I'm in favour of supporting `file://` URLs, which is why I'm in favour of this change. I'm just trying to make it clear that I'll otherwise draw a fairly strong line in the sand at ""this far but no further"". There is a limit to what Requests should be able to do in non-HTTP protocols.
",Lukasa,jvantuyl
1710,2013-10-30 02:54:52,"I guess what I really mean, is that your goal is achievable without these changes which may be minor but which are likely to be removed in future refactors of the code-base. As @Lukasa explained, we're really primarily concerned with HTTP and while other adapters don't run into this issue, there are probably other ways around this. One thing is that the implementation details of the Adapter are completely opaque to requests and should always be. Certainly `file://` is common among browsers and even curl, but why not do something more clever?

For example, you could just as easily have your adapter receive requests on: `http://localhost:64000/` which I'm pretty sure is safe from conflict. Really, it could match on anything the user wants. If they're wrapping an API (like I am), they can add an adapter for the API they're wrapping, e.g.:



This would allow the user to not need to specify `file://` when making their ""mocked"" requests.

My last comment was not meant to sell you Betamax.

Frankly here are the reasons I'm :-1:
- The code is likely to get lost in some future shuffle. This will break your adapter then and we'll have to rehash this issue.
- There are other ways that don't require this change
- There are other libraries that seem to satisfy your needs and other people's needs
  - Betamax
  - requestions
  - vcrpy
  - and the list goes on
- Historically we as trio have been wont to encourage people to use the `file://` scheme and to support it
",sigmavirus24,Lukasa
1710,2013-10-30 17:19:43,"@kennethreitz That's basically what this change does, makes the PreparedRequest a little less paranoid, so I can subsume this type of handling inside the adapter.  Without this change any PreparedRequest bombs out when it tries to handle the non-native URL.
",jvantuyl,kennethreitz
1710,2013-10-30 17:20:53,"@kennethreitz Creating?  Well, sort of.  When you create a Request, it automatically creates a PreparedResponse and prepares it.  I _could_ create one manually, not run any of the sanity checks, and then use it for this.  Though, that makes something that's otherwise fairly transparent into something that's significantly more complicated.
",jvantuyl,kennethreitz
1709,2013-10-28 17:27:48,"@samuelhug is the the only thing that doesn't work on GAE?
",kennethreitz,samuelhug
1709,2013-10-28 17:36:03,"@kennethreitz I haven't run extensive tests. However, I came across this issue while testing out coto/gae-boilerplate and this fix was the only modification required.
",samuelhug,kennethreitz
1705,2013-10-26 17:05:16,"@pikumar I can't reproduce this at all on a new Debian 7.2 container.
`register123.blip.us` resolves for me to `176.74.176.178`.
Is this the whole testscript?
Are you using some form of async environment (gevent etc)?
Maybe you are behind a firewall/proxy which drops packets with the `TCP` `RST` flag set?
",t-8ch,pikumar
1705,2013-10-26 17:20:37,"@t-8ch Thanks for the quick reply. Perhaps there was a typo in your request? Please try : requests.get(""https://register123.blib.us"", verify=False) - you seem to be trying blip.us? register123.blib.us should resolve to  216.230.230.76.
",pikumar,t-8ch
1705,2013-10-26 17:46:56,"@pikumar You are right, I had the wrong domain in the container (while testing on my main box I got it right)
But on the other hand the behaviour with the right domain is the same as with the wrong one.
`TCP RST` followed by a an exception from requests.
You could use wireshark to sniff the traffic and analyse or upload the sniffed data.
",t-8ch,pikumar
1705,2013-10-26 21:42:23,"@t-8ch Indeed wget/curl/requests all hang on this new box. Looking into it. 
",pikumar,t-8ch
1705,2013-10-27 02:19:10,"@t-8ch This is what is happening on wireshark: http://cloudshark.org/captures/7e86057afd34 (Its a debian Virtualbox VM under Win7 = Box 1). openssl version 1.0.1e. With openssl version 1.0.0 on another box (Box 2), things look fine (A non-existent https+sni domain access gives --> curl: (35) Unknown SSL protocol error in connection to register123.blib.us:443) . For Box 1, this is what a correct url + successful connection looks like : http://cloudshark.org/captures/687634100c81. (Server = https+SNI).

On Box 1, requests/wget/curl all hang with : https://non-existant-domain.blib.us - but this fixes the hang for wget:
wget ----secure-protocol=SSLv3 https://non-existant-domain.blib.us

It might not be a firewall issue? It probably is an openssl/tls issue? It might be my server not handling TLS 1.2 properly - even in that case, the clients should not hang I would think. (Remember, for existing domains, everything seems to be fine (both box 1/2), its only the non-existent domains that hang things)
",pikumar,t-8ch
1705,2013-10-27 08:38:28,"@pikumar I don't see SNI in your requests at all. There should be a `Extension: server_name` in your `Client hello`.
My Debian does this.
The trace from your hanging request looks like mine from the working one.
Where did you take the trace? Could it be the `RST` packet gets dropped after that?
(Like you traced on the Windows System and the Debian box itself does some firewalling)

To ask again: You are not using gevent or anything like that?

You can also force requests to use `SSLv3`.
Like always theres a blog post by @Lukasa for that: http://lukasa.co.uk/2013/01/Choosing_SSL_Version_In_Requests/
",t-8ch,Lukasa
1705,2013-10-27 08:38:28,"@pikumar I don't see SNI in your requests at all. There should be a `Extension: server_name` in your `Client hello`.
My Debian does this.
The trace from your hanging request looks like mine from the working one.
Where did you take the trace? Could it be the `RST` packet gets dropped after that?
(Like you traced on the Windows System and the Debian box itself does some firewalling)

To ask again: You are not using gevent or anything like that?

You can also force requests to use `SSLv3`.
Like always theres a blog post by @Lukasa for that: http://lukasa.co.uk/2013/01/Choosing_SSL_Version_In_Requests/
",t-8ch,pikumar
1705,2013-10-27 17:27:32,"@t-8ch http://cloudshark.org/captures/1f95da7bf5a7 - this is a capture from inside the debian box. And you are right: I do not see the RST in there. What baffles me, is that my iptables is completely open. Everything else works as it should, except non-existent domains. 

Your answers:

SNI : Indeed register.blib.us is not SNIed, but any existing name is: for example https://rrc.imp.blib.us should be.

I am not using gevent, unless requests automatically picks it up when ndg-.., pyasn1,... is installed. I did install the dependencies in urllib3 that helps with SNI. 
",pikumar,t-8ch
1705,2013-10-27 18:22:36,"@pikumar requests doesn't automatically pick anything up. @t-8ch was only asking because you hadn't answered earlier and wanted to make sure this wasn't one of the few issues we're starting to see with requests + gevent.
",sigmavirus24,t-8ch
1705,2013-10-27 18:22:36,"@pikumar requests doesn't automatically pick anything up. @t-8ch was only asking because you hadn't answered earlier and wanted to make sure this wasn't one of the few issues we're starting to see with requests + gevent.
",sigmavirus24,pikumar
1705,2013-10-27 20:44:35,"@pikumar Yes, but your `Client Hello` should contain the `servername` extensions. This should happen if you have the mentioned dependencies installed (ndg-, pyasn1, pyopenssl). So it seems the installation of those isn't correct.

However this has no relation to the missing `RST` packets. And it's most likely out of scope for this bugtracker.

I would nevertheless like to know if you in fact used the pyopenssl codepath (missing SNI extension suggests otherwise) because the request with specified timeout should probably not hang forever.
",t-8ch,pikumar
1704,2013-10-26 06:57:31,"Once again @sigmavirus24 is right, that's almost certainly the problem.
",Lukasa,sigmavirus24
1704,2013-10-30 00:45:04,"@zhuffman if we don't hear back from you soon, I'm going to close this. We have no way of reliably testing this so until you give us more details it is essentially invalid.
",sigmavirus24,zhuffman
1704,2013-10-30 21:28:20,"Hi all, I apologize for keeping everyone waiting, and appreciate all the quick and thoughtful responses.

Indeed, @sigmavirus24 was correct--our service was performing a redirect, and requests handles those differently in v0.13.3 and v2.0.1. Making the requests with allow_redirects=False yielded a 301: moved permanently, so that was eye-opening. It turns out that in these tests we were using an old version of our api, so the redirect is, as I understand, the solution we implemented for the sake of backwards compatibility with older versions of the apis. The older version of requests will allow a redirect when it receives a 301 as long as a) allow_redirects is True (which it is by default--in both versions) and b) the method was not a POST [v 0.13.3, requests/modules.py, line 260]. In the new version, the same is true with the allow_redirects boolean but the method _must_ be either a GET or a HEAD--and of course, delete is neither of those [v 2.0.2, requests/sessions.py, line 116]. Thus, exactly as @sigmavirus24 thought, DELETE gets changed to GET.

I actually finally managed to sit down with the last guy that worked on apis and he postulated that I should be able to avoid a redirect by explicitly specifying the most recent version of our apis in the tests, so that's the next step for me. But, if I may ask, why the change? Is the method change to GET a safety measure that we just so happen to not need because we've successfully setup a redirect due to the changing apis? Had we not implemented that, would the older version of requests not be functioning properly? On the same note, I'm curious about the comments above each line:

v 0.13.3: # Do what the browsers do if strict_mode is off...
v 2.0.1: # Do what browsers do, despite standards...

Thanks!
",zhuffman,sigmavirus24
1704,2013-10-30 21:37:44,"The comments indicate the exact rationale. The standard specifies that a 301 (and a 302) in response to something that isn't a GET (or a HEAD) should be followed by the same method, but only after confirming with the user. However, web browsers have basically never done that, instead following a POST with a GET if they receive a redirect.

What happened in 2.X to explain the comments is that we gave up on `strict_mode`. I'm not 100% on the changing DELETE to GET for 301s though. I quite want to see what @sigmavirus24 thinks.
",Lukasa,sigmavirus24
1704,2013-11-17 14:27:04,"@Lukasa I guess it is backwards incompatible but this behaviour is extraordinarily wrong. I doubt there is anyone actually relying on this behaviour so we should be safe fixing it.
",sigmavirus24,Lukasa
1704,2013-11-24 14:54:14,"@Lukasa just to be clear, when you say ""301 was rewritten to GET"", do you mean all verbs that receive a 301 are rewritten to a GET? That's the only explanation that makes sense to me given how IE behaves. Thanks for digging in and getting the right answer here.
",sigmavirus24,Lukasa
1703,2013-10-25 13:38:46,"@canibanoglu Heh, this is not a great beginner problem. This issue is that, when HTTPS proxies were added to urllib3 they added a `proxy_headers` parameter which adds headers to all messages sent via a proxy. This is clearly the right place for `Proxy Authorization` to go. However, it turns out urllib3 only applies the HTTP versions when you use `ProxyManager.urlopen()`, which Requests doesn't do. Hence the work I'm doing in urllib3 to try to make some kind of consistent interface. =)
",Lukasa,canibanoglu
1703,2013-10-25 15:09:23,"@canibanoglu The question is not actually technically difficult, but it's about how urllib3 should be structured and how its interfaces should be used. You're easily technically good enough to do it: the only advantage I have is that I'm familiar with urllib3. =) I invite you to take a look at the issues I linked to above, and consider how you'd solve them. If you come up with a solution you like, open a pull request. I'm always happy for you to do the work instead of me!
",Lukasa,canibanoglu
1698,2013-10-24 09:37:41,"@drsm79 I agree that there's a documentation problem, but I don't think that is it. Let's start from the start. =)

The standard way to interact with Requests is to ignore `Request/PreparedRequest` objects altogether, and to just use the standard functional API (e.g. `Session.get()`, `Session.post()` etc.). However, some people want additional control over the form of their HTTP requests. For that reason, as part of 1.0, Kenneth made `Request` and `PreparedRequest` objects available. The contract for using them was this: if you use `PreparedRequest` objects, we aren't going to touch them. You give us a `PreparedRequest` and we send it, exactly as is. We don't apply any of our `Session`-level settings, because they might overwrite something you applied to the `PreparedRequest`. After all, that's what it's for!

In 1.0, the expected flow was what you laid out in your comment above.

Towards the 2.0 release date a few people mentioned that they'd like the ability to apply `Session` data before mutating their `PreparedRequest` objects. That seemed reasonable, so we added `Session.prepare_request()`. This essentially replaces the `Request.prepare()` call (though under the covers it does call that method) in your code. As part of what it does it applies relevant `Session` context, then returns you the `PreparedRequest` object to mutate at your leisure.

That's the difference between these two flows. `Request.prepare()` requires you to take full control over preparing your Request objects, including maintaining state. `Session.prepare_request()` does not require you to maintain as much state: we'll apply some for you.

`Session.send()` is a very thin wrapper over the underlying transport layer. It basically does connection pooling and that's about it.

I believe the problem with the documentation is that we don't adequately lay out both flows, and the reasons you'd do one instead of the other.
",Lukasa,drsm79
1698,2013-10-24 09:51:57,"Thanks @Lukasa that makes a lot of sense. I'll fix up the problem in my code, get it deployed and then try to make a patch to the docs covering what you've got above. 

I think Session.send() not applying things like the session auth is pretty counter intuitive. While I get why you'd want a nice thin wrapper there it needs to be clearer that its not actually going to use much of the session.
",drsm79,Lukasa
1696,2013-10-23 01:48:00,":+1: This is a no brainer

Good work @canibanoglu 
",sigmavirus24,canibanoglu
1694,2013-10-21 08:11:16,"@sigmavirus24 that one doesn't work when there's a referrer.
",kennethreitz,sigmavirus24
1693,2013-10-20 17:26:08,"I might go through and update all of these to use `#format` instead in the library. It is the ""future"" after all. Any objections @Lukasa @kennethreitz ?
",sigmavirus24,Lukasa
1693,2013-10-20 17:26:49,"And thanks @kevinburke. LGTM!
",sigmavirus24,kevinburke
1693,2013-10-21 10:10:39,"@sigmavirus24 Go for it. =)
",Lukasa,sigmavirus24
1691,2014-03-19 13:20:45,"@Lukasa do you think an IPv6 Transport Adapter would be a good addition to the [toolbelt](/sigmavirus24/requests-toolbelt)?
",sigmavirus24,Lukasa
1690,2013-10-19 05:25:58,"Thanks for the quick response @sigmavirus24! I think this could be very useful in bandwidth constrained environments but admittedly I'm unfamiliar with requests inner workings so I completely understand if it's unfeasible to implement. Let me know if this does become a possibility in the future roadmap. Happy to contribute any way I can.
",shrredd,sigmavirus24
1690,2013-10-19 07:05:58,"The answer is it's not in the roadmap. There are discussions afoot about removing urllib3's dependence on httplib, and if that ever happens it's a discussion we can reopen. However, as @sigmavirus24 quite rightly suggests, even if urllib3 could do it we wouldn't expose it. There's just not a nice way to do it.
",Lukasa,sigmavirus24
1687,2013-10-20 16:25:27,"@kevinburke I'm quite baffled by this. How do we not already have a custom theme? Certainly we share it with Flask and the other Flask related projects that use it (and anyone else who decides to use it contrary the license) but this is not a standard or built-in theme.

Also, why move off of Sphinx? We would give up the automatic updates provided by rtfd.org.
",sigmavirus24,kevinburke
1686,2013-10-29 18:24:01,"@rbtcollins Just to confirm, are you testing with a proxy?
",Lukasa,rbtcollins
1686,2013-10-30 08:13:42,"@Lukasa I misinterepreted and yes it was hanging.

The cause is as smoser says  it's a squid bug when you have a concurrent request for one url with the first reader stalled neither closing nor reading.

 This was caused because

cq = requests.get(sys.argv[1], stream=True)
cq.raw.read(1024)
cq.close()

doesn't actually close the socket for cq until you do 'del cq'. Thats a bug IMO.
",rbtcollins,Lukasa
1686,2013-10-30 10:03:30,"@smoser Good spot. We very deliberately do not close the connection. We provide connection-pooling via urllib3, because opening a new connection each time is nuts. =D
",Lukasa,smoser
1686,2013-10-30 11:47:04,"This is absolutely not a bug in requests. As @Lukasa rightly states this is a very intentional design decision.
",sigmavirus24,Lukasa
1685,2014-11-02 18:56:42,"@sigmavirus24 Wow.  Great work!   It looks like you may have pinpointed the hot spot in the code.
As for tracing the which object is responsible for the memory leak, you might get some extra hints by using objgraph like so:



Lemme know if I can help in any way.

-Matt
",mhjohnson,sigmavirus24
1685,2014-11-04 00:14:35,"@sigmavirus24 You're crushing it!  :)

Hmm... Python only releases memory to be reused by itself again, and the system doesn't get the memory back until the process terminates.  So, I would think the flatline you are seeing at 13.3MiB is probably indication there is not a memory leak present with urllib2, unlike with urllib3.

It would be nice to confirm that the problem can be isolated to urllib3.  Can you share the scripts you're using to test with urllib2?  
",mhjohnson,sigmavirus24
1685,2014-11-04 00:18:30,"So I'm starting to wonder if this doesn't have something to do with the `HTTPConnection` objects. If you do



The first three should print 1, the last 3. [1] I already identified that an HTTPConnection has `_HTTPConnection__response` which is a reference to `_original_response`. So I was expecting that number to be 3. What I cannot figure out is what is holding the reference to the 3rd copy. 

For further entertainment, add the following



to the beginning of the script. There are 2 unreachable objects after making the call to requests which is interesting, but nothing was uncollectable. If you add this to the script @mhjohnson provided and you filter the output for the lines with unreachable in them, you'll see that there are plenty of times where there are well over 300 unreachable objects. I don't yet know what the significance of unreachable objects it though. As always, I'll keep y'all posted.

[1]: `sys.getrefcount` always prints 1 more than the actual count. To verify this try: `o = object(); sys.getrefcount(o)`
",sigmavirus24,mhjohnson
1685,2014-11-04 00:19:28,"@mhjohnson to test urllib3, just replace your call to `requests.get` with `urllib2.urlopen` (also I should probably have been doing `r.read()` but I wasn't).
",sigmavirus24,mhjohnson
1685,2014-11-04 00:36:56,"So I took @mhjohnson's previous suggestion and used `objgraph` to figure out where the other reference was, but objgraph can't seem to find it. I added:



In the script 2 comments above and got the following: 
![requests](https://cloud.githubusercontent.com/assets/240830/4893019/97635df8-63ba-11e4-9518-c25704b4ce2e.png) which only shows that there would be 2 references to it. I wonder if there's something up with how `sys.getrefcount` works that's unreliable.
",sigmavirus24,mhjohnson
1685,2014-11-04 05:27:56,"@sigmavirus24 
Yeah, I got a little lost with that last graphic.  Probably because I don't know the code base very well, nor am I very seasoned on debugging memory leaks.  

Do you know which object this is that I am pointing at with the red arrow in this screenshot of your graphic? 
http://cl.ly/image/3l3g410p3r1C
",mhjohnson,sigmavirus24
1685,2014-11-04 05:43:48,"I was able to get the code to show the same slowly increasing memory usage
on python3 by replacing urllib3/requests with urllib.request.urlopen.

Modified code here: https://gist.github.com/kevinburke/f99053641fab0e2259f0

## 

Kevin Burke
phone: 925.271.7005 | twentymilliseconds.com

On Mon, Nov 3, 2014 at 9:28 PM, Matthew Johnson notifications@github.com
wrote:

> @sigmavirus24 https://github.com/sigmavirus24
> Yeah, I got a little lost with that last graphic. Probably because I don't
> know the code base very well, nor am I very seasoned on debugging memory
> leaks.
> 
> Do you know which object this is that I am pointing at with the red arrow
> in this screenshot of your graphic?
> http://cl.ly/image/3l3g410p3r1C
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/1685#issuecomment-61595362
> .
",kevinburke,sigmavirus24
1685,2014-11-04 06:11:23,"As far as I can tell making requests to a website that returns a
Connection: close header (for example https://api.twilio.com/2010-04-01.json)
does not increase the memory usage by a significant amount. The caveat is
there are multiple different factors and I'm just assuming it's a socket
related issue.

## 

Kevin Burke
phone: 925.271.7005 | twentymilliseconds.com

On Mon, Nov 3, 2014 at 9:43 PM, Kevin Burke kev@inburke.com wrote:

> I was able to get the code to show the same slowly increasing memory usage
> on python3 by replacing urllib3/requests with urllib.request.urlopen.
> 
> Modified code here:
> https://gist.github.com/kevinburke/f99053641fab0e2259f0
> 
> ## 
> 
> Kevin Burke
> phone: 925.271.7005 | twentymilliseconds.com
> 
> On Mon, Nov 3, 2014 at 9:28 PM, Matthew Johnson notifications@github.com
> wrote:
> 
> > @sigmavirus24 https://github.com/sigmavirus24
> > Yeah, I got a little lost with that last graphic. Probably because I
> > don't know the code base very well, nor am I very seasoned on debugging
> > memory leaks.
> > 
> > Do you know which object this is that I am pointing at with the red arrow
> > in this screenshot of your graphic?
> > http://cl.ly/image/3l3g410p3r1C
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > https://github.com/kennethreitz/requests/issues/1685#issuecomment-61595362
> > .
",kevinburke,sigmavirus24
1685,2014-11-04 15:04:03,"@mhjohnson that seems to be the number of references to the metatype `type` by `object` which is of type `type`. In other words, I think that's all the references of either `object` or `type`, but I'm not quite sure. Either way, if I try to exclude those, the graph becomes something like 2 nodes.
",sigmavirus24,mhjohnson
1685,2014-11-07 10:28:47,"After spending some time on this together with @mhjohnson, I can confirm @kevinburke theory related to the way GC treats the sockets on PyPy.

The 3c0b94047c1ccfca4ac4f2fe32afef0ae314094e commit is an interesting one. Specifically the line https://github.com/kennethreitz/requests/blob/master/requests/models.py#L736

Calling `self.raw.release_conn()` before returning content reduced significantly the used memory on PyPy, though there's still room for improvements.

Also, I think it would be better if we document the `.close()` calls that relate to the session and response classes, as also mentioned by @sigmavirus24. Requests users should be aware of those methods, because in most of the cases the methods are not called implicitly.

I also have a question and a suggestion related to the QA of this project. May I ask the maintainers why we don't use a CI to ensure the integrity of our tests? Having a CI would also allow us writing benchmark test cases where we can profile and keep a track of any performance/memory regressions.

A good example of such an approach can be found in the pq project:
https://github.com/malthe/pq/blob/master/pq/tests.py#L287

Thanks to everyone who jumped on this and decided to help!
We will keep investigating other theories causing this.
",stas,kevinburke
1685,2014-11-07 10:28:47,"After spending some time on this together with @mhjohnson, I can confirm @kevinburke theory related to the way GC treats the sockets on PyPy.

The 3c0b94047c1ccfca4ac4f2fe32afef0ae314094e commit is an interesting one. Specifically the line https://github.com/kennethreitz/requests/blob/master/requests/models.py#L736

Calling `self.raw.release_conn()` before returning content reduced significantly the used memory on PyPy, though there's still room for improvements.

Also, I think it would be better if we document the `.close()` calls that relate to the session and response classes, as also mentioned by @sigmavirus24. Requests users should be aware of those methods, because in most of the cases the methods are not called implicitly.

I also have a question and a suggestion related to the QA of this project. May I ask the maintainers why we don't use a CI to ensure the integrity of our tests? Having a CI would also allow us writing benchmark test cases where we can profile and keep a track of any performance/memory regressions.

A good example of such an approach can be found in the pq project:
https://github.com/malthe/pq/blob/master/pq/tests.py#L287

Thanks to everyone who jumped on this and decided to help!
We will keep investigating other theories causing this.
",stas,sigmavirus24
1685,2014-11-07 10:28:47,"After spending some time on this together with @mhjohnson, I can confirm @kevinburke theory related to the way GC treats the sockets on PyPy.

The 3c0b94047c1ccfca4ac4f2fe32afef0ae314094e commit is an interesting one. Specifically the line https://github.com/kennethreitz/requests/blob/master/requests/models.py#L736

Calling `self.raw.release_conn()` before returning content reduced significantly the used memory on PyPy, though there's still room for improvements.

Also, I think it would be better if we document the `.close()` calls that relate to the session and response classes, as also mentioned by @sigmavirus24. Requests users should be aware of those methods, because in most of the cases the methods are not called implicitly.

I also have a question and a suggestion related to the QA of this project. May I ask the maintainers why we don't use a CI to ensure the integrity of our tests? Having a CI would also allow us writing benchmark test cases where we can profile and keep a track of any performance/memory regressions.

A good example of such an approach can be found in the pq project:
https://github.com/malthe/pq/blob/master/pq/tests.py#L287

Thanks to everyone who jumped on this and decided to help!
We will keep investigating other theories causing this.
",stas,mhjohnson
1685,2014-11-07 13:35:55,"@stas I want to address one thing:

> Requests users should be aware of those methods, because in most of the cases the methods are not called implicitly.

Leaving PyPy aside for a moment, those methods shouldn't _need_ to be called explicitly. If the socket objects become unreachable in CPython they will get auto gc'd, which includes closing the file handles. This is not an argument to not-document those methods, but it is a warning to not focus overmuch on them.

We are meant to use a CI, but it appears to be unwell at the moment, and only @kennethreitz is in a position to fix it. He'll get to it when he has time. Note, however, that benchmark tests are extremely difficult to get right in a way that doesn't make them extremely noisy.
",Lukasa,stas
1685,2014-11-08 23:58:15,"@alex, 

I believe that @stas had used regular http (non-SSL/TLS) connection for this benchmark.  Just in case, I also used @stas's benchmark script and preformed it on my Mac (OSX 10.9.5 2.5 GHz i5 8 GB 1600 MHz DDR3) with a regular http connection.

If it helps, here are my results to compare (using your instructions):
https://gist.github.com/mhjohnson/a13f6403c8c3a3d49b8d

Let me know what you think.

Thanks,

-Matt
",mhjohnson,alex
1685,2014-11-08 23:58:15,"@alex, 

I believe that @stas had used regular http (non-SSL/TLS) connection for this benchmark.  Just in case, I also used @stas's benchmark script and preformed it on my Mac (OSX 10.9.5 2.5 GHz i5 8 GB 1600 MHz DDR3) with a regular http connection.

If it helps, here are my results to compare (using your instructions):
https://gist.github.com/mhjohnson/a13f6403c8c3a3d49b8d

Let me know what you think.

Thanks,

-Matt
",mhjohnson,stas
1685,2016-01-22 13:55:22,"@barroca that's a different issue. You're likely using a Session across threads and using `stream=True`. If you're closing a response before you've finished reading it, the socket is placed back into the connection pool with that data still in it (if I remember correctly). If that's not happening it's also plausible that you're picking up the most recent connection and receiving a cached response from the server. Either way, this is not an indication of a memory leak.
",sigmavirus24,barroca
1685,2016-01-22 14:50:09,"@sigmavirus24 Thanks Ian, It was some miss use of the Session across threads as you've mention. Thanks for the explanation and sorry for updating the wrong issue.
",barroca,sigmavirus24
1685,2016-01-22 15:06:56,"No worries @barroca :)
",sigmavirus24,barroca
1684,2013-10-18 07:47:17,"Unfortunately, users setting the `Content-Type` header explicitly seems very common. Equally common is users specifying `Host`, `Content-Length` and `User-Agent`, all of them unnecessarily.

With that said, I think I'm with @kennethreitz on this one. While I can't think of a reason a user would legitimately want to do this, they _might_ (I guess misbehaving servers could be a reason). If they do, we shouldn't be getting in their way.
",Lukasa,kennethreitz
1682,2013-10-17 14:30:49,"So if you were to set it to `None` as @Lukasa suggests, we would guess that it was encoded as `ISO-8859-2` instead of what Chrome detects it as.

I tried this on OSX w/ Python 2.7.5. It appears you're using 3.3 though and that may be entirely relevant too. I'll test with that too.

With Python 3.3, using `r.text` I too get the exception and the same guessed encoding. Setting `r.encoding` to `None` does not cause the exception to be raised though.

To be fair, I'm not sure the exception is entirely incorrect on Python's part. It seems like an extraordinary exception that they were not even planning on finding. That said, I'm sure they'll either update the documentation or they'll fix it to return either of those two errors (more likely LookupError).

I'm also bugging someone who can get your bug report out of the spam filter.
",sigmavirus24,Lukasa
1674,2013-10-16 15:36:06,"Context, (the original) pysnap seems to be a snapchat api wrapper that is no longer on GitHub or PyPI. I'd be interested to see what version of requests it is looking for.

@jkatzer `python -c 'import requests; print(requests.__version__)'` will give you the version of requests you're using.
",sigmavirus24,jkatzer
1674,2013-10-20 16:42:06,"@jkatzer can you tell us what `self.encoding` is when encountering this issue? Furthermore, can you tell us what `guess_json_utf(self.content)` returns? Something @mjpieters or @sburns contributed seems to be causing the issue here.

The stack trace seems to imply that `self.encoding` is `None` or some other falsey value (e.g., `''`) so we use `guess_json_utf` with `self.content`. `self.content` at that point is the raw bytes object we get from urllib3. So we use `self.content.decode(encoding)` which seems to be what's causing this issue. Judging by the stack trace (again) it seems that `guess_json_utf` is returning `utf8`.

One other note is that on requests master (on python 2.7), when I use `r.json()` the title of this issue comes back replaced like so: `u'""\u010d"" - UTF-8 UnicodeDecodeError'` which if I remember correctly is how the stdlib replaces errors and is a consequence of us always using `errors='replace'`. This suggests that the call to `str.decode` on line 692 needs an `errors='replace'` parameter passed in since that's what we do for `self.text`. 

Objections? I feel like using that particular option is a bad idea but we'd break the API were we to change it now.
",sigmavirus24,jkatzer
1672,2013-10-15 08:24:23,"@BernardoLima It shouldn't be. Requests Sessions are generally excellent at persisting cookies and connections. It would be interesting to see a sample of your code if possible (feel free to remove sensitive information e.g. posted data, usernames, passwords etc.)
",Lukasa,BernardoLima
1672,2013-10-15 15:21:30,"@Lukasa , thank you very much for your help.
I've create a test account so if you need, you can test the script.
I've made a few commentaries in the code, if you need anything, please tell me.

If you run over and over again, like ten times, it should has at least one fail attempt.

Here's it:
https://gist.github.com/BernardoLima/e4f7f8018e44f62393a5
",BernardoLima,Lukasa
1672,2013-10-16 03:33:31,"@BernardoLima one issue might be that you're setting the `Content-Length` header yourself. That's generally an awful idea. We'll do that for you and do it correctly. You should also _never_ set it on a Session object.

I would also suggest doing this



We will handle the urlencoding of everything for you and set the appropriate headers.
",sigmavirus24,BernardoLima
1672,2013-10-16 10:40:21,"@sigmavirus24 , I didn't knew it was automatic, I thought when you used HTTP requests you should specify every detail, thanks for the tip.

@Lukasa, yes, unfortunately I guess that it might be the only option, I'm going to test it in Chrome, maybe it happens there too.

Thanks for the help guys.

> > Edit:
> > Guys, it's defitnely a server problem, it happens also in Chorme, shame on you bad server.
",BernardoLima,Lukasa
1672,2013-10-16 10:40:21,"@sigmavirus24 , I didn't knew it was automatic, I thought when you used HTTP requests you should specify every detail, thanks for the tip.

@Lukasa, yes, unfortunately I guess that it might be the only option, I'm going to test it in Chrome, maybe it happens there too.

Thanks for the help guys.

> > Edit:
> > Guys, it's defitnely a server problem, it happens also in Chorme, shame on you bad server.
",BernardoLima,sigmavirus24
1671,2013-10-21 11:40:51,"@Lukasa I think so. My concern is that we keep special-casing areas of the library around this and I really really hate that.
",sigmavirus24,Lukasa
1671,2014-02-17 14:18:05,"@Lukasa do we still want to do this?

I can't think of a better name for the attribute frankly, but a better implementation might be:



That should be slightly faster than checking if the header is in the dictionary and then retrieving it. 

I don't like `asserted_host` at all.

I continue to be :-1: on changes like this though. Further, do we even do any of the certificate name checking or is that entirely in urllib3?
",sigmavirus24,Lukasa
1669,2013-10-13 15:07:17,"@Lukasa this definitely looks correct. If there's a better way to do this in urllib3, then I'll hold off on providing testing advice, especially since the best way I can think to test this particular bit is through a urllib3 inspired manner.
",sigmavirus24,Lukasa
1669,2013-10-14 00:07:16,"@kennethreitz the only thing failing in the tests are the regression tests I added to prevent cookies from being lost on authentication challenge responses. If you run the tests against a local HTTPBin instance (based off of master) then you'll see green. Regardless, I think this wouldn't hurt if we merged it early as a sort of bug fix. I would, however, like to see some of the functionality move up into urllib3 though.
",sigmavirus24,kennethreitz
1669,2013-10-14 00:08:51,"@sigmavirus24 :cake:
",kennethreitz,sigmavirus24
1666,2013-10-12 20:04:42,"We have a new, better plan, thanks to @sigmavirus24.
",Lukasa,sigmavirus24
1665,2013-10-12 19:56:18,"Here's a question:

If we use `self.content`, `self.text` is populated and in fact `self.text` is really a [method](https://github.com/kennethreitz/requests/blob/master/requests/models.py#L646..L677) being called on the Response object. Why do we need the `or` there? It doesn't prevent an exception caused in `self.text`, and the only (major) version of Python on which using `self.content` to parse JSON with is 2. Would it not simply be better to remove the `or self.content`? What purpose does it really serve there?

Let's assume that `self.text` is an empty string again, if on Python 3 you do: `json.loads('')` you receive an exception but it is a `ValueError`. This is ostensibly a bit less confusing than what @campadrenalin but it also prevents breaking the API in 2.1 with #1666. Technically this is the right behaviour (for `json.loads` to raise a `ValueError` when there is no JSON to be loaded).
",sigmavirus24,campadrenalin
1665,2013-10-12 20:04:11,"@sigmavirus24 Yeah, that's a much better way. Let's do that instead.
",Lukasa,sigmavirus24
1665,2013-10-12 20:11:38,":heart: @Lukasa 
",sigmavirus24,Lukasa
1663,2013-11-11 12:55:02,"@Lukasa, I'm having similar issue right now:



and this started to happen after 2.0.1 upgrade.
",dmakhno,Lukasa
1663,2013-11-11 13:20:52,"@dmakhno Really? Mine pass fine from 2.0.1. Can I see your full testing output?
",Lukasa,dmakhno
1663,2013-11-11 14:12:13,"Looks like the change where @daftshady wasn't expecting to be receiving CookieJar objects instead of dictionaries. There's a fix for this somewhere and should be in 2.0.2 then again, I can't be certain without confirmation from @dmakhno as to what `self.context.cookies` actually is.
",sigmavirus24,dmakhno
1663,2013-11-11 15:46:00,"I think it seems to be fault of my commit as @sigmavirus24 said.
I already attached pull request for that issue in #1713.
That will be fixed in 2.0.2 by merging my pull request or some better way to fix it.
Sorry for the inconvenience.
",daftshady,sigmavirus24
1662,2013-10-10 22:47:16,"Hi, @voberoi!

It is a [Dream Come True](http://www.youtube.com/watch?v=HH5lEIhjOHY).
",mattspitz,voberoi
1662,2013-10-10 22:47:54,"@mattspitz you're just filled with those youtube videos, aren't you? :P
",kennethreitz,mattspitz
1662,2013-10-10 22:49:53,"@mattspitz writes the best test cases.
",voberoi,mattspitz
1659,2013-10-24 11:13:13,"@dstufft requests explicitly prefers not to rely on the system certs due to massive compatibility headaches the variance in distros often cause. I believe we have a utility function to enable this functionality built-in though. Will update shortly.
",kennethreitz,dstufft
1659,2013-10-24 11:20:11,"@dstufft one thing to clarify here: historically, we only provide security releases for the latest release. We now have rigid API stability, so this shouldn't effect Pip at all. But, it is something to be aware of. 
",kennethreitz,dstufft
1659,2013-10-24 11:28:30,"@dstufft are you aware of any sort of alert system for cert blacklisting? Is it possible to automate this? 

I may make a simple service for this.
",kennethreitz,dstufft
1659,2013-10-24 11:34:32,"@dstufft interesting. What ""transformations"" need to be done against Mozilla's raw cert file?

http://mxr.mozilla.org/mozilla/source/security/nss/lib/ckfw/builtins/certdata.txt

I believe the workflow I used was taking the certs provided by [curl's caextract](http://curl.haxx.se/docs/caextract.html) and manually removing blacklisted certs.
",kennethreitz,dstufft
1659,2013-10-24 11:35:52,"Go is perfect if it functions properly. @dstufft if you're up for it, could you vet it?
",kennethreitz,dstufft
1659,2013-10-24 11:41:05,"You know who does know go, right? @Lukasa that's who. ;)
",sigmavirus24,Lukasa
1659,2013-10-24 12:07:18,"Tentative new cert is available here:

http://ci.kennethreitz.org/job/ca-bundle/lastSuccessfulBuild/artifact/certs.pem

+1 from @dstufft, @agl, or @tiran would be appreciated :)

Once I receive a +1, a new release will be cut.
",kennethreitz,dstufft
1659,2013-10-24 12:36:28,"@tiran Really? I don't know my PEM well enough, but it looks fine to me based on that Gist...
",Lukasa,tiran
1659,2013-10-24 13:11:28,"@tiran noticed the CVS_ID was missing, I guessed my way to https://gist.github.com/dstufft/7137007
",dstufft,tiran
1659,2013-10-24 13:15:06,"@dstufft Exactly what I did.https://github.com/Lukasa/extract-nss-root-certs/commit/cdead23c389fde8cf7c0d9648b4b52b891fa8d93
",Lukasa,dstufft
1659,2013-10-24 13:19:19,"Agreed. =) I think that @tiran's solution is the best for Pull-Request action, because it's a lot smarter than mine. =) Gotta fix up the slightly inconsistent indentation in it though (L162).
",Lukasa,tiran
1659,2013-10-24 13:21:37,"@kennethreitz If you're interested, I can try to wrap this code up into a nice binary that does all of the work itself: run one command and you're done. Sound good?
",Lukasa,kennethreitz
1659,2013-10-24 13:36:49,"@Lukasa i think this will work fine, but i won't stop you :)
",kennethreitz,Lukasa
1657,2013-10-11 01:49:53,"@Lukasa any further review on the code? 

In reality, if we change the future behaviour it's a really simple way of actually merging the two sets of hooks. I had that in my first stab at this [here](https://github.com/sigmavirus24/requests/commit/5bf396d5d12bf9debc667a509c84f640560da517#diff-28e67177469c0d36b068d68d9f6043bfR84).
",sigmavirus24,Lukasa
1657,2013-11-27 14:41:57,"@Lukasa @kennethreitz can we get this merged soon? I just realized I'll probably need this for github3.py so that I don't have to do [this](https://github.com/sigmavirus24/github3.py/commit/9a1b4d892a31bd12ee477f39caada76670cfc062#diff-8a0b7651f0a27fb4490e3f2d87206eedR67) ;)
",sigmavirus24,Lukasa
1655,2013-10-07 18:03:12,"@mgax can you verify this commit fixes this for you? https://github.com/sigmavirus24/requests/commit/5bf396d5d12bf9debc667a509c84f640560da517
",sigmavirus24,mgax
1655,2013-10-07 18:18:54,"@sigmavirus24 :+1:
",mgax,sigmavirus24
1654,2013-10-05 14:38:33,"I think a lot of negativity in this discussion comes from language 
barriers. Even though i agree with @sigmavirus24 about the incorrect 
description, the resulting diff looks reasonable, and i'd like to see 
these changes in requests, regardless of how they're ""described"" or 
""labelled"".
",untitaker,sigmavirus24
1654,2013-10-05 14:49:11,"@untitaker I was not objecting to the changes themselves. I have far more to maintain than just requests and I do far more offline than anyone really knows or cares to know about. @kennethreitz has an ostensibly similar schedule (which is why he has minions) and having exact language to review in an email makes our lives far easier. Stressing that point with @riyadparvez will only make future pull requests to this and other projects on his behalf better and perchance allow them to be merged in a quicker fashion.
",sigmavirus24,riyadparvez
1654,2013-10-05 14:49:11,"@untitaker I was not objecting to the changes themselves. I have far more to maintain than just requests and I do far more offline than anyone really knows or cares to know about. @kennethreitz has an ostensibly similar schedule (which is why he has minions) and having exact language to review in an email makes our lives far easier. Stressing that point with @riyadparvez will only make future pull requests to this and other projects on his behalf better and perchance allow them to be merged in a quicker fashion.
",sigmavirus24,untitaker
1654,2013-10-05 16:55:25,"LGTM.

@riyadparvez if you feel that strongly about `BaseException` then feel free to submit a PR to shazow/urllib3
",sigmavirus24,riyadparvez
1654,2013-10-05 17:02:18,"Cool, this is great. Thanks @riyadparvez! :cake:
",Lukasa,riyadparvez
1652,2013-10-04 13:52:06,"@sigmavirus24 How would you handle the use case in the issue then (you want to use generators, but avoid chunked transfer encoding)? The Content-Length is valuable information, why should it be impossible to transmit? There is no way ""requests"" can know how much data the generator outputs. I suggest that ""requests"" throws an Exception if the generator generates less/more bytes than the specified Content-Length says. Of course, the data will already be sent by then, but at least the error will probably get discovered before production, and people will be aware of the fact that it needs to generate the right amount of bytes.
",ysangkok,sigmavirus24
1652,2013-10-04 15:00:56,"@ysangkok The use case in the issue is just an example of the behaviour. There is no justification there for why you're doing what you're doing. If you want to use a generator but don't want to send the `Transfer-Encoding` header I suggest you not special case and use magic behaviour to achieve what you want but instead use the features we already provide.

Don't just use the plain requests API, instead prepare a request yourself (which is now infinitely easier) and remove the header. If you want to set your own `Content-Length` header then it will already be there. Take greater advantage of the existing API.

Furthermore, requests doesn't count the bytes that you send, ever, unless it's determining the `Content-Length` header. If you're concerned about what you're sending and possibly sending too much you should probably be testing your code better. Tests would catch this for you, it isn't our responsibility.
",sigmavirus24,ysangkok
1652,2015-08-19 12:51:40,"@Bluehorn you failed to provide us:
- Code to reproduce your issue
- The versions you tested
- A server to test against

Given you've provided us nothing to work with, we can't help you.
",sigmavirus24,Bluehorn
1652,2015-08-24 21:18:03,"@sigmavirus24: I do not need help, I have a trivial patch that makes requests use the Content-Length again. After our code base works with the current requests for a while we will probably just do what you suggested:

> Don't just use the plain requests API, instead prepare a request yourself (which is now infinitely easier) and remove the header

I just liked the approach that the caller can set the headers without having requests overwrite/delete them better.
",Bluehorn,sigmavirus24
1650,2013-10-04 06:15:00,"@GrahamDumpleton is the best type of contributor.
",kennethreitz,GrahamDumpleton
1650,2013-10-04 14:12:42,"@GrahamDumpleton is the best kind of everything
",sigmavirus24,GrahamDumpleton
1648,2013-10-04 20:16:23,"@sigmavirus24 You're not using Requests 2.0.0 on your Python 3.3 installation.
",ysangkok,sigmavirus24
1648,2013-10-04 21:02:09,"@ysangkok good point. :-) With 2.0 it's definitely broken. Regardless my opinion is definitely in the court of us removing the `Content-Length` header.
",sigmavirus24,ysangkok
1648,2013-10-05 05:59:56,"You're probably right @sigmavirus24, we should probably strip the Content-Length header. 
",Lukasa,sigmavirus24
1648,2013-10-05 15:01:27,"Hm, I'm not entirely convinced of my own argument anymore. I went looking for the old discussions surrounding users setting the `Content-Length` header themselves and I must be remembering old IRC conversations.

I'm still of the opinion that users should really not be setting those headers themselves and that we should remove them, however, I think this issue points out a far more important issue, which is the vastly different behaviour of requests on two different versions of Python.

On 2.7 (as I demonstrated) setting the `Content-Length` header does nothing to change how requests uploads the data. On 3.3, however, @ysangkok is correct that setting it sends everything as soon as it can (it still uses the generator but does not send it in an actually chunked manor). 

One easy way to fix this is to remove the header when using a generator (or always to provide a consistent behaviour), the flaw with this is that this is backwards incompatible behaviour.

The other easy way is to break the consistency of the API by not always using chunked transfer encoding with a generator. @Lukasa this definitely needs some deeper thought since I can't find the old conversations where users were admonished for setting that header themselves.

To be honest though, I would never expect that setting a header would change the behaviour of using a generator though.

This certainly is an extremely sticky situation
",sigmavirus24,ysangkok
1648,2013-10-05 15:01:27,"Hm, I'm not entirely convinced of my own argument anymore. I went looking for the old discussions surrounding users setting the `Content-Length` header themselves and I must be remembering old IRC conversations.

I'm still of the opinion that users should really not be setting those headers themselves and that we should remove them, however, I think this issue points out a far more important issue, which is the vastly different behaviour of requests on two different versions of Python.

On 2.7 (as I demonstrated) setting the `Content-Length` header does nothing to change how requests uploads the data. On 3.3, however, @ysangkok is correct that setting it sends everything as soon as it can (it still uses the generator but does not send it in an actually chunked manor). 

One easy way to fix this is to remove the header when using a generator (or always to provide a consistent behaviour), the flaw with this is that this is backwards incompatible behaviour.

The other easy way is to break the consistency of the API by not always using chunked transfer encoding with a generator. @Lukasa this definitely needs some deeper thought since I can't find the old conversations where users were admonished for setting that header themselves.

To be honest though, I would never expect that setting a header would change the behaviour of using a generator though.

This certainly is an extremely sticky situation
",sigmavirus24,Lukasa
1648,2013-11-15 12:55:21,"@bryanhelmig does the server you're uploading to require you to send the Content-Length header?
",sigmavirus24,bryanhelmig
1648,2013-11-15 16:15:58,"@bryanhelmig did you see the comments in the linked pull request?

Anyway, I don't understand why Content-Transfer-Encoding isn't just a flag. No need to delete any headers (or do any other kind of hand-holding), it was never a question of the Content-Length being right or wrong, the actual issue is that the presence of Content-Length semi-disables Content-Transfer-Encoding, which makes no sense at all. But just making Requests ignore Content-Length is not solving the real problem, which is, that Requests uses Content-Transfer-Encoding when it feels like it (sounds like that is supposed to be when reading from a generator), even though many web servers don't even support it.

Ignoring Content-Length will confuse people who supply it. If you (@sigmavirus24) insist on not transmitting it, why not just throw an exception? As you said, this functionality is probably not used widely.

In the pull request, you said ""The use case in the issue is just an example of the behaviour. There is no justification there for why you're doing what you're doing."". I disagree, I think the original code in this issue is perfectly normal behaviour, and in fact I think that streaming of POST data is a huge use case, and that it's ridiculous if one is forced to use Content-Transfer-Encoding or resort to lower-level libraries when streaming/using generators.

So to summarize: Content-Transfer-Encoding should be a flag, illegal parameter combinations should provoke exceptions, and user-supplied flags should be sent when possible. And of course, it shouldn't be possible to semi-disable Content-Transfer-Encoding.
",ysangkok,sigmavirus24
1648,2013-11-15 16:15:58,"@bryanhelmig did you see the comments in the linked pull request?

Anyway, I don't understand why Content-Transfer-Encoding isn't just a flag. No need to delete any headers (or do any other kind of hand-holding), it was never a question of the Content-Length being right or wrong, the actual issue is that the presence of Content-Length semi-disables Content-Transfer-Encoding, which makes no sense at all. But just making Requests ignore Content-Length is not solving the real problem, which is, that Requests uses Content-Transfer-Encoding when it feels like it (sounds like that is supposed to be when reading from a generator), even though many web servers don't even support it.

Ignoring Content-Length will confuse people who supply it. If you (@sigmavirus24) insist on not transmitting it, why not just throw an exception? As you said, this functionality is probably not used widely.

In the pull request, you said ""The use case in the issue is just an example of the behaviour. There is no justification there for why you're doing what you're doing."". I disagree, I think the original code in this issue is perfectly normal behaviour, and in fact I think that streaming of POST data is a huge use case, and that it's ridiculous if one is forced to use Content-Transfer-Encoding or resort to lower-level libraries when streaming/using generators.

So to summarize: Content-Transfer-Encoding should be a flag, illegal parameter combinations should provoke exceptions, and user-supplied flags should be sent when possible. And of course, it shouldn't be possible to semi-disable Content-Transfer-Encoding.
",ysangkok,bryanhelmig
1648,2013-11-15 17:06:24,"Stop stop stop.

Everyone take a breather.

@ysangkok You can do streaming uploads without generators just fine. Provide Requests a file-like object in the data parameter and that will work. Yes, it's not as simple as using a generator, but that's ok because it's still not very hard.

In the meantime, Requests should not suggest that it is chunking data when it does not. We are all agreed on that. The question is what we should do _in your specific case_: namely, providing a generator and a `Content-Length`. You and @sigmavirus24 legitimately disagree on this issue, _which is fine_. However, can we all please acknowledge that both camps have rational reasons to expect their position?

@ysangkok You've said that ""Ignoring Content-Length will confuse people who supply it."" @sigmavirus24 contends that [ignoring the very clear documentation](http://docs.python-requests.org/en/latest/user/advanced/#chunk-encoded-requests) when provided with a generator will confuse people who do _that_. You are both right.

(As a side note, the fact that many servers don't understand `Transfer-Encoding` is just a wild assertion based on no evidence that I've seen. Until any evidence is provided, I'm choosing to ignore it.)

One way or another we're going to have to pick what we do here. It's possible that the correct decision is to throw an exception when both a generator and `Content-Length` are provided. That's viable. It doesn't even make @bryanhelmig's case worse, because he should just be passing `response.raw` straight through rather than wrapping it in a decorator ([for your benefit, Bryan](http://docs.python-requests.org/en/latest/user/advanced/#streaming-uploads)).

I'm naturally inclined to sit on the fence here and throw a `YoureACrazyPerson` exception, but I can see why both of you believe what you believe. In particular, making decisions based on user-supplied headers is lame and confusing, and we should try not to do that. The following, however, are hard lines:
1. Controlling `Transfer-Encoding` will not be a flag. Not now, not ever. Requests does not do tiny special case flags like this.
2. We cannot do nothing.
3. Requests is _not_ obliged to support all use cases. I will happily throw either use case under a bus if it makes the API better.
",Lukasa,ysangkok
1648,2013-11-15 17:06:24,"Stop stop stop.

Everyone take a breather.

@ysangkok You can do streaming uploads without generators just fine. Provide Requests a file-like object in the data parameter and that will work. Yes, it's not as simple as using a generator, but that's ok because it's still not very hard.

In the meantime, Requests should not suggest that it is chunking data when it does not. We are all agreed on that. The question is what we should do _in your specific case_: namely, providing a generator and a `Content-Length`. You and @sigmavirus24 legitimately disagree on this issue, _which is fine_. However, can we all please acknowledge that both camps have rational reasons to expect their position?

@ysangkok You've said that ""Ignoring Content-Length will confuse people who supply it."" @sigmavirus24 contends that [ignoring the very clear documentation](http://docs.python-requests.org/en/latest/user/advanced/#chunk-encoded-requests) when provided with a generator will confuse people who do _that_. You are both right.

(As a side note, the fact that many servers don't understand `Transfer-Encoding` is just a wild assertion based on no evidence that I've seen. Until any evidence is provided, I'm choosing to ignore it.)

One way or another we're going to have to pick what we do here. It's possible that the correct decision is to throw an exception when both a generator and `Content-Length` are provided. That's viable. It doesn't even make @bryanhelmig's case worse, because he should just be passing `response.raw` straight through rather than wrapping it in a decorator ([for your benefit, Bryan](http://docs.python-requests.org/en/latest/user/advanced/#streaming-uploads)).

I'm naturally inclined to sit on the fence here and throw a `YoureACrazyPerson` exception, but I can see why both of you believe what you believe. In particular, making decisions based on user-supplied headers is lame and confusing, and we should try not to do that. The following, however, are hard lines:
1. Controlling `Transfer-Encoding` will not be a flag. Not now, not ever. Requests does not do tiny special case flags like this.
2. We cannot do nothing.
3. Requests is _not_ obliged to support all use cases. I will happily throw either use case under a bus if it makes the API better.
",Lukasa,sigmavirus24
1648,2013-11-15 17:06:24,"Stop stop stop.

Everyone take a breather.

@ysangkok You can do streaming uploads without generators just fine. Provide Requests a file-like object in the data parameter and that will work. Yes, it's not as simple as using a generator, but that's ok because it's still not very hard.

In the meantime, Requests should not suggest that it is chunking data when it does not. We are all agreed on that. The question is what we should do _in your specific case_: namely, providing a generator and a `Content-Length`. You and @sigmavirus24 legitimately disagree on this issue, _which is fine_. However, can we all please acknowledge that both camps have rational reasons to expect their position?

@ysangkok You've said that ""Ignoring Content-Length will confuse people who supply it."" @sigmavirus24 contends that [ignoring the very clear documentation](http://docs.python-requests.org/en/latest/user/advanced/#chunk-encoded-requests) when provided with a generator will confuse people who do _that_. You are both right.

(As a side note, the fact that many servers don't understand `Transfer-Encoding` is just a wild assertion based on no evidence that I've seen. Until any evidence is provided, I'm choosing to ignore it.)

One way or another we're going to have to pick what we do here. It's possible that the correct decision is to throw an exception when both a generator and `Content-Length` are provided. That's viable. It doesn't even make @bryanhelmig's case worse, because he should just be passing `response.raw` straight through rather than wrapping it in a decorator ([for your benefit, Bryan](http://docs.python-requests.org/en/latest/user/advanced/#streaming-uploads)).

I'm naturally inclined to sit on the fence here and throw a `YoureACrazyPerson` exception, but I can see why both of you believe what you believe. In particular, making decisions based on user-supplied headers is lame and confusing, and we should try not to do that. The following, however, are hard lines:
1. Controlling `Transfer-Encoding` will not be a flag. Not now, not ever. Requests does not do tiny special case flags like this.
2. We cannot do nothing.
3. Requests is _not_ obliged to support all use cases. I will happily throw either use case under a bus if it makes the API better.
",Lukasa,bryanhelmig
1648,2013-11-15 19:41:53,"> does the server you're uploading to require you to send the Content-Length header?

@sigmavirus24 it does. :-( 411 for attempts without a Content-Length. Quite annoying IMO.

> did you see the comments in the linked pull request?

@ysangkok I did.

> he should just be passing response.raw straight through rather than wrapping it in a decorator

@Lukasa That was my original edit actually, but I'm not sure what that nets us in this case besides a chance to link to the docs. Unless I am mistaken, a file object isn't guaranteed to give you a length, generators are just guaranteed to _not_ give a length. I noticed in testing that small files didn't get chucked, a generator forced it.

Thanks for all the thorough replies everyone. Really though, it is fine if users in exotic situations have to swap to a different lib for one out of a 100 requests. Life is much easier for the 99 other cases.
",bryanhelmig,ysangkok
1648,2013-11-15 19:41:53,"> does the server you're uploading to require you to send the Content-Length header?

@sigmavirus24 it does. :-( 411 for attempts without a Content-Length. Quite annoying IMO.

> did you see the comments in the linked pull request?

@ysangkok I did.

> he should just be passing response.raw straight through rather than wrapping it in a decorator

@Lukasa That was my original edit actually, but I'm not sure what that nets us in this case besides a chance to link to the docs. Unless I am mistaken, a file object isn't guaranteed to give you a length, generators are just guaranteed to _not_ give a length. I noticed in testing that small files didn't get chucked, a generator forced it.

Thanks for all the thorough replies everyone. Really though, it is fine if users in exotic situations have to swap to a different lib for one out of a 100 requests. Life is much easier for the 99 other cases.
",bryanhelmig,Lukasa
1648,2013-11-15 19:41:53,"> does the server you're uploading to require you to send the Content-Length header?

@sigmavirus24 it does. :-( 411 for attempts without a Content-Length. Quite annoying IMO.

> did you see the comments in the linked pull request?

@ysangkok I did.

> he should just be passing response.raw straight through rather than wrapping it in a decorator

@Lukasa That was my original edit actually, but I'm not sure what that nets us in this case besides a chance to link to the docs. Unless I am mistaken, a file object isn't guaranteed to give you a length, generators are just guaranteed to _not_ give a length. I noticed in testing that small files didn't get chucked, a generator forced it.

Thanks for all the thorough replies everyone. Really though, it is fine if users in exotic situations have to swap to a different lib for one out of a 100 requests. Life is much easier for the 99 other cases.
",bryanhelmig,sigmavirus24
1648,2013-11-17 17:19:37,"@piotr-dobrogost Agreed, but if making a hard thing possible requires making an easy thing harder, we'd rather leave the easy thing easy. =)
",Lukasa,piotr-dobrogost
1648,2013-11-17 19:30:06,"A few things:

As far as I'm concerned, this has been (and will continue to be, until we come to a decision) undefined behaviour

> As a side note, the fact that many servers don't understand Transfer-Encoding is just a wild assertion based on no evidence that I've seen. Until any evidence is provided, I'm choosing to ignore it.

There's a difference between servers not understanding a chunked Transfer-Encoding and servers not wanting to respect it. I suspect the latter is the case in @bryanhelmig's case. Alternatively, the application could have been written by someone who doesn't understand or know about Transfer-Encoding and so requires a Content-Length.

> It's possible that the correct decision is to throw an exception when both a generator and Content-Length are provided. That's viable.

Exceptions might be too extreme in this case. We don't generally raise exceptions for anything other than invalid URLs. The way we process the data and files parameters can raise exceptions but we don't special case anything there. That said, we've been talking about how poor an idea it is when users specify their own Content-Length and Host headers (among others which I'm probably forgetting). Since these aren't technically invalid practices, but rather practices we advise against, I suggest that we instead trigger a warning and then do the right thing in certain well documented situations.
- If we receive a Host header we raise a warning but do not delete it.
- If we receive a object whose size we can determine and the Content-Length header is provided, we should raise a warning that providing it in such a case they shouldn't do so. I'm not certain if we should override their setting though.
- If we receive a generator and a Content-Length header, we should raise a warning and remove the header.

Using warnings is a more gentle way to inform the user what they're doing is not advisable. It also covers the fact that so many of our users do not seem to bother to read the documentation and so the library becomes self-documenting of sorts. This also gives us the ability to change the behaviour in the future. And even better, if users want to disable the warnings, they can because python provides a way of silencing warnings.
",sigmavirus24,bryanhelmig
1648,2013-11-17 20:14:55,"> There's a difference between servers not understanding a chunked Transfer-Encoding and servers not wanting to respect it. I suspect the latter is the case in @bryanhelmig's case. Alternatively, the application could have been written by someone who doesn't understand or know about Transfer-Encoding and so requires a Content-Length.

I actually think this is very likely the case in most of my examples. We (@zapier) attempt to upload files to over a dozen different APIs and the few of them that require Content-Length (seem to) timeout with chunked Transfer-Encoding.

> As a side note, the fact that many servers don't understand Transfer-Encoding is just a wild assertion based on no evidence that I've seen. Until any evidence is provided, I'm choosing to ignore it.

I could put together a test suite of how various services respond to Content-Length/Transfer-Encoding, but I feel like even if it is incorrectly implemented APIs, that shouldn't really advise the design of python-requests. Easier still, I could just name names based on my experience fighting this for the last week, but again, if they are API/server bugs, what is the use of such information to python-requests?

> I suggest that we instead trigger a warning and then do the right thing in certain well documented situations.

Agree on default behavior, but sometimes reality trumps the ""right thing"" (especially when you have no control over a potentially broken server). It might be nice to document a technique to override (even if it advocates for the user to do a lot of work, like writing a custom Adapter).
",bryanhelmig,bryanhelmig
1648,2013-11-18 08:42:57,"Bleh. This makes me sad.

Ok, I think @sigmavirus24's plan is the best one here, at least for now.
",Lukasa,sigmavirus24
1648,2014-02-18 07:49:58,"I have the same exact use case : uploading data to a server that does not support chunked encoding. Data length is known in advance, but does not come from a file (from a generator).
I expected that setting a content length header would disable length calculation in requests, and also disable chunked transfer encoding.
By now I manage to work-around this problem by adding an additional 'len' attribute to my generator object, so that requests utils.super_len() returns something so chunked encoding is not chosen by requests : this is ugly and brillant.
As a unix user (and like @piotr-dobrogost ), I would expect that programmer knows what he does, and library should obey : providing a content-length header is a clear indication that chunked encoding is not to be used. But this conflicts with your sentence above : ""I would never expect that setting a header would change the behaviour of using a generator though"". Well, if is clearly documented I don't see the point. It wouln't break API, would it ?
",netheosgithub,piotr-dobrogost
1648,2014-02-18 08:45:25,"@netheosgithub I'd argue that changing this behaviour absolutely constitutes a change in the API. Consider the current state of the API, [from the documentation](http://docs.python-requests.org/en/latest/user/advanced/#chunk-encoded-requests):

> Requests also supports Chunked transfer encoding for outgoing and incoming requests. To send a chunk-encoded request, simply provide a generator (or any iterator without a length) for your body.

Note that the documentation _doesn't say_ ""To send a chunk-encoded request, simply provide a generator and do not set the Content-Length header."" This makes this an API change in my book. Not just any API change: a bad one. The idea that setting a header changes the body of the request makes me feel deeply uncomfortable. Consider if someone asked us for a similar request whereby if they set the header `Content-Type: application/json`, we should JSON-encode the `data` parameter instead of form-encoding it! I'd throw that request out in a heartbeat.

I think we should try to address the root of the problem: why are people using generators in this situation?
",Lukasa,netheosgithub
1648,2014-02-18 09:18:41,"@bryanhelmig Mm, yes. I think the correct solution there is from now on to use the [requests toolbelt](https://github.com/sigmavirus24/requests-toolbelt)'s streaming multipart encoder.

Right now though, it's unclear to me why there is such strong resistance to using file-like objects here.
",Lukasa,bryanhelmig
1648,2014-02-19 07:45:48,"@gholms Nope. `sys.stdin` is so _not_ file-like it triggered chunked encoding. If you pass us a file object we will stream it, not chunk it. The problem with `sys.stdin` is that it has no length, which falls under the description provided in the docs as mentioned above:

> any iterator without length

The general expectation is that users will never explicitly set the Content-Length header. This is because Requests may end up messing with how the request body is set up.

Finally, the idea that 'one might expect' that providing a Content-Length header will disable chunking isn't true either: I'd expect us to remove the header. =)
",Lukasa,gholms
1648,2014-02-19 08:50:32,"Anyway, this discussion has gone on long enough. When provided with a lengthless iterator and a user-supplied Content-Length header, we have these options:
1. Raise an exception.
2. Blow away the Content-Length header
3. Don't chunk.

Any of these three options brings us into compliance with the RFC. It is clear that everyone raising this issue prefers (3). @sigmavirus24?
",Lukasa,sigmavirus24
1648,2014-02-20 19:12:36,"I'm inclined to agree with you @sigmavirus24. I'll see if I can get a few minutes with Kenneth at some point soon to talk this over with him.
",Lukasa,sigmavirus24
1648,2014-05-12 08:41:43,"I'm in the same boat as @bryanhelmig and @netheosgithub, I have a generator where I know in advance what size the combined chunks will have, and have a server that does not support chunked uploads (a WSGI app, WSGI according to my research does not support chunked encoding at all). The data from the generator is too big to fit into RAM, so combining the chunks before and passing it to requests is out of the question.
Has there been any new development regarding this issue?
",jbaiter,netheosgithub
1648,2014-05-12 08:41:43,"I'm in the same boat as @bryanhelmig and @netheosgithub, I have a generator where I know in advance what size the combined chunks will have, and have a server that does not support chunked uploads (a WSGI app, WSGI according to my research does not support chunked encoding at all). The data from the generator is too big to fit into RAM, so combining the chunks before and passing it to requests is out of the question.
Has there been any new development regarding this issue?
",jbaiter,bryanhelmig
1648,2014-05-12 13:36:44,"@jbaiter then what you need is to pass an object that behaves like a file with a `__len__` defined. Take, for example, [the toolbelt's MultipartEncoder](https://gitlab.com/sigmavirus24/toolbelt/blob/master/requests_toolbelt/multipart/encoder.py#L19). You want streaming, so in general all you need is something like this which has a `read` method and a way of determining its length (preferably by implementing `__len__`). Your object's API could look something like:



Granted I haven't tried to see if that code will work, but something along those lines should.
",sigmavirus24,jbaiter
1648,2014-05-12 13:51:02,"Thanks @sigmavirus24 :+1:, I did precisely that, i was just wondering if by now there was a more elegant way to go about it
",jbaiter,sigmavirus24
1648,2014-05-12 13:54:33,"Perhaps there's a good way of providing this via the toolbelt, eh @Lukasa ?
",sigmavirus24,Lukasa
1648,2014-05-12 18:06:44,"@sigmavirus24 Absolutely. =)
",Lukasa,sigmavirus24
1648,2014-05-13 02:33:40,"@Lukasa any feedback on https://gitlab.com/sigmavirus24/toolbelt/merge_requests/2
",sigmavirus24,Lukasa
1648,2016-03-25 10:18:39,"@sigmavirus24 ""We're not doing anything wrong by sending it regardless of the encoding, the server is doing the wrong thing by not ignoring it."" is actually now wrong:

RFC 7230: ""A sender MUST NOT send a Content-Length header field in any message that contains a Transfer-Encoding header field.""
",ztane,sigmavirus24
1648,2016-03-25 14:06:27,"@ztane this is a different case than what that stack overflow question is referring to. In the future, please closely read a discussion before generating notifications for all of its participants.
",sigmavirus24,ztane
1648,2016-03-26 19:16:34,"@sigmavirus24 that was caused by ""transfer-Encoding: chunked, content-length set => body not chunked."" And RFC 7230 **forbids setting Content-Length with Transfer-Encoding**.
",ztane,sigmavirus24
1648,2016-03-28 15:16:02,"@ztane thank you for continuing to not read this discussion. This particular bug is about someone setting a Content-Length header by hand and expecting requests to not use a chunked upload when they provide a generator (which always triggers a chunked upload with requests). The stackoverflow link you provided and what you're talking about is a different bug that is being handled elsewhere. You're muddying this discussion with irrelevant details because we allow users to shoot themselves in the feet. For example, we allow users to specify an incorrect content-length header or invalid host header, or basically whatever they please. We don't police everything nor will we. Please focus your conversation on the _correct_ issue in the future.
",sigmavirus24,ztane
1648,2016-05-06 20:58:47,"@timuralp I remain opposed to adding a flag for this. It's really just unacceptable to have a HTTP/1.1 implementation that cannot handle chunked transfer encoding in 2016. It's been a specification requirement for so long that the first specification that required it is nearly old enough to vote in the United States of America: I don't think we can keep cutting entities slack for not doing it.

From my perspective the bug here remains that we may incorrectly emit both Content-Length and Transfer-Encoding. Of course, my perspective is non-binding. ;)
",Lukasa,timuralp
1648,2016-05-06 22:15:03,"@Lukasa @sigmavirus24 fair enough -- thanks for the prompt reply. I'll continue to look to fix the boto issue in that project.
",timuralp,Lukasa
1648,2016-05-06 22:15:03,"@Lukasa @sigmavirus24 fair enough -- thanks for the prompt reply. I'll continue to look to fix the boto issue in that project.
",timuralp,sigmavirus24
1647,2013-10-05 05:47:17,"@t-8ch It's very possible that it's not allowed unencoded, I didn't check, I just checked if urlparse would split it.
",Lukasa,t-8ch
1647,2013-10-05 07:40:45,"Scumbag urlparse, ruining other libraries from the grave. :P 

@t-8ch Worth removing what I just added? Seems harmless. (/me imagines urlparse authors saying this a decade ago.) 
",shazow,t-8ch
1647,2013-10-05 08:53:52,"@shazow Doesn't seem to hurt.

Using `str.rsplit` would be easier on the eyes though :smiley:


",t-8ch,shazow
1647,2013-10-05 09:00:36,"@t-8ch Ugh I literally spent 5 minutes yesterday looking for that method, and gave up. Thank you. :) My venture into golangland has ruined my pyfu. Fixed in https://github.com/shazow/urllib3/commit/9552344989bfd8c06214692e612aac9a9fc83abc
",shazow,t-8ch
1643,2013-10-04 14:20:53,"I want tests before we merge it but I can added them for @william-p afterwards.
",sigmavirus24,william-p
1640,2013-10-01 08:11:00,"This in principle looks great @abarnert, thanks so much!

We wouldn't need to wait until 3.0, because this change isn't backwards incompatible: adding the ability to pass a 4-tuple can be done in a minor release (e.g. 2.1.0) according to [semver](http://semver.org/).

The real question is whether we think this API extension is the right way to handle it. As you pointed out in #1640, we already poorly document this corner of the API so that definitely has to be fixed.

@kennethreitz: Are you happy with this extension to the API, or would you like to reconsider the multipart file API entirely?
",Lukasa,abarnert
1640,2013-10-02 18:28:26,"@Lukasa Here's an example of what I'm trying to do: http://paste2.org/kgNztbBv

Basically, each part of the multipart has a different `Content-Type`, separate and distinct from the one in the message header.
",mrichman,Lukasa
1640,2013-10-04 08:49:59,"@abarnert Yes it is. =)
",Lukasa,abarnert
1640,2013-10-07 23:21:15,"I'm +1, specifically because this changes makes the code much easier to understand.

Thanks, @abarnert.
",kennethreitz,abarnert
1638,2013-10-01 08:21:56,"@dknecht We're in a tough place here. We absolutely should not monkeypatch cookielib, that's craziness. However, the default implementation of `request_host` calls `Request.get_full_url()`. It might be possible to adjust what we return from there, but that seems pretty crappy.
",Lukasa,dknecht
1638,2013-10-01 08:50:30,"@Lukasa  Yeah i wasn't suggesting monkeypatch cookielib.  That was just for a quick work around in case anybody needed it.  My comment around about what requests might consider was as you said adjusting what get_full_url returns in MockRequests.  (comment clarified above)
",dknecht,Lukasa
1638,2013-10-01 14:56:56,"@Lukasa Also wanted to mention that the use case by @mtourne was meant as easy example.  The main use case is being able to test sites where we need to change the IP from what the domain would normally resolve to.  

Maybe the right way to solve this is how curl solved it recently?

Previously in curl you would do `curl -H'Host: fake.net' http://127.0.0.1` and now you can do `curl -v --resolve fake.net:80:127.0.0.1 http://fake.net``


",dknecht,mtourne
1638,2013-10-01 14:56:56,"@Lukasa Also wanted to mention that the use case by @mtourne was meant as easy example.  The main use case is being able to test sites where we need to change the IP from what the domain would normally resolve to.  

Maybe the right way to solve this is how curl solved it recently?

Previously in curl you would do `curl -H'Host: fake.net' http://127.0.0.1` and now you can do `curl -v --resolve fake.net:80:127.0.0.1 http://fake.net``


",dknecht,Lukasa
1638,2013-10-01 14:59:28,"@dknecht Oh I absolutely realised that. =) My thing is that it feels like the wrong way of doing it. cURL's solution also seems like an ugly hack for what is fundamentally a solved problem: just edit your hosts file. =D
",Lukasa,dknecht
1638,2013-10-01 15:02:32,"@Lukasa  Cool.  Just trying to throw out other options that are cleaner.  We broke most of our test suite with when we upgraded to get some of the other new features and trying to avoid downgrading.  
",dknecht,Lukasa
1638,2013-10-05 14:55:33,"@Lukasa agreed. 
",sigmavirus24,Lukasa
1638,2013-10-08 02:47:25,"@dknecht can you confirm that [this](https://github.com/sigmavirus24/requests/commit/0bb8be0e5ba58dfff9648b4058acaa927837e605) fixes the issue? I'm still not convinced this is common, but at this point, I'd rather just close out this issue on a somewhat positive note.
",sigmavirus24,dknecht
1638,2013-10-09 15:22:53,"@sigmavirus24 Go ahead and open the PR with that commit. =)
",Lukasa,sigmavirus24
1638,2013-10-09 15:54:42,"@sigmavirus24  Thank you.  I tested and it works.  
",dknecht,sigmavirus24
1638,2013-10-22 16:39:15,"@Lukasa Do you think we can get this merged in?  =)
",dknecht,Lukasa
1638,2013-10-22 17:36:24,"@dknecht I do. =) The relevant PR is #1660. The way Requests works is that Ian and I triage and code review, but do not merge code: Kenneth does. This gives him final say. I'm confident he'll merge it, but we have to wait until he gets some free time to take a look. Sorry about the delay!
",Lukasa,dknecht
1635,2013-09-28 16:10:39,"@Lukasa I pushed updated version, Thanks!
",daftshady,Lukasa
1635,2013-09-28 17:16:26,"LGTM to me too. :+1: @daftshady 

Please add yourself to the [AUTHORS.rst](https://github.com/kennethreitz/requests/blob/master/AUTHORS.rst#patches-and-suggestions) file?
",sigmavirus24,daftshady
1634,2013-09-28 15:24:29,"@Lukasa `WWW-Authenticate` you meant...
",thikonom,Lukasa
1630,2013-09-27 11:36:55,"@Lukasa I will try on it.
I think implementing deepcopy of models.PreparedRequest and call req.deepcopy() instead of req.copy() will fix super extra bonus bug easily. Is there any expectable drawback of using deepcopy?
",daftshady,Lukasa
1630,2013-09-27 11:44:00,"@daftshady It's unnecessary. =) `PreparedRequest.copy()` should just make sure it calls `req.headers.copy()` to get the new head dict. =)
",Lukasa,daftshady
1630,2013-09-27 12:08:51,"@Lukasa It' s strange.. I think even if PreparedRequest.copy creates new object, as you know,  assignment like this (p.headers = self.headers) only make shallow copy of CaseInsensitiveDict. so the change in new `p` created by copy() can affect origin so that extra bonus bug happens.
Anyway, i will try another way to fix it if you think deepcopy is not suitable =)
",daftshady,Lukasa
1630,2013-09-27 12:59:03,"@Lukasa I understand. I misunderstood that you are saying 'p.headers = self.headers' is just enough.
As you said, `CaseInsensitiveDict.copy()` works correctly. So just fixing it to 'p.headers = self.headers.copy() is enough, not deepcopy.

In your example, if

> > > b = {'first' : [1,2,3]}
> > > d = b.copy()
> > > b is d
> > > False
> > > b['first'].append(4)
> > > b
> > > {'first' : [1,2,3,4]}
> > > d
> > > {'first' : [1,2,3,4]}
> > > Changes that affect B can affect D in this case.

But, as you said, `CaseInsensitiveDict.copy()` is just enough to fix extra bonus bug.
Sorry for my carelessness =)
",daftshady,Lukasa
1629,2013-10-05 17:22:34,"@kennethreitz Did you forget about this? Can't find a relevant commit ;)
",schlamar,kennethreitz
1628,2013-09-26 12:51:53,"@Lukasa Right, It's quite tricky. 

I guess a solution could be to remember that a custom `Host` has been supplied and use it only as an alias of the one included in the request URL for any subsequent requests to the same host within a session. Let me try to demonstrate:


",jakubroztocil,Lukasa
1628,2013-09-26 16:11:21,"@Scorpil That's not a bad idea, but I disagree about your final statement. When written like that it's an obvious programming error, but when hidden in variables and loops it's absolutely not clear. _Especially_ as it was the example you gave in your original post. =)

We have two options: we write this so that you cannot hit this redirection loop, or we say ""Hey, if you're playing with the `Host` header you want to worry about redirect loops. Maybe turn `allow_redirects` off?"".
",Lukasa,Scorpil
1628,2013-10-09 15:22:20,"@Scorpil _Ping_.

Want to update this PR?
",Lukasa,Scorpil
1628,2013-10-10 09:55:38,"Any feedback on update, @Lukasa?
",Scorpil,Lukasa
1628,2013-10-11 17:12:05,"@Scorpil can you rebase your pull request? Something like this should work:



Note that there will be conflicts while rebasing so you should be very careful in resolving them and us in reviewing the resulting diff.
",sigmavirus24,Scorpil
1628,2013-10-12 01:41:06,"Thanks @Scorpil !
",sigmavirus24,Scorpil
1628,2013-11-27 02:28:20,"@kennethreitz I have avoided being too negative about this, but I agree with you. I don't see anything wrong with the current behaviour. We're inconveniencing users who are doing something we strongly recommend against, that doesn't seem like a bad thing frankly.
",sigmavirus24,kennethreitz
1628,2013-12-05 22:40:07,"@Scorpil keep the pull requests coming! This was a really great change! Sorry we couldn't accept it right now :)

Everything about your contribution was perfect, though. Requests just is overly opinionated :)
",kennethreitz,Scorpil
1628,2013-12-06 11:29:10,"Thanks @kennethreitz 
No problem, if you think it's not appropriate change - then it's not. No hard feelings :)
",Scorpil,kennethreitz
1628,2013-12-06 12:35:35,"@Scorpil Here's some :cake:. You deserve it. :)
",sigmavirus24,Scorpil
1626,2013-10-05 17:05:48,"@massifor do you have any more details for us?
",sigmavirus24,massifor
1625,2013-09-25 14:59:54,"@Lukasa Sure. It's just a request. :)
",f,Lukasa
1625,2013-09-25 15:34:20,"@fka This is really cool! I think I'll hold off on it for now, though. :)
",kennethreitz,fka
1625,2013-09-25 15:58:22,"Thanks @kennethreitz :) It's up to you.
",f,kennethreitz
1624,2013-09-28 17:43:55,"No worries @monocasual 

Also, @Lukasa sometimes you have to ask the obvioius question
",sigmavirus24,Lukasa
1624,2013-09-28 17:43:55,"No worries @monocasual 

Also, @Lukasa sometimes you have to ask the obvioius question
",sigmavirus24,monocasual
1623,2013-09-26 02:56:42,"I think it's working correctly when i tested with md5.
In my opinion, that UnboundLocalError only happens when 'www-authenticate' header has another hashing algorithm (not md5, sha1)
@linuxhngg Are you trying to use 'MD5' digest authentication?
",daftshady,linuxhngg
1623,2013-09-26 05:43:32,"@linuxhngg 
oh.. there is no way to parse 'MD5-sess' in current DigestAuthenticator.
According to http standard, 'MD5-sess' is valid algorithm name.
I will attach quick fix on it.
",daftshady,linuxhngg
1622,2013-09-25 08:01:34,"@GrahamDumpleton Could you please run these tests against plain urllib3, too (see https://gist.github.com/schlamar/5080598#file-test_proxy-py for example code)?
",schlamar,GrahamDumpleton
1622,2013-09-25 10:54:38,"@GrahamDumpleton I'm pretty sure the only issue is your setup. Please check first how curl behaves (I expect exactly like requests 2.x).

> they have broken something that used to work

That's wrong. Proxy support was broken before, it is fixed with 2.x. 

`http_port_with_https_scheme` is now exactly how it should work. There is just a TCP connection to the proxy (doesn't matter if port 80 or port 443). All HTTP data is encrypted with the cert of the target. There is no security issue here (you can check with curl/wireshark).

`https_port_with_https_scheme` should fail at cert validation because squid is configured as MITM proxy. Adding your custom cert to the request call should work.
",schlamar,GrahamDumpleton
1622,2013-09-26 09:39:44,"> Then why did you ever bother making the scheme mandatory in the proxies map when you ignore it? You could have still accepted host:port aline and just always made it use http anyway.

I have asked that myself. Because curl and other HTTP clients do support `http_proxy=host:port` without scheme. @Lukasa ?

> Anyway, what it comes down to is the documentation needs to state then that the scheme is totally irrelevant and that http:// is the only one that makes sense, because https:// is a lie.

[subjective opinion] The documentation shows only examples with http:// so this kind of implies that. I think you cannot expect CONNECT to work via SSL while it is not explicitly stated. 
",schlamar,Lukasa
1622,2013-10-11 11:08:20,"@schlamar Sorry I let this slip past me. The answer there is 'maybe'. @t-8ch was one of the people strongly pushing for explicit proxy schemes: do you have an opinion?
",Lukasa,schlamar
1622,2013-10-12 11:22:09,"@Lukasa 
Like @schlamar I think the 1.x behaviour was wrong.
The proposal for explicit schemes in lieu of a default `http`-scheme was only about the second zen of python: `Explicit is better than implicit.`

If people are more comfortable with a default scheme I'm totally fine with this.
",t-8ch,schlamar
1622,2013-10-12 11:22:09,"@Lukasa 
Like @schlamar I think the 1.x behaviour was wrong.
The proposal for explicit schemes in lieu of a default `http`-scheme was only about the second zen of python: `Explicit is better than implicit.`

If people are more comfortable with a default scheme I'm totally fine with this.
",t-8ch,Lukasa
1622,2013-10-12 13:43:25,"> If people are more comfortable with a default scheme I'm totally fine with this.

I would think telling people to explicitly set the scheme is good but having a default for it is just as good. If this helps @GrahamDumpleton in anyway I'm especially :+1: for it.
",sigmavirus24,GrahamDumpleton
1622,2013-10-12 20:03:04,"@schlamar this is not introducing a new feature per-se. Are you instead considering semantic versioning? Technically this would be backwards compatible since we would still be allowing/encouraging users to use a scheme, so it is also permissible by semantic versioning.
",sigmavirus24,schlamar
1621,2013-09-25 01:43:50,"@davidfischer I think the link would be fantastic, actually. Want to add it?
",kennethreitz,davidfischer
1621,2013-09-25 02:01:21,"@kennethreitz linked!
",davidfischer,kennethreitz
1621,2013-09-25 07:43:16,"This is awesome @davidfischer, thanks! =D :cake:
",Lukasa,davidfischer
1618,2013-09-24 11:30:10,"Yeah I was of a similar opinion @smokey42 but I agree with the decision in the issue linkes by @Lukasa. It allows for the most degrees of freedom.
",sigmavirus24,smokey42
1618,2013-09-24 11:30:10,"Yeah I was of a similar opinion @smokey42 but I agree with the decision in the issue linkes by @Lukasa. It allows for the most degrees of freedom.
",sigmavirus24,Lukasa
1617,2013-09-24 02:45:49,"@laruellef yeah I was just about to go looking for that issue. It almost certainly seems related to #1577. We're still waiting on the urllib3 PR that @Lukasa alluded to in that issue so I'm going to close this while we wait for that to work itself out so we can provide that interface to you.

That said, if you run into timeout issues again, and setting it lower doesn't resolve your issue, then open an issue. Otherwise it might be safe to assume it's related to #1577.
",sigmavirus24,laruellef
1617,2013-09-24 02:52:47,"oh, but wait,
the proposed work around is not working here... :-(
ie
https://github.com/kennethreitz/requests/issues/1577#issuecomment-23794576

Fred~

On Mon, Sep 23, 2013 at 7:46 PM, Ian Cordasco notifications@github.comwrote:

> @laruellef https://github.com/laruellef yeah I was just about to go
> looking for that issue. It almost certainly seems related to #1577https://github.com/kennethreitz/requests/issues/1577.
> We're still waiting on the urllib3 PR that @Lukasahttps://github.com/Lukasaalluded to in that issue so I'm going to close this while we wait for that
> to work itself out so we can provide that interface to you.
> 
> That said, if you run into timeout issues again, and setting it lower
> doesn't resolve your issue, then open an issue. Otherwise it might be safe
> to assume it's related to #1577https://github.com/kennethreitz/requests/issues/1577
> .
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/issues/1617#issuecomment-24971338
> .
",laruellef,laruellef
1616,2013-09-21 14:53:07,"@Lukasa my searches were bad! Thanks for the quick response.
",abn,Lukasa
1615,2013-09-21 13:42:08,"@Lukasa they are thoroughly distinct values in this context. I would be okay with seeing `timeout=0` and immediately raising a `Timeout` exception before doing anything. Since we don't return a response (or request) this won't affect the API.

While I don't think we're doing anything wrong by passing this along and catching (and raising) the right exception, I think @Damgaard is justified in his confusion.

Regardless, I'm not entirely convinced this justifies a change. Perhaps the documentation could be improved, but if we do change this it will be a breaking API change and will have to be put off until 2.0. There are probably plenty of people properly handling this and changing the exception raised would break their code.
",sigmavirus24,Damgaard
1615,2013-09-21 13:42:08,"@Lukasa they are thoroughly distinct values in this context. I would be okay with seeing `timeout=0` and immediately raising a `Timeout` exception before doing anything. Since we don't return a response (or request) this won't affect the API.

While I don't think we're doing anything wrong by passing this along and catching (and raising) the right exception, I think @Damgaard is justified in his confusion.

Regardless, I'm not entirely convinced this justifies a change. Perhaps the documentation could be improved, but if we do change this it will be a breaking API change and will have to be put off until 2.0. There are probably plenty of people properly handling this and changing the exception raised would break their code.
",sigmavirus24,Lukasa
1615,2013-09-23 07:29:39,"Good spot @piotr-dobrogost, that should solve our problems nicely.
",Lukasa,piotr-dobrogost
1612,2013-09-24 02:21:19,"@sanmayaj do I understand correctly that you expect requests to string-ify everything you send in? Why can you not do this yourself on your inputs?

> Be liberal in what you receive and conservative in what you send.

You should be assuming that everything is a string (or byte) that leaves your computer via requests. This includes headers too. Personally I'm -1 on this. Up to @kennethreitz and @Lukasa though
",sigmavirus24,sanmayaj
1612,2013-09-24 08:11:41,"Sorry, I should have spotted this, but we fixed it already! Merged into the 2.0 branch as PR #1537. @sanmayaj Can you try the 2.0 branch and confirm that it works fine for your use case?
",Lukasa,sanmayaj
1610,2013-09-24 17:51:02,"@maruel if you send a pull request, i'll merge it :)
",kennethreitz,maruel
1609,2013-09-19 07:01:08,"@chadgh `mock.issues.com` is down, whereas `issues.com` isn't. Can you compare the results with `httpie` and `curl`. `http -h http://mock.issues.com/500`, `curl -i http://mock.issues.com/500`
",kracekumar,chadgh
1609,2013-09-19 07:59:14,"In fact @chadgh, if your ISP is anything like mine they'll find they can't resolve the DNS and then secretly point you to their own servers for advertising purposes. That would cause the behaviour you're seeing.
",Lukasa,chadgh
1608,2013-09-17 15:43:45,"This is not the same issue. @sebastien-bratieres claims that he observed this using the 2.0 branch of Requests. If that's the case, we still have a problem. =)
",Lukasa,sebastien-bratieres
1608,2013-09-17 15:49:32,"However, I don't observe this problem on the 2.0 branch:



@sebastien-bratieres Can you attempt the exact same test I just did?

NB: Wireshark also shows correct behaviour.
",Lukasa,sebastien-bratieres
1608,2013-09-18 11:32:23,"@sebastien-bratieres Sorry, did you run that using the 2.0 branch? Because when I run the equivalent test using the 2.0 branch, I continue not to get duplicated headers.
",Lukasa,sebastien-bratieres
1608,2013-09-18 12:02:50,"To the best of my knowledge, yes... but doubting is in order as I'm just
starting to use git.
To make sure, I have removed my previous version of requests, obtained an
error on import, downloaded the github zip of branch 2.0 (
https://github.com/kennethreitz/requests/archive/2.0.zip ), ran setup.py
from there, and ran my tests again, with the same result. The install
directory is
c:\winpython-64bit-3.3.2.3\python-3.3.2.amd64\lib\site-packages\requests-1.2.3-py3.3.egg

I also ran this:

import requests
print(sys.version)
print(requests.**file**)

3.3.2 (v3.3.2:d047928ae3f6, May 16 2013, 00:06:53) [MSC v.1600 64 bit
(AMD64)]
C:\WinPython-64bit-3.3.2.3\python-3.3.2.amd64\lib\site-packages\requests-1.2.3-py3.3.egg\requests__init__.py

Anything to do with the urllib version used in Python 3.3.2 ?

I'm not a confirmed Python user, so if there's any extra test I could run,
please let me know.

HTH
Sebastien

2013/9/18 Cory Benfield notifications@github.com

> @sebastien-bratieres https://github.com/sebastien-bratieres Sorry, did
> you run that using the 2.0 branch? Because when I run the equivalent test
> using the 2.0 branch, I continue not to get duplicated headers.
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/issues/1608#issuecomment-24657611
> .
",sebastien-bratieres,sebastien-bratieres
1606,2013-09-17 07:53:39,"Hi @borfig! Thanks for doing this work!

I have no idea if this'll get accepted or not. We're very reluctant to add new things to the Requests functional API at this stage, so this boils down to whether or not @kennethreitz believes this is a feature worth exposing at that level. I simply don't know what he'll decide. =)

Regardless of what Kenneth decides, the patch itself appears to be in good shape, so if he wants the feature this patch is a good solid implementation of it.
",Lukasa,borfig
1606,2013-09-17 11:14:20,"And @t-8ch's suggestion is exactly why I'm kind of -1 on this. I would love extra conveniences for accessing some urllib3's extra features but I don't want us caught in parameter hell.

Would it make sense to consolidate a bunch of these options into a dictionary? 
",sigmavirus24,t-8ch
1605,2013-09-17 03:59:10,"1. We have been asking that all Pull Requests be pointed at the requests 2.0 branch
2. This is a major API change and as such should only be considered for 2.0. I'm not adverse to exposing this API, but I'm also not convinced this is necessary. I don't think we expose everything `urllib3` can do and so I don't see us as being obligated to expose this.
3. Regardless, @borfig if I were you I would close this, and open a new PR that instead chooses kennethreitz:2.0 as the base instead of kennethreitz:master.
",sigmavirus24,borfig
1604,2013-09-15 18:18:08,"Thanks for this @lavr!

This is a deliberate design decision for Requests. We're following the spec unless we find ourselves in a position where the specification diverges so wildly from real world behaviour that it becomes a problem (e.g. GET after 302 response to a POST).

If the upstream server knows what the correct encoding is, it should signal it. Otherwise, we're going to follow what the spec says. =)

If you think the spec default is a bad one, I highly encourage you to get involved with the RFC process for HTTP/2.0 in order to get this default changed. =)
",Lukasa,lavr
1604,2013-09-15 18:21:21,"What @Lukasa said + the fact that if the encoding retrieved from the headers is non-existent we rely on [charade](https://github.com/sigmavirus24/charade) to guess at the encoding. With so few characters, charade will not return anything definitive because it uses statistical data to **guess** at what the right encoding is.

Frankly, the year makes no difference and does not change specification either.

If you know what encoding you're expecting you can also do the decoding yourself like so:



There is nothing wrong with requests as far as I'm concerned and this is not a bug in charade either. Since @Lukasa seems to agree with me, I'm closing this.
",sigmavirus24,Lukasa
1604,2013-09-15 18:26:16,"@lavr (/cc @sigmavirus24), even easier than that, you can simply provide the encoding yourself.



Then, proceed normally.
",kennethreitz,sigmavirus24
1604,2013-09-15 18:26:16,"@lavr (/cc @sigmavirus24), even easier than that, you can simply provide the encoding yourself.



Then, proceed normally.
",kennethreitz,lavr
1604,2013-09-15 18:27:29,"@kennethreitz that's disappointing. Why are we making that easy for people? =P
",sigmavirus24,kennethreitz
1604,2013-09-15 18:58:19,"@sigmavirus24
please note, that utils.get_encoding_from_headers always returns 'ISO-8859-1', and charade has no chance to be called. 
so bug is: we expect that charade is used to guess encoding, but it is not. 
",lavr,sigmavirus24
1604,2013-09-16 09:18:48,"@lavr Sorry, we didn't make this very clear. We do _not_ expect charade to be called in this case. The RFC is very clear: if you don't specify a charset, and the MIME type is `text/*`, the encoding must be assumed to be ISO-8859-1. That means ""don't guess"". =)
",Lukasa,lavr
1604,2013-09-16 09:20:14,"@lavr: just set `r.encoding` to `None`, and it'll work as you expect (I think). 
",kennethreitz,lavr
1604,2013-09-16 19:12:33,"@lavr I was covering the non-text bases. You can rule out the `charset` possibility by using this condition instead:


",Lukasa,lavr
1604,2013-09-16 19:38:00,"@Lukasa 
Well, I can use this hack.
And everybody in Eastern Europe and Asia can use it.

But what if we fix it in requests ? ;)
What if requests can honestly set `enconding=None` on response without charset ?
",lavr,Lukasa
1602,2013-09-14 13:21:08,"@Lukasa yeah that makes perfect sense actually (about `auth-int`). It [`auth-int`] looks (at a glance) like it is a variation on the `auth` algorithm. I would be perfectly willing to punt on this but at some point it looks like it was planned to be added: https://github.com/sigmavirus24/requests/commit/22e31b4b737c2a3b61b3ab4fccd534b2eee65a87#L0R126

Regardless #1601 will be fixed by this PR and since we haven't seen many complaints I would guess that `auth-int` isn't as widely used.
",sigmavirus24,Lukasa
1599,2013-09-13 15:21:29,"@Lukasa  you don't have to explain to me such obvious things, really ;-)

The problem is that it happens very rare, when it happened it was Connection Reset probably. Also it happened from daemonized thread (daemon=True). The real problem is that it happened once a milion reqs or so. I wonder if it's not a GC ""problem"" in python or something.
",pigmej,Lukasa
1599,2015-06-26 08:00:23,"@dalanmiller Can you follow @sigmavirus24's advice from earlier in the thread, please?
",Lukasa,dalanmiller
1599,2015-06-26 08:00:23,"@dalanmiller Can you follow @sigmavirus24's advice from earlier in the thread, please?
",Lukasa,sigmavirus24
1595,2013-09-12 21:24:07,"Fair :)

Thank you @Lukasa !
",woozyking,Lukasa
1595,2013-12-20 07:11:50,"@Lukasa - would you consider using ujson by default if it is installed?

Something like:


",ziadsawalha,Lukasa
1593,2013-09-12 08:05:09,"Hi @alsroot, thanks for raising this issue!

That looks like a nasty problem, and thanks so much for investigating it. However, your changes are in urllib3. This is actually a [separate module](https://github.com/shazow/urllib3) that we vendor a copy of into Requests. Any bugs in urllib3 should be fixed over there, and we'll grab the fixes when we prepare a new release. This means you should open this issue on that repository. =) Andrey will be delighted to have it.

Thanks so much for investigating this, and I'm looking forward to seeing a fix! :cake:
",Lukasa,alsroot
1593,2013-09-12 09:18:54,"Thanks so much @alsroot!
",Lukasa,alsroot
1592,2013-09-12 07:58:43,"@homm Thanks for this! Unfortunately, I don't agree with most of these documentation changes. My notes, in the order the change appears in the diff:
1. `stream` (or more accurately `prefetch`) was always required for `Response.iter_content`, but not always required for `Response.raw`. The portion of the documentation you're changing is about the changes introduced in the move to 1.x, and so correctly reflects the change in behaviour. This should not be changed.
2. I'm happy with this one. =)
3. Whitespace change, always fine.
4. As (3).
5. We should not be hiding `Response.raw` from people. This portion of the documentation is clear about what `Response.raw` is, so I see no reason to believe that it's dangerous. =)

If you strongly disagaree with any of those points, please let me know. Otherwise, if you remove changes 1 and 5 I'll happily merge this. =)
",Lukasa,homm
1592,2013-09-12 09:23:56,"Beautiful, thanks so much @homm! Would you like to add yourself to the AUTHORS file as well?
",Lukasa,homm
1592,2013-09-12 09:33:05,"@Lukasa I accidentally use unicode — instead of - in authors list. Sorry. Fix it please.
",homm,Lukasa
1589,2013-09-10 20:52:30,"Thanks for this @jdennis! It's great work.

Unfortunately, this pull request won't be accepted in its current form. This is for a few reasons:
1. Requests used to check the content header of HTML pages for charsets in older versions. We explicitly removed this functionality because Requests is a HTTP library, not a HTML library. We should not assume that returned data will be HTML (often it won't be). This fact means that the notion of checking the content will not be accepted into Requests in any form, sadly.
2. The ISO-8859-1 default applies not only to HTML but any form of text encoding delivered by HTTP where no other encoding is specified. This behaviour is given in [RFC 2616](http://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html#sec3.7.1) (aka the HTTP/1.1 spec), and we describe our compliance with it [here](http://docs.python-requests.org/en/latest/user/advanced/#compliance).
   We implemented this because it was causing genuine confusion with older websites and web servers. Choosing UTF-8 as a default is just as arbitrary a choice as ISO-8859-1, with the added disadvantage of being directly against the relevant specification. Until RFC 2616 is superseded (it will be soon), changing this behaviour would be unwise.
   (It's worth noting that `get_encoding_from_headers()` does not always return an encoding. It _will_ always return an encoding with correctly `Content-Type`d HTML, but there are plenty of other things accessed via HTTP.)

With that said, this is an excellent Pull Request and thank you so much for opening it. =) Unfortunately, it's just not in line with how we want the library to function. I'm sorry we can't accept it, and please do keep contributing! :cake:
",Lukasa,jdennis
1589,2013-09-11 10:54:48,"@jdennis I edited your first comment to make the meta tag appear. =)

So I'm totally sympathetic to your position here. I'll respond to a few of your points:
1. If the `Content-Type` header is `text/html`, I think we'd be significantly more justified in searching for the `meta` tag. If you open a PR that does that it has a much better chance of being accepted! I'd love to see it. =)
2. It's important to be clear on the role of the `response.text` property. `response.text` is not limited to working only with HTML. Instead, its purpose is for anything that has a character encoding. This can include HTML, but needn't: for instance, it applies to JSON and XML as well. For that reason, `response.text` has no right to interrogate the body. All it does is assume the encoding that the HTTP states, unless the HTTP does not state one.

It's worth noting that if you are worried about having problems with HTML, and you know that's what you're fetching, you can use this flow:



At that point, `r.text` will work correctly.

With that said, I'm prepared to believe that we can make some useful extensions to the encodings flow. For instance, JSON should _always_ be UTF-8, so we could special-case this logic to enforce that. Similarly, for specific MIME types (I'm thinking `text/html`, `application/xhtml+xml`, `application/xml` and `application/json` at the moment) we could provide some special case logic. In the HTML, XHTML and XML cases, that could use `get_encodings_from_content`. We will still not set UTF-8 as the fallback default, as that continues to be unwise. =)

Does this sound like an acceptable compromise?
",Lukasa,jdennis
1589,2013-09-11 17:15:51,"@jdennis I am most definitely ignoring the W3C rules. =) That's because, as mentioned earlier, Requests is _not_ a HTML library, it's a HTTP library. The W3C does not make the HTTP specs, the IETF do, and they have been very clear about what the correct behaviour is here.

The same rationale applies regarding having this logic in the core library. The idea that things should 'work as I expect' (aka the principle of least surprise) is great, but only the things the library actually _claims_ to do should work as you expect. Given that Requests is explicitly a HTTP library, not a HTML library, you should only assume that the HTTP behaviour of the library works as you expect it to. Requests' documentation is clear on what will happen regarding encodings. From the [section on response content](http://docs.python-requests.org/en/latest/user/quickstart/#response-content):

> When you make a request, Requests makes educated guesses about the encoding of the response based on the HTTP headers. The text encoding guessed by Requests is used when you access r.text. You can find out what encoding Requests is using, and change it, using the r.encoding property...

We don't claim to parse HTML to find the right encoding, and we don't even claim we'll get it right. We say we'll ""make educated guesses based on the HTTP headers"", and that's all we do. =)

Finally, we're not asking all Requests users to implement this logic. We're asking the ones who need it to implement it. By and large Requests does not help users with getting their data into a form that is useful to them. The only exception I can think of is `Response.json()`, and we only add that because it's almost no work for us and overwhelmingly the most common use-case for Requests. =)
",Lukasa,jdennis
1586,2014-01-23 13:06:02,"@yydonny why wouldn't you use streaming on a file like that anyway? Or are you just trying to argue a point? Passing `stream=True` will not decompress the file for you unless you use `iter_content` (or `iter_lines`). @Lukasa is 100% correct that you should be using the response's `raw` method. It is documented and it was the prior conclusion of this issue. For 98% of our users, the decompression is exactly what they want. For the 2% use case we have built in considerations but those users need to be considerate enough to read the documentation first.

Arguing that we should abandon what we're doing because there are servers that do the wrong thing on the web is like saying certain countries should start devaluing its currency because other nations do the same thing. That approach to economic ""development"" is as harmful to the citizens as deactivating automatic gzip decompression would be to the 98% of our users.
",sigmavirus24,yydonny
1586,2014-01-23 13:06:02,"@yydonny why wouldn't you use streaming on a file like that anyway? Or are you just trying to argue a point? Passing `stream=True` will not decompress the file for you unless you use `iter_content` (or `iter_lines`). @Lukasa is 100% correct that you should be using the response's `raw` method. It is documented and it was the prior conclusion of this issue. For 98% of our users, the decompression is exactly what they want. For the 2% use case we have built in considerations but those users need to be considerate enough to read the documentation first.

Arguing that we should abandon what we're doing because there are servers that do the wrong thing on the web is like saying certain countries should start devaluing its currency because other nations do the same thing. That approach to economic ""development"" is as harmful to the citizens as deactivating automatic gzip decompression would be to the 98% of our users.
",sigmavirus24,Lukasa
1584,2013-09-09 09:58:26,"Hi @robi-wan, thanks for raising this issue!

Firstly, I'll address the question you actually asked. Currently Requests does not support streaming multipart-encoded files. If you want to do this you'll need to provide a file-like-object wrapper for your file that does the multipart encoding itself, and then pass that to the `data` object as described [here](http://docs.python-requests.org/en/latest/user/advanced/#streaming-uploads).

Secondly, I'll address your actual problem. Your specific error is the Winsock error WSAENOBUFS. It should not be easily possible to hit this error in Requests because we use blocking sockets, which ought to block until there is sufficient buffer space available. You don't appear to be running out of memory in your process, so I don't think the file size itself has anything to do with this problem.

I'm going to take an educated guess and say that you're running out of ephemeral ports. By default, Windows only exposes 5000 ephemeral ports: sufficiently many long-running uploads could exhaust the supply and cause this error. Does that sound possible in your case? If so, take a look [here](http://support.microsoft.com/kb/196271).
",Lukasa,robi-wan
1584,2013-09-10 09:01:36,"Hi @Lukasa and @robi-wan I'm delighted to see this question and already find it answered by you. I happen to just have hit the same issue with missing streaming functionality for multipart file uploads.

I suggest to you to look at the _poster_ module - I have used this instead for realizing the upload in a script that otherwise uses requests. I have implemented the whole upload request with this other module and urllib2, however it might as well be possible to use it to prepare the file-like `data` argument for requests.

The support for this kind of streaming uploads was a prime reason for going with requests, therefore I was disappointed when encountering the `NotImplementedError` that is thrown by `PreparedRequest.prepare_body` when the `files` as well as the `data` argument is provided. This could be made clearer in the documentation.
",avallen,robi-wan
1584,2013-09-10 09:01:36,"Hi @Lukasa and @robi-wan I'm delighted to see this question and already find it answered by you. I happen to just have hit the same issue with missing streaming functionality for multipart file uploads.

I suggest to you to look at the _poster_ module - I have used this instead for realizing the upload in a script that otherwise uses requests. I have implemented the whole upload request with this other module and urllib2, however it might as well be possible to use it to prepare the file-like `data` argument for requests.

The support for this kind of streaming uploads was a prime reason for going with requests, therefore I was disappointed when encountering the `NotImplementedError` that is thrown by `PreparedRequest.prepare_body` when the `files` as well as the `data` argument is provided. This could be made clearer in the documentation.
",avallen,Lukasa
1584,2013-09-10 10:36:53,"Hi @Lukasa thanks for your quick response. I read the Microsoft Knowledge Base Article and tried the suggested solution without success.

@avallen In the meantime I found _poster_ and used it for solving this problem:



This works... but I would like to use _requests_ for task like this.
",robi-wan,Lukasa
1584,2013-09-10 10:36:53,"Hi @Lukasa thanks for your quick response. I read the Microsoft Knowledge Base Article and tried the suggested solution without success.

@avallen In the meantime I found _poster_ and used it for solving this problem:



This works... but I would like to use _requests_ for task like this.
",robi-wan,avallen
1584,2013-10-15 01:35:56,"@sigmavirus24 sorry, you're right, I will email you, if you don't mind.
Thank you very much.
",BernardoLima,sigmavirus24
1583,2013-09-09 11:36:20,"@Lukasa is this documented anywhere? 
",sigmavirus24,Lukasa
1583,2013-09-09 12:09:19,"@sigmavirus24 Not as a single flow, no. It's not clear to me that it should be part of the formal documentation though: perhaps another blog post?
",Lukasa,sigmavirus24
1582,2016-03-10 21:16:40,"Hello gents.  I apologize to wake the sleeping giant.   However Ive been trying to track down a viable workaround to request https: with NTLM proxy authentication and have come up short.  @Lukasa et al I  appreciate all the work to date.   As of what is currently available it doesn't look like this is supported in urllib3, requests-ntlm or request.   What does one do?   Any suggestions?
",ryandebruyn,Lukasa
1582,2016-03-11 04:08:57,"@Lukasa. From what I can tell  requests-ntlm will let you do NTLM auth with a proxy when using HTTP, but not with HTTPS.   Am I missing something?  Thanks again for your time.
",ryandebruyn,Lukasa
1582,2016-03-11 09:37:35,"Ah, yes, @ryandebruyn that's correct. Unfortunately, httplib makes it very difficult to authenticate to proxies when setting up a TLS tunnel because for any non-200 response to the CONNECT it will throw an exception and lose the response data. We can in principle work around that but it's _extremely_ difficult to do and potentially breaks quite a few behaviours.

Sadly, this is just something that is very, very difficult to do.
",Lukasa,ryandebruyn
1581,2013-09-08 18:45:33,"@t-8ch is so :metal:
",sigmavirus24,t-8ch
1580,2013-09-07 17:37:48,"@vivekhub In case you are interested you can start a project like `requests-websocket` similar to [requests-oauth](https://github.com/requests/requests-oauthlib). `Requests` doesn't contain `socks` support in code base, idea of this is to have library core as small as possible but extensible.  
",kracekumar,vivekhub
1580,2013-09-08 05:35:48,"Good point let me first get my project done and then will figure out how to
open-source it
On Sep 7, 2013 11:07 PM, ""kracekumar"" notifications@github.com wrote:

> @vivekhub https://github.com/vivekhub In case you are interested you
> can start a project like requests-websocket similar to requests-oauthhttps://github.com/requests/requests-oauthlib.
> Requests doesn't contain socks support in code base, idea of this is to
> have library core as small as possible but extensible.
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/issues/1580#issuecomment-24005894
> .
",vivekhub,vivekhub
1580,2013-09-08 13:40:04,"@vivekhub how familiar are you with ws4py? If you look at that library you'll recognize that requests and its API are not very suitable for websockets.
",sigmavirus24,vivekhub
1580,2013-09-10 10:06:21,"@sigmavirus24 thanks for the pointer to ws4py looks interesting.  With my limited understanding of both the codebases it sounds like while I can start with request to upgrade a connection to a Websocket I would have to do heart surgery to get hold of the actual TCP connection which would not be the right thing to do is your concern am I right?
",vivekhub,sigmavirus24
1580,2013-09-11 02:55:31,"@vivekhub requests uses urllib3. urllib3 uses httplib for a large portion of its work and I wouldn't know where to begin getting a TCP connection out of there. I think your plan to use requests is just untenable. If you want to attempt to build an API for websockets then you're best bet is to use ws4py but I'm going to have to warn you that the requests API really seems far less than suitable for websockets.
",sigmavirus24,vivekhub
1579,2013-09-08 15:04:57,"This is excellent @yang, thank you so much! :cake:
",Lukasa,yang
1578,2013-09-05 06:44:21,"Well deserved @jparise, thanks so much for your work! :grin:
",Lukasa,jparise
1577,2013-09-04 14:18:34,"Thanks for raising this issue @laruellef!

The `timeout` parameter does not work the way most people seem to expect it to. Whether that is actually a bug is up for discussion. =)

The actual behaviour is that `timeout` is set as the socket timeout. This timeout applies to each individual blocking socket operation, _not_ to the connection as a whole. This means you only trigger the timeout if connecting takes longer than a second, or if any of the page responses take more than a second to download.
",Lukasa,laruellef
1577,2013-09-04 14:58:27,"@laruellef Creating another timeout parameter is something I'm considering. It suffers from creating two parameters that sound similar but do different things. If we redefined the current timeout parameter that'll break a _lot_ of working code, which would be bad. But most importantly, we cannot easily redefine the timeout parameter without setting `stream=True` as the default (I think).
",Lukasa,laruellef
1577,2013-09-05 08:16:44,"So, @laruellef, it looks like @kevinburke is doing some work on the `urllib3` side to add better timeout control. When that gets sorted we'll probably try to plumb it through to Requests. I think waiting for that issue (shazow/urllib3#231) to be resolved is the correct next step here.

Thanks for raising this, and keep track of the `urllib3` issue!
",Lukasa,laruellef
1577,2016-04-25 11:26:59,"@blubber - sorry for a blast from the past. I'm experiencing stuck sockets in ESTABLISHED although the other side closed the socket, and timeout (new implementation apparently) is in place.
You mentioned `socket.setdefaulttimeout`. Where exactly did you change it?
",boazin,blubber
1576,2013-09-04 05:18:45,"Thanks for this @karanlyons! As far as I can see, this is difficult to do in this particular case. The server's response to the TLS handshake was to close the connection, which is exactly the error we report. Unfortunately, it's not clear whether that TCP RST segment has anything to do with the TLS negotiation. For instance, if the server doesn't support TLS at all you're likely to get similar behaviour.

Despite my pessimism regarding this, we do currently have a feature request open about handing this behaviour, in #1570. I'm going to close this issue in favour of centralising discussion there. =)

Thanks again for raising this issue!
",Lukasa,karanlyons
1573,2013-09-03 14:46:38,"@sigmavirus24 
The tuple is for `(certificate, key)`. Currently there is no support for encrypted keyfiles.
The [stdlib](file:///usr/share/doc/python/html/library/ssl.html#ssl.SSLContext.load_cert_chain) only got support for those in version 3.3.
",t-8ch,sigmavirus24
1573,2013-09-03 15:05:15,"Heh, @t-8ch, you accidentally linked to a file on your local FS. ;) [Correct link](http://docs.python.org/3.3/library/ssl.html#ssl.SSLContext.load_cert_chain).
",Lukasa,t-8ch
1573,2013-09-04 02:43:57,"Quite right @t-8ch. This is why I should never answer issues from the bus. :/
",sigmavirus24,t-8ch
1573,2014-07-30 15:06:41,"@maxnoel I'm pretty sure this is in OpenSSL's hands but if you can answer @Lukasa's question (the last comment on this issue) it would be very helpful in giving a definite answer regarding if there was anything we can do to help.
",sigmavirus24,maxnoel
1573,2014-07-30 15:06:41,"@maxnoel I'm pretty sure this is in OpenSSL's hands but if you can answer @Lukasa's question (the last comment on this issue) it would be very helpful in giving a definite answer regarding if there was anything we can do to help.
",sigmavirus24,Lukasa
1573,2015-04-16 19:42:08,"@reaperhulk It's done from in urllib3, [here](https://github.com/shazow/urllib3/blob/master/urllib3/contrib/pyopenssl.py#L252-L260).
",Lukasa,reaperhulk
1573,2015-12-18 10:27:26,"@mikelupo Yup.
",Lukasa,mikelupo
1573,2016-01-08 07:22:53,"@telam @mikelupo 
I have the same problem and Googled a lot, finally, I solved it by using pycurl.
In my situation, I use openssl to convert my .pfx file to .pem file which contains both cert & key(encrypted with pass phrase), then invoke the following code.



BTW, for security, it's better to not do hardcode for `pass phrase`
",Altynai,telam
1573,2016-01-08 07:22:53,"@telam @mikelupo 
I have the same problem and Googled a lot, finally, I solved it by using pycurl.
In my situation, I use openssl to convert my .pfx file to .pem file which contains both cert & key(encrypted with pass phrase), then invoke the following code.



BTW, for security, it's better to not do hardcode for `pass phrase`
",Altynai,mikelupo
1573,2016-02-24 07:36:00,"@botondus I think I found a simpler way to achieve this with request library. I am documenting this for other people who are facing the issue.

I assume that you have a .p12 certificate and a passphrase for the key.

### Generate certificate and private key.



Well, we are not done yet and we need to generate the key that doesn't require the PEM password every time it needs to talk to the server.

### Generate key without passphrase.



Now, you will have `certificate.pem` and `plainkey.pem`, both of the files required to talk to the API using requests.

Here is an example request using these cert and keys.



Hope this helps:

cc @kennethreitz @Lukasa @sigmavirus24 
",vinitkumar,Lukasa
1573,2016-02-24 07:36:00,"@botondus I think I found a simpler way to achieve this with request library. I am documenting this for other people who are facing the issue.

I assume that you have a .p12 certificate and a passphrase for the key.

### Generate certificate and private key.



Well, we are not done yet and we need to generate the key that doesn't require the PEM password every time it needs to talk to the server.

### Generate key without passphrase.



Now, you will have `certificate.pem` and `plainkey.pem`, both of the files required to talk to the API using requests.

Here is an example request using these cert and keys.



Hope this helps:

cc @kennethreitz @Lukasa @sigmavirus24 
",vinitkumar,sigmavirus24
1573,2016-02-24 07:36:00,"@botondus I think I found a simpler way to achieve this with request library. I am documenting this for other people who are facing the issue.

I assume that you have a .p12 certificate and a passphrase for the key.

### Generate certificate and private key.



Well, we are not done yet and we need to generate the key that doesn't require the PEM password every time it needs to talk to the server.

### Generate key without passphrase.



Now, you will have `certificate.pem` and `plainkey.pem`, both of the files required to talk to the API using requests.

Here is an example request using these cert and keys.



Hope this helps:

cc @kennethreitz @Lukasa @sigmavirus24 
",vinitkumar,botondus
1573,2016-08-24 14:24:15,"@ahnolds: Does this also work for for PKCS#12 files, or is this PEM only?

@Lukasa: Is the PKCS#12 case really supposed to be handled here, or should I open a separate issue for that?
",vog,ahnolds
1573,2016-08-24 14:24:15,"@ahnolds: Does this also work for for PKCS#12 files, or is this PEM only?

@Lukasa: Is the PKCS#12 case really supposed to be handled here, or should I open a separate issue for that?
",vog,Lukasa
1573,2016-08-24 14:49:21,"@Lukasa: I was thinking more of providing a good high-level API in requests. For example, simply providing the `client_cert.p12` filename and the password through the `cert=...` keyword parameter.
",vog,Lukasa
1573,2016-08-24 14:50:49,"@vog What code do you believe would be required to make that work?
",Lukasa,vog
1573,2016-08-24 16:40:53,"@Lukasa I'm not sure about the internals of `requests`, so maybe I underestimate what is already there, but I think one of the following things needs to be done:
- Either we have a way to provide a PKCS#12 filename directly to the lower layers (urllib3, etc.). And maybe the password, too. (Because I know nobody who wants a URL library to ask the admin interactively to enter their PKCS#12 password on a tool that runs server-side.)
- If that is impossible, we would need to convert PKCS#12 (+password) to PEM, then providing these to the lower levels. This is done with a few calls directly to the `OpenSSL` binding. However, the result is the PEM certificate as a string, and I haven't yet found a way to provide the (unencrypted) PEM as string to the lower layers (except maybe by using OpenSSL / python ""ssl"" ""buffer"" wrapper, e.g. `wrap_bio`, but this is only available in latest Python 3 versions, not Python 2).
- So if that is impossible, too, we would not only need to convert PKCS#12 to PEM, but also have to create a temporary file containing the (unencrypted) PEM data.

Note that the last point is what I'm essentially doing at the moment, but I don't like this at all. Why can't I provide a simple string to OpenSSL containing the certificate? Moreover, why can't I simply pass the PKCS#12 filename and password to the lower layers?
",vog,Lukasa
1573,2016-08-24 16:46:56,"I'm going to tag in @reaperhulk as an OpenSSL expert, but my understanding is that there are no APIs for OpenSSL to load PKCS#12 format certs for client certs. This means we'd need to absolutely convert to PEM. Doing it in memory is certainly possible, but at a certain point I wonder if we don't just want to consider this expert enough that we will delegate to whatever SSLContext you pass us.
",Lukasa,reaperhulk
1573,2016-08-24 16:55:43,"@Lukasa Thanks for taking this issue seriously. Sorry if this sounds too technical, but essentially it is just this:

You want to access a service through a client certificate. Almost everywhere you get this as a file and a password (where the file is PKCS#12 encoded). In most APIs, such as the Java standard library, you simply give it the filename and password, and are done with it.

However, in Python this is complicated as hell.

That's why almost nobody does it. Instead, they convert their file and password to a PEM file by hand, through OpenSSL, and use that file. This is administrative overhead for every user of such an application. Because they can't simply name the (PKCS#12) file and password.

I think the `requests` library should make it at least as simple as in Java.

`requests` already does a great job of simplifying stupid complex APIs, and the PKCS#12 use case is just another example of a stupid complex API.
",vog,Lukasa
1573,2016-08-24 17:06:50,"> the PKCS#12 use case is just another example of a stupid complex API.

Yeah, I don't disagree with that at all: I'd be totally happy to have some sort of solution for PKCS#12 support somewhere in the stack.

What I'm trying to get a feel for is what code is required to make that work, and as a result where this should be placed. My reasoning is like this:
1. Generally speaking, Requests only adds to its API surface if there is substantial utility in doing so (that is, if it's used by lots of people or used very heavily by some), and if the thing we're doing is difficult to get right or has subtle edge cases.
2. Normally supporting PKCS#12 would count as an addition to the API surface, but if it doesn't change the syntax of `cert=` at all (just widens the things it will support) and doesn't regress the behaviour (that is, we can reliably tell the difference between PKCS#12 files and PEM files, or we can easily just process through both chains of logic), I'd say it counts as a sufficiently minor change to the surface that it's probably worth it.
3. However, there are other places this may go. For example, at the Transport Adapter level, or as a helper in the Requests Toolbelt, or something else.

That means I want to weigh up how subtle this is, how complex the code is, whether it requires extra dependencies, and then use that information to work out where best to put the code. For example, I have a _suspicion_ right now that the standard library cannot handle PKCS#12, which would mean that _at best_ Requests would only be able to use PKCS#12 with the `[security]` extra installed. In an even worse case we may not have the functions available in any OpenSSL binding at all, in which case we'll have to do some real bonkers stuff to get it to work. That's why I wanted @reaperhulk to weigh in: he'll likely be able to clarify this for us faster than I could do the research.

I would like to see this support added: I just need to get some people who know what the scope of the work is to comment here and let me know how big the mountain we need to move actually is.
",Lukasa,reaperhulk
1573,2017-02-10 08:30:21,"@erikbern To be clear, I am happy to approach and merge any solution that works somewhat consistently. For example, a solution to using PKCS#12 through the PyOpenSSL contrib module in urllib3 would be acceptable.

However, a temporary file solution is not acceptable (as noted by @vog). This means support for PKCS#12 is unlikely to work with the standard library, as the standard library `ssl` module does not expose any support for it, and so it will not be supported in all Requests configurations.",Lukasa,erikbern
1573,2017-02-10 08:30:21,"@erikbern To be clear, I am happy to approach and merge any solution that works somewhat consistently. For example, a solution to using PKCS#12 through the PyOpenSSL contrib module in urllib3 would be acceptable.

However, a temporary file solution is not acceptable (as noted by @vog). This means support for PKCS#12 is unlikely to work with the standard library, as the standard library `ssl` module does not expose any support for it, and so it will not be supported in all Requests configurations.",Lukasa,vog
1573,2017-02-17 08:23:18,"@erikbern To be clear, almost any solution like that will work better just by passing an appropriately-configured `SSLContext` object to urllib3 using a `TransportAdapter`.",Lukasa,erikbern
1571,2013-09-03 05:23:22,"Thanks for raising this issue, @czardoz!

This is a longstanding issue that we've known about for a very long time. A quick GitHub search for ""proxy https"" reveals 118 issues that have been raised at one point or another about this problem. Hell, I even [wrote a blog post about it](https://lukasa.co.uk/2013/07/Python_Requests_And_Proxies/).

You'll be pleased to know that thanks to the sterling work of a few people over at urllib3, we merged a fix for this (as #1515) into our branch for the next release. This should be fixed as part of Requests 2.0.

Thanks again!
",Lukasa,czardoz
1570,2013-09-02 18:23:09,"@sigmavirus24 That was not a `SSLError` but a `socket.error` which doesn't really indicate a SSL problem.
",t-8ch,sigmavirus24
1570,2013-09-02 18:25:45,"@t-8ch maybe I wasn't clear but I never said that issue was caused by this, just that discussion about negotiating a different version could be found there. =)
",sigmavirus24,t-8ch
1570,2013-10-05 17:04:33,"@karanlyons could you submit a PR for that documentation? You have greater context on the issue than I do.
",sigmavirus24,karanlyons
1568,2013-09-02 15:06:27,"Thanks for this @alekibango, and thanks for providing a PR!

It's worth noting that this was an issue we already knew about: see #1426. We work very closely with @shazow on his urllib3 project because we build Requests on top of it, and we regularly take newer versions of urllib3 with releases of Requests. I'm really glad you opened this issue, but in future it is worth quickly searching the GitHub issue tracker to see if someone else raised it before you. It would have saved you a little time! :smile:

Thanks again for this!
",Lukasa,alekibango
1567,2013-09-02 14:35:53,"@jameh can you inspect the headers that Firefox receives when you visit that site? I have an idea that this might be unrelated to SSL but I cannot be sure. Thanks in advance.
",sigmavirus24,jameh
1567,2013-09-02 15:17:32,"I can reproduce this on Arch x64 with both python2 and 3.
If I force the SSL version to be `TLSv1` like described on [@Lukasa s blog](http://lukasa.co.uk/2013/01/Choosing_SSL_Version_In_Requests/) it works for me.
",t-8ch,Lukasa
1567,2013-09-02 16:43:58,"@t-8ch so is it then safe to say this is not an issue with requests?

Should we turn this into a feature request for falling back on older versions of TLS? I would really rather we didn't given how broken some of them are. If we automatically fall back on them we'll avoid bug reports like this but we will be giving our users a somewhat false sense of security, no? I for one would rather deal with these bug reports than compromise our users' security.

On a related note, could you provide your method of detecting what the server will accept/respond to? I would rather not keep bugging you @t-8ch. I don't want to keep nagging you to figure out our TLS/SSL issues.
",sigmavirus24,t-8ch
1567,2013-09-02 17:10:42,"Yes, I don't think this is an issue with requests.

Falling back to older versions of TLS shouldn't be much of an issue securitywise (if we stay in TLS land).
On the other hand broken servers respond in all imaginable ways to handshakes.
If they behave correctly they could simply negotiate a lower version of TLS or
abort the handshake correctly which will result in an SSLError with a more or
less understandable error message.
If they are broken it's down to pure guessing what an error means.
(Servers reset the connection, send no certificate, time out and whatnot)
I really doubt we want to do this :smile:

@sigmavirus24
The best way is probably the utility `gnutls-cli` which is part of `gnutls`.
One can use it like this:



This means: Use your normal settings, but only use TLSv1.2 as TLS version, then
connect to the given host (port 443 is the default).
(Full docs are here: http://www.gnutls.org/manual/html_node/Priority-Strings.html, `openssl s_client` has similary functionality)
",t-8ch,sigmavirus24
1567,2013-09-02 17:32:09,"@t-8ch how complex is the re-negotiation process? I think the only good idea is to raise an exception when the server is doing something unexpected and re-negotiate only when it explicitly asks us to. If you don't have time to describe the process of negotiation, feel free to link the RFC.

In the meantime, I'm going to open an issue/feature request for this and close this issue. Thanks for opening @jameh
",sigmavirus24,jameh
1567,2013-09-02 17:32:09,"@t-8ch how complex is the re-negotiation process? I think the only good idea is to raise an exception when the server is doing something unexpected and re-negotiate only when it explicitly asks us to. If you don't have time to describe the process of negotiation, feel free to link the RFC.

In the meantime, I'm going to open an issue/feature request for this and close this issue. Thanks for opening @jameh
",sigmavirus24,t-8ch
1567,2013-09-02 17:34:31,"Thanks a lot @t-8ch and everyone. Yes I was on Arch Linux x64. Subclassing the HTTPAdapter class as shown on [your blog](http://lukasa.co.uk/2013/01/Choosing_SSL_Version_In_Requests/) and indicating to use `ssl.PROTOCOL_TLSv1` got me my 200 response. :)
",jameh,t-8ch
1567,2013-09-02 17:47:20,"@sigmavirus24 I got it a bit wrong. There isn't an explicit negotiation of the TLS version. Instead the client sends a handshake with the highest version it supports the server then can respond with the version it wishes to use.

[RFC 5246 Appendix E](https://tools.ietf.org/html/rfc5246#appendix-E):



So a server timing out is simply broken.
",t-8ch,sigmavirus24
1567,2013-09-02 17:58:08,"@Lukasa but we could support an older version of TLS.
",sigmavirus24,Lukasa
1567,2013-09-02 17:59:11,"We do. =D

As @t-8ch points out, there is an explicit negotiation for TLS version, and we support them all. The problem here was that the server utterly failed to perform that negotiation.
",Lukasa,t-8ch
1565,2013-09-03 11:46:17,"@aesptux That's an excellent question. =)

An absolute import here will search `sys.path` for a module called `requests`. That works brilliant in 99.9% of cases, which is where people have done `pip install requests`. It'll find the correct module (namely the one we're running from), and then correct find its submodules and grab the right data. All great. The time it doesn't work is when you either don't have `requests` in any of `sys.path`, or you have something else called `requests` ahead of the actual module.

The specific bug report that triggered this fix is the first one. @dstufft was building a python project that included Requests as part of its source code under a different name, to avoid clashing with a user install of Requests. He was therefore getting requests using something like `import vend_requests` or `import vend_requests as requests`. That works great for almost everything, because they were all using relative imports, so they didn't care about whether the project was actually called `requests`. This line, however, does care.

To double up the pain, the line is deliberately inside a `try...except` block that catches `ImportError`. That makes this bug particularly tough to find, as we swallow the exception that would have told you about it.
",Lukasa,aesptux
1565,2013-09-03 11:46:17,"@aesptux That's an excellent question. =)

An absolute import here will search `sys.path` for a module called `requests`. That works brilliant in 99.9% of cases, which is where people have done `pip install requests`. It'll find the correct module (namely the one we're running from), and then correct find its submodules and grab the right data. All great. The time it doesn't work is when you either don't have `requests` in any of `sys.path`, or you have something else called `requests` ahead of the actual module.

The specific bug report that triggered this fix is the first one. @dstufft was building a python project that included Requests as part of its source code under a different name, to avoid clashing with a user install of Requests. He was therefore getting requests using something like `import vend_requests` or `import vend_requests as requests`. That works great for almost everything, because they were all using relative imports, so they didn't care about whether the project was actually called `requests`. This line, however, does care.

To double up the pain, the line is deliberately inside a `try...except` block that catches `ImportError`. That makes this bug particularly tough to find, as we swallow the exception that would have told you about it.
",Lukasa,dstufft
1565,2013-09-03 12:12:13,"@dstufft @Lukasa Thanks for the clarification! :+1: 
",aesptux,Lukasa
1565,2013-09-03 12:12:13,"@dstufft @Lukasa Thanks for the clarification! :+1: 
",aesptux,dstufft
1564,2013-08-31 05:59:56,"Thanks for raising this @jcea!

This is a 'nice to have' feature: if we can get it we'll be happy, but I don't view its absence as a bug or horribly missing feature. Given that this feature belongs in [`urllib3`](https://github.com/shazow/urllib3), the connection-handling library we use, I suggest you propose it over there. If it gets implemented (or indeed if you implement it), Requests should automatically use it as well. =)

Again, thanks for suggesting this!
",Lukasa,jcea
1562,2013-08-30 21:39:30,"HI @llonchj, thanks for opening this PR! What's the rationale for doing this? I'm not 100% sold on this being a good idea.
",Lukasa,llonchj
1562,2013-08-31 00:08:03,"I agree with you, that has several points of view. There's no PEP about. To
me makes sense than a source distribution includes documentation.

Kindly close the PR if not 100% sure.

2013/8/31 Cory Benfield notifications@github.com

> HI @llonchj https://github.com/llonchj, thanks for opening this PR!
> What's the rationale for doing this? I'm not 100% sold on this being a good
> idea.
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/pull/1562#issuecomment-23590816
> .
",llonchj,llonchj
1561,2013-08-30 21:17:45,"It's been doing that for over a year so I would guess that this is perfectly fine otherwise we would have run into issues much sooner. In other words @Lukasa, I agree with you.
",sigmavirus24,Lukasa
1561,2013-08-31 05:49:27,"@kennethreitz Looks like urllib3 changed its behaviour. I'll see if we can fix it up upstream.
",Lukasa,kennethreitz
1561,2013-08-31 14:46:38,"@Lukasa, if you look at the blame on that file though, the change was a year ago. Something quite possibly may have changed but it isn't on that line of that file. It is probably somewhere else and in a change far more recent.
",sigmavirus24,Lukasa
1560,2013-08-30 12:03:29,"Thanks for raising this issue @dstufft! It's not at all clear to me why that import wouldn't work when requests is vendored, though. Can you elaborate on what the problem is?
",Lukasa,dstufft
1560,2013-08-30 12:05:14,"Yea what @kennethreitz said.
",dstufft,kennethreitz
1558,2013-08-28 11:34:51,"@ssbarnea To the best of my knowledge we have no plans to release any further 1.2.X point releases. The next planned release is 2.0.0. I'm potentially open to you making a patch against master, but I'd want to check with @kennethreitz first.
",Lukasa,ssbarnea
1558,2013-08-28 12:09:55,"Go for it



Kenneth Reitz

On Wed, Aug 28, 2013 at 7:34 AM, Cory Benfield notifications@github.com
wrote:

> ## @ssbarnea To the best of my knowledge we have no plans to release any further 1.2.X point releases. The next planned release is 2.0.0. I'm potentially open to you making a patch against master, but I'd want to check with @kennethreitz first.
> 
> Reply to this email directly or view it on GitHub:
> https://github.com/kennethreitz/requests/issues/1558#issuecomment-23407592
",kennethreitz,ssbarnea
1558,2014-02-07 14:09:25,"@Lukasa Using cloudpickle from piclouds cloud library(https://pypi.python.org/pypi/cloud) seems like a good option to me. It supports a lot more types than pythons pickle module, including functions (http://docs.picloud.com/client_pitfall.html#nonsupported-objects lists the exceptions). It would require adding a dependency on cloud, but we could just copy the source, the same way as urllib3 and chardet since the serialization code is not likely to change.

If there are no objections, I can implement this and send in a pull request.
",ankitson,Lukasa
1554,2013-08-26 22:21:20,"Wow, thanks @Lukasa ... I tried your code but it seems that I don't understand it ;)



will join all line-breaks? The desired XML paylout itself also contains ""\r\n""



so I helped me with: 



works fine for me :) 
",wiesson,Lukasa
1554,2014-01-19 14:28:31,"@Lukasa beat me to this, but I was going to include the caveat that questions should be posted to [StackOverflow](http://stackoverflow.com/questions/tagged/python-requests) in the future.
",sigmavirus24,Lukasa
1554,2014-01-19 14:51:55,"@sigmavirus24 sorry for that, I will post my problems on stackoverflow in the future. :) @Lukasa, thanks for the quick answer!
",wiesson,Lukasa
1554,2014-01-19 14:51:55,"@sigmavirus24 sorry for that, I will post my problems on stackoverflow in the future. :) @Lukasa, thanks for the quick answer!
",wiesson,sigmavirus24
1554,2014-01-19 14:54:47,"No worries @wiesson . I had the answer ready to go too. I don't mind helping, which is why I keep SO open in a tab 24/7. :)
",sigmavirus24,wiesson
1554,2014-01-19 16:49:19,"@Lukasa, here is my approach: Any comments or suggestions for improvements? No clue how to catch a disconnect :)


",wiesson,Lukasa
1554,2014-01-19 17:12:30,"@wiesson the first thing I see is that you never set `flag = False` so you're look will go forever. If that's what you intend you could simplify it to:



As for waiting for a disconnect, you can probably simulate that by looking at the tests in [shazow/urllib3](/shazow/urllib3). I believe there are a few dummy servers which simulate dropped connections so you could see how your code responds.
",sigmavirus24,wiesson
1550,2013-08-23 05:10:43,"@Cosmius That's a good point! I think we should fix this up for 2.0.
",Lukasa,Cosmius
1547,2013-09-09 11:35:11,"I like the way you think @kennethreitz. :P

It sounds more to me, however, that 64 bit cygwin is broken (maybe).
",sigmavirus24,kennethreitz
1547,2013-09-22 04:12:53,"Have any details you can add @tanzaho ?
",sigmavirus24,tanzaho
1547,2013-09-24 02:16:58,"> Also, I get core dumps if I try to execute the command above

@mikewn could you provide those? I'm wondering if this isn't an issue in the way Python is built for Windows now. I don't have a windows box to test with though. (Also I didn't know anyone was willing to subject themselves to the horror of Windows 8 ;))
",sigmavirus24,mikewn
1547,2013-09-24 08:09:29,"@sigmavirus24 I have Windows 8. =) Strongly considering moving to Linux on that machine though. Anyway, off-topic.

It's worth noting that this is absolutely not reproducible on 32-bit Cygwin on Windows 7, which strongly points to a problem with Cygwin.
",Lukasa,sigmavirus24
1547,2013-09-24 17:26:24,"@sigmavirus24 - here's the dump.  Basically running python3 or python3.2 both yield stack dumps for python 3.2.  And no, this isn't my personal box that I'm running Windows 8 on.  :)

https://gist.github.com/mikewn/6688148
",mikewn,sigmavirus24
1547,2013-10-02 12:21:31,"Thanks @ahmadia! I'll close this when you let us know the outcome. Otherwise I want it to be easy to find
",sigmavirus24,ahmadia
1547,2013-10-03 13:39:28,"Since there's a solution here, I'm actually tempted to close this. How do you feel @Lukasa ?
",sigmavirus24,Lukasa
1547,2013-10-04 20:17:15,"@Lukasa ?
",sigmavirus24,Lukasa
1547,2013-10-05 16:57:36,"@Lukasa sometimes people don't even look at open ones =P. Either way you're correct. I wonder if we might want a special tag for this kind of issue (where there's a problem we cannot resolve but has some kind of fix available.
",sigmavirus24,Lukasa
1547,2013-10-07 11:53:12,"It's all @ahmadia's fault. ;) Thank him since he found the relevant issues and possible solutions. 
",sigmavirus24,ahmadia
1547,2013-10-07 15:02:48,"I just tried @ahmadia's fix and got the following messages from the patch tool:



When I looked at /usr/lib/python2.7/uuid.py I noticed that the patch was already applied even though I had only just minutes before done a clean install of cygwin64. Importing uuid in python still segfaults even with the fix applied.
",joshfriend,ahmadia
1547,2013-10-07 18:18:23,"@sigmavirus24 Nope. It just quits with no traceback.
",joshfriend,sigmavirus24
1547,2013-10-07 19:21:41,"Sorry @sigmavirus24, I promise to stop after this post. @ahmadia: you can email me at the address listed on my profile.
",joshfriend,ahmadia
1547,2013-10-07 19:21:41,"Sorry @sigmavirus24, I promise to stop after this post. @ahmadia: you can email me at the address listed on my profile.
",joshfriend,sigmavirus24
1547,2013-10-08 15:26:43,"Like @jvstein I traced my issue back to dlltool:



However, I checked and did have `binutils` installed. I had several co-workers with the same issue as me run the latest cygwin setup.exe again and reinstall binutils. Unfortunately, this did not fix the issue for them (or me). I did also verify that we had the same versions of installed packages using `cygcheck -s`. I then removed cygwin and reinstalled from scratch, which fixed the issue for me. It may be worth noting that my previous cygwin installation was performed before the Oct 3rd patch to `uuid.py`.

I suppose the resolution steps are:
1. Verify that python has the patch that was previously mentioned
2. Update/Reinstall/Install binutils as needed
3. As a last resort, reinstall cygwin64 from scratch
",joshfriend,jvstein
1546,2013-08-21 07:51:18,"Thanks for asking this question @JerryKwan! The short and pithy answer is: because it's better to be slow and correct than fast and wrong. =)

If we were concerned about speed we'd simply not have the `Response.text` property at all, and only ever use `Response.content` (with a silly hack for `Response.json()`). This avoids performing any unicode decoding at all, which will save even more time.

Having a 'default' encoding is just wild optimism, because no such default exists on the web. Saying that we'll use UTF-8 whenever we don't know what the correct encoding is means that some users will find that Requests very quickly downloads gibberish. They will then conclude that Requests, while very fast, also doesn't work properly, and they'll go and use another library. =)

**EDIT**: A user can also simulate this behaviour by searching for a `Content-Type` header with the encoding, and if it fails to find one set `Response.encoding = 'utf-8'`.
",Lukasa,JerryKwan
1545,2013-08-20 19:16:10,"Hi @Aaron1011! That's a perfectly good idea. However, 100% code coverage has never been an explicit goal of this project. In the absence of a particular code coverage target, I'm not convinced that adding coverage by default is a good plan. I'll defer to @kennethreitz's judgement in this case.

(Side note: You can do your own code coverage testing very easily by installing the [pytest-cov](https://pypi.python.org/pypi/pytest-cov) plugin for py.test. Then testing requests is done by the command `py.test --cov-report term --cov=requests test_requests.py`. This creates a fair amount of noise because it tests our coverage of our vendored libraries (charade and urllib3), which is irrelevant here. Excluding those we're at roughly 75% test coverage, which is pretty solid. Sample output from master:


",Lukasa,Aaron1011
1544,2013-08-20 12:44:58,"Thanks for this @MarioVilas! I agree that this would fix a bug if this happens, though I can't reproduce this with Google's robots.txt. I have to admit that this is an unlikely bug though: surely the associated page would just never work for any browser?
",Lukasa,MarioVilas
1544,2013-08-20 12:48:18,"I've only seen it with Google's robots.txt, but only if I try to access it
using a wrong URL (notice the missing ""www."" in the beginning). I don't
know if Google is sending the wrong headers, or if Requests (or urllib3) is
keeping the old Location header for some reason.

To be honest I didn't look much into the cause of the bug, I needed
something fixed for tomorrow and this did the trick :) but I thought you
should know anyhow.

On Tue, Aug 20, 2013 at 2:45 PM, Cory Benfield notifications@github.comwrote:

> Thanks for this @MarioVilas https://github.com/MarioVilas! I agree that
> this would fix a bug if this happens, though I can't reproduce this with
> Google's robots.txt. I have to admit that this is an unlikely bug though:
> surely the associated page would just never work for any browser?
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/pull/1544#issuecomment-22942116
> .

## 

“There's a reason we separate military and the police: one fights the enemy
of the state, the other serves and protects the people. When the military
becomes both, then the enemies of the state tend to become the people.”
",MarioVilas,MarioVilas
1544,2013-08-20 12:51:20,"@MarioVilas See, that's my point, I can't reproduce it with `robots.txt` regardless of which URL I use. What version of Requests are you using? (`requests.__version__` will tell you)
",Lukasa,MarioVilas
1542,2013-08-20 01:48:47,"@kqdtran I opened that URL in the browser and received the same 403 error. Do you authenticate before getting to that page? If so you need to login with python/requests first.
",sigmavirus24,kqdtran
1542,2013-09-24 03:00:45,"@kqdtran I just looked at this again with python 2.7.3 (so still not exactly what you're using but close):



Are you still having this issue? It works with curl too:


",sigmavirus24,kqdtran
1542,2013-09-24 03:27:53,"Thanks for helping me confirming the problem, @sigmavirus24!
",kqdtran,sigmavirus24
1542,2014-05-20 15:33:26,"@kqdtran did you ever figure out this issue? I'm facing a similar Heroku-specific problem with a GET request. 
",cvolawless,kqdtran
1542,2014-05-20 16:57:43,"@cvolawless can you try making the same request as if it were a curl request? If you receive the same response as requests, you should contact Heroku support. If you get a different response feel free to open a new issue so @Lukasa and I can be of use. :)
",sigmavirus24,cvolawless
1542,2014-05-20 16:59:21,"@cvolawless also, if you can't discuss the issue publicly, @Lukasa and I both have our emails on our profile pages, so you can send us an email as an alternative for private (confidential) support.
",sigmavirus24,cvolawless
1542,2014-05-20 19:51:32,"@sigmavirus24 yup, curl fails as well. Have a ticket open to Heroku--will update if I find out anything helpful. 
",cvolawless,sigmavirus24
1542,2016-04-25 10:26:37,"@arkangelx I'm sorry, what specifically is your issue and what does it have to do with the requests library?
",Lukasa,arkangelx
1542,2016-04-25 10:33:29,"i have pointed my todoapp to my own Parse.serverUrl as instructed, however I can’t perform a login/sign up now. I can save objects easily to my account with mLabs. So in essence

 Parse.$ = jQuery;

  // Initialize Parse with your Parse application javascript keys
     Parse.initialize(""5N5FqrkAmn1C8g6hVHvb"",""aFZ7RUqpgrUVZrDKfM6f”); // this works using the standard Parse sdk WITHOUT hosting myself (login/signup save data)
adding the line below to point to the heroic setup after importing all my data from my dashboard now doesn’t allow me to sign up

Logs

2016-04-25T09:54:06.692075+00:00 heroku[router]: at=info method=POST path=""/parse/1/login"" host=intense-hollows-60991.herokuapp.com request_id=d934f856-85be-4315-baf4-7bb64ac501c8 fwd=""5.148.6.116"" dyno=web.1 connect=1ms service=14ms status=403 bytes=466

So I tried to change the version of the parse sdk I’m using from 1.1.12-min.js to 1.6.14-min.js or parse-latest.js, this throws an error saying that Parse.Collections isn’t referenced.

   Parse.serverURL = 'https://intense-hollows-60991.herokuapp.com/parse/' <https://intense-hollows-60991.herokuapp.com/parse/'> (login/signup save data)

> On 25 Apr 2016, at 11:27, Cory Benfield notifications@github.com wrote:
> 
> @arkangelx https://github.com/arkangelx I'm sorry, what specifically is your issue and what does it have to do with the requests library?
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub https://github.com/kennethreitz/requests/issues/1542#issuecomment-214251913
",arkangelx,arkangelx
1542,2016-04-25 10:35:19,"@arkangelx That really seems to have nothing to do with the Requests library. The sample code you're providing appears to be Javascript code, which of course is not applicable to this Python library. So once again: where does the Python Requests library come into your problem?
",Lukasa,arkangelx
1542,2016-04-25 10:36:45,"My apologies, I posted to the wrong session

> On 25 Apr 2016, at 11:35, Cory Benfield notifications@github.com wrote:
> 
> @arkangelx https://github.com/arkangelx That really seems to have nothing to do with the Requests library. The sample code you're providing appears to be Javascript code, which of course is not applicable to this Python library. So once again: where does the Python Requests library come into your problem?
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub https://github.com/kennethreitz/requests/issues/1542#issuecomment-214257644
",arkangelx,arkangelx
1537,2013-08-21 18:29:06,"@Lukasa will this interfere with the undocumented streaming upload functionality?
",kennethreitz,Lukasa
1537,2013-08-21 18:55:36,"@kennethreitz I'm pretty sure it is documented. I'd tell you, but the docs are 503ing.
",Lukasa,kennethreitz
1537,2013-08-21 18:56:33,"@kennethreitz Yup: https://github.com/kennethreitz/requests/blob/master/docs/user/advanced.rst#streaming-uploads
",Lukasa,kennethreitz
1533,2013-08-23 05:08:18,"@jaraco If you desperately need a workaround you can change the header key on L398 of models.py to be `b'Content-Type'`. That should work. I'm not expecting a huge wait for the 2.0 release, but that's really up to Kenneth.
",Lukasa,jaraco
1524,2013-08-12 12:24:47,"Mm, I'm broadly with @sigmavirus24. Seeing as you'll need to monkeypatch _anyway_ to get any of your hooks to be called, it's not a huge chore to monkeypatch this method to do the right thing. If we were going to take this, we'd have to decide to take a whole 'custom hooks' package, and that just seems like massive unnecessary complexity.

Thanks for the pull request though, and please do keep contributing!
",Lukasa,sigmavirus24
1524,2013-08-12 17:43:41,"@piotr-dobrogost No-one ever talked about them. Not in blog posts, not to Kenneth, not in GitHub issues, no handlers used them. We also got, as far as I recall, no complaints when we removed them. =)
",Lukasa,piotr-dobrogost
1523,2013-08-12 11:42:45,"I'm with @Lukasa. This is convenient but non-obvious frankly. Most people will expect this to return the entire header not just one portion of what was returned. Even if this had a chance of making it I would want to rename it but there unfortunately isn't a better name.
",sigmavirus24,Lukasa
1522,2013-08-30 22:29:54,"@t-8ch Here are three machines I tried it on: (2 Windows7 boxes and 1 Ubuntu)

# 

c:\Python27\Scripts>..\python.exe
Python 2.7.3 (default, Apr 10 2012, 23:31:26) [MSC v.1500 32 bit (Intel)] on win
32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.

> > > import requests
> > > requests.get(""https://nonexistant-1111111111.blib.us"")
> > > <Hangs>

Ctrl+C does not work anymore.

# 

Machine 2 (Windows 7 again):

c:\Python27>python
Python 2.7.3 (default, Apr 10 2012, 23:31:26) [MSC v.1500 32 bit (Intel)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.

> > > import requests
> > > requests.get(""https://nonexistant-1111111111.blib.us"")
> > > Traceback (most recent call last):
> > >   File ""<stdin>"", line 1, in <module>
> > >   File ""c:\Python27\lib\site-packages\requests\api.py"", line 55, in get
> > >     return request('get', url, *_kwargs)
> > >   File ""c:\Python27\lib\site-packages\requests\api.py"", line 44, in request
> > >     return session.request(method=method, url=url, *_kwargs)
> > >   File ""c:\Python27\lib\site-packages\requests\sessions.py"", line 354, in request
> > >     resp = self.send(prep, *_send_kwargs)
> > >   File ""c:\Python27\lib\site-packages\requests\sessions.py"", line 460, in send
> > >     r = adapter.send(request, *_kwargs)
> > >   File ""c:\Python27\lib\site-packages\requests\adapters.py"", line 246, in send
> > >     raise ConnectionError(e)
> > > requests.exceptions.ConnectionError: HTTPSConnectionPool(host='nonexistant-1111111111.blib.us', port=443): Max retries exceeded with url: / (Caused by <class 'socket.error'>: [Errno 10054] An existing connection was forcibly closed by the remote host)

# 

Machine 3 (ubuntu box):

> > > requests.get(""https://nonexistant-1111111111.blib.us"")
> > > Traceback (most recent call last):
> > >   File ""<stdin>"", line 1, in <module>
> > >   File ""/usr/local/lib/python2.7/dist-packages/requests/api.py"", line 55, in get
> > >     return request('get', url, *_kwargs)
> > >   File ""/usr/local/lib/python2.7/dist-packages/requests/api.py"", line 44, in request
> > >     return session.request(method=method, url=url, *_kwargs)
> > >   File ""/usr/local/lib/python2.7/dist-packages/requests/sessions.py"", line 354, in request
> > >     resp = self.send(prep, *_send_kwargs)
> > >   File ""/usr/local/lib/python2.7/dist-packages/requests/sessions.py"", line 460, in send
> > >     r = adapter.send(request, *_kwargs)
> > >   File ""/usr/local/lib/python2.7/dist-packages/requests/adapters.py"", line 246, in send
> > >     raise ConnectionError(e)
> > > requests.exceptions.ConnectionError: HTTPSConnectionPool(host='nonexistant-1111111111.blib.us', port
> > > =443): Max retries exceeded with url: / (Caused by <class 'socket.error'>: [Errno 104] Connection re
> > > set by peer)

========================================== Back to Machine 1

On Machine 1 where things hang, if I change the timeout:

> > > requests.get(""https://nonexistant-1111111111.blib.us"", timeout=5.0)
> > > Traceback (most recent call last):
> > >   File ""<stdin>"", line 1, in <module>
> > >   File ""c:\Python27\lib\site-packages\requests-1.2.3-py2.7.egg\requests\api.py"",
> > >  line 55, in get
> > >     return request('get', url, *_kwargs)
> > >   File ""c:\Python27\lib\site-packages\requests-1.2.3-py2.7.egg\requests\api.py"",
> > >  line 44, in request
> > >     return session.request(method=method, url=url, *_kwargs)
> > >   File ""c:\Python27\lib\site-packages\requests-1.2.3-py2.7.egg\requests\sessions
> > > .py"", line 335, in request
> > >     resp = self.send(prep, *_send_kwargs)
> > >   File ""c:\Python27\lib\site-packages\requests-1.2.3-py2.7.egg\requests\sessions
> > > .py"", line 438, in send
> > >     r = adapter.send(request, *_kwargs)
> > >   File ""c:\Python27\lib\site-packages\requests-1.2.3-py2.7.egg\requests\adapters
> > > .py"", line 331, in send
> > >     raise SSLError(e)
> > > requests.exceptions.SSLError: _ssl.c:489: The handshake operation timed out

I am guessing that when one does not provide a timeout, this is the place where the code hangs? (And ctrl+c is disabled ? )
",pikumar,t-8ch
1522,2013-08-31 06:01:32,"Mm, this looks like it isn't a Requests issue. I do hope you work out what's going on, though, @pikumar!

Thanks again @t-8ch!
",Lukasa,t-8ch
1522,2013-08-31 06:01:32,"Mm, this looks like it isn't a Requests issue. I do hope you work out what's going on, though, @pikumar!

Thanks again @t-8ch!
",Lukasa,pikumar
1515,2013-08-03 18:15:31,"Looks like these test failures are transient, I think this is good. Thanks so much for your work @schlamar, as well as all of the urllib3 people who worked on this! =)
",Lukasa,schlamar
1507,2013-08-01 01:12:45,"Here's my hang-up. It consists of two parts:
1. > 90% of our users won't ever need this as such, our existing strategy is to use Request objects as organizational throw away objects, as such we never present them to the user directly as part of a Response. What we do allow is for a user to replicate what we do in order to send a PreparedRequest. So as the API is concerned, the most import objects are the Session and Response object. Next most important are PreparedRequest objects because those are directly exposed via the Response object and finally the Request object. I fail to see how we can not just explicitly document for the user that the Request object will be mutated. We're not using a purely functional language so there's no reasonable expectation that the Request will not be mutated.
2. The second part, contingent on that first, is that when you prepare a request, you're not going to get it back and so it may be mutated. If you're using this advanced API then you should have read the docs where we can explicitly document that those objects will be mutated. Does this hamper your use case? Yes. Does it meet the needs of what is likely > 90% of our users? More emphatically, yes.

I haven't skimmed your PR because it seems (from the conversation that I've read via email) that it's very much in flux. That said, if you can sell @Lukasa and me on why we need Ruby-ish methods here, we can probably sell @kennethreitz and frankly you haven't sold me.
",sigmavirus24,Lukasa
1507,2013-08-01 01:18:10,"@kennethreitz That's what this diff does.
",erydo,kennethreitz
1507,2013-08-01 06:10:58,"@erydo Despite what it looks like above, I really did want this change. =) Just wanted the best possible form of it. I think, looking at the diff that got merged, this is probably either that or extremely close to it. Thanks so much for your work! (And for putting up with my constant badgering. :grin:)
",Lukasa,erydo
1507,2013-08-01 06:24:10,"@Lukasa I actually very much appreciate the level of feedback on this :) And I'm glad it ended up with a better solution than my original suggestion. :+1: Thanks!
",erydo,Lukasa
1503,2013-07-30 07:27:44,"@skastel I think the answer here is yes. Here's why:

If the user passes the `files` parameter, Requests will _always_ do a `multipart/form-data` upload. Providing a simple plain text string as well doesn't easily work alongside that kind of upload, at least not in the current API. For this reason, I think the user should probably get a `ValueError` explaining that `data` may not be a string if the `files` parameter is present.
",Lukasa,skastel
1503,2013-07-30 13:50:55,"@Lukasa I'd actually much prefer a `ValueError` here, it would be clear that something is wrong with the request rather than making the request without uploading the files. So rather than silently ""succeeding"" it would explicitly fail and the raised error could indicate that a `data` was a string instead of a dict. I agree it changes the API, but I don't think this is part of the documented API, is it?
",skastel,Lukasa
1503,2013-07-30 13:53:20,"@skastel part of the documented API or not, it's a behaviour some people may (possibly unintentionally) rely on. People do weird things with code you release and they don't always tell you until you change it. This is a backwards incompatible change either way you slice it unfortunately.
",sigmavirus24,skastel
1503,2013-07-30 13:54:53,"@sigmavirus24 is right. We're fascistic about the API. It should be set in stone outside of major version increases. Happily, 2.0 isn't too far away, so this is well timed.
",Lukasa,sigmavirus24
1503,2013-08-01 01:22:34,"@Lukasa recommendation?
",kennethreitz,Lukasa
1503,2013-08-01 06:02:06,"I'm strongly +1 on changing this to throw an explanatory `ValueError`. If @skastel is happy to make that change, I'll be totally happy with merging this. =)
",Lukasa,skastel
1498,2016-05-18 14:00:03,"@yoyoprs Your code definitely blocks on `status_code`? You won't be able to read any json response because the response is incomplete, but the `status_code` will be fine unless the server hasn't responded _at all_.
",Lukasa,yoyoprs
1498,2016-05-18 14:45:54,"@Lukasa To sum up, i send file with some parameters, on server side according to the request parameters i start to read file **or** return a json with code 200. UWSGI doesn't like when the body is not consumed and raise error (IncompleteRead, i deal with it).
Request returned by server is ok (200) :



But i can't read `status_code` or json ...
",yoyoprs,Lukasa
1498,2016-05-18 16:28:07,"@Lukasa I just noticed that the variable `r` is no longer found in the namespace
No NameError **!**

tcpdump (no reset):



traceback :


",yoyoprs,Lukasa
1498,2016-05-18 16:43:44,"@Lukasa You are right, now i get `status_code` and `ChunkedEncodingError` exception is raised when i try to read `r.json()`. This behaviour suits me. Thank you :)
",yoyoprs,Lukasa
1498,2016-05-18 17:58:14,"@Lukasa One last question: I think there is a bug when i try to read a custom header (the problem seems to be string encoding)

Example OK (original custom content: _TODO.md_)



Example KO (original custom content: _hackiñg.txt_)



Some times (in second case) code block on printing `Transfer-Encoding` header
I use python 2.7
",yoyoprs,Lukasa
1497,2013-07-29 05:10:19,"@sigmavirus24 Yeah, I thought about that. Unfortunately, users can send directly into a Transport Adapter (it's a supported flow), and if they did that then they would lose this validation.
",Lukasa,sigmavirus24
1496,2013-07-27 07:20:34,"@tz18 For those approaching this issue in the future, please note that the fix is currently in the repository, it just hasn't been pushed to PyPI. Feel free to check out the GitHub code instead if you think you'll use NTLM.
",Lukasa,tz18
1495,2013-07-30 13:57:13,"@sholsapp is it possible to close this? You can open a pull request if and when you get these features accepted into urllib3. You can still reference this in that pull request but I cannot guarantee this will be accepted either way.
",sigmavirus24,sholsapp
1495,2014-10-31 11:02:15,"@sharpshadow You can do this today, and if you'd looked around a little bit longer: say, at the urllib3 issue linked above, or [at the docs](http://docs.python-requests.org/en/latest/user/quickstart/#post-a-multipart-encoded-file), you'd have found that out.

Files takes a dictionary of keys to tuples, with the following meanings of each place in the tuple:


",Lukasa,sharpshadow
1495,2014-10-31 20:42:20,"@Lukasa
Thank you very much. As mostly it was not the problem of missing ressources or bad dokumentation.
",sharpshadow,Lukasa
1493,2013-07-25 18:14:22,"Thanks for raising this issue @julie777!

I'm strictly +0 on this. I can see the utility of it, but I don't know that a context manager on `Response` is semantically the right way to approach this issue. If @kennethreitz is +1 on this, though, I think it'd make a good addition to V2.0.
",Lukasa,julie777
1493,2014-01-15 09:15:37,"I actually want to revisit this issue.

A not-uncommon use case of Requests is to open connections to download _part_ of a response, without downloading the whole body. In that case, to ensure that the connection pooling logic of `Session` objects still works, you need to make sure you call `Response.close()`. In this sort of use-case it does actually seem not-unreasonable to turn `Response` objects into context managers. @sigmavirus24, thoughts?
",Lukasa,sigmavirus24
1493,2014-01-16 03:04:36,"@pepijndevos It's not very hard to make objects context managers. (Not to sound like a pompous ass who's done this a million times)

@Lukasa I think I know what you're talking about -- there have been a fair number of bug reports/stackoverflow questions about this. I'm willing (because people seem to find `Response#close` annoying) to bend on this. I still think that logically it's a bit of nonsense but from a purely utilitarian perspective, it makes sense.
",sigmavirus24,pepijndevos
1493,2014-01-16 03:04:36,"@pepijndevos It's not very hard to make objects context managers. (Not to sound like a pompous ass who's done this a million times)

@Lukasa I think I know what you're talking about -- there have been a fair number of bug reports/stackoverflow questions about this. I'm willing (because people seem to find `Response#close` annoying) to bend on this. I still think that logically it's a bit of nonsense but from a purely utilitarian perspective, it makes sense.
",sigmavirus24,Lukasa
1493,2014-01-16 03:06:58,"@pepijndevos I totally missed what you meant (contextlib provides a lot of things to help _make_ context managers). I didn't realize that it would close the thing. On that topic I'm 100% for documenting that pattern instead of making the `Response` object a context manager unto itself.
",sigmavirus24,pepijndevos
1492,2013-07-25 18:11:50,"Thanks for this suggestion @julie777! Should be implemented by #1494.
",Lukasa,julie777
1489,2013-07-24 11:39:38,"Hi @kefir-, thanks for the Pull Request!

Unfortunately, I don't think this will be accepted. As mentioned in #1488, you can set the `Session.cookies` parameter to an arbitrary cookiejar without difficulty. That is the expected way to use a different CookieJar type.
",Lukasa,kefir-
1487,2013-08-01 01:14:09,"Yes. It does. Thank you @dpursehouse 
",sigmavirus24,dpursehouse
1486,2013-07-23 16:03:22,"@Lukasa love it :)
",kennethreitz,Lukasa
1481,2013-07-23 15:15:44,"Thanks so much for doing the detective work @t-8ch! The interface is very clear, you cannot use `Response.raw` unless `stream=True`, and that has been the case for a very long time. The `stream` parameter was introduced in 1.0.0 and the note you're referencing was also added in the same release.

With such a note in place, Requests was within its rights to do whatever it wants to the `raw` parameter when `stream` isn't set to `True`. I recommend opening a PR on easywebdav to fix this problem.

Thanks for raising this issue @skestle, and thanks so much for your investigation @t-8ch! :star2: :cake: :star2:
",Lukasa,t-8ch
1481,2013-07-23 15:15:44,"Thanks so much for doing the detective work @t-8ch! The interface is very clear, you cannot use `Response.raw` unless `stream=True`, and that has been the case for a very long time. The `stream` parameter was introduced in 1.0.0 and the note you're referencing was also added in the same release.

With such a note in place, Requests was within its rights to do whatever it wants to the `raw` parameter when `stream` isn't set to `True`. I recommend opening a PR on easywebdav to fix this problem.

Thanks for raising this issue @skestle, and thanks so much for your investigation @t-8ch! :star2: :cake: :star2:
",Lukasa,skestle
1481,2015-01-13 04:21:33,"@bryanhelmig your problem here is entirely unrelated to the bug originally reported. The original report is about consuming from requests' API first and then consuming from the raw attribute. This seems different to me even thought the message from the gzip library is the same
",sigmavirus24,bryanhelmig
1480,2013-08-13 20:00:55,"@Lukasa I said in my last message that's what I did.

Just to clarify - I did an `rm -fr venv && virtualenv venv`
",jsullivanlive,Lukasa
1480,2013-08-13 20:22:13,"These tracebacks make it look like you have an SSL problem. I shall now invoke Requests' resident SSL specialist by saying his name three times: @t-8ch @t-8ch @t-8ch 
",Lukasa,t-8ch
1480,2013-08-13 21:03:29,"@jsullivanlive I can't reproduce this on my end.
Could you provide a minimal code sample?
If I'm looking at your example about `tasks.py` I don't see any timeouts involved. On the other hand this behaviour should only occur if non-blocking mode is active which isn't the default.

A shot in the dark would be to go to `requests/packages/urllib3/util.py` and modify the `ssl_wrap_socket` function to look like this:


",t-8ch,jsullivanlive
1480,2013-08-13 23:07:23,"Thanks for the help, but this was my mistake - I had a local import in my breakout which had the line `from gevent import monkey; monkey.patch_socket()` embedded in it.  It apparently wasn't enough of a breakout!  When switching patch_socket to patch_all the gevent error goes away.  Maybe @rubik 's server is using patch_socket instead, I found that in examples of auto-reload servers.
",jsullivanlive,rubik
1480,2013-08-13 23:14:37,"@jsullivanlive: thanks for the cue, I'll look for that!
",rubik,jsullivanlive
1480,2013-08-14 11:52:23,"Thanks for posting your solution @jsullivanlive. :+1:
",sigmavirus24,jsullivanlive
1476,2013-07-21 13:54:51,"Actually, @Lukasa, you can do that for me with this issue. :-P
",sigmavirus24,Lukasa
1476,2013-07-26 15:48:20,"@kennethreitz is this good for merging?
",sigmavirus24,kennethreitz
1475,2013-07-20 20:09:08,"@sigmavirus24 i agree, but it's so easy, might as well. It's one line :)
",kennethreitz,sigmavirus24
1475,2013-08-02 02:48:40,"@kennethreitz indeed the best problem ever. Fwiw I'd be happy to help with httpbin's PRs.

Also @kracekumar we can rebuild him faster, stronger, better. ;)
",sigmavirus24,kennethreitz
1475,2013-08-02 02:48:40,"@kennethreitz indeed the best problem ever. Fwiw I'd be happy to help with httpbin's PRs.

Also @kracekumar we can rebuild him faster, stronger, better. ;)
",sigmavirus24,kracekumar
1475,2013-08-02 11:14:45,"Uh, @kennethreitz, I think you may have merged #1509 into this instead of the other way around...

**EDIT**: No, I'm just an idiot.
",Lukasa,kennethreitz
1473,2013-08-01 01:28:41,"@kennethreitz it's failing on travis because there's a related PR on HTTPbin that needs to be merged for the tests to pass. This doesn't need to be rebased at the moment, there's no merge conflicts according to GitHub.
",sigmavirus24,kennethreitz
1469,2013-07-19 13:00:57,"Hi @timmyschweer, thanks for opening this issue!

You should upload this as a file, rather than as a single giant data string. Try using:


",Lukasa,timmyschweer
1469,2013-07-19 13:13:00,"Hi @Lukasa, thank you for your reply =)
I already used the way you suggest. For small files that does work, but for large ones python crashes after 99% memory usage..
The idea was to use upload streaming without copying the whole file to memory:
http://docs.python-requests.org/en/latest/user/advanced/#streaming-uploads
",timmyschweer,Lukasa
1469,2013-07-19 13:46:09,"@Lukasa i already tried that! then the boundary is missing =/
",timmyschweer,Lukasa
1469,2013-07-19 15:14:46,"@Lukasa thank you for your brain muscle action!
I investigated the apache commons Fileupload class and it ONLY accepts multipart uploads with an boundary.. Due the fact that i can't write an FileUpload class that can handle streams without boundary i don't have a solution for my problem..

Is there another way of using



without copying the file to memory?
",timmyschweer,Lukasa
1469,2013-07-19 18:45:47,"okay, would never have thought it would work that simple... Dependent on the mediatype I'm using the file upload multipart parser or directly writing the HTTP stream to disk.

@Lukasa  thx for your help!

Here's my java restlet code:


",timmyschweer,Lukasa
1466,2013-07-19 11:12:27,"Brilliant stuff @s7v7nislands, thanks so much! :stars: 
",Lukasa,s7v7nislands
1465,2013-07-19 13:06:07,"Hi @dpursehouse, thanks for this PR!

I think this would be better placed as a subheading under the `Basic Authentication` section. Do you mind moving it?
",Lukasa,dpursehouse
1465,2013-07-19 13:41:55,"@Lukasa No problem.  I just added a new commit.
",dpursehouse,Lukasa
1464,2013-07-19 08:19:51,"Awesome, thanks for this @dpursehouse! :star2: 
",Lukasa,dpursehouse
1461,2013-07-17 16:48:54,"@t-8ch did you mean for us to add it to the copy of `cacert.pem` in requests or just for @heetderks to do that for himself? If the former, I'm not sure we should be adding extra certs to that file, especially if it seems to be a server issue.
",sigmavirus24,heetderks
1461,2013-07-17 16:48:54,"@t-8ch did you mean for us to add it to the copy of `cacert.pem` in requests or just for @heetderks to do that for himself? If the former, I'm not sure we should be adding extra certs to that file, especially if it seems to be a server issue.
",sigmavirus24,t-8ch
1461,2013-07-17 17:07:36,"I meant @heetderks. The certificate isn't included with Firefox or Chrome, so I don't think requests should ship it by default.
As you said, this is a server issue.
",t-8ch,heetderks
1459,2013-07-15 17:53:46,"I agree with @t-8ch's first two points. (It's worth noting that the first point actually requires upstream work.) I'm not really bothered about his third, I'd be happy to leave it as-is.

On top of that, I'd want to consider:
- Provide a `Session.build_request()` function (or equivalent). The biggest problem with preparing `Request`s yourself is that you have to reimplement portions of `Session.request()`. We should make it as easy as possible to not do that.
",Lukasa,t-8ch
1459,2013-07-16 02:32:35,"Also, I've been meaning to take a crack at #1166 and @Lukasa's suggestion to provide `Session.build_request()` would make that much much easier. `mock` makes it nice but making the methods as patchable as possible is also very nice.

For the record, I haven't looked closely at any of the vcr clones, but none of them really work the way I want them too. (Yes I know, that means I should make my own. But time is precious now that I have a real job.)
",sigmavirus24,Lukasa
1459,2013-09-04 02:45:16,"I think it is probably reasonable. Getting a huge improvement in proxy support for 2.0 would be fantastic. Make sure you submit your PR to the 2.0 branch and not master.

Thanks @Anorov 
:metal: :cake: :beer: :cactus: 
",sigmavirus24,Anorov
1459,2013-09-04 03:14:41,"@sigmavirus24 Yep, I fully agree.

I'd just like to get the go-ahead from @kennethreitz before fully continuing, in the event he thinks it's not deserving of being put into requests core.
",Anorov,kennethreitz
1459,2013-09-04 03:14:41,"@sigmavirus24 Yep, I fully agree.

I'd just like to get the go-ahead from @kennethreitz before fully continuing, in the event he thinks it's not deserving of being put into requests core.
",Anorov,sigmavirus24
1459,2013-09-05 06:52:17,"@kevinburke That looks awesome. Great job! It'll resolve some of our open issues, like #1577.

We need to propose our API changes. I really like the current clean form of `timeout`, so I think the best API is to do the following:
- Add a `requests.utils.Timeout` class (which is a simple subclass of the `urllib3` timeout class.
- Extend the `timeout` parameter to be a number or optionally an instance of the `Timeout` class.

@kevinburke, when this gets merged you should let me know, and I can either implement the Requests' side myself or code review your version of it. =)
",Lukasa,kevinburke
1459,2013-09-08 13:53:47,"@kevinburke request:

Allow the Timeout class's `__init__` method to take an instance of itself and handles `None`. If it does, we can always just easily do (on our end):



If you can do that :heart:
",sigmavirus24,kevinburke
1459,2013-09-08 14:17:30,"@sigmavirus24 The only issue there is that None has a specific meaning for timeouts - it implies never timeout, vs. some sentinel object which signifies ""use the system default timeout""
",kevinburke,sigmavirus24
1459,2013-09-08 18:05:28,"@kevinburke since this is going into 2.0 which do you think is better. Currently I believe we do not operate with any default timeout (i.e., None) so keeping that behaviour would not be a big deal. Using some ""system default"" (which I frankly wasn't aware existed) would be a breaking API change and would be okay since 2.0 allows for those.

I personally am in favor of keeping the current behaviour but I'm open to arguments for using a system default.
",sigmavirus24,kevinburke
1458,2013-07-13 13:15:29,"Thanks for the pull request @poirier! However, that's not a typo: it means 'highlight this like it's python in an interactive console'. =D
",Lukasa,poirier
1457,2013-07-13 09:09:57,"@t-8ch Response to your questions:




",liquidscorpio,t-8ch
1457,2013-07-13 09:16:20,"@t-8ch Let me try.
",liquidscorpio,t-8ch
1457,2013-07-13 09:21:23,"Ok, I cannot reproduce this on a fresh Fedora install, which means it's either a problem with your system setup or your network. I'm going to let @t-8ch take the lead on the system setup end because he knows it better than me. =)
",Lukasa,t-8ch
1457,2013-07-13 09:31:18,"@Lukasa Thanks for the help. I am setting up a virtenv for py3 and trying it out as per @t-8ch's sugesstion.
",liquidscorpio,t-8ch
1457,2013-07-13 09:31:18,"@Lukasa Thanks for the help. I am setting up a virtenv for py3 and trying it out as per @t-8ch's sugesstion.
",liquidscorpio,Lukasa
1457,2013-07-13 09:34:51,"@t-8ch Using python3, the problem persists


",liquidscorpio,t-8ch
1457,2013-07-15 08:56:22,"@t-8ch 
I messed up my Fedora 18 installation and did a clean install of F19; the problem persists. (Note: also persist on Mint 15). Here is the output of your code:


",liquidscorpio,t-8ch
1457,2013-07-15 10:48:35,"HEAD requests are not supposed to return any content in their body, so this is expected.
@Lukasa is right. Your installation of requests is seriously broken.
What do `requests.__file__` and `requests.__dict__` contain?
",t-8ch,Lukasa
1457,2013-07-17 17:39:22,"As and when I do figure out the situation, I will post it here. BTW, Thanks @Lukasa and @t-8ch, you have been quite helpful. :)
",liquidscorpio,t-8ch
1457,2013-07-17 17:39:22,"As and when I do figure out the situation, I will post it here. BTW, Thanks @Lukasa and @t-8ch, you have been quite helpful. :)
",liquidscorpio,Lukasa
1457,2013-08-01 11:20:08,"All right, now I have managed to get the last code posted by @t-8ch (using urllib3) to work properly and I am not getting any error even on https. So I guess my installation is now fine.

But still the original problem with requests persists. 
",liquidscorpio,t-8ch
1457,2013-08-02 01:32:16,"@drepo then please clarify what you mean by this:

> All right, now I have managed to get the last code posted by @t-8ch (using urllib3) to work properly and I am not getting any error even on https. So I guess my installation is now fine.

Because it sounds to me that the installation of requests is fine. The last code block @t-8ch posted involved trying to get `requests.__file__` and `requests.__dict__` which you couldn't previously do. Can you print that output now?
",sigmavirus24,t-8ch
1457,2013-08-02 01:38:53,"@sigmavirus24 That post was in reference to https://github.com/kennethreitz/requests/issues/1457#issuecomment-20958172 suggesting that my request installation is not good. But now the code is returning as expected.

However I am still facing the problem that I am stated in the first post.


",liquidscorpio,sigmavirus24
1456,2013-07-13 08:19:59,"@phndiaye Awesome, thanks for this! While we're here, do you want to fix up 208? It should be ""already_reported"".
",Lukasa,phndiaye
1453,2013-07-11 08:21:23,"Hi @alanhamlett, thanks for raising this issue!

Unfortunately, this would not be simple to do. Requests doesn't handle any of the low-level HTTP connection processing, we pass that off to [urllib3](https://github.com/shazow/urllib3). urllib3 also appears not to have any explicit hostname resolution calls in it, which means that it passes that job off to httplib in the standard library.

If httplib provided that feature this would be easy enough to plumb through, but it doesn't. This means any change made in urllib3 would have to circumvent the standard httplib hostname resolution behaviour, which would be pretty awkward. Altogether I doubt there's going to be much appetite for this feature.
",Lukasa,alanhamlett
1453,2016-07-15 07:10:27,"@rr- You're taking a long time to resolve `localhost`? That name should be served from your hosts file, and shouldn't take long _at all_. If you're only using `localhost` as an example, you should consider adding a hosts file entry for the server in question.
",Lukasa,rr-
1453,2016-07-15 07:19:18,"@rr- You should not need `/etc/hosts` for anything else except `localhost`.
If you need a DNS cache check out `nscd` or `dnsmasq`
",t-8ch,rr-
1452,2013-07-10 08:21:15,"Hi @s7v7nislands, thanks for opening this issue!

We're always open to having documentation updates! If you feel the need to make this change, we would expect Python 2 to be the version in the documentation.
",Lukasa,s7v7nislands
1452,2013-07-10 13:40:47,"@s7v7nislands basically I think it somewhat safe to assume that people using python 3000 realize that print is now a function that requires parentheses. As for displaying data, you can use `b'[{""repository"": ""kennethreitz/requests""}]'` because on python 2 `b'string' == 'string'` unlike python 3000 so that distinction is actually good to point out.

I guess, I'm saying that the better way is to put as much subtext in as possible (if that makes sense). I've had ""improve requests documentation"" on my todo list for a while now but I keep finding it hard to find the time. I know I won't have time this weekend but ideally next weekend I might be able to get to some of this. We really need to explicitly define the behaviour differences on python 2 and python 3. For simple examples, you can stick to whichever you feel is best. Python 2 (or python 3.3) might give you the most flexibility so you can clearly illustrate  ""HEY THIS IS UNICODE!"" with a `u'string'`.

And either way, it's good practice whether writing python 2 or 3 to use parentheses when writing print statements (especially if you want your code to work on both without much effort) so tacitly enforcing that with users is a good thing. :) In all the examples for github3.py (for [example](http://github3py.readthedocs.org/en/latest/#example)) I use syntax which I'm certain will work on python 2.6+ since the library works on python 2.6+. 
",sigmavirus24,s7v7nislands
1450,2013-07-10 07:10:19,"Thanks for the pull request @matthewlmcclure! Unfortunately, I think Ian is right on this one. We haven't had this feature for a long time and it's not really one that's missed.
",Lukasa,matthewlmcclure
1449,2013-07-09 20:54:43,"To elaborate on what @t-8ch has just said, this is a known bug in Requests and has been around for some time (see #1359). Until we manage to get proper HTTPS proxying support in urllib3 there is very little we can do about it. Try following @t-8ch's suggestions for the moment.
",Lukasa,t-8ch
1449,2013-07-10 02:36:35,"I know this isn't exactly helpful but thank you for such an excellent issue report. One thing I'd ask to see, out of some level of curiosity, is which version of openssl you're running. While @t-8ch and @lukasa are 100% correct, I wonder of this isn't an SSL issue being masked by proxies.
",sigmavirus24,t-8ch
1449,2013-07-10 03:03:28,"As a member of a debug team of a broad software stack, I understand the 
agony of trying to find the root cause of bugs from customers that don't 
submit detailed bug reports. It took a good chunk of time out of my 
lunch break to prepare it, but I just thought that making it easier on 
you guys would speed things along. I really appreciate the requests 
library and how easy it makes Python web interactions. I'd like to thank 
you guys for all the work you do to keep this open source project active 
and going in the right direction. If I do find a work around, I'll be 
sure to post it here so others can find it.

Feel free to close this if you deem it necessary.

On 7/9/2013 7:36 PM, Ian Cordasco wrote:

> I know this isn't exactly helpful but thank you for such an excellent 
> issue report. One thing I'd ask to see, out of some level of 
> curiosity, is which version of openssl you're running. While @t-8ch 
> https://github.com/t-8ch and @lukasa https://github.com/lukasa are 
> 100% correct, I wonder of this isn't an SSL issue being masked by proxies.
> 
> —
> Reply to this email directly or view it on GitHub 
> https://github.com/kennethreitz/requests/issues/1449#issuecomment-20718442.
",kylestev,t-8ch
1449,2013-07-10 06:17:14,"It's perfectly possible this is an SSL issue, SSL negotiation problems tend to show similar errors. I need to spend some time looking into the proxy stuff in Requests, and get clear in my head what we can and can't do. In the meantime, @kylestev, it would be worth trying to grab tcpdump from the connection: it's possible the SSL handshake isn't completing.
",Lukasa,kylestev
1449,2013-07-11 03:51:40,"I can do this tomorrow. I was out of office today and my dev machine (w/
rhel) is at the office. Will update tomorrow with tcpdump
On Jul 9, 2013 11:18 PM, ""Cory Benfield"" notifications@github.com wrote:

> It's perfectly possible this is an SSL issue, SSL negotiation problems
> tend to show similar errors. I need to spend some time looking into the
> proxy stuff in Requests, and get clear in my head what we can and can't do.
> In the meantime, @kylestev https://github.com/kylestev, it would be
> worth trying to grab tcpdump from the connection: it's possible the SSL
> handshake isn't completing.
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/issues/1449#issuecomment-20723581
> .
",kylestev,kylestev
1449,2013-07-13 02:08:59,"@kylestev any update? 
",sigmavirus24,kylestev
1446,2013-07-09 07:59:19,"Hi @yunruiwatts! Thanks for raising this issue!

The breaking change was 49a3664222d9e17ab83d089e4d1b1e30c998f59c. The reason for this change is that the old behaviour was incorrect. `Response.cookies` [is defined as](http://docs.python-requests.org/en/latest/api.html#requests.Response):

> A CookieJar of Cookies the server sent back.

The intention behind this documentation is that it's a CookieJar of Cookies the server sent back _on that response_. Those aren't the cookies you want, you want the ones the server sent on the _previous_ response.

There are two ways for you to get the cookies you want: either extract them from a `Session` or extract them from `r.history[-1]`. Either should work. If your use case is only that you're later going to make requests that you want to authenticate with that cookie, you should just be using a `Session` to send all your requests: that will handle cookie persistence for you.
",Lukasa,yunruiwatts
1445,2013-07-31 03:35:00,"I actually want this too.

To clarify the use case, I expect to be able to do something like this.



However, `Session.send` doesn't apply its configuration (e.g. `auth`) that way. Essentially, I'd like the ""thing that creates request objects"" independent of the Session.

In our real code, `FooApiRequests` represents a larger API request-factory creating many kinds of requests; and whose requests may get reused elsewhere for things like automated-retries-after-auth, etc.

I'd be _basically_ okay doing this:



but I think this would be even better:



In which `Session.prepare_request()` creates a `PreparedRequest` whose settings have been merged with the session's.

I'd be okay making that change and submitting a PR if it seems like a good solution. It's a non-breaking API change. @sigmavirus24 and @Lukasa, if I were to go ahead with that, would it be likely to be merged in? Any suggestions?
",erydo,Lukasa
1445,2013-07-31 03:35:00,"I actually want this too.

To clarify the use case, I expect to be able to do something like this.



However, `Session.send` doesn't apply its configuration (e.g. `auth`) that way. Essentially, I'd like the ""thing that creates request objects"" independent of the Session.

In our real code, `FooApiRequests` represents a larger API request-factory creating many kinds of requests; and whose requests may get reused elsewhere for things like automated-retries-after-auth, etc.

I'd be _basically_ okay doing this:



but I think this would be even better:



In which `Session.prepare_request()` creates a `PreparedRequest` whose settings have been merged with the session's.

I'd be okay making that change and submitting a PR if it seems like a good solution. It's a non-breaking API change. @sigmavirus24 and @Lukasa, if I were to go ahead with that, would it be likely to be merged in? Any suggestions?
",erydo,sigmavirus24
1442,2013-07-05 20:29:39,"Hi @jvanasco, thanks for opening this issue!

Have you tried `RequestsCookieJar.set_cookie()`? This should do your cookie adding. As for retrieving cookies, you should be able to iterate over the `RequestsCookieJar` to get the cookies out. Does that seem like what you need?
",Lukasa,jvanasco
1442,2013-07-06 20:57:12,"@sigmavirus24  done - https://github.com/kennethreitz/requests/issues/1443
",jvanasco,sigmavirus24
1440,2013-07-01 20:38:27,"Hi @fcurella! Thanks so much for the pull request! This is good work. :cake:

The relevant section of RFC 6265 is this one:



This suggests to me that the cookie value includes the double-quotes. Otherwise, the spec would read:



Of course, reading RFCs is a dark art, and so I could be wrong here. Nevertheless, I think we're safest if we assume that the quotes should be included. That way, if the remote end considers them semantically meaningful, it'll get them back; and if it doesn't, it should be stripping them (since it added them in the first place).

I do, however, agree that we shouldn't be _escaping_ them in the cookie values. =)
",Lukasa,fcurella
1440,2013-07-15 13:47:15,"@Lukasa: I've added https://github.com/kennethreitz/requests/pull/1440/files#L2R173 already, but I can write more. What kind of tests would like fro me to add?
",fcurella,Lukasa
1440,2013-07-15 13:51:57,"Ah, sorry, I totally missed that.

@kennethreitz this is good-to-go. =)
",Lukasa,kennethreitz
1439,2013-06-28 07:55:43,"This looks excellent @voberoi, thank you! :cake:
",Lukasa,voberoi
1437,2013-06-27 18:18:51,"Thanks so much @lukaszb! :cake:
",Lukasa,lukaszb
1436,2013-06-26 07:45:43,"Hi @yieldsfalsehood, thanks for raising this issue!

That verification logic is the odd-one-out in `Session.request()`, as it's the only thing that isn't directly related to preparing `PreparedRequest`s.

I think the correct behaviour would be to move the two blocks that handle verification into `Session.send()` instead. It shouldn't go into the Adapter because the Adapter doesn't know about trusting environment variables (and shouldn't). If you have time I encourage you to open a Pull Request, otherwise I'll get around to this later today. =)
",Lukasa,yieldsfalsehood
1434,2013-06-25 20:53:10,"Hi @chinux23, thanks for raising this issue! I agree, that looks fairly clearly wrong to me. I don't have time to fix it right now, so you should feel free to open a Pull Request if you want to fix it yourself. Otherwise, I'm marking this as Contributor Friendly until I can get around to it. =)
",Lukasa,chinux23
1433,2013-06-25 08:06:52,"Hi @jaraco, thanks for opening this issue! =)

I'll begin by addressing your actual problem. Simply put, the easiest way to resolve the password after the server has prompted for it is to not attach an auth handler to the original request, then check the response code:



If you feel as though this has too much boilerplate for you (which it would do if you do this a lot), take a look at what the Digest Auth handler does to handle 401 responses. You can easily write a Basic Auth handler that exhibits the same behaviour:



These should both allow the kind of delayed activation you want, and indeed the `handle_401()` hook should be able to block as long as necessary (e.g. to accept user input).

Hopefully these address your specific problem. Now I'll address your more philosophical questions.

One of the things that Requests desperately tries to avoid is the complexity of urllib2's password API. It's not that we don't think separating credential management and authentication is _ipso facto_ a bad idea, but the API is dire.

Requests is pragmatic. Design decisions are made with simplicity and pragmatism as the primary two concerns. This means we have to answer two questions:
1. Can this separation of concerns be implemented in a way that doesn't inconvenience the vast majority of users, who honestly couldn't care less?
2. Is this separation of concerns actually worth having in the library proper?

I'm not sure about (1), though my gut feeling is 'no'. I'm certainly open to someone proving me wrong. More importantly, I don't know that I think the answer to (2) is 'yes'. I'm prepared to be convinced, but at the moment I'm pretty comfortably leaning towards this being something that people who want it have to implement themselves.
",Lukasa,jaraco
1433,2013-06-25 16:23:01,"@Lukasa 
Thanks so much for the detailed and informative response.

I had gone down the 'handle_401' route, but didn't like it because it didn't seem reusable and seemed to combine too many factors (authentication, request/response workflow, password management), so that's why I abandoned it and ended up here.

I thought maybe I was just framing the problem wrong and sure enough, your simple example works very nicely for my needs.

Modeled after your example, I [implemented this](https://bitbucket.org/jaraco/lpaste/src/4e127b565867eb149b62d753d9c467cbfd196453/lpaste/lpaste.py?at=default#cl-139), which greatly simplified the implementation.

Regarding the separation of concerns, I believe it should be possible to provide an extensible mechanism for auth that separate concerns but degrade nicely to the simple, default pattern. I'll have to defer that work for later, though, and leave this issue closed in the meantime.
",jaraco,Lukasa
1430,2013-06-23 20:30:25,"Hi @Chris2048! Does the Hacker News API not treat the encoded version of the `+` character the same as an unencoded version?
",Lukasa,Chris2048
1428,2013-06-21 07:55:51,"HI @andrewgross, thanks for this Pull Request! I like the idea, I'm just wondering if there's a better place in the docs for this. I'm going to have a think about it, but I didn't want you to think no-one had seen your PR, so I thought I'd let you know. =)
",Lukasa,andrewgross
1427,2013-06-21 07:43:21,"Sorry @mrfatboy for not spotting this last night, but `add_dict_to_cookiejar()` doesn't allow for cookies specified in that manner. `add_dict_to_cookiejar()` creates a set of cookies whose names are equal to the keys of your dict, and whose values are their values. You can't set their properties using that method.

Instead, try the following:


",Lukasa,mrfatboy
1427,2013-06-21 16:50:22,"Thanks @Lukasa.  I found out a way to do by importing CookieJar, Cookie, and cookies.  I like your way better :)  However, with your way I was not able to specify the ""port_specified"", ""domain_specified"", ""domain_initial_dot"" or ""path_specified"" attributes.  The ""set"" method does it automatically with default values.   I'm trying to scrape a website and their cookie has different values in those attributes. As I am new to all of this I'm not sure if that really matters yet.   I was able to make this work with your help. :)

   my_cookie = {
        ""version"":0,
        ""name"":'COOKIE_NAME',
        ""value"":'true',
        ""port"":None,
//       ""port_specified"":False,
        ""domain"":'www.mydomain.com',
//       ""domain_specified"":False,
//        ""domain_initial_dot"":False,
        ""path"":'/',
//        ""path_specified"":True,
        ""secure"":False,
        ""expires"":None,
        ""discard"":True,
        ""comment"":None,
        ""comment_url"":None,
        ""rest"":{},  
        ""rfc2109"":False
    }

s = requests.Session()
s.cookies.set(**my_cookie)

note: the '//' were replacements for the '#'  (comment out) because they were causing funky formatting in this post.
",mrfatboy,Lukasa
1424,2013-06-17 20:49:03,"Cool, no worries. :)

!m @sigmavirus24 
",whit537,sigmavirus24
1423,2013-06-18 16:20:39,"Hi @gangli, thanks for opening this issue and thanks for providing a patch!

This is a fairly specialised use-case. Requests primarily attempts to solve the 90% use-case, which this does not come under. Additionally, we are currently in feature-freeze, which means we aren't accepting new features at this time.

Unfortunately, this means it's unlikely we are going to implement these changes in the main library itself.

I'm sorry that I can't be of more help. Please keep contributing to the library, and let me know if you need anything else! Thanks again. =) :cake:
",Lukasa,gangli
1420,2013-06-13 08:16:58,"Hi @jase1987! Thanks so much for this pull request. :cake:

Unfortunately, I think it's really very unlikely that we'll accept this. This has nothing to do with the PR itself: the code is in great shape. However, I don't think we want the feature.

Matrix parameters are really very, very infrequently used. The [URI RFC](http://pretty-rfc.herokuapp.com/RFC3986) does not include them, On top of that, the article you linked to doesn't just say they stopped being supported in '01: they hadn't been supported up until then _either_. I haven't done exhaustive research here, but it's quite possible they were never part of the URI (or URL) standards.

You've also only tackled a subset of the problem. Insanely, matrix parameters can apply to any (and all) portions of the path element, e.g. `http://example.com/res/categories;name=foo/objects;name=green/?page=1`. This is very difficult to cleanly represent in Requests' standard functional API.

Given that they're not part of the standard, are very infrequently used, and are difficult to do 'properly', I think we won't want to put this in mainline Requests. With that said, I encourage you to maintain a downstream fork of Requests that includes them if that would be useful to you.

Also, I'm very sorry that you have to interact with such an unhelpful REST API. =)

I'm going to leave this open until @kennethreitz takes a look, but I'd be surprised if he wanted to add support for matrix parameters.

Again, thanks so much for the work!
",Lukasa,jase1987
1420,2013-06-14 16:30:37,"The example you give @Lukasa looks like this is part of RFC 6570 and is supported in [uritemplate.py](/sigmavirus24/uritemplate).

If they can come up with templates for the URIs they're handling, they can pass the construction off to my library and then pass the string returned directly into requests. Unfortunately the library isn't well-documented at the moment, but it is well-tested and it follows the RFC to the 't'. @jase1987 you should look into ""Level 3"" expansions for semi-colon prefixed paths. It should fit your needs well. An example can be found in my tests [here](https://github.com/sigmavirus24/uritemplate/blob/master/test_uritemplate.py#L131). The template which is the key for the dictionary and the expected values are what should be of interest to you.
",sigmavirus24,jase1987
1420,2013-06-14 16:30:37,"The example you give @Lukasa looks like this is part of RFC 6570 and is supported in [uritemplate.py](/sigmavirus24/uritemplate).

If they can come up with templates for the URIs they're handling, they can pass the construction off to my library and then pass the string returned directly into requests. Unfortunately the library isn't well-documented at the moment, but it is well-tested and it follows the RFC to the 't'. @jase1987 you should look into ""Level 3"" expansions for semi-colon prefixed paths. It should fit your needs well. An example can be found in my tests [here](https://github.com/sigmavirus24/uritemplate/blob/master/test_uritemplate.py#L131). The template which is the key for the dictionary and the expected values are what should be of interest to you.
",sigmavirus24,Lukasa
1419,2013-06-15 05:50:42,"@Lukasa is correct :)

Thanks @kevinburke!
",kennethreitz,kevinburke
1419,2013-06-15 05:50:42,"@Lukasa is correct :)

Thanks @kevinburke!
",kennethreitz,Lukasa
1418,2013-06-12 08:09:22,"@t-8ch Thanks for pointing it out, when I get my internet connection back I'll do just that. =)
",Lukasa,t-8ch
1417,2013-06-11 13:10:21,"Hi @esaurito, thanks for raising this issue!

By design Requests' primary API supports the 90% use-case, which involves persisting cookies. We are very unlikely to add a parameter to the main methods to allow you not to persist cookies.

If you absolutely don't want to, my recommendation is that you use a session along with a subclass of the `RequestsCookieJar`. I haven't tried the following, but it ought to work:



Subsequent requests through that session should throw cookies away. Bear in mind that the `BlackHoleCookieJar` above is for example purposes only: in real code there are a few other methods you'd want to override, like `.copy()`.

Is this helpful?
",Lukasa,esaurito
1416,2013-06-11 08:31:40,"We did. =) This is fixed by 2ed976ea7147a9d0c18998e02b16d691b6798a3e, which hasn't yet made it into PyPI.

Thanks for pointing it out @michaelhelmick! The fix should be out fairly soon.
",Lukasa,michaelhelmick
1416,2013-07-26 08:26:16,"@tz18 The commit mentioned above has still not made it into PyPI. =)
",Lukasa,tz18
1416,2013-09-24 08:15:37,"@mayfield We need to get to a state where we're happy releasing the 2.0 release. =) For what it's worth we're not aware of any real problems caused by this, it just looks nasty.
",Lukasa,mayfield
1416,2014-09-20 22:28:33,"@jmrosal This bug was fixed a long time ago. You can tell, because it prevented _anyone_ from installing the library on Python 3, and affected version 1.2.3. We are now on version 2.4.1. Presumably we did not get through that many versions without noticing that no-one could install the package.

More importantly, your traceback doesn't seem to point to any requests code at all. I just did a test install of requests on Python 3 and found no problems. =)
",Lukasa,jmrosal
1416,2014-09-21 14:56:03,"@jmrosal you're using an abysmally old version of pip (1.3.1). It would seem (since you're inside a virtualenv) that you're also using an extremely old version of virtualenv. I would:
1. deactivate your virtualenv
2. upgrade virtualenv
3. recreate the virtualenv you're trying to work in
4. install requests
",sigmavirus24,jmrosal
1416,2014-09-22 19:10:45,"@jmrosal no worries. In the future you might try hopping onto IRC first or emailing one of us.
",sigmavirus24,jmrosal
1415,2013-06-11 08:26:36,"Hi @olemoudi, thanks for raising this issue!

This is actually being done in urllib3, the library that manages our HTTP connections. Specifically, [here](https://github.com/shazow/urllib3/blob/master/urllib3/response.py#L221).

Ordinarily I'd say that you should raise this issue on the urllib3 repository (and you are obviously free to do so), but in this case I don't consider this to be a bug. [RFC 2616 has the following to say](http://www.w3.org/Protocols/rfc2616/rfc2616-sec4.html#sec4.2):

> ""Multiple message-header fields with the same field-name MAY be present in a message if and only if the entire field-value for that header field is defined as a comma-separated list. It MUST be possible to combine the multiple header fields into one ""field-name: field-value"" pair, without changing the semantics of the message, by appending each subsequent field-value to the first, each separated by a comma.

Given that there is a defined behaviour for multiple header values, we would need a very good argument for complicating the general (RFC specified and correct) case for the sake of making easier the unusual (contrary to RFC and incorrect) case.
",Lukasa,olemoudi
1415,2013-06-11 10:20:07,"Makes perfect sense!

Thanks for the reply. Keep up the great work.

Martin

On Tue, Jun 11, 2013 at 10:26 AM, Cory Benfield notifications@github.comwrote:

> Hi @olemoudi https://github.com/olemoudi, thanks for raising this issue!
> 
> This is actually being done in urllib3, the library that manages our HTTP
> connections. Specifically, herehttps://github.com/shazow/urllib3/blob/master/urllib3/response.py#L221
> .
> 
> Ordinarily I'd say that you should raise this issue on the urllib3
> repository (and you are obviously free to do so), but in this case I don't
> consider this to be a bug. RFC 2616 has the following to sayhttp://www.w3.org/Protocols/rfc2616/rfc2616-sec4.html#sec4.2
> :
> 
> ""Multiple message-header fields with the same field-name MAY be present in
> a message if and only if the entire field-value for that header field is
> defined as a comma-separated list. It MUST be possible to combine the
> multiple header fields into one ""field-name: field-value"" pair, without
> changing the semantics of the message, by appending each subsequent
> field-value to the first, each separated by a comma.
> 
> Given that there is a defined behaviour for multiple header values, we
> would need a very good argument for complicating the general (RFC specified
> and correct) case for the sake of making easier the unusual (contrary to
> RFC and incorrect) case.
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/issues/1415#issuecomment-19248592
> .
",olemoudi,olemoudi
1415,2013-06-23 17:14:58,"@Lukasa 
You linked to the master branch which is a moving target which made your link obsolete by now... Please remember to always link to specific revision/tag :)

@olemoudi 
Merging headers often is asking for trouble. See http://bugs.python.org/issue1660009 for details. This is fixed in Python 3 - headers are not being merged. See http://lists.w3.org/Archives/Public/ietf-http-wg/2013JanMar/0016.html for an example of problems with merged headers.
",piotr-dobrogost,olemoudi
1415,2013-06-23 17:14:58,"@Lukasa 
You linked to the master branch which is a moving target which made your link obsolete by now... Please remember to always link to specific revision/tag :)

@olemoudi 
Merging headers often is asking for trouble. See http://bugs.python.org/issue1660009 for details. This is fixed in Python 3 - headers are not being merged. See http://lists.w3.org/Archives/Public/ietf-http-wg/2013JanMar/0016.html for an example of problems with merged headers.
",piotr-dobrogost,Lukasa
1414,2013-06-12 18:12:15,"@Lukasa feel free to merge something like this next time :)
",kennethreitz,Lukasa
1410,2013-06-07 10:41:13,"Hi @matheusgr, thanks for opening this issue!

To be clear, are you experiencing that we're not sending cookies that we should be sending, or are you simply frustrated about being unable to easily see the full set of cookies? If it's the second, have you tried looking at `Session.cookies`?
",Lukasa,matheusgr
1410,2013-06-08 07:53:23,"@matheusgr That shouldn't be true unless the remote server is unsetting them. If the remote server _is_ unsetting them then the previous behaviour was actually a bug. Are you finding that we're definitely losing the other cookies?
",Lukasa,matheusgr
1409,2013-06-07 10:37:55,"Hi @rsalmond, thanks for opening this issue! I just wanted to let you know that I've seen this and will circle back to it at some point this weekend when I have actual time to address it. =)
",Lukasa,rsalmond
1409,2013-06-08 08:00:26,"Can I just clarify, @rsalmond: are you trying to POST json-encoded data, or are you expecting to POST a normal HTML form?
",Lukasa,rsalmond
1409,2013-06-12 15:11:14,"Sorry @rsalmond, I lost track of this briefly.

I think @sigmavirus24 has basically got the right idea here. JSON has to be the way to go, and your two options there are either to send multipart data with JSON in one part and the file in another, or to JSONify the whole thing. I'd go with his idea for now.

Btw, on the topic of nested dictionaries working without files, I don't see that behaviour:


",Lukasa,sigmavirus24
1409,2013-06-12 15:11:14,"Sorry @rsalmond, I lost track of this briefly.

I think @sigmavirus24 has basically got the right idea here. JSON has to be the way to go, and your two options there are either to send multipart data with JSON in one part and the file in another, or to JSONify the whole thing. I'd go with his idea for now.

Btw, on the topic of nested dictionaries working without files, I don't see that behaviour:


",Lukasa,rsalmond
1408,2013-06-07 10:21:18,"Hi @wasw100, thanks for raising this pull request!

Unfortunately, it's not clear to me whether we want to continue supporting adding Morsels to `RequestsCookieJars`. Can I get a call on this @kennethreitz?
",Lukasa,wasw100
1408,2013-06-07 10:32:36,"Hi @Lukasa , thanks for your reply.

If we handler response cookie by ourself, Morsels to RequestCookieJar is usefull.
I use these methods in other projectes.
",wasw100,Lukasa
1406,2013-06-06 08:52:33,"Hi @carlio, thanks for raising this issue!

This has been raised before.  I'm increasingly bothered by it, but the keyword argument is really useful. For the moment, downstream libraries like github3.py can wrap their file-like objects in classes that accept-and-drop this keyword argument. That's unsatisfying, but in the interim is probably the way to go.
",Lukasa,carlio
1405,2013-06-04 08:00:41,"Hi @borfig, thanks for this pull request!

As @t-8ch said, in its current form we can't accept this Pull Request. We do not make local changes to urllib3, so the urllib3 portions of this work would have to be contributed separately upstream. You should feel free to do that if you'd like to. As for why we have vendored urllib3 instead of using a git submodule, I'll point you at the discussion in #1384 which went over the decision in great detail. (Short answer: the standard Python repository structure does not lend itself to being a git submodule.)

Approaching this more generally, Requests has very strong opinions about SSL. Even if this feature was added to urllib3, I don't think we'd add it to the functional Requests API. Instead, I think we'd expect people to use Transport Adapters (as @t-8ch has demonstrated) to get that functionality.
",Lukasa,borfig
1405,2013-06-04 08:00:41,"Hi @borfig, thanks for this pull request!

As @t-8ch said, in its current form we can't accept this Pull Request. We do not make local changes to urllib3, so the urllib3 portions of this work would have to be contributed separately upstream. You should feel free to do that if you'd like to. As for why we have vendored urllib3 instead of using a git submodule, I'll point you at the discussion in #1384 which went over the decision in great detail. (Short answer: the standard Python repository structure does not lend itself to being a git submodule.)

Approaching this more generally, Requests has very strong opinions about SSL. Even if this feature was added to urllib3, I don't think we'd add it to the functional Requests API. Instead, I think we'd expect people to use Transport Adapters (as @t-8ch has demonstrated) to get that functionality.
",Lukasa,t-8ch
1405,2013-06-08 08:13:13,"This has been implemented by @borfig in shazow/urllib3#194. It has already been merged into master.
",t-8ch,borfig
1401,2013-06-03 09:03:26,"Hi @foxx, thanks for opening this issue! Let me address these in no particular order.
1. Implementing .read(). Requests explicitly provides the underlying transport library's notion of a response in the `Response.raw` property. If you believe that `decode_content=True` should be the default behaviour (and I agree that it should), I recommend you open a pull request to that effect on [urllib3](https://github.com/shazow/urllib3). I'm sure @shazow will be happy to take a look at it. =)
2. Changing the default read size is a bit of an odder one. Given the point I made in **1**, we probably can't do it in Requests itself. I would also argue that any method named `read()` should behave like a file-like object, which doesn't have a minimum read value. When I get internet in my new house, I'm going to take a look at implementing the idea in shazow/urllib3#186, which should be able to paper over this issue.
3. Default `iter_content()` size is a sore topic around here: see #844, which has still not been completely resolved. Your input is welcome there. =)
4. Where is the memory leak in `iter_content()`?
",Lukasa,foxx
1400,2013-06-08 08:04:53,"@jam _bump_. :smile:
",Lukasa,jam
1397,2013-06-06 21:44:02,"@Lukasa Perhaps requests could wrap the exception and display that? `The server specified a chunked Transfer-Encoding response but did not send chunked data.` or something along those lines.
",Anorov,Lukasa
1397,2013-07-25 14:39:34,"Interesting! This is some awesome detective work @kracekumar! Does look like a very specific bug here, but I'm still not convinced we can neatly work our way around it.
",Lukasa,kracekumar
1396,2013-06-10 13:33:14,"Not sure what path I'd use for httpie's requirements file; there doesn't seem to be one I can spot on the PyPI page for the package. However, on a whim, I went ahead & just tried `pip install --upgrade httpie` again, and now it's working like a charm.

@sigmavirus24, I suspect you're right, that I was ending up at a mirror that was out-of-date, and now I'm either pulling from PyPI directly, or that mirror or mirrors have caught up.

Gonna close this out, as it appears to have resolved itself independently. Thanks all for your suggestions/help!
",jeffbyrnes,sigmavirus24
1395,2013-07-03 15:52:48,"Hi @lukesneeringer!

If you bring up the blame on that section of code, you'll notice that I added that comment on the 18th of June, when I proposed pull request #1425, which added urllib3 code that I contributed to that library as part of shazow/urllib3#198 (merged on the 17th of June). The comment you're quoting was made on the 30th of May, more than two weeks before the code in question made it into the library. Previously there was no urllib3 special-case.

However, as you rightly point out, now there is. I think we can safely remove the arguments to the `.read()` call now, as we shouldn't be using it with urllib3.
",Lukasa,lukesneeringer
1391,2013-05-29 09:04:12,"Hi @revolunet, thanks for the fix!

Can I suggest that, instead of using the phrase ""status between 400 and 600"", you use the phrase ""a 4XX client error or 5XX server error response""? It makes it clearer that these errors are categories, not a continuum of numbers.
",Lukasa,revolunet
1391,2013-05-29 09:56:40,"thanks @Lukasa you're perfectly right. i'll close this one and issue another PR
",revolunet,Lukasa
1390,2016-06-09 04:27:02,"We have some unofficial plans to unveil something around async within the next year, @Lukasa and I were discussing this at PyCon.

It's something that will take some time and a lot of energy to introduce, and would no longer be a ""tacked on"" bit of functionality like the original ""requests.async"" (now known as ""grequests"") was. 
",kennethreitz,Lukasa
1390,2016-06-09 04:30:46,"I added some comments here, which I will re-iterate: https://github.com/kennethreitz/requests/issues/1390#issuecomment-224797206

---

We have some unofficial plans to unveil something around async within the next year, @Lukasa and I were discussing this at PyCon.

It's something that will take some time and a lot of energy to introduce, and would no longer be a ""tacked on"" bit of functionality like the original ""requests.async"" (now known as ""grequests"") was.
",kennethreitz,Lukasa
1390,2016-06-09 04:54:57,"@kennethreitz @Lukasa It's something that I seriously need so am happy to contribute.  Would you be willing to take help contributing to it?  I am CTO of my company and have been using python for years.
",smorin,kennethreitz
1390,2016-06-09 04:54:57,"@kennethreitz @Lukasa It's something that I seriously need so am happy to contribute.  Would you be willing to take help contributing to it?  I am CTO of my company and have been using python for years.
",smorin,Lukasa
1390,2016-06-09 07:36:17,"@Lukasa I seem to recall it sounding like defereds (perhaps with a slight (upcoming) alteration) would be the best implementation approach for us?
",kennethreitz,Lukasa
1390,2016-06-09 08:33:40,"@kennethreitz The answer to that is _maybe_. The problem with using Deferreds is that Deferreds don't make things magically async: they're literally just fancy callback containers. We can almost certainly _use_ Deferreds to build an abstraction layer on top of multiple concurrency models if that's a route we want to go.

This is a problem that really needs to be approached _very_ carefully, because as long as Python 2.7 is around we cannot unconditionally expect that the standard library will have an event loop inside it that we can use. That represents a concern: we need a way to execute _without_ an event loop. This is why Deferreds are attractive: they can execute in just such a manner because they're _literally_ just callback holders.

So one approach here would be to write an entirely callback-based HTTP/1.1 + HTTP/2 client library that uses Deferreds (probably on top of @njsmith's [h11](https://github.com/njsmith/h11) and @python-hyper's [h2](https://github.com/python-hyper/h2)).

We could then try to think about how we can integrate that into the various different event loops, though again, I'm not 100% certain of how we'd do that best. _Probably_ we'd have to write (sigh) an event-loop abstraction layer that basically defines certain functions as returning deferreds (""data_from_network""), and that then plugs into the relevant event loops. In practice, though I've said ""event loops"", we really only need two implementations to get started: one that implements the Twisted/asyncio ""protocol"" notion, and one that is synchronous and blocking (so that we continue to work in the absence of an event loop). In fact, we could even just start with the second one.

This is one of those things that it'd be really interesting to get someone like @glyph to weigh in on, in part because @glyph is strongly incentivised to help us get this right (no more treq!) and in part because he's a lot smarter than me. Certainly, however, I'd love to have a version of requests that you can either use synchronously in its current form without noticing the difference _or_ that you can use on top of ${EVENT_LOOP}. Requests provides enough goodness that we should really reimplement as little of it as possible on different platforms.
",Lukasa,kennethreitz
1390,2016-06-09 17:18:59,"@Lukasa @kennethreitz Guess the question is exactly what to support.  In a ideal world it would be a universal design that's elegant to support both, in practice if you wanted to support both a event loop model and a twisted model, then maybe it would be better to support two models one for each.  Alternatively I think a lot of people are using (Gevent/Asyncio/Trollius) so maybe work on a implementation that supports that then see how the model can be altered to include twisted?

Above you mention ""In practice, though I've said ""event loops"", we really only need two implementations to get started: one that implements the Twisted/asyncio ""protocol"" notion, and one that is synchronous and blocking (so that we continue to work in the absence of an event loop)""

My initial thoughts are is there a reason to majorly reimplement or alter the existing requests library aka (""one that is synchronous and blocking"") and just maybe cleanly refactor things under the hood so the can be shared between the ""synchronous"" implementation and the ""async"" implementation?  Or is there a motivating factor beyond that I am not aware of?

@smorin 
",smorin,kennethreitz
1390,2016-06-09 17:18:59,"@Lukasa @kennethreitz Guess the question is exactly what to support.  In a ideal world it would be a universal design that's elegant to support both, in practice if you wanted to support both a event loop model and a twisted model, then maybe it would be better to support two models one for each.  Alternatively I think a lot of people are using (Gevent/Asyncio/Trollius) so maybe work on a implementation that supports that then see how the model can be altered to include twisted?

Above you mention ""In practice, though I've said ""event loops"", we really only need two implementations to get started: one that implements the Twisted/asyncio ""protocol"" notion, and one that is synchronous and blocking (so that we continue to work in the absence of an event loop)""

My initial thoughts are is there a reason to majorly reimplement or alter the existing requests library aka (""one that is synchronous and blocking"") and just maybe cleanly refactor things under the hood so the can be shared between the ""synchronous"" implementation and the ""async"" implementation?  Or is there a motivating factor beyond that I am not aware of?

@smorin 
",smorin,Lukasa
1390,2016-06-09 17:18:59,"@Lukasa @kennethreitz Guess the question is exactly what to support.  In a ideal world it would be a universal design that's elegant to support both, in practice if you wanted to support both a event loop model and a twisted model, then maybe it would be better to support two models one for each.  Alternatively I think a lot of people are using (Gevent/Asyncio/Trollius) so maybe work on a implementation that supports that then see how the model can be altered to include twisted?

Above you mention ""In practice, though I've said ""event loops"", we really only need two implementations to get started: one that implements the Twisted/asyncio ""protocol"" notion, and one that is synchronous and blocking (so that we continue to work in the absence of an event loop)""

My initial thoughts are is there a reason to majorly reimplement or alter the existing requests library aka (""one that is synchronous and blocking"") and just maybe cleanly refactor things under the hood so the can be shared between the ""synchronous"" implementation and the ""async"" implementation?  Or is there a motivating factor beyond that I am not aware of?

@smorin 
",smorin,smorin
1390,2016-06-09 17:54:58,"@smorin the idea is that async is going to be a lot more important in the future than it is now (as is most likely should be, especially for io-bound tasks), and Requests both wants to stay ahead for the long-term future and help people build better software.

Nothing would change about the current API, except it may be secretly powered by the new asynchronous methods under the hood, just in a blocking fashion. 
",kennethreitz,smorin
1390,2016-06-11 02:55:28,"It's at least an interesting exercise to try and imagine what the core of requests would look like if written in the style of h2/h11. Even if in the end it turns out to not be the best approach -- it's certainly more complex than any of the existing examples, so I guess we'd at least learn something :-). (And hey @Lukasa, what's this backsliding to concrete I/O APIs as soon as the problem gets more complicated? ;-))

Wildly sketching, since I have no idea how the internals of requests actually look in any kind of detail: I guess the basic I/O operations that requests needs to do are to request a connection (possibly with TLS and possibly with its own hacks to the TLS handshake logic), to do reads and writes on a connection, maybe to set socket options, and to close a connection. And while in h2/h11 the basic state object is a Connection that manages a single socket, for requests it would be a Session that manages a socket pool.

And we'd want to make it possible to wrap this in different APIs like:
- `blocking_session.get(...)` -> returns a `BlockingResponse`
- `await coroutine_session.get(...)` -> returns a `CoroutineResponse`
- `twisted_session.get(...)` -> returns a `Deferred` that when fired returns a `TwistedResponse`

(the different response classes would differ in the API they exposed for streaming reads of the response body)

Since the user is going to be calling methods on individual response wrapper objects, probably we want to let each of these wrappers handle the I/O coupling for each connection. So maybe the API would look something like this:



I dunno, maybe this is only interesting to me :-). Certainly it would be a lot of work, but the result might be rather nice...
",njsmith,Lukasa
1390,2016-06-11 13:23:36,"Heh @njsmith, this is a problem because you don't know what Requests looks like internally. =)

The internals of Requests itself already _are_ very, very close to the design of h2/h11: that is, they almost entirely twiddle data around in in-memory objects. Unlike in aiohttp where it seems like every single function/class in the library touches or handles I/O in some way, the vast majority of requests is function calls.

The functions that either directly or transitively do I/O or that would need to be adjusted in some way are the following:
- `requests.api.*`: all of these transitively wrap `Session.request`.
- `requests.sessions.Session.request`: this transitively calls `Session.send`, which actually does do I/O
- `requests.sessions.SessionRedirectMixin.resolve_redirects`: this both reads and writes (using `Session.send`), and so is clearly an I/O based function, but mostly just uses the primitives we'd have to work out at the lower level.
- `requests.sessions.Session.send`: This is the _only_ top-level requests function that actually does I/O (excepting file reads/writes which don't count because Unix).
  `requests.adapters.HTTPAdapter.send`: This does the bulk of the low-level I/O, though it calls one helper function it defines on itself that is I/O-adjacent, namely...
- `requests.adapters.HTTPAdapter.get_connection`: This obtains a connection object from urllib3's connection pool. This is going to be different in concurrent and synchronous modes because the way we wait for connections to show up will be different.

In this case, for requests, I'd ideally like to touch _only_ those functions. That ensures that the rest of Requests goes on functioning exactly as it does today. This reduces the churn in Requests and reduces the odds of us introducing bugs in terrible places.

From my perspective, the easiest way to do this is to just write `@asyncio.coroutine` on top of all of them and throw some `yield from`s in there, then wrap it all in `asyncio.run_until_complete` if people want to use the blocking APIs.

However, if we decide we need a separate blocking backend this _all_ gets way harder. In that kind of model, your approach may work slightly better, because it may be quite painful (and indeed impossible?) to use `Deferred`s in such a way that the top-level user of requests doesn't have to think about them in synchronous code. In that kind of model, we actually do need to write things quite differently, and at _that_ point I'm willing to discuss tearing the whole library apart to rebuild it.

Otherwise, I'm extremely reluctant to do a big rewrite: there's just no way that we'll persist all the useful functionality that the library provides in its current form. =(
",Lukasa,njsmith
1390,2016-06-11 21:32:13,"> Ah, right, I'm assuming that requests dropping support for python 2 is still like... 3-5 years away (is that wrong?)

Heh, that's part of what this issue is about. =D

I think this whole issue is going to be part of my list of ""things I want to deal with in the next year"". It's my genuine belief that Requests needs its own async story. Right now I'm mostly interested in soliciting opinions about how we could do it (which is why I also tagged @glyph and @shazow).

However, from my perspective, all options @kennethreitz will entertain are on the table. If @kennethreitz is willing to consider dropping Python 2 support at some time soon, I'm willing to consider that approach, even if we don't end up doing it. 
",Lukasa,kennethreitz
1390,2016-06-11 21:32:13,"> Ah, right, I'm assuming that requests dropping support for python 2 is still like... 3-5 years away (is that wrong?)

Heh, that's part of what this issue is about. =D

I think this whole issue is going to be part of my list of ""things I want to deal with in the next year"". It's my genuine belief that Requests needs its own async story. Right now I'm mostly interested in soliciting opinions about how we could do it (which is why I also tagged @glyph and @shazow).

However, from my perspective, all options @kennethreitz will entertain are on the table. If @kennethreitz is willing to consider dropping Python 2 support at some time soon, I'm willing to consider that approach, even if we don't end up doing it. 
",Lukasa,shazow
1390,2016-06-11 21:32:13,"> Ah, right, I'm assuming that requests dropping support for python 2 is still like... 3-5 years away (is that wrong?)

Heh, that's part of what this issue is about. =D

I think this whole issue is going to be part of my list of ""things I want to deal with in the next year"". It's my genuine belief that Requests needs its own async story. Right now I'm mostly interested in soliciting opinions about how we could do it (which is why I also tagged @glyph and @shazow).

However, from my perspective, all options @kennethreitz will entertain are on the table. If @kennethreitz is willing to consider dropping Python 2 support at some time soon, I'm willing to consider that approach, even if we don't end up doing it. 
",Lukasa,glyph
1390,2016-06-17 03:34:45,"@kennethreitz - so are you thinking purely in terms of asyncio.Future objects?  You... _might_ be able to vendor in trollius to get a basic asyncio event loop going.
",glyph,kennethreitz
1390,2016-07-09 02:34:19,"Since @Lukasa was so kind in his assessment of my abilities I figure I should weigh in pretty substantively.  (As always, my estimate last month of ""a couple of days"" was hilariously optimistic, given that I had another conference and a week of vacation to do after that...)

Let me first address some of the questions around the possibility of using `Deferred` directly to facilitate asynchrony.

There's a stand-alone version of Deferred available from &lt;https://pypi.python.org/pypi/deferred&gt;.  I've recently been added as a package index owner, and we (Twisted generally) plan to actively maintain this going forward, and hopefully split it out of Twisted entirely.  This is a much smaller dependency than asyncio or one of its various clones, mostly due to the fact that it does not do any I/O; it's _just_ the callback abstraction.  Right now it still includes stuff like `DeferredFilesystemLock` but we are probably going to remove that and leave it in Twisted proper.

There are two problems that `Deferred` could potentially solve within `requests`.  Let me start with the simpler one: providing a front-end for Twisted - replacing `treq`.  Whatever asynchronous solution you end up going with, it should be easy enough to have a `requests.frontend.twisted` which translates whatever not-yet-available-result.

The other, more substantive one, is providing a common abstraction between the different layers within requests to compose with each other.  @Lukasa laid out some of these layers [in this comment](https://github.com/kennethreitz/requests/issues/1390#issuecomment-225361421).  My understanding is pretty superficial, but at the highest level, `requests.get` wraps `Session.request` and expects it to block, and then returns its result.  What you would be using `Deferred` for here would be to have a single return type which could stand in for ""expects it to block"", and anything that currently blocks could simply start returning a `Deferred`, which would be fired by the external I/O machinery feeding bytes into it somehow, h11-style.

If you have to continue supporting Python 2, `Deferred` is definitely a friendlier abstraction for this than a `Future`-alike, since `asyncio.Future`'s extremely spare API is specifically designed to be a primitive that you interact with indirectly, behind the syntactic convenience of `await`.  (I think it's still a better internal abstraction )

The question then is, having retrofitted everything internally to use this new non-blocking abstraction, how do you still support blocking code?  This is actually a bit more generic than just `Deferred`, because any non-blocking approach will have the same problem, but I'll discuss it in the language of `Deferred` because it provides an easier shorthand :).  This breaks down into two sub-problems: how do you deal with external callers calling into e.g. `requests.get`, and how do you facilitate plugging in a blocking implementation of something that lives in the middle of the stack?

For the former, the answer is really simple: you just have a wrapper that lives at the edge, which presents a blocking API, and when called, dispatches into an event loop which just does I/O until the `Deferred` returned by the lower level has fired.  Doing this in a natively asynchronous program is problematic because the whole point is you want to share an event loop and you don't want to use it re-entrantly (and Twisted, in particular, has never really been retrofitted all the way to make loops easy to instantiate, although we'll get there one day).  But your ""event loop"" can just be a little function that does blocking `socket.recv` / `socket.send` and stuffs the results into the underlying I/O layer; you don't need full-blown event-driven concurrency in this layer.

To plug something in in the middle is a bit more complex in implementation, but essentially the same conceptually: at every possible public integration point, you have a wrapper for the layer below like the one I just described, and you have a wrapper for the layer above which just calls the blocking thing and then wraps the result in an already-fired `Deferred`; this is sort of what [`succeed`](https://twistedmatrix.com/documents/16.2.0/api/twisted.internet.defer.html#succeed) is for.

The original pioneer of the [synchronous `Deferred` approach](https://github.com/radix/synchronous-deferred), @radix, would tell you to use his newer library, [effect](https://github.com/python-effect/effect), to thread the non-blocking needle through the core of `requests`.  And he might even be right!  This is really a matter of taste; functionally the way you'd be writing code and dealing with the edges of the system where things become blocking or attach to a concrete event loop is similar.  Effect has a bit more action-at-a-distance which can be a little tricky to reason about - which is by design, the separation of intents and performers is one of its architectural features - but which also facilitates dispatching to different backends which requests might need.

I'll probably have more thoughts on this later, but I should probably yield the floor at this point - we've still got 11 months before this needs to roll out, right? :)
",glyph,Lukasa
1389,2013-06-08 08:05:35,"@lovesh Does using a `Session` resolve your problem?
",Lukasa,lovesh
1389,2013-06-08 08:21:38,"Yes it does.
Thanks

On Sat, Jun 8, 2013 at 1:35 PM, Cory Benfield notifications@github.comwrote:

> @lovesh https://github.com/lovesh Does using a Session resolve your
> problem?
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/issues/1389#issuecomment-19145022
> .

## 

Thanks for your time
",lovesh,lovesh
1388,2013-05-25 08:03:59,"@Arfrever Unfortunately we rushed the last two releases out due to a pair of nasty bugs, one affecting everyone on 3.3.2 and one affecting anyone using OAuth (both bad). Presumably @kennethreitz was midway through working on something when he published the last two releases, and my badgering him to release fast caused him to lose track of it.

However, the library itself functions fine, only the tests fail to run. Is it sufficient for you to simply change the tests locally?
",Lukasa,Arfrever
1384,2013-05-24 07:51:49,"Hi @tasuk, thanks for providing that pull request and for opening this issue!

You are right, there are a few good ways to manage dependencies. What Requests does is to 'vendor' its dependencies: that is, to provide them as part of the library itself. The main reason we do this is because it has the effect of pinning us to a single version of our dependencies, without limiting what other users or libraries install.

Let me provide an example. If we moved urllib3 to `requirements.txt`, we would need to pin a specific version (to avoid being broken when/if urllib3 does a major version update that changes the API). So we'd add something like `urllib==1.5`. This all works great until you install another library that also uses urllib3, and also pins a version, e.g. `urllib==1.6`. At this stage, one of your libraries is going to be using a version of urllib3 that it wasn't expecting, and runs the risk of breaking. 

You see that Requests breaks and, being the good user that you are, you open an issue saying 'Requests broke over upgrade'. We run around like headless chickens for a while, and then say 'uh, none of us can reproduce this'. Eventually, after some amount of wasted time we discover that you're using a version of urllib3 that we don't support. Then we say ""there's nothing we can do about this, you'll have to change all the source code of Requests yourself or wait until we update the version of urllib3 we test against"".

As you can see, this is _crap_. So we can't be doing that.

As for git submodules, yes, we could in principle use that. Unfortunately, the general consensus is that git submodules are [a bit crap](http://codingkilledthecat.wordpress.com/2012/04/28/why-your-company-shouldnt-use-git-submodules/). Even if they were good, we'd gain very little from that: once again, we're pinned to a specific version of urllib3, so the ability to bring in its updates is not helpful. It's easier just to grab tagged versions of urllib3 and call it a day. =)

Does that answer your question?
",Lukasa,tasuk
1384,2013-05-24 11:41:39,"Git submodules aren't even just a bad idea for the reason you posted @Lukasa, they include the entire repository not just the library we're looking for. In other words, let's say we vendored urllib3 as a submodule, the structure would be as follows



So then we wouldn't be able to do: `from .packages import urllib3` or `from .packages import charade`. We couldn't even do `from .packages.urllib3 import urllib3` because the repository itself is not a package.
",sigmavirus24,Lukasa
1384,2013-05-24 11:56:25,"@kennethreitz: Of course not. =) I was explaining why we do it, rather than suggesting that we'd ever change. Given @tasuk is relatively new to this, though, it seemed like explaining why lots of libraries do this.
",Lukasa,kennethreitz
1384,2013-05-24 11:56:25,"@kennethreitz: Of course not. =) I was explaining why we do it, rather than suggesting that we'd ever change. Given @tasuk is relatively new to this, though, it seemed like explaining why lots of libraries do this.
",Lukasa,tasuk
1384,2013-05-24 11:57:28,"@Lukasa it could be fixed by structuring the project like logilab structures theirs (example: [astng](https://bitbucket.org/logilab/astng/src)) but I really really hate that structure.

And @kennethreitz, we're just trying to teach our new friend. (At least I hope he'll be our friend.)
",sigmavirus24,kennethreitz
1384,2013-05-24 11:57:28,"@Lukasa it could be fixed by structuring the project like logilab structures theirs (example: [astng](https://bitbucket.org/logilab/astng/src)) but I really really hate that structure.

And @kennethreitz, we're just trying to teach our new friend. (At least I hope he'll be our friend.)
",sigmavirus24,Lukasa
1384,2013-05-24 13:13:21,"@Lukasa, thank you for a very comprehensive answer!

So, basically, dependency management is also broken in Python :)

Coming from an inferior language where we're facing similar problems, I was hoping someone solved dependencies in Python. Maybe it's one of those problems that can never really be solved. I guess you'd have to be able to have several versions of one package installed in parallel and choose which one you're importing. Polluting imports with version numbers doesn't seem like the right thing to do though.

As for submodules, I know they're problematic (been there seen that), but I don't see why the package root shouldn't be the project root (though Python people don't seem to do that). That looks like lesser evil than including whole 3rd party libs in your repo. I'm a bit surprised that's standard practice in Python-world.

It's an interesting situation, each of the solutions sucks in its own specific way. In PHP world, we use Composer (our version of ""pip -r""). Sometimes we get impossible-to-resolve conflicts and sometimes dependency updates break stuff (though there should be no api changes for minor releases), but we learned to live with it.

One more question &ndash; what is the reason to use requirements.txt for `pytest` and `invoke`, but not for `urllib3`?

@sigmavirus24 yes I'll be your github-friend, @kennethreitz thanks for the cookie!
",tasuk,kennethreitz
1384,2013-05-24 13:13:21,"@Lukasa, thank you for a very comprehensive answer!

So, basically, dependency management is also broken in Python :)

Coming from an inferior language where we're facing similar problems, I was hoping someone solved dependencies in Python. Maybe it's one of those problems that can never really be solved. I guess you'd have to be able to have several versions of one package installed in parallel and choose which one you're importing. Polluting imports with version numbers doesn't seem like the right thing to do though.

As for submodules, I know they're problematic (been there seen that), but I don't see why the package root shouldn't be the project root (though Python people don't seem to do that). That looks like lesser evil than including whole 3rd party libs in your repo. I'm a bit surprised that's standard practice in Python-world.

It's an interesting situation, each of the solutions sucks in its own specific way. In PHP world, we use Composer (our version of ""pip -r""). Sometimes we get impossible-to-resolve conflicts and sometimes dependency updates break stuff (though there should be no api changes for minor releases), but we learned to live with it.

One more question &ndash; what is the reason to use requirements.txt for `pytest` and `invoke`, but not for `urllib3`?

@sigmavirus24 yes I'll be your github-friend, @kennethreitz thanks for the cookie!
",tasuk,Lukasa
1384,2013-05-24 13:13:21,"@Lukasa, thank you for a very comprehensive answer!

So, basically, dependency management is also broken in Python :)

Coming from an inferior language where we're facing similar problems, I was hoping someone solved dependencies in Python. Maybe it's one of those problems that can never really be solved. I guess you'd have to be able to have several versions of one package installed in parallel and choose which one you're importing. Polluting imports with version numbers doesn't seem like the right thing to do though.

As for submodules, I know they're problematic (been there seen that), but I don't see why the package root shouldn't be the project root (though Python people don't seem to do that). That looks like lesser evil than including whole 3rd party libs in your repo. I'm a bit surprised that's standard practice in Python-world.

It's an interesting situation, each of the solutions sucks in its own specific way. In PHP world, we use Composer (our version of ""pip -r""). Sometimes we get impossible-to-resolve conflicts and sometimes dependency updates break stuff (though there should be no api changes for minor releases), but we learned to live with it.

One more question &ndash; what is the reason to use requirements.txt for `pytest` and `invoke`, but not for `urllib3`?

@sigmavirus24 yes I'll be your github-friend, @kennethreitz thanks for the cookie!
",tasuk,sigmavirus24
1384,2013-05-24 13:19:06,"Actually @tasuk it isn't necessarily common practice to vendor dependencies. @kennethreitz uses it here because of how incredibly popular requests is and what a headache it would be to have different versions of dependencies floating around.

On the other hand, most of my libraries (the ones that have dependencies) use pip to install the dependencies since (for the most part) the libraries I use I actively help develop, so I can anticipate changes that my cause issues.

As for requirements.txt, we use that for [Travis CI](travis-ci.org) to install dependencies for testing requests. Those aren't installation dependencies at all. Since that's only used for testing, and we only tests requests, we have no need to use our vendored libraries' requirements.txt files simply because we aren't running their test suites.

Some people us requirements.txt to also determine what needs to be installed for their package in general, but that isn't it's only use, just like not all projects vendor dependencies. :)
",sigmavirus24,kennethreitz
1384,2013-05-24 13:19:06,"Actually @tasuk it isn't necessarily common practice to vendor dependencies. @kennethreitz uses it here because of how incredibly popular requests is and what a headache it would be to have different versions of dependencies floating around.

On the other hand, most of my libraries (the ones that have dependencies) use pip to install the dependencies since (for the most part) the libraries I use I actively help develop, so I can anticipate changes that my cause issues.

As for requirements.txt, we use that for [Travis CI](travis-ci.org) to install dependencies for testing requests. Those aren't installation dependencies at all. Since that's only used for testing, and we only tests requests, we have no need to use our vendored libraries' requirements.txt files simply because we aren't running their test suites.

Some people us requirements.txt to also determine what needs to be installed for their package in general, but that isn't it's only use, just like not all projects vendor dependencies. :)
",sigmavirus24,tasuk
1384,2013-05-24 13:24:51,"@sigmavirus24 ah cool, that explains everything!
",tasuk,sigmavirus24
1384,2013-05-24 13:28:15,"@sigmavirus24 has covered most of the points I was going to. =)

Like him, my projects overwhelmingly use Pip to manage dependencies, for the exact same reason. I have written a few requests plugins, and because I am a requests contributor I can predict problems that might come up. Though even then, I pin to a minimum Requests version, so I'm not totally immune. =P

You raised making the package root be the project root. There is a very good reason we don't do that in Python, and that reason is because in Python, folders define your namespaces. If you look at the Requests project root, you'll see it looks like this:



If we put an `__init__.py` here, turning this into the package root, the following lines of code would become possible:



We don't want people to be able to import any of these things: they aren't part of the library itself. But we need to be able to keep them under version control as well, because they are still very important. For this reason, we use a subfolder as the root of our package, which allows us to easily manage the other package metadata.

(A note: the fact that we have our testing dependencies in `requirements.txt` annoys me: I don't do it in my own projects. However, it's really minor so I just let it go. =) )
",Lukasa,sigmavirus24
1384,2013-05-24 19:41:32,"@Lukasa, that all makes perfect sense.

Even in PHP-land it's usual to separate things in a similar way (see e.g. [guzzle](https://github.com/guzzle/guzzle)). But we have funky class-loading mechanisms instead of adhering to the directory structure for namespacing, allowing us to be a bit more lenient about which files go where.

To reiterate my previous point - wouldn't it be wonderful if pip was able to manage packages locally in a recursive way? (""I need tag 3.1 of package 'x' and commit 45fe8a2 of package 'y', can you put them in requests/packages for me?"")
",tasuk,Lukasa
1384,2013-05-28 20:22:28,"@Lukasa 

> A note: the fact that we have our testing dependencies in `requirements.txt` annoys me (...)

I'm glad it annoys you as it annoys me, too :) If these are testing only dependencies I think the file should be named `requirements-testing.txt` or something similar to make it clear what kind of dependencies it enumerates.
",piotr-dobrogost,Lukasa
1382,2014-03-15 21:17:17,"@hynekcer we do not upgrade urllib3 until we are about to cut a release.
",sigmavirus24,hynekcer
1380,2013-05-22 21:09:50,"Thank you @sigmavirus24. I'm aware of this workaround, I brought this up only because it broke existing code of mine (so possibly others' who haven't updated to 1.2.x).

I'm :+1: for a `__serialize__`. Magic methods are sometimes nice.
",woozyking,sigmavirus24
1380,2013-05-22 21:18:13,"I'm with @sigmavirus24 here. This is a pain, but the Case-Insensitive Dict is awesome.

As for magic methods, I think JSON will never add one. JSON should be round-trippable, e.g: `json.loads(json.dumps(obj)) == obj`. Allowing arbitrary serialisation breaks that functionality.

@woozyking, for what it's worth, if you find yourself doing this a lot, you can use a [custom JSON encoder with the `default` method defined`](http://docs.python.org/2/library/json.html#json.JSONEncoder.default). Define the `default` method such that it turns the Case-Insensitive Dict into a dict. Slightly less code repetition if you want it.

Sorry that we caused you this inconvenience, but I think things are going to stay as they are. Thanks so much for raising the issue though! :cake:
",Lukasa,sigmavirus24
1380,2013-05-22 21:18:13,"I'm with @sigmavirus24 here. This is a pain, but the Case-Insensitive Dict is awesome.

As for magic methods, I think JSON will never add one. JSON should be round-trippable, e.g: `json.loads(json.dumps(obj)) == obj`. Allowing arbitrary serialisation breaks that functionality.

@woozyking, for what it's worth, if you find yourself doing this a lot, you can use a [custom JSON encoder with the `default` method defined`](http://docs.python.org/2/library/json.html#json.JSONEncoder.default). Define the `default` method such that it turns the Case-Insensitive Dict into a dict. Slightly less code repetition if you want it.

Sorry that we caused you this inconvenience, but I think things are going to stay as they are. Thanks so much for raising the issue though! :cake:
",Lukasa,woozyking
1380,2013-05-22 21:20:50,"@sigmavirus24 a quick test with the latest simplejson indicates that they don't have built-in resolution for this case.

@Lukasa fully understand.
",woozyking,Lukasa
1380,2013-05-22 21:20:50,"@sigmavirus24 a quick test with the latest simplejson indicates that they don't have built-in resolution for this case.

@Lukasa fully understand.
",woozyking,sigmavirus24
1380,2013-05-22 21:58:56,"@Lukasa that's true. But it would still be convenient. :-P
",sigmavirus24,Lukasa
1374,2013-05-21 19:21:47,"@Lukasa OK, I will.  Sorry for misleading you by my comment.
",papaeye,Lukasa
1374,2013-05-21 19:25:11,"@papaeye: Not at all! I mislead myself by being stupid. =D
",Lukasa,papaeye
1372,2013-05-21 21:25:30,"@kennethreitz are you using a clean virtual environment to test it? Using both old and new versions of virtualenv I found the same results: https://gist.github.com/sigmavirus24/5623374
",sigmavirus24,kennethreitz
1369,2013-05-20 19:21:58,"@jellyflower Thanks for reporting this bug. It's actually one we already knew about: see requests/requests-oauthlib#43.

I wonder if 3.3.2 has introduced a bug in Requests. @sigmavirus24, I'm leaving this for you. =)
",Lukasa,jellyflower
1367,2013-05-19 17:45:47,"If you want to take a crack at a patch @git2samus, take a look at the Session class. That's picklable and has all the info there. If you would rather one of us do it, I can start to tackle it in half an hour.
",sigmavirus24,git2samus
1367,2013-11-03 05:30:16,"This would be a big help for caching via [Cache Control](https://github.com/ionrock/cachecontrol). At the moment, it is very difficult to cache a response outside of memory because it is difficult to rebuild the object hierarchy.

I did look into caching the raw response and using it to rebuild the response from scratch when a cache is hit, but that would have taken some pretty serious monkey patching of httplib and urllib3.

The patches by @tanelikaivola seem like they would world without too much trouble.
",ionrock,tanelikaivola
1367,2013-11-05 04:00:02,"@ionrock why not do something like [Betamax](https://github.com/sigmavirus24/betamax) instead of trying to use pickle?
",sigmavirus24,ionrock
1367,2013-11-05 22:38:57,"@sigmavirus24 Thanks for pointing out [Betamax](https://github.com/sigmavirus24/betamax)! I'd argue the serialize/deserialize_response functions would be a great addition to the Response object. If the goal is to avoid pickle, this seems like a great option. Sometimes pickle is a good option though, so I still believe it is worthwhile to add the functionality. I've tested the patch @tanelikaivola and they work well. What else would need to be done to potentially get them merged? Obviously some tests would be helpful. I'd also be happy to see about adding the serialize/deserialize code from Betamax if that would be alright with @sigmavirus24. 

Let me know if there is a good way to proceed. I'd be happy to put the code together. 

@sigmavirus24 thanks again for the Betamax suggestion. I will be switching [Cache Control](https://github.com/ionrock/cachecontrol) to use that methodology.
",ionrock,tanelikaivola
1367,2013-11-05 22:38:57,"@sigmavirus24 Thanks for pointing out [Betamax](https://github.com/sigmavirus24/betamax)! I'd argue the serialize/deserialize_response functions would be a great addition to the Response object. If the goal is to avoid pickle, this seems like a great option. Sometimes pickle is a good option though, so I still believe it is worthwhile to add the functionality. I've tested the patch @tanelikaivola and they work well. What else would need to be done to potentially get them merged? Obviously some tests would be helpful. I'd also be happy to see about adding the serialize/deserialize code from Betamax if that would be alright with @sigmavirus24. 

Let me know if there is a good way to proceed. I'd be happy to put the code together. 

@sigmavirus24 thanks again for the Betamax suggestion. I will be switching [Cache Control](https://github.com/ionrock/cachecontrol) to use that methodology.
",ionrock,sigmavirus24
1367,2013-11-06 03:28:06,"@sigmavirus24 Fair enough. I'm most definitely not the expert on everything that a Response object, so I'm happy to take your word for it. When I looked at what a response object contained it was semi-complex as it was a wrapper around a urllib3 HTTPResponse, which is in turn a wrapper around a httplib response, which assumes the content comes directly from a socket (not a file like object). 

With that being the case, it seems to make sense to support pickling of Response object. Again, I'm happy to write some tests for @tanelikaivola's patches if there is a consensus. Otherwise, I'd like to understand where it falls short. At the very least I'd like to try and fix it for myself. 

Thanks for the discussion!
",ionrock,tanelikaivola
1367,2013-11-06 03:28:06,"@sigmavirus24 Fair enough. I'm most definitely not the expert on everything that a Response object, so I'm happy to take your word for it. When I looked at what a response object contained it was semi-complex as it was a wrapper around a urllib3 HTTPResponse, which is in turn a wrapper around a httplib response, which assumes the content comes directly from a socket (not a file like object). 

With that being the case, it seems to make sense to support pickling of Response object. Again, I'm happy to write some tests for @tanelikaivola's patches if there is a consensus. Otherwise, I'd like to understand where it falls short. At the very least I'd like to try and fix it for myself. 

Thanks for the discussion!
",ionrock,sigmavirus24
1367,2013-11-08 12:41:42,"@ionrock :shipit: 
",sigmavirus24,ionrock
1367,2013-11-08 23:19:38,"@sigmavirus24 I have no clue what a detective squirrel means (that is a detective squirrel right?), but I'll assume there will be comments later ;)

The tests are pretty thin, so I'm happy to add more tests. That patch is just my baseline.
",ionrock,sigmavirus24
1364,2013-06-07 05:07:30,"@sigmavirus24 - this has now been added in shazow/urllib3#187.  Is there a regular ""timetable"" for when you pull in a new `urllib3`?  Or is it just whenever they have a release?
",eteq,sigmavirus24
1364,2013-06-07 10:42:02,"@eteq: It's mostly 'whenever'. I'll bear in mind that this is a high priority for you though, and try to bug Kenneth to include an update in our next release. =)
",Lukasa,eteq
1363,2013-05-16 17:22:29,"@sigmavirus24 does 6e76ab7 do the job?  I'm relatively new to git so it wouldn't surprise me if I mucked something up.
",dave-shawley,sigmavirus24
1363,2013-05-16 17:39:33,"@dave-shawley it most certainly does! Thank you kind sir!

:+1: for @kennethreitz when he comes around these parts again
",sigmavirus24,dave-shawley
1363,2013-05-17 15:50:25,"Congrats @dave-shawley :cake:
",sigmavirus24,dave-shawley
1359,2013-07-19 08:36:32,"@astratto Can you try: https://github.com/shazow/urllib3/pull/170#issuecomment-21234629
",schlamar,astratto
1359,2013-07-19 09:20:08,"@schlamar I've just tried and it works! Great!


",astratto,schlamar
1357,2013-05-12 00:58:38,"I'm personally in favor of #1327 because it doesn't require re-sorting and reversing the list of adapter prefixes every-time we make a request (we use `get_adapter` every time we call `request` which is every time we make a request). Sorry @Zoramite 
",sigmavirus24,Zoramite
1357,2013-05-12 01:00:30,"@sigmavirus24 no worries :) That is a better approach, I should have done a search for it before just blindly fixing it :)
",Zoramite,sigmavirus24
1357,2013-05-12 01:04:15,"Thanks for the request anyway @Zoramite ! Your other one looks good though. :cake:
",sigmavirus24,Zoramite
1356,2013-05-12 08:29:05,"I think this PR is in great shape, but for reasons that are nothing to do with the code I'm slightly more lukewarm to it than @sigmavirus24 is. I consider subclassing the Transport Adapter to be a legitimate part of Requests' API: the documentation talks about it, some interfaces require it, etc. etc.

With that said, I'm +0.5 on this. I think it'd be a good addition to the library and it doesn't look like it'll break anything, but I don't think there is anything wrong with subclassing the adapter either. On balance I'd rather take it than leave it. That said, I don't want adding arguments to the constructor of the adapter to become a trend. =P
",Lukasa,sigmavirus24
1356,2013-05-12 17:01:24,"@Lukasa but sub-classing the adapter just to rewrite one (or two methods) to include an extra parameter seems a bit like over-kill to me. There are times when subclassing is appropriate and times when it isn't.

We may be better off doing something else though:

What if we rewrite `HTTPAdapter` to be more intelligent. We'll modify the `__init__` method to take `**kwargs` that are meant only for the `poolmanager` initialization, so something like:



Because really we have never used `self.config` on the `HTTPAdapter`, nor have we ever really needed to set `_pool_connections` or `_pool_maxsize` twice (which we do both in `__init__` and `__setstate__`). `init_poolmanager` really doesn't need parameters to work well.
",sigmavirus24,Lukasa
1353,2013-05-07 19:07:44,"@Lukasa why you no protect the critical variable?
",sigmavirus24,Lukasa
1353,2013-05-07 23:39:49,"@Lukasa You are still reading in chunks and you have no control while that is going on, and timing of your socket close request depends on state of communication, not good. Imagine a long polling, push message use case, or connection about to timeout in 10 seconds, still you cannot drop it.

For sockets, simply calling close from another thread is a common practice, method should be thread safe of course.
",mua,Lukasa
1349,2013-05-05 18:07:37,"@jcarbaugh :cake:
",kennethreitz,jcarbaugh
1347,2013-05-04 08:17:06,"@sigmavirus24: If it is that's awesome.
",Lukasa,sigmavirus24
1345,2013-05-02 22:47:17,"95% sure this will be rejected. We're currently in a feature freeze (#1165) so no changes to the API are allowed to be accepted unless they were previously planned. Sorry @maurycyp 
",sigmavirus24,maurycyp
1338,2013-05-01 18:44:56,"@Lukasa rebase ;)
",sigmavirus24,Lukasa
1338,2013-05-01 19:11:23,"@Lukasa the tests fail on python3 where you have `'accept'.encode('ascii')` because of the new CaseInsensitiveDict (#1339)
",sigmavirus24,Lukasa
1338,2013-05-01 20:51:23,"@cdunklau That was a great idea, and is now done.
",Lukasa,cdunklau
1338,2013-05-13 09:03:24,"@oliverjanik: The reason this wasn't merged straight away is that it is a significant, backward-incompatible change. It is generally unwise to merge such a change immediately. It's important to add it in a release which gets a minor version bump (e.g. 1.2.x to 1.3.x), and with good documentation (which reminds me, I should document this change somewhere).

This, or something similar, will get merged in good time. =)
",Lukasa,oliverjanik
1338,2013-06-09 07:26:10,"@oliverjanik If only it were that simple. =) This will affect the behaviour of the headers dictionary. People will have spotted that Requests sets its headers to bytes on Python3 and have written code that takes that into account. That code will break.

We simply can't rush this into a Requests release. With that said, this (or something like it) will definitely end up in Requests eventually. =)
",Lukasa,oliverjanik
1338,2013-06-21 17:20:55,"@Lukasa It's a shame this PR is a breaking API change in Python 3. I've really been looking forward to the next major release of requests when you can finally include it. A big thank you for the excellent work!
",paparomeo,Lukasa
1336,2013-05-01 16:45:02,"New option after a [conversation](https://botbot.me/freenode/python-requests/msg/2960213/) with @gazpachoking (which I would link to if botbot.me was working; _edit_ added the link since it's now up and running):

Since extracting the cookies before the hook could result in cookies being stored that hook authors might not want stored, we can safely move the extraction to after all redirects have been handled because then the history will be available. We also know that the history and the responses in the history will be correct because if there were redirects, the hooks are called on those responses too. So we can do something akin to:



This way the cookies are set and expired in the right order and only the cookies the hook author wants us to see are extracted. We're in the clear with this because the Auth Handlers properly also manage history (at least the built-in ones do and requests-kerberos does too. I'll check ntlm later).
",sigmavirus24,gazpachoking
1336,2013-05-02 07:22:58,"@sigmavirus24: If I understand this right, we'd also have to add code to `DigestAuth` to propagate cookies for the redirects it handles. Correct?
",Lukasa,sigmavirus24
1336,2013-05-02 13:56:06,"@Lukasa the cookie extraction would take place on the history. DigestAuth won't require any changes at all. It already maintains history and we'll be parsing that history. 
",sigmavirus24,Lukasa
1336,2013-05-04 20:54:38,"@sigmavirus24 I think so, yeah.
",Lukasa,sigmavirus24
1336,2013-05-28 17:04:09,"@Lukasa's edit is exactly what's going on. I'm currently in the middle of a move and don't have time to work on this at the moment. Anyone who wants to tackle this using @gazpachoking's and my instructions above can and I'll be happy to code-review it if you want me to do so before sending a pull request.
",sigmavirus24,gazpachoking
1336,2013-05-28 17:04:09,"@Lukasa's edit is exactly what's going on. I'm currently in the middle of a move and don't have time to work on this at the moment. Anyone who wants to tackle this using @gazpachoking's and my instructions above can and I'll be happy to code-review it if you want me to do so before sending a pull request.
",sigmavirus24,Lukasa
1336,2013-06-02 18:30:26,"Could anyone review this change ? Perhaps @sigmavirus24 ?
",elricL,sigmavirus24
1336,2013-06-06 08:36:55,"Sorry @ericL, both @sigmavirus24 and I have just moved house and don't have domestic internet connections. In addition, I also get lousy mobile data reception in my new house. I can only get EDGE, which means I can't really tether to my mobile phone either.

I think we'll just have to sit on this for a little while until @sigmavirus24 and I have better access and more time.
",Lukasa,sigmavirus24
1335,2013-06-08 10:13:12,"@Lukasa thank you! Btw, you accidentally pushed a merge conflict :(
",va1en0k,Lukasa
1334,2013-05-30 21:07:55,"@kennethreitz Can you restart https://travis-ci.org/kennethreitz/requests/builds/7518571 ? It appears I can't do it without making another commit.
",rcarz,kennethreitz
1334,2013-06-06 08:48:10,"@rcarz That's odd, vanilla requests throws exceptions when I do a get on `HTTP://www.google.com/`. TravisCI does too.

I'm not sure you're right, @Anorov. As @rcarz points out, this change here only affects redirects. You can see this by looking at the Travis CI output for this change, where a test with upper-case scheme fails.

#1385 is more comprehensive than this fix. I need to sit down and confirm whether #1385 covers all cases this fix does.
",Lukasa,rcarz
1334,2013-06-06 16:55:44,"@Lukasa Ah yeah, you're right, sorry. I only looked at the changes and not the full file.

@rcarz Web servers and DNS servers should always be treating hostnames case insensitively, yes. So it doesn't really matter if `requests` lowercases it or not. I think it would just go along well, aesthetically, if the scheme is also lowercased.

Either way, I think this should all be resolved with one pull (whether it be this one or the other, I'm not sure). I think all schemes should be strictly lowercased early on, before processing any further.

If you place the changes made in this commit, essentially `url = '%s://%s' % (scheme.lower(), uri)`, somewhere early on in `Session.send`, I think every bird would be killed with one stone (so to speak). Anyone disagree?
",Anorov,rcarz
1334,2013-06-06 16:55:44,"@Lukasa Ah yeah, you're right, sorry. I only looked at the changes and not the full file.

@rcarz Web servers and DNS servers should always be treating hostnames case insensitively, yes. So it doesn't really matter if `requests` lowercases it or not. I think it would just go along well, aesthetically, if the scheme is also lowercased.

Either way, I think this should all be resolved with one pull (whether it be this one or the other, I'm not sure). I think all schemes should be strictly lowercased early on, before processing any further.

If you place the changes made in this commit, essentially `url = '%s://%s' % (scheme.lower(), uri)`, somewhere early on in `Session.send`, I think every bird would be killed with one stone (so to speak). Anyone disagree?
",Anorov,Lukasa
1333,2013-04-29 19:47:51,"@sigmavirus24 So it doesn't break pickles. The loaded session pickle just uses the original's dict:


",cdunklau,sigmavirus24
1333,2013-04-29 21:56:55,"@piotr-dobrogost There was some back and forth on IRC about it, and I was swayed. The undefined behavior is preferred because:
1. Simplicity. (KISS)
2. More dict-like in some circumstances.

But I can't remember the finer details. @gazpachoking, @Lukasa can you chime in?
",cdunklau,piotr-dobrogost
1333,2013-04-29 22:17:22,"@piotr-dobrogost Regular dicts have undefined behavior when you define two keys that are the same, since they are unordered `{'a': 3, 'a': 4}`, this would be the same as `CaseInsensitiveDict{{'A': 3, 'a': 4})`
",gazpachoking,piotr-dobrogost
1333,2013-04-30 08:28:35,"Yeah, I was under the impression that doing what @gazpachoking described would cause a `ValueError`, but (totally insanely IMHO) it doesn't. So we should just match that behaviour.
",Lukasa,gazpachoking
1328,2013-04-26 15:55:05,"Thanks @gazpachoking you rock! :cake:
",sigmavirus24,gazpachoking
1327,2013-04-25 12:58:53,"After testing it, it is probably not going to be as painless as this.

:+1: Nice work @ambv 
",sigmavirus24,ambv
1327,2013-04-25 14:15:38,"@kennethreitz the idea behind this is that you want to ensure the longest match is the first you'll find. So if you have an adapter for `http://example.com/endpoint1` and `http://example.com` and you issue a reqeust for `http://example.com/endpoint1/resource` you're going to get the first one and find it faster. Of course for the simplest case your performance is sort of awful, but most of those cases won't see people using adapters and as Lukasa mentioned, I doubt anyone is going to have too many adapters mounted in the first place. :)
",sigmavirus24,kennethreitz
1327,2013-04-25 14:31:40,"@sigmavirus24's explanation was always my understanding of how you intended transport adapters to behave. If mount-order becomes the condition, then your adapters become surprising. For instance, changing the `Session` constructor from 



to:



causes all your HTTPS traffic hits the wrong adapter. There's no functional effect because all that logic is handled underneath, but it sure is strange.
",Lukasa,sigmavirus24
1327,2013-04-25 17:35:09,"So if we do order added then we'd have to do something else when we find the adapter @kennethreitz that would be akin to what @Lukasa's last pull request did. So the benefit of ordering would make no difference.

This just ensures that any adapters you have with freakishly long prefixes are matched first and returned.
",sigmavirus24,kennethreitz
1327,2013-04-25 17:35:09,"So if we do order added then we'd have to do something else when we find the adapter @kennethreitz that would be akin to what @Lukasa's last pull request did. So the benefit of ordering would make no difference.

This just ensures that any adapters you have with freakishly long prefixes are matched first and returned.
",sigmavirus24,Lukasa
1327,2013-05-01 18:19:11,"Rebased the PR.

The status here, as described by @sigmavirus24 in [comment 6](https://github.com/kennethreitz/requests/pull/1327#issuecomment-17009475) and confirmed by @Lukasa in [comment 7](https://github.com/kennethreitz/requests/pull/1327#issuecomment-17010424), is that the existing prefix-based `mount` syntax



always suggested that the longest match will be used, for example `get('http://github.com/about/us/')` will always match rule 3 (never the default http:// nor rules 1 or 2), `get('http://gittip.com')` will always match rule 1 (never the default http://), and so on.

The `mount()` invocation order does not matter, only the length of the matching prefix.
",ambv,Lukasa
1327,2013-05-01 18:19:11,"Rebased the PR.

The status here, as described by @sigmavirus24 in [comment 6](https://github.com/kennethreitz/requests/pull/1327#issuecomment-17009475) and confirmed by @Lukasa in [comment 7](https://github.com/kennethreitz/requests/pull/1327#issuecomment-17010424), is that the existing prefix-based `mount` syntax



always suggested that the longest match will be used, for example `get('http://github.com/about/us/')` will always match rule 3 (never the default http:// nor rules 1 or 2), `get('http://gittip.com')` will always match rule 1 (never the default http://), and so on.

The `mount()` invocation order does not matter, only the length of the matching prefix.
",ambv,sigmavirus24
1325,2013-10-11 15:47:32,"Thanks @oohlaf!
",sigmavirus24,oohlaf
1325,2014-08-07 21:05:29,"@jamshid I hope next time you'll investigate the [actual behaviour](https://github.com/kennethreitz/requests/blob/master/requests/sessions.py#L134..L149) of requests. For the status codes that we [may](http://tools.ietf.org/html/rfc7231#section-6.4.2) change the method, we do because this is the unfortunate de facto behaviour that @lukasa referred to. We do **not** change them on 307 or 308 responses, ergo we already follow the new set of RFCs for HTTP/1.1.
",sigmavirus24,jamshid
1324,2013-04-26 10:57:12,"@gazpachoking seems to have fixed this.
",sigmavirus24,gazpachoking
1321,2013-04-24 13:27:08,"@sigmavirus24 Thank you. I will fix it ASAP.
",shaung,sigmavirus24
1321,2013-04-24 14:25:02,"@shaung when you fix it, ping me. GitHub won't tell me when you've added commits so don't be afraid to mention me in the commit message or just add a comment here. :)
",sigmavirus24,shaung
1321,2013-04-24 15:25:28,"@sigmavirus24 Fixed :-)
",shaung,sigmavirus24
1321,2013-05-01 19:22:18,"@sigmavirus24 Just tested against the new master after my PR was merged, this issue is not fixed.
",cdunklau,sigmavirus24
1321,2013-05-02 05:28:00,"@kennethreitz Thanks! I'm going to do it tonight.
",shaung,kennethreitz
1321,2013-05-02 15:57:19,"@kennethreitz  Ready for merging (=´ー｀)ノ
",shaung,kennethreitz
1320,2013-04-24 15:32:33,"Actually, @ambv is right. Here's the source code for `get_adapter()`:



This is awkward, because I've provided Transport Adapter information in the past that directly contradicts this behaviour. I think we need to fix this, because the docs behaviour should be correct. I'm happy to take a swing at this.
",Lukasa,ambv
1320,2013-04-24 15:52:20,"I am sincerely sorry @ambv. That'll teach me to work from memory ever again. 

Here are my thoughts about this with the code above:
- We could collect a list of matching adapters instead of returning the first one we find. The problem is then deciding which adapter to use
- We could maintain two separate adapters registries: 1) user-created 2) default. The user-created adapters would be the first to be searched through and if there's a match in them we could then return that. If none of those match we would then search the default adapters and if nothing matches from there raise the `InvalidSchema` error. To preserve the API we could make `adapters` a property. The `@adapters.setter` method would then only set adapters on the user-created dictionary. The returned information would then be best represented as a list of two-tuples where the user-created items come first and is then followed by the default. This gives an intuitive idea of the overall ordering of discovery of adapters. This, however, would break the case where someone tries to do `session.adapters['http://']`
- We could create our own `AdaptersRegistry` object which behaves like a dictionary, i.e., has the `__setitem__`, `__getitem__`, `get`, `set`, &c., methods, and does the search for us. Then we just maintain that as the `adapters` attribute.

I could be vastly over-thinking the problem though.
",sigmavirus24,ambv
1319,2013-04-25 20:21:28,"Yup, upgrading to 1.2 fixed it. (Sorry for the noob issue.) Thanks, @Lukasa. 
",jmakeig,Lukasa
1317,2013-04-22 17:33:26,"Thanks @dmckeone 
",sigmavirus24,dmckeone
1317,2013-04-23 14:09:29,"@sigmavirus24 dependency walker gives me two missing dlls, but irrevelant ones.
I agree with you, and came to the conclusion that py2exe is having some problems packing requests dependencies.
Running py2exe indicates some missing modules, corresponding to requests modules (urllib3, etc.)
Using modulefinder I've found where the import of those missing modules are. The issue seems to be that requests is using try statements to test python version (2 or 3) and use the corresponding import.
Now, I'm using python 2.7 and this should be working. But it's not. And I cannot figure out why or how to fix it without messing woth requests source code...

@dmckeone : Wich version of Python are you using?
",fedeanna,sigmavirus24
1317,2013-04-23 14:09:29,"@sigmavirus24 dependency walker gives me two missing dlls, but irrevelant ones.
I agree with you, and came to the conclusion that py2exe is having some problems packing requests dependencies.
Running py2exe indicates some missing modules, corresponding to requests modules (urllib3, etc.)
Using modulefinder I've found where the import of those missing modules are. The issue seems to be that requests is using try statements to test python version (2 or 3) and use the corresponding import.
Now, I'm using python 2.7 and this should be working. But it's not. And I cannot figure out why or how to fix it without messing woth requests source code...

@dmckeone : Wich version of Python are you using?
",fedeanna,dmckeone
1317,2013-04-23 14:49:55,"@fedeanna I had it working on Python 2.7.3, and now on Python 2.7.4 both 32-bit and 64-bit.

Since I know how much of a hassle this is, here is my `setup.py` script (It's a little ugly, sorry):



It freezes two .exe files on Windows (a service and a command line program), as well as an .app bundle on the Mac with py2app.  Hopefully that can help you par it down to what makes it work.  This app is a Flask app that uses babel, so if you aren't using babel you can probably get rid of some of that stuff.  

Note that I am missing a copy of the requests cacert.pem file, because I'm not using SSL requests.  If you did need verified SSL then you'd need to do something similar to what I did for babel, and use the `requests.get('http://127.0.0.1', cert=path_to_cacert_pem)` or monkey-patch `requests.utils.DEFAULT_CA_BUNDLE_PATH` to be your cacert path

I hope that helps, but if it doesn't then your next stop should likely be the py2exe forums, since I'm fairly certain it's not a Requests issue.
",dmckeone,fedeanna
1316,2013-05-01 19:03:04,"Hey @dmckeone could you rebase this against current master?
",sigmavirus24,dmckeone
1316,2013-05-01 19:38:05,"@sigmavirus24 No problem!  I've actually never done a rebase before.  I think it went ok, but let me know if I should do something differently.
",dmckeone,sigmavirus24
1316,2013-05-01 21:04:57,"Well, looks like the rebase didn't go properly.  `from_key_val_list()` should be returning an `OrderedMultiDict`.  I'm going to review all the changes again.

@sigmavirus24 Since this has gotten a little messy with my screwed up rebase, should I just create a separate pull request and close this one?
",dmckeone,sigmavirus24
1316,2013-05-01 21:30:12,"@gazpachoking Yes, definitely.  That was actually my first step.
",dmckeone,gazpachoking
1316,2013-05-01 22:55:30,"@cdunklau I also noticed that `CaseInsensitiveDict` wasn't caught at any place that checked `isinstance( ... , dict)`, so I altered all of those checks to use `collections.Mapping` instead.  As for the sequence check, that looks ok to me as well, happy to integrate it in this PR, but for now I haven't included it.
",dmckeone,cdunklau
1316,2013-05-01 22:56:33,"@dmckeone yeah I noticed the same thing somewhere in `.models`... but I haven't looked hard enough at it.
",cdunklau,dmckeone
1316,2013-05-01 23:46:52,"@cdunklau After some more thought about your changes, I'm not sure if it would handle all inputs in the same way as the previous code.  If a `value` was a generator, then `all()` would exhaust it, and the setting of `item` would not work correctly.  Perhaps that is why `list()` is used?  Your checks for `basestring` are good though, so I included those in a new commit.
",dmckeone,cdunklau
1316,2013-05-02 00:51:01,"@cdunklau Agreed.  Made the commit.
",dmckeone,cdunklau
1316,2013-05-02 01:11:47,"@cdunklau Updated.  I had initially kept the same Exception message on purpose, but since it all needs to be updated, I suppose it is better to update the language.
",dmckeone,cdunklau
1316,2013-05-02 05:11:50,"Discussed a bit more with @cdunklau on IRC, we are thinking that keys from arguments in a request should still continue to override the same keys (rather than add more of those keys) from the session param.

Still not sure if there should be some way to specifically add to the existing session keys for a given request, maybe if the request argument is explicitly a multidict.

Other opinions are certainly needed, as I'm not sure if I'm considering all use cases. I'm thinking though that the most common would be to want to override a session key rather than add a second same key to a dict.
",gazpachoking,cdunklau
1316,2013-05-02 07:27:45,"I agree with @gazpachoking and @cdunklau, overriding already set keys is the way to go. Sadly, that'll need special-case code.
",Lukasa,gazpachoking
1316,2013-05-02 07:27:45,"I agree with @gazpachoking and @cdunklau, overriding already set keys is the way to go. Sadly, that'll need special-case code.
",Lukasa,cdunklau
1315,2013-05-01 18:37:48,"@kennethreitz done. 

Let me know if something is wrong, I'm doing rebase first time.
",reclosedev,kennethreitz
1308,2013-04-12 14:13:20,"@Lukasa Thanks for the quick answer. I've looked at your post and I think it's a good start to solve my problem maybe (which is setting the ciphers, not the SSL version :p)

The VerifiedHTTPSConnection wraps the socket and that's where I need to provide ciphers (this is the only way apparently). I'm not sure i'll be able to create a pool and then use it in a adapter, all without having to copy/paste existing code (I'm looking for a clean way of doing it)

Many thanks again
",damiengermonville,Lukasa
1308,2013-04-12 15:45:31,"@damiengermonville you might then need to petition over at shazow/urllib3 for an easier way to pass in the ciphers to the VerifiedHTTPSConnection object. We have no clue what your code looks like, but I imagine this might make it easier. It's also something I don't believe @shazow would be that adverse to, but I'm pretty sick at the moment so it's probable that I'm wrong. :)
",sigmavirus24,damiengermonville
1308,2013-04-15 07:39:50,"@shazow I'm afraid that adjustments have to be made in VerifiedHTTPSConnection but also in urllib3/util.py ssl_wrap_socket() so it can give the ciphers argument to ssl.wrap_socket().
",damiengermonville,shazow
1308,2013-04-15 17:51:50,"@damiengermonville Sounds fine. :) 
",shazow,damiengermonville
1308,2017-02-23 12:43:12,"*Please* do not do it this way. Instead, follow the approach in [this blog post](https://lukasa.co.uk/2017/02/Configuring_TLS_With_Requests/) and change the ciphers only for specific sites. RC4 is *extremely* dangerous and broken, and enabling it in the way shown in @athoik's comment will enable it for all sites, opening you up to trivial attacks. If you must allow RC4 or other removed ciphers, do so on a site-by-site basis.",Lukasa,athoik
1308,2017-02-23 13:05:54,"@Lukasa, I had to enable only the AES256-SHA in my case. 



I just copy / pasted the original solution. ",athoik,Lukasa
1307,2013-04-11 21:30:01,"@sigmavirus24 That's not how character encodings work. Encoding as CP-1252 gives a series of bytes: decoding those bytes as UTF-8 will give you another 'unicode' string, but its characters will be meaningless. If you encode to CP-1252, you can only ever meaningfully decode as CP-1252.
",Lukasa,sigmavirus24
1305,2013-04-11 12:31:10,"+1 to this and @Lukasa 's suggestion
",sigmavirus24,Lukasa
1305,2013-04-11 15:19:06,"@Lukasa Good idea. I'm not too good at copy, I'd probably just confuse the hell out of somebody; so if anybody has suggestions as to what to update the preamble to, I'm open to suggestions :D
",michaelhelmick,Lukasa
1305,2013-04-11 22:39:04,"@kennethreitz Do _yoooou_ have suggestions on how to introduce `requests_oauthlib` into the docs :smirk: 
",michaelhelmick,kennethreitz
1305,2013-04-12 00:22:06,"@kennethreitz I've made changes to the Other Authentication section, let me know what you think!
",michaelhelmick,kennethreitz
1303,2013-04-10 17:26:08,"@Lukasa False https://github.com/kennethreitz/requests/blob/master/requests/sessions.py#L121
",kennethreitz,Lukasa
1303,2014-02-12 02:35:39,"@danc86 for what it's worth, @Lukasa took care of that for you. =)
",sigmavirus24,danc86
1303,2014-02-12 02:35:39,"@danc86 for what it's worth, @Lukasa took care of that for you. =)
",sigmavirus24,Lukasa
1300,2013-04-10 14:36:28,"@sigmavirus24 this issue is different than #1301. Here, I just expected the `max_redirects` work per request, because it is the obvious way. I don't want to create a `requests.Session` to be able to set `max_redirects`.
",iurisilvio,sigmavirus24
1300,2013-04-10 14:44:07,"To continue what @Lukasa explained, `max_retries` was an option pre 1.x but was explicitly removed with malice of forethought during the refactor. Really, when you make a specific request there are things that pertain to that specific request, e.g., certificate verification, whether that request should follow redirects, etc. but not how many times that request should be sent. A request is a single object. A browser may retry a request a maximum number of times, but a request is not a browser, a session is the closest thing to a browser you will ever find in requests (but it is not a browser).

That said, this is a feature request and combined with the fact this feature was already removed and that we're in a feature freeze this will remain closed.
",sigmavirus24,Lukasa
1297,2013-04-10 11:00:48,"@jvtm Thanks! Now I can update the sample code and so we can include it in the documentation.
",ssbarnea,jvtm
1297,2013-04-14 16:54:46,"@ssbarnea's pull request was merged closing this issue.
",sigmavirus24,ssbarnea
1296,2013-04-08 17:03:44,"Good point @sigmavirus24. Fixed my wording.
",sursh,sigmavirus24
1294,2013-06-30 03:32:09,"@kennethreitz @Lukasa @GP89 is there any interest in this issue any longer? I'm not keen on the solution I had started to work out and the issue wasn't really ours in the first place as can be seen by the discussion on e4e7eb8
",sigmavirus24,GP89
1294,2014-10-20 18:33:29,"Hey @jgillmanjr 

It doesn't seem that the `TypeError` is caused by how you're handling the exception. It seems like there's a line in your script (`exceptionTest.py`) that reads `print con.request()` and whatever `con.request()` returns cannot be coerced to a string.
",sigmavirus24,jgillmanjr
1294,2014-10-21 12:42:15,"@sigmavirus24 

Here's a gist that I created to give more context: https://gist.github.com/jgillmanjr/fb4431bec403de48cf22

I ultimately realized that it was the cert on the testing box that wasn't passing verification. But I'd still like to know if I was trying to get the exception properly or if the exception that is being returned could use some fixing.

Thanks again!
",jgillmanjr,sigmavirus24
1294,2014-10-22 20:18:04,"@jgillmanjr sorry for the delay. I tried out something on my end with requests 2.4.3



So I'm not sure why you're seeing a problem honestly.
",sigmavirus24,jgillmanjr
1294,2014-10-22 22:19:55,"@sigmavirus24 Not a problem.

It seems when I  run it from the interactive shell, it sort of works:



However, it looks as though I'm using 2.3.0

/// Break ///

Upgraded to 2.4.3 and still seeing the same thing.
",jgillmanjr,sigmavirus24
1294,2014-10-23 01:37:02,"@jgillmanjr, I just realized that this is likely due to your using pyOpenSSL, ndg-httpsclient, and pyasn1. I'm going to test with those installed to see if I can reproduce this.
",sigmavirus24,jgillmanjr
1294,2014-10-23 12:03:21,"@sigmavirus24 Well, I _may_ have figured it out.

I forgot that I actually initially installed pip via apt. This subsequently required the package `python-requests` which was only at version 2.3.0. I realized this once I started getting issues with requests after I did a pip upgrade on it and pip proceeded to not work very well any more.

Now that I've removed the packages, installed pip via the `get-pip.py` script, I am not recieving any more errors:



I might have to report a bug to the package managers.

Sorry for the time sink!

-Jason
",jgillmanjr,sigmavirus24
1294,2014-10-23 13:17:02,"@sigmavirus24 If you care to weigh in, here is a link to the bug report: https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=766481
",jgillmanjr,sigmavirus24
1294,2014-10-23 14:12:38,"That's really bizarre @jgillmanjr I'm going to keep my neck out of that bug report but I trust @eriolv will be a great help to you.
",sigmavirus24,jgillmanjr
1294,2014-10-23 15:37:24,"@sigmavirus24 Thanks as usual for bringing me in! :)
I have investigated a bit and the problem seems related to pyasn1. See my reply
https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=766481#10
Unfortunately I can check deeply right now...
",eriol,sigmavirus24
1294,2014-11-02 02:35:39,"@sigmavirus24 It seems to be somting related to pyasn1: I can reproduce on a virtualenv and @jgillmanjr confirmed it. Can I link the Debian Bug with this one? Or you prefer a new issue since the TypeError is only due pyasn1?
",eriol,jgillmanjr
1294,2014-11-02 02:35:39,"@sigmavirus24 It seems to be somting related to pyasn1: I can reproduce on a virtualenv and @jgillmanjr confirmed it. Can I link the Debian Bug with this one? Or you prefer a new issue since the TypeError is only due pyasn1?
",eriol,sigmavirus24
1294,2014-11-02 03:09:45,"@sigmavirus24 `<class 'requests.packages.urllib3.exceptions.SSLError'>` is what I get when I run `print type(e.message)`
",jgillmanjr,sigmavirus24
1292,2014-06-26 07:38:05,"@miteshsc Sorry, but how is that comment relevant to this issue?
",Lukasa,miteshsc
1292,2014-06-26 07:45:10,"@Lukasa: Sorry for posting irrelevant comment but i was facing problem, thats why i have posted my cUrl call.
",miteshsc,Lukasa
1292,2016-07-15 16:52:01,"@josetiagobispo this seems not at all relevant to this project. Please do not post bug reports related to other projects on this projects issue tracker.
",sigmavirus24,josetiagobispo
1289,2013-07-22 07:52:40,"@Lukasa Thanks for the suggested solution. Although, I'm not using SSL in my requests. 
",PerfectedTech,Lukasa
1289,2013-07-22 09:10:04,"@Lukasa, It very well could be an error with the script and not the library. The odd thing here is that the address in question is completely accessible.

Here's a traceback:


",PerfectedTech,Lukasa
1289,2013-07-22 14:33:39,"The issue was related to urllib3 and AWS ELB. It was resolved when routing the request without the ELB. Hardly a desirable option... Regardless, thanks for your help @Lukasa .

""First, ELB has internal ""feature"" of closing all incoming HTTP connections that do not get response within 60 seconds. I.e. if your client executes GET ... and waits for more then 60 seconds - ELB will close the connection. This timeout is currently unconfigurable through amazon API and even not advertised in Amazon docs. ""
http://tech.zarmory.com/2013/02/handling-tmeouts-with-amazon-elastic.html
",PerfectedTech,Lukasa
1289,2013-08-22 07:50:56,"@Lukasa No. Just a locally run Python script that does simple HTTP requests. I'm on a Kubuntu GNU/Linux x86_64 desktop.
",mnemonicflow,Lukasa
1289,2013-08-22 11:42:54,"@Lukasa I've reinstalled requests after I deleted it first, and installed it with <b>sudo pip install git+git://github.com/kennethreitz/requests.git</b> . I get exactly the same TypeError, that is: 


",mnemonicflow,Lukasa
1289,2013-08-22 11:49:01,"@Lukasa httplib.HTTPConnection.getresponse() doesn't accept any keyword arguments in Python 3.3 

See:
1) http://docs.python.org/3.3/library/http.client.html?highlight=httpconnection#http.client.HTTPConnection.getresponse
2) http://hg.python.org/cpython/file/3.3/Lib/http/client.py#l1101

The comments are incorrect in /usr/local/lib/python3.3/dist-packages/requests/packages/urllib3/connectionpool.py line 290:



I'll try to send a fix to urllib3.
",mnemonicflow,Lukasa
1289,2013-12-23 23:49:13,"See what I don't get is that most these seem to be URLs that are unreachable at times, and I've tried IPs locally that are unreachable but otherwise _might_ work. I never even get close to the `getresponse` code, and I have the same concern as you @Lukasa .
",sigmavirus24,Lukasa
1289,2013-12-23 23:53:00,"@bodiam based on earlier discussion, that coment may be a lie. ~~But the existing comments sound correct.~~

That aside, the reason we're so concerned is that we do the following



Which means try to get the response with buffering, but if that doesn't work catch an exception:



In otherwords, the exception that you're seeing: `TypeError: getresponse() got an unexpected keyword argument 'buffering'` should be caught by the second snippet I posted and handled without ever bubbling up to you.
",sigmavirus24,bodiam
1289,2013-12-23 23:53:54,"@bodiam The comment is misleading. Python 3.3 doesn't take keyword arguments on `getresponse()`, which would also throw a `TypeError`. That causes the `TypeError` reported in all of these stack traces. But it should be immediately caught and passed into the next block. 

For some _insane_ reason that seems not to be happening here. This bothers me because I've never seen it, @sigmavirus24 has never seen it, and when I try isolated tests that attempt to reproduce this problem I can't do it.

This seems impossible, which means it's a really nasty bug.
",Lukasa,bodiam
1289,2013-12-23 23:53:54,"@bodiam The comment is misleading. Python 3.3 doesn't take keyword arguments on `getresponse()`, which would also throw a `TypeError`. That causes the `TypeError` reported in all of these stack traces. But it should be immediately caught and passed into the next block. 

For some _insane_ reason that seems not to be happening here. This bothers me because I've never seen it, @sigmavirus24 has never seen it, and when I try isolated tests that attempt to reproduce this problem I can't do it.

This seems impossible, which means it's a really nasty bug.
",Lukasa,sigmavirus24
1289,2013-12-23 23:59:48,"@Lukasa You're right. The comment is incorrect, it always goes to the second (the 2.6) part, I assume because the conn is a http.client.HTTPConnection (I checked, it is)

I'm also not sure why this error happens, but like I said, it seems to only happen in case of a connection error. I'll take a look once it occurs again, hopefully within the next days! Thanks so far guys!
",bodiam,Lukasa
1289,2013-12-28 11:14:55,"@bodiam That's exactly the same outcome I get when I'm making requests to Wikipedia API (they have max simultaneous requests from the same IP limit, and when you exceed they will close the connection)
",mnemonicflow,bodiam
1289,2013-12-28 11:30:07,"Riiiight, now that makes sense.

This is an unforeseen problem to do with how exception tracebacks are being reported in Python 3. [PEP 3134](http://www.python.org/dev/peps/pep-3134/) introduced this 'chaining exceptions' reporting that you can see in @bodiam's traceback. The purpose of this error reporting is to highlight that some exceptions occur in `except` blocks, and to work out what chain of exceptions was hit. This is potentially very useful: for instance, you can hit an exception after destroying a resource and then attempt to use that resource in the `except` block, which hits another exception. It's helpful to be able to see both exceptions at once.

The key is that the `TypeError` raised as the first exception is _unrelated_ to the subsequent ones. In fact, that's the standard control flow in `urllib3`. This means that the real exception that's being raised here is the `request.exceptions.ConnectionError` exception that wraps the `urllib3.exceptions.MaxRetryError` exception being raised in `urllib3`.

This is _not_ a Requests bug, it's just an ugly traceback introduced by Python 3. We can try to reduce the nastiness of it somewhat by refactoring the method in `urllib3` (though I don't believe it's possible: @shazow?), but that'll only remove the `TypeError` from the chain: the rest will stay in place.
",Lukasa,bodiam
1289,2013-12-28 19:00:22,"Yeah, that's roughly what I thought you'd say. It's a shame this trace back is so misleading. I'll investigate whether there is some way to strip the irrelevant bits of the TB in situations like this, because @sigmavirus24 and I have spent many more hours than I'd like chasing this issue. 
",Lukasa,sigmavirus24
1289,2014-01-17 06:53:17,"@mrfatboy I'm afraid that's all you can do: this is just an artefact of the library design on 3.3. 
",Lukasa,mrfatboy
1289,2015-11-26 08:49:02,"Sorry, @juliusakula, but what exactly is the bug here? Currently it's not clear that we're doing anything wrong: the remote server is closing the connection unexpectedly and we are reporting the event. What you should investigate is _why_ the remote server is closing the connection.
",Lukasa,juliusakula
1289,2016-03-13 03:32:10,"@hwkns apparently it's undocumented, but Python 2 accepted `buffering`: https://hg.python.org/cpython/file/2.7/Lib/httplib.py#l1099
",sigmavirus24,hwkns
1289,2016-04-26 19:15:41,"@sandeepnassa sorry, but this isn't the appropriate place to ask for help with that.  Also, when you do find the right place, no one will be able to tell you why the Jira API is closing your connection without seeing your code.
",hwkns,sandeepnassa
1289,2016-04-26 19:22:35,"@sandeepnassa That exception is caused by the machine hosting the URL you're using forcibly tearing the connection down. It is very hard to work out why that would be happening without extremely large amounts of detail. However, if they've been working fine for a month: did you update JIRA? They may have changed their API.
",Lukasa,sandeepnassa
1289,2016-04-26 19:31:22,"Thanks @Lukasa for response
I check with Jira API, they are in denial that they have changed anything about those apis.
I want to double check from my side that it Is nothing related to python (which is owned by me), there is no setting in python which I should use to correct this.. Before I can ask Jira guys again firmly that everything Is correct from my side. 

So was looking for expert knowledge.

BTW.. this is the header I am using to call those Jira restful APIs
",sandeepnassa,Lukasa
1289,2016-05-16 02:20:30,"@zbyte64 except that doesn't work consistently across all versions of Python 3. 3.5 for example doesn't allow that to work as easily
",sigmavirus24,zbyte64
1286,2013-04-16 18:02:30,"@Lukasa, the binary can't hear you.
",sigmavirus24,Lukasa
1285,2013-04-02 22:00:59,"@sigmavirus24 
Could you provide more details/a transcription of the IRC conversation?
I created a really minimal VM of ArchLinux (base installation + python from the repos + requests from pip and from the repos) and everything worked fine.
",t-8ch,sigmavirus24
1285,2013-04-02 22:06:59,"@t-8ch

The following is the start of the conversation: 
https://botbot.me/freenode/python-requests/msg/2523881/

You can see I worked through it with upmauro until we realized that his 
version of python 3 had been compiled without support for `ssl`.  
(https://botbot.me/freenode/python-requests/msg/2523881/ to save you some 
reading)

I know that in one of the recent changes I saw in urllib3 the following:



I suspect the fact that no exception was raised was because of the silent 
failure of importing the `ssl` module. I obviously have to attempt to build a 
system in which the ssl module would be unavailable in a version of python to 
be absolutely certain, but judging by the behaviour it seems a fair 
conclusion.
",sigmavirus24,t-8ch
1284,2013-04-02 13:31:37,"@KamilSzot why would you want the unprepared request in the first place? You can do this very easily on your own and that aside, you can not send an unprepared request to `Session#send` or the underlying adapter. The changes don't make any sense from that point of view. Beyond that, the ability to return the unsent request was **deliberately** removed during the refactor for the 1.x release
",sigmavirus24,KamilSzot
1284,2013-04-02 18:42:15,"@KamilSzot would you mind closing this?
",sigmavirus24,KamilSzot
1281,2013-04-02 12:11:31,"@kennethreitz why reintroduce this behaviour? It was a documentation mistake on my part.
",sigmavirus24,kennethreitz
1273,2013-04-03 13:28:35,"I agree with @lukasa. In fact to expand a bit on his point about the last statement I'll say this: 

We emulate browser behaviour in some instances because in spite of the relevant RFC some servers will only behave ""correctly"" where ""correctly"" is how browsers expect it to behave. In those cases, RFC be damned, that's how we have to behave but at no point is requests a replacement for a browser, or a programmatic browser. Were that the case, we would have to keep a session history tracing back to the very first request. We provide histories for individual requests because being able to know that a redirect occurred is very important. It isn't so important on a session.

I was and remain frankly -0 on this because this existed well into 1.x (I think until 1.1.0) but when we asked if anyone was using it, no one replied until now. If it isn't used, it is just causing code-smell and will only cause confusion for future editors of and contributors to the project.

@Lukasa's solution is elegant and works very well. Even easier might be a hook like so:



Admittedly I don't have the time to test that, but I'm 90% sure it will work. :)
",sigmavirus24,Lukasa
1273,2013-04-03 17:38:58,"Thanks for the examples; I'll consider them if this PR doesn't go through.

@sigmavirus24: When/where was it asked if anyone was using this response property? I didn't even hear about it until I started pulling straight from github (rather than PyPI).
",rowedonalde,sigmavirus24
1270,2013-03-29 13:18:31,"@Lukasa 

Thanks ;)
Glad to be helpful.
",makto,Lukasa
1269,2013-03-29 11:34:04,"@Lukasa 

Thank you for your reply :)
Session object works well most of the time on my machine. This exception occurs only in rare cases.
I don't think it has anything to do with the installation.
Never mind, it didn't bother me right now. Just want to help find some potential bugs :)
",makto,Lukasa
1269,2013-03-29 12:50:55,"@makto could you perhaps give us more relevant information, such as the version of requests you're using and the context (in code) of the exception?
",sigmavirus24,makto
1269,2013-03-29 13:15:25,"@sigmavirus24 
I traced the error and thought it could be brought by `max_redirects`'s missing in **attr**.
I pickled the session object first and then unpickled it. I believe the max_redirects is missing during the process.

See my [pull request](https://github.com/kennethreitz/requests/pull/1270)
",makto,sigmavirus24
1266,2013-04-01 18:30:34,"Changes to fix this, according to your estimation of the problem @dabono, are in 1.2.0 which is out on PyPI and Crate. I'm closing this but if you still have this issue, let me know and I'll reopen the issue.
",sigmavirus24,dabono
1265,2013-04-06 19:18:03,"@zopyx could you also report on your version of openssl (`openssl version`).
",sigmavirus24,zopyx
1265,2013-04-08 16:12:38,"I'm very tempted to close this since @zopyx is unresponsive.
",sigmavirus24,zopyx
1265,2013-04-08 17:35:36,"- What does the following code show you (in the relevant venv)?
  (And does the file exist)



_Edit:_
Do you have `REQUESTS_CA_BUNDLE` set in your environment?
<\/edit>
- Does this happen with plain requests (`requests.get(""https://httpbin.org"")`?
- Where did you get python27 from?
- Does it work with the default python26?
- Which version of bandersnatch are you using?
- Does your bandersnatcher change the location of the cert bundle? (via the `verify=` parameter)

I can't reproduce this.

@sigmavirus24 My ubuntu container was slow to install :-)
",t-8ch,sigmavirus24
1265,2013-04-25 18:43:23,"@zopyx: Just wanted to remind you about this issue. =)
",Lukasa,zopyx
1258,2013-03-22 21:40:16,"@sigmavirus24 something like the following:


",geoff-kruss,sigmavirus24
1255,2013-03-22 16:02:32,"@Lukasa I can't wait until all the chunks are there, because this is a file upload server and it's imperative that only the current chunk from the client stays in memory at any one given time. If they were all being saved, writing a generator or a file-like object would be a cakewalk. 

@sigmavirus24 I can't reliably provide `Content-Length`, as I'm accepting uploads from users and I can't know for certain the size of the files they're uploading. They could easily upload a 4GB file and lie to me about the content length being 1024B. 

Your example makes a lot more sense of the problem than I've seen so far, so kudos on that! Essentially, it would seem that I'd have to implement a blocking queue and a thread to make this happen:



Something like the above would seem to be able to solve my problem, right? Essentially, when a new file comes down the pipe, an upload request is generated in another thread so that it won't block the main thread. Immediately after being created, it attempts to get from the queue, which blocks because it's empty. Then, when the upload handler receives a chunk, it puts it into the queue, which is read by the request. The put will block if there's more than one chunk in the queue, so this will manage memory well. This will continue until the upload is done. Once it's done, we simply set `is_running` to `False`, which breaks the iterator's loop and the thread (presumably) exits gracefully.

The only problem I can see so far is trying to handle the case in which a request failed. We'd have to check in the `on_file_chunk` method and break the whole process if the request failed. Does this seem to work? Like I said, I'm  a bit new at this whole generator thing, but I understand threading fairly well. 
",naftulikay,Lukasa
1255,2013-03-22 16:02:32,"@Lukasa I can't wait until all the chunks are there, because this is a file upload server and it's imperative that only the current chunk from the client stays in memory at any one given time. If they were all being saved, writing a generator or a file-like object would be a cakewalk. 

@sigmavirus24 I can't reliably provide `Content-Length`, as I'm accepting uploads from users and I can't know for certain the size of the files they're uploading. They could easily upload a 4GB file and lie to me about the content length being 1024B. 

Your example makes a lot more sense of the problem than I've seen so far, so kudos on that! Essentially, it would seem that I'd have to implement a blocking queue and a thread to make this happen:



Something like the above would seem to be able to solve my problem, right? Essentially, when a new file comes down the pipe, an upload request is generated in another thread so that it won't block the main thread. Immediately after being created, it attempts to get from the queue, which blocks because it's empty. Then, when the upload handler receives a chunk, it puts it into the queue, which is read by the request. The put will block if there's more than one chunk in the queue, so this will manage memory well. This will continue until the upload is done. Once it's done, we simply set `is_running` to `False`, which breaks the iterator's loop and the thread (presumably) exits gracefully.

The only problem I can see so far is trying to handle the case in which a request failed. We'd have to check in the `on_file_chunk` method and break the whole process if the request failed. Does this seem to work? Like I said, I'm  a bit new at this whole generator thing, but I understand threading fairly well. 
",naftulikay,sigmavirus24
1255,2013-03-25 16:09:05,"@sigmavirus24 In your example, how then would I pass data received in `on_file_chunk` to `generate_data`?

Since I imagine that the call to `requests.put` blocks until completed, I would imagine that this code would essentially immediately hang. Looks like I'm working with threads.
",naftulikay,sigmavirus24
1252,2013-03-27 18:23:42,"@grillermo it isn't a matter of reproducing the bug or not believing you. The stack trace is fairly clear on the matter. The problem is that we're all quite busy.
",sigmavirus24,grillermo
1252,2013-03-27 21:30:27,"@Lukasa We recently had a discussion about header encoding in [shazow/urllib3#164](https://github.com/shazow/urllib3/pull/164#issuecomment-15366629) at which you might want to look.
",t-8ch,Lukasa
1252,2013-03-28 02:40:12,"If we ensure everything is bytes, this will work well for Python 2 because `str`s are `bytes` objects. In python 3 this seems to produce an issue like @t-8ch mentioned. Naturally it's perfectly fine for there to be multiple header values and there are no bizarre characters in headers (and cannot be if I remember the spec properly) so the coercion to whatever will be fine. You might think this falls on our shoulders because it doesn't seem that too many urllib3 users have reported this issue, but you're wrong.

The problem with doing this is exactly the case where we're reading binary data which is a very common use case. If we're provided a file (or file-like object). We have no way of knowing if it's binary data or not and images and the like can't be coerced to text. This makes me think that the burden lies on urllib3 to coerce everything together.

Either way, I feel obligated to leave [this](http://nedbatchelder.com/text/unipain.html) behind.
",sigmavirus24,t-8ch
1252,2013-03-28 10:34:26,"Yeah, I was excluding Requests' behaviour for a moment, and just trying to nail down what urllib3 should be doing. Then we could change Requests to program to that interface. =)

Thomas, I'm also quite happy with your proposal there. If @shazow thinks that's the way it should go, the fix belongs outside urllib3. :fireworks:
",Lukasa,shazow
1252,2013-03-28 15:10:49,"So this can not block 1.2.0 unless @kennethreitz really wants it to. 

Perhaps to satisfy @michaelhelmick and company we should add a notice to the release that we realize that this is broken and a fix is being worked on in shazow/urllib3
",sigmavirus24,michaelhelmick
1252,2013-03-28 16:45:27,"@michaelhelmick I hope you got better sleep than I did. :) And yes, as soon as this gets fixed, I would be certain to bug @kennethreitz about a bump to 1.2.1

And there's no need to apologize.
",sigmavirus24,michaelhelmick
1252,2013-03-28 16:47:29,"@sigmavirus24 I got about 9 hours, haha. And alright, and if he doesn't bump on your first request; we'll start a trending topic on Twitter ;D
",michaelhelmick,sigmavirus24
1252,2013-03-30 05:05:55,"@sigmavirus24 I'd like to treat urllib3 as more of an expected-input-expected-output library, and Requests to do the ""do silly thing to input to make behaviour more user-friendly"" stuff. Does that make sense?
",shazow,sigmavirus24
1252,2013-04-02 18:26:31,"Possibly in order to punish us (:wink:), requests-oauthlib does not work on Python3 if you upload files. That's because Requests uses `encode_multipart_formdata` from urllib3, which returns the content-type as bytes. @shazow: is that intentional? If so, I can work around it here. If not, I can offer you a PR to fix it.
",Lukasa,shazow
1252,2013-05-15 07:59:27,"@marselester: What version of Requests are you using? I can't reproduce this in Requests v1.2.0, using either Python 2.7 or Python 3.3.
",Lukasa,marselester
1252,2013-05-15 09:08:18,"@Lukasa, thank you. When I can use `requests.post()` I use it :)
",marselester,Lukasa
1248,2013-03-19 13:54:41,"@robin-jarry for what it's worth, HEAD is 98% of the time Pretty Damn Stable&trade;
",sigmavirus24,robin-jarry
1247,2013-03-19 20:10:47,"+1 to @Lukasa's comment :)
",kennethreitz,Lukasa
1247,2013-03-21 12:59:52,"I'll third @Lukasa's comment.
",sigmavirus24,Lukasa
1247,2013-03-21 15:51:03,"@ross I tweeted about it and put it in the wiki: https://github.com/kennethreitz/requests/wiki

If others come asking about similar items, I'll be sure to point them in your direction. :) Thanks
",sigmavirus24,ross
1246,2013-04-16 13:33:58,"@Lukasa 
""Explicit is better than implicit"" does not really apply to this at all. If someone downloads RELEASE then it's because he wants RELEASE and he knows that he will get RELEASE and what RELEASE means. Let me summarize with another quote - ""All problems in computer science can be solved by another level of indirection""
",piotr-dobrogost,Lukasa
1246,2013-04-16 13:44:32,"More simply put, this is @kennethreitz's project and he can do as he pleases and he doesn't need to explain his actions to anyone. If he would rather not do this then that is his call.
",sigmavirus24,kennethreitz
1246,2013-04-16 13:57:28,"@piotr-dobrogost: I respectfully disagree, but happily this is both a) a matter of opinion and b) not something about which our opinions matter at all. =)
",Lukasa,piotr-dobrogost
1246,2013-04-16 15:53:13,"@sigmavirus24 <3
",kennethreitz,sigmavirus24
1242,2013-03-11 14:25:43,"Sorry @Lukasa I've been pretty sick for the past few days. I'll take a look later this week I hope. I would think though that the standard string operations would be enough and provide less overhead than a regular expression. 
",sigmavirus24,Lukasa
1242,2013-03-12 12:20:09,"@sigmavirus24 All this can be done with standard string operations, but you would have to implement parts of what the re module already has. It would be easy if we could just do a s_auth.lower() on the whole thing and replace what we need, but the case on the rest of the string is important.

The way I see it there are 2 choices. Either we use the re module, or we add code to do a case insensitive replace in requests itself. Basically its a choice between overhead (which I'm not convinced is much) and code complexity. :)
Pisses me off that web server developers don't read the damn standards before implementing something :angry: . 

P.S. Get better soon! :D
",gabriel-samfira,sigmavirus24
1242,2013-03-22 19:52:01,"@kennethreitz you need to update .travis.yml
",sigmavirus24,kennethreitz
1242,2013-03-28 03:55:54,"@kennethreitz this looks good.
",sigmavirus24,kennethreitz
1241,2013-03-15 20:44:20,"@dhagrow That seems a bit unintuitive.
",Anorov,dhagrow
1241,2013-03-19 14:00:49,"@schlamar actually I believe urllib3 handles getting it via the environment variables for us, so there really shouldn't be an API change necessary.

I'm also :-1: on this
",sigmavirus24,schlamar
1236,2013-06-05 21:55:17,"Yep, @dmalenko is right. We're getting this exact issue too (with the same traceback). I _don't_ think it's simply urllib3's fault, because it's requests that's calling `urllib3.Response.read()` and not handling errors around that call.

Arguably urllib3 should handle socket.timeout or other errors too, and convert them to a urllib3.TimeoutError. But even if it did that, requests would still need to catch that and re-raise a `requests.Timeout`, otherwise the user of requests would still have to know to catch `urllib3.TimeoutError`.

So I think requests should simply catch the socket.timeout / socket.error in `models.Response.iter_content()`. It could also catch `urllib3.TimeoutError` and do the same thing, in case urllib3 fixes their part of this issue.
",benhoyt,dmalenko
1236,2013-06-06 08:38:10,"@benhoyt We're in total agreement. I had this assigned to me but haven't had time to do anything about it, and won't do for a few weeks. We're open to a Pull Request if you'd like to write one, otherwise this is unlikely to happen in the next couple of weeks.
",Lukasa,benhoyt
1236,2013-06-13 19:36:39,"I looked into this a bit. I didn't get too far, but I'll note my observations for the benefit of others. [The documentation](http://docs.python-requests.org/en/latest/user/quickstart/#timeouts) says:

> timeout only effects the connection process itself, not the downloading of the response body.

As seen here, it's not true. In @dmalenko's case, the connection is opened and the headers are sent fast enough, but the server sends data so slowly that recv timeouts. The same thing happens with `iter_content`.

This happens because urllib3 sets the timeout for the underlying httplib socket to catch connection timeouts. After the connection is established, the timeout stays set and affects the recv calls, but the errors aren't catched anymore.
",miikka,dmalenko
1236,2013-06-13 21:22:16,"Thanks, @miikka. I notice that part of the documentation is inside a ""Note:"" -- to me it looks like that's basically an implementation detail. I'd think when I specify a timeout I want it to be a timeout on the whole thing, so maybe in this case the documentation can simply be changed to match reality?

Note that I don't think the above affects the issue in any way -- we still want this to be a `requests.Timeout`.
",benhoyt,miikka
1236,2013-06-14 08:18:41,"Yeah @miikka, you're right, it isn't true. Sometimes.

We confusingly use `timeout` in two different ways. For non-chunked requests, we use `timeout` as a parameter on the underlying HTTP connection, which applies it just as httplib would. For chunked uploads, we use it as a timeout on getting the connection from the connection pool, and then apply no timeout to the connection itself (see #1422). I'll look into getting this mess sorted out, but if you want to try to tackle it yourselves please do. =)
",Lukasa,miikka
1236,2014-08-20 23:43:18,"Thanks @miikka for the explanation that let me reliably reproduce this error using HTTPBin's Drip endpoint:



I'm working around as @kaisarea suggested, catching both the `RequestException` exception and the `socket.timeout`.
",johnboxall,miikka
1236,2014-08-20 23:43:18,"Thanks @miikka for the explanation that let me reliably reproduce this error using HTTPBin's Drip endpoint:



I'm working around as @kaisarea suggested, catching both the `RequestException` exception and the `socket.timeout`.
",johnboxall,kaisarea
1236,2015-08-21 14:00:08,"Ok @Lukasa , thanks! I need to implement a ""retry"" logic and I can't use urllib3 retries because I need more control. So I needed to know which exceptions should I catch, but I'll catch `Exception` anyway and catch others to do custom stuff for each one.
",fjsj,Lukasa
1236,2015-12-02 18:08:47,"@sbv-csis What version of requests, and where did you install it from?
",Lukasa,sbv-csis
1236,2015-12-03 08:28:00,"@Lukasa We use 2.8.1 from https://pypi.python.org/pypi/requests
",sbv-csis,Lukasa
1236,2015-12-03 08:37:39,"@sbv-csis Can you post any code that will reproduce this? And the traceback?
",Lukasa,sbv-csis
1233,2013-03-06 17:58:57,"Yeah, and re @schlamar's concern: https://github.com/shazow/urllib3/pull/56#issuecomment-14412603 would indicate we'd have to wait longer than I think you'd like to.
",sigmavirus24,schlamar
1233,2013-03-27 18:30:15,"Tonight, I'm going to put together a changelog as best as I can. I post it here, and after @Lukasa wraps up #1252 I think we should cut 1.2 tonight.
",sigmavirus24,Lukasa
1228,2013-03-01 23:55:57,"@JohnCC330 how are you passing the `SESSIONID` cookie? Could you give us a snippet so we can attempt to reproduce it?
",sigmavirus24,JohnCC330
1228,2013-03-02 01:07:18,"On Fri, 01 Mar 2013 15:56:14 -0800
Ian Cordasco notifications@github.com wrote:

> @JohnCC330 how are you passing the `SESSIONID` cookie? Could you give us a snippet so we can attempt to reproduce it?

Hope this helps... Tell me if you need anything else.

John


",JohnCC330,JohnCC330
1228,2013-03-09 07:45:10,"@neurostar, @JohnCC330: Right, sorry for my confusion. I botched my versions yesterday and only tested using the released version, which works. Git version indeed fails.
",miikka,JohnCC330
1228,2013-03-09 07:45:10,"@neurostar, @JohnCC330: Right, sorry for my confusion. I botched my versions yesterday and only tested using the released version, which works. Git version indeed fails.
",miikka,neurostar
1228,2013-09-27 08:22:38,"@tumb1er That doesn't seem to be true for me:



Looks to me like they were set on the 302 and sent back on the request after the redirect. 
",Lukasa,tumb1er
1227,2013-03-02 09:47:53,"I'm +0 on this. As you said @dmedvinsky, it's a minor refactor. I have no objection to it being in the library, and it's an internal API really, so not too problematic. I'm happy to go with whatever @kennethreitz feels is right here.
",Lukasa,dmedvinsky
1222,2013-02-28 22:11:06,"Ah @t-8ch sorry for the confusion. I must have misread @Lukasa's response to #1221 last night (or had some rather wild dreams of what it said). Thanks for the correction.
",sigmavirus24,t-8ch
1221,2013-03-11 22:19:07,"@dalanmiller what version are you trying to install?
",sigmavirus24,dalanmiller
1220,2013-02-28 22:13:14,"@mjallday well there should only be one obvious way to do it. ;)
",sigmavirus24,mjallday
1220,2013-03-01 23:54:46,"Ah I see the issue. @Lukasa's comment could be losely misunderstood as meaning setting `allow_redirects` to False would raise an exception when a redirect was found. A pattern that could work for you in it's stead is this:



Where I'm making an assumption that you're returning the response without any evidence of that ;)
",sigmavirus24,Lukasa
1220,2013-03-01 23:57:15,"@sigmavirus24 you've implemented pretty much exactly what I have on my screen right now!

Feel free to close this ticket if you don't think the variable naming is misleading, my immediate concern is addressed.
",mjallday,sigmavirus24
1220,2013-03-01 23:59:11,"@mjallday I'm psychic but don't tell anyone. ;)

Yeah, I think I might re-ticket this to see if we should raise exceptions for stati >= 500 too though. That seems like something worth discussing more.
",sigmavirus24,mjallday
1220,2013-03-02 09:45:27,"@sigmavirus24: [We already do](https://github.com/kennethreitz/requests/blob/master/requests/models.py#L656).
",Lukasa,sigmavirus24
1219,2013-03-03 06:27:22,"Uh, @kennethreitz, did you intend to merge this? Your comment seems like you weren't going to, but then you kinda did...
",Lukasa,kennethreitz
1218,2013-02-26 18:09:51,"I'll recap the issue with the scheme :-)

Most people use one proxy for both http and https.
The connection between requests and the proxy is mostly http. (This is what the
scheme in the proxy url is for)
The proxy can handle plain http requests just fine.
Encrypted connections are just some random tcp stream to the proxy. This is
where the http 'CONNECT' verb is used. Http requests made using CONNECT are sent to the proxy
where the body of the request is the encoded _raw_ data of the tcp connection.
The proxies then decodes the body and forwards the raw data, receives the
response, reencodes it and sends the raw, encoded response back to the requester.
(This is how all those http proxy tunnels work)
Urllib3 doesn't support CONNECT at the moment (will be supported very soon!).
Proxied https requests are therefore sent in plaintext in the form of
`GET https://httpbin.org/ HTTP/1.1`
This is not how https should be proxied and why you get the error at your
proxy. (My squid for example doesn't care and does the right, erm.. wrong thing)
(@gfairchild: this is the reason for your last error)

Again, the reason one should add the scheme is that you most will likely use the same
proxy for both http and https. As requests will guess the _proxy_ scheme based
on the _request_ scheme.
This is unfortunate as both aren't correlated.
This way you may end up connecting to your proxy on the same port/ip one time
with plain http and the other time with https.
Your proxy most likely won't support this.
This is the reason for all those ""Unknown SSL version"" bug reports. Request is
trying to establish a ssl connection with a _non-ssl_ proxy which sends plain
http responses and therefore the handshake has no chance to succeed.
",t-8ch,gfairchild
1215,2013-02-25 10:32:57,"@sigmavirus24 sure, this once :)
",kennethreitz,sigmavirus24
1213,2013-02-25 10:30:46,"I agree with @Lukasa. 

However, the reverse lookup is something I've wanted to add for a while:


",kennethreitz,Lukasa
1211,2013-02-27 18:41:00,"@bluefoxicy @Lukasa is this still an open issue? It looks like its being resolved for the opener offline.
",phildini,Lukasa
1211,2013-02-27 18:41:00,"@bluefoxicy @Lukasa is this still an open issue? It looks like its being resolved for the opener offline.
",phildini,bluefoxicy
1211,2013-02-27 18:44:38,"My particular issue was a bug in my code.  This still sends GET requests.
Looks like I missed the documentation for this, so maybe a documentation
issue but this report is otherwise invalid.
On Feb 27, 2013 1:41 PM, ""Philip James"" notifications@github.com wrote:

> @bluefoxicy https://github.com/bluefoxicy @Lukasahttps://github.com/Lukasais this still an open issue? It looks like its being resolved for the
> opener offline.
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/issues/1211#issuecomment-14191018
> .
",bluefoxicy,bluefoxicy
1211,2013-02-27 18:47:00,"@phildini, @bluefoxicy is simply providing sensitive information to @Lukasa so that he can get a better idea of what is happening in his specific case. The issue of dumping content is a moot one, IMO, but do-able as @Lukasa suggested. This feature has been requested several times before (unsuccessfully obviously) and combining its track record with our feature freeze, it likely won't be added to requests.

It could be done in the form of a response hook (beyond what @Lukasa mentioned), e.g.,



Also since @bluefoxicy has claimed this to be invalid on his own accord, I'm closing this issue.
",sigmavirus24,phildini
1211,2013-02-27 18:47:00,"@phildini, @bluefoxicy is simply providing sensitive information to @Lukasa so that he can get a better idea of what is happening in his specific case. The issue of dumping content is a moot one, IMO, but do-able as @Lukasa suggested. This feature has been requested several times before (unsuccessfully obviously) and combining its track record with our feature freeze, it likely won't be added to requests.

It could be done in the form of a response hook (beyond what @Lukasa mentioned), e.g.,



Also since @bluefoxicy has claimed this to be invalid on his own accord, I'm closing this issue.
",sigmavirus24,Lukasa
1211,2013-02-27 18:47:00,"@phildini, @bluefoxicy is simply providing sensitive information to @Lukasa so that he can get a better idea of what is happening in his specific case. The issue of dumping content is a moot one, IMO, but do-able as @Lukasa suggested. This feature has been requested several times before (unsuccessfully obviously) and combining its track record with our feature freeze, it likely won't be added to requests.

It could be done in the form of a response hook (beyond what @Lukasa mentioned), e.g.,



Also since @bluefoxicy has claimed this to be invalid on his own accord, I'm closing this issue.
",sigmavirus24,bluefoxicy
1209,2013-02-22 06:58:39,"@Lukasa 
And how do you explain this:


",schlamar,Lukasa
1209,2013-02-22 08:02:20,"@Lukasa After reading http://curl.haxx.se/mail/lib-2010-03/0018.html I would say that the current behavior is broken in requests! Doing `GET https` on a proxy is non-standard behavior.

However it should be fixed with https://github.com/shazow/urllib3/issues/139 (assuming the cert verification issue will be solved) so I think this issue can be left as closed.
",schlamar,Lukasa
1209,2013-02-22 08:11:16,"@schlamar: I think you're right too, though I don't have the time right now to really explore it.

As and when urllib3 gets sorted I'll probably give you a nudge because it sounds like your network is set up to test this. =D
",Lukasa,schlamar
1209,2013-02-22 17:18:58,"@Lukasa Please also nudge me :)

Being able to access an HTTPS site through an HTTP(S) proxy is a critical requirement for me, and I have the environment to test it.

FYI, my workaround right now is to read stdout from `curl` in a subprocess. Yuck :p
",maxbane,Lukasa
1209,2013-02-22 17:27:57,"@maxbane What about urllib2: https://gist.github.com/schlamar/2993700
",schlamar,maxbane
1209,2013-02-26 22:07:45,"@schlamar That would be one option. I actually ended up using pycurl, the python bindings for libcurl.
",maxbane,schlamar
1209,2013-02-26 23:27:17,"@maxbane consider yourself on a list :-P
",sigmavirus24,maxbane
1209,2013-03-04 07:49:32,"@Lukasa urllib3 seems to be fixed with https://github.com/shazow/urllib3/pull/139. But requests on top of it is still not doing the certificate verification. See my comment (which includes a fully automated test): https://github.com/shazow/urllib3/pull/139#issuecomment-14367341 Any idea?

@maxbane Mind if you run the test mentioned above on your setup? :)
",schlamar,maxbane
1209,2013-03-04 07:49:32,"@Lukasa urllib3 seems to be fixed with https://github.com/shazow/urllib3/pull/139. But requests on top of it is still not doing the certificate verification. See my comment (which includes a fully automated test): https://github.com/shazow/urllib3/pull/139#issuecomment-14367341 Any idea?

@maxbane Mind if you run the test mentioned above on your setup? :)
",schlamar,Lukasa
1209,2013-03-04 08:28:50,"@Lukasa Found the issue, see my changes at https://github.com/schlamar/requests/tree/new-urllib3-api.
",schlamar,Lukasa
1209,2013-07-19 08:02:11,"@maxbane @ciphercast There is a requests HTTPS proxy ready version to test, can you give it a try: https://github.com/shazow/urllib3/pull/170#issuecomment-21234629

@sigmavirus24 Anyone else on your list? :)
",schlamar,maxbane
1209,2013-07-19 08:02:11,"@maxbane @ciphercast There is a requests HTTPS proxy ready version to test, can you give it a try: https://github.com/shazow/urllib3/pull/170#issuecomment-21234629

@sigmavirus24 Anyone else on your list? :)
",schlamar,sigmavirus24
1208,2013-02-27 16:10:28,"@Wilfred, I'm not sure why you submitted a pull request. Also I'm not sure why this issue stayed open when there were two sufficient solutions for fixing this.
",sigmavirus24,Wilfred
1208,2013-02-27 16:34:14,"@sigmavirus24 The two solutions given are not documented anywhere. It would be nice to at least document how to do this.

That being said, it's a shame that a simple GET to a URL becomes ~5 lines of code in requests v1.0, when it was just one line before.
",Wilfred,sigmavirus24
1204,2013-02-20 16:03:38,"@t-8ch this is probably just due to not sending 'CONNECT'
",sigmavirus24,t-8ch
1204,2013-02-20 21:20:16,"@eddie-dunn: To be clear, how are you passing the proxy information to Requests? Keyword argument or environment variable?
",Lukasa,eddie-dunn
1204,2013-02-21 09:47:48,"Excellent, that demonstrates the bug.

As @t-8ch and @sigmavirus24 pointed out, we currently cannot use HTTPS over proxies because we do not support the CONNECT verb. This is a known bug, and we're waiting on urllib3 before we can resolve it.
",Lukasa,t-8ch
1204,2013-02-21 09:47:48,"Excellent, that demonstrates the bug.

As @t-8ch and @sigmavirus24 pointed out, we currently cannot use HTTPS over proxies because we do not support the CONNECT verb. This is a known bug, and we're waiting on urllib3 before we can resolve it.
",Lukasa,sigmavirus24
1202,2013-10-13 14:35:06,"Was your gevent grabed by `pip`?
@oargon 
This bug probably was result from the low version of the gevent.
You can refer this: http://jesiah.net/post/59711878434/gevent-problems-gevent-dns-dnserror
",liushuaikobe,oargon
1198,2013-04-09 09:49:14,"@Lukasa, I'm not sure you do need to consider that -- Kenneth said [here](https://github.com/kennethreitz/requests/pull/1219) that Requests explicitly shouldn't support retries as part of its API.
",benhoyt,Lukasa
1198,2013-04-09 13:15:06,"Right, but there's no way to prevent a user from actually doing so.

My plan, for the record, is to traverse as far downwards as possible to the lowest level exception and use that instead. The problem with @benhoyt 's example is that it seems the socket error exception is unavailable to us. (Just by looking at what he has pasted. I haven't tried to reproduce it yet and play with it.)
",sigmavirus24,benhoyt
1198,2013-04-09 13:18:28,"@gabor 's example actually makes this easy to reproduce. Catching the exception that's raised, I did the following:



So the best we could do is only use the message stored in `e.args[0].args[0]` which could potentially be confusing as well, but probably less so than what @benhoyt encountered. Either way, we will not parse error messages to try to get more or less details because that would just be utter insanity.
",sigmavirus24,gabor
1198,2013-04-09 13:18:28,"@gabor 's example actually makes this easy to reproduce. Catching the exception that's raised, I did the following:



So the best we could do is only use the message stored in `e.args[0].args[0]` which could potentially be confusing as well, but probably less so than what @benhoyt encountered. Either way, we will not parse error messages to try to get more or less details because that would just be utter insanity.
",sigmavirus24,benhoyt
1198,2013-04-09 23:05:47,"@sigmavirus24, I agree string parsing in exceptions is a terrible idea. However, urllib3's MaxRetryError already exposes a `reason` attribute which contains the underlying exception (see [source code](https://github.com/kennethreitz/requests/blob/master/requests/packages/urllib3/exceptions.py#L38)). So you can get what you want with `e.args[0].reason`.

So continuing with the example above, `e.args[0].reason` is an instance of `socket.error`:


",benhoyt,sigmavirus24
1198,2013-04-10 01:53:35,"Nice catch @benhoyt. I'm not as familiar with urllib3 as I would like to be. 
",sigmavirus24,benhoyt
1198,2013-04-10 21:05:45,"@piotr-dobrogost, the main problem (for me) was the fact that it talks about ""max retries exceeded"", when there's no retrying involved at all. At first I thought it was the web service I was using saying that, so I contacted them. Then, digging further, I discovered this was a urllib3 quirk. So you can see the confusion.
",benhoyt,piotr-dobrogost
1198,2014-02-08 13:18:06,"@Lukasa thanks, I'm refreshing myself on the backlog here. If I get a chance to dive in I will definitely reach out.
",ksnavely,Lukasa
1198,2014-02-19 11:54:42,"@shazow great, I'd be game to if I can find the cycles. I'll ping if I have anything.
",ksnavely,shazow
1198,2016-02-22 11:12:12,"@SiddheshS This issue was fixed by rewording some exceptions: it has nothing to do with the actual connection refused error. To ask for help with a problem you should consider using [Stack Overflow](https://stackoverflow.com).
",Lukasa,SiddheshS
1198,2016-05-03 07:43:09,"@nkjulia The connection attempt is timing out, which suggests that the remote server is overloaded or that your connection timeout is too low.
",Lukasa,nkjulia
1195,2013-02-14 14:01:10,"@Lukasa 

What's wrong with designating one branch, where development of new features could be halted without hindering development in other branches?
",piotr-dobrogost,Lukasa
1193,2013-02-17 22:55:30,"So it turns out that there is no simple way to get around this Syntax Error. `six` has a good solution, but we currently don't vendor it in. The way I see it, we have three options:
1. Use the version of `six.py` in urllib3. This is bad: `six.py` is an implementation detail in urllib3, which is itself an implementation detail of Requests. -1
2. Add the bit of `six.py` that we need to our `compat.py`. This has a minimal effect on our code, but is a bit weird. +0
3. Vendor in a copy of `six.py`. This is the cleanest, but adds a whole new Python file from which we only need one line. Probably a small perf hit too. +0

@kennethreitz, @sigmavirus24: I don't know which is best from options 2 and 3. Thoughts?
",Lukasa,sigmavirus24
1192,2013-02-14 19:33:21,"@t-8ch: Unfortunately, changes to the API are not allowed in minor version bumps, and we're not going to go to v2.0.0 for quite a while. Additionally, I would argue that the [feature freeze](http://docs.python-requests.org/en/latest/dev/todo/#feature-freeze) means that the API is essentially frozen as well.
",Lukasa,t-8ch
1192,2013-02-14 19:51:43,"@t-8ch: Requests' feature freeze is perpetual, there is no plan to exit the freeze. =) Your arguments are totally understandable, but the API is unlikely to be changed in any way from now on, and certainly not as major a change as that one.
",Lukasa,t-8ch
1192,2013-02-15 00:02:10,"FWIW, I believe that HTTPS proxies will work once fine once https://github.com/shazow/urllib3/pull/139 is merged. cc @t-8ch 
",wolever,t-8ch
1192,2013-02-15 16:09:30,"@wolever I don't doubt this! I only doubt the commonness of the need for two distinct proxies. Anyways I'll shut up now as I totally understand the decision.
",t-8ch,wolever
1191,2013-02-14 22:44:16,"@nathankot thanks for catching this. Could you try with the latest commit added to this PR?
",sigmavirus24,nathankot
1191,2013-02-15 00:12:02,"@sigmavirus24 looks good :)
",nathankot,sigmavirus24
1189,2013-02-14 03:15:22,"@Lukasa you can also try sigmavirus24/requests @fix1189
",sigmavirus24,Lukasa
1189,2013-02-17 01:17:07,"@gazpachoking, you're quite right, this has been resolved. Thanks guys! 
",Lukasa,gazpachoking
1188,2013-02-14 00:25:16,"@Lukasa In that case an update to the documentation is warranted—the front page of the project says “thread-safe” which I assumed meant, you know, the whole project. :)
",brandon-rhodes,Lukasa
1188,2013-02-14 04:18:31,"@brandon-rhodes has a point. I also have to wonder why we mask those exceptions.
",sigmavirus24,brandon-rhodes
1188,2013-02-14 05:12:22,"Actually, looking at this, the only way this could happen is if `PoolManager.clear()` gets called. We only do that from the `HTTPAdapter.close()` method, and we only hit _that_ if you call `Session.close()`. @brandon-rhodes, do you ever call `Session.close()` or use the Session as a context manager?
",Lukasa,brandon-rhodes
1188,2013-04-01 18:32:50,"@brandon-rhodes any updates? Changes made to address this (in part) were included in 1.2.0
",sigmavirus24,brandon-rhodes
1188,2014-10-05 16:44:33,"Many thanks @ddoskind!
",kennethreitz,ddoskind
1187,2013-02-14 03:58:54,"@kennethreitz we could maintain backwards compatibility by keeping the old exceptions and sub-classing them for the new ones. It's ugly and I don't like it, but we would be able to raise ""properly"" named exceptions whilst maintaining backwards compatibility.
",sigmavirus24,kennethreitz
1187,2013-02-14 19:00:58,"@untitaker the only catch is that the error strings for these errors use ""Schema"" and should be changed to ""Scheme"" to be consistent, which is why I suggested sub-classing the originals.
",sigmavirus24,untitaker
1186,2013-02-13 19:01:36,"Guess I'll be working on MultiDict support this weekend. I'm going to close this in deference to #1155 though, if that's ok with you @mattboehm 

**Note** The issue is not yet fixed.
",sigmavirus24,mattboehm
1183,2013-02-11 21:40:52,"@piotr-dobrogost, this is about not allowing redirects in certain cases. That is about modifying things passed via the `params` parameter because they only get `PreparedRequest`s.

I'm inclined to say that the `response` hook should be called on redirects. The initial response should be assuming that there isn't a redirect in that first request. I think we expect a 401 upon which we authenticate. With that we get a response with a request attribute. That request attribute has the original hooks. I'll take a look at how we re-send the request with the proper authentication.
",sigmavirus24,piotr-dobrogost
1183,2013-02-12 17:35:52,"So @mkomitee, I've thought about this some more. Where are you concerned about redirects? In the initial request? If so, I understand your concern. If you're instead concerned about redirects after the 401 has been handled, you can prevent those yourself. You have access over the response at that point I believe.
",sigmavirus24,mkomitee
1183,2013-02-12 17:51:04,"I'm concerned about it in the initial request.

On Tue, Feb 12, 2013 at 12:36 PM, Ian Cordasco notifications@github.com
wrote:

> ## So @mkomitee, I've thought about this some more. Where are you concerned about redirects? In the initial request? If so, I understand your concern. If you're instead concerned about redirects after the 401 has been handled, you can prevent those yourself. You have access over the response at that point I believe.
> 
> Reply to this email directly or view it on GitHub:
> https://github.com/kennethreitz/requests/issues/1183#issuecomment-13444844
",mkomitee,mkomitee
1183,2013-02-12 18:41:56,"Ah, I see what you mean. We resolve redirects before dispatching hooks. If that came before the redirect resolution, that would mean you could catch the redirect, correct?

Semantically I think it should be there anyway. If a redirect is resolved, every request except the first has the hook called on it (because each subsequent request calls `send` with `allow_redirects=False`). It should before the resolution unless @kennethreitz or @Lukasa disagree.
",sigmavirus24,Lukasa
1183,2013-02-12 23:22:11,"@mkomitee I have no real opinion about forcing Auth providers to accept all of those parameters, but I doubt it will be accepted. On that topic though, `allow_redirects` should be handled by the Session without a doubt. The Auth providers should have no reason to worry about that. I can understand needing the `proxies`, `timeout`, `stream` and `cert` parameters, but I thoroughly disagree that it is the responsibility of the auth provider to handle  redirects. With that said (twice), there shouldn't be a need to send the `Session` object itself.
",sigmavirus24,mkomitee
1183,2013-02-13 01:45:30,"If authentication is required for the redirect, the 401 goes to the auth provider and there's no way back to the redirect handling code. That's all out of scope for this issue though.

On Tue, Feb 12, 2013 at 6:22 PM, Ian Cordasco notifications@github.com
wrote:

> ## @mkomitee I have no real opinion about forcing Auth providers to accept all of those parameters, but I doubt it will be accepted. On that topic though, `allow_redirects` should be handled by the Session without a doubt. The Auth providers should have no reason to worry about that. I can understand needing the `proxies`, `timeout`, `stream` and `cert` parameters, but I thoroughly disagree that it is the responsibility of the auth provider to handle  redirects. With that said (twice), there shouldn't be a need to send the `Session` object itself.
> 
> Reply to this email directly or view it on GitHub:
> https://github.com/kennethreitz/requests/issues/1183#issuecomment-13465713
",mkomitee,mkomitee
1182,2013-02-12 04:13:22,"@Miarevo I'm guessing it was either a change in our proxy handling or a change in urllib3's. Nice and specific eh? :-P Frankly though, @Lukasa has been more engaged in the proxy work lately so he might have an idea.
",sigmavirus24,Miarevo
1182,2013-02-12 04:30:46,"@Miarevo: You did, you just didn't notice. =) If you don't provide a scheme, Requests guesses at what it should be. For HTTP proxies, it guesses it should be 'http', and for HTTPS proxies it guesses it should be 'https'.
",Lukasa,Miarevo
1182,2013-02-12 04:43:36,"@Lukasa thanks for reminding me about that issue. I would love that. I could get stuff working on PythonAnywhere then. :)
",sigmavirus24,Lukasa
1182,2013-02-13 16:30:36,"@t-8ch we do very little work with proxies. If I remember our code correctly, we just pass it to urllib3 and they handle all of that for us. I guess we _could_ validate it before passing it to urllib3 though. I'll defer to @kennethreitz and @Lukasa about whether we should be doing that or not though.
",sigmavirus24,t-8ch
1182,2013-02-13 16:30:36,"@t-8ch we do very little work with proxies. If I remember our code correctly, we just pass it to urllib3 and they handle all of that for us. I guess we _could_ validate it before passing it to urllib3 though. I'll defer to @kennethreitz and @Lukasa about whether we should be doing that or not though.
",sigmavirus24,Lukasa
1182,2013-02-13 21:44:56,"I think @t-8ch is probably right. It's a quick fix, I'll open a PR and see what Kenneth thinks.
",Lukasa,t-8ch
1181,2013-02-12 06:52:53,"@Lukasa
Thanks! Please check the fix.
",denis-ryzhkov,Lukasa
1181,2013-02-12 09:08:03,"@kennethreitz

`latin1` (aka ISO-8859-1) is recommended for header **values** by the very HTTP/1.1 spec
http://tools.ietf.org/html/rfc2616#section-4.2
- Header **name** is always `ascii`:


- Header **value** is `latin1` by default, but may be any other charset too:



This RFC is MIME for headers: http://tools.ietf.org/html/rfc2047

Example of encoded value:



It can be decoded with:
http://docs.python.org/2/library/email.header.html#email.header.decode_header

So it's a bit more complex than just `latin1`.

I guess it is a good idea to merge the fix for header **name** first,
and then return to header **value** issues.
",denis-ryzhkov,kennethreitz
1177,2013-02-09 22:57:01,"@shezi would you like to make the changes you and @piotr-dobrogost mentioned and push to the same branch? This sounds like a good pull IMO.
",sigmavirus24,piotr-dobrogost
1177,2013-02-09 22:57:01,"@shezi would you like to make the changes you and @piotr-dobrogost mentioned and push to the same branch? This sounds like a good pull IMO.
",sigmavirus24,shezi
1177,2013-02-10 11:44:35,"@shezi
You raise a valid point. If we use `builtin_str` then we could left the `0` string literal as is, indeed. As to using `bytes` in Python 3 I believe we should set a boundary after which all headers are bytes - see my comment at https://github.com/kennethreitz/requests/pull/1116#issuecomment-12621940 However this hasn't been decided, yet.
",piotr-dobrogost,shezi
1177,2013-02-10 12:03:56,"@piotr-dobrogost That's what I had thought. However, I turned the string literal into a conversion to mark it for future edits: if at one point the decision is made to convert to `bytes` here, it should be more clear that all three calls must be converted.

(sorry for using the wrong name above!)
",shezi,piotr-dobrogost
1177,2013-02-11 21:36:50,"> thus the sent body will be converted to unicode during concatenation by urllib, producing a UnicodeDecodeError when sending binary data

@shezi
How come is path concatenated with the body? Can you point us to the exact place where it happens?
",piotr-dobrogost,shezi
1177,2013-02-11 23:00:39,"You can see that here:



That's the last line of the error stack from above. As @Lukasa pointed out, in the end the whole request has to be serialized as a string. It doesn't matter that there are headers in between, because if you do `unicode + str` the result will be `unicode`. So as long as there is at least one `unicode` element before the message body, the message body will have to be converted to `unicode` too. Basically, a single `unicode` element in URL, fragment, query string, headers or body ""infects"" all other elements.

In any case, the specific problem I had can be solved in a client library, so that's what I did and submitted a pull request there.
",shezi,Lukasa
1177,2013-02-12 09:12:35,"@shezi
Thanks for explanation and sorry I didn't notice this myself in the stacktrace.
What about Python 3?
",piotr-dobrogost,shezi
1177,2013-02-12 09:25:00,"@piotr-dobrogost No worries, it took me some time, too, to figure it out. =) I haven't tried it in Python3.
",shezi,piotr-dobrogost
1176,2013-02-09 23:52:11,"

**Verify return code: 9 (certificate is not yet valid)**

Thanks @t-8ch!  

The time was wrong, after setting up ntp everything works.
",athoik,t-8ch
1176,2013-02-10 00:14:33,"Thanks @t-8ch for helping @athoik clear this up.
",sigmavirus24,athoik
1176,2013-02-10 00:14:33,"Thanks @t-8ch for helping @athoik clear this up.
",sigmavirus24,t-8ch
1174,2013-02-08 20:07:09,"@nikolay pre-request hooks were removed in the refactor. An (albeit ugly) way of doing this would be to use `allow_redirects=False` and manage it yourself, but as we're not accepting new [features](https://github.com/kennethreitz/requests/issues/1165), it is unlikely either hooks will come back or sending a referrer header will be accepted. The rest of your changes look great though. Thank you for them! :+1:
",sigmavirus24,nikolay
1174,2013-02-09 22:59:58,"@nikolay would you rather I remove the referrer tracking or would you be willing to do it? Kenneth mentioned liking the rest of the changes and his desire to merge the pull for those changes. If not, just let me know.
",sigmavirus24,nikolay
1173,2013-02-10 22:19:10,"Rebased. And @kennethreitz I reconstruct a `PreparedRequest` because previously the original was what we made the `request` attribute on the `Response` object. This is just to keep backwards compatibility and not surprise anyone relying on that.
",sigmavirus24,kennethreitz
1172,2013-02-07 22:09:53,"This seems entirely pointless.

@grzn sorry if that seems blunt, but I already made my intentions clear in the other conversation we had :)
",kennethreitz,grzn
1171,2013-02-08 09:16:34,"Uh, hang on. Two different users have posted in this issue. The traceback in the original post does not indicate what the URL was, so I can't tell whether this is one issue or two. @norey @t-8ch, are you two talking about the same problem, and if so, how on earth do you know? :confused: 
",Lukasa,norey
1171,2013-02-08 09:16:34,"Uh, hang on. Two different users have posted in this issue. The traceback in the original post does not indicate what the URL was, so I can't tell whether this is one issue or two. @norey @t-8ch, are you two talking about the same problem, and if so, how on earth do you know? :confused: 
",Lukasa,t-8ch
1171,2013-02-08 09:17:47,"In fact, rereading this, there's simply no way these are the same issue. @norey claims that **any** URL fails, which is clearly not a header related problem. @t-8ch, can you please open a new issue instead of hijacking this one? =)
",Lukasa,norey
1171,2013-02-08 09:17:47,"In fact, rereading this, there's simply no way these are the same issue. @norey claims that **any** URL fails, which is clearly not a header related problem. @t-8ch, can you please open a new issue instead of hijacking this one? =)
",Lukasa,t-8ch
1171,2013-02-08 09:28:20,"Following up on @norey's problem:

I can't reproduce this. I'm running Windows 7 x64, Python 3.3 (64-bit version), and can quite happily send get requests to Google.
",Lukasa,norey
1171,2013-02-08 09:44:16,"I just tried requesting Google and some other URLs, and it worked fine. But that URL still gives me the same error... I don't get the reason especially when @sigmavirus24 didn't have any issues.
",norey,sigmavirus24
1171,2013-02-08 09:47:05,"@norey: Just to be totally clear, what exact URL is causing you problems?
",Lukasa,norey
1171,2013-02-08 09:51:51,"@Lukasa the one shown in the paste I posted.

I just tried it on Python2.7.3 and it works !! but not on Python3.3 !! 
Looks like something FUBARed at my end but can't seem to locate it.
",norey,Lukasa
1171,2013-02-08 10:40:51,"I get the same output ( @Lukasa's output of 3.3)  on my Linux box running Python 3.2.3

So seems that it's an issue in Python 3, right ?
",norey,Lukasa
1171,2013-02-08 13:03:27,"Right, so @norey, your exception seems to be specific to you. @sigmavirus24 and I are getting the ""can't parse headers"" issue.
",Lukasa,norey
1171,2013-02-08 13:03:27,"Right, so @norey, your exception seems to be specific to you. @sigmavirus24 and I are getting the ""can't parse headers"" issue.
",Lukasa,sigmavirus24
1171,2013-02-08 17:10:39,"The root of the problem lies in `email.feedparser.FeedParser._feedgen()` where it aborts on the first non-header line.
If a line is a header is determined by matching a regex (`email.feedparser.headerRE`), which by default doesn't allow spaces.

If you change this regex from



to something like the following it should work.



This snippet would allow spaces in headers:



@norey
This is a workaround, not a bugfix.
The whole problem is caused by the server not sending conformant HTTP responses.
Requests and python do the right thing.
Anyway, I hope this helps you.
",t-8ch,norey
1171,2013-02-08 20:02:24,"@t-8ch if this isn't standards compliant report it upstream at bugs.python.org please. You seem to have a good understanding of what is happening.

On a separate topic, @norey, you said this happens with any URL you use. Were you exaggerating or were you being serious? The reason I ask is because using requests on python 3.3 works perfectly fine for me.
",sigmavirus24,norey
1171,2013-02-08 20:02:24,"@t-8ch if this isn't standards compliant report it upstream at bugs.python.org please. You seem to have a good understanding of what is happening.

On a separate topic, @norey, you said this happens with any URL you use. Were you exaggerating or were you being serious? The reason I ask is because using requests on python 3.3 works perfectly fine for me.
",sigmavirus24,t-8ch
1171,2013-02-08 20:39:12,"@sigmavirus24 I actually tried with a couple of URLs not everything, so I just assumed. But I tried again on Google and it works fine the other URL that gives the exception is ( it is the same server I guess, giving that crap)



Ok now, let's disregard its behavior and concentrate on the exception I am getting, what is its reason? (it is in my first post). 
",norey,sigmavirus24
1171,2013-02-08 22:42:41,"@norey



What I did notice is that for that website, the headers dictionary was completely empty. That is horribly uncharacteristic.

It seems like every website you're looking at does something wacky with their headers. I'm +1 on closing this issue right now, but if you can find a different URL that also causes this error for you, I don't mind keeping it open.
",sigmavirus24,norey
1171,2013-02-09 01:19:48,"@norey I didn't mean you should close this, just that the evidence so far is that some people don't know how to run servers. If you can find other sites that do this, feel free to contact @t-8ch or myself via email, or just post it back here.
",sigmavirus24,norey
1171,2013-02-09 01:19:48,"@norey I didn't mean you should close this, just that the evidence so far is that some people don't know how to run servers. If you can find other sites that do this, feel free to contact @t-8ch or myself via email, or just post it back here.
",sigmavirus24,t-8ch
1170,2013-02-07 20:21:08,"@grzn, You can do that yourself.


",sigmavirus24,grzn
1170,2013-02-07 20:37:33,"@kennethreitz but some things are much harder/tricky to change after you've prepared the requests, and even so the way to modify is to overload the send method and modify the argument it receives. that's ugly

@sigmavirus24 yes you can do that yourself by inheriting the Session class and replacing the request method with a similar functionality, but thats also ugly -- this method is quite long and does a number of things and I don't duplicate the whole damn thing.

all i'm asking it to take the 10 lines that create the requests before 'preparing it' and extract it to a separate method. overloading such method would be the equivalent for the old pre_request hook

and also, some documentation on this in the hooks page would help :)
",grzn,kennethreitz
1170,2013-02-07 20:37:33,"@kennethreitz but some things are much harder/tricky to change after you've prepared the requests, and even so the way to modify is to overload the send method and modify the argument it receives. that's ugly

@sigmavirus24 yes you can do that yourself by inheriting the Session class and replacing the request method with a similar functionality, but thats also ugly -- this method is quite long and does a number of things and I don't duplicate the whole damn thing.

all i'm asking it to take the 10 lines that create the requests before 'preparing it' and extract it to a separate method. overloading such method would be the equivalent for the old pre_request hook

and also, some documentation on this in the hooks page would help :)
",grzn,sigmavirus24
1166,2013-02-07 15:30:30,"Btw, my comment was not meant as a way of saying ""Pick me because I'll do the work!"", more of a, there are a couple different ways of doing it. I've helped @kanzure by just reading over kanzure/python-requestions a couple times and I like it, but I'm also quite familiar using mock directly. Testing redirects would be fairly ugly. If we were to do something like github3.py does it, we could have a method to craft the Response objects as the `return_value` on the mock and if we patched on the Adapter level, we would have to use mock's ability to have multiple return_values in succession. I have to re-read the docs to see how that's done though. Just an example of one of the slightly more challenging tests we might have to consider.
",sigmavirus24,kanzure
1166,2013-03-04 21:51:03,"@schlamar if I remember correctly, this has been brought up before. Look up CherryPy or Twisted on the issue tracker. We could easily mock responses and test the like. It isn't that difficult. @kanzure and I do the same for our projects.
",sigmavirus24,kanzure
1166,2013-03-04 21:51:03,"@schlamar if I remember correctly, this has been brought up before. Look up CherryPy or Twisted on the issue tracker. We could easily mock responses and test the like. It isn't that difficult. @kanzure and I do the same for our projects.
",sigmavirus24,schlamar
1166,2013-07-20 21:07:59,"@sigmavirus24, well, while you're at it, please deprecate my requestions library. I made some poor decisions in its implementation that I think you can just copy. I know you're probably just copying the vcr gem from ruby, but maybe you can add in those decorators for mocking out tests etc.
",kanzure,sigmavirus24
1165,2013-02-06 19:00:15,"@kennethreitz Can you elaborate on what you want contributing.md to be, and how it should relate to the current dev docs?
",Lukasa,kennethreitz
1163,2013-02-05 15:07:42,"@piotr-dobrogost is correct, it is the default. @zwass, are you okay with closing this issue?
",sigmavirus24,piotr-dobrogost
1163,2013-02-05 15:07:42,"@piotr-dobrogost is correct, it is the default. @zwass, are you okay with closing this issue?
",sigmavirus24,zwass
1163,2013-02-05 15:26:20,"@zwass you want to use `raise_for_status`. That was never the purpose of `danger_mode`. `danger_mode` simply suppressed underlying exceptions during the connection or cert validation, basically anything that sub-classed `requests.exceptions.RequestException`. If you look at the end of the page [here](http://docs.python-requests.org/en/v0.14.2/user/quickstart/?highlight=safe_mode) then you might understand it better.
",sigmavirus24,zwass
1163,2013-02-05 17:06:53,"@sigmavirus24 according to that documentation (and the pull request), `danger_mode` caused `raise_for_status` to be called at the end of any request. If this were the default behavior in 1.1.0, my example above would have thrown an `HTTPError: 404 Client Error: Not Found`.

It seems that the default behavior in 1.1.0 is `safe_mode`, but there is no ability to use the equivalent of `danger_mode`.
",zwass,sigmavirus24
1163,2013-02-05 17:25:48,"That's equivalent to re-adding part of the configuration API. It's also a new 
feature and as @kennethreitz mentioned, we aren't adding new features.
",sigmavirus24,kennethreitz
1161,2013-02-08 03:42:56,"@kennethreitz, PreparedRequests don't use the CaseInsensitiveDict currently. If you would like it to, let me know.
",sigmavirus24,kennethreitz
1159,2013-02-01 09:36:57,"@theaeolianmachine is right, Content-Length is set when you prepare the request based on the body. This is reasonable, Requests shouldn't let you set this incorrectly unless you _really_ want to.

However, it sounds like you do. With that in mind, you can interfere with this process in a few ways. The easiest way is likely to be to subclass the `HTTPConnectionAdapter` like so:



Then attach your new adapter as follows:



Any requests through that session to your nginx host should now have their Content-Length forcibly set to the empty string. If you want more intelligent behaviour, note that the `send` method takes a `PreparedRequest`, so any data available on a `PreparedRequest` is available to you to modify your behaviour.

NB: I have not tested the above code, I just wrote it in the browser. There may be typos or some minor syntax errors. In general, however, it should work.
",Lukasa,theaeolianmachine
1159,2013-02-01 15:28:28,"also @mtourne, you should look into PreparedRequest. It's built for exactly this.
",kennethreitz,mtourne
1159,2013-02-01 16:50:31,"@kennethreitz I'll check why we set it if they set it themselves.
",sigmavirus24,kennethreitz
1159,2013-02-09 03:18:19,"@mtourne I for one really think the PreparedRequest is what you should be using in this instance the more I think about it.



It's a bit of extra code, but it isn't too bad and it gives you far more control over the request to mess with as you please.
",sigmavirus24,mtourne
1155,2013-02-15 22:41:17,"@xealot yeah, that was my opinion. I have something working currently, but it so minimal that I'm not sure it is covers all the corner cases. .... I'll be pushing it to a branch in my repo tonight.
",sigmavirus24,xealot
1155,2013-02-19 15:40:00,"https://github.com/sigmavirus24/requests/tree/multidict has the first version of the MultiDict. (I forgot to update this issue a couple days ago). I haven't switched any of the internal logic over to using it, but it observes order. @dmckeone perhaps you could explain the importance of `multi=True`. Maybe it's because I haven't had my coffee yet, but this doesn't look at all obvious as to the reasoning to me. Then again, I haven't used Werkzeug's MultiDict and should probably go read those docs.
",sigmavirus24,dmckeone
1155,2013-02-20 14:33:15,"@sigmavirus24: The `multi=True` argument allows not just ordered items in the dictionary, but also multiple keys in the dictionary.  If the `multi=True` argument is not passed, then all duplicate keys are removed when iterating over the the keys/items/values.  It's required for me because I'm working with a legacy system that requires ordered, duplicate GET parmeters.  Something like: http://www.example.com/item?a=1&b=steve&a=2&b=mary&a=3&b=bob
",dmckeone,sigmavirus24
1155,2013-02-20 14:39:23,"Thanks for the concrete example @dmckeone 
",sigmavirus24,dmckeone
1155,2013-04-15 14:50:02,"@ihodes I started working on it [here](https://github.com/sigmavirus24/requests/tree/multidict) but I've been too busy to do much. It needs to be updated with upstream master and more work needs to be done with it. The tricky part is that deciding how best to utilize the multidict in the parameters to the different methods, specifically to things like `params`, `headers`, etc. The former needs to preserve order which is why we use the OrderedDict, the later isn't so much trouble but may require a different method than what we already have for accepting and merging lists of `(key, value)` tuples. Also, don't take the above as discouragement from doing this. You'll probably be able to come up with a much better solution than me and if you want code review on what you do, I'll be happy to help! :)
",sigmavirus24,ihodes
1155,2013-04-15 22:46:42,"@ihodes `OrderedMultiDict` is slower, but in practice I'm not sure that it will matter.  I may be confused about where this is needed throughout all of Requests, but for input the user will make the performance decision.  If I (the user) wish to take the performance hit because I need ordered support, then I will pass requests an `OrderedMultiDict`.  If I don't, then I'll use regular `MultiDict`.  All Requests should have to do is iterate through what the user gives it, and put it into the format it needs internally (which I think is an ordered list of 2-tuples for most things).  Since both `OrderedMultiDict` and `MultiDict` can be iterated through with something like `[(key, value) for key, value in headers.iteritems(multi=True)]` (where `headers` is an instance of `OrderedMultiDict` or `MultiDict`), Requests should be able to be indifferent about the performance implications.
",dmckeone,ihodes
1155,2013-04-16 03:13:25,"@dmckeone that'd be (sort of) ideally how it would work were we to go with this additional datastructures. One issue is that there isn't a consistent API to these structures—we'd need to explicitly dispatch on the type (because we have to pass `multi=True` if the dict is a MultiDict). That's unideal. Also, for the user, constructing these dictionaries is a PITA (in my opinion) and requires importing another datastructure in order to use. 

What if we allowed the user to input a list of (key, val) tuples instead (which works right now, but order/multiples are lost)? That way it's lightweight, and much easier to understand & use than importing and building up an OrderedMultiDict.

E.g. 



Note that the second GET request is NOT equivalent to the currently possible request: 



…because the ""yep_addn"" key/val won't be interleaved between the two ""yep"" params.

I can see in #1103 that this was requested before, but I don't know if it was agreed to be a good/bad idea? Right now, it makes the most sense to me. This almost works now, it just doesn't allow for _ordered AND repeated_ params (c.f. @dmckeone's example in #1103). 
",ihodes,dmckeone
1155,2013-04-16 13:51:19,"So the reason we want the `MultiDict`s underneath everything is so that the users don't have to concern themselves with it. They can always pass it in and conversion _should_ would just fine. We currently accept both dicts and ordered pairs as @ihodes demonstrated and both are (as far as we're concerned under the hood) converted to `OrderedDict`s. The issue is that this doesn't allow for multiple items (again as @ihodes demonstrated).

What I wonder is if we could subclass our existing `OrderedDict`s, and force them to allow multiple entries. Then with that in mind, we could probably also subclass that for the `CaseInsensitiveDict` so to return as the headers attribute on `PreparedRequest`s and `Response`s. Headers can occur several times so this is really the most crucial case where we need the implementation. It would certainly take more thought and work than simply copying flask/werkzeug's implementations, but it would also likely involve less copying (which is bothersome to me even with permission).

To be clear:
- We cannot drop the condition that they are ordered (even if sent in as a dict) because that would be a regression
- We cannot drop the ability to accept anything like above
",sigmavirus24,ihodes
1155,2013-04-16 18:17:22,"@sigmavirus24 Agree about the MultiDict.items(); it is odd, and I don't know why it's like that.  Not changing the API makes sense too.

So, currently Requests does this (Please feel free to correct me if some of this is wrong): 
- `OrderedDict` for everything, instantiated by `from_key_val_list()`
- Multi-value is supported by passing an iterable value
- `merge_kwargs` in `sessions.py` seems to rely on unique, case-independent keys  (It wouldn't concat multi-keys)

@sigmavirus24 Could you do a quick explanation why `OrderedDict` is used in place of dict?  Everything I've seen seems to not care all that much about order in the parameters.  Is this a relic of when requests used to accept a list of 2-tuples (as the docstring in `from_key_val_list()` seems to indicate)?

To me, the ideal would be to use `MultiDict` internally in `from_key_val_list()`, with some kind of session/response based switch that could use `OrderedMultiDict` in it's stead.  This would mean:
- Changing `from_key_val_list()` to interpret any non-`MultiDict` dictionaries as `MultiDict` (e.g. change `OrderedDict` to `Dict`
- Multi-value would still work, because we aren't affecting the way values are stored by using `MultiDict` (just enabling multi-key)  
- `merge_kwargs` in sessions.py would need some kind of concatenation behaviour if order mattered, and `MutliDict` would need to be case insensitive.
",dmckeone,sigmavirus24
1155,2013-04-16 18:34:21,"On a side note, I just browsed through `merge_kwargs` on GitHub and would like to ask anyone who's reading this thread to **please** rewrite the case-insensitive logic in there. It will cause a huge performance hit for a large number of `original_keys`.

Replying to @dmckeone 

Yeah I wouldn't mind going directly to `OrderedMultiDict` but like I said, it isn't necessary _everywhere_. ;)
",sigmavirus24,dmckeone
1155,2013-04-16 18:42:58,"Er, yeah, `items` (old habits…)

Okay! I'll take a stab today or tomorrow. And if anything we owe you & @kennethreitz!
",ihodes,kennethreitz
1155,2013-04-18 04:13:53,"Had some time to get [something started](https://github.com/dmckeone/requests/commit/7f8a4d4207a1618f3f2c6e7ca36976e611a1bb6f).  Other than adding all the werkzeug structures (which will need some Python 3 work), it took relatively few changes to get all the tests to pass.  Even added an additional test that is similar to @ihodes example above.

There is probably quite a bit more work to do, but since I'm not really familiar with all the ins and outs of Requests, I thought I'd just put this up as food for thought for another implementation, or for refinements on this one.
",dmckeone,ihodes
1155,2013-04-18 04:38:12,"Hah we just did the exact same thing, sans the test (though I just fiddled with the OrderedMultiDict, setting the multi param's default to `True` in `iteritems` and `items`…). Unfortunately, I messed up something with the copy-paste or something and have been in traceback hell. 

I read your changes, and they look like we all agreed on above; why not submit a PR? @sigmavirus24, could you CR this? Awesomesauce.
",ihodes,sigmavirus24
1155,2013-04-18 12:36:53,"I looked over @dmckeone changes. I might fork his branch and do some more work on it tonight because I am far better at reviewing code in vim than on GitHub. I tend to miss stuff on the site. But so far it looks fantastic. Thank you @dmckeone :cake: :cake: :beer:
",sigmavirus24,dmckeone
1155,2013-04-18 14:37:24,"Since everyone seems to like it, I don't mind keeping at it.   @sigmavirus24 if you find anything then let me know, and I'll be happy to add it in.   I'll go fix that test first.  Would you prefer I do a Pull Request after that, or continue changes on this branch as you do CR?

Are there any recommendations for doing performance profiling for Requests?  Is there a benchmark that is commonly used?

As far as Python 3 support goes, I tested it under Python 3.3 and everything seems to behave the same as 2.7.  I was mostly talking about Werkzeug's lack of Python 3 support.  For example, should we keep all the iter...() methods around in MultiDict and OrderedMultiDict?  There is a BadRequestKeyError that is currently just assigned to KeyError.  In Werkzeug that is used to indicate that a bad request was sent, but since Requests is a little different, perhaps we should change the name or throw a different exception?
",dmckeone,sigmavirus24
1155,2013-04-18 20:13:32,"> I don't mind keeping at it.

I just didn't want to overwork you :)

You can start a pull request and then as you push to it just ping me to do more code review. It'll also get @Lukasa and @kennethreitz's attention.

> Are there any recommendations for doing performance profiling for Requests?

No... but let's make one!

>  For example, should we keep all the iter...() methods around in MultiDict and OrderedMultiDict?

If I remember correctly, those handle the work for `.items` on both. I'm 100% for renaming `iteritems` to `items` though. Also for setting the default for `multi` to `True`. (Like @ihodes did)

> perhaps we should change the name or throw a different exception?

I'm okay with throwing a plain ol' `KeyError`. That seems logical and is something we could reasonable expect.

So to review, the following `request` parameters need to use an `OrderedMultiDict`:
- [x] params
- [ ] data
- [ ] files

And it would be ideal if both `request`s and `response`s use a `MultiDict` for headers, i.e., convert headers to a `MultiDict` when taking them from the user and when returning a response use a `MultiDict` to hold the headers present. I doubt any server is using a header more than once, but it's part of the spec that we can send more than one, so we may as well be safe and expect the more horribly written servers to think they can return one.

Thanks again @dmckeone :cake:
",sigmavirus24,kennethreitz
1155,2013-04-18 20:13:32,"> I don't mind keeping at it.

I just didn't want to overwork you :)

You can start a pull request and then as you push to it just ping me to do more code review. It'll also get @Lukasa and @kennethreitz's attention.

> Are there any recommendations for doing performance profiling for Requests?

No... but let's make one!

>  For example, should we keep all the iter...() methods around in MultiDict and OrderedMultiDict?

If I remember correctly, those handle the work for `.items` on both. I'm 100% for renaming `iteritems` to `items` though. Also for setting the default for `multi` to `True`. (Like @ihodes did)

> perhaps we should change the name or throw a different exception?

I'm okay with throwing a plain ol' `KeyError`. That seems logical and is something we could reasonable expect.

So to review, the following `request` parameters need to use an `OrderedMultiDict`:
- [x] params
- [ ] data
- [ ] files

And it would be ideal if both `request`s and `response`s use a `MultiDict` for headers, i.e., convert headers to a `MultiDict` when taking them from the user and when returning a response use a `MultiDict` to hold the headers present. I doubt any server is using a header more than once, but it's part of the spec that we can send more than one, so we may as well be safe and expect the more horribly written servers to think they can return one.

Thanks again @dmckeone :cake:
",sigmavirus24,Lukasa
1155,2013-04-18 20:13:32,"> I don't mind keeping at it.

I just didn't want to overwork you :)

You can start a pull request and then as you push to it just ping me to do more code review. It'll also get @Lukasa and @kennethreitz's attention.

> Are there any recommendations for doing performance profiling for Requests?

No... but let's make one!

>  For example, should we keep all the iter...() methods around in MultiDict and OrderedMultiDict?

If I remember correctly, those handle the work for `.items` on both. I'm 100% for renaming `iteritems` to `items` though. Also for setting the default for `multi` to `True`. (Like @ihodes did)

> perhaps we should change the name or throw a different exception?

I'm okay with throwing a plain ol' `KeyError`. That seems logical and is something we could reasonable expect.

So to review, the following `request` parameters need to use an `OrderedMultiDict`:
- [x] params
- [ ] data
- [ ] files

And it would be ideal if both `request`s and `response`s use a `MultiDict` for headers, i.e., convert headers to a `MultiDict` when taking them from the user and when returning a response use a `MultiDict` to hold the headers present. I doubt any server is using a header more than once, but it's part of the spec that we can send more than one, so we may as well be safe and expect the more horribly written servers to think they can return one.

Thanks again @dmckeone :cake:
",sigmavirus24,ihodes
1155,2013-04-18 20:13:32,"> I don't mind keeping at it.

I just didn't want to overwork you :)

You can start a pull request and then as you push to it just ping me to do more code review. It'll also get @Lukasa and @kennethreitz's attention.

> Are there any recommendations for doing performance profiling for Requests?

No... but let's make one!

>  For example, should we keep all the iter...() methods around in MultiDict and OrderedMultiDict?

If I remember correctly, those handle the work for `.items` on both. I'm 100% for renaming `iteritems` to `items` though. Also for setting the default for `multi` to `True`. (Like @ihodes did)

> perhaps we should change the name or throw a different exception?

I'm okay with throwing a plain ol' `KeyError`. That seems logical and is something we could reasonable expect.

So to review, the following `request` parameters need to use an `OrderedMultiDict`:
- [x] params
- [ ] data
- [ ] files

And it would be ideal if both `request`s and `response`s use a `MultiDict` for headers, i.e., convert headers to a `MultiDict` when taking them from the user and when returning a response use a `MultiDict` to hold the headers present. I doubt any server is using a header more than once, but it's part of the spec that we can send more than one, so we may as well be safe and expect the more horribly written servers to think they can return one.

Thanks again @dmckeone :cake:
",sigmavirus24,dmckeone
1155,2013-04-18 20:24:24,"> I'd add that headers should all be case-insensitive. Whether or not you 
> force `lower()` on all keys or maintain a separate case-insensitive key 
> lookup dictionary (as is currently done) is probably worth thinking about 
> (the spec says headers are case-insensitive, but if they're passed with 
> certain capitalization, should we just leave them be?)

Yep, I had completely forgotten that. 

> ALSO, unfortunately, header order DOES matter (for headers with the same name), per the spec. So, we need a CaseInsentitiveOrderedMultiDict (:cry:). 

And here's why I need to go and read all the specs before ever speaking again.  
:-P

> I'm willing to step in here and work on something, but let's figure out some 
> sort of division of labor so we don't write the same code again :) But if 
> you're happy to continue on this feature, I'm happy to let you & work on a 
> different issue!

You two can work this out. There are also plenty of issues I'm sure you could 
help with @ihodes. If you want guidance with any of them, let me know. I'll be 
happy to point you in the generally correct direction.
",sigmavirus24,ihodes
1155,2013-04-18 21:32:33,"I'll continue on with what I have for params, data and files.  I'll do the Pull Request and make sure that data and files are supported as well.  

Is requests attempting to have any kind of parity with werkzeug's structures?  While this was a good proof-of-concept to see if MultiDict would work, I don't think its a good idea to change the internal structure of them too much (flipping the default, for example).  Ideally, one day these structures will be in a common place that both Requests and Werkzeug could use (alluded to by @kennethreitz's [blog post](http://kennethreitz.org/exposures/the-future-of-python-http)).  I know that from my perspective it would be great if I could pass a Werkzeug `MultiDict` and have it work in Requests as-is.  That way the meaning of `MultiDict` isn't fragmented, and we can end up with a de-facto standard `MultiDict`.

When it comes to the implementation of the headers, I'm less sure.  I can see the use case for params and data, and I can take a look at files because I can envision it being the same, but I'm not sure I want to change it for headers.  Does the current implementation not work in some way that `MultiDict` would solve?  Implementing this `CaseInsensitiveOrderedMultiDict` just seems like we'd be diverting too far from simple, and too far from existing structures, and doing so for very little benefit.  So if someone else who knows why this would be important wants to implement it, I'm happy to leave it to them.
",dmckeone,kennethreitz
1155,2013-04-19 18:57:16,"@sigmavirus24 I added the pull request and came up with a blend that I think gets both of us what we want.  I set all the `multi=False` kwargs to be `multi=True`. However, I don't think it will be too big of an issue for users, because, unless I missed something, the `MultiDict`s are mostly internal to how Requests processes sessions.  The only way you can ""see"" them is by interrogating body in the prepared request afterwards.  `params=` is the exception, because you can set it on the `Session`.

Additionally, in all places that take in a `MultiDict`, I removed all explicit checks for `isinstance(d, MultiDict)`, and replaced it with a `try: except TypeError:` or a check for the needed attribute, `iterlists`.

I also added a couple extra tests to prove the ordering in `data=` and `files=`
",dmckeone,sigmavirus24
1155,2013-04-21 17:23:43,"@Lukasa I took a look through and I believe that my changes to `params=`, `data=`, and `files=` are all safe from an API perspective:  
- `PreparedRequest` does not expose `params`, `data` or `files`
- `Session` has a `params` attribute that I initialize to OrderedMultiDict, but it is hidden because it isn't declared in the `__attrs__`
-  `Request` has `params`, `data`, and `files` exposed, but the user can't get to that because of `__attrs__`

In the future, I don't think there is any way around an API change for `headers` (which is why I shied away from that change), because a `dict`, or in this case a `CaseInsensitiveDict`, just can't represent multiple keys.  So that implementation will require some thinking about the API, and hopefully a structure more dedicated to handling HTTP Headers;  maybe just a `dict`-like class called `HTTPHeader` that has the behaviour of the earlier mentioned `CaseInsensitiveOrderedMultiDict`, but is clearly bound to conform to HTTP Header behaviours and rules.  After all, `CaseInsensitiveDict` is really only used for headers anyway; perhaps it's best to call a spade a spade?
",dmckeone,Lukasa
1155,2013-04-21 19:22:23,"@dmckeone I think either you or I am misunderstanding the role of `__attrs__`. As far as I know, it does not hide any information from the user but presents to libraries like `multiprocessing` and `pickle` what they need to properly serialize and reconstruct those objects. So, yes, in fact `params` is entirely exposed to the user. There would be no other point in having it. :)

These were all planned and endorsed changes by @kennethreitz. And to a degree they're quite necessary to make requests entirely functional. Leaving `headers` as is, right now, seems okay to me. I'm just not sure how well having the `Session.headers` attribute as a `MultiDict` will work.
",sigmavirus24,kennethreitz
1155,2013-04-21 19:22:23,"@dmckeone I think either you or I am misunderstanding the role of `__attrs__`. As far as I know, it does not hide any information from the user but presents to libraries like `multiprocessing` and `pickle` what they need to properly serialize and reconstruct those objects. So, yes, in fact `params` is entirely exposed to the user. There would be no other point in having it. :)

These were all planned and endorsed changes by @kennethreitz. And to a degree they're quite necessary to make requests entirely functional. Leaving `headers` as is, right now, seems okay to me. I'm just not sure how well having the `Session.headers` attribute as a `MultiDict` will work.
",sigmavirus24,dmckeone
1155,2013-04-21 19:53:24,"@sigmavirus24 is correct, `__attrs__` does not limit exposure of those parameters. In particular, `Session.params`, `Request.params`, `Request.data` and `Request.files` are deliberately exposed parts of the API, intended to be used by users.

I think if we're going to make backward-incompatible changes to the API we should do it in one fell swoop. I'm still remembering the less-than-positive initial reaction to turning `Response.json` into a method, and this change is far more wide-ranging than that one.

Changing any of these parameters, especially if `multi=True` is the default (which it probably should be), is a backward-incompatible change. **I'm all for doing it**, as I think the current behaviour could be improved, but any implementation needs to very carefully planned, and needs to be clearly documented and accompanied by a decent version number bump.
",Lukasa,sigmavirus24
1155,2013-04-21 20:54:27,"@sigmavirus24 @Lukasa Thanks guys.  Mea culpa about the `__attrs__`, you're right.

> Changing any of these parameters, especially if multi=True is the default (which it probably should be), is a backward-incompatible change

You may be right (and certainly would be if `headers=` was included), but I believe its only backwards incompatible with the current change if you tried to use assume order or duplicate values, and since that wouldn't be possible in older versions due to the use of `dict`, I don't think it's incompatible in a breaking way.  

I'm thinking of cases like this (where `request` is an instance of `Request`):
- `isinstance(request.params, dict)` 
- `request.params['param']` for single and iterable values.  
- `request.params == {'param': 1}`

Have I missed a case here, or is it a reasonable expectation to say that, if you want to use ordered duplicate `params`, `data`, or `files` then 1.2X (or whatever version this ends up in) is your baseline?  If you don't use ordered or duplicate `params`, `data`, or `files` then things should work identically for all versions.

So with that in mind, how would you like to proceed with this?  Should I be incorporating `headers=` into my branch and targeting a full version release with docs and all that, or is the pull request I submitted a way to start that works well enough, and then headers can be approached separately.  I'm fine with either, or something else entirely, but since I'm new to this project I'm happy to follow your more experienced lead.
",dmckeone,Lukasa
1155,2013-04-21 20:54:27,"@sigmavirus24 @Lukasa Thanks guys.  Mea culpa about the `__attrs__`, you're right.

> Changing any of these parameters, especially if multi=True is the default (which it probably should be), is a backward-incompatible change

You may be right (and certainly would be if `headers=` was included), but I believe its only backwards incompatible with the current change if you tried to use assume order or duplicate values, and since that wouldn't be possible in older versions due to the use of `dict`, I don't think it's incompatible in a breaking way.  

I'm thinking of cases like this (where `request` is an instance of `Request`):
- `isinstance(request.params, dict)` 
- `request.params['param']` for single and iterable values.  
- `request.params == {'param': 1}`

Have I missed a case here, or is it a reasonable expectation to say that, if you want to use ordered duplicate `params`, `data`, or `files` then 1.2X (or whatever version this ends up in) is your baseline?  If you don't use ordered or duplicate `params`, `data`, or `files` then things should work identically for all versions.

So with that in mind, how would you like to proceed with this?  Should I be incorporating `headers=` into my branch and targeting a full version release with docs and all that, or is the pull request I submitted a way to start that works well enough, and then headers can be approached separately.  I'm fine with either, or something else entirely, but since I'm new to this project I'm happy to follow your more experienced lead.
",dmckeone,sigmavirus24
1155,2013-04-21 21:13:54,"Not a worry @dmckeone . I get confused plenty about stuff too. @Lukasa usually corrects me kindly. :)

I think for the current pull request you can leave out `headers` if you're that uncomfortable with it, but it has to be implemented before the next release. 

Part of the documentation will have to include a section about how the `MultiDict` works with respect to `__setattr__` and `__getattr__`. That (if I remember correctly) is non-obvious behaviour and in this instance I have no criticism of this aspect of the API. :P
",sigmavirus24,Lukasa
1155,2013-04-21 21:13:54,"Not a worry @dmckeone . I get confused plenty about stuff too. @Lukasa usually corrects me kindly. :)

I think for the current pull request you can leave out `headers` if you're that uncomfortable with it, but it has to be implemented before the next release. 

Part of the documentation will have to include a section about how the `MultiDict` works with respect to `__setattr__` and `__getattr__`. That (if I remember correctly) is non-obvious behaviour and in this instance I have no criticism of this aspect of the API. :P
",sigmavirus24,dmckeone
1155,2013-04-21 21:20:37,"Getting stuff wrong in public is basically the definition of OSS development, as far as I can tell. =P I certainly do it enough.

I think my concern primarily applies to changing `headers`. We don't expect `param`, `data` or `files` to be mutated by the library, so the user should get out whatever the hell they put in. However, we mutate `headers` quite a bit. If we change what we do there, it's definitely breaking.

I'm with @sigmavirus24 on this: I want to change it, and I think we have to change it, but changing headers is breaking. We might want to consult with @kennethreitz before we go charging ahead on that part of it, to see if he wants to hold off until some later release.

And in case I've seemed a little blunt in this conversation (reading over it again I sure feel like I did), I want to be clear: your work is excellent, and I want to thank you for taking it on. =)
",Lukasa,kennethreitz
1155,2013-04-21 21:20:37,"Getting stuff wrong in public is basically the definition of OSS development, as far as I can tell. =P I certainly do it enough.

I think my concern primarily applies to changing `headers`. We don't expect `param`, `data` or `files` to be mutated by the library, so the user should get out whatever the hell they put in. However, we mutate `headers` quite a bit. If we change what we do there, it's definitely breaking.

I'm with @sigmavirus24 on this: I want to change it, and I think we have to change it, but changing headers is breaking. We might want to consult with @kennethreitz before we go charging ahead on that part of it, to see if he wants to hold off until some later release.

And in case I've seemed a little blunt in this conversation (reading over it again I sure feel like I did), I want to be clear: your work is excellent, and I want to thank you for taking it on. =)
",Lukasa,sigmavirus24
1155,2013-04-21 22:19:35,"@sigmavirus24 Fully documenting how MultiDict behaves is probably a good idea when we do `headers`.  As far as `headers` goes, I'm not necessarily uncomfortable with the actual work, just the implication of the API change and the associated fallout if it were to be done anything less than perfect.  In that way I like @Lukasa's idea to get consensus first, and do it as part of a unique version number.

@Lukasa I actually like direct tone because its clearer, so I didn't mind the bluntness.  Thanks for the compliment as well!

So just to summarize what I think I've read here for everyone's sake (there is quite a long comment thread here): 

MultiDict support will come in two stages: 
1) #1316 for `params`, `data`, and `files` in something relatively soon
2) Full documentation of OrderedMultiDict, as well as a future class that combines the behaviours of OrderedMultiDict with CaseInsensitiveDict, and an implementation of the earlier mentioned future class under the `headers` kwarg. All of which happening under a unique version that incorporates an anticipated API change.
",dmckeone,Lukasa
1155,2013-04-21 22:19:35,"@sigmavirus24 Fully documenting how MultiDict behaves is probably a good idea when we do `headers`.  As far as `headers` goes, I'm not necessarily uncomfortable with the actual work, just the implication of the API change and the associated fallout if it were to be done anything less than perfect.  In that way I like @Lukasa's idea to get consensus first, and do it as part of a unique version number.

@Lukasa I actually like direct tone because its clearer, so I didn't mind the bluntness.  Thanks for the compliment as well!

So just to summarize what I think I've read here for everyone's sake (there is quite a long comment thread here): 

MultiDict support will come in two stages: 
1) #1316 for `params`, `data`, and `files` in something relatively soon
2) Full documentation of OrderedMultiDict, as well as a future class that combines the behaviours of OrderedMultiDict with CaseInsensitiveDict, and an implementation of the earlier mentioned future class under the `headers` kwarg. All of which happening under a unique version that incorporates an anticipated API change.
",dmckeone,sigmavirus24
1155,2013-05-02 04:52:07,"Whelp, I just found this thread after commenting a bunch on @dmckeone's open PR, so I'm going to spend some time over the next couple days to try to understand the issues far better before I make additional comments. I mainly wanted to point out that `Session.headers` is a `CaseInsensitiveDict` now after the #1339 merge, just in case the `headers` change is still floating around as an option.
",cdunklau,dmckeone
1155,2013-05-11 19:52:28,"> Sorry for the delay in this comment.  Things have been busy.

Take your time. @kennethreitz is in transit so there's no rush. Also, I 
wouldn't blame you if you wanted to wait for those other pull requests to be 
merged before submitting a new PR. That seems like the most sane thing to do 
at the moment in my humble opinion.
",sigmavirus24,kennethreitz
1155,2013-05-11 20:25:04,"@sigmavirus24 That is what I will likely do.  I will begin work on making the required `MultiDict` changes and tests in the next few days and then just rebase everything once those pull requests go in.
",dmckeone,sigmavirus24
1155,2013-05-11 20:44:48,"@dmckeone there's no need to close this. You can leave it open until your pull is issued & accepted. I don't think any of us mind.
",sigmavirus24,dmckeone
1155,2013-05-13 15:31:58,"@Lukasa @sigmavirus24 My mistake.  I had closed the PR on purpose, but not this.   Thanks for re-opening.
",dmckeone,Lukasa
1155,2013-05-13 15:31:58,"@Lukasa @sigmavirus24 My mistake.  I had closed the PR on purpose, but not this.   Thanks for re-opening.
",dmckeone,sigmavirus24
1155,2013-12-06 16:21:24,"@dmckeone Sadly there's been no particular progress here, largely because we need to combine this with our current `CaseInsensitiveDict` to produce some kind of horrible monstrosity like a `CaseInsensitiveMultiDict`.

Some discussion occurred over on shazow/urllib3#236 about this. The summary was basically:
- Steal Django's `MultiDict`
- Add key case-insensitivity.
- Implement the structure in urllib3, and just have Requests steal it.

If you fancy working on that, go nuts!
",Lukasa,dmckeone
1155,2013-12-06 17:05:20,"Werkzeug has a multidict too if you need more inspiration

On Dec 6, 2013, at 10:21 AM, Cory Benfield notifications@github.com wrote:

> @dmckeone Sadly there's been no particular progress here, largely because we need to combine this with our current CaseInsensitiveDict to produce some kind of horrible monstrosity like a CaseInsensitiveMultiDict.
> 
> Some discussion occurred over on shazow/urllib3#236 about this. The summary was basically:
> 
> Steal Django's MultiDict
> Add key case-insensitivity.
> Implement the structure in urllib3, and just have Requests steal it.
> If you fancy working on that, go nuts!
> 
> —
> Reply to this email directly or view it on GitHub.
",cdunklau,dmckeone
1155,2014-09-10 20:03:20,"Thanks for the thorough reply, @Lukasa. Great that requests' headers representation is not as wrong as I was afraid of. I'd think this would be worth documenting though, given that so many other libraries use a data structure which preserves multiple headers more closely to how they were sent, leading other requests users to maybe ask you the same thing I did.

Also worth logging a separate bug for joining multiple cookie headers with semicolons?
",requiredfield,Lukasa
1155,2016-07-28 20:01:04,"@BigBlueHat is there a reason a list of tuples is not sufficient for your use-cases? Also what do you mean ""individual headers per line""?
",sigmavirus24,BigBlueHat
1155,2016-07-28 20:57:10,"Thanks @Lukasa. I do like what I'm seeing wrt to the [HTTPHeaderDict](https://github.com/shazow/urllib3/blob/8513dcb2e2f3bf7b53452ddeb22e6e0c322c0e22/urllib3/_collections.py#L101) class. Guess I'll do some digging to see if that would handle sending repeats one per line.

Cheers.
:tophat:
",BigBlueHat,Lukasa
1154,2013-01-31 13:46:36,"I whole-heartedly agree with @Lukasa
",sigmavirus24,Lukasa
1150,2013-01-29 19:25:03,"To clarify @sigmavirus24's point, we only default to ISO-8859-1 in [some cases](http://docs.python-requests.org/en/latest/user/advanced/#encodings).
",Lukasa,sigmavirus24
1149,2013-02-05 17:16:55,"@ansel1, why remove blacklisted certs? Because otherwise, urllib3/ssl will assume we want to trust those certs when we shouldn't.
",sigmavirus24,ansel1
1147,2013-01-27 19:20:16,"Hm, are you sure this works with urllib3? I can understand why it would work for httplib. The import is right there, but I see no import of socket that would be reachable from simply import urllib3.

And yeah @kennethreitz I think it would be great too, that's why I'm exploring it.
",sigmavirus24,kennethreitz
1147,2013-01-27 19:27:32,"Yeah @kennethreitz that would always be the ideal case. :)

@infodox, in a clean virtualenv I just installed urllib3 (1.5 from pypi) and downloaded the socks.py file.

This is my interactive session:



Are you sure it works fine on urllib3?

It might work if you did: `socks.wrapmodule(urllib3.connectionpool)` and `socks.wrapmodule(urllib3.util)` but it definitely doesn't work using only urllib3.

As for us, I think we could import socket from `urllib3.connectionpool` and it would work since that is the first import in `__init__.py`, I'm just not sure if this would pollute the `requests` namespace.
",sigmavirus24,kennethreitz
1147,2013-01-27 19:27:32,"Yeah @kennethreitz that would always be the ideal case. :)

@infodox, in a clean virtualenv I just installed urllib3 (1.5 from pypi) and downloaded the socks.py file.

This is my interactive session:



Are you sure it works fine on urllib3?

It might work if you did: `socks.wrapmodule(urllib3.connectionpool)` and `socks.wrapmodule(urllib3.util)` but it definitely doesn't work using only urllib3.

As for us, I think we could import socket from `urllib3.connectionpool` and it would work since that is the first import in `__init__.py`, I'm just not sure if this would pollute the `requests` namespace.
",sigmavirus24,infodox
1147,2013-02-09 23:17:52,"@infodox, having given it some thought, I honestly think that `socks.py` could be written better. In short, if they want to patch the socket module, they're probably going to want to do so for the entire process, not just for one module. They should be replacing the `socket` entry in `sys.modules`, not trying to rely on a module having `socket` in their namespace. As such, I'm +1 on closing this unless @kennethreitz wants to add a reference to the `socket` stdlib to `requests`, i.e., in `requests/__init__.py` putting `import socket`.
",sigmavirus24,kennethreitz
1147,2013-02-09 23:17:52,"@infodox, having given it some thought, I honestly think that `socks.py` could be written better. In short, if they want to patch the socket module, they're probably going to want to do so for the entire process, not just for one module. They should be replacing the `socket` entry in `sys.modules`, not trying to rely on a module having `socket` in their namespace. As such, I'm +1 on closing this unless @kennethreitz wants to add a reference to the `socket` stdlib to `requests`, i.e., in `requests/__init__.py` putting `import socket`.
",sigmavirus24,infodox
1146,2013-01-28 14:28:03,"@piotr-dobrogost why is it a bad idea?
",sigmavirus24,piotr-dobrogost
1146,2013-01-28 15:58:26,"@sigmavirus24 

Because one blank line is a good way of separating blocks of code inside a function. In light of this separating functions with only one blank line doesn't separate them enough.
",piotr-dobrogost,sigmavirus24
1146,2013-01-28 16:19:36,"@piotr-dobrogost I guess it is merely a matter of preference. I use PEP8 for all my projects. I've never had a problem determining between two methods. And two functions (not methods) are always separated by 2 blank lines as per pep8.
",sigmavirus24,piotr-dobrogost
1146,2013-01-28 16:42:20,"@piotr-dobrogost calm down
",kennethreitz,piotr-dobrogost
1144,2013-01-26 15:13:29,"This is what I see pre #1142



And after #1142 



So @Lukasa is correct, and this is a duplicate.
",sigmavirus24,Lukasa
1143,2013-01-26 17:07:12,"Thanks @sigmavirus24!
",Lukasa,sigmavirus24
1143,2013-01-28 18:36:30,"@kennethreitz: Rebased. =)
",Lukasa,kennethreitz
1142,2013-01-26 04:57:16,"Ah, @kennethreitz beat you to it. MOAR COMMITS!
",theaeolianmachine,kennethreitz
1141,2013-01-24 18:23:52,"@Lukasa line or chunk
",kennethreitz,Lukasa
1140,2013-01-25 03:50:48,"The test failure lies in unittest in python 2.6 not having the `assertIn` method. I'll work from @ralphbean's pr and add a work around tomorrow.
",sigmavirus24,ralphbean
1138,2013-01-24 18:45:37,"@sigmavirus24 Your point about streaming responses is correct; I'm not sure that a simple 'end - start' timer is useful for streaming responses _anyway_, but it IS useful for everything else (which is what I need).

As far as whether it's better as a method or a property - I honestly don't care. It's such a small change that if it gets rewritten to be a single property instead of a method, hooray! All I want is the ability to _get_ this information, from requests, without having to patch it myself every time I use it. @kennethreitz didn't seem to be convinced that this functionality even _belongs_ in requests, so I'm hoping that even if this particular commit doesn't get accepted, the general idea is considered to be worthwhile and some kind of implementation makes its way in.
",clee,sigmavirus24
1138,2013-01-26 15:47:27,"@kennethreitz which start and finish are you talking about? The locals in the adapters file?
",sigmavirus24,kennethreitz
1133,2013-01-24 13:55:48,"@alanhamlett I think he means customizing the the Request class itself and using that instead of the one that comes with requests.
",sigmavirus24,alanhamlett
1133,2013-01-24 15:19:56,"@sigmavirus24 exactly what I was saying.

The primary issue I see today is that if I want to either manipulate a Request instance or subclass the Request object and do my own thing I lose the functionality of the `Session.request` method. It would seem pretty terrible to have to reimplement that method in an ad hoc way just for things like timeouts on the request.

The broader point I'm driving at is `Session.request` explicitly instantiates `Request` which means a caller can't control the behavior of that object. It would really be a shame to have to implement a custom `Session.request` with what would effectively be almost an entire repetition of the request method's logic save for one line's difference.

Unless I'm missing something, this seems to be encouraging some non-idiomatic solutions so perhaps it would be nice to allow a caller to tell Session which Request object to use?
",maxcountryman,sigmavirus24
1133,2013-01-24 17:42:16,"@kennethreitz yes but there's a caveat to that, unless I'm misunderstanding: if you want the wrapper around `send`, i.e. `request` and its helpers, then you have to write your own `Session.send` in order to manipulate the `Request`/`PreparedRequest`, yes?
",maxcountryman,kennethreitz
1133,2013-01-24 17:49:28,"@kennethreitz where do you have access to `Session.request` then? The request has to be sent via `s.send` which eschews all the cool logic happening in `s.request`, right? I can't bootstrap the `Session` with my `Request` instance so I don't see how I can ever use `s.request`.
",maxcountryman,kennethreitz
1133,2013-01-24 18:05:10,"@maxcountryman, that's the cool part. All s.request does is create a Request object, which you already did :)
",kennethreitz,maxcountryman
1133,2013-01-24 18:08:27,"@kennethreitz that ""sugar"" is really handy to have around and it's also expected by anyone who uses the requests standard API, e.g. via `requests.request`. So for instance, say you want to pass `allow_redirects`, unfortunately if I have to invoke `s.send` directly this means I need to write my own wrapper around `s.send` which would basically only change one line of code in `s.request`, i.e. the `req = Request()` line. Am I totally off the mark here?
",maxcountryman,kennethreitz
1133,2013-01-24 18:12:56,"@kennethreitz how can you pass `allow_redirects` to `s.send`? Unless I'm totally missing something, the answer is you can't, because `s.request` does more than just construct a `Request` object. It does neat things like extrapolate on that object to follow redirects, it handles cookies, etc. If I want all that, i.e. if I want to write a custom interface around a Request object that allows the caller to pass in the exact same params they could to `requests.request` then I have to reimplement `s.request` because there's just one line of code there that is fixed and I can't touch, namely `req = Request()`. If all `s.request` did was construct a `Request` object then that would be fine, but from what I can see there's a bunch of stuff happening in its scope which I am in need of. :)
",maxcountryman,kennethreitz
1133,2013-01-24 18:15:00,"@kennethreitz: He wants to use what is essentially a subclass of the `Request` class, which presumably implements different logic for either `Request.prepare` or one of the data fields.

We can solve this, however, by using MAGIC!



Done. Hell yeah. :sunglasses:
",Lukasa,kennethreitz
1133,2013-01-24 18:15:26,"@maxcountryman It's a parameter of send.


",kennethreitz,maxcountryman
1133,2013-01-24 18:17:10,"@kennethreitz I get a keyword error on that...


",maxcountryman,kennethreitz
1133,2013-01-24 18:18:21,"@kennethreitz heh, yeah it's quite unfortunate. :p
",maxcountryman,kennethreitz
1133,2013-01-24 18:18:42,"@kennethreitz: Nope.


",Lukasa,kennethreitz
1133,2013-01-24 18:22:51,"Awesome! Thanks for bearing with me @kennethreitz. :) I may have a chance to make a pull request later on if someone doesn't beat me to it.
",maxcountryman,kennethreitz
1133,2013-01-24 20:23:21,"@maxcountryman, @kennethreitz and I discussed this on IRC (https://botbot.me/freenode/python-requests/1756572/ from the highlighted line, down) and there are some changes that need to be made (which should have taken place during the refactor) which are going to be made now. So you might want to hold off on working on adapters and custom Request objects just yet.
",sigmavirus24,kennethreitz
1133,2013-01-24 20:23:21,"@maxcountryman, @kennethreitz and I discussed this on IRC (https://botbot.me/freenode/python-requests/1756572/ from the highlighted line, down) and there are some changes that need to be made (which should have taken place during the refactor) which are going to be made now. So you might want to hold off on working on adapters and custom Request objects just yet.
",sigmavirus24,maxcountryman
1133,2013-01-24 20:24:32,"@sigmavirus24 Thanks for the update, most appreciated! I'll hold off for now.
",maxcountryman,sigmavirus24
1133,2013-01-25 05:13:53,"So one way of doing this, as you can see from that IRC discussion is to do:



Which should satisfy your needs but which is ugly and horribly complicated. You shouldn't need to care about preparing all of those items. So I'm wondering if there is room for a new Session level method that will pretend to act like `request` but will do this for you and accept `Request` or `reparedRequest` objects. If it takes the former, then it could be the basis for our `request` method. Make sense @kennethreitz ?
",sigmavirus24,kennethreitz
1133,2013-01-28 17:37:26,"@sigmavirus24 that seems good to me. What does @kennethreitz think?
",maxcountryman,kennethreitz
1133,2013-01-28 17:37:26,"@sigmavirus24 that seems good to me. What does @kennethreitz think?
",maxcountryman,sigmavirus24
1129,2013-01-23 13:23:14,"Well said, @Lukasa. 

Thanks for the pull request, but I won't be accepting this at this time. You're not the first person to send this, actually :)
",kennethreitz,Lukasa
1129,2013-01-23 18:44:59,":+1: go get 'em @alanhamlett
",sigmavirus24,alanhamlett
1124,2013-01-22 16:15:47,"@Lukasa you are absolutely correct in your deduction: I am in fact using Python 2.7.x.

That is terribly unfortunate apropos of urllib3 and a little surprising considering how widespread Python 2.x usage still is...
",maxcountryman,Lukasa
1124,2013-01-22 18:00:39,"@maxcountryman: The reason the changes have not yet been made in urllib3 is that the SNI hostname parameter is not exposed in the default library ssl module before Python 3.2. To implement this behaviour would require doing something intelligent with PyOpenSSL, a third party module. I'm beginning to think about doing this, but I certainly won't start before the weekend. =)
",Lukasa,maxcountryman
1124,2013-01-26 17:51:56,"@Lukasa you're right. It is exclusive to python 2.x (I tested on 2.6). Testing again on python 3.2 works just fine.
",sigmavirus24,Lukasa
1124,2013-02-09 23:14:27,"Also, the cert is being updated in #1149. We just need to remove any blacklisted certs that are in there. /ping @nacht ;)
",sigmavirus24,nacht
1123,2013-01-23 17:19:53,"@andrewjesaitis, I have to look up where I think I did this then, because I'm getting the same thing on 2.6. :/ Sorry for the confusion.

And yeah, placing it on the auth in the pattern I mentioned above would work. Inside the if-statement you increment it by one, outside you set it to 1. That way if the auth is passed around (say in a session) it won't have incorrect counts.
",sigmavirus24,andrewjesaitis
1123,2013-01-23 18:43:51,"@kennethreitz looks good to me. :+1:
",sigmavirus24,kennethreitz
1123,2013-01-26 05:00:31,":sparkles: @sigmavirus24 has spoken! :sparkles:
",kennethreitz,sigmavirus24
1119,2013-01-21 13:43:50,"@sigmavirus24 that's exactly what i was thinking.

Need to think about if more. @Lukasa makes a point.
",kennethreitz,Lukasa
1119,2013-01-21 13:43:50,"@sigmavirus24 that's exactly what i was thinking.

Need to think about if more. @Lukasa makes a point.
",kennethreitz,sigmavirus24
1119,2013-01-21 14:55:50,"I agree with @Lukasa 's point, and we've already broken the pre 1.x behaviour, so restoring it is not a must. Raising a `ValueError` seems to be the pythonic thing to do here, otherwise.
",sigmavirus24,Lukasa
1119,2013-01-26 15:48:53,"@kennethreitz: You didn't seem to have checked in any code for this, so I decided to give you an easy option. =)

I feel like the `getattr` check is overbroad, users might want to use custom PreparedRequest objects where the `prepare()` method simply mutates the current object as opposed to returning a new one. Normally I wouldn't use it, but as we're testing for 'user makes a boo-boo', maybe we should use it.
",Lukasa,kennethreitz
1119,2013-01-28 14:44:24,"@Lukasa I rebased and made the changes for you [here](https://github.com/sigmavirus24/requests/tree/lukasa/diags)
",sigmavirus24,Lukasa
1119,2013-02-09 23:19:14,"@Lukasa rebase again please? ;P
",sigmavirus24,Lukasa
1116,2013-01-19 18:12:56,"@Lukasa just use the `io` module. It has `StringIO` in it in python 2.6-3.3
",sigmavirus24,Lukasa
1116,2013-01-19 18:16:27,"@sigmavirus24:



The problem isn't in my code, it's that stupid StringIO _does_ have the method, but unconditionally throws exceptions. Given that Requests tries to use it, that's a problem. =P
",Lukasa,sigmavirus24
1116,2013-01-23 20:59:22,"@Lukasa 

As to type of this header, I agree, it should not be unicode. More generally no header should be unicode. Actually, thinking of it, no header in prepared request should be unicode; there could be some value in leaving unicode in unprepared request.
",piotr-dobrogost,Lukasa
1116,2013-01-24 12:43:03,"@kennethreitz Yeah, I was wondering about that. It seems like headers on the `Request` object should be free to be either unicode or bytes. With that said, somewhere before urllib3 we should get rid of any unicode headers, in order to avoid problems like #1082. Maybe `prepare_headers` should turn them into bytes?
",Lukasa,kennethreitz
1109,2014-09-30 08:57:08,"@kennethreitz : I just came across this while looking for a way to do this.  Can you explain why allowing requests to open file scheme URLs would be a ""conceptual flaw""?
",BrenBarn,kennethreitz
1109,2014-10-05 16:43:43,"They are also a big security risk for people that expecting to be able to trust that this is an HTTP library :)

Many thanks, @Lukasa :)
",kennethreitz,Lukasa
1104,2013-01-17 03:17:10,"@c00w if you fixed it, close this issue please? 
",sigmavirus24,c00w
1100,2013-01-25 04:05:19,"This should be working now after #1099. Can you test from the repo and confirm @jimothyGator?
",sigmavirus24,jimothyGator
1099,2013-01-23 13:35:58,"@sprt can you update this? It's a bit out of date now :)
",kennethreitz,sprt
1099,2013-01-23 15:14:18,"@sprt, I'll work on this.
",sigmavirus24,sprt
1099,2013-01-23 16:48:51,"@kennethreitz, @sprt makes a valid point. My concern is dispatching a hook more than once. Someone using requests in @sprt's case has a valid reason to believe the hook should be called which makes it the responsibility of either the adapter's `send` or the session's `send` method. The former also catches the case where a user just instantiates the adapter and uses that directly (which is a bad idea). The latter would allow the hook to be dispatched once, and would remove it from the `Session.request` and `<Adapter>.send`. This I see as being the preferable option, but I wanted to make sure this is okay.

The problem, of course, is that only `request` has the hooks passed by the user. Naturally, this shouldn't be a problem because the prepared request has those hooks and the auth hooks generated by the authentication handler, so we can just rely on that. No where in between are hooks removed. A rogue adapter may do this in its `send` method, but that would be author's problem, not ours.
",sigmavirus24,kennethreitz
1099,2013-01-23 16:48:51,"@kennethreitz, @sprt makes a valid point. My concern is dispatching a hook more than once. Someone using requests in @sprt's case has a valid reason to believe the hook should be called which makes it the responsibility of either the adapter's `send` or the session's `send` method. The former also catches the case where a user just instantiates the adapter and uses that directly (which is a bad idea). The latter would allow the hook to be dispatched once, and would remove it from the `Session.request` and `<Adapter>.send`. This I see as being the preferable option, but I wanted to make sure this is okay.

The problem, of course, is that only `request` has the hooks passed by the user. Naturally, this shouldn't be a problem because the prepared request has those hooks and the auth hooks generated by the authentication handler, so we can just rely on that. No where in between are hooks removed. A rogue adapter may do this in its `send` method, but that would be author's problem, not ours.
",sigmavirus24,sprt
1099,2013-01-24 14:06:49,"@sprt those changes pass with your tests. If you like them merge them into your branch, they'll be added to this pull and @kennethreitz could accept it.
",sigmavirus24,kennethreitz
1099,2013-01-24 14:06:49,"@sprt those changes pass with your tests. If you like them merge them into your branch, they'll be added to this pull and @kennethreitz could accept it.
",sigmavirus24,sprt
1099,2013-01-24 18:04:10,"@sigmavirus24 was this not ready? send another pr :)
",kennethreitz,sigmavirus24
1099,2013-01-24 21:24:53,"Ah indeed, forgot about mounting!

Then I suggest @kennethreitz edit [his blog post](http://www.kennethreitz.com/announcing-requests-v100.html) (cf. ""Connection Adapters"") :)
",sprt,kennethreitz
1090,2013-01-10 20:35:47,"@mikofski no you're just doing it wrong. You should be doing


",sigmavirus24,mikofski
1090,2013-01-10 20:51:10,"To be clear @mikofski there's a difference between headers and parameters. In your first example, you're encoding what you call headers to be used as parameters and then you append them to `URL`. If you look at the [docs](http://python-requests.org) you'll see headers are entirely different from parameters. If the Netflix API is calling them headers in their documentation then they need to be corrected.
",sigmavirus24,mikofski
1090,2013-01-10 20:59:48,"WOW! Thank you @sigmavirus24! I am a moron! The netflix api does call them params, it was my stupid mistake. I can't believe how long I was stymied by this, when as usual the answer was right in front of me.
",mikofski,sigmavirus24
1090,2013-01-10 21:08:13,"@mikofski don't beat yourself up. We all mistakes. I could probably go on for a couple hours about the mistakes I've made that held me up for days or weeks on end. If you have any future questions, feel free to email me directly or the mailing list. I'll answer on either. Cheers!
",sigmavirus24,mikofski
1087,2013-01-08 03:50:00,"@sigmavirus24 I noticed the issue with the title of this article:

http://www.theverge.com/2013/1/4/3836944/robot-band-compressorhead-plays-motorhead-ace-of-spades

a few days ago, it was reporting an encoding of `iso-8859-1`. I don't see the issue now, however. It would be a little weird if they just happened to fix the issue since I last looked... but I don't have a better explanation. Content-Type is now:

`Content-Type: text/html; charset=utf-8`
",akavlie,sigmavirus24
1087,2013-01-10 08:21:12,"@kennethreitz understood after reviewing your comments in #156. A mention of the function in the docs would be useful.
",akavlie,kennethreitz
1085,2013-01-06 04:27:07,"Heh, good catch. Would have been good if @Lukasa or I had actually looked at the PR. ;)
",sigmavirus24,Lukasa
1084,2013-01-23 23:07:10,"@sigmavirus24

> If I understand correctly, the behaviour on redirect should be that we don't follow the redirect but instead just return the 307 response to the user, correct?

Not quite. Section [`7.4. Redirection 3xx`](http://trac.tools.ietf.org/html/draft-ietf-httpbis-p2-semantics-21#section-7.4) in the current draft from httpbis working group states

>   This class of status code indicates that further action needs to be
>    taken by the user agent in order to fulfill the request.  If the
>    required action involves a subsequent HTTP request, it MAY be carried
>    out by the user agent without interaction with the user if and only
>    if the method used in the second request is known to be ""safe"", as
>    defined in Section 5.2.1.

So speaking simply in case of 307 status code in reply to request with unsafe method (method other than the GET, HEAD, OPTIONS, and TRACE) we MAY NOT carry the redirect without interaction with the user. Now, the question is how to define _interaction with the user_ for http library. I agree with @jaraco saying that we should _require that the request be configured to ""re-post on 307""._

> Are there any other cases like this? 

Yes, soon to be accepted 308 status code - look for `draft-reschke-http-status-308-07.txt` at http://www.rfc-editor.org/cluster_info.php?cid=C160



Side note: using `codes.moved, codes.found` etc. in the source code instead of codes' numeric values does not help in quickly identifying these codes.
",piotr-dobrogost,sigmavirus24
1084,2013-01-23 23:07:10,"@sigmavirus24

> If I understand correctly, the behaviour on redirect should be that we don't follow the redirect but instead just return the 307 response to the user, correct?

Not quite. Section [`7.4. Redirection 3xx`](http://trac.tools.ietf.org/html/draft-ietf-httpbis-p2-semantics-21#section-7.4) in the current draft from httpbis working group states

>   This class of status code indicates that further action needs to be
>    taken by the user agent in order to fulfill the request.  If the
>    required action involves a subsequent HTTP request, it MAY be carried
>    out by the user agent without interaction with the user if and only
>    if the method used in the second request is known to be ""safe"", as
>    defined in Section 5.2.1.

So speaking simply in case of 307 status code in reply to request with unsafe method (method other than the GET, HEAD, OPTIONS, and TRACE) we MAY NOT carry the redirect without interaction with the user. Now, the question is how to define _interaction with the user_ for http library. I agree with @jaraco saying that we should _require that the request be configured to ""re-post on 307""._

> Are there any other cases like this? 

Yes, soon to be accepted 308 status code - look for `draft-reschke-http-status-308-07.txt` at http://www.rfc-editor.org/cluster_info.php?cid=C160



Side note: using `codes.moved, codes.found` etc. in the source code instead of codes' numeric values does not help in quickly identifying these codes.
",piotr-dobrogost,jaraco
1084,2013-01-24 17:28:45,"@piotr-dobrogost my comment was in the context of a POST or PUT request. Otherwise, my opinion is that POST or PUTs should return the 307 as our way of interacting with the user, otherwise follow it. It seems that the decision has been made that `allow_redirects` should be set to False, otherwise requests understands the interaction to be that it should re-POST/PUT the data, which seems reasonable to me (in the case the user is expecting a 307).

**Edit**: I'm clearly too busy. I conflated the #975 with this.
",sigmavirus24,piotr-dobrogost
1084,2013-01-26 17:48:24,"@cbare you're correct about that call to `request` not being complete.

So let me just walk through the steps of the request before submitting a pull request to fix that.

In a normal case (not chunked encoding), the user calls `requests.post(url, data={'key': 'value'}, files={'foo': open('foo', 'rb')})`. In this case, the `Request` object is created and prepared turning into a `PreparedRequest` which is what we receive as `req` in `resolve_redirects`. This is stored in `req.body`. Since this is prepared, we can do this (in `resolve_redirects`):



This works because when `data` receives a string, it sends that.

The problematic case is when the user is using chunked encoding (I think). The problem is, I'm not entirely sure what happens with chunked encoding at the moment. Maybe @kennethreitz can explain how that works because I haven't presently looked at it at all.

I could be wrong and it could all be handled as one case though.
",sigmavirus24,cbare
1084,2013-01-26 20:56:23,"@sigmavirus24 

> And by implementing it themselves, they could use hooks

Hooks are for custom actions not mandated by any RFC. We should not force users to write/use hooks to accomplish something specified in the RFC. Also there's no collection of commonly needed hooks packaged with Requests which makes any solution based on hooks more problematic for users.

@cbare

> the call to the request method in lines 122-135 of sessions.py doesn't propagate the body of the original request

Body will have to be kept when supporting 307/308.
",piotr-dobrogost,sigmavirus24
1084,2013-01-26 20:56:23,"@sigmavirus24 

> And by implementing it themselves, they could use hooks

Hooks are for custom actions not mandated by any RFC. We should not force users to write/use hooks to accomplish something specified in the RFC. Also there's no collection of commonly needed hooks packaged with Requests which makes any solution based on hooks more problematic for users.

@cbare

> the call to the request method in lines 122-135 of sessions.py doesn't propagate the body of the original request

Body will have to be kept when supporting 307/308.
",piotr-dobrogost,cbare
1084,2013-02-11 22:25:43,"@kennethreitz 

`allow_redirects` is about allowing redirects in general but in case of 307/308 status codes web browser asks user about permission that's why we have to similarly ""ask"" user of the library.
",piotr-dobrogost,kennethreitz
1084,2013-02-13 12:56:44,"So with my second to last pull request merged, we now re-post the data on 307. I missed that it should also be on 308, so if @kennethreitz doesn't mind I might just push the one-line fix for that instead of issuing a PR.

If I understand correctly, this issue would then be fixed. If users want to be ""asked"" they will just have to pass False to `allow_redirects` which will need to be documented. 
",sigmavirus24,kennethreitz
1084,2013-02-13 16:03:35,"@sigmavirus24 

As I wrote in my last comment 

> `allow_redirects` is about allowing redirects in general but in case of 307/308 status codes web browser asks user about permission that's why we have to similarly ""ask"" user of the library.

We need another param to let users decide if they want to follow 307/308 with unsafe http method.
",piotr-dobrogost,sigmavirus24
1075,2013-01-10 04:26:39,"@kennethreitz 
Let me clarify the reason In my understanding the reason is my patch has a dependency on ""netaddr"" module. Is it correct? If so this is the policy of this module and I am happy to honor that.

Lastly, do you have a plan to support x.y.z.w/prefix form in no_proxy?
",amotoki,kennethreitz
1074,2013-01-08 09:57:11,"@MicksMix: You've actually confused two different issues, which is totally understandable. =)

#905 is specifically a problem with HTTPS over proxies: in particular, urllib3 does not use the CONNECT verb. _This_ issue is affecting all proxies, and is a result of the refactor.

@trentvb, @erikcw, @mishari, @MicksMix: Some similar issues were raised earlier: see #1056 and #1058. I submitted a fix for those in PR #1060, which has not yet made it to PyPI. Try using the version of Requests from `master` and see if that solves your problem.
",Lukasa,MicksMix
1074,2013-01-08 09:57:11,"@MicksMix: You've actually confused two different issues, which is totally understandable. =)

#905 is specifically a problem with HTTPS over proxies: in particular, urllib3 does not use the CONNECT verb. _This_ issue is affecting all proxies, and is a result of the refactor.

@trentvb, @erikcw, @mishari, @MicksMix: Some similar issues were raised earlier: see #1056 and #1058. I submitted a fix for those in PR #1060, which has not yet made it to PyPI. Try using the version of Requests from `master` and see if that solves your problem.
",Lukasa,trentvb
1074,2013-01-08 09:57:11,"@MicksMix: You've actually confused two different issues, which is totally understandable. =)

#905 is specifically a problem with HTTPS over proxies: in particular, urllib3 does not use the CONNECT verb. _This_ issue is affecting all proxies, and is a result of the refactor.

@trentvb, @erikcw, @mishari, @MicksMix: Some similar issues were raised earlier: see #1056 and #1058. I submitted a fix for those in PR #1060, which has not yet made it to PyPI. Try using the version of Requests from `master` and see if that solves your problem.
",Lukasa,erikcw
1074,2013-01-08 09:57:11,"@MicksMix: You've actually confused two different issues, which is totally understandable. =)

#905 is specifically a problem with HTTPS over proxies: in particular, urllib3 does not use the CONNECT verb. _This_ issue is affecting all proxies, and is a result of the refactor.

@trentvb, @erikcw, @mishari, @MicksMix: Some similar issues were raised earlier: see #1056 and #1058. I submitted a fix for those in PR #1060, which has not yet made it to PyPI. Try using the version of Requests from `master` and see if that solves your problem.
",Lukasa,mishari
1074,2013-01-08 14:02:06,"@Lukasa Thanks!
",MicksMix,Lukasa
1073,2013-01-02 19:31:15,"@lardissone, @piotr-dobrogost is correct. You can use what I did in that pull request to close it yourself. There just isn't a clean API for it in requests unfortunately.

To recapitulate, look into `stream=True` and then after checking the status_code use `Response.raw` and work from there to incorporate the changes in the PR @piotr-dobrogost referenced above.
",sigmavirus24,piotr-dobrogost
1073,2013-01-02 19:31:15,"@lardissone, @piotr-dobrogost is correct. You can use what I did in that pull request to close it yourself. There just isn't a clean API for it in requests unfortunately.

To recapitulate, look into `stream=True` and then after checking the status_code use `Response.raw` and work from there to incorporate the changes in the PR @piotr-dobrogost referenced above.
",sigmavirus24,lardissone
1073,2014-02-20 00:38:06,"Hm, something similar is confusing me. Is partially reading responses by chunks somehow interfering with keeping the session alive?

This is what I am trying to do:



I want to only read enough of the image to be able to parse it using PIL and detect its size - and then cancel the current request, but keep the session alive, because I need to do it for all images on a website. Still, each time I call this snippet (with the same session) a new SSL handshake is established. Did I understand @piotr-dobrogost's PR wrong?
",metakermit,piotr-dobrogost
1071,2012-12-28 15:36:03,"@Lukasa I'll merge if you remove the print statement :)
",kennethreitz,Lukasa
1070,2012-12-26 02:26:13,"Yeah, I'm not sure if we need to re-pass the parameters. This is a good catch @alefnula. If you want to ready a commit that removes this, feel free to. I'm sure @Lukasa or @kennethreitz will weigh in after the holidays.
",sigmavirus24,alefnula
1069,2012-12-27 11:45:14,"The plot thickens:



Tell me, @leechou, are you being redirected to a different domain or path?
",Lukasa,leechou
1058,2012-12-22 12:12:55,"@sigmavirus24 is right. I've just posted a possible fix in #1056, so please follow the discussion there. Thanks for the report!
",Lukasa,sigmavirus24
1056,2012-12-22 12:14:23,"So, I've got a branch with a possible fix for this issue [here](https://github.com/Lukasa/requests/tree/proxy). @python273 and @leoluk, would you like to try downloading it and making the same request? If it works, I'll submit a Pull Request.
",Lukasa,python273
1053,2013-01-04 09:22:55,"Thanks @dttocs !
So I catch it, `curl` set a `> Host: pypi.python.org` in header that `requests` doesn't.
When I set it manually it works.

And another thing, `requests` failed to auth on proxy, I have to set myself the `Proxy-Authorization` header.
For information I set proxy like that:



My password have some specials characters like `$` for information.

I think it's something already mentionned in another issue. 
I will test with git version of `requests`.

Many thanks.
socketubs.
",toxinu,dttocs
1053,2013-01-19 17:11:26,"@Lukasa There are a bunch of other outstanding pulls, some of which may address this. I haven't gone through my backlog in a while. :( But you're welcome to throw something in just in case.
",shazow,Lukasa
1053,2013-02-09 03:19:25,"@socketubs, I think @Lukasa is close to fixing this for you. (Just to keep you up to date)
",sigmavirus24,Lukasa
1053,2013-02-09 14:17:56,"I think we're just waiting for @shazow to merge a change over on urllib3 before we pull that in.
",sigmavirus24,shazow
1051,2012-12-21 13:25:48,"I'm not saying we ignore it, I'm just saying it isn't against spec. And yeah, I agree that the 503 looks like it's a malformed request error. I'll mock up conditional addition for GETs tonight and see if @kennethreitz wouldn't mind the minor extra complexity.
",sigmavirus24,kennethreitz
1050,2012-12-19 21:54:21,"@kennethreitz I love this refactor more and more.

@PaulMcMillan can you post the call you're making or an example of it? This fix was introduced ~0.14.1

And by example, I mean: fake header and fake URI
",sigmavirus24,PaulMcMillan
1050,2012-12-19 21:54:21,"@kennethreitz I love this refactor more and more.

@PaulMcMillan can you post the call you're making or an example of it? This fix was introduced ~0.14.1

And by example, I mean: fake header and fake URI
",sigmavirus24,kennethreitz
1050,2013-01-24 12:37:48,"I'm closing this as resolved by v1.0.4. =) @PaulMcMillan, I understand your worries about the maturity of v1, but there's not much I can do about it other than to say that any bugs that are in v0.14 will never be fixed. Bugs in v1 will be. =) Thanks for raising this issue!
",Lukasa,PaulMcMillan
1049,2012-12-19 21:34:11,"@Lukasa  import it :) 
",kennethreitz,Lukasa
1049,2012-12-19 21:35:59,"C'mon @Lukasa you need some sleep brother.
",sigmavirus24,Lukasa
1049,2012-12-19 21:56:02,"@Lukasa take a break from everything, you deserve it. :fistbump:
",sigmavirus24,Lukasa
1049,2012-12-22 10:55:45,"@kennethreitz, what's the state of this? Got `basestring` in now, so all should be good. =)
",Lukasa,kennethreitz
1045,2012-12-19 20:31:49,"@slingamn :heart: 

Quite right, the cookiejar should be empty. I clearly need to take a nap. Or a holiday.

Not a bug! Thanks for the report, @odedgolan, I'm now closing this. =)
",Lukasa,slingamn
1045,2012-12-19 20:31:49,"@slingamn :heart: 

Quite right, the cookiejar should be empty. I clearly need to take a nap. Or a holiday.

Not a bug! Thanks for the report, @odedgolan, I'm now closing this. =)
",Lukasa,odedgolan
1045,2012-12-19 20:45:05,"True, it is the standard mechanism for deleting cookies. 
This is why it needs to be included in the CookieJar so it can be sent in turn to a browser, and the browser can delete its local cookie.

I do not wish to use Requests as a full implementation of a browser but as a vessel for HTTP requests. 
@Lukasa @slingamn What do you think? Don't you think I'm right on this?

Thank you,
Oded.
",odedgolan,slingamn
1045,2012-12-19 20:45:05,"True, it is the standard mechanism for deleting cookies. 
This is why it needs to be included in the CookieJar so it can be sent in turn to a browser, and the browser can delete its local cookie.

I do not wish to use Requests as a full implementation of a browser but as a vessel for HTTP requests. 
@Lukasa @slingamn What do you think? Don't you think I'm right on this?

Thank you,
Oded.
",odedgolan,Lukasa
1045,2012-12-19 21:37:00,"@odedgolan no offense, but that looks sketchy as hell.

Even so, are the cookie settings not left in `Response.headers`? Why not just pass them along from there?
",sigmavirus24,odedgolan
1045,2012-12-20 06:18:16,"+1 for @sigmavirus24 here; it sounds like you want to pass through the original headers, including the Cookie header.

However, if you really want to do this via `CookieJar`, you can write your own `CookieJar` class that implements the policy you want and then pass it in to Requests via the `cookies` kwarg. Requests has its own `RequestsCookieJar` but it's only a way to provide a convenient external API, internally we should be able to work with any `CookieJar` subclass.
",slingamn,sigmavirus24
1045,2012-12-20 07:55:14,"@sigmavirus24 non taken, no worries. 

I cannot just pass the header, I need some manipulation ability on the cookies.

Subclassing CookieJar sounds like the right solution but passing it to Requests via cookies kwarg will set the ""to send"" CookieJar, not the ""to receive"" CookieJar.
",odedgolan,sigmavirus24
1044,2012-12-19 23:14:07,"@alonho 
This spec is soon to be obsolete by new version - see http://lists.w3.org/Archives/Public/ietf-http-wg/2012OctDec/0103.html

[7.4.2 301 Moved Permanently](http://tools.ietf.org/html/draft-ietf-httpbis-p2-semantics-21#section-7.4.2)

> Note: For historic reasons, user agents MAY change the request method from POST to GET for the subsequent request. If this behavior is undesired, status code 307 (Temporary Redirect) can be used instead.

See issue #269 where this topic was discussed.
",piotr-dobrogost,alonho
1041,2012-12-19 02:34:30,"@bboe your server and your test works fine.
",sigmavirus24,bboe
1041,2012-12-19 02:35:21,"@sigmavirus24 Do you mean you are unable to reproduce the problem?
",bboe,sigmavirus24
1041,2012-12-19 02:37:00,"@bboe exact opposite. I was just going to edit that since I realized it was a bit vague. Even calling release_conn doesn't work.
",sigmavirus24,bboe
1041,2012-12-19 02:43:14,"@sigmavirus24 Oh okay good. I was hoping I wouldn't have to find the set of platform specific settings to reproduce :) Does this mean you are going to look into this a bit then?
",bboe,sigmavirus24
1041,2012-12-19 02:52:25,"@bboe looking into it right now.
",sigmavirus24,bboe
1041,2012-12-19 03:06:25,"@sigmavirus24 Thanks for the quick fix. I'll use that for now.
",bboe,sigmavirus24
1041,2012-12-19 03:10:24,"@bboe glad to help. I'm trying to figure out what the best approach might be with urllib3 for now and how to test it.
",sigmavirus24,bboe
1041,2012-12-19 03:48:50,"@sigmavirus24 I'm thinking the issue should be fixed in urllib3. See https://github.com/kennethreitz/requests/blob/master/requests/models.py#L466

It seems that the entire content is received by requests, thus urllib3 should be aware of whether or not the connection is reusable as this point. If the connection is not re-usable, the socket should be closed.
",bboe,sigmavirus24
1041,2013-01-23 13:40:13,"@bboe your PR on urllib3 was accepted and we just updated urllib3 in requests, care to close this? 
",sigmavirus24,bboe
1034,2012-12-18 16:16:32,"> Kenneth thinks this API is better/cleaner.

@kennethreitz 

Do you, really? If yes, can you reference any articles proposing this or libraries doing this?
",piotr-dobrogost,kennethreitz
1034,2012-12-18 18:06:46,"@piotr-dobrogost: can you stop harassing me?
",kennethreitz,piotr-dobrogost
1034,2012-12-18 22:42:08,"> can you stop harassing me?

@kennethreitz 

I can't stop doing something I never started :) Relax. Whenever somebody states (Lukasa in this case) what the other person thinks (you in this case) I want to confirm if it's really the case. That's the reason I asked if you really think this API is better/cleaner. Not using parameters in a constructor of rather complex object in object orientated language is novelty to me. That's the reason I asked about some references. As everybody can see both questions are natural and I have no idea why did you come up with this harassment thing.
",piotr-dobrogost,kennethreitz
1034,2014-10-13 07:21:10,"@piotr-dobrogost While I do not like that decisison either API design is mainly based on a subjective point of view so there is nothing to discuss here. I have now read multiple issues in requests within the range of a few years and in all of them (!) you are trolling. Please stop doing that you are not helpful.
",schlamar,piotr-dobrogost
1034,2014-10-13 13:41:54,"@schlamar I would also remind you of the fact that attacks on other contributors are not welcome here. If you aren't going to be cordial towards @piotr-dobrogost and every other contributor to requests, you will not [be welcomed](http://www.kennethreitz.org/essays/be-cordial-or-be-on-your-way) here. No contributor to this project has the right to attack another and it will not be tolerated.
",sigmavirus24,piotr-dobrogost
1034,2014-10-13 13:41:54,"@schlamar I would also remind you of the fact that attacks on other contributors are not welcome here. If you aren't going to be cordial towards @piotr-dobrogost and every other contributor to requests, you will not [be welcomed](http://www.kennethreitz.org/essays/be-cordial-or-be-on-your-way) here. No contributor to this project has the right to attack another and it will not be tolerated.
",sigmavirus24,schlamar
1031,2012-12-18 14:52:26,"Always welcome @kennethreitz 
",sigmavirus24,kennethreitz
1029,2012-12-18 02:44:00,"@gmr if you think it is the pool size, try subclassing the `HTTPAdapter`, e.g.,



**Edit** where session above is a `Session` instance that you've already created.
",sigmavirus24,gmr
1028,2012-12-18 00:49:22,"And looking at the new json method, I like it. Attributes shouldn't raise exceptions realistically when accessed unless they don't exist. Making it a method and exposing a possible issue with loading the json makes far more sense than the way the old attribute worked.

@rdegges you won't be the only one updating your dependencies ;)
",sigmavirus24,rdegges
1028,2013-02-15 20:19:13,"Just weighing in as I only noticed this change today. First, @kennethreitz, I totally agree with your reasons for changing it and I'm glad you did! This is a good upgrade.

That said, I might have preferred a staged transition, like:

v 0.15.0:
- `response.json` is renamed to `response.json_property`
- `response.json_method` is the method version of `response.json_property`
- `response.json` is a new property that issues a DeprecationWarning and returns `response.json_property`

End user actions: replace `response.json` with `response.json_property` throughout their code. Other co-installed projects designed for 0.14.1 can keep using `response.json` for now. Installations can upgrade from 0.14.1 to 0.15.0 without breakage.

v 1.0.0:
- `response.json` now points to `response.json_method`

End user actions: replace `response.json_property` with `response.json()` throughout their code. Other co-installed projects designed for 0.15.0 can keep using `response.json_property` for now. Installations can upgrade from 0.15.0 to 1.0.0 without breakage.

v 1.1.0:
- `response.json_property` and `response.json_method` are removed

I find myself in the situation where it's easy enough to upgrade the code I'm writing to use the 1.0.0 API (which I think is great!), but I have to coordinate with other departments so that we all make the from from 0.14.1 to 1.0.0 at the same moment. There's no intermediate state that supports both the old and new styles at the same time.
",kstrauser,kennethreitz
1025,2012-12-17 23:03:42,"So @sigmavirus24 it seems that GAE support is deprecated :(
",PanosJee,sigmavirus24
1025,2012-12-17 23:08:31,"Thanks @shazow , I ll try to patch it first!
",PanosJee,shazow
1023,2012-12-18 14:52:16,"@myzhan this should be fixed in 1.0.3, try upgrading to that. On a side note, if you wouldn't mind posting the encode function's code, I'd be interested in playing with it.
",sigmavirus24,myzhan
1023,2012-12-19 01:44:49,"@sigmavirus24 Cool!!! 1.0.3 works fine with my code, thx for your great help. But encode function's code is used for commercial encrypted file, I can't post it, sorry. 
",myzhan,sigmavirus24
1021,2012-12-17 19:12:37,"@piotr-dobrogost Python 3.
",kennethreitz,piotr-dobrogost
1018,2012-12-17 18:13:08,"@kennethreitz Well, that just isn't going to do. All code must be shipped at once. No :cake: for you. :rage4: 
",michaelhelmick,kennethreitz
1018,2012-12-17 18:13:24,"@Lukasa Sure.
",michaelhelmick,Lukasa
1018,2012-12-17 18:15:08,"@michaelhelmick pull requests accepted :)
",kennethreitz,michaelhelmick
1018,2012-12-17 18:17:32,":+1: If I find some time to fix it up, I def. will. Currently at work though!  

## 

Mike Helmick

On Monday, December 17, 2012 at 1:15 PM, Kenneth Reitz wrote:

> @michaelhelmick (https://github.com/michaelhelmick) pull requests accepted :)
> 
> —
> Reply to this email directly or view it on GitHub (https://github.com/kennethreitz/requests/issues/1018#issuecomment-11452999).  
",michaelhelmick,michaelhelmick
1010,2012-12-16 03:57:43,"I'm getting the same thing when attempting to install from the repository (this). I've mirrored @harmesy's fix in a [pull request](https://github.com/kennethreitz/requests/pull/1011).
",kuyan,harmesy
1010,2012-12-16 04:00:48,"And, @harmesy, you went about this in a completely correct way. :+1: 
",kuyan,harmesy
1005,2012-12-17 22:37:05,"@woozyking that was in the notes for 1.0, json is now a method not an attribute, i.e., use `response.json()` instead of `response.json`
",sigmavirus24,woozyking
1005,2012-12-17 22:37:33,"Whoops thought this was a bug report 9_9. But yeah, should be a transition section like @woozyking suggests.
",sigmavirus24,woozyking
1005,2012-12-17 22:37:57,"@sigmavirus24 yep, I saw that. Just saying to highlight it in the porting doc :)
",woozyking,sigmavirus24
997,2012-12-19 02:47:56,"@jeanmarcrousseau is this still an issue with v1.0.3?
",sigmavirus24,jeanmarcrousseau
997,2012-12-20 14:58:45,"I'm not sure - Lukasa was going to address, so rather check with him...

JM

On Tue, Dec 18, 2012 at 9:47 PM, Ian Cordasco notifications@github.comwrote:

> @jeanmarcrousseau https://github.com/jeanmarcrousseau is this still an
> issue with v1.0.3?
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/issues/997#issuecomment-11515467.
",jeanmarcrousseau,jeanmarcrousseau
995,2012-12-12 09:56:28,"@shazow Last I checked, @kennethreitz's 'official' position was that we must support 2.7 and 3.3, and anything else is a bonus.
",Lukasa,kennethreitz
995,2012-12-12 09:56:28,"@shazow Last I checked, @kennethreitz's 'official' position was that we must support 2.7 and 3.3, and anything else is a bonus.
",Lukasa,shazow
995,2012-12-13 18:10:31,"@shazow see you over at shazow/urllib3#9 ...
",fhsm,shazow
995,2012-12-16 06:04:27,"@kennethreitz I've had a look around urllib3 and started working on getting `source_address` into the API. Making it work is easy, making it fail nicely is much harder. How would you envision exposing `source_address` in the the requests' API? 

I'm struggling with a good way to avoid making an API that invites user's to over specify the outbound sockets used by the connection pool such that they are slammed with _error 98_'s and we are slammed with all sorts of impossible to reproduce issues around socket allocation/release.
",fhsm,kennethreitz
995,2013-02-09 23:23:23,"@fhsm, think about how you might sanely like to access the `source_address` attribute? I would think it could be an attribute on the `Response` object, but then we're currently under a feature freeze (see: #1168, #1165). I would hope this doesn't get bumped due to it though, since it was an accepted feature before the freeze started.
",sigmavirus24,fhsm
995,2013-04-03 23:23:28,"@fhsm if you specify a port of 0 then that's the magic code to let the OS pick a free port for you.

I have a branch with source_address support working, conversation will probably happen here: https://github.com/shazow/urllib3/issues/9#issuecomment-15871420 but I'll leave a comment here for people landing here from google.
",infracanophile,fhsm
992,2012-12-06 21:56:27,"@mwielgoszewski just to elaborate on @kennethreitz's comment.

With the `pre_fetch=False` option, your solution will not work for `__len__` when the server doesn't provide a Content-Length header. As for iterating over the response, there's far more to a response than just the content. So iterating over a response makes little sense in my personal opinion. As it is, the API for iterating over the content is complete for both values of `pre_fetch`.

Stick around, I'm sure there's far more you can contribute elsewhere in the project.
",sigmavirus24,kennethreitz
992,2012-12-06 21:56:27,"@mwielgoszewski just to elaborate on @kennethreitz's comment.

With the `pre_fetch=False` option, your solution will not work for `__len__` when the server doesn't provide a Content-Length header. As for iterating over the response, there's far more to a response than just the content. So iterating over a response makes little sense in my personal opinion. As it is, the API for iterating over the content is complete for both values of `pre_fetch`.

Stick around, I'm sure there's far more you can contribute elsewhere in the project.
",sigmavirus24,mwielgoszewski
990,2012-12-06 16:53:43,"Hi @ekratskih, as you might be able to tell, this is a problem with urllib3 (which I maintain). I welcome you to submit a pull request with a similar fix directly to urllib3, but please include a unit test. https://github.com/shazow/urllib3
",shazow,ekratskih
990,2012-12-06 18:23:54,"Thanks for the report @ekratskih, and thanks for taking this off our hands @shazow! :cake:

Closed as 'not our fault'.
",Lukasa,ekratskih
990,2012-12-06 18:23:54,"Thanks for the report @ekratskih, and thanks for taking this off our hands @shazow! :cake:

Closed as 'not our fault'.
",Lukasa,shazow
989,2012-12-06 21:52:26,"Also @gdamjan it was already decided if I remember correctly so you can close this if you feel your needs were met.

At this point, with the refactor coming up we could change it to half the current size since we can really announce the breaking changes then. But that still wouldn't fix his problem. To try to phrase this how @kennethreitz will see it, 90% of people's cases will be sufficiently met by this default, and probably 10% will be affected negatively. He likes to ignore that 10% if possible. (Paraphrasing from one of his talks.)
",sigmavirus24,gdamjan
989,2012-12-06 22:14:54,"@slingamn yes, the first item on your list I believe is related
",sigmavirus24,slingamn
989,2012-12-06 22:34:26,"@sigmavirus24: Good point well made. =D

Nevertheless, I'd be inclined to go slightly smaller. 512 is tempting.
",Lukasa,sigmavirus24
988,2012-12-05 16:29:35,"@SmithSamuelM you should probably wait until the great refactor (#970) is over before you look into adding it, but at that, I think the logging you're looking for belongs in (if it doesn't already exist in) urllib3.
",sigmavirus24,SmithSamuelM
988,2012-12-22 11:15:36,"I'm sure @shazow would be delighted to have extra logging in urllib3. =D
",Lukasa,shazow
982,2012-12-23 11:32:02,"@vlaci 
Have you seen https://github.com/shazow/urllib3/blob/master/dummyserver/server.py?
",piotr-dobrogost,vlaci
981,2012-12-01 19:41:09,"Check out the `magic_json` branch on my github3.py project. The hook @piotr-dobrogost mentioned can be seen in action there and is written for my needs.
",sigmavirus24,piotr-dobrogost
981,2012-12-01 20:35:02,"@sigmavirus24, he referenced #769 in his post, he knows. =)
",Lukasa,sigmavirus24
976,2012-11-29 19:50:46,"@gawel I doubt it. Just use the 0.14.2 tag and install from there, i.e., find what commit it belongs to and do `pip install -e git+git://github.com/kennethreitz/requests.git@sha`
",sigmavirus24,gawel
976,2012-11-29 20:34:57,"Yeah I'm working on a single fork of chardet that works under both python 2 and 3. That's over at sigmavirus24/charade (pull requests welcome) and that will become the new standard. @kennethreitz pulled in a non-functional version to replace chardet with. Any imports referring to chardet should now be broken and replaced by charade. For right now, the charade tests are failing unfortunately and I hope to have them passing in the majority soon. The latest version should be entirely functional.
",sigmavirus24,kennethreitz
975,2013-01-23 18:11:32,"@sigmavirus24: I'd forgotten about this, so thankyou! I'll take a look at it tonight and see how problematic it's likely to be.
",Lukasa,sigmavirus24
975,2013-01-23 19:20:41,"Should auth persist across redirects to subdomains? What about from one subdomain to another? Put another way, should auth persist in these situations (not likely to happen in real life)?

http://google.com/ -> http://mail.google.com/
http://www.google.com/ -> http://mail.google.com/

Thoughts @sigmavirus24 @kennethreitz?
",Lukasa,kennethreitz
975,2013-01-23 19:20:41,"Should auth persist across redirects to subdomains? What about from one subdomain to another? Put another way, should auth persist in these situations (not likely to happen in real life)?

http://google.com/ -> http://mail.google.com/
http://www.google.com/ -> http://mail.google.com/

Thoughts @sigmavirus24 @kennethreitz?
",Lukasa,sigmavirus24
975,2013-01-23 20:46:15,"@kennethreitz, while I like that I think if we see a 307 (after a POST) we should just return that response as if the user had specified allow redirects to be False. It's safer and it follows spec.
",sigmavirus24,kennethreitz
975,2013-01-23 21:04:15,"@kennethreitz; Are you happy to make that the policy? If so, I'm happy to simply say that the user should disallow redirects and not write any code. =P
",Lukasa,kennethreitz
972,2012-11-28 03:15:11,"@matthewlmcclure try out my branch, this should fix your issue.
",sigmavirus24,matthewlmcclure
970,2012-12-15 21:11:21,"@sigmavirus24 the best part is that it's fully mutable :)
",kennethreitz,sigmavirus24
968,2012-11-27 18:47:55,"@kennethreitz: I can't see that requirement in the License instructions...
",Lukasa,kennethreitz
968,2012-11-27 18:50:10,"@Lukasa hmm, it's a long document. I might be crazy ;)
",kennethreitz,Lukasa
968,2012-11-27 18:53:26,"Humorously enough @kennethreitz, you might be thinking of the GPL's requirement to have itself in **every** file ;)
",sigmavirus24,kennethreitz
968,2012-11-27 19:05:52,"@Lukasa I think it's appropriate.

What shall we call it? Chardetter? FuckUnicode?
",kennethreitz,Lukasa
968,2012-11-27 19:10:07,"CharStar? Pointers and whatnot :P

@shazow I was thinking of that too ;)
",sigmavirus24,shazow
968,2012-11-27 19:34:15,"@Lukasa and all they saw when reading your comment was ""Don't bother doing this"" 
",sigmavirus24,Lukasa
967,2012-11-27 18:45:29,"Thanks @kennethreitz.
",matthewlmcclure,kennethreitz
964,2012-11-27 12:09:08,"@Lukasa that's exactly the intention: :cocktail: 
",kennethreitz,Lukasa
963,2012-11-27 16:56:06,"@Lukasa for the time being I thinik it's fine to add it to the Requests documentation since that's where everyone will be looking for the time being.

@matthewlmcclure it won't hurt to submit a PR. On the other hand, afaik, oauthlib doesn't have OAuth2 completely finished. Their API might be finished but having information in the tutorial about that may be misleading for the moment. As for a start-to-finish tutorial using the Twitter API as an example, perhaps we could get Twython's permission to excerpt/adapt some of their code for the example?
",sigmavirus24,Lukasa
963,2012-11-27 16:56:06,"@Lukasa for the time being I thinik it's fine to add it to the Requests documentation since that's where everyone will be looking for the time being.

@matthewlmcclure it won't hurt to submit a PR. On the other hand, afaik, oauthlib doesn't have OAuth2 completely finished. Their API might be finished but having information in the tutorial about that may be misleading for the moment. As for a start-to-finish tutorial using the Twitter API as an example, perhaps we could get Twython's permission to excerpt/adapt some of their code for the example?
",sigmavirus24,matthewlmcclure
963,2012-12-19 05:38:30,"@r1chardj0n3s it'll be updated shortly. The new library that provides the OAuth class is `requests-oauthlib`
",kennethreitz,r1chardj0n3s
962,2012-11-26 20:24:54,"@Lukasa I should have added that I'm +10 on waiting for @kennethreitz to weigh in. ;)
",sigmavirus24,Lukasa
961,2012-11-25 22:25:21,"@Lukasa : I'm not surprised the web site is misbehaving (I should have probed more thoroughly)  --  thanks for the mitigation tip.
",inactivist,Lukasa
961,2012-11-25 22:27:29,"@inactivist: No worries, it's not always easy to probe. Besides, your bug report was awesome: thorough and contained tons of information. Thanks!
",Lukasa,inactivist
961,2012-11-25 22:31:39,"@Lukasa: Thanks for the positive feedback.  I aims to please!

I'm definitely a requests newbie, but so far I love it.  And I have to say, the response here (on a Sunday, no less!) was freaking awesome.  Thanks, @all!
",inactivist,Lukasa
960,2012-11-25 23:39:31,"@piotr-dobrogost both ;)

Also, if a separate repository is not acceptable to @kennethreitz, my next choice would be to add a sort of ""Recipes"" section to the docs which could include hook recipes.
",sigmavirus24,piotr-dobrogost
959,2012-11-26 01:12:26,"@kennethreitz [PEP386](http://www.python.org/dev/peps/pep-0386/) actually allows for the syntax I suggested. See this [section](http://www.python.org/dev/peps/pep-0386/#the-new-versioning-algorithm) in specific.
",sigmavirus24,kennethreitz
957,2012-11-25 09:34:32,"@sigmavirus24 Re: handle Python 2 and 3, I'd like to suggest [Pythonbrew](https://github.com/utahta/pythonbrew).
",Lukasa,sigmavirus24
957,2012-11-25 12:34:33,"@Lukasa very interesting. Thanks! I'll look more closely at it later tonight or tomorrow. 
",sigmavirus24,Lukasa
956,2012-11-26 19:01:12,"I'm not that surprised that it broke. =(

When @idan can work out what the break is I'll open another PR to fix the issue.
",Lukasa,idan
953,2012-11-24 00:02:01,"@piotr-dobrogost I saw your pull request after I had made the fix, and since it was closed I figured I could open this one. I don't really care which solution is merged as long as it solves the problem :).
",heyman,piotr-dobrogost
953,2012-11-26 08:44:39,"@kennethreitz 

I'm curious why didn't you merge my 6 months old pull request #671 which solves problem for all cases but you merged this one which does not? Thanks.
",piotr-dobrogost,kennethreitz
953,2012-11-26 08:46:03,"@piotr-dobrogost the new one is far simpler. The less code the better.
",kennethreitz,piotr-dobrogost
951,2012-11-24 11:24:35,"@Lukasa @kennethreitz #939 solves these problems without any major disadvantage I can think of. Furthermore it has the slight advantage of letting the package maintainers easily add separate versions of any other package `requests` may depend on in the future without having to follow this same forking approach everytime. But this is just my humble opinion, of course :-)
",ghost,kennethreitz
950,2013-08-29 12:43:57,"@swapnilpatne The answer depends entirely on how the upstream API expects the data to be delivered. Most likely it wants multipart data. If that's the case, Requests does not provide an easy abstraction for this use case. You'd need to build the correct multipart body yourself and provide it as opaque data for Requests to send.

Unfortunately, providing advice on how to do this is quite a bit beyond the scope of this bug tracker. =) May I recommend asking the question at Stack Overflow, or attempting to contact someone in our IRC chatroom?
",Lukasa,swapnilpatne
945,2012-11-18 21:21:28,"@Lukasa py.test is still the best :)
",piotr-dobrogost,Lukasa
944,2012-11-19 01:38:46,"@kennethreitz is the plan to fix it for 3.2, or to wait until Py3.3?
",sybrenstuvel,kennethreitz
944,2012-11-23 10:24:06,"Had to be done, I just didn't want to be the one to do it because @kennethreitz might make me maintain it. The idea of working with OAuth keeps me up at night.
",Lukasa,kennethreitz
944,2012-11-23 10:29:55,"@sybrenstuvel: Sucks to be you. =D

@kennethreitz: Mm, there's a lot on your plate. I tried to go through and close a few issues last week, because it had gotten a bit out of control. ATM I'm trying to triage new issues so I can keep the trivial ones off your radar.
",Lukasa,kennethreitz
944,2012-11-23 10:29:55,"@sybrenstuvel: Sucks to be you. =D

@kennethreitz: Mm, there's a lot on your plate. I tried to go through and close a few issues last week, because it had gotten a bit out of control. ATM I'm trying to triage new issues so I can keep the trivial ones off your radar.
",Lukasa,sybrenstuvel
944,2012-11-23 10:31:17,"@Lukasa you're doing an absolutely kick-ass job, and you've made this project twice as sustainable as it already was. This Thanksgiving, I'm thankful for you :P

:sparkles: :cake: :sparkles:
",kennethreitz,Lukasa
944,2012-11-23 10:33:08,"@Lukasa haha thanks :P

@kennethreitz and @Lukasa thank you both for the great lib! My work has become a lot easier thanks to you guys.
",sybrenstuvel,kennethreitz
944,2012-11-23 10:33:08,"@Lukasa haha thanks :P

@kennethreitz and @Lukasa thank you both for the great lib! My work has become a lot easier thanks to you guys.
",sybrenstuvel,Lukasa
944,2012-11-23 11:03:56,"@sybrenstuvel: In all seriousness, I run around, patch small holes, close issues and write docs. The general awesomeness of this library was in place before I got to this project, I'm just basking in reflected glory. =)

@kennethreitz: Thanks Kenneth, always affirming to know that my contribution is valuable. =D This thanksgiving (and the one before, and the one before that...) I'm thankful for your ludicrous productivity. If I was half as productive as you I'd be way better paid. =P
",Lukasa,kennethreitz
944,2012-11-23 11:03:56,"@sybrenstuvel: In all seriousness, I run around, patch small holes, close issues and write docs. The general awesomeness of this library was in place before I got to this project, I'm just basking in reflected glory. =)

@kennethreitz: Thanks Kenneth, always affirming to know that my contribution is valuable. =D This thanksgiving (and the one before, and the one before that...) I'm thankful for your ludicrous productivity. If I was half as productive as you I'd be way better paid. =P
",Lukasa,sybrenstuvel
938,2012-11-15 20:03:16,"As @kennethreitz pointed out, the org needs a logo. I'm design-blind and have no taste, so someone else should take that on. =D
",Lukasa,kennethreitz
936,2012-12-02 19:28:08,"@sigmavirus24 is correct
",kennethreitz,sigmavirus24
935,2012-11-10 17:38:01,"@kennethreitz 
What's great about using `files` as the name of the parameter for sending multipart data? Multipart data has nothing to do with sending files. Using (abusing) `files` to send data when there already exists `data` param is illogical. The fact it's wrong comes from the fact that every file is kind of data but data is not always a file. If anything we could expect users to pass files to `data` param but not the other way around like it is now. Sending files may require giving more metadata than sending plain data so it's kind of ok to have additional `files` param for this purpose. But there's no justification for forcing users to pass their data to `files` param if all they want is to send _data_ not files. `Files` param is currently abused to send multipart data only because it happens that uploading files uses multipart transfer encoding in http which is just implementation detail and as such shouldn't be exposed in any way through the API.

Also, could you elaborate on what's wrong with tuples as `files` value?
",piotr-dobrogost,kennethreitz
935,2015-07-24 23:39:44,"@vidula-mediamath questions belong on [StackOverflow](https://stackoverflow.com/questions/tagged/python-requests)
",sigmavirus24,vidula-mediamath
935,2016-01-31 07:58:37,"@kennethreitz It already exists [here](http://toolbelt.readthedocs.org/en/latest/uploading-data.html#streaming-multipart-data-encoder).
",Lukasa,kennethreitz
934,2012-11-13 00:50:56,"Use git-svn @mastahyeti ;)
",sigmavirus24,mastahyeti
934,2012-11-15 11:02:17,"@Lukasa I'd feel much more comfortable with that myself :)
",kennethreitz,Lukasa
934,2012-11-15 11:08:19,"Cool. I'll have a think this evening about how best to handle this. Maybe something in the docs.

In the meantime, @mastahyeti, how do you feel about acting as maintainer of a Requests NTLM authentication package?
",Lukasa,mastahyeti
934,2012-11-15 13:12:01,"Sounds like a good idea, @sigmavirus24. I won't look into it until I finish work, but that combined with some documentation might be a good way to keep everything together.
",Lukasa,sigmavirus24
934,2012-11-15 14:09:22,"Documentation makes everything better. Let's just wait on @kennethreitz to make the org if he agrees. 
",sigmavirus24,kennethreitz
934,2012-11-15 14:18:59,"@kennethreitz Yeah, that occurred to me too. I wouldn't have proposed moving it: it's yours, and besides, we'd break ALL THE LINKS. I then thought about mirroring requests, but that's also a bad idea: there should only be one place on Github where you can go for requests itself.

Otherwise, that might be a decent idea. What auth stuff would we move? (This, obviously, but what else would you like to pull out?)
",Lukasa,kennethreitz
934,2012-11-15 14:23:33,"_/me casually deletes out-of-date post._

Awesome, sounds good to me. @mastahyeti, you cool with maintaining a repo there instead of this pull request?
",Lukasa,mastahyeti
934,2012-11-15 14:35:41,"@mastahyeti, you should have push access to the repo now :)
",kennethreitz,mastahyeti
931,2012-11-16 22:36:34,"@jric: Is this still a problem?
",Lukasa,jric
927,2012-11-24 03:47:42,"@Lukasa there shouldn't be such a parameter ;)
",sigmavirus24,Lukasa
927,2012-11-24 08:45:39,"@Lukasa Yes, it does, thank you for thorough answer. And i just tested, the #44 would be satisfied now that requests actually send Basic right away. Everything is consistent now.

Thank you.
",temoto,Lukasa
924,2012-11-24 15:55:34,"Yeah, there are some that don't, but if @aleray can handle his particular problem using HEAD then that will work well. =D
",Lukasa,aleray
924,2012-11-25 01:56:24,"@aleray: you can do that today:


",kennethreitz,aleray
924,2012-11-25 01:59:27,"On Sat, Nov 24, 2012 at 05:56:36PM -0800, Kenneth Reitz wrote:

> @aleray: you can do that today:
> 
> 

I was just thinking of this. With a normal socket (or file-descriptor-like 
API), you can seek backward to the beginning of the file, can you do that with 
the raw socket? If so, the peek method becomes trivial.
",sigmavirus24,aleray
924,2012-11-25 17:17:43,"

@kennethreitz
But this is not peeking what OP askes for but reading which would influence data returned by the api like `.content` etc., right? Is the untold assumption that a client must remember to prepend data read outside of the api to what api returns later?

It looks like it would be possible to do with slightly modified version of [`tee`](http://docs.python.org/2/library/itertools.html#itertools.tee) from `itertools`. The modification would be to allow for removing given deques so that they would not receive and accumulate further data.

UPDATE

This is not as straightforward as `iter_content` is a generator not an iterator. Still I think it's doable by making `Response.raw` an iterator and teeing it.
",piotr-dobrogost,kennethreitz
924,2012-12-02 18:07:47,"@piotr-dobrogost `raw` is a file-like object, no? Can you not iterate over it? If not, that might be a nifty feature to add to urllib3 (if I remember correctly that raw is a urllib3 object).
",sigmavirus24,piotr-dobrogost
924,2013-01-26 18:21:01,"@aleray, did @piotr-dobrogost's solution satisfy your needs?
",sigmavirus24,aleray
924,2013-01-26 18:21:01,"@aleray, did @piotr-dobrogost's solution satisfy your needs?
",sigmavirus24,piotr-dobrogost
917,2012-11-23 19:38:38,"I think passing params without values should be supported by `params` parameter. Currently setting param's value to `None` in `params` dict makes Requests skip adding the param to a query string. I propose to change this behavior so that in such case param is added without value. @kennethreitz What do you think?
",piotr-dobrogost,kennethreitz
917,2012-11-24 03:46:43,"@piotr-dobrogost what about the following case:



With your proposed change, `response.request.url == '...?param1&param2=default2''`. Perhaps we could use `''` an empty string instead (if in fact we're realistically considering this)?
",sigmavirus24,piotr-dobrogost
917,2012-11-24 13:29:41,"Right, I forgot about this scenario. Related issue #378

@airnandez
Would `https://www.example.com/?versioning=` work in your case?
",piotr-dobrogost,airnandez
917,2012-11-25 04:07:55,"@piotr-dobrogost I meant implement `''` as a special-case that wouldn't add the `=`
",sigmavirus24,piotr-dobrogost
917,2012-11-25 12:37:11,"@piotr-dobrogost I love how you seem to find the specifications when they're needed.
",sigmavirus24,piotr-dobrogost
917,2012-11-25 23:48:30,"But yeah, Requests does conform to the spec (proof for anyone who's wary to believe me and @piotr-dobrogost):


",sigmavirus24,piotr-dobrogost
917,2012-11-26 03:12:35,"Hello, OP here,

@piotr-dobrogost: ""Would https://www.example.com/?versioning= work in your case?""
Unfortunately not. The REST APIs I need to use requires the query string to have the form `?versioning` or `?acl` (i.e. without '=' and without value). 

@piotr-dobrogost: ""That's why I would base introduction of support for free form query string based on real world usage of such query strings.""
See for instance the [Google storage API](https://developers.google.com/storage/docs/reference-methods) or [Amazon S3 REST API](http://docs.amazonwebservices.com/AmazonS3/latest/API/RESTBucketGETversioningStatus.html).
I just tested against Amazon S3 server and can confirm that a query string of the form `?versioning=` is not accepted.

From the `Requests` user point of view, it would be nice to be able to express the contents of the query string in the `params` argument. From this exchange, I understand that associating a value of `None` or `''` (empty string) to a key in the params dict is not a viable solution. I would therefore suggest to allow for `Requests` users to provide the pre-formatted query string in the `params` argument. This user-provided query string should be non URL-encoded, for instance:



I think this way of doing would be cleaner for the user than to embed the query string in the URL (even if this works in my particular case) and not to use the `params` argument. However, I acknowledge that this is very subjective.
",airnandez,piotr-dobrogost
917,2012-11-26 07:56:58,"> it would be nice to be able to express the contents of the query string in the `params` argument

Totally agree. It's misleading at best to have to embed query string into url in spite of the fact there's `params` param. Making `params` support pre-formatted query string goes in right direction. However having one parameter for both makes it impossible to have both at the same time. I would suggest adding new parameter `query_string` and combining contents of it with contents of `params` and the query string part of the url itself.
@kennethreitz Is this ok?
",piotr-dobrogost,kennethreitz
913,2012-11-16 22:29:09,"@kennethreitz I don't see `encode_urls` anywhere.
",sigmavirus24,kennethreitz
910,2012-10-26 13:03:04,"@alex-ethier you have to also look at the response provided from `'http://httpbin.org/post'` It interprets that as form data. From the same code block you referenced:



Try it yourself with HTTPbin. You'll be able to see how the server interprets it yourself.

And I agree that the docs may seem to be misleading in this sense, but they do say:

> Typically, you want to send some form-encoded data — much like an HTML form. To do this, simply pass a dictionary to the data argument. Your dictionary of data will automatically be form-encoded when the request is made

Also right below the code block you reference, there's this:

> There are many times that you want to send data that is not form-encoded. If you pass in a string instead of a dict, that data will be posted directly.

And the example directly beneath it is for the GitHub API (which I have [experience](/sigmavirus24/github3.py) with) which accepts JSON requests.

---

And please delete your gist entirely. You have the client_{key,secret} there and that's a terrible idea.
",sigmavirus24,alex-ethier
910,2012-10-26 17:31:54,"@sigmavirus24 Thank you for your prompt reply.

I'm realizing that I wasn't clear enough about what I'm trying to accomplish.  Sorry about that.

We are a PaaS provider. I'm writing a client for the public api of our service.  ( I left the api key and secret in the gist since they are from a dev server running on my laptop. ) 

One of the call I have to make is a POST to http://localhost:8003/jobs.  This call requires a job parameters JSON data in the request body such as:



and also requires a signed oauth header.  Our api implement 2-legged oauth with header signature type.

I tried bunch of different variations:

---

if I pass a simple dictionary structure as data such as:



requests generate the following request:



It includes an Authorization header, set the Content-Type to application/x-www-form-urlencoded and some data in the body as I would expect.

---

if I replace the simple dictionary structure above with something more complex, such as:



requests generates a traceback:



---

if i pass JSON encoded string as data such as: 



requests generate the following request:



It contains the JSON data as string but no Authorization header and not content-type

---

if i pass JSON encoded string as data and specify the content type such as: 



requests generate the following request:



It contains JSON data, Content-Type but no Authorization header

---

So basically the client I'm writing has to be able to send json data and must contain an Authorization header.
Is this possible using the requests module? 

I have successfully built a ruby client that does what I need.  I like working with requests and really want to figure this out so I can do the same with requests.

Thank you for your time
",alex-ethier,sigmavirus24
910,2012-10-26 21:11:59,"Thank you @sigmavirus24.

@kennethreitz, Any thoughts on this? 

Thanks!
A
",alex-ethier,kennethreitz
910,2012-10-26 21:11:59,"Thank you @sigmavirus24.

@kennethreitz, Any thoughts on this? 

Thanks!
A
",alex-ethier,sigmavirus24
910,2012-11-24 20:29:57,"Thankyou @idan! I think this means we've got a bug in the OAuth code here. I'll fix it up. Do you mind if I notify you when I've done it so you can take a quick look?
",Lukasa,idan
910,2012-11-24 20:30:56,"Go for it. Open a ticket on idan/oauthlib :)  

On Saturday, November 24, 2012 at 10:30 PM, Cory Benfield wrote:

> Thankyou @idan (https://github.com/idan)! I think this means we've got a bug in the OAuth code here. I'll fix it up. Do you mind if I notify you when I've done it so you can take a quick look?
> 
> —
> Reply to this email directly or view it on GitHub (https://github.com/kennethreitz/requests/issues/910#issuecomment-10683343).  
",idan,idan
910,2013-02-13 13:02:58,"Any updates on this? I think there were some changes to requests/requests-oauth recently that were important for 0.3. @frankslaughter and @alex-ethier any updates on this? 
",sigmavirus24,alex-ethier
910,2013-02-13 13:02:58,"Any updates on this? I think there were some changes to requests/requests-oauth recently that were important for 0.3. @frankslaughter and @alex-ethier any updates on this? 
",sigmavirus24,frankslaughter
910,2013-02-20 17:27:03,"@idan @Lukasa any opinions?
",sigmavirus24,idan
910,2013-02-20 17:27:03,"@idan @Lukasa any opinions?
",sigmavirus24,Lukasa
910,2013-02-20 21:50:44,"@daycoder: Goddamn I hate OAuth. Ok, so, `requests-oauthlib` _should_ correctly handle whether or not to include the body data in signing. What kind of data are you sending?
",Lukasa,daycoder
910,2013-02-21 10:52:05,"@Lukasa You don't know how happy you've made me by saying ""Goddam I hate OAuth"" :D

http://api.projectplace.com/index.php/DocumentUpload is the one that was giving me problems. The body content is just the contents of a file.



(This works with my local copy `prepare` bodge).
",daycoder,Lukasa
908,2012-10-24 18:26:22,"@bmannix
See issue #361.

@kennethreitz
How about contrib module?
",piotr-dobrogost,kennethreitz
908,2012-10-24 18:26:22,"@bmannix
See issue #361.

@kennethreitz
How about contrib module?
",piotr-dobrogost,bmannix
907,2012-11-28 03:08:28,"@etherealite do you realize how many different kinds of environments exist right? Do you realize the amount of work:;

A) to find all of them
B) to support all of them
C) to track all of them

On top of that, why are you bothering with Arch's package repository and not using pip? In fact, why does anyone bother with that?
",sigmavirus24,etherealite
907,2012-11-28 13:31:49,"@Lukasa ironic you say that given how many people's Arch installs get fubarred because of the rolling release model that has notoriously had all sorts of problems.
",sigmavirus24,Lukasa
907,2012-11-28 16:18:48,"@piotr-dobrogost in this case there seems to be one HUGE issue with Arch's python package requirement: **they only support python3** Look at the command they **require** you to use for an Arch package, it explicitly requires python2. What if you want to install under python3? Also, when would a system package every be preferable to using pip, especially across multiple versions?
- System packages cannot be used under virtualenvs so they're useless there.
- Arch seems to only support python 2, so much for all the work that goes into compatibility towards python 3
- Package maintainers occasionally make their own changes without reporting them upstream
  - this begs the question, why should we support a decision we didn't make?
- Package maintainers are occasionally required to use options, e.g., `--optimize=1`, that put the pressure on us to revert explicit design decisions
  - Following arbitrary guidelines will only lead to insanity, tracking them to further insanity.
  - Why follow arbitrarily decided guidelines when we have a package manager that works perfectly for us?
",sigmavirus24,piotr-dobrogost
907,2012-11-28 19:47:48,"@Lukasa that's a much better explanation than I wrote. +1 good sir
",sigmavirus24,Lukasa
907,2012-11-29 13:13:06,"I agree with @slingamn and have a feeling the whole discussion here went in the wrong direction. It's not task of any Python library to set variables such as `PYTHONDONTWRITEBYTECODE` similar to how it's not its task to configure logging. Both are tasks of users of the library. Conclusion, `PYTHONDONTWRITEBYTECODE` should have never been manipulated by Requests. This pull removes manipulation so it fixes the problem. We should merge this and close this issue as fixed.
",piotr-dobrogost,slingamn
907,2012-11-29 13:29:44,"@sigmavirus24: I :heart: you
",kennethreitz,sigmavirus24
907,2012-11-29 13:55:10,"@sigmavirus24 how close is it to being in a usable state?
",kennethreitz,sigmavirus24
907,2012-11-29 13:58:37,"Hopefully ~1 week. I haven't changed anything that would change the code, I'm 
just doing the 2to3 work and making it so that flake8 won't kill vim whenever 
I open a file (when possible). Also there's not a single test for this, so 
having an easy guide as to it working in python3 won't help.

I'm going to reference that guide from Dive into Python3 though to make sure I 
haven't missed anything. And be sure, the latter part (pep8-ifying the files) 
is the time consuming part.
On Thu, Nov 29, 2012 at 05:55:21AM -0800, Kenneth Reitz wrote:

> @sigmavirus24 how close is it to being in a usable state?
> 
> ---
> 
> Reply to this email directly or view it on GitHub:
> https://github.com/kennethreitz/requests/pull/907#issuecomment-10847816
",sigmavirus24,sigmavirus24
905,2012-11-05 08:14:39,"@dahpgjgamgan: I looked at the repo, and it seems that foxx's repository is actually far behind of kennethreitz's. If I get the time, I'll test if it works. For now, we have re-written our code to use `urllib2`. So it's not very pressing for us.

Additionally, one can clone a specific branch with the '-b' flag:



I suppose the one in question is `develop`?
",exhuma,dahpgjgamgan
905,2012-11-06 21:00:31,"@dahpgjgamgan Ahh yes i ended up manually applying the patches one by one by hand before i realized i could just clone his branch (i'm a bit of a git newb).  It ended up working swimingly and i'm using a local copy of requests bundled with my project.   I would really prefer that the support/fixes be merged into requests's master branch and a new release was cut so i can get my org to sign off on that instead of having to incl an extra library to support (like requests is doing with urllib3).  I didn't want to go the urllib2 route as i need concurrency/keep alives to use grequests
",aleks-mariusz,dahpgjgamgan
905,2012-11-26 18:05:26,"@machinae two things:

First, @kennethreitz is working on a rewrite using transport adapters which should alleviate or fix this issue.

Second, if you want the patch at shadow/urllib3 to get through so it can be used here, go and work on that issue. It's already known to be am issue that affects plenty of people but complaining won't help anyone. I'm sure @shazow would love help on that pull request but until he gets some, it isn't going anywhere fast.
",sigmavirus24,machinae
905,2012-11-26 19:03:57,"@sigmavirus24: +1.

The problem is in urllib3, not Requests. I'm only leaving the issue open to attempt to prevent too many people opening duplicates. Problems in urllib3 should be raised against and resolved in urllib3.
",Lukasa,sigmavirus24
905,2012-11-27 23:07:51,"@Lukasa I was under the impression this was already fixed in urllib3 as per https://github.com/shazow/urllib3/pull/68#issuecomment-10127574 and Requests just needs to be updated with the latest version of urllib3.
",machinae,Lukasa
905,2012-11-28 00:41:33,"That's quite wrong but understandable 

Ilya Lichtenstein notifications@github.com wrote:

> @Lukasa I was under the impression this was already fixed in urllib3 as
> per https://github.com/shazow/urllib3/pull/68#issuecomment-10127574 and
> Requests just needs to be updated with the latest version of urllib3.
> 
> ---
> 
> Reply to this email directly or view it on GitHub:
> https://github.com/kennethreitz/requests/issues/905#issuecomment-10781919
",sigmavirus24,Lukasa
896,2013-01-25 04:00:43,"@michaelhelmick any updates?
",sigmavirus24,michaelhelmick
896,2013-02-09 23:10:59,"@michaelhelmick since the fixes are currently in Requests and you're just waiting on requests_oauthlib, I'm going to close this, especially since it doesn't seem to affect 1.1.0. If you guys experience still experience it, let us know. Thanks.
",sigmavirus24,michaelhelmick
896,2013-02-09 23:11:59,"Also, @michaelhelmick, I just checked and 0.3.0 was [released](https://crate.io/packages/requests-oauthlib/).
",sigmavirus24,michaelhelmick
895,2012-11-27 19:03:56,"@kennethreitz don't forget to reopen this one too ;)
",sigmavirus24,kennethreitz
890,2012-10-12 23:45:55,"@chrisdrackett _cough_ check again ;)
",kennethreitz,chrisdrackett
888,2012-11-24 11:10:38,"As shown above we actually have two open pull requests to resolve this issue, so I'm going to direct any discussion that way. Thanks for the report @flytwokites!
",Lukasa,flytwokites
886,2012-11-17 10:45:43,"I'm sorry, that was a lousy explanation, I was pretty tired when I wrote it. =)

The long answer is that Kenneth is refactoring `requests` along [these lines](http://kennethreitz.com/the-future-of-python-http.html), which you can see in #895. This means that Kenneth is pretty unlikely to get to this any time soon (this issue is already a month old and I only just got to it, and I've got way more free time than he does). So Kenneth won't be adding this himself, at least, not for a while.

I also won't be making this change, partly because I suck at API design and partly because this looks to me like a 20 percent feature: the vast majority of people will never use it, and those that want it are plenty competent enough to make the change themselves. Obviously, Kenneth might think differently, but I doubt he'll look at this issue for a while.

For that reason, I suggested to @kravietz that he write the change himself and submit a Pull Request. The worst case scenario there is that Kenneth decides he doesn't want to change the API for this feature, and so @kravietz ends up maintaining a fork; best case, he likes it and merges it.

If you want a second opinion, please let me know and I'll nudge someone else to take a look at this issue. =)

**tl;dr:** No-one in the last month expressed any interest in making this change, Kenneth won't look at this for ages, and I have no plans to make this change, so I judged this issue as dead.
",Lukasa,kravietz
886,2015-08-31 12:40:47,"@jayfk I'm certain there's no other information about this other than what's in the issue.
",sigmavirus24,jayfk
885,2016-05-04 11:56:02,"@amitt001 That should work in the current version v2.10.0, and indeed in all of the v2.X.X series release.
",Lukasa,amitt001
884,2012-10-09 02:40:41,"@piotr-dobrogost `CJK` is a collective term for `Chinese`, `Japanese`, and `Korean`. You can find more details [here](http://en.wikipedia.org/wiki/CJK_characters).
",everbird,piotr-dobrogost
882,2012-10-07 18:52:43,"> I see no need to split.

I don't quite catch what you mean @kennethreitz 
",sigmavirus24,kennethreitz
882,2012-11-24 11:57:26,"@kennethreitz: Do we want to fix this, or just stop claiming that we support Python 3.1, seeing as we don't test on it anyway?
",Lukasa,kennethreitz
882,2012-11-27 02:04:10,"Well in that case, perhaps removing the guarantee of 3.1 compatibility is the way to go. It would seem @Lukasa nipped this in #955 though, so this should be fixed. @Arfrever care to confirm?
",sigmavirus24,Lukasa
882,2012-11-27 02:04:10,"Well in that case, perhaps removing the guarantee of 3.1 compatibility is the way to go. It would seem @Lukasa nipped this in #955 though, so this should be fixed. @Arfrever care to confirm?
",sigmavirus24,Arfrever
882,2012-11-27 04:09:58,"@Arfrever there's this from Kenneth:

> I want to officially support 3.3 and up. Anything else is nice-to-have :)

But the other issue is that automating (regression) tests for python 3.1 is a pain. If someone wants to volunteer a server, they can use any of the GitHub API wrappers to verify the tests work on 3.1 and post as much.
",sigmavirus24,Arfrever
882,2012-11-27 10:12:06,"@Arfrever: Such a project can continue to use Requests 0.14.2 (or whatever version we get to where we finally break 3.2) with no ill effects.

Regarding supported versions, if Travis doesn't test 3.1 for us then we have to assume that anyone raising a PR has not tested on 3.1 either. This requires Kenneth (or another contributor) to patch a copy of Requests with the PR and test it themselves, which is boring and time-inefficient. It's for this reason that bugs like this one crop up in the library: we simply do not have time to catch regressions.

If I thought supporting 3.1 and 3.2 was a long-term priority I'd try to use a local box to run these tests automatically, but the reality is we're simply going to stop supporting these versions. To be clear, we aren't going to break these _right now_: I'm not going to raise a PR tomorrow that puts unicode literals on every string, for instance. 3.1 and 3.2 will likely continue to function until 1.0, when Kenneth finishes his Transport Adapters stuff.

In the meantime, the original issue is fixed, so I'm closing it. Thanks for the bug report, please keep them coming! =)
",Lukasa,Arfrever
882,2012-11-27 16:28:41,"@Lukasa well explained. Thank you.
",sigmavirus24,Lukasa
881,2012-10-05 15:55:33,"@shoibalc I thought I saw an email about you mentioning that 'http_proxy' was the more widely used environment variable... am I hallucinating?
",sigmavirus24,shoibalc
879,2012-11-17 10:22:52,"By default, `requests` trusts any environment variables the system has in place, and assumes you want to use them (which is usually the default behaviour of UNIXy programs). You can disable this behaviour either by using @timsavage's method (which is clever, nice workaround), or by setting the `trust_env` config value to `False`, like so:



You can see everything you can change with the config dictionary [here](http://docs.python-requests.org/en/latest/api/#configurations).

Hope that helps!
",Lukasa,timsavage
879,2013-10-28 22:50:53,"Hi @Lukasa , I was having this issue with Requests 1.2. After reading this thread, I thought it will go away on upgrading to 2.0.1 (which contains your fix). However, it did not fix it for me. 
The only way I got around it was to add the following code: 


",tanmay9,Lukasa
879,2013-10-29 18:28:56,"@tanmay9 How are you setting the environment variable in question?
",Lukasa,tanmay9
879,2015-03-16 02:08:58,"Neither of these appear to work for me.

@timsavage's method does nothing at all and the config= trick results in:

TypeError: request() got an unexpected keyword argument 'config'

This is python 3.4.0 and requests 2.2.1.

I had to resort to @tanmay9's method of setting the no_http environmental.

Just FYI.
",bhechinger,tanmay9
879,2015-03-16 02:08:58,"Neither of these appear to work for me.

@timsavage's method does nothing at all and the config= trick results in:

TypeError: request() got an unexpected keyword argument 'config'

This is python 3.4.0 and requests 2.2.1.

I had to resort to @tanmay9's method of setting the no_http environmental.

Just FYI.
",bhechinger,timsavage
875,2012-11-27 04:18:07,"So to flesh out @vlaci's proposed patch:



should be feasible? I don't have a proxy to test against.

@vlaci if I'm misspeaking please pipe up. It'd even be awesome if you submitted the Pull Request to fix this.
",sigmavirus24,vlaci
875,2012-11-27 16:44:59,"@vlaci awesome. Thanks for finding, reporting and fixing this.
",sigmavirus24,vlaci
871,2012-10-01 16:52:57,"@yegle care to update?
",kennethreitz,yegle
868,2012-09-26 19:32:04,"> Well that was painful

@sigmavirus24 What do you mean? Btw, thanks for creating this issue.
",piotr-dobrogost,sigmavirus24
868,2012-09-29 18:49:51,"Agreed @shazow, I'm probably going to change some of them back and squash the commits before turning that branch into a Pull Request which is probably a while off.
",sigmavirus24,shazow
868,2012-09-29 19:07:21,"@shazow Then please show us which ones are good and why? :) Btw, yours _is_foo helper_ idea was heading in right direction. This is what ABCs in collections module are for although it looks like they specify too few interfaces.
",piotr-dobrogost,shazow
868,2012-09-29 21:11:31,"@piotr-dobrogost For example when the type needs to be checked for py2 vs py3 compatibility.
",shazow,piotr-dobrogost
868,2012-09-29 21:35:29,"@kennethreitz
People use different containers - see for example http://sebsauvage.net/python/snyppets/index.html#dbdict If api is restricted only to standard ones then people can't use their custom made containers.
",piotr-dobrogost,kennethreitz
868,2012-09-29 21:54:46,"@shazow 
I meant the ones we are talking here :) Like checking for dict, list, set etc.
",piotr-dobrogost,shazow
868,2012-09-29 21:56:04,"@piotr-dobrogost A big chunk of the results in @sigmavirus24's grep were for backwards compat. Another chunk was the base case reduction to 'str', which otherwise uses duck typing as you prefer. :) I suspect about half of the rest could be removed/improved.
",shazow,piotr-dobrogost
868,2012-09-29 21:56:04,"@piotr-dobrogost A big chunk of the results in @sigmavirus24's grep were for backwards compat. Another chunk was the base case reduction to 'str', which otherwise uses duck typing as you prefer. :) I suspect about half of the rest could be removed/improved.
",shazow,sigmavirus24
867,2012-09-26 19:42:17,"@sigmavirus24 sounds reasonable!
",sweenzor,sigmavirus24
864,2012-09-23 23:48:31,"I'm with @shazow on this one, and I hadn't even seen it in use with PHP until I saw it [here](http://stackoverflow.com/questions/353379/how-to-get-multiple-parameters-with-same-name-from-a-url-in-php).
",sigmavirus24,shazow
857,2012-11-24 03:52:10,"@ruli look at `requests/models.py` around line 735, status_code will never change as a result of using `iter_lines` or `iter_chunks`. `Response.headers` does not seem to change either unfortunately.

I think there is retry logic recipe (in general) in the docs written as a hook. I'm not 100% sure though. Hope this gives you a hand.
",sigmavirus24,ruli
857,2013-01-26 19:54:24,"@jkl1337 we don't set the options on the sockets and I'm not sure if urllib3 does either (although if it does, it would probably be a good idea to submit the idea to @shazow). That aside, it would be ideal if we could use `select` but that would break Windows support.

We could definitely catch and re-throw the `socket.error`. That's easy. The problem it seems (to me) is that reading from a broken socket isn't raising any exception at all.
",sigmavirus24,jkl1337
856,2012-09-17 13:00:38,"@dangra that's a great point. Although, on 64 bit machines (like I'm on at the moment) I did this:



Just so no one tries to say you're wrong, this is in fact the same problem. I'm also using py26, which probably explains the difference in Errors. Also python3 doesn't apparently have `sys.maxint` so I just used the value returned above from py26 and got the OverflowError.
",sigmavirus24,dangra
856,2012-09-17 14:36:48,"Hrmmm. Is there a workaround for this? Is this a showstopper? Does
downloading a 2GB file with requests break anything else?

On Mon, Sep 17, 2012 at 8:00 AM, Ian Cordasco notifications@github.comwrote:

> @dangra https://github.com/dangra that's a great point. Although, on 64
> bit machines (like I'm on at the moment) I did this:
> 
> > > > import sys
> > > > class A:
> > > > ...     def **len**(self):
> > > > ...             return sys.maxint+1
> > > > ...
> > > > a = A()
> > > > a.**len**()
> > > > 9223372036854775808L
> > > > len(a)
> > > > Traceback (most recent call last):
> > > >   File ""<stdin>"", line 1, in <module>
> > > > TypeError: **len**() should return an int
> > > > class A:
> > > > ...     def **len**(self):
> > > > ...             return int(sys.maxint + 1)
> > > > ...
> > > > a = A()
> > > > len(a)
> > > > Traceback (most recent call last):
> > > >   File ""<stdin>"", line 1, in <module>
> > > > TypeError: **len**() should return an int
> > > > a.**len**()
> > > > 9223372036854775808L
> 
> Just so no one tries to say you're wrong, this is in fact the same
> problem. I'm also using py26, which probably explains the difference in
> Errors. Also python3 doesn't apparently have sys.maxint so I just used
> the value returned above from py26 and got the OverflowError.
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/pull/856#issuecomment-8613139.

## 

-Ben Toews
",mastahyeti,dangra
856,2012-09-17 18:00:54,"On an entirely unrelated note @mastahyeti , it appears your commiter email address and the address you used to sign up for GitHub are not one and the same. You can add that email address to your GitHub profile if you want the committer/author on the diffs to point back to you.
",sigmavirus24,mastahyeti
851,2012-09-24 15:54:33,"@joeshaw If the issue is under VerifiedHTTPSConnection, then please re-open this under https://github.com/shazow/urllib3. :)
",shazow,joeshaw
851,2012-09-24 16:10:15,"@joequery not your fault ;)
",kennethreitz,joequery
851,2013-01-21 21:07:33,"@joequery: Is your specific issue that you need to force SSLv3 on Ubuntu?
",Lukasa,joequery
851,2013-01-21 21:15:47,"@joequery If you are able to run (or update to) requests 1.0, https://github.com/kennethreitz/requests/issues/1083#issuecomment-11853729 might be helpful.
",joeshaw,joequery
851,2013-01-21 21:20:36,"Yes. I'm not trying to cause a stir or anything, it's just that none of the commits since opening the issue have resolved my problem. The issue only occurs on certain sites. University websites seem to be prone to enforcing SSLv3.

@joeshaw #1083 looks like a good solution. However, I'm a bit confused since the code provided says it works for pre 1.0. I don't see examples provided for post 1.0. 

And the when/how/if is pretty much the exact problem I'm having. If there's a way to interface with urllib3 to choose ssl version, I don't see it in the docs. 
",joequery,joeshaw
851,2013-01-21 21:23:22,"@joequery: You aren't creating a stir. =) If you're having a problem, we want to fix it.

#1083 absolutely does **not** work pre version 1. I should know, I wrote it. =) In fact, the first sentence reads:

> Requests does not support doing this before version 1.

The examples provided are for post v1.

You are correct that this is not in the docs, and I don't think it belongs there. If you want a more detailed explanation, I have a [blog post about it](http://lukasa.co.uk/2013/01/Choosing_SSL_Version_In_Requests/).
",Lukasa,joequery
849,2012-09-12 17:39:15,"I actually think this is because `dict.items()` returns an iterator now instead of a list like in python2. This should be fixable by doing `list(dict.items())` but I could be wrong. Another option would be to do:



Since you're not changing the dictionary you're iterating over (although this is just an expansion on what @barberj was say).
",sigmavirus24,barberj
847,2013-06-19 08:05:44,"I'm glad you got something working for your use case, @mlissner!

I doubt Requests will ever have direct support in the main library for the `file:` URL scheme. Aside from Kenneth's point above about it being a security risk, Requests is strictly an HTTP library. `file:` URLs do not use HTTP, so they're outside the Requests use case.

If you wanted a more transparent example, you should be able to use something like this:



Mounting that transport adapter _should_ (I haven't tested it) provide you with the behaviour you want.
",Lukasa,mlissner
847,2013-06-19 19:15:05,"Thanks @Lukasa. I gave that a whirl and tried to make it work for a bit, but I ran into an error I can't get past.

This line is the one causing trouble: https://github.com/kennethreitz/requests/blob/master/requests/cookies.py#L109

From what I can tell, setting response.raw to a file or StringIO object won't work because: 



I'm not sure how to proceed with this, though I read your FTP Adapter code and blog entry (but there's not much else on transport adapters yet!). I'm tempted to say that not allowing file-like objects is a bug, but I know that's been closed before as a wont-fix. I've also gotten the impression that _original_response shouldn't be used for anything (though it is used in cookie.py). So...I'm confused how to make this work properly, whether _original_response shouldn't be used the way it is, and what the proper fix for this should be.
",mlissner,Lukasa
847,2013-06-19 19:37:56,"@sigmavirus24 I took a look at requestions, but don't see any use of adapters (or workarounds) in there. Regardless, I'd rather avoid the dependency if possible.
",mlissner,sigmavirus24
844,2012-09-07 07:05:33,"@slingamn thanks so much for stepping up with this ;)
",kennethreitz,slingamn
844,2012-09-07 15:48:01,"@shazow Because the synchronous network read call will not return until the specified amount of bytes has been read or the stream is closed. Thus, is you use a 1K read size and the server returns lines averaging 100 bytes, you will have to wait for the server to return at least 11 lines before `iter_lines` return any lines and your Python code is executed. If one line is sent every minute, it will take 11 minutes before your code is even aware of the first line sent by the server.
",mponton,shazow
844,2012-09-08 01:17:42,"@mponton these are good points.

Does anyone know what browsers typically do when reading streaming text data without chunked encoding?
",slingamn,mponton
844,2012-10-04 08:42:48,"@slingamn Could you elaborate on point 3 from your initial list?
",piotr-dobrogost,slingamn
844,2013-01-23 08:25:02,"@slingamn pointed out that there are a few issues here that are still unresolved. Sometime today I'll go through and work out which ones still haven't been done.
",Lukasa,slingamn
844,2013-02-10 22:56:32,"@slingamn #4's a documentation problem and needs to be corrected.
",kennethreitz,slingamn
844,2013-02-11 23:22:17,"I would like to jump in because it appears that a bit of the above discussion is based upon a false premise: that a synchronous network receive of _n_ bytes will block until at least _n_ bytes are received (or, presumably, the socket has closed).

This is not, in fact, the case, and Unix network programming would be a shambles if it were — think of the disaster that every network program would face: applications would have to make the horrible decision to either read data byte-by-byte, or block indefinitely if they overestimated, even very slightly, the amount of data about to arrive on a socket. Network programmers would all stand impaled upon the horns of a dilemma. Everyone might use Windows for network programming instead. :)

But, fortunately, the plain normal vanilla blocking synchronous version of the `recv()` call is merciful to us: it only blocks waiting as long as _no data at all_ is ready to be consumed! As soon as _even a single byte_ of data arrives in an incoming packet, your beefy call to `recv(1048576)` will return _that single byte_ and your program is off and running again. There is _no penalty_ for giving `recv()` permission to return lots of data if a lot of data arrives in a single packet, or arrives in several packets while the operating system is still getting around to scheduling your thread again.

It is true that, for those rare exceptional cases where you really _want_ to stay blocked because you really know that you need _n_ bytes before you can do anything useful, there exists a POSIX flag `MSG_WAITALL` that you can pass to `recv()` so that your program really does block until lots of data arrives. But that is a very rare flag to specify, for all of these reasons that we have put on the table here.

So what is the problem here, you ask?

Well, @mponton actually lets the cat out of the bag without knowing it! Look carefully at this phrase from his reply to @shazow:

“…the synchronous network read call…”

“Read” call? What? Who would do a `read()` call on a socket? While POSIX does allow `read()` and `write()` on sockets for those extremely rare cases where you pass a socket to a library that is only designed for talking to files, it's not something you would ever do in a network program—you would lock yourself up waiting for _n_ bytes to arrive even if less than _n_ bytes were available immediately! Who would do _that?_

Why, the author of `httplib`, of course!

Yes, that's right. Instead of simply sitting in a tidy standard `recv()` loop and slowly filling a buffer until an end-of-line is visible, the author of `httplib` **wraps the socket in a fake file-like object with `makefile()` and completely abandons all of the benefits of network programming under Unix!**

They gain a tiny bit of convenience — and maybe, way back when it was written, C-level performance? — by having a Python file-like object watch for the end-of-line character for them. But they **completely disabled the ability to stream live data from the network** by making this choice, probably because they were operating in an era when people read and wrote network payloads whole anyway.

I recommend that Requests move off of `httplib`. It does very, very little for you; it could be re-implemented much more simply with `recv()` and `send()` in a day or two, given the simplicity of HTTP. There is no reason why Python or Requests shouldn't have line-based or content-based iteration that returns the moment that enough data has arrived to satisfy the caller; you just need to move off of a broken implementation that wantonly imposes the semantics of the `read()` call on what is really a socket capable of doing `recv()` if you'll just ask nicely!
",brandon-rhodes,shazow
844,2013-02-11 23:22:17,"I would like to jump in because it appears that a bit of the above discussion is based upon a false premise: that a synchronous network receive of _n_ bytes will block until at least _n_ bytes are received (or, presumably, the socket has closed).

This is not, in fact, the case, and Unix network programming would be a shambles if it were — think of the disaster that every network program would face: applications would have to make the horrible decision to either read data byte-by-byte, or block indefinitely if they overestimated, even very slightly, the amount of data about to arrive on a socket. Network programmers would all stand impaled upon the horns of a dilemma. Everyone might use Windows for network programming instead. :)

But, fortunately, the plain normal vanilla blocking synchronous version of the `recv()` call is merciful to us: it only blocks waiting as long as _no data at all_ is ready to be consumed! As soon as _even a single byte_ of data arrives in an incoming packet, your beefy call to `recv(1048576)` will return _that single byte_ and your program is off and running again. There is _no penalty_ for giving `recv()` permission to return lots of data if a lot of data arrives in a single packet, or arrives in several packets while the operating system is still getting around to scheduling your thread again.

It is true that, for those rare exceptional cases where you really _want_ to stay blocked because you really know that you need _n_ bytes before you can do anything useful, there exists a POSIX flag `MSG_WAITALL` that you can pass to `recv()` so that your program really does block until lots of data arrives. But that is a very rare flag to specify, for all of these reasons that we have put on the table here.

So what is the problem here, you ask?

Well, @mponton actually lets the cat out of the bag without knowing it! Look carefully at this phrase from his reply to @shazow:

“…the synchronous network read call…”

“Read” call? What? Who would do a `read()` call on a socket? While POSIX does allow `read()` and `write()` on sockets for those extremely rare cases where you pass a socket to a library that is only designed for talking to files, it's not something you would ever do in a network program—you would lock yourself up waiting for _n_ bytes to arrive even if less than _n_ bytes were available immediately! Who would do _that?_

Why, the author of `httplib`, of course!

Yes, that's right. Instead of simply sitting in a tidy standard `recv()` loop and slowly filling a buffer until an end-of-line is visible, the author of `httplib` **wraps the socket in a fake file-like object with `makefile()` and completely abandons all of the benefits of network programming under Unix!**

They gain a tiny bit of convenience — and maybe, way back when it was written, C-level performance? — by having a Python file-like object watch for the end-of-line character for them. But they **completely disabled the ability to stream live data from the network** by making this choice, probably because they were operating in an era when people read and wrote network payloads whole anyway.

I recommend that Requests move off of `httplib`. It does very, very little for you; it could be re-implemented much more simply with `recv()` and `send()` in a day or two, given the simplicity of HTTP. There is no reason why Python or Requests shouldn't have line-based or content-based iteration that returns the moment that enough data has arrived to satisfy the caller; you just need to move off of a broken implementation that wantonly imposes the semantics of the `read()` call on what is really a socket capable of doing `recv()` if you'll just ask nicely!
",brandon-rhodes,mponton
844,2013-02-12 00:18:00,"@brandon-rhodes thanks very much, this was extremely illuminating.
",slingamn,brandon-rhodes
844,2013-02-12 00:33:16,"@shazow: I'm all for that! Let me know if you get off the ground with it and I'll do my best to help out.
",Lukasa,shazow
844,2013-02-12 03:15:04,"@slingamn There is a danger here: once the socket has been wrapped with a file-like object by `makefile()`, there is a chance that the stdio buffering will have read a bit _past_ the end of the first line and, after returning that line to you, will hold in its buffer a good bit of data that lay _after_ the line ending.

Usually this is no problem, as when you next issue `readline()` or `read()` the file buffering simply draws data from its buffer first before then turning to the file descriptor for more.

But if you try to go _around_ the stdio input buffering and read directly from the socket, you will be missing the chunk of data (if any) that had been read past the end of the line! Unfortunately, stdio provides no way to introspect a `FILE *` object and rescue the contents of its buffer without also probably inducing the punishment of a blocking `read()`. So once you have even for a moment used a file-like object to do your reading for you, you are committed: you cannot return to raw socket I/O.

It might be possible to craft your own file-like object; I found, for example, that `pywsgi.py` in `gevent` let me provide my own `rfile` object that implements `read()` and `readline()` on top of a socket, and the problem was solved.

It is also possible that you might want a way around the `FILE *` problem, or even a guarantee that no data will be trapped there. I am often wrong about such things, and you may see a way out where I have not yet.

But I think the whole issue is what lead the Python 3 people to ditch `FILE *` I/O entirely and write a new dedicated Python buffering subsystem directly atop POSIX system calls. If only we were working under Python 3, we might not even face these issues. Alas that all of our favorite toys keep us toiling under Python 2!
",brandon-rhodes,slingamn
844,2013-02-12 06:28:24,"@brandon-rhodes `makefile()` is implemented entirely in Python (in 2.7 at least) and is a wrapper around the low-level socket object. It is not a FILE \* object so it has nothing to do with stdio. Its internal buffering is implemented using StringIO. It has a `readline()` method that does exactly what we need (i.e. use `recv()` to get data from the network into a buffer and return immediately when an EOL is encountered) for `iter_lines()` (minus unicode decoding).

@slingamn The file-like object in `Response.raw` is a `urllib3` `HTTPResponse` wrapping a `httplib` `HTTPResponse`. The later uses `makefile()` to wrap the socket object but does no expose that socket object and thus it is not possible to call `recv()` from a `Response` object (well, unless you dig deep, see below).

As an example, here's an ugly kludge that would use an ""optimized"" way to read lines from the network:



Here's an example of the result when reading 10 lines generated at 5 seconds interval with a buffer size of 10K:

Original `iter_lines` (stuck in `read()` for 50 seconds):



""Fixed"" `iter_lines` (properly streaming every 5 seconds):



I'm sorry if my previous comments on network I/O might have been overly simplistic, but in the current context, they seemed right enough.

@brandon-rhodes Is right, the proper way of fixing this would be to either use `recv()` directly or use the `makefile()`-generated file-like object's `readline()` (which does exactly what Brandon describes). Considering that `httplib`'s HTTPResponse class does not ""officialy"" expose the file-like object it uses, using it for our own purpose may not be a good idea. This would be solved if `urllib3` stopped using `httplib` and properly exposed the methods/properties we need to fix this issue correctly but this would require some surgery...
",mponton,brandon-rhodes
844,2013-02-12 06:28:24,"@brandon-rhodes `makefile()` is implemented entirely in Python (in 2.7 at least) and is a wrapper around the low-level socket object. It is not a FILE \* object so it has nothing to do with stdio. Its internal buffering is implemented using StringIO. It has a `readline()` method that does exactly what we need (i.e. use `recv()` to get data from the network into a buffer and return immediately when an EOL is encountered) for `iter_lines()` (minus unicode decoding).

@slingamn The file-like object in `Response.raw` is a `urllib3` `HTTPResponse` wrapping a `httplib` `HTTPResponse`. The later uses `makefile()` to wrap the socket object but does no expose that socket object and thus it is not possible to call `recv()` from a `Response` object (well, unless you dig deep, see below).

As an example, here's an ugly kludge that would use an ""optimized"" way to read lines from the network:



Here's an example of the result when reading 10 lines generated at 5 seconds interval with a buffer size of 10K:

Original `iter_lines` (stuck in `read()` for 50 seconds):



""Fixed"" `iter_lines` (properly streaming every 5 seconds):



I'm sorry if my previous comments on network I/O might have been overly simplistic, but in the current context, they seemed right enough.

@brandon-rhodes Is right, the proper way of fixing this would be to either use `recv()` directly or use the `makefile()`-generated file-like object's `readline()` (which does exactly what Brandon describes). Considering that `httplib`'s HTTPResponse class does not ""officialy"" expose the file-like object it uses, using it for our own purpose may not be a good idea. This would be solved if `urllib3` stopped using `httplib` and properly exposed the methods/properties we need to fix this issue correctly but this would require some surgery...
",mponton,slingamn
844,2013-02-12 06:44:35,"@mponton, you are AWESOME — thank you for stopping me from making this fix more work than was necessary for everyone! My information was outdated. The last time I read `makefile()`, it merely did this with the socket (here I'm quoting from Python 2.3, which happens to be on my hard drive):



I am VERY happy that the modern `makefile()`, even in Python 2.7, has been spruced up and improved and does not impose on Requests all of the problems that I was wildly claiming.
",brandon-rhodes,mponton
844,2013-02-12 06:48:28,"Oh: and, I'd recommend that Requests go with @mponton's solution here and not worry about whether `httplib` “officially” exposes the object or not; the library at this point has been entombed within amber for more than a decade and I would be surprised, given how much code depends upon its every peculiarity, whether it will ever change again at this point. If Python 3.4 or 3.5 does ever invalidate the access to `_fp` then a more difficult solution could be constructed at that point.
",brandon-rhodes,mponton
844,2013-02-12 09:51:27,"Lots of great food for thought here. I agree with @brandon-rhodes that we should do something like @mponton's patch. It looks like the socket module's implementation of `readline` does exactly what we need, and the use of a non-public interface to get the underlying socket object shouldn't be a serious problem.

The observation about UTF-8 is great too. We may not end up supporting `iter_lines` for encodings other than ASCII and UTF-8, but that should be OK as long as we properly clarify.

re. ""universal newlines"": there's a minor issue where currently clients do not expect to get back the `\r` from a line terminated with `\r\n`. But resolving this is probably as simple as returning `line.rstrip()` instead of `line`.

If this works it will resolve every one of my original items, except for 2.
",slingamn,brandon-rhodes
844,2013-02-12 09:51:27,"Lots of great food for thought here. I agree with @brandon-rhodes that we should do something like @mponton's patch. It looks like the socket module's implementation of `readline` does exactly what we need, and the use of a non-public interface to get the underlying socket object shouldn't be a serious problem.

The observation about UTF-8 is great too. We may not end up supporting `iter_lines` for encodings other than ASCII and UTF-8, but that should be OK as long as we properly clarify.

re. ""universal newlines"": there's a minor issue where currently clients do not expect to get back the `\r` from a line terminated with `\r\n`. But resolving this is probably as simple as returning `line.rstrip()` instead of `line`.

If this works it will resolve every one of my original items, except for 2.
",slingamn,mponton
844,2013-02-12 09:54:29,"@kennethreitz when you say that item 4 [here](https://github.com/kennethreitz/requests/issues/844#issuecomment-12587649) is a documentation issue, do you mean that there is currently no implementation of chunked encoding for incoming requests, and that the documentation needs to be changed to clarify that?
",slingamn,kennethreitz
844,2013-02-12 17:14:23,"@brandon-rhodes Well, thanks for the kind words! I did not know older versions of Python implemented the file-like object that way. Interesting. Glad they changed it.

@slingamn `line.rstrip()` will remove all newline characters and whitespaces from the right. This could change the lines in an undesirable way for some clients (maybe someone does expect/want right padding whitespace). I wonder if `iter_lines` should have kept the line break in the first place now... Just so it behaves more like the `file` iterator. A bit late now though... :-)

As for considering my kludgy example a fix, I'm a bit concerned :-) I suppose the `httplib` `HTTPResponse.fp` member probably won't change nor be removed as @brandon-rhodes said. I am more concerned by the access to `urllib3`'s `HTTPResponse._fp` member. Not only is this one underscore-prefixed, `urllib3` is still in active development. Maybe access to an exposed `fp` member (in the short term directly represented by the `httplib`'s own `fp`) could be provided officially by `urllib3` by @shazow ? (This would have implication for the future support/implementation of this now public member though...)

Also, please note that I did not validate any of this on Python 3. I'm still using 2.7 (yes, I'm late to the party, I know). Maybe the socket file-like object implementation or `httplib` module differ on this version.
",mponton,brandon-rhodes
844,2013-02-12 17:14:23,"@brandon-rhodes Well, thanks for the kind words! I did not know older versions of Python implemented the file-like object that way. Interesting. Glad they changed it.

@slingamn `line.rstrip()` will remove all newline characters and whitespaces from the right. This could change the lines in an undesirable way for some clients (maybe someone does expect/want right padding whitespace). I wonder if `iter_lines` should have kept the line break in the first place now... Just so it behaves more like the `file` iterator. A bit late now though... :-)

As for considering my kludgy example a fix, I'm a bit concerned :-) I suppose the `httplib` `HTTPResponse.fp` member probably won't change nor be removed as @brandon-rhodes said. I am more concerned by the access to `urllib3`'s `HTTPResponse._fp` member. Not only is this one underscore-prefixed, `urllib3` is still in active development. Maybe access to an exposed `fp` member (in the short term directly represented by the `httplib`'s own `fp`) could be provided officially by `urllib3` by @shazow ? (This would have implication for the future support/implementation of this now public member though...)

Also, please note that I did not validate any of this on Python 3. I'm still using 2.7 (yes, I'm late to the party, I know). Maybe the socket file-like object implementation or `httplib` module differ on this version.
",mponton,shazow
844,2013-02-12 17:14:23,"@brandon-rhodes Well, thanks for the kind words! I did not know older versions of Python implemented the file-like object that way. Interesting. Glad they changed it.

@slingamn `line.rstrip()` will remove all newline characters and whitespaces from the right. This could change the lines in an undesirable way for some clients (maybe someone does expect/want right padding whitespace). I wonder if `iter_lines` should have kept the line break in the first place now... Just so it behaves more like the `file` iterator. A bit late now though... :-)

As for considering my kludgy example a fix, I'm a bit concerned :-) I suppose the `httplib` `HTTPResponse.fp` member probably won't change nor be removed as @brandon-rhodes said. I am more concerned by the access to `urllib3`'s `HTTPResponse._fp` member. Not only is this one underscore-prefixed, `urllib3` is still in active development. Maybe access to an exposed `fp` member (in the short term directly represented by the `httplib`'s own `fp`) could be provided officially by `urllib3` by @shazow ? (This would have implication for the future support/implementation of this now public member though...)

Also, please note that I did not validate any of this on Python 3. I'm still using 2.7 (yes, I'm late to the party, I know). Maybe the socket file-like object implementation or `httplib` module differ on this version.
",mponton,slingamn
844,2013-02-12 19:39:12,"@shazow I guess since `requests` and `urllib3` are such good neighbours (and `requests` packages its own version), there is little benefit to make that official and instead, should `urllib3` internals change later, only `requests` will have to be adapted. I just feel dirty when I have to use `something._xyz` :-)

@slingamn On that previous `rstrip()` comment: I had a brain fart. We can simply specify `\n\r` to `rstrip()`. Dhuh! (Apparently I don't use `rstrip()` often enough...)

If everyone agrees this is an acceptable solution, I can make a pull-request in the next few days for it.
",mponton,shazow
844,2013-02-12 19:39:12,"@shazow I guess since `requests` and `urllib3` are such good neighbours (and `requests` packages its own version), there is little benefit to make that official and instead, should `urllib3` internals change later, only `requests` will have to be adapted. I just feel dirty when I have to use `something._xyz` :-)

@slingamn On that previous `rstrip()` comment: I had a brain fart. We can simply specify `\n\r` to `rstrip()`. Dhuh! (Apparently I don't use `rstrip()` often enough...)

If everyone agrees this is an acceptable solution, I can make a pull-request in the next few days for it.
",mponton,slingamn
844,2013-02-12 22:21:38,"Agreed that ideally `iter_lines` would return strings terminated by the original newline character(s), since that's how the file iterator works. @kennethreitz would this be too big of an API break to introduce?
",slingamn,kennethreitz
844,2013-04-17 07:26:43,"@Lukasa not unless it's the default.

Try this:

`requests.get('http://www.stkierans.org/')`
",akavlie,Lukasa
844,2013-04-17 18:41:38,"@akavlie That page is just using frames to hide this page: 'http://www.catholicweb.com/splash/stkierens/'. I'd suggest that you just target that page in Requests, but when I do it I get a 'Connection Reset By Peer'. Which is obnoxious.
",Lukasa,akavlie
844,2013-04-18 04:49:04,"@Lukasa Thanks for all the digging in... I've seen lots of pointless iframes and other horrible practices on various church sites, so this does not suprise me.

I'm targeting a lot of sites with requests in this application, and specifying overrides for bad behavior like this isn't very realistic at this point.

Is it reasonable to expect Requests to catch and wrap an exception like this? It looked to me like Requests itself had a bug.
",akavlie,Lukasa
844,2013-04-20 06:07:26,"@Lukasa It would indeed be great if IncompleteRead was wrapped. Currently, when calling requests.get or requests.post, I need to catch both requests.exceptions.RequestException and httplib.IncompleteRead, which does not make sense. IncompleteRead should be turned into a RequestException or its subclass. 
",jpaalasm,Lukasa
835,2012-09-03 13:10:32,"@alicebob it would be fairly simple.



If there is a merge conflict at this stage, you have to resolve it and commit the difference. Otherwise, just follow this up with `git push --force origin`.
",sigmavirus24,alicebob
832,2012-10-27 16:37:02,"> These libraries are vendored to make it possible to easily vendor Requests itself.

@kennethreitz 
How about changing the way vendorization is done by including dependencies in a zip and adding an option to setup.py to unzip them? This way vendorization could be sort of opt-in.
",piotr-dobrogost,kennethreitz
832,2012-10-27 16:57:50,"It's really not that complicated or that big of a deal.

On Oct 27, 2012, at 11:37 AM, Piotr Dobrogost notifications@github.com wrote:

> These libraries are vendored to make it possible to easily vendor Requests itself.
> 
> @kennethreitz 
> How about changing the way vendorization is done by including dependencies in a zip and adding an option to setup.py to unzip them? This way vendorization could be sort of opt-in.
> 
> —
> Reply to this email directly or view it on GitHub.
",kennethreitz,kennethreitz
830,2012-09-11 03:05:51,"Thanks for the awesome patch! Would you mind resending it with the advise given by @sigmavirus24? Thanks!
",kennethreitz,sigmavirus24
823,2012-08-30 13:14:05,"Thanks @Lukasa for centralizing the discussion. I'm personally a little torn about the `save_as` function for the Response object. It could easily become unwieldy. For example, what if someone also wants to save the response headers? Then they have to rewrite this recipe for themselves and the function serves no benefit. Still others might want as much information from the Response object as possible including the url they used (if they're using this towards logging). I think it is certainly useful, I just think it could introduce some possible unnecessary arguments in the future over how much it should include. Besides, developers can easily write a function for themselves to dump the content that they want.

I would lean on the side of simplicity and leave it out.

As for adding tests for `iter_content()`, I'm surprised it didn't already exist. :+1: for catching that.
",sigmavirus24,Lukasa
823,2012-08-30 13:26:52,"(Thanks @Lukasa for fixing my messy pullrequest)

I see your reasoning, but because of the prefetch=False gotcha I didn't get it right the first time (I needed to look at the source of iter_content() to figure out how to use it). Having a save_as() which Just Works seems to be nice, if only as an example for anyone to make their own. An other option would be to add it to the documentation as 'recipe'.
",alicebob,Lukasa
823,2012-08-30 18:45:15,"I'm +1 on @alicebob's idea of using `save_as()` as a documented 'recipe'. Actually, I'm +1 on having a section of the documentation devoted to interesting examples or use cases. Not sure it should be part of the base library though.
",Lukasa,alicebob
823,2012-08-31 23:51:44,"@alicebob ping me again once you update this to just include the tests, and i'll happily include it :)
",kennethreitz,alicebob
821,2013-05-13 02:38:52,"@oliverjanik I think you're being bitten by something entirely different. For one, I would bet this happens even without the proxy. Secondly, are you setting the headers case-insensitively? If so, try doing `headers={'Accept': 'application/json', ...}`. Third, this was closed because it was fixed [upstream](https://github.com/shazow/urllib3/pull/93) in urllib3. If it is still present then it is likely still an issue with urllib3. Again, I'm fairly certain you're not being affected by this bug.
",sigmavirus24,oliverjanik
821,2013-05-13 03:04:55,"@sigmavirus24 I am using the same code as original poster: 



Without proxy:



Headers (via WireShark):



So it's not happening without proxy.
",oliverjanik,sigmavirus24
817,2012-08-28 13:53:33,"@kennethreitz 

> Thanks for the contribution, but I don't really see how modifying the very simple dictionary is difficult at all.

Hmmm, maybe I'm missing some easy way to do this? Or maybe you're not familiar with or recalling the recent changes in how `params` is handled? (see @sigmavirus24's comment also)

`params` **used to** be a dict until very recently. Now it is a list of tuples. It's not rocket science of course to append to a list of tuples -- the issue is that it's tricky for folks to write code that reliably handles `params` when they don't know which version of requests is installed. Not impossible, but harder than it needs to be. Here's what I have in my client code now:



If I had the `add_params` method then this simply becomes:


",msabramo,kennethreitz
817,2012-08-28 13:53:33,"@kennethreitz 

> Thanks for the contribution, but I don't really see how modifying the very simple dictionary is difficult at all.

Hmmm, maybe I'm missing some easy way to do this? Or maybe you're not familiar with or recalling the recent changes in how `params` is handled? (see @sigmavirus24's comment also)

`params` **used to** be a dict until very recently. Now it is a list of tuples. It's not rocket science of course to append to a list of tuples -- the issue is that it's tricky for folks to write code that reliably handles `params` when they don't know which version of requests is installed. Not impossible, but harder than it needs to be. Here's what I have in my client code now:



If I had the `add_params` method then this simply becomes:


",msabramo,sigmavirus24
817,2012-08-28 17:34:22,"@msabramo that was my initial thought as well, yes.
",kennethreitz,msabramo
817,2012-08-28 17:37:38,"@shazow :cake:
",kennethreitz,shazow
817,2012-08-28 20:29:47,"@sigmavirus24 no fault needs to be had, just improvements to a great pull request :)
",kennethreitz,sigmavirus24
815,2012-08-26 16:47:26,"@kennethreitz So fast!!!!!
",ayanamist,kennethreitz
808,2012-10-29 16:02:00,"I'm getting the same error - does anyone have this working with the latest version of Twython?  

@y2rayk how do you get the oauth_signature param for the headers in your workaround?
",danxshap,y2rayk
804,2012-08-25 14:34:30,"Fixed by @weak, thanks!
",kennethreitz,weak
804,2012-08-25 14:34:57,"@sigmavirus24 feel free to send your final version when it's ready :)
",kennethreitz,sigmavirus24
803,2012-08-25 14:35:36,"@vasa81 thanks for the contribution, but we're going to hold off for now until we figure out a better way to approach this.
",kennethreitz,vasa81
800,2012-08-24 15:26:05,"@Lukasa Thank you. Either form is accepted. If the issue can be fixed, i don't care what and how is modified.
",ayanamist,Lukasa
800,2012-08-25 14:37:49,"@ayanamist hmm, this seems like an odd way to fix the problem. I need to get a better understanding of why this is necessary. Any additional info would be great!
",kennethreitz,ayanamist
799,2012-08-20 05:38:27,"@joequery You should be making this pull request against urllib3, not requests :)
",kennethreitz,joequery
799,2012-09-24 16:01:57,"@kennethreitz :O You merged urllib3 pull request code into requests. Naughty.
",shazow,kennethreitz
799,2014-01-07 15:41:44,"I'm a little confused as to the status of what happened, here. My perception is that one change was made to urllib (allowing a SSL version to be passed) and that this PR was reverted, leaving only the @joequery fork of requests, where the only real change was to attempt SSL3 and _then_ SSL23.

There still exists a problem where any error will be caught and another connection reattempted using SSL23, sometimes or always causing the ""unknown protocol"" error. This can simply be fixed by reraising the exception unless the message matches the TLS-version error:



I'd submit an issue, but apparently forks can't have issues.

@joequery I think your fork might still be the only solution. Please confirm. If so, please consider the fix, above.
",dsoprea,joequery
799,2014-01-07 15:57:51,"@dsoprea Please think very carefully before commenting on two-year-old issues. There was definitely a better, less-noisy way to handle providing your input.

Your reading of this issue is wrong. This pull request contained only changes to urllib3, which is a separate module to requests, maintained elsewhere. We do not accept code changes in urllib3 in this repository, as we simply bring in urllib3 wholesale. Any urllib3 changes therefore need to be made at the core repository, shazow/urllib3. This means even if we wanted to, we could not consider this fix, because it's not our fix to make.

Kenneth accidentally merged this fix when he intended to close it, so he also reverted it.

If you'd carefully checked the internet, you'd know that there is an accepted way to choose the SSL version used by Requests. It is documented [in this blog post](https://lukasa.co.uk/2013/01/Choosing_SSL_Version_In_Requests/) (now more than a year old), [this StackOverflow question](http://stackoverflow.com/questions/14102416/python-requests-requests-exceptions-sslerror-errno-8-ssl-c504-eof-occurred), [this Requests issue](https://github.com/kennethreitz/requests/issues/1083), and will be implemented in [this open-source library](https://github.com/sigmavirus24/requests-toolbelt).
",Lukasa,dsoprea
799,2014-01-07 18:47:53,"I did a search for the bug that I was having, and I ended-up in that PR. I
apologize for posting the comment. Joe seemed to be fixing a bug that I
encountered first thing in the morning, and it seemed the most relevant to
a bug that never got fixed. I did not notice the timestamp, and got lost in
all of the various cross-linking. I appreciate the links in your response.

I would caution you against being so abrasive. I'll be careful to respond
to you as you have responded to me, should you ever have a question about
anomalous behavior in one of my projects that you might suddenly find
yourself relying on.

On Tue, Jan 7, 2014 at 10:58 AM, Cory Benfield notifications@github.comwrote:

> @dsoprea https://github.com/dsoprea Please think very carefully before
> commenting on two-year-old issues. There was definitely a better,
> less-noisy way to handle providing your input.
> 
> Your reading of this issue is wrong. This pull request contained only
> changes to urllib3, which is a separate module to requests, maintained
> elsewhere. We do not accept code changes in urllib3 in this repository, as
> we simply bring in urllib3 wholesale. Any urllib3 changes therefore need to
> be made at the core repository, shazow/urllib3. This means even if we
> wanted to, we could not consider this fix, because it's not our fix to make.
> 
> Kenneth accidentally merged this fix when he intended to close it, so he
> also reverted it.
> 
> If you'd carefully checked the internet, you'd know that there is an
> accepted way to choose the SSL version used by Requests. It is documented in
> this blog posthttps://lukasa.co.uk/2013/01/Choosing_SSL_Version_In_Requests/(now more than a year old), this
> StackOverflow questionhttp://stackoverflow.com/questions/14102416/python-requests-requests-exceptions-sslerror-errno-8-ssl-c504-eof-occurred,
> this Requests issue https://github.com/kennethreitz/requests/issues/1083,
> and will be implemented in this open-source libraryhttps://github.com/sigmavirus24/requests-toolbelt
> .
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/pull/799#issuecomment-31749628
> .
",dsoprea,dsoprea
799,2014-01-08 16:51:59,"@dsoprea My fork existed back when documentation concerning choosing SSL versions wasn't as easy to find. I still use my fork from time to time when I deal with scraping sites which exclusively deal with sslv3, because I'm lazy and don't feel like doing that mounting process :P

I'll make the update you provided anyway, so thanks for that. I also didn't think your initial comment was rude or inconsiderate at all, and I don't see how commenting on an issue similar to your own is ""noisy"".
",joequery,dsoprea
799,2014-01-08 17:09:26,"He's a good guy. I was confused and annoyed by the presence of no seemingly
standard, staightforward solution, and he was just annoyed by something
being brought-up that seemed a bit more straightforward to him, and
resurrected an old thread.

I saw a reference by Ken to ""transport adapters"" on another thread, and
then, through more searches than I would've hoped, finally found my way to
the ""transport adapters"" section in the docs. I then just overrode
_init_poolmanager_, and added ssl_version as a parameter.

I think that HTTPAdapter should have this already, but I can't imagine it
wouldn't be there unless there was an exceptional reason.

I'll propose a PR that introduces an SSL example in the ""transport
adapters"" section of the documentation that I referred-to, above.

Dustin

On Wed, Jan 8, 2014 at 11:52 AM, Joseph McCullough <notifications@github.com

> wrote:
> 
> @dsoprea https://github.com/dsoprea My fork existed back before
> documentation concerning choosing SSL versions wasn't as easy to find. I
> still use my fork from time to time when I deal with scraping sites which
> exclusively deal with sslv3, because I'm lazy and don't feel like doing
> that mounting process :P
> 
> I'll make the update you provided anyway, so thanks for that. I also
> didn't think your initial comment was rude or inconsiderate at all, and I
> don't see how commenting on an issue similar to your own is ""noisy"".
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/pull/799#issuecomment-31852884
> .
",dsoprea,dsoprea
799,2014-01-08 17:27:01,"@joequery As @dsoprea just said, we both managed to get our wires crossed and weren't entirely our best selves. These things happen, apologies all round and everyone's friends again. =)

@dsoprea You mention that you're not sure why the SSL version isn't available on the default `HTTPAdapter`. This is a fair question. The short answer is that we don't want to provide options on the `HTTPAdapter` for _every single argument_ urllib3 takes on any of the functions we call. That will lead to a nightmare god-object, and worse, will lead to us duplicating the default values that are already in urllib3. That's a potential source of many many bugs.

With that in mind, we had to pick which attributes we thought were important enough to specify explicitly and which we thought weren't. This was an arbitrary decision, and Kenneth made it as he saw fit. In principle the `ssl_version` should never need to be set: it looks like the bug we're encountering here is related tangentially to buggy versions of OpenSSL (see a very recent discussion at #1847 and #1850, where the tone of the conversation provides an idea of how frustrating the maintainer + contributors find this SSL bugginess). This makes `ssl_version` a prime candidate to simply leave at its default urllib3 value.

This dovetails with the fact that the Transport Adapter interface is _intended_ to be an inheritance-based interface, not a function-parameters one. Again, [I've written briefly about this](https://lukasa.co.uk/2013/04/Requests_Two_APIs/) (though if you read the `SSLAdapter` post you'll find this retreads a lot of ground).

If you strongly disagree with this design decision, I can get Kenneth to weigh in on it, but we're strongly resisting changes to Requests' interfaces.

As for proposing an SSL Transport Adapter example in the documentation, I think we'd be happy to have it. =) I've documented it myself in a number of places that aren't the official documentation, and it's now present in what is likely to be a very popular companion library, but it also makes a good example.
",Lukasa,dsoprea
799,2014-01-08 17:27:01,"@joequery As @dsoprea just said, we both managed to get our wires crossed and weren't entirely our best selves. These things happen, apologies all round and everyone's friends again. =)

@dsoprea You mention that you're not sure why the SSL version isn't available on the default `HTTPAdapter`. This is a fair question. The short answer is that we don't want to provide options on the `HTTPAdapter` for _every single argument_ urllib3 takes on any of the functions we call. That will lead to a nightmare god-object, and worse, will lead to us duplicating the default values that are already in urllib3. That's a potential source of many many bugs.

With that in mind, we had to pick which attributes we thought were important enough to specify explicitly and which we thought weren't. This was an arbitrary decision, and Kenneth made it as he saw fit. In principle the `ssl_version` should never need to be set: it looks like the bug we're encountering here is related tangentially to buggy versions of OpenSSL (see a very recent discussion at #1847 and #1850, where the tone of the conversation provides an idea of how frustrating the maintainer + contributors find this SSL bugginess). This makes `ssl_version` a prime candidate to simply leave at its default urllib3 value.

This dovetails with the fact that the Transport Adapter interface is _intended_ to be an inheritance-based interface, not a function-parameters one. Again, [I've written briefly about this](https://lukasa.co.uk/2013/04/Requests_Two_APIs/) (though if you read the `SSLAdapter` post you'll find this retreads a lot of ground).

If you strongly disagree with this design decision, I can get Kenneth to weigh in on it, but we're strongly resisting changes to Requests' interfaces.

As for proposing an SSL Transport Adapter example in the documentation, I think we'd be happy to have it. =) I've documented it myself in a number of places that aren't the official documentation, and it's now present in what is likely to be a very popular companion library, but it also makes a good example.
",Lukasa,joequery
791,2012-08-18 20:31:57,"@kennethreitz @Lukasa should I start a new branch off of develop and cherry pick my commits on top of it to avoid the merges? Besides, one of the merges would not have been able to be automatic, so it would have required a merge commit anyway.
",sigmavirus24,Lukasa
791,2012-08-18 22:34:40,"@kennethreitz done.
",sigmavirus24,kennethreitz
790,2012-08-20 07:17:25,"@idan I understand the complete workflow, but the design purpose make all things complicated.

You can see how kennethreitz/requests use requests, and i agree what they do.

If i pass the auth to requests, i think requests should do all auth stuffs, but not like current situation, i must manually add oauth signature. If requests needs manual operation, why i pass the auth arguments to it?

Passing dict object as post data parameters is a normal demand, but not doing something special!!! Every developer are doing like this after read your documentation. Passing the auth and thinking auth function will do things automatically is also a normal thought. If i want to do something special, i will do it before pass the whole thing into requests.

The content-type is set by your code not mine, and the conditional which ignore the post data is also part of your logical but not mine. So what a special thing you think may happen is made by your code but not mine.

> but only if the content-type header is set to ""application/x-www-form-urlencoded"", otherwise the request body is ignored for the purpose of signing.

Why you ignore to sign? What's the reason? So i must sign the request by myself although i told the requests ""plz do the auth work""? Does your thought make things simpler or more complicated?
",ayanamist,idan
790,2012-08-20 07:51:18,"In fact, https://github.com/kennethreitz/requests/blob/develop/requests/models.py#L514 here set content-type to 'application/x-www-form-urlencoded' and if it's not set, it will still be set on https://github.com/kennethreitz/requests/blob/develop/requests/auth.py#L97
So you are doing duplicate work and make the whole thing really complicated.

And if someone set incorrect content-type(anything other than 'multipart/form-encoded') and post a file, you just let the incorrect thing pass by?

So what's your root design purpose @idan ? If you think developers will do things special, you should remove the code which add content-type automatically, or remove the content-type detect conditions in order to make signing work.
",ayanamist,idan
790,2012-08-20 09:11:43,"Relax, @ayanamist, @idan is agreeing with you. He said that the parameters should be taken as input to the signing function only if the `Content-Type` header is `application/x-www-form-urlencoded`. If you pass in a dict, such a header is set, so we should be signing it, but we aren't. **This is a bug.**

As for ""why is the request body ignored for signing in other cases"", it's because the specification says to ignore it. This isn't Requests' decision, it's just what the specification says to do.
",Lukasa,idan
790,2012-08-20 09:11:43,"Relax, @ayanamist, @idan is agreeing with you. He said that the parameters should be taken as input to the signing function only if the `Content-Type` header is `application/x-www-form-urlencoded`. If you pass in a dict, such a header is set, so we should be signing it, but we aren't. **This is a bug.**

As for ""why is the request body ignored for signing in other cases"", it's because the specification says to ignore it. This isn't Requests' decision, it's just what the specification says to do.
",Lukasa,ayanamist
790,2012-08-25 14:47:48,"@kennethreitz Thank you very much :-)
",ayanamist,kennethreitz
785,2012-08-25 14:51:10,"@sigmavirus24 :+1:
",kennethreitz,sigmavirus24
785,2012-09-06 14:27:23,"Once @kennethreitz has time to review #833, I'll start working on this. I have a feeling opening a branch for this would cause a merge conflict if I were to have two Pull Requests that are ignorant of each other for the same file. Could be wrong though. Also, I'm in no rush since I'm fairly busy and I know @kennethreitz is more busy than I am with conferences and whatnot. Just wanted to keep @flub updated.
",sigmavirus24,flub
785,2012-09-06 14:27:23,"Once @kennethreitz has time to review #833, I'll start working on this. I have a feeling opening a branch for this would cause a merge conflict if I were to have two Pull Requests that are ignorant of each other for the same file. Could be wrong though. Also, I'm in no rush since I'm fairly busy and I know @kennethreitz is more busy than I am with conferences and whatnot. Just wanted to keep @flub updated.
",sigmavirus24,kennethreitz
783,2012-08-19 00:45:05,"@doda @Lukasa I agree :)
",kennethreitz,doda
783,2012-08-19 00:45:05,"@doda @Lukasa I agree :)
",kennethreitz,Lukasa
782,2012-08-25 15:00:12,"@Lukasa want to send a pull request? :)
",kennethreitz,Lukasa
781,2012-08-25 14:36:46,"@slingamn if you do that, I'll accept it :)
",kennethreitz,slingamn
776,2012-08-13 15:34:25,"@Lukasa It appears that `hostname` is already available on `_p`, from the previous `urlparse()`.

So `_p.hostname.endswith` should be the new callable to pass to `map`, in my opinion.
",Pewpewarrows,Lukasa
774,2012-08-11 09:02:20,"@travisbot, you want to try testing this again? /poke
",Lukasa,travisbot
774,2012-08-13 20:24:13,"Yeah, damn straight it does @travisbot. Would have passed the first time too, if your VM hadn't broken.
",Lukasa,travisbot
773,2012-08-11 12:33:20,"I think neither I nor @slingamn are saying this pull request is anything other than brilliant. We just don't understand how the tests were passing to begin with. I always test locally before I submit a pull request, including in 2.6, and never had the tests fail.

For me, this is just a curiosity, nothing more. =)
",Lukasa,slingamn
773,2012-08-14 03:54:05,"@radomir: @Lukasa is right, this change was great. I was just worried that the testing infrastructure for Requests was broken somehow. But `test_requests_ext.py` is intentionally excluded, so that explains it. Thanks for your contribution!
",slingamn,Lukasa
771,2012-08-11 14:21:54,"Using @slingamn's Gist, I can also reproduce the bug.

Using Python with the following build info:



And using the `develop` branch of Requests @ 27b55a7, I get the output:


",Lukasa,slingamn
771,2012-09-10 12:10:34,"@slingamn my first instinct about the docs telling you to use shutdown first is that it changes the behavior of the socket just like shutdown would do with [C sockets](http://beej.us/guide/bgnet/output/html/singlepage/bgnet.html#closedown). I would agree with you that the shutdown call will not likely help, but it is worth trying.
",sigmavirus24,slingamn
771,2012-09-20 08:35:08,"@shazow interesting.

It is sufficient for this ticket for `_fp.fp` to be set to None, as it should be after the call to `r.content`. I think we'd only need to have urllib3 set `raw._fp` to None as well if we were trying to release the socket without reading all the content.

However, I suspect this is not a good use case; if you try to reuse a connection while it still has unread data from the last request, I think it may not work correctly. (Possibly depending on implementation details of the server.) Thoughts?
",slingamn,shazow
771,2012-10-20 01:14:23,"@slingamn thanks for reminding me about 223 ;)
",sigmavirus24,slingamn
765,2012-08-07 10:13:03,"@josemariaruiz, do you have sample strings to reproduce the bug?
",turicas,josemariaruiz
765,2012-08-07 10:19:36,"@josemariaruiz, reduce it to the characters that are creating the problem and post the representation of the object here (`repr(my_string)`).
",turicas,josemariaruiz
761,2012-11-28 13:30:20,"@llama it isn't closed because it was fixed, it was closed because it isn't an issue with requests. It **is** an issue with a library requests relies on which @geier indicated in their last comment.
",sigmavirus24,llama
761,2012-11-28 13:30:20,"@llama it isn't closed because it was fixed, it was closed because it isn't an issue with requests. It **is** an issue with a library requests relies on which @geier indicated in their last comment.
",sigmavirus24,geier
761,2012-11-28 13:33:55,"Ahh understood, thanks for that @sigmavirus24.
",llama,sigmavirus24
761,2013-09-24 01:09:44,"I'm using requests version 1.2.0 and seeing the same problem: `requests` crashes with `SSLError` on https://selectedpapers.net, whereas `openssl s_client -connect selectedpapers.net:443` verifies the certificate successfully (and any browser not IE on XP accepts the certificate;  here's another example of the same problem: http://hearsum.ca/blog/python-and-ssl-certificate-verification/).  I searched the urllib3 issue tracker but couldn't find a clear match to this bug. 

@geier @sigmavirus24 can you give a link to the urllib3 issue that tracks this?  I'd like to see whether urllib3 has actually fixed this, and if so why requests 1.2.0 still doesn't work for this https request.
",cjlee112,geier
761,2013-09-24 01:09:44,"I'm using requests version 1.2.0 and seeing the same problem: `requests` crashes with `SSLError` on https://selectedpapers.net, whereas `openssl s_client -connect selectedpapers.net:443` verifies the certificate successfully (and any browser not IE on XP accepts the certificate;  here's another example of the same problem: http://hearsum.ca/blog/python-and-ssl-certificate-verification/).  I searched the urllib3 issue tracker but couldn't find a clear match to this bug. 

@geier @sigmavirus24 can you give a link to the urllib3 issue that tracks this?  I'd like to see whether urllib3 has actually fixed this, and if so why requests 1.2.0 still doesn't work for this https request.
",cjlee112,sigmavirus24
761,2013-09-24 01:46:19,"@cjlee112 you're using an outdated version of requests. Re-attempt with that version and if it persists open a new issue with the details.
",sigmavirus24,cjlee112
761,2013-09-24 01:54:20,"Oddly enough @cjlee112 running your `openssl` command on my machine ends with:



But on 1.2.3 I am seeing the same SSLError because of a certificate mismatch. So the certificate you purchased will not work with requests unless you use the system certificates (which seem to work for you but not for me) by specifying as is described in the blog post you linked.

Incidentally if I do



that will verify the certificate correctly.
",sigmavirus24,cjlee112
761,2013-09-24 04:51:08,"@sigmavirus24 Thanks for looking into this!!  I wasn't looking for a workaround, but rather trying to find the urllib3 issue number that tracks the resolution of this bug.   You guys said ""it looks like this is an urllib3 problem, will take it there"" but gave no details, and hunting around there I couldn't find a clear match.  I think those details should be entered here, for the record, since it strongly affects `requests` users.  For example, if `urllib3` (for whatever reason) ends up not truly fixing this bug, it will (continue to) be a big issue for people trying to use `requests`.  So the `requests` issue tracker should have an explicit record of what `urllib3` issue number(s) address this.
",cjlee112,sigmavirus24
759,2012-08-06 15:42:49,"Nope, @deadbeef404 is correct. You shouldn't depend on urllib3 to coerce your types for you. If we want ints, longs, and whatever else to be auto-coerced into strings, then please do that in Requests before passing it to urllib3.

I do plan to remove the backwards compatibility block, so please make sure everything will continue working in Requests. :)
",shazow,deadbeef404
757,2012-08-03 22:56:03,"@kennethreitz , the problem is not how browsers work, there are some APIs which use OAuth and need specifically to receive `%20` as the scape character for `space` according to the [RFC 5849](http://tools.ietf.org/html/rfc5849#section-3.6)

This should be an available option, as if it is done other way, it would result in a 401 response
",dlitvakb,kennethreitz
757,2013-01-07 22:56:52,"@ronnix since we now provide the fully mutable PreparedRequest object, you can change the url to be whatever you need :)
",kennethreitz,ronnix
757,2013-01-08 00:59:31,"@ronnix yeah that will work too
",sigmavirus24,ronnix
757,2013-01-08 10:28:38,"@sigmavirus24 



Out of curiosity; how can user know what is safe to change after the request had been prepared? It looks like playing with fire :)
",piotr-dobrogost,sigmavirus24
756,2017-02-14 21:45:38,"@zen0x90 this was related to the pull request #757 which was no accepted. This feature wasn't accepted either. If you want to have your data `quote_plus`'d, you should do it yourself with [`quote_plus`](https://docs.python.org/3.5/library/urllib.parse.html#urllib.parse.quote_plus)",sigmavirus24,zen0x90
755,2012-08-03 16:23:50,"@enginous if you pull develop and merge, your PR should pass unless there's a way to kick @travisbot to merge it against the current head.
",sigmavirus24,travisbot
755,2012-08-03 16:26:44,"Ah, okay. I hadn't tried them, I just know that @travisbot doesn't 
always keep up.
",sigmavirus24,travisbot
749,2013-01-21 20:17:44,"@shazow: Presumably you have no interest in making urllib3 depend on PyOpenSSL?
",Lukasa,shazow
749,2013-02-09 23:13:36,"It looks like @t-8ch is knocking it out of the park with the work on urllib3. I'll let him tell us when this can be closed.
",sigmavirus24,t-8ch
749,2013-05-04 16:01:57,"@pythonmobile spin a top. If it doesn't stop spinning, you're dreaming.

![inception](https://gs1.wac.edgecastcdn.net/8019B6/data.tumblr.com/tumblr_lzpbw5SP2X1r94e9jo1_500.gif)
",sigmavirus24,pythonmobile
749,2013-06-10 03:24:14,"@mshang Version 1.2.3 should do this for you already; are you sure you're running that version?
",hobarrera,mshang
749,2013-06-10 04:09:12,"@hobarrera Yes, I'm running 1.2.3. `requests.packages.urllib3.contrib` was there, but that import statements still threw a bunch of import errors. I had to install `ndg-httpsclient` and I forget what else.
",mshang,hobarrera
749,2013-06-10 13:48:01,"@pythonmobile If that's the case, and you have a good reproducible test case, please open an issue highlighting it on urllib3. Requests has no SNI-specific code (as far as I know), so a fix there will fix us as well.

Unless urllib3 is fixed already, which seems to happen a lot these days. Shazow _et. al._ are pretty awesome. @t-8ch, have you guys seen/fixed this already?
",Lukasa,pythonmobile
749,2013-06-10 13:48:01,"@pythonmobile If that's the case, and you have a good reproducible test case, please open an issue highlighting it on urllib3. Requests has no SNI-specific code (as far as I know), so a fix there will fix us as well.

Unless urllib3 is fixed already, which seems to happen a lot these days. Shazow _et. al._ are pretty awesome. @t-8ch, have you guys seen/fixed this already?
",Lukasa,t-8ch
749,2013-06-11 08:29:11,"Sorry @pythonmobile, I don't agree. =)

SNI is explicitly urllib3 functionality. If SNI is broken in urllib3 you absolutely should provide that project with a fix, a test, or at the very least a bug report. When that project is fixed, Requests will magically become fixed as well (when we next update urllib3, which we do fairly frequently). =)
",Lukasa,pythonmobile
749,2013-06-11 17:25:12,"While more tests for requests certainly wouldn't hurt, as @Lukasa said, you'll need to write a test for urllib3 if you'd like to see this actually fixed. :)

We already have many tests in a similar vein, ping me if you need any help.
",shazow,Lukasa
749,2013-06-11 19:01:29,"It seems the timeout behaviour of pyOpenSSL is different from that of the
standard ssl module:



This throws a `WantReadError`, the same that is also thrown out from urllib3
when specifying a timeout.

The [documentation](http://pythonhosted.org/pyOpenSSL/openssl-ssl.html) has
this to say about the error:



I don't know how where the timeout should be handled. Does the socket magically
remember the timeout over invocations of `select()` or is this the task of
urllib3?
Keeping track of the timeout by hand doesn't sound really funny.

@pythonmobile What do you mean with nonexistent domains. I can't follow you.
",t-8ch,pythonmobile
749,2013-08-27 17:34:13,"@pythonmobile Could you try the changes from shazow/urllib3#233 and look if the code works with pyopenssl and timeouts?
",t-8ch,pythonmobile
749,2013-08-27 22:10:55,"@t-8ch @pythonmobile  : Can you guys look at this thread as well: https://github.com/kennethreitz/requests/issues/1522#issuecomment-22443282

It would be nice to get these requests in a unit test inside urllib3 or requests. 
",pikumar,pythonmobile
749,2013-08-27 22:10:55,"@t-8ch @pythonmobile  : Can you guys look at this thread as well: https://github.com/kennethreitz/requests/issues/1522#issuecomment-22443282

It would be nice to get these requests in a unit test inside urllib3 or requests. 
",pikumar,t-8ch
749,2013-08-28 07:49:21,"The problem with getting these as a unit test is that they only appear in very specific configurations. For instance, I have never been able to reproduce either the second half of this issue or #1522 on any of my systems, whether it's Windows, OS X or Ubuntu. That's why I asked @pythonmobile for a reproducible unit test. If we can find one, we can develop against it, otherwise we're stuck hoping that @t-8ch continues to be a genius and magically fix all our problems up at `urllib3`.

NB: @t-8ch, I don't think I thank you enough for your work, both here and at `urllib3`. You're awesome. =D :cake: :pineapple: :banana: :cookie: :beers: 
",Lukasa,pythonmobile
749,2013-08-28 07:49:21,"The problem with getting these as a unit test is that they only appear in very specific configurations. For instance, I have never been able to reproduce either the second half of this issue or #1522 on any of my systems, whether it's Windows, OS X or Ubuntu. That's why I asked @pythonmobile for a reproducible unit test. If we can find one, we can develop against it, otherwise we're stuck hoping that @t-8ch continues to be a genius and magically fix all our problems up at `urllib3`.

NB: @t-8ch, I don't think I thank you enough for your work, both here and at `urllib3`. You're awesome. =D :cake: :pineapple: :banana: :cookie: :beers: 
",Lukasa,t-8ch
749,2013-08-29 15:50:48,"@t-8ch Do it. :+1: 
",Lukasa,t-8ch
749,2013-09-14 08:24:13,"@rcoup We'll pull in an up-to-date version into Requests when we release 2.0. =)
",Lukasa,rcoup
747,2012-08-04 00:50:32,"I started working on this and wrote a regression test that exposes the problem, but I wanted to get your opinion on how you want this fixed @kennethreitz . My first question is do you want the content-type header to be text/plain or do you just want it unset? The second question is in how to fix it. The issue is that the compat.py rebinds str to be the unicode class so in models.py line 496 where there is a check for `instanceof(self.data, str)` where str is actually unicode and not the __builtin__.str. Should I just add in another isinstance check for the __builtin__ str type or do something else? Let me know how you would like this fixed and I can put in a pull request.
",volker48,kennethreitz
746,2012-07-27 16:10:05,"@Lukasa if you edit the verbosity of the tests in a local branch of yours and set up Travis with it, you should be able to get more information on your own from Travis.
",sigmavirus24,Lukasa
741,2013-01-08 20:42:42,"@karlcow

_And my server implementation is wrong as it should send a list of csv for the header Link:._

I think you are wrong here. Firstly, there seems to be no change in rules regarding multiple header fields with the same field name between RFC 2616 and the draft of httpbis you cited. The wording has changed but the sense is the same. Secondly, [RFC 5988 section 5](http://tools.ietf.org/html/rfc5988#section-5) defines the value of `Link` header as a list of comma separated values (`Link           = ""Link"" "":"" #link-value`) which means, according to the rules you cited, that there CAN be multiple `Link` headers and that they CAN be merged into one, comma separated list of values.
",piotr-dobrogost,karlcow
741,2013-01-09 01:57:24,"@sigmavirus24 because it is still not sure sure if it has be reopened or not.  Trying to understand the issue first. In the other hand, if there is a better place for it. I will be happy to discuss it there. 
",karlcow,sigmavirus24
740,2012-07-27 05:28:31,"Closing this, @r1chardj0n3s thanks for the patch! Can you send again with only 6f0a224? 
",kennethreitz,r1chardj0n3s
738,2012-10-02 21:06:21,"@kennethreitz Could you point us to some details on this known problem?
",piotr-dobrogost,kennethreitz
737,2012-07-27 05:43:53,"@Lukasa :+1:
",kennethreitz,Lukasa
731,2012-07-18 14:44:35,"Thanks guys!

And thanks for the rundown @Lukasa :)
",kennethreitz,Lukasa
726,2014-01-27 02:01:03,"@paragbaxi this issue was last commented on over 2 years ago. The proper place for questions about requests is [StackOverflow](http://stackoverflow.com/questions/tagged/python-requests) which you've already done. I will answer your question there.
",sigmavirus24,paragbaxi
724,2012-07-27 05:25:54,"thanks for the fix for this issue @muhtasib and @kennethreitz  . 
",amenasse,kennethreitz
724,2012-07-27 05:25:54,"thanks for the fix for this issue @muhtasib and @kennethreitz  . 
",amenasse,muhtasib
722,2012-07-12 23:36:28,"@gulopine Ah, thanks! I couldn't find my comment in 684! Haha, I was looking for it. And what does Linkedin use? Query or body? 
",michaelhelmick,gulopine
719,2012-07-12 18:55:10,"@kennethreitz Then what does the max_retries do in requests.defaults?

http://docs.python-requests.org/en/latest/api/#configurations
",JakeAustwick,kennethreitz
713,2012-07-14 02:25:32,"_nod_ We can do this, but it'll be easiest with a rewritten http client library. I've got one that's mostly API compatible with httplib (http://code.google.com/p/httpplus/) that I'm writing as part of my 20% work on Mercurial that we could try.

Note that the flow proposed in the initial bug isn't strictly RFC compliant - you can't wait indefinitely for the response to the Expect: header. You have to wait an unspecified delay and then continue optimistically, as servers SHOULD (not MUST) understand Expect: headers. Reverse proxies are allowed to strip Expect headers entirely, and many do so.

@kennethreitz I don't have any particular time to dedicate to this, but I'm happy to chat more about what we can do to make this better. Are you going to be at OSCON?
",durin42,kennethreitz
713,2013-01-23 14:02:11,"@kennethreitz do you have support for streaming uploads? I've not followed development of requests closely.
",durin42,kennethreitz
713,2013-07-18 10:35:25,"@edevil Nope. Feel free to pick it up if you want something to hack on. =)
",Lukasa,edevil
713,2014-04-12 04:07:51,"For those interested, this is probably not possible with our current stack. We talked with @durin42 tonight and it seems that httplib just swallows the 100-Continue and does not really wait for it either. This may be far more difficult than originally thought.
",sigmavirus24,durin42
713,2015-07-22 16:48:26,"@edevil Id your patched httplib available anywhere for as a package?

BTW, Amazon S3 recommends using 100-continues: http://docs.aws.amazon.com/AmazonS3/latest/dev/RESTRedirect.html
",cancan101,edevil
713,2015-07-22 16:53:48,"@cancan101 Sorry, no. But you have the patch in the ticket: http://bugs.python.org/file26357/issue1346874-273.patch
",edevil,cancan101
713,2016-10-08 14:08:50,"@redixin httplib never exposes a 100-continue to us. It's not a fully-fledged response per the RFC so we never see it. We can't extend code to support something if we never see it.
",sigmavirus24,redixin
713,2016-10-08 15:38:40,"@sigmavirus24 what do you mean ""never exposes""? We are sending data directly to socket, and then reading response [0]. We can read and write anything we want. This may be very simple patch, just few more lines and tests.

[0] https://github.com/kennethreitz/requests/blob/v2.11.1/requests/adapters.py#L434-L465
",redixin,sigmavirus24
713,2016-10-12 10:51:59,"> We can read and write anything we want. This may be very simple patch, just few more lines and tests.

@redixin If you believe that to be true, show us the code. Write that patch, write those tests, demonstrate that it works. But currently anyone else who would write that code believes that it is a _much_ more substantial patch than that, and is unwilling to contribute such a patch.
",Lukasa,redixin
713,2016-10-17 16:46:55,"@jallirs to be clear, Python 2.7 will likely never see this and the earliest version of Python to see this would be in the 3.x series (3.7, 3.8, etc.) It's not something that would likely ever be backported.
",sigmavirus24,jallirs
711,2012-07-12 20:55:51,"@steveklabnik I suppose Link headers aren't going anywhere just because of HAL. 

One question you might be more familiar with .. is the rel attribute a namespace of sorts for links? So suppose you had two ;rel=profile headers should one of them be ignored? I'm asking because the implementation would be nice as an attribute accessor like so:



HTML pages can link to multiple stylesheets, as is common.
",jokull,steveklabnik
711,2012-07-12 20:59:37,"@jokull this is quickly getting off topic, and this might be a great question for the hypermedia@librelist.com list, but quickly:

Not so much a namespace as like a tag. Think about a collection: 



Each one is an `item`, so each one would get an `item` rel. http://www.iana.org/assignments/link-relations/link-relations.xml

You could easily have it return an array rather than a string.

Note that [non-standard link relations are supposed to be URIs](http://tools.ietf.org/html/rfc5988#section-4.2), so accessing it via a method (sorry, Rubyist here primarily) means that when I have



you're not gonna be able to use this interface to get `response.links.http://mysite.com/rels/blog-post`.
",steveklabnik,jokull
711,2012-07-13 02:53:51,"@kennethreitz Good point about headers vs body. Are you looking for contributors or suggested implementations? 

One more reference: the [official Link Relations](http://www.iana.org/assignments/link-relations/link-relations.xml).
",jokull,kennethreitz
707,2012-07-03 08:59:17,"@kiennt Google is redirecting to a localized domain, so there is a cookie for `google.com` and `google.xyz`.

You can verify this with:


",schlamar,kiennt
700,2012-11-27 18:46:44,"@kennethreitz : Are you abandoning plans to add caching support, or is this work continuing elsewhere?
",inactivist,kennethreitz
700,2012-11-27 18:48:20,"@inactivist I didn't intentionally close the ticket! I am doing some cleanup and switched the default branch from 'develop' to 'master'. That apparently closed all pending pull requests :(
",kennethreitz,inactivist
700,2012-11-27 20:21:27,"@kennethreitz : Thanks.  Actually, I was interested in helping with the effort, as I'd like to take advantage of caching.  What can I do to assist?
",inactivist,kennethreitz
700,2014-02-24 20:15:47,"@ssokolow your comment seems entirely baffling to me given that there are several options to use when you want to use caching in requests. The last comment on this issue is over a year old and the issue is closed. If you're looking for those resources, I suggest you look elsewhere.
",sigmavirus24,ssokolow
700,2014-02-24 20:40:19,"@sigmavirus24

Ahh. Sorry about that.

I went looking but only managed to find a cache implementation which was purely timer based (no ETags, no `If-Modified-Since`) so I assumed they didn't exist and this was the most promising avenue.

It's quite likely I just screwed up my search keywords somehow but it'd still be a good idea to to add some kind of ""Related Projects"" list to the wiki.

@Lukasa Thanks! That couldn't be more perfect.
",ssokolow,Lukasa
700,2014-02-24 20:40:19,"@sigmavirus24

Ahh. Sorry about that.

I went looking but only managed to find a cache implementation which was purely timer based (no ETags, no `If-Modified-Since`) so I assumed they didn't exist and this was the most promising avenue.

It's quite likely I just screwed up my search keywords somehow but it'd still be a good idea to to add some kind of ""Related Projects"" list to the wiki.

@Lukasa Thanks! That couldn't be more perfect.
",ssokolow,sigmavirus24
700,2014-02-24 21:20:15,"Yes, thanks, @Lukasa, I searched recently and missed Cache-Control as well.  If it's not there now, why not mention it in the Requests docs?  (Hmm.... pull request, anyone?)
",inactivist,Lukasa
700,2014-02-24 22:29:11,"What about on the wiki then?

It does match the wiki's ""either hasn't made into the documentation yet, or doesn't belong there"" criterion and given the following points, it's pretty intuitive that wiki contents are community-supplied and neither endorsed nor supported by the project:
- The project doesn't use the wiki for its website or documentation.
- The project's documentation is much more polished than the wiki
- It's pretty well understood that, when a project has both Sphinx documentation **and** a wiki, the wiki is there to take advantage of the ""anyone can edit"" aspect of wiki-ness.
- The Wiki's ""Home"" page specifically encourages people to add things.

(I'd add it myself, but I don't know all of the ""several options"" that @sigmavirus24 mentioned existing. I'd rather have all known caching extensions listed, both to let people make their own decisions and to further drive home the ""this is not advice"" point.)
",ssokolow,sigmavirus24
700,2014-02-24 22:34:55,"Good points, @Lukasa -- totally understandable.  I admit it's been a little while (a month?) since I last researched Requests caching; I suspect I developed a blindness towards your (deprecated) project so I probably didn't click the link in my last search and thus didn't see your mention.  My bad.

I do like the idea of listing all known 'compatible' caching solutions on the wiki, along with the standard disclaimers.
",inactivist,Lukasa
700,2014-04-15 23:44:52,"@anentropic also https://github.com/ionrock/cachecontrol (same as the bitbucket repo you linked)
",sigmavirus24,anentropic
700,2014-04-16 08:24:03,"@sigmavirus24 ah yes, that looks like the best one to me so far
",anentropic,sigmavirus24
699,2012-06-28 17:14:30,"Hi @amenasse, I'm +1 on differentiating between the two timeouts. If you're going to do this on the urllib3 side, please make the pull request on that repo.

I'd suggest keeping TimeoutError as the base exception class for the two new ones you want to create. Something like SocketTimeout(TimeoutError) and PoolTimeout(TimeoutError).
",shazow,amenasse
697,2012-08-10 09:29:17,"@terrycojones Is this bug still affecting you, or has it been resolved?
",Lukasa,terrycojones
696,2013-11-18 08:36:51,"@Lukasa Thanks :) Sorry for posting here, will take care from next time.
",bhoomit,Lukasa
691,2012-07-30 12:10:19,"@kennethreitz I got bitten by this today. To me the behavior of `response.json` is very unintuitive. If I use `response.json` that's because I expect the response to actually be json-formatted. And I expect this call to fail in case it's not proper json. Say an API I use is mis-behaving and returns weird errors, if an exception occurs here I know something is wrong. Otherwise I just get a `None` object, things start to fail elsewhere in my system and I need to figure out why an object is `None` instead of a proper api response and where this comes from.

This is also inconsistent with python's `json.loads`.

At the very least this property should be renamed to `try_json` or `json_or_none` / `json_maybe` ;). But the current implementation is weak. When you say:

> there simply isn't any JSON, so the value is None

If I translate this to reading a file, this could also mean `open(filename, 'r')` returning `None` if the file doesn't exist. What you're currently saying is basically ""if you expect json data. don't use `response.json`"", which makes me wonder why there is such an API in the first place.

Sorry to be that guy, I'm rather strongly opposed to silent errors because they tend to catch you by surprise at the wrong moment. Hope you'll reconsider this change.

Thanks!
",brutasse,kennethreitz
691,2012-07-30 13:01:36,"@brutasse If you keep the request object then you can test if `r.json` is None. If so, then check the response object for the content of the response (e.g., `r.content`) and that will tell you what was returned by the API. I'm handling things that way myself.
",sigmavirus24,brutasse
691,2012-07-30 13:17:51,"@sigmavirus24 I know I can do it this way but it doesn't address the fact that the API is misleading: I expect some decoding happen and if the content is malformed then this decoding should fail loudly. I'd rather use json.loads on the response content directly but again, why is there a helper for json decoding if you have to hack around it to even notice there was a decoding error…

To quote pep20, ""Errors should never pass silently unless explicitly silenced."" I know this silencing is documented but I wouldn't qualify it as explicit.
",brutasse,sigmavirus24
691,2012-08-05 20:12:06,"I'm agree with @brutasse and @merwok 
",GMLudo,brutasse
691,2012-08-05 20:12:06,"I'm agree with @brutasse and @merwok 
",GMLudo,merwok
688,2012-06-21 19:21:24,"Works fine for me. Check your `proxy` variable is actually the dictionary you want to pass, e.g.:



Make sure that you have the full schema present.

EDIT: Just did a copy-paste of @kennethreitz's code, and noticed that it just made standard requests. The issue seems to be that there is a colon in the dict key that shouldn't be there. Try removing that colon.
",Lukasa,kennethreitz
684,2012-06-20 20:45:05,"Yes! Thank you.
I was actually just going to make an issue. 

I was updating a lib and was wondering why my signatures were invalid when passing the `params` kwarg and were valid when I just did `http://api.example.com/method/?myquerystring=parameters` and thought of this issue to be the only explanation 

@gulopine Think you can get @travisbot to successfully pass?

 :+1:
",michaelhelmick,travisbot
684,2012-06-20 20:45:05,"Yes! Thank you.
I was actually just going to make an issue. 

I was updating a lib and was wondering why my signatures were invalid when passing the `params` kwarg and were valid when I just did `http://api.example.com/method/?myquerystring=parameters` and thought of this issue to be the only explanation 

@gulopine Think you can get @travisbot to successfully pass?

 :+1:
",michaelhelmick,gulopine
684,2012-06-29 16:19:45,"@kennethreitz @gulopine I feel like this didn't fix the issue. I went to update `Twython` to use



instead of 



and the request failed, response was None :(
",michaelhelmick,kennethreitz
684,2012-06-29 16:19:45,"@kennethreitz @gulopine I feel like this didn't fix the issue. I went to update `Twython` to use



instead of 



and the request failed, response was None :(
",michaelhelmick,gulopine
677,2012-06-21 01:07:39,"The comment I deleted stands: I ran the tests on my machine with python 3000 and nose and got the same errors on my upstream branch as I get on my branch. I'll dig in to see if I can figure out where these tests are breaking.

**Update**: I tried running the tests individually, e.g.,



Only the last one passes with python3 on my machine on my branch tracking develop

**Update #2** After getting the tests working (thanks again @kennethreitz), I have a feeling the failure is related to nose and not requests or my changes. From what I can tell, the exception is occurring because of my having OpenDNS installed and running. This causes an error to be raised in the test for invalid content which causes the exception in nosetests. @jpellerin, @kumar303, any clues about the message above?
",sigmavirus24,kennethreitz
677,2012-06-21 18:40:25,"I disagree, the tests are legitimately failing. The four tests that are erroring out provide `data` as a string. In Python 3, strings implement the `__iter__` method. The same problem occurs with byte arrays in Python 3, which also implement `__iter__`. The tests will pass if you adjust your conditional to:



That said, the conditional becomes less nice in that form. I'm ok with having the `else` block all in one place, but I think this is about what PEP 20 suggests. I don't know which form fits PEP 20 better, and @kennethreitz has a better understanding of it than I do.
",Lukasa,kennethreitz
677,2012-06-21 18:53:40,"@Lukasa I was trying to find the bug using virtualenvs as well and couldn't get any better output than I described.

I'll take your word for it even though I don't understand why `not isinstance(data, str) and not isinstance(data, bytes)` would be necessary - neither has an `__iter__` attribute.

And yeah, if this is in fact the case, I'm all in favor of keeping it in the old style because that conditional would be horribly ugly. I just vastly dislike having so many return statements in one function.

Anyway, the other commit 0624b04 should still be considered since it's only a documentation change.

Sorry for the confusion
",sigmavirus24,Lukasa
677,2012-06-21 18:57:11,"@sigmavirus24 Try opening a Python3 interpreter and typing the following:



It should return True. Strings and Bytes both have `__iter__` methods in Python 3, which is why your tests fail in Python 3. No need to apologise, I didn't know Python 3 strings had `__iter__` methods either, until I did my investigation.
",Lukasa,sigmavirus24
677,2012-06-21 19:01:41,"Huh. When I was testing the hooks section the API wrapper I'm working on 
this morning, I thought I saw that this was passing on python 3 and 
failing on python 2. 

_shrug_. I'll close this and re-work my fork to only have the 
documentation change (and maybe one more that I promised @kennethreitz).

Thanks again for your help @Lukasa
",sigmavirus24,kennethreitz
677,2012-06-21 19:01:41,"Huh. When I was testing the hooks section the API wrapper I'm working on 
this morning, I thought I saw that this was passing on python 3 and 
failing on python 2. 

_shrug_. I'll close this and re-work my fork to only have the 
documentation change (and maybe one more that I promised @kennethreitz).

Thanks again for your help @Lukasa
",sigmavirus24,Lukasa
671,2012-11-23 21:03:59,"@sigmavirus24
Yes, I believe it fixes #888 too.
",piotr-dobrogost,sigmavirus24
671,2012-11-23 21:37:23,"@piotr-dobrogost can you not re-open this? If not, rebase and push to the same branch and re-open. It'll trigger Travis and your fix should work.
",sigmavirus24,piotr-dobrogost
664,2012-06-19 13:20:52,"Thanks @shazow @kennethreitz!
",lsemel,shazow
658,2012-07-03 06:16:27,"@kennethreitz ?
",schlamar,kennethreitz
655,2012-06-25 14:18:16,"@shazow I think the problem is that I am doing HTTPS over an HTTP proxy. It seems that this is fixed in #478. 

@kennethreitz I think this pull request hadn't made it into requests, right?
",schlamar,kennethreitz
655,2012-06-25 14:18:16,"@shazow I think the problem is that I am doing HTTPS over an HTTP proxy. It seems that this is fixed in #478. 

@kennethreitz I think this pull request hadn't made it into requests, right?
",schlamar,shazow
655,2012-06-25 16:42:58,"@kennethreitz Not sure what you merged, but if it was anything to do with this bug, it probably isn't a good idea. :)

urllib3 has some related pull requests and an in-progress branch but none of them are ready (and in fact has known bugs).
",shazow,kennethreitz
648,2012-06-02 02:22:34,"@lgastako See also: https://github.com/shazow/urllib3/pull/71

If you have anything to add to the conversation, I'd like to hear it. :)
",shazow,lgastako
647,2012-07-19 15:17:24,"Can't wait for this to be pulled, so that I can pull. :)

Thank you, @ncoghlan for the write-up.
",jpmens,ncoghlan
639,2012-08-02 15:34:46,"+1 for this @piotr-dobrogost requests fork fixes exceptions when using grequest
",locojay,piotr-dobrogost
633,2012-05-31 15:39:24,"Hey Kenneth, I would recommend against wrapping the whole method within a try/except ValueError block. I think my pull request #641 should be sufficient in addressing @tzuryby's issue. If there is another edge case that would raise an exception here, it would be valuable for us to understand why it did so.  Catching the exception and continuing would prevent us or the developer from even knowing there was an issue with the URI.
",mwielgoszewski,tzuryby
632,2012-05-29 23:41:37,"@amalakar :cake: :)
",kennethreitz,amalakar
632,2012-05-30 00:00:08,"Yay! Thanks @kennethreitz for the encouraging words. We can close Issue #505 now. I need to find the next bug to fix now. Feel free to assign/recommend something :)
",amalakar,kennethreitz
630,2012-05-23 16:05:08,"@tzuryby so make a pullrequest !
",toutouastro,tzuryby
627,2012-11-26 21:53:11,"@vlcinsky does this still exist in 0.14.2?
",sigmavirus24,vlcinsky
627,2012-11-26 22:11:01,"@vlcinsky in my impatience I installed cherrypy in a virtualenv and ran your script. If you instead return the response for the awspage function instead of calling assertions, I get 



What's interesting is that both the Signature query string and Authorization header can not both be specified. There-in lies your problem.

You can probably fix this with a hook that will remove the auth header on redirect. @kennethreitz @Lukasa @piotr-dobrogost would any of you be able to help him write the hook?
",sigmavirus24,vlcinsky
627,2012-11-26 22:24:32,":heart: @Lukasa 
",sigmavirus24,Lukasa
627,2012-11-26 23:17:32,"@Lukasa 
Nice find. Generally there's asymmetry between the first request and all subsequent ones. Notice history is not passed, either.
",piotr-dobrogost,Lukasa
627,2012-11-26 23:18:53,"@piotr-dobrogost, history can't be passed to even the first request, but it isn't exactly important.
",sigmavirus24,piotr-dobrogost
627,2012-11-26 23:25:34,"Ah, and this is why I asked you guys for help with the hook writing. I'm not as experienced with writing callbacks and am too distracted with measure theory to have thought this through as you did. Also, I'm sure you have it written correctly locally but you need to return `request` (more for @vlcinsky's benefit).
",sigmavirus24,vlcinsky
627,2012-11-26 23:45:14,"@sigmavirus24, @Lukasa , @piotr-dobrogost, you are awesome.
Now I see, it was worth to write the test code.

I found that the file, I was using for testing, got removed from my AWS bucket (I forgot about testing purpose of it).

The file is back, so it shall return some content as soon as this solution comes into effect.

I will have to study your hooks and this stuff, which I am not so familiar with yet, but probably not today, as we have here deep night 00:44 a.m CET.

Thanks for your effort.
",vlcinsky,piotr-dobrogost
627,2012-11-26 23:45:14,"@sigmavirus24, @Lukasa , @piotr-dobrogost, you are awesome.
Now I see, it was worth to write the test code.

I found that the file, I was using for testing, got removed from my AWS bucket (I forgot about testing purpose of it).

The file is back, so it shall return some content as soon as this solution comes into effect.

I will have to study your hooks and this stuff, which I am not so familiar with yet, but probably not today, as we have here deep night 00:44 a.m CET.

Thanks for your effort.
",vlcinsky,Lukasa
627,2012-11-26 23:45:14,"@sigmavirus24, @Lukasa , @piotr-dobrogost, you are awesome.
Now I see, it was worth to write the test code.

I found that the file, I was using for testing, got removed from my AWS bucket (I forgot about testing purpose of it).

The file is back, so it shall return some content as soon as this solution comes into effect.

I will have to study your hooks and this stuff, which I am not so familiar with yet, but probably not today, as we have here deep night 00:44 a.m CET.

Thanks for your effort.
",vlcinsky,sigmavirus24
627,2012-11-27 09:56:57,"@sigmavirus24: Actually you don't always have to return the `Request`, as I didn't do it locally. =) The docs aren't super clear on this, but my reading of them is that if you don't return anything from the hook then the variable passed into the hook is used. Because Python is primarily pass-by-reference, when I pop the item from the dict it affects the version of the dict in the original `Request`. Admittedly it's not super explicit and so returning the `Request` is better form, but it does work.

@vlcinsky: Please note that there is probably a bug in Requests that will prevent that hook from functioning as written. When we resolve that, I will post some sample code that functions correctly. =)
",Lukasa,vlcinsky
627,2012-11-27 09:56:57,"@sigmavirus24: Actually you don't always have to return the `Request`, as I didn't do it locally. =) The docs aren't super clear on this, but my reading of them is that if you don't return anything from the hook then the variable passed into the hook is used. Because Python is primarily pass-by-reference, when I pop the item from the dict it affects the version of the dict in the original `Request`. Admittedly it's not super explicit and so returning the `Request` is better form, but it does work.

@vlcinsky: Please note that there is probably a bug in Requests that will prevent that hook from functioning as written. When we resolve that, I will post some sample code that functions correctly. =)
",Lukasa,sigmavirus24
627,2012-11-27 14:40:50,"@piotr-dobrogost :-)
My question regarding filing a bug has two sides:

#### Shall it be filed at all?

In case, the problem is almost fixed and about to be committed and adopted into code, then fileing a new issue is not worth to do.

#### Who is the best person to write such a bug report

I do my best to describe things, I am at least familiar with or where I can provide good quality description.

But with hook being ignored - I did not even try to use one so far and I would be talking about completely new topic to me.

I agree with @sigmavirus24 we could keep this issue as discussed bug report **hooks are not passed to a new request on redirect**

I will edit my original description to point to this aspect, if anyone is interested in detailing, do it.

@all this is my first experience with Github issue cooperation incorporating more then two persons - I love it. So fast, so effective. It is like chatting while being quite productive.
",vlcinsky,piotr-dobrogost
627,2012-11-27 14:40:50,"@piotr-dobrogost :-)
My question regarding filing a bug has two sides:

#### Shall it be filed at all?

In case, the problem is almost fixed and about to be committed and adopted into code, then fileing a new issue is not worth to do.

#### Who is the best person to write such a bug report

I do my best to describe things, I am at least familiar with or where I can provide good quality description.

But with hook being ignored - I did not even try to use one so far and I would be talking about completely new topic to me.

I agree with @sigmavirus24 we could keep this issue as discussed bug report **hooks are not passed to a new request on redirect**

I will edit my original description to point to this aspect, if anyone is interested in detailing, do it.

@all this is my first experience with Github issue cooperation incorporating more then two persons - I love it. So fast, so effective. It is like chatting while being quite productive.
",vlcinsky,sigmavirus24
627,2012-11-27 14:52:30,"@vlcinsky: It's probably not worth filing a bug report, I'll just open a pull request with the fix when I get home from work this evening. =)
",Lukasa,vlcinsky
627,2012-11-27 16:27:41,"Also, @Lukasa TIL
",sigmavirus24,Lukasa
627,2012-11-27 20:14:25,"Maybe I'm missing something here but do we know how major browsers behave in various scenarios described by @vlcinsky in this thread? I think we don't and as long as we don't find out and make Requests follow their behavior the issue of sending authorization data at right moment still exists.

@sigmavirus24 

> history can't be passed to even the first request, but it isn't exactly important.

It should be passed to the first request (being empty at this moment of course) and all subsequent requests. Every request should have knowledge about history as this could be needed to make decisions in hooks for example.
I did not imply it's important in this particular issue but it's logical to note that here, as this is yet another missing piece of data not propagated to subsequent requests similarly to hooks.
If you agree I'd like to open issue for this.
",piotr-dobrogost,vlcinsky
627,2012-11-27 20:14:25,"Maybe I'm missing something here but do we know how major browsers behave in various scenarios described by @vlcinsky in this thread? I think we don't and as long as we don't find out and make Requests follow their behavior the issue of sending authorization data at right moment still exists.

@sigmavirus24 

> history can't be passed to even the first request, but it isn't exactly important.

It should be passed to the first request (being empty at this moment of course) and all subsequent requests. Every request should have knowledge about history as this could be needed to make decisions in hooks for example.
I did not imply it's important in this particular issue but it's logical to note that here, as this is yet another missing piece of data not propagated to subsequent requests similarly to hooks.
If you agree I'd like to open issue for this.
",piotr-dobrogost,sigmavirus24
627,2012-11-28 00:07:02,"@piotr-dobrogost I fully agree, that perfect solution should follow, how browsers are doing redirection.

To move this on, I have spent couple of hours, sniffing **Firefox, Chromium and IE** and have found, they do **follow Home AutoAuthorization pattern** as described in my longest text above.

Following the same pattern by Requests would follow current ""beauty of simple and intuitive API by design"" approach, which I like a lot. 

It is also possible, current behaviour is sort of security risk - the Authorization header is ""reporting"" sensitive information to another server. However, I cannot claim, it could be really misused, I am not expert on this topic and there could be some sort of protection against record - reply attack.

**Could someone reopen this issue?**

For history not being passed, I would ask someone like @piotr-dobrogost to file separate issue, as it is not really affecting resolution of this one, but deserves some attention.
",vlcinsky,piotr-dobrogost
627,2012-11-28 09:28:06,"@Lukasa I see, you prefer issues being closed :-).

But: you agree, the Auth handler is doing things wrong - and you prefer to keep the issue closed.

Isn't this sort of issue maintainance anti-pattern?

Is anyone supposed to review closed issues after 1.0.0 is completed? Are we hoping, someone will just remember it?

To me, milestone ""later"" being assigned to this issue seems natural. And as 1.0.0 is on the table, it is quite likely, it would be closed even before 1.0.0 is completed.

You and others put some effort on analysing and resolving current problem (so there is some temporary solution in devel branch now), I put some effort on providing bug report and proposal for handling authorization. I think, having this issue open and living under ""later"" milestone would ensure, this effort will not be forgotten and is more likely to contribute to higher usability and quality of Requests in future.

Other option would be to extract remaining issues from this bug (handling Authorization header with redirect and History not being kept over border of redirect), assign them to ""later"" milestone and close this bug, as there is temporary solution available.
",vlcinsky,Lukasa
627,2012-11-28 14:53:33,"@vlcinsky see #975 if you haven't already
",sigmavirus24,vlcinsky
624,2012-05-19 17:37:16,"@toutouastro that would be fantastic!
",kennethreitz,toutouastro
624,2013-04-18 02:00:10,"@kennethreitz any chance for a Portuguese repository? :)
",duailibe,kennethreitz
624,2013-04-18 02:00:59,"@duailibe are you volunteering? :)
",kennethreitz,duailibe
624,2013-04-18 02:01:56,"@kennethreitz yes! :)
",duailibe,kennethreitz
624,2013-04-18 02:06:37,"@duailibe thanks so much! Just push here :) https://github.com/requests/requests-docs-pt
",kennethreitz,duailibe
624,2013-04-18 02:10:51,"@kennethreitz awesome! Thanks! :cake: 
",duailibe,kennethreitz
624,2013-04-18 17:23:15,"@serpulga I'd like to contribute to the Spanish translation too. I see you're doing it now, but I think some things have to be changed, and I'd also like to help you finish the entire translation if you haven't. What should be the procedure?
",omederos,serpulga
624,2013-04-18 17:43:36,"Hi @omederos. I think regular git-flow / pull requests would be the way to go.
I think I pushed the one document that was left, and I honestly don't know
what's next. Let's see if we can get some pointers from @kennethreitz  for the
other translators.

Thanks,

Sergio
",serpulga,kennethreitz
624,2013-04-18 17:43:36,"Hi @omederos. I think regular git-flow / pull requests would be the way to go.
I think I pushed the one document that was left, and I honestly don't know
what's next. Let's see if we can get some pointers from @kennethreitz  for the
other translators.

Thanks,

Sergio
",serpulga,omederos
624,2013-04-19 19:13:04,"@Kwpolska all set! Let me know when you have an update so I can make it official :)

https://github.com/requests/requests-docs-pl

Thanks so much :)
",kennethreitz,Kwpolska
624,2014-01-24 15:23:05,"Can I ask what does it need to put the spanish documentation online, @kennethreitz ? 
(Besides some update, it has 9 months old, as I see here: https://github.com/requests/requests-docs-es)
",esparta,kennethreitz
624,2015-01-28 00:12:15,"@deusExCore start a fork and start translating the documentation in place. When we can grab his attention, we'll have @kennethreitz add you to the requests organization and help you move your fork to github.com/requests/requests-docs-tr and then he, or I will have to set up DNS to resolve that for you
",sigmavirus24,kennethreitz
624,2015-05-28 13:15:56,"Hi guys, here you have a candidate for translating the docs to Italian!
@kennethreitz @sigmavirus24 if it's a good thing, I can start working on that on a fork
",csparpa,kennethreitz
624,2015-05-28 13:15:56,"Hi guys, here you have a candidate for translating the docs to Italian!
@kennethreitz @sigmavirus24 if it's a good thing, I can start working on that on a fork
",csparpa,sigmavirus24
624,2015-05-30 19:10:23,"@csparpa go for it!
",kennethreitz,csparpa
624,2015-06-01 18:25:27,"@kennethreitz would you setup a specific repo under https://github.com/requests for the IT translation? I'll wait for that if you're going to create it
",csparpa,kennethreitz
624,2015-06-01 19:09:32,"@csparpa please start translating and as soon as it's mostly done is when we usually import your fork into the @requests org
",sigmavirus24,csparpa
624,2015-06-01 19:43:39,"Got it, thanks ;)

Claudio Sparpaglione
http://csparpa.github.io
On 1 Jun 2015 9:10 pm, ""Ian Cordasco"" notifications@github.com wrote:

> @csparpa https://github.com/csparpa please start translating and as
> soon as it's mostly done is when we usually import your fork into the
> @requests https://github.com/requests org
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/624#issuecomment-107673876
> .
",csparpa,csparpa
624,2015-06-14 12:33:45,"Hi @sigmavirus24 I'm done with the Italian translation - this is on a dedicated branch on my fork. What's the next step? Thanks
",csparpa,sigmavirus24
624,2015-06-14 16:43:13,"So now's the time when @Lukasa or @kennethreitz should add you (@csparpa) to the translators team. They also need to set up a repository for the translation and import it from your fork. I'll set up the stuff so that it appears on python-requests.org, once they've taken care of that. (I'm not an owner/admin on the @requests org so I can't do that stuff for you.)
",sigmavirus24,kennethreitz
624,2015-06-14 16:43:13,"So now's the time when @Lukasa or @kennethreitz should add you (@csparpa) to the translators team. They also need to set up a repository for the translation and import it from your fork. I'll set up the stuff so that it appears on python-requests.org, once they've taken care of that. (I'm not an owner/admin on the @requests org so I can't do that stuff for you.)
",sigmavirus24,csparpa
624,2015-06-14 16:43:55,"@sigmavirus24 That's a terrible oversight, I'll add you as an admin as well.
",Lukasa,sigmavirus24
624,2015-06-14 16:50:39,"@csparpa You are now the proud father of [one repository of Italian-translated docs](https://github.com/requests/requests-docs-it)!
",Lukasa,csparpa
624,2015-06-15 10:31:59,"@Lukasa I'll keep on working on the translation (there are lots of typos and possible rephrasings).
Can you add the repo https://github.com/requests/requests-docs-it to the Translation Team repositories?Thanks
",csparpa,Lukasa
624,2015-06-15 10:34:51,"@csparpa Sorry, done =)
",Lukasa,csparpa
624,2015-08-24 17:01:21,"That's a bit more @sigmavirus24's area of expertise. Ping ping. =)
",Lukasa,sigmavirus24
624,2015-08-24 18:38:15,"@carlosvargas sorry about that. I didn't realize we were missing that. I just set up the CNAME for that so it should hopefully be fixed shortly
",sigmavirus24,carlosvargas
624,2015-08-24 18:45:59,"@sigmavirus24 can I take the chance to notify that on the main Requests
page (http://www.python-requests.org/en/latest/) there is no link to the
Italian translation of the documentation (http://it.python-requests.org/) ?
;-)

Thank you

Cla
On 24 Aug 2015 8:38 pm, ""Ian Cordasco"" notifications@github.com wrote:

> @carlosvargas https://github.com/carlosvargas sorry about that. I
> didn't realize we were missing that. I just set up the CNAME for that so it
> should hopefully be fixed shortly
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/624#issuecomment-134332797
> .
",csparpa,carlosvargas
624,2015-08-24 18:45:59,"@sigmavirus24 can I take the chance to notify that on the main Requests
page (http://www.python-requests.org/en/latest/) there is no link to the
Italian translation of the documentation (http://it.python-requests.org/) ?
;-)

Thank you

Cla
On 24 Aug 2015 8:38 pm, ""Ian Cordasco"" notifications@github.com wrote:

> @carlosvargas https://github.com/carlosvargas sorry about that. I
> didn't realize we were missing that. I just set up the CNAME for that so it
> should hopefully be fixed shortly
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/624#issuecomment-134332797
> .
",csparpa,sigmavirus24
624,2015-08-25 00:57:56,"@csparpa I think that's merely a matter of sending a PR here to add it. ;)
",sigmavirus24,csparpa
624,2015-08-25 07:20:13,"Of course ;) I just wanted to notify the issue so it gets tracked somewhere
but I had better track it on a github issue instead of here... Sorry guys,
I'll do it! Ciao

Cla
On 25 Aug 2015 2:58 am, ""Ian Cordasco"" notifications@github.com wrote:

> @csparpa https://github.com/csparpa I think that's merely a matter of
> sending a PR here to add it. ;)
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/624#issuecomment-134431167
> .
",csparpa,csparpa
624,2016-03-02 14:05:04,"@kennethreitz @Lukasa @sigmavirus24 
I'm a native speaker of Chinese (Simplified) and Chinese (Traditional) .
Current translation is kinda outdated. Could you please add me to the organization, so that I can work on it?
",caizixian,kennethreitz
624,2016-03-02 14:05:04,"@kennethreitz @Lukasa @sigmavirus24 
I'm a native speaker of Chinese (Simplified) and Chinese (Traditional) .
Current translation is kinda outdated. Could you please add me to the organization, so that I can work on it?
",caizixian,Lukasa
624,2016-03-02 14:05:04,"@kennethreitz @Lukasa @sigmavirus24 
I'm a native speaker of Chinese (Simplified) and Chinese (Traditional) .
Current translation is kinda outdated. Could you please add me to the organization, so that I can work on it?
",caizixian,sigmavirus24
624,2017-01-25 05:09:44,@ferozah83 go for it!,kennethreitz,ferozah83
614,2012-05-15 15:34:28,"Yea sorry about that. Only use 2.7 myself and forgot that 3 does not allow u''. Nice work @travisbot =) 

brb updating...
",ib-lundgren,travisbot
614,2012-05-15 18:43:17,"@ib-lundgren 

3.3 will [support](http://docs.python.org/dev/whatsnew/3.3.html#pep-414-explicit-unicode-literals) explicit unicode literals. 
",piotr-dobrogost,ib-lundgren
612,2012-05-15 14:12:26,"@arteme Check out #578. There are plans to implements keys(), values(), and items(), so that a user of Requests can call dict(resp.cookies) and get a name value dict.
",joshimhoff,arteme
611,2012-05-14 15:02:26,"@toutouastro always a work in progress :)
",kennethreitz,toutouastro
608,2012-05-14 16:33:46,"@kracekumar no, this is a backwards incompatible change.
",kennethreitz,kracekumar
606,2012-08-26 16:15:52,"@von 
What version of openssl are you using? Is your connection using TLSv1?

I do not get the same problem when accessing torproject.org:

> import requests
> requests.get('https://www.torproject.org/projects/torbrowser.html.en')
> < Response [200] >

This issue must be related to your network setup or the version of openssl you are using.
",davidfischer,von
606,2012-08-26 19:20:57,"@davidfischer > What version of openssl are you using? Is your connection using TLSv1?

$ openssl version
OpenSSL 0.9.8r 8 Feb 2011

EDITED to correct:

> > > ssl.OPENSSL_VERSION
> > > 'OpenSSL 0.9.7l 28 Sep 2006'

$ python -V
Python 2.7.1

Looks like TLSv1 from what I can tell...
$ openssl s_client -connect www.torproject.org:443 | grep Protocol
depth=2 /C=US/O=DigiCert Inc/OU=www.digicert.com/CN=DigiCert High Assurance EV Root CA
verify error:num=20:unable to get local issuer certificate
verify return:0
    Protocol  : TLSv1
...

And to verify:

> > > import requests
> > > requests.get('https://www.torproject.org/projects/torbrowser.html.en')
> > > Traceback (most recent call last):
> > >   File ""<stdin>"", line 1, in <module>
> > >   File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/requests-0.13.9-py2.7.egg/requests/api.py"", line 65, in get
> > >     return request('get', url, *_kwargs)
> > >   File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/requests-0.13.9-py2.7.egg/requests/safe_mode.py"", line 39, in wrapped
> > >     return function(method, url, *_kwargs)
> > >   File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/requests-0.13.9-py2.7.egg/requests/api.py"", line 51, in request
> > >     return session.request(method=method, url=url, **kwargs)
> > >   File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/requests-0.13.9-py2.7.egg/requests/sessions.py"", line 252, in request
> > >     r.send(prefetch=prefetch)
> > >   File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/requests-0.13.9-py2.7.egg/requests/models.py"", line 632, in send
> > >     raise SSLError(e)
> > > requests.exceptions.SSLError: [Errno 1] _ssl.c:499: error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown protocol
",von,davidfischer
606,2012-08-26 19:40:49,"@davidfischer Thanks for the pointer about the old version of openssl, updating to 2.7.3 with the latest Mac installer fixed it.


",von,davidfischer
606,2012-08-26 19:49:35,"@von 
Good point about the version of openssl Python is using, not just the openssl version on the system. Glad we got it resolved.
",davidfischer,von
601,2012-11-26 21:21:13,"@slingamn do #611 and #962 satisfy your desire for better documented testing practices?
",sigmavirus24,slingamn
600,2012-05-08 22:31:31,"@Arfrever are you familiar with the forking / pull-request model? http://help.github.com/send-pull-requests/
",slingamn,Arfrever
600,2012-06-27 19:50:42,"@Arfrever
Fork this repository. Clone it locally to your machine. Make the change. Push it to your fork. Submit a pull request referencing this issue. It will give you attribution for the fix.
",sigmavirus24,Arfrever
599,2012-05-13 21:16:54,"@piotr-dobrogost indeed, the plan is to disable streaming by default but give it a simple flag
",kennethreitz,piotr-dobrogost
598,2012-05-09 09:54:07,"@reclosedev Interesting, but not the problem I'm having ;-)
@piotr-dobrogost Okay, I suppose it is my setup indeed:



I'm on Ubuntu 12.04, behind a proxy (which is known to do funny things), could that be the problem? If so, why does it work with `urllib2`, but not with `requests` and `urllib`?
",maebert,piotr-dobrogost
598,2012-05-09 09:54:07,"@reclosedev Interesting, but not the problem I'm having ;-)
@piotr-dobrogost Okay, I suppose it is my setup indeed:



I'm on Ubuntu 12.04, behind a proxy (which is known to do funny things), could that be the problem? If so, why does it work with `urllib2`, but not with `requests` and `urllib`?
",maebert,reclosedev
598,2016-10-30 03:53:17,"@maebert 

I run it in Python3.5, seems your `]​` is special char.


",guyskk,maebert
597,2012-07-02 22:49:48,"I agree with @mponton — the current default chunk_size value of 1 for iter_content() seems broken. We were bitten by this performance regression when upgrading to 0.13.2.
",berg,mponton
595,2012-05-08 20:26:58,"@reclosedev 

Nice work.

> Yes, but `get` is patched in this testcase

... and all sync tests are being run using async api as well. As your two requests in the async test are sequential you might want to replace sync test with the async one (which tests more) and remove the latter. You might also want to reference request from reply instead the other way around in case the link in this direction is removed in the future :)

Btw, please remember not to link to moving target (develop) as this doesn't make sense.
",piotr-dobrogost,reclosedev
595,2012-05-09 06:36:54,"@piotr-dobrogost 

Maybe you are right about  sync test, but I think it's not superfluous, because, for example, async api may use another way to create requests in the future, but `return_response=False` should be tested.

> You might also want to reference request from reply instead the other way around in case the link in this direction is removed in the future :)

Can you elaborate?

> Btw, please remember not to link to moving target (develop) as this doesn't make sense.

Are you talking about links in comments? Fixed :)
",reclosedev,piotr-dobrogost
584,2012-05-04 18:56:12,"@piotr-dobrogost can you clarify what happens now if you `pip install requests` on Windows? Does it crash?
",slingamn,piotr-dobrogost
583,2012-05-08 16:00:24,"@joshimhoff go for it :)
",kennethreitz,joshimhoff
583,2012-05-10 18:11:16,"@joshimhoff fantastic work.
",kennethreitz,joshimhoff
578,2012-05-15 14:52:54,"@slingamn I'll work on a patch for this, if no one else is doing it.
",joshimhoff,slingamn
578,2012-05-15 18:36:58,"@dhagrow 

> It seems reasonable to have cookies.keys return cookie names, and cookies.values return values (...)

What with cookies with the same name and different domain or path?
",piotr-dobrogost,dhagrow
578,2012-05-15 19:21:36,"@slingamn 
Sorry for not responding earlier. I don't think I'll find the time for this soon. If @joshimhoff is willing, that would be great.

@piotr-dobrogost
I see two use-cases for `RequestsCookieJar`:
- One is the dev interested in a single domain, in which case `dict(res.cookies)` would be sufficient.
- The other is the dev interested in multiple domains, in which case it is necessary to be explicit.

I think I would default to the first case, but force the dev to use the second when necessary to avoid ambiguity. In other words, when using the simple API would cause a conflict, raise an exception.



I'm not sure if it would be better to raise the exception when more than one domain/path is stored in the jar, or only when there is an actually conflicting cookie. I would say the former to favor explicitness. It doesn't make much sense to have a `dict` of cookies from multiple domains.

The simple API would have `res.cookies` act as a standard `dict` object, including the `.get` method. I would then add a special API for `Cookie` objects:
- `.get_cookie(name, domain, path, ...)` returns a `Cookie` object.
- `.set_cookie(name, value, domain, path, ...)` sets/creates a `Cookie`.
- `.get_dict(domain, path, ...)` returns a `name:value` dict of cookies that match the specified requirements.
- perhaps some useful info methods as well, like `.list_domains`.

Anyway, it's not a simple problem. It's worth some discussion to get it right.
",dhagrow,joshimhoff
578,2012-05-15 19:21:36,"@slingamn 
Sorry for not responding earlier. I don't think I'll find the time for this soon. If @joshimhoff is willing, that would be great.

@piotr-dobrogost
I see two use-cases for `RequestsCookieJar`:
- One is the dev interested in a single domain, in which case `dict(res.cookies)` would be sufficient.
- The other is the dev interested in multiple domains, in which case it is necessary to be explicit.

I think I would default to the first case, but force the dev to use the second when necessary to avoid ambiguity. In other words, when using the simple API would cause a conflict, raise an exception.



I'm not sure if it would be better to raise the exception when more than one domain/path is stored in the jar, or only when there is an actually conflicting cookie. I would say the former to favor explicitness. It doesn't make much sense to have a `dict` of cookies from multiple domains.

The simple API would have `res.cookies` act as a standard `dict` object, including the `.get` method. I would then add a special API for `Cookie` objects:
- `.get_cookie(name, domain, path, ...)` returns a `Cookie` object.
- `.set_cookie(name, value, domain, path, ...)` sets/creates a `Cookie`.
- `.get_dict(domain, path, ...)` returns a `name:value` dict of cookies that match the specified requirements.
- perhaps some useful info methods as well, like `.list_domains`.

Anyway, it's not a simple problem. It's worth some discussion to get it right.
",dhagrow,piotr-dobrogost
578,2012-05-15 19:21:36,"@slingamn 
Sorry for not responding earlier. I don't think I'll find the time for this soon. If @joshimhoff is willing, that would be great.

@piotr-dobrogost
I see two use-cases for `RequestsCookieJar`:
- One is the dev interested in a single domain, in which case `dict(res.cookies)` would be sufficient.
- The other is the dev interested in multiple domains, in which case it is necessary to be explicit.

I think I would default to the first case, but force the dev to use the second when necessary to avoid ambiguity. In other words, when using the simple API would cause a conflict, raise an exception.



I'm not sure if it would be better to raise the exception when more than one domain/path is stored in the jar, or only when there is an actually conflicting cookie. I would say the former to favor explicitness. It doesn't make much sense to have a `dict` of cookies from multiple domains.

The simple API would have `res.cookies` act as a standard `dict` object, including the `.get` method. I would then add a special API for `Cookie` objects:
- `.get_cookie(name, domain, path, ...)` returns a `Cookie` object.
- `.set_cookie(name, value, domain, path, ...)` sets/creates a `Cookie`.
- `.get_dict(domain, path, ...)` returns a `name:value` dict of cookies that match the specified requirements.
- perhaps some useful info methods as well, like `.list_domains`.

Anyway, it's not a simple problem. It's worth some discussion to get it right.
",dhagrow,slingamn
578,2012-05-15 19:49:42,"@joshimhoff go for it :-)
",slingamn,joshimhoff
578,2012-05-18 04:15:19,"@kennethreitz Any opinion on the error handling that @dhagrow mentions? I'm working on a patch, but I don't really think I should make that decision.
",joshimhoff,dhagrow
565,2012-04-26 20:19:15,"@travisbot that's the same traceback I get from `develop` under Python 3.2.
",slingamn,travisbot
565,2012-04-26 20:59:16,"@slingamn I'll update the repo to pass today :)
",kennethreitz,slingamn
565,2012-05-02 00:21:38,"Thanks! The core implementation is all @dhagrow's, I just rearranged stuff and made the tests green :-)
",slingamn,dhagrow
565,2012-05-02 00:22:30,"@dhagrow and @slingamn  are my heroes.

:sparkles: :cake: :sparkles:
",kennethreitz,slingamn
565,2012-05-02 00:22:30,"@dhagrow and @slingamn  are my heroes.

:sparkles: :cake: :sparkles:
",kennethreitz,dhagrow
559,2012-05-06 08:29:42,"@barberj there is some (likely trivial) merge conflict now; can you update? Sorry.
",slingamn,barberj
559,2012-05-06 21:54:50,"@slingamn done
",barberj,slingamn
559,2012-05-06 22:02:45,"@barberj hey sorry, it looks like the merge is messed up; upstream changes since your branch are showing as part of your branch.

Can you try this?


",slingamn,barberj
559,2012-05-06 22:16:52,"@slingamn its not letting me push...
! [rejected]        issue_526 -> issue_526 (non-fast-forward)
",barberj,slingamn
559,2012-05-06 22:19:47,"@slingamn looks like that did it. thanks
",barberj,slingamn
559,2012-05-06 23:26:44,"@slingamn good call. obvious test case that i missed... mmm cake.
",barberj,slingamn
558,2012-04-20 03:34:25,"@laurentb exactly.
",kennethreitz,laurentb
557,2012-06-19 09:52:10,"@gkappel:  
I also faced a similar error when using pyinstaller with Httplib2.
You need to explicitly pass the path to ssl client cert file to the 'cert' argument http request. Also, you would need to package your. .pem file as a data file in pyinstaller.

Hope it helps!
",sahilgupta,gkappel
557,2012-06-19 17:08:40,"@sahilgupta 

That works! Thanks a lot.
",pjwerneck,sahilgupta
557,2012-07-11 04:21:02,"@sahilgupta Could you elaborate on how you fixed this? I'm having this problem as well and it would be great to know your solution.
",seanfisk,sahilgupta
557,2012-07-19 03:37:58,"@sahilgupta Thanks for the help! I think I've figured out how to package the data file with PyInstaller, but I couldn't figure out how to pass that to requests to use, and I've already got enough code that uses requests that I'm not willing to switch to httplib2. For now, I've just disabled SSL verification.
",seanfisk,sahilgupta
557,2012-07-19 11:33:01,"@seanfisk  Did some further digging. Specifying certificate is even easier in requests than with httplib2. Try something on the lines of :



and enable SSL verification.
This would force requests to use this certificate instead of the default one as per http://kennethreitz.com/major-progress-for-requests.html
",sahilgupta,seanfisk
557,2012-07-21 02:47:27,"@sahilgupta Thanks, that worked. It was actually a RTFM on my part. Checked the [SSL Verification section](http://docs.python-requests.org/en/latest/user/advanced/#ssl-cert-verification) in the requests documentation and it was right there, and now it works. I could swear I was passing the path to `verify` before, but it's working now, so who cares. Here's the code I used for future reference:





Thanks for your help!
",seanfisk,sahilgupta
557,2012-12-11 12:27:32,"Which version of requests do you have installed and how did you install it? 

@viciu thanks for editing your comment with the versions.
",sigmavirus24,viciu
557,2012-12-13 00:06:15,"@slingamn is it possible @viciu doesn't have that path? Maybe some other package he's installed removed it/replaced it/moved it?
",sigmavirus24,slingamn
557,2012-12-13 00:06:15,"@slingamn is it possible @viciu doesn't have that path? Maybe some other package he's installed removed it/replaced it/moved it?
",sigmavirus24,viciu
557,2013-01-21 20:04:48,"@slingamn: Given #1065 (which was opened after a discussion on #1033), can we close this?
",Lukasa,slingamn
557,2013-01-26 18:06:17,"@Lukasa I'm for closing it. 
",sigmavirus24,Lukasa
557,2014-08-05 22:08:21,"@rwolst I think you have the wrong project altogether.
",sigmavirus24,rwolst
557,2014-10-15 08:10:17,"@rwolst Thank you! Your addition to `httplib2/__init__.py` did the trick for me!
",AlexVonB,rwolst
556,2012-04-22 11:49:16,"@barberj I don't know—that's the point. I'm not sure what the correct approach is for py2 AND py3.
",idan,barberj
555,2012-04-19 01:32:33,"@kennethreitz: Can you suggest a workaround?
",bhadra,kennethreitz
554,2012-04-20 03:41:54,"@kmike essentially i need ""native"" strings (unicode in 3, bytes in 2) with the same literal, since these are integration tests, not unit tests. That's why I can't use just do `from __future_ import unicode_literals`.

But I also need explicit unicode strings and explicit bytecode strings.
",kennethreitz,kmike
552,2012-04-23 10:39:30,"@kennethreitz, quick question.  Do you consider `DEFAULT_CA_BUNDLE_PATH` part of the supported API now?  I'm just wondering if I can call `assert` against it, and expect code to carry on working for the foreseeable future.  Not asking for promises, just a nod ;)

Perhaps, the One Blessed Method for disabling the `certifi` support should be stated in a packaging document?  I'd be surprised if the changes here allowed packagers to use the package as-is, and a continuing variety of solutions is quite difficult to support for naïve `requests` users like me.
",JNRowe,kennethreitz
552,2012-04-23 14:51:26,"@JNRowe Yeah, it's tricky. Perhaps, as the administrator, you could `export REQUESTS_CA_BUNDLE` in `/etc/profile.d` or something similar? That would globally override local installations of `certifi`.

What further changes do you think would be required for packaging?
",slingamn,JNRowe
552,2012-04-23 19:04:35,"_EDIT_: Oh wow, sorry guys.  I didn't realise this comment was so long, it'll teach me to keep to the tiny textbox next time.

> @JNRowe Yeah, it's tricky. Perhaps, as the administrator, you could `export REQUESTS_CA_BUNDLE` in `/etc/profile.d` or something similar? That would globally override local installations of `certifi`.

Good idea, and it would work for managed desktop and server deployment.

My question was more concerned with a stable method for testing the environment we're running under on client systems.  I guess if `requests.utils.get_os_ca_bundle_path()` is part of the stable API we could check that, `.startswith('/etc/')` is most of the way there for me.

It is far better than the current situation, needing a lot of code pasting to check for various changes.  On Debian,  the `where()` call is just replaced with `/etc/ssl/certs/ca-certificates.crt`.  On Gentoo, `certifi` is installed with a symlink to the system certs, which necessitates a call to `os.path.realpath()`.

> What further changes do you think would be required for packaging?

Without trying to be deliberately obtuse I'd say ""ask the packagers"".  I may be totally wrong, the next round of distributor packages could accept this behaviour and ship vanilla packages.  The only reason I doubt it is the automagic nature of the changes, the certs being chosen depending on whether another package is currently installed system-wide or available in `$PYTHONUSERBASE` or somewhere else seems a little off.

Don't get me wrong, I think you and @kennethreitz came to the right conclusion for the common case.  Most people simply don't care about this stuff that deeply, and the ones that do will patch the behaviour anyway.  That is why I resigned myself to checking at runtime, and asked for a little confirmation on an acceptable way to do it.
",JNRowe,JNRowe
552,2012-04-23 19:04:35,"_EDIT_: Oh wow, sorry guys.  I didn't realise this comment was so long, it'll teach me to keep to the tiny textbox next time.

> @JNRowe Yeah, it's tricky. Perhaps, as the administrator, you could `export REQUESTS_CA_BUNDLE` in `/etc/profile.d` or something similar? That would globally override local installations of `certifi`.

Good idea, and it would work for managed desktop and server deployment.

My question was more concerned with a stable method for testing the environment we're running under on client systems.  I guess if `requests.utils.get_os_ca_bundle_path()` is part of the stable API we could check that, `.startswith('/etc/')` is most of the way there for me.

It is far better than the current situation, needing a lot of code pasting to check for various changes.  On Debian,  the `where()` call is just replaced with `/etc/ssl/certs/ca-certificates.crt`.  On Gentoo, `certifi` is installed with a symlink to the system certs, which necessitates a call to `os.path.realpath()`.

> What further changes do you think would be required for packaging?

Without trying to be deliberately obtuse I'd say ""ask the packagers"".  I may be totally wrong, the next round of distributor packages could accept this behaviour and ship vanilla packages.  The only reason I doubt it is the automagic nature of the changes, the certs being chosen depending on whether another package is currently installed system-wide or available in `$PYTHONUSERBASE` or somewhere else seems a little off.

Don't get me wrong, I think you and @kennethreitz came to the right conclusion for the common case.  Most people simply don't care about this stuff that deeply, and the ones that do will patch the behaviour anyway.  That is why I resigned myself to checking at runtime, and asked for a little confirmation on an acceptable way to do it.
",JNRowe,kennethreitz
552,2012-04-23 19:33:36,"@JNRowe Thanks, these are really good observations. I'll get in touch with the guy who maintains `requests` for Fedora and see if this change is sufficient for him to package `python-requests` with a dependency on `ca-certificates`.

I understand now why you wanted to be able to test `DEFAULT_CA_BUNDLE_PATH`. I'd put in a tentative +1 for making this part of the API, but my understanding of how stable that would be is shaky at best :-)

By the way, do you know what the standard path to the CA certificates is on Gentoo, and the name of the package that provides them?
",slingamn,JNRowe
552,2012-04-23 20:22:32,"Fedora maintainer here, @slingamn this is a clean approach, i was lucky that people allowed me to package python-certifi, i would be happy to remove any dependency on certifi and default to system CA certs.
",sagarun,slingamn
543,2012-04-10 04:22:02,"This literally appears to have been made with requests in mind (and is mentioned in the examples as an acceptable project). Input @shazow, if you don't mind?
",TkTech,shazow
541,2012-05-03 07:27:09,"@slingamn you're a machine
",kennethreitz,slingamn
539,2012-04-10 22:39:30,"Good thoughts, @mponton :) +1 on both of them.
",dbrgn,mponton
539,2012-05-06 19:56:31,"@mponton when `chunk_size` was set to `1`, requests of more than a few KB were exponentially slower than with any other client. 
",kennethreitz,mponton
539,2012-05-07 00:19:21,"@kennethreitz Well, of course it will be. You're reading ""chunks"" of 1 byte at a time. However, when we talked about changing the default `chunk_size`, it was for `iter_lines()`, not `iter_chunk()` (see my pull request). The reason was to allow `iter_lines()` to behave like most people using requests would expect it to behave: Read lines as they come in. If you do use `iter_lines()` to read from a streaming API like the Twitter API, and it was used with an `chunk_size` of 10K, you may have to wait a freaking long time before you ever get any line.

That said, I'm open to alternatives, but unless you start polling the socket in non-blocking mode, the `read(chunk_size)` will block until it get `chunk_size` or the connection is closed AFAIK. Please let me know if I'm missing something.

@piotr-dobrogost If you know of another implementation to read lines as they come in from a socket while using blocking I/O, please feel free to let me (or us) know. I'm not trying to sell my quick fix to anyone, I'm simply trying to find a way to have `iter_lines()` behave like (I hope) it was meant to behave.

@kennethreitz Also, if you feel `chunk_size` should not be set to 1 in the distribution, please change it back, I don't want to impose on anybody. I can specify it in my code but I'm pretty sure you'll get regular ""issues"" opened about this thing by people wondering why their ""lines"" are not coming in when sent by the server as expected. You already had two soon after the initial code change, @gwrtheyrn and I.
",mponton,kennethreitz
539,2012-05-07 00:19:21,"@kennethreitz Well, of course it will be. You're reading ""chunks"" of 1 byte at a time. However, when we talked about changing the default `chunk_size`, it was for `iter_lines()`, not `iter_chunk()` (see my pull request). The reason was to allow `iter_lines()` to behave like most people using requests would expect it to behave: Read lines as they come in. If you do use `iter_lines()` to read from a streaming API like the Twitter API, and it was used with an `chunk_size` of 10K, you may have to wait a freaking long time before you ever get any line.

That said, I'm open to alternatives, but unless you start polling the socket in non-blocking mode, the `read(chunk_size)` will block until it get `chunk_size` or the connection is closed AFAIK. Please let me know if I'm missing something.

@piotr-dobrogost If you know of another implementation to read lines as they come in from a socket while using blocking I/O, please feel free to let me (or us) know. I'm not trying to sell my quick fix to anyone, I'm simply trying to find a way to have `iter_lines()` behave like (I hope) it was meant to behave.

@kennethreitz Also, if you feel `chunk_size` should not be set to 1 in the distribution, please change it back, I don't want to impose on anybody. I can specify it in my code but I'm pretty sure you'll get regular ""issues"" opened about this thing by people wondering why their ""lines"" are not coming in when sent by the server as expected. You already had two soon after the initial code change, @gwrtheyrn and I.
",mponton,piotr-dobrogost
539,2012-05-08 05:53:22,"@mponton would `iter_lines` ideally put the socket into nonblocking mode and read until it sees `\n`?
",slingamn,mponton
538,2012-04-06 20:56:23,"@EwyynTomato 

It's true that the library's name which consists of two general words doesn't help when searching the web. Searching would be easier if the lib was named for instance pyrequests following the common pattern among Python libs. Well, I guess you have to deal with it.

Searching for word `python-requests` (http://stackoverflow.com/search?q=python-requests&submit=search) gives 45 results and searching for tag `python-requests` (http://stackoverflow.com/questions/tagged/python-requests) gives 10 results. If you don't find the question you're looking for please ask one.
",piotr-dobrogost,EwyynTomato
536,2012-04-21 19:32:44,"@benmao 
What Python version are we talking about?

@rwzehner 
What does HTML spec have to do with HTTP headers?
",piotr-dobrogost,rwzehner
536,2012-04-21 19:32:44,"@benmao 
What Python version are we talking about?

@rwzehner 
What does HTML spec have to do with HTTP headers?
",piotr-dobrogost,benmao
536,2012-04-22 03:30:17,"@piotr-dobrogost Python Version: 2.7.2

I found monkeys.py is copy of Cookie.py which from Python Lib.

urllib2 + cookielib works well.

So I do not think ""monkeys.py"" have problem.
",benmao,piotr-dobrogost
536,2012-04-29 19:41:33,"@benmao

> The result should be like this.



I guess you meant 



because backslash which is not part of escape sequence is doubled in string representation.

According to RFC 2109 backslash is not allowed in cookie's value unless the value is quoted. As [`Cookie`](http://docs.python.org/library/cookie.html) module implements this RFC (albeit with modifications) this could be the reason for not _accepting_ value `\u4E2A\u4EBA` as a valid cookie's value. The reason it works in `cookielib` is because it has its own `Cookie` class and apparently different parser - see [parse_ns_headers](http://hg.python.org/cpython/file/2.7/Lib/cookielib.py#l444) method. [Cookies](https://github.com/sashahart/cookies) lib doesn't allow this cookie value as well:



However, there is a difference when using double quoted value of cookie:



RFC 2109 defines cookie's value (`value`) as:



, where `quoted-string` is defined in RFC 2068 as:



Because there's no further description of quoting mechanism I don't know which behavior is right (see https://github.com/sashahart/cookies/issues/1).

What's interesting is that RFC 6265 (which obsoletes RFC 2109 and which Cookies supposedly implements) states this:



which does not allow backslash in cookie's value at all. This is surprising as this RFC was meant to take into account existing practice and relax rules of previous RFCs.
",piotr-dobrogost,benmao
536,2012-04-29 20:02:35,"@rwzehner 

> Bottom line, although HTTP allows one to specify different encodings for the body of a reply, all of the header information must be Latin-1 encoded.

No, it does not have to be Latin-1. See http://stackoverflow.com/a/818188/95735
",piotr-dobrogost,rwzehner
536,2012-05-03 07:33:42,"I forked @benmao's gist at https://gist.github.com/2584020 , changing it to use the new `CookieJar` API.


",slingamn,benmao
536,2012-05-03 08:09:49,"Anyway, I think this is probably closeable, although @piotr-dobrogost and @rwzehner may want to weigh in on the standards-compliance issues. Basically, for better or worse, cookie parsing is now being deferred directly to `cookielib`.
",slingamn,rwzehner
536,2012-05-03 08:09:49,"Anyway, I think this is probably closeable, although @piotr-dobrogost and @rwzehner may want to weigh in on the standards-compliance issues. Basically, for better or worse, cookie parsing is now being deferred directly to `cookielib`.
",slingamn,piotr-dobrogost
522,2012-11-26 23:28:21,"@idan @kennethreitz you never answered my question, and beyond that accepting update sequences everywhere seems a bit unreasonable
",sigmavirus24,kennethreitz
522,2012-11-26 23:28:21,"@idan @kennethreitz you never answered my question, and beyond that accepting update sequences everywhere seems a bit unreasonable
",sigmavirus24,idan
520,2012-03-31 16:26:32,"Are we talking about urllib3 request/response objects, or requests request/response objects?

urllib3 cleans up in the [response.release_conn](https://github.com/shazow/urllib3/blob/master/urllib3/response.py#L96) method. The connection gets None'd for the exact reason that @Bluehorn mentioned. We could also None the `self._fp` but that's more for custom interfaces; I don't think httplib uses it.

We can definitely be more aggressive with closing connections in the cleanup of Requests's context manager. Right now to manually close connections, you'd need to iterate for each pool and pop its queue of connections and close them if they're open. If that's what we want to do, then I can definitely add a helper in urllib3 for ""closing pools"" which will do that in one call.
",shazow,Bluehorn
520,2012-05-06 10:09:20,"Here's my understanding of this issue so far:
1. @Bluehorn and @gsakkis are right: the root problem is the reference cycle between `requests.Request` and `requests.Response`.
2. Breaking this cycle in a crude way, e.g., removing the `request` member of the `Response` class, fixes @Bluehorn's test case.
3. Although it might be possible to improve the way Requests uses urllib3, that can't fix the test case, because by default Requests returns the `Response` object with the socket still open and the body unread. Therefore, we rely on garbage collection to close the socket when clients don't read all the data. (Technically, since the test case is doing a `HEAD` request, the implementation of `requests.head` could be changed to close the socket. But the problem would still exist for the analogous test case calling `requests.get`.)

So here are the three viable approaches that occur to me:
1. Replace `Response.request` with a `weakref.proxy`, as @gsakkis suggested. This breaks the following test: `r = requests.get(httpbin('get)); assert r.request.sent` (because `r.request` has been garbage-collected already, so we get ""ReferenceError: weakly-referenced object no longer exists""). Anyway, this involves a smallish API breakage.
2. Remove `Response.request`. This has the advantage of not using weak references. This breaks the above test and also breaks `HTTPDigestAuth`, which uses the `response` hook and expects the `Response` object it operates on to have a `request` member. We could work around this by changing the hook API, but either way, it seems like a larger API break.
3. Enable `prefetch` and disable `keep_alive` by default, and then ensure that all the data is read and the socket is closed before we return to the client.

Random thought: it seems like keep-alive should be off by default. Outside the context of a session, we have no way to reuse the connection anyway.
",slingamn,Bluehorn
520,2012-05-06 10:09:20,"Here's my understanding of this issue so far:
1. @Bluehorn and @gsakkis are right: the root problem is the reference cycle between `requests.Request` and `requests.Response`.
2. Breaking this cycle in a crude way, e.g., removing the `request` member of the `Response` class, fixes @Bluehorn's test case.
3. Although it might be possible to improve the way Requests uses urllib3, that can't fix the test case, because by default Requests returns the `Response` object with the socket still open and the body unread. Therefore, we rely on garbage collection to close the socket when clients don't read all the data. (Technically, since the test case is doing a `HEAD` request, the implementation of `requests.head` could be changed to close the socket. But the problem would still exist for the analogous test case calling `requests.get`.)

So here are the three viable approaches that occur to me:
1. Replace `Response.request` with a `weakref.proxy`, as @gsakkis suggested. This breaks the following test: `r = requests.get(httpbin('get)); assert r.request.sent` (because `r.request` has been garbage-collected already, so we get ""ReferenceError: weakly-referenced object no longer exists""). Anyway, this involves a smallish API breakage.
2. Remove `Response.request`. This has the advantage of not using weak references. This breaks the above test and also breaks `HTTPDigestAuth`, which uses the `response` hook and expects the `Response` object it operates on to have a `request` member. We could work around this by changing the hook API, but either way, it seems like a larger API break.
3. Enable `prefetch` and disable `keep_alive` by default, and then ensure that all the data is read and the socket is closed before we return to the client.

Random thought: it seems like keep-alive should be off by default. Outside the context of a session, we have no way to reuse the connection anyway.
",slingamn,gsakkis
520,2012-05-06 19:08:15,"@shazow 

Can you clarify the reasons for defaulting `prefetch` to off instead of on?

So --- defaulting `keep_alive=True` in the context of a session makes sense. But in the context of the vanilla API, that is to say, a plain `requests.get` or `requests.post`, there doesn't seem to be any mechanism for reusing the keep-alive connections, because an ephemeral `Session` object is created and it is given an ephemeral `PoolManager` in `init_poolmanager()`.
",slingamn,shazow
520,2012-05-06 19:29:56,"Is an O(1) leak really a leak? ;)

I'm -0 on `prefetch=True` just because I feel the current settings cover the +80% use case, but I wouldn't shed any tears. Up to @kennethreitz, I'd say.
",shazow,kennethreitz
520,2012-05-06 19:51:44,"@piotr-dobrogost if we don't prefetch, I don't see a good solution that works on all platforms, including those like PyPy that lack eager garbage collection.

For comparison, here's something that always infuriated me about urllib2:



Seems like the 80%, batteries-included thing is to have an API that reads the whole page in a fire-and-forget way.
",slingamn,piotr-dobrogost
520,2012-05-06 20:45:16,"@slingamn keep-alive should be on, we're using it for 301s and the like.
",kennethreitz,slingamn
520,2012-05-06 21:50:09,"@piotr-dobrogost my take is that the client already ""knows"" what was in the request, i.e., has a local-variable reference to the URL they requested, or knows that they are using a proxy, etc., etc. Also, the async idiom is ""create a request, submit it, expect the response to be available as a member of the request object.""

I don't have super strong feelings about this, though; probably my real motivation is that removing `Response.request` seems to involve changing fewer lines of code ;-)
",slingamn,piotr-dobrogost
520,2012-05-06 22:16:55,"@laurentb yeah, @gsakkis suggested this. One problem is outlined in https://github.com/kennethreitz/requests/issues/520#issuecomment-5534515 --- in the common case, the `Request` object will in fact be garbage-collected and the weak reference will dangle.
",slingamn,laurentb
520,2012-05-06 22:16:55,"@laurentb yeah, @gsakkis suggested this. One problem is outlined in https://github.com/kennethreitz/requests/issues/520#issuecomment-5534515 --- in the common case, the `Request` object will in fact be garbage-collected and the weak reference will dangle.
",slingamn,gsakkis
520,2012-05-07 20:09:55,"@Bluehorn that's fair. I think the test case points towards other gotchas that aren't fixed by doing a HEAD. For example:
1. The client doing a POST and then reading only the status code
2. The client doing a GET and then reading the content on a 200 but not on a 404.
",slingamn,Bluehorn
520,2013-10-15 00:52:20,"@Lukasa thanks for the tip.  After more research I am pretty sure this is a problem with the server side code in etcd.

Thanks!
",rca,Lukasa
520,2014-02-24 21:21:47,"@mouadino The commit you're talking about was the rewrite that took place for V1.0.0. This was basically a total rewrite of the library, so the fix was not 'reverted', it was just no longer necessary.

I'm not entirely clear on what you're asking in the last part, but you've got a substantial misunderstanding of how Requests behaves. When we create a connection pool we don't open all of those connections at once: that would be ridiculous, and HTTP doesn't work that way anyway. We open one connection when you make your first request, and then reuse it as much as possible.

There _is_ an inefficiency when using `requests.get()` and friends, but that inefficiency is acceptable to get that clean API. People who don't want the inefficiency can simply use a `Session` object as described in the documentation.
",Lukasa,mouadino
520,2014-02-24 21:40:56,"@Lukasa Thanks for the quick response, and you miss understood my question, sorry for not being clear, what i meant is actually what is described in this bug report. 

If i have in my script a call like this `request.get(....)`, this call will create a new Session object which create a new connection pool, and from this later a connection will be created/borrowed to send my GET request, this connection will not be closed by the server (b/c of keep-alive, assuming that the server understand it), so we got now one connection in the pool that is still open and ready to be re-used right ?

But how about if i do another `requests.get(...)`This will also create another session which mean another connection pool and like before the result will be one connection from this new pool that is still open and ready to be re-use, but as you can see they will not be re-used because in the end the session object is not re-used.

So the result now is that we have 2 connection open to our server which will not be close until my program stop (clean up by the kernel !?) or the server clean them up (if it's support a keep alive timeout), so you can imagine what will happen if i have a lot of `requests.get(...)` calls (as an example).

First of all does this make sense ? If so what what is the guide line of using ephemeral session ? if this laters will not clean up after them self, unless i call `session.close(...)` (which will close opened connection from the pool), and if the close call is needed why wasn't it part of the api shortcuts ? And wouldn't it make more sense to disable connection pooling in a use case like this one ?
",mouadino,Lukasa
513,2012-03-27 14:01:08,"@morogoro plese do what it says(use --upgrade) since you may have requests module in the right place without the dependencies which may confuse pip.
",BYK,morogoro
513,2012-03-28 09:29:14,"@morogoro It should be
C:\Python27\Scripts>pip install --upgrade requests
",BYK,morogoro
513,2012-03-29 06:17:11,"@morogoro Then can you please try to _delete_ everything seems like ""requests"" in your site-packages folder and try again?
",BYK,morogoro
513,2012-04-02 11:20:51,"its working now thanks a lot @BYK @johtso 
",morogoro,BYK
513,2012-04-02 11:20:51,"its working now thanks a lot @BYK @johtso 
",morogoro,johtso
513,2012-04-02 22:21:41,"Hey @morogoro, I seem to be having the exact same problem you were having. I've tried deleting everything and re-installing but have had no luck. 
May I ask what exactly you did to get it working? 
Thanks. 
",rbui,morogoro
513,2012-04-02 22:23:40,"@rbui have you tried doing `pip uninstall requests` maybe?
",johtso,rbui
513,2012-04-02 22:28:30,"@johtso That was exactly what I was looking for, thanks for the quick reply!
",rbui,johtso
513,2012-04-16 05:31:18,"Hey @rbui sorry for not reply quickly, I was not around..........
",morogoro,rbui
511,2012-03-24 15:51:10,"Hi @kvonhorn, thanks for noticing that bug in urllib3. Could you make this change to urllib3 and pull request there?
",shazow,kvonhorn
511,2012-03-24 18:27:55,"Is there any chance of getting commit 8c5d06148bfa94ba87f369202b909d8c79897568 pulled, @kennethreitz?
",kvonhorn,kennethreitz
507,2012-03-23 21:40:37,"@johtso 
As I don't get the above description, could you please write a short example showing what you have in mind ?
",piotr-dobrogost,johtso
507,2012-03-23 22:26:20,"@piotr-dobrogost
So, currently ""Any dictionaries that you pass to a request method will be merged with the session-level values that are set"". If you wanted to send a request with none of the default params defined for the session, you would have to do something like this:

Setup:



The request:



A possible solution would be to allow something like this:



I was then contemplating the possibility of someone wanting to send a request with none of the default values for a parameter, and also with some new values, which would currently be achieved like so:



This might not be a use case worth worrying about, it certainly makes things trickier as the nice concise `=False` solution wouldn't do the job. Just throwing it out there.
",johtso,piotr-dobrogost
507,2012-03-27 07:46:42,"@kennethreitz 

How do you like this?
",piotr-dobrogost,kennethreitz
507,2012-07-31 20:39:29,"@kennethreitz 

Why did you close this?
",piotr-dobrogost,kennethreitz
505,2012-04-26 12:32:43,"@johtso are you working on this?
",barberj,johtso
505,2012-04-26 22:40:41,"@barberj I'm afraid I haven't had a chance to work on this, no.
",johtso,barberj
505,2012-04-29 22:00:49,"@berkerpeksag awesome! The second one (with `config`) is definitely best
",kennethreitz,berkerpeksag
505,2012-04-30 07:10:31,"@kennethreitz thanks!
",berkerpeksag,kennethreitz
503,2012-06-13 12:37:37,"@plaes Awesome, thanks for the quick response.
",kylerob,plaes
503,2012-06-14 14:53:50,"Cool, it's much better now. It is not perfectly copy'n pastable because of the >>> prefix in each line.
@kylerob I would remove that "">>>"" because for every pythonista it is clear that you can type into the terminal. A side effect would be that one can copy and paste.
",jups23,kylerob
503,2012-06-17 14:13:19,"@jups23 that's a good idea. @kennethreitz, what do you think about me removing all of the >>> prefixes from all code examples to allow easy copy/paste?
",kylerob,jups23
503,2012-06-18 20:45:16,"While we're here, the first code block [here](http://docs.python-requests.org/en/latest/user/quickstart/#response-status-codes) isn't rendering properly in my browser (Chrome 20.0.1132.34 beta and Safari 5.1.7 (7534.57.2) on OS X 10.7). Might be worth tackling as part of this issue as well. (Not sure why it's not displaying properly, because [GitHub shows it fine](https://github.com/kennethreitz/requests/blob/develop/docs/user/quickstart.rst#response-status-codes).)

I'm all for @kylerob removing the >>> prefixes where appropriate. Might even be worth adjusting the way some of the text flows to remove as many as possible, allowing for more copy-pasting.
",Lukasa,kylerob
502,2012-04-07 17:31:40,"@maxcountryman right now, original/modified values are all mixed together. Its unclear. `body` needs to be called `content`. 
",kennethreitz,maxcountryman
502,2012-04-07 23:19:38,"@maxcountryman 

I'm talking about passing the original value of `content_type` to hooks not the ability to modify it. Passing content and not passing content type feels wrong.
",piotr-dobrogost,maxcountryman
502,2012-04-07 23:22:24,"@piotr-dobrogost even so I think it's beyond the scope of this particular bug, no? Easy enough to add but, it just seems like this patch is starting to grow past the original issue... Maybe we should break the two objectives off into separate commits?
",maxcountryman,piotr-dobrogost
500,2012-05-07 06:48:37,"@piotr-dobrogost doesn't reproduce for me in Linux. But yeah, it wouldn't surprise me if it were a cross-platform issue with gevent's monkeypatching. See if it goes away under the abovementioned patch?
",slingamn,piotr-dobrogost
499,2013-04-25 16:20:19,"@jmoiron @kennethreitz I'm sorry, but how can it be currently done?
Building API client for another service that uses signature-based authentication and when I'm trying to add parameters by editing r.params, I receive:
AttributeError: 'PreparedRequest' object has no attribute 'params'
",pitsevich,kennethreitz
499,2013-04-25 16:20:19,"@jmoiron @kennethreitz I'm sorry, but how can it be currently done?
Building API client for another service that uses signature-based authentication and when I'm trying to add parameters by editing r.params, I receive:
AttributeError: 'PreparedRequest' object has no attribute 'params'
",pitsevich,jmoiron
499,2013-04-25 16:45:48,"@pitsevich the API has changed in the most recent releases of requests. As of 1.x you have no access to the params attribute. If you want to modify the params you'll have to parse the url and unquote the param string, add your params, re-quote it and then unparse the url.
",sigmavirus24,pitsevich
499,2013-04-25 17:28:34,"@sigmavirus24 thanks, this worked!
",pitsevich,sigmavirus24
499,2013-04-25 17:35:19,"@pitsevich glad to help.
",sigmavirus24,pitsevich
498,2012-03-19 14:10:32,"@kennethreitz How do you want to handle merging in the latest urllib3? Once that's dealt with, I can make the necessary changes to make `requests` compatible with App Engine on Python 2.7.
",singingwolfboy,kennethreitz
498,2012-03-31 03:00:36,"@shazow I'm a bit of a noob but could try my hand at a merge.... this would be incredibly helpful to those of us in the python environment on app engine!
",rdixit,shazow
498,2012-03-31 03:13:48,"Hey, I just tried this again with the current master branch. Looks like the same AttributeError referenced in the above stackoverflow post, which @shazow mentioned probably has to do with using the local filesystem for certain things in Requests. The local filesystem is not available on AppEngine- I tend to use StringIO in a lot cases where I'd relied on the file system in the past, but not sure how how helpful that is.
",rdixit,shazow
498,2012-03-31 03:16:17,"@rdixit can you try it again with the latest from the `develop` branch?
",kennethreitz,rdixit
498,2012-03-31 07:23:09,"@shazow, I choose you!

![](http://4.bp.blogspot.com/_J0fSQHG901k/Sw40rKFnN8I/AAAAAAAAAT4/cLXi4xhQTNQ/s1600/ash+trowing+pokeball.png)
",kennethreitz,shazow
498,2012-03-31 21:39:56,"@rdixit care to try again?
",kennethreitz,rdixit
498,2012-03-31 21:41:01,"Would be nice if we had some sort of (semi?) automated testing for AppEngine. You should hook us up somehow, @rdixit. ;)
",shazow,rdixit
498,2012-03-31 21:41:52,"@shazow if only we knew a Google Employee...
",kennethreitz,shazow
498,2012-04-27 08:13:05,"A slight possibility but the problem @rdixit having might be due to different Python versions. As many know GAE comes with two Python runtimes: 2.5 and 2.7 where @rdixit appears to be using 2.6
",BYK,rdixit
493,2012-03-18 23:16:20,"Btw, @kennethreitz: Requests still does not support AppEngine, since you're depending on various filesystem things for configuration magic that AppEngine doesn't have. Not sure if you want to reopen this bug.
",shazow,kennethreitz
493,2012-03-18 23:25:20,"@shazow those should all be supplementary. I guess I should fire up an app engine app and find out :)
",kennethreitz,shazow
493,2012-03-18 23:26:03,"@kennethreitz Here's a failure for example: http://stackoverflow.com/questions/9762685/using-the-requests-python-library-in-google-app-engine/9763217
",shazow,kennethreitz
493,2013-01-23 00:41:25,"yeah, it would still be appreciated! @singingwolfboy, is requests using the 2.5 environment functional on App Engine as is..?
",rdixit,singingwolfboy
493,2013-01-23 00:43:14,"@rdixit It was working when last I touched my fork. I haven't touched it in a _long_ time, though.
",singingwolfboy,rdixit
482,2012-03-15 17:42:58,"As @kennethreitz knows, I'm preparing to use requests in a different project, and was testing out the GitHub API. The following is something I did from the python command line interpreter:


",sigmavirus24,kennethreitz
482,2012-05-17 18:40:20,"@kennethreitz: Whereabouts in the docs did you want these? The only place that currently exists that looks suitable is docs/quickstart.rst, but that document is getting increasingly large. Should we consider busting the verbs out into a new file? Or are you happy to extend the quickstart? Either way, I'm happy to write these and add them into #619.
",Lukasa,kennethreitz
482,2013-11-20 08:54:52,"@RAINCEN Really? _Really?_
",Lukasa,RAINCEN
482,2013-11-20 14:29:47,"@Lukasa the emails I received from @RAINCEN had the same message that @Fighter42 posted about 20 times (of which I deleted all but one). I have to wonder if these are just spam accounts or if perhaps this is related to [recent attempts to brute force passwords on accounts](https://github.com/blog/1698-weak-passwords-brute-forced). Regardless, it's probably advisable to just ignore everyone who makes similar comments on this issue. It's been closed for so long.
",sigmavirus24,Lukasa
482,2013-11-20 14:29:47,"@Lukasa the emails I received from @RAINCEN had the same message that @Fighter42 posted about 20 times (of which I deleted all but one). I have to wonder if these are just spam accounts or if perhaps this is related to [recent attempts to brute force passwords on accounts](https://github.com/blog/1698-weak-passwords-brute-forced). Regardless, it's probably advisable to just ignore everyone who makes similar comments on this issue. It's been closed for so long.
",sigmavirus24,RAINCEN
480,2012-03-31 15:21:37,"@kennethreitz If I'm understanding you, I don't think this issue is solved from my perspective. Let me give you my use case.

When I fetch an HTML resource with requests, I want to parse the HTML with the encoding as it was intended. That means:
1. Looking in the HTTP header.
2. Looking in the meta tag.
3. Making a dumb guess using something like chardet.

_Setting_ the encoding on a request doesn't help me there, because I want to get the natural encoding response. And if it failed to get it from the headers, I can continue down the line and look at the meta tag.

Right now in my code I'm working around this by just checking to see if requests responded with ISO-8859-1 and ignoring it because I don't have any guarantees it was actually there. The better solution for me would be for this method to return None.
",umbrae,kennethreitz
478,2012-03-10 14:39:22,"@Anorov just saw your question about keep-alive's. Currently, keep-alive's are forcibly disabled for SSL over HTTP proxy, however SOCKS4/5 and HTTP over HTTP proxy keep-alive's will work without problem.
",foxx,Anorov
478,2012-03-13 03:02:40,"@foxx
Thanks for looking into this issue.
If you have a chance, would you be able to provide an example of authenticating to the proxy in case of https over http?

Cheers
",vly,foxx
478,2012-03-13 12:47:28,"Hi @vly,

Not a problem, I'll take a look at this today and update the test file to reflect this. 
",foxx,vly
478,2012-03-14 16:59:54,"@vly I can confirm that currently the code does not currently have proxy authentication support (however afaik, proxy authentication is not currently in the original code either).

It does still however work for remote authentication (i.e. using auth= to authenticate to the site).

@kennethreitz can you confirm if you would be happy to merge the code in its current state, with proxy authentication being done at a later date?
",foxx,vly
478,2012-03-15 04:50:13,"@foxx 
Thanks for the confirmation.
Was wondering if I just didn't know the correct syntax. http via http proxy auth works fine either passed via user:pass@ or Authorization basic header item.
",vly,foxx
478,2012-03-15 13:28:08,"@vly Excellent stuff - it would be great to have proxy authentication support, however this patch was contributed as open source from work done for a clients specific requirement - which sadly doesn't require proxy authentication. I'm hoping @kennethreitz will accept the patch without, and either myself or someone else will add proxy authentication support at a later date.
",foxx,vly
478,2012-03-15 15:17:31,"@foxx Hey there! @wolever has been helping me code review and manage with this particular feature request. Could you take a look at this @wolever?

Superquick glance at it looks alright. Our long-term redesign ambitions will make this code simpler but that's a way's out so let's not wait for that. Cleanup into urllib3's codebase and some unit tests in urllib3's suite would be great. :)
",shazow,foxx
478,2012-03-15 15:53:55,"@shazow Sounds good man. @wolever can you make sure you are happy with this code and approach, if so I will make the necessary fork/mods. ---edited---
",foxx,shazow
478,2012-03-19 16:42:00,"@wolever Thank you for the feedback on this! It all looks fine, with the slight change that connections to HTTPS over HTTP proxy should default to the 'CONNECT' method (without needing to add +connect).

I've booked out some hours for Friday to get the modifications done, and I can also make the necessary Python Requests modifications afterwards too. However - if anyone else wants to have a shot at this then please do feel free!
",foxx,wolever
478,2012-03-21 11:51:35,"@Anorov Many thanks for the updates! On Friday (if no one else has done the patch by then) - I will get a new urllib3 fork created with the necessary changes - assuming they are accepted I will then update my branch on this pull request too.
",foxx,Anorov
478,2013-09-23 11:55:00,"@YS- This issue was originally opened more than two years ago, and it was closed more than a year ago. Please try not to resurrect old issues. =)

As to your question, we have HTTPS proxy support in the `2.0` branch of Requests, which will be pushed out as our next release. This took a while to develop, which is why it took so long. We currently do not have SOCKS proxy support, we're waiting for that code to be included in urllib3. There are plans afoot for this, but no definitive schedule at this time.
",Lukasa,YS-
468,2012-03-14 17:15:39,"@maraujop - A critical point of OAuthLib is to provide a library of utilities and functions audited by security people at Google, Canonical, et al. There is not a bit of requests in it, which sucks because urllib2 sucks. However, because OAuthLib is currently dependency free, that means projects that can't support requests can use it trivially. This means that not will OAuthLib be able to help replace python-oauth2 in existing systems, it can also target systems where people have cooked up their own hacky solutions because of the impossibility of dependency management.

As for OAuth2, we would love to have used that as our target. Unfortunately, as OAuth2 is not a finalized specification, that's not realistic. Even if we did it anyway, the security teams can't/won't audit it.
",pydanny,maraujop
468,2012-03-17 22:24:28,"@pydanny @kennethreitz I'm very aware of what oauthlib is. I totally understand its advantages, and let me insist that I've never wanted to state it's a bad project or idea. It also will ease releasing providers for different Python frameworks, something as I said is missing and that is a hard  task.

When I started requests-oauth, I wanted to add OAuth consumer ability to requests, which I guess we agree it's the future of http libraries in Python. 

I guess what I don't understand is why oauthlib started from scratch, when there were very good projects with lots of corner cases covered. The code base of those projects was a good starting point. Projects like mine or python-oauth2 (mine was based on it in the beginning), could have been refactored into a independent library. I'm guessing with the entrance of these great new devs, this is now a reality, as they are capturing their know-how in oauthlib, great.

> All of the excellent work you've done for bringing oauth into requests could be immensely useful to the development of oauthlib, which is the best thing going forward for the Python community as a whole. Not just requests.

Thanks. I'm working hard on trying to maintain my hook up to date as you know, as this is the most used choice for requests and several people depend on my development to continue working on their projects.

> The reason your pull requests as of late have been sitting there is because they are great changes that need to happen and can't be merged in immediately because of the fundamental changes they make. Normally it'd be easier, but there's underway a rearchitecture of requests / flask / werkzeug / cache / httplib / dns security libraries. This is all pretty much undocumented at this point, but I hope to fix that this week. Lots of great progress happened at PyCon.

I'm looking forward to seeing what this is all about. It felt that 27 days for getting an answer on an issue that was blocking the development of an important plugin of the application, was too long, considering 7 days before, I got an answer on an other issue #468. I know myself how busy we can all be with our lives and open source all at the same time. In your case maintaining big projects such as requests, it's even more demanding.

What I'm saying is that at the moment, requests-oauth is what people are using, until OAuth gets merged into core, there needs to be a way I can keep development alive. If I stop maintaining today my hook because something is superseding it, there will be a gap of time in which people will have no choice. I depend now on you to be able to maintain it.

Cheers,
Miguel
",maraujop,pydanny
468,2012-03-17 22:24:28,"@pydanny @kennethreitz I'm very aware of what oauthlib is. I totally understand its advantages, and let me insist that I've never wanted to state it's a bad project or idea. It also will ease releasing providers for different Python frameworks, something as I said is missing and that is a hard  task.

When I started requests-oauth, I wanted to add OAuth consumer ability to requests, which I guess we agree it's the future of http libraries in Python. 

I guess what I don't understand is why oauthlib started from scratch, when there were very good projects with lots of corner cases covered. The code base of those projects was a good starting point. Projects like mine or python-oauth2 (mine was based on it in the beginning), could have been refactored into a independent library. I'm guessing with the entrance of these great new devs, this is now a reality, as they are capturing their know-how in oauthlib, great.

> All of the excellent work you've done for bringing oauth into requests could be immensely useful to the development of oauthlib, which is the best thing going forward for the Python community as a whole. Not just requests.

Thanks. I'm working hard on trying to maintain my hook up to date as you know, as this is the most used choice for requests and several people depend on my development to continue working on their projects.

> The reason your pull requests as of late have been sitting there is because they are great changes that need to happen and can't be merged in immediately because of the fundamental changes they make. Normally it'd be easier, but there's underway a rearchitecture of requests / flask / werkzeug / cache / httplib / dns security libraries. This is all pretty much undocumented at this point, but I hope to fix that this week. Lots of great progress happened at PyCon.

I'm looking forward to seeing what this is all about. It felt that 27 days for getting an answer on an issue that was blocking the development of an important plugin of the application, was too long, considering 7 days before, I got an answer on an other issue #468. I know myself how busy we can all be with our lives and open source all at the same time. In your case maintaining big projects such as requests, it's even more demanding.

What I'm saying is that at the moment, requests-oauth is what people are using, until OAuth gets merged into core, there needs to be a way I can keep development alive. If I stop maintaining today my hook because something is superseding it, there will be a gap of time in which people will have no choice. I depend now on you to be able to maintain it.

Cheers,
Miguel
",maraujop,kennethreitz
467,2012-03-02 06:34:37,"@kennethreitz Yes guy, it has been fixed, and you are really efficient :)
",reorx,kennethreitz
465,2012-07-26 12:32:41,"Yes, but this solution is still cleaner (and IMO easier) than doing the decompression at a second place because this is already built-in in requests.

But I agree with you in general, a `r.file` (or something like this) has much more use cases than `r.raw`. So I would like to see this included in requests, too. @kennethreitz 
",schlamar,kennethreitz
465,2013-02-07 19:05:00,"@kernc: That's a bizarre thing to be doing. `response.content` is already a bytestring, so what you're doing here is decoding the content with whatever the hell codec Python chooses, then re-encoding it as utf-8.

This is _not_ a bug, and it is quite definitely not the bug you suggested. If you really need a file-like object, I recommend StringIO and BytesIO.
",Lukasa,kernc
465,2013-02-07 19:16:56,"@Lukasa is correct. `content` should always be a bytestring (in Python 3 it's an explicit bytestring; in Python 2 str == bytes). The only item that is not a bytestring is `text`.
",sigmavirus24,Lukasa
465,2013-03-19 06:38:15,"@kennethreitz any news on this? This is a pretty serious design bug and it's best to sort it out early. The more code gets written to work around it, the more costly it becomes for everyone.
",scoder,kennethreitz
465,2013-03-19 08:52:20,"@Lukasa
I can't really see how filing the bug against urllib3 would fix the API of requests, at least not all by itself.

And I agree that your ""use case"" is contrieved. As I said, if the client cannot positively control the compression on the server side (and it disable it, but not reliably enable it), so relying on it to be able to save a compressed file to disk is, well, not so interesting.
",scoder,Lukasa
465,2013-03-19 08:55:45,"@schlamar
I agree that it can be read as such. I assure you that I'm fine with anything that solves this problem. If opening a new ticket is required in order to get there, so be it.
",scoder,schlamar
465,2013-03-19 10:24:54,"> Although, does that actually work? I.e. are the decompressors stateful and incremental? 

Oh, doesn't seem so. I didn't read the docstring.

However, here is my proposal:
1. Patch urllib3 so that `HTTPResponse.read` works with `amt` and `decode_content` concurrently.
2. Make HTTPResponse._decode_content a public member (so you can do `response.raw.decode_content = True` instead of patching the `read` method).
3. Drop decompression in requests completely by using `decode_content=True` in `iter_content`

@Lukasa I think this won't violate the feature freeze, right?
",schlamar,Lukasa
465,2013-03-19 12:19:24,"@schlamar: In principle, sure. As long as the API remains unchanged, internal changes _should_ be ok, and I'd be +1 on this one. However, bear in mind that I'm not the BDFL, =)
",Lukasa,schlamar
458,2012-03-29 07:35:27,"@kennethreitz - what if I explicitly want it off?
",yuvadm,kennethreitz
458,2012-03-31 05:31:01,"@yuvadm why?
",kennethreitz,yuvadm
458,2012-03-31 10:44:55,"@kennethreitz for example, if I'm on a server with limited resources and I know for sure that I'm making a single connection that should immediately be closed.
",yuvadm,kennethreitz
456,2012-03-14 23:04:07,"@chadnickbok this looks great! Moving discussion to that thread.
",kennethreitz,chadnickbok
454,2012-02-24 18:48:22,"@piotr-dobrogost, i bet in one you don't have the 'chardet' dependency available.
",kennethreitz,piotr-dobrogost
445,2012-04-07 22:56:08,"@maraujop

> As you can see in this line:

If you refer to code please remember to link to specific commit not to moving target (head) :)
",piotr-dobrogost,maraujop
441,2012-02-21 17:59:37,"Like @umbrae said, this is an HTTP library, not an HTML library.
",kennethreitz,umbrae
440,2012-02-21 21:35:11,"The reason it's necessary/desired is that [RFC 2965](http://www.ietf.org/rfc/rfc2965.txt) requires that a cookie value either be a token, [which may not contain an '=' character](http://www.ietf.org/rfc/rfc2068.txt), or a quoted string. If the server isn't quoting the string, the server is non-compliant. As @kennethreitz says, Oreos already allows non-RFC-compliant characters in cookie keys because they're commonly sent out.
",Lukasa,kennethreitz
436,2012-02-28 15:51:39,"@RonnyPfannschmidt Thanks for the insight Ronny.
",umbrae,RonnyPfannschmidt
436,2012-03-08 01:29:17,"@RonnyPfannschmidt care to add it? :)
",kennethreitz,RonnyPfannschmidt
429,2012-03-08 03:09:14,"Also @mgiuca, thank you very much for raising the issue about the + and %20 stuff, our patch was failing on several tests due to this problem - so you've saved us a lot of time :) Thank you again.
",foxx,mgiuca
429,2012-03-08 04:10:43,"@mgiuca good to know :)
",kennethreitz,mgiuca
429,2012-03-31 06:32:51,"@mgiuca we could really use your help with a new collaboration with @mitsuhiko to merge requests and werkzeug. Would you like to help?
",kennethreitz,mgiuca
429,2012-03-31 07:08:58,"@mgiuca It'll likely be over the next month or two :)
",kennethreitz,mgiuca
429,2012-04-01 16:07:32,"@piotr-dobrogost I'm planning on doing a blog post on it today
",kennethreitz,piotr-dobrogost
427,2012-02-19 13:29:22,"@kennethreitz Hm, I thought you would consider adding Python 2.5 support again.. at least that's what I understood when I asked you  about it regarding requests adoption in pip. Mind elaborating?
",jezdez,kennethreitz
427,2012-02-19 13:46:17,"@kennethreitz Not sure what more is needed than a pip developer saying that requests adoption requires 2.5. A public statement?
",jezdez,kennethreitz
427,2012-08-24 10:27:30,"@Lukasa Good idea +1
",jezdez,Lukasa
426,2012-02-15 00:15:15,"@kennethreitz Thanks for merging. I forgot to add my name to AUTHORS. Could you please do that (Matt Giuca)?
@foxx I don't think we are talking about the same issue. I'll respond on the talk page for Issue #429.
",mgiuca,foxx
426,2012-02-15 00:17:49,"@mgiuca ah - i see what you mean, looks like it is a completely different issue, thanks!
",foxx,mgiuca
426,2016-01-30 15:58:27,"@dorafmon Why would you want to avoid requests encoding the percentage sign? That URL is not valid: there's a trailing percentage sign in it that _must_ be encoded.

However, if you really must have that, you should use the [prepared request flow](http://docs.python-requests.org/en/latest/user/advanced/#prepared-requests) and edit `prepped.url` to look the way you'd like it to.
",Lukasa,dorafmon
426,2016-01-30 18:35:45,"@Lukasa , because some website do have a percentage sign in their get parameters so I kind of need to avoid the percentage sign to be encoded.
",dorafmon,Lukasa
426,2016-01-30 19:16:01,"@dorafmon Unfortunately those websites are broken. There's no requirement for us to support that in the general API, so you'll need to use the prepared request flow as linked above.
",Lukasa,dorafmon
424,2012-05-04 01:54:22,"This is working for me in trunk:



@foxx check your use case, see if this is closeable?
",slingamn,foxx
417,2012-02-09 19:31:22,"> We shouldn't be relying on urllib3 for decompression right now, actually.
> The fact that we are is a bug.

@kennethreitz I would agree. FWIW, I've come across some issues using requests (urllib3) with some gzipped responses.
",JeffPaine,kennethreitz
417,2012-02-10 00:44:31,"@kennethreitz Any reasonable guesses as to when it might be squashed? Thanks very much.
",JeffPaine,kennethreitz
404,2012-01-31 15:27:51,"@yuvadm Yep, this is really a deal breaker for me. Its a shame 'cause requests fill all my needs but this redirection problem break the tool...

@piotr-dobrogost I hope this will be fixed soon, #269 was open 3 months ago...
",namlook,yuvadm
404,2012-01-31 15:27:51,"@yuvadm Yep, this is really a deal breaker for me. Its a shame 'cause requests fill all my needs but this redirection problem break the tool...

@piotr-dobrogost I hope this will be fixed soon, #269 was open 3 months ago...
",namlook,piotr-dobrogost
404,2012-02-03 18:27:49,"@piotr-dobrogost I don't see how this would be related to that at all.
",kennethreitz,piotr-dobrogost
394,2014-05-14 14:30:34,"@petri Your assertion that this is not out of scope is incorrect, given that the scope of the Requests project is defined entirely by Kenneth. =)

Specifically, we view it as being out of scope for the core requests project. However, you can still do it, and it's not even very hard. [This discussion](https://github.com/kennethreitz/requests/issues/2008#issuecomment-40793099) covers how to do it.
",Lukasa,petri
392,2012-03-08 16:30:36,"@yole there are always edge cases.
",kennethreitz,yole
388,2012-01-26 08:49:13,"I think the idea of having different requirements files is nice and worth merging. Btw, why is nose a requirement?

@seanjensengrey You say all tests including new async ones pass. I'm very surprised. See issue #383 Any comments on this?
",piotr-dobrogost,seanjensengrey
388,2012-01-26 18:59:12,"@kennethreitz What if I added in using the 2to3 into the Makefile so we could generate a Python 3 compatible install? The vast majority of users are still on the Python 2.x line and will be for the foreseeable future.

Requests is great and I would like to see it replace all the other cruft out there by being available everywhere. It would be nice if I could run it on cPython 2.5 and Jython, it limits my use of it on the systems I use as I can't migrate programs across environments.
",seanjensengrey,kennethreitz
386,2012-01-25 15:32:58,"@shazow +1
",kennethreitz,shazow
386,2012-01-25 17:34:06,"@piotr-dobrogost Not sure what you mean by multiple backends.

Originally we had a wsgiref simple server, then we moved to an evenlet wsgi app server, now we're moving to a Tornado wsgi server and a socket-handler based thread server.
",shazow,piotr-dobrogost
383,2012-01-26 18:20:24,"@piotr-dobrogost I am seeing all the async tests pass under python 2.6, I am not able test under 2.5 right now as my gevent isn't installed properly.



Is that expected behavior?
",seanjensengrey,piotr-dobrogost
378,2012-01-22 17:32:58,"@johtso Strange thing. I got notification about your post which begins with _Ah yes, of course, forgot about 0. What about False? if (v is None) or (v is False): v =_ but I don't see your post from notification...
",piotr-dobrogost,johtso
378,2012-02-07 14:08:59,"I agree with @piotr-dobrogost, keys with `None` values should be removed like the other maps.
",francescomari,piotr-dobrogost
378,2012-02-14 21:41:07,"@foxx If you want to send empty string as a value of some param you should set its value to empty string. Why should requests send `key=''` when you pass `key=None` and not skip this param altogether? 
btw, using `==` when comparing with `None` is bad style.
",piotr-dobrogost,foxx
377,2012-04-09 03:43:11,"Fixed thanks to @newmaniese 
",kennethreitz,newmaniese
369,2012-02-11 19:30:01,"@stringfellow that's a bug. It's double-encoding. This happened when I added Python 3 support. I'm working on it.

@gfxmonk as I said, someone's working on a patch to make the escaping optional.
",kennethreitz,stringfellow
369,2012-02-11 23:55:25,"@kennethreitz but you seem to be indicating that it is correct behaviour for most (or even some) cases. I can't see how it could ever be right, so it bothers me to see it even implemented (regardless of whether you can turn it off).
",timbertson,kennethreitz
369,2012-02-12 10:33:48,"@kennethreitz What do you mean by optimised? Quicker? Or restricted to non-ASCII characters?

I admit I hadn't heard of IRIs before now, but from a quick skim of the spec converting IRIs sounds like it must not escape an existing percent character, which the current code is doing.
",timbertson,kennethreitz
362,2012-02-01 17:12:23,"@kennethreitz For which? How do you implement proxies? Are you using ProxyManager?
",shazow,kennethreitz
362,2012-02-01 17:13:31,"@shazow https://github.com/kennethreitz/requests/blob/develop/requests/models.py#L447
",kennethreitz,shazow
362,2012-02-01 17:19:41,"@kennethreitz Thanks! Hmm. The underlying HTTPSConnectionPool should be making a CONNECT.

Are you sure you're actually setting it to HTTPS? Because if you look at the example...



Both of the proxies are http and port 80, so urllib3 has no way of knowing it's HTTPS unless you do it explicitly.

Anyways, if it is urllib3's fault, if someone writes a failing test in urllib3, I'll look into it this weekend. :)

Something between...
https://github.com/shazow/urllib3/blob/master/test/with_dummyserver/test_https.py and
https://github.com/shazow/urllib3/blob/master/test/with_dummyserver/test_socketlevel.py
",shazow,kennethreitz
362,2012-02-11 21:00:18,"@senko Hmm interesting. And this only works in 2.7 and silently passes in earlier versions?

I'd love to see the actual changes, would be great if you sent a pull request to urllib3. HTTPS over HTTP is not something I considered before, so it's something I'd like to look into with your help. :-)

Bonus points if you include an HTTPS over HTTP unit test... (Might be challenging)
",shazow,senko
361,2012-01-19 20:49:54,"@kennethreitz Note that this would be a trivial user addition if multiple hooks were supported, which would be a nice way to add this for people who care without adding two syscalls (albeit an extremely cheap call on modern Unix variants) per request for everyone.
",acdha,kennethreitz
361,2012-01-19 20:51:00,"@acdha multiple hooks would be huge. I want. Badly.
",kennethreitz,acdha
361,2012-01-23 07:59:16,"@acdha multiple hooks are in!
",kennethreitz,acdha
359,2012-02-28 14:15:36,"@RonnyPfannschmidt I'm looking into debugging issue #436 with generate_chunked and noticed you merged this. I was wondering if you had any insight on what the trouble is there?
",umbrae,RonnyPfannschmidt
358,2012-08-09 14:22:47,"What's the state of this @kennethreitz ? Because it looks done to me...
",Lukasa,kennethreitz
337,2012-08-25 15:09:11,"@samuelclay make sure you're using the latest release. I can't reproduce any of these :)
",kennethreitz,samuelclay
335,2012-02-23 18:59:28,"Hmmm, just encountered a CookieError =/, Is there any way to just ignore all cookie errors @kennethreitz? 

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Library/Python/2.6/site-packages/requests/api.py"", line 79, in get
    :param url: URL for the new :class:`Request` object.
  File ""/Library/Python/2.6/site-packages/requests/api.py"", line 66, in request
    """"""Sends a HEAD request. Returns :class:`Response` object.
  File ""/Library/Python/2.6/site-packages/requests/sessions.py"", line 191, in request

  File ""/Library/Python/2.6/site-packages/requests/models.py"", line 462, in send
    r = self.proxy_auth(self)
  File ""/Library/Python/2.6/site-packages/requests/models.py"", line 242, in _build_response
    except KeyError:
  File ""/Library/Python/2.6/site-packages/requests/models.py"", line 462, in send
    r = self.proxy_auth(self)
  File ""/Library/Python/2.6/site-packages/requests/models.py"", line 186, in _build_response
    response.cookies = cookies
  File ""/Library/Python/2.6/site-packages/requests/models.py"", line 169, in build
    response.status_code = getattr(resp, 'status', None)
  File ""/Library/Python/2.6/site-packages/requests/packages/oreos/core.py"", line 19, in dict_from_string
    c.load(s)
  File ""/Library/Python/2.6/site-packages/requests/packages/oreos/monkeys.py"", line 641, in load
    map(Cookie.__setitem__, d.keys(), d.values())
  File ""/Library/Python/2.6/site-packages/requests/packages/oreos/monkeys.py"", line 674, in __ParseString
    M[ K ] = _unquote(V)
  File ""/Library/Python/2.6/site-packages/requests/packages/oreos/monkeys.py"", line 594, in __set
    def __set(self, key, real_value, coded_value):
  File ""/Library/Python/2.6/site-packages/requests/packages/oreos/monkeys.py"", line 468, in set
    if key.lower() in self._reserved:
requests.packages.oreos.monkeys.CookieError: Illegal key value: 10:54:14&CookieExpireDate
",dalanmiller,kennethreitz
335,2012-02-23 20:21:55,"@dalanmiller 
How can we reproduce this?
",piotr-dobrogost,dalanmiller
335,2012-02-23 20:28:14,"@piotr-dobrogost Here's at least one sample url: http://mylikes.com/l/1tsn6

Like: 



I can scrape up a couple more in a bit.
",umbrae,piotr-dobrogost
335,2012-02-23 21:02:23,"@piotr-dobrogost Try sending a get request with requests at http://nordstrom.com
",dalanmiller,piotr-dobrogost
335,2012-02-23 21:11:03,"@dalanmiller What version of requests are you using? When I hit http://nordstrom.com/, I find the illegal cookie key to be ""13:05:29&CookieExpireDate"", which is illegal because it contains colons. This was fixed in commit f72c13f .
",Lukasa,dalanmiller
335,2012-02-23 21:18:54,"@Lukasa According to pip I'm using Requests 0.10.4



As well:



Results in 



I don't even need cookies for what I'm trying to do, is there anyway I can just set something to ignore them and errors?
",dalanmiller,Lukasa
335,2012-02-23 21:24:33,"The two failures given by @umbrae fail because the first one contains parentheses and the second one contains a question mark. I'm reluctant to just keep patching up failures when they arise, I'd be more inclined to rewrite the code to correctly handle all of the cookies. I'm looking to get around to it sometime in the next few days, but if someone else wants to tackle it they're welcome.

@dalanmiller Odd. If I launch a new virtualenv, pip install requests and then perform a get on http://nordstrom.com/ , I don't get a CookieError, and pip claims I'm running 0.10.4 as well.
",Lukasa,umbrae
335,2012-02-23 21:24:33,"The two failures given by @umbrae fail because the first one contains parentheses and the second one contains a question mark. I'm reluctant to just keep patching up failures when they arise, I'd be more inclined to rewrite the code to correctly handle all of the cookies. I'm looking to get around to it sometime in the next few days, but if someone else wants to tackle it they're welcome.

@dalanmiller Odd. If I launch a new virtualenv, pip install requests and then perform a get on http://nordstrom.com/ , I don't get a CookieError, and pip claims I'm running 0.10.4 as well.
",Lukasa,dalanmiller
335,2012-02-23 21:29:47,"@Lukasa Tried using a new virtualenv, then installing requests via pip and everything worked perfectly fine like you did. 

Going to try uninstalling / reinstalling requests.

Thank you for all your help! 

p.s.

A `pip uninstall requests` and then a `pip install requests --upgrade` did the trick! Now working! Thank you @Lukasa, @KennethReitz, @piotr-dobrogost. 
",dalanmiller,piotr-dobrogost
335,2012-02-23 21:29:47,"@Lukasa Tried using a new virtualenv, then installing requests via pip and everything worked perfectly fine like you did. 

Going to try uninstalling / reinstalling requests.

Thank you for all your help! 

p.s.

A `pip uninstall requests` and then a `pip install requests --upgrade` did the trick! Now working! Thank you @Lukasa, @KennethReitz, @piotr-dobrogost. 
",dalanmiller,Lukasa
333,2012-01-05 18:29:58,"@gazpachoking Yeah, a bit torn on the name. `raise_status` or `always_raise` may be better.
",bryanhelmig,gazpachoking
333,2012-01-05 18:44:04,"@bryanhelmig Heh, both of those name have actually been in my code at some point.
@kennethreitz Yeah, now that you mention it as a complement to `safe_mode` I like it a bit better. I just feel like the name doesn't imply that it is going to raise errors. Feels more like it would do something like prefetch mode to me.
",gazpachoking,kennethreitz
333,2012-01-05 18:44:04,"@bryanhelmig Heh, both of those name have actually been in my code at some point.
@kennethreitz Yeah, now that you mention it as a complement to `safe_mode` I like it a bit better. I just feel like the name doesn't imply that it is going to raise errors. Feels more like it would do something like prefetch mode to me.
",gazpachoking,bryanhelmig
333,2012-01-05 19:30:41,"@kennethreitz Let me know if this is enough accept the pull, I'm happy to do any additional legwork.
",bryanhelmig,kennethreitz
333,2012-01-05 19:31:47,"@bryanhelmig I'd be happy to pull now, but I'll give you a sparkly piece of cake if you add it to the docs :)
",kennethreitz,bryanhelmig
333,2012-01-05 19:34:20,"@kennethreitz I do love cake. :-) Where in the docs is this appropriate to mention? I assume the makefile will grab the appropriate docstring and add to http://docs.python-requests.org/en/latest/api/#configurations automatically?
",bryanhelmig,kennethreitz
333,2012-01-05 19:35:16,"@kennethreitz Would http://docs.python-requests.org/en/latest/user/quickstart/#errors-and-exceptions be appropriate?
",bryanhelmig,kennethreitz
333,2012-01-05 19:37:30,"@bryanhelmig even better!
",kennethreitz,bryanhelmig
333,2012-01-05 20:14:07,"@kennethreitz Added something rather minor.

On another note, it might be a good idea to flesh out the configuration section of the advanced page at some point in the future, as it just points to the API docs right now.
",bryanhelmig,kennethreitz
326,2012-01-01 10:48:31,"@cactus yes, and they are all equally incompatible ;)
",kennethreitz,cactus
326,2012-01-26 17:55:39,"@davidpshaw a simple `pip install requests` takes care of everything.

If you're just having the certifi is missing error, `pip install certifi`.
",kennethreitz,davidpshaw
326,2017-02-25 16:25:58,"@kennethreitz Thanks, I had same issue and is resolved by that command
`pip install requests` just saved my life.",swhshamsi,kennethreitz
310,2011-12-19 17:10:08,"We can do as tastypie does, requests doesn't need to require all the
libraries for each content-type. If a particular content type is used
without the correct library it will raise an exception show in that.
In that way requests will not grow in dependencies, we can support the
basics json, yaml and xml, and if someone want more, a system as
@megaman821 says can help.

On Mon, Dec 19, 2011 at 8:48 AM, megaman821
reply@reply.github.com
wrote:

> I think a system where you could register deserialization handlers to content types would be ideal. By default Requests would just include a plain-text handler for everything and the end-user could register their own.
> 
> ---
> 
> Reply to this email directly or view it on GitHub:
> https://github.com/kennethreitz/requests/issues/310#issuecomment-3203260

## 

Jorge Eduardo Cardona
jorgeecardona@gmail.com
jorgeecardona.blogspot.com

## github.com/jorgeecardona

Linux registered user  #391186

## Registered machine    #291871
",jorgeecardona,megaman821
306,2011-12-13 16:19:18,"@piotr-dobrogost: urllib3 is, as always, not an issue. 

Probally worth noting to @shazow though.
",kennethreitz,piotr-dobrogost
306,2012-01-24 13:32:17,"@shazow - asking here rather than filing an issue on urllib3, as I'm looking for clarification.

I've just run into this exact issue myself. Seeing as how it's been marked closed 'urllib3 bug', would you like an issue opened for it? I'm not seeing one currently.

**Edit:** For reference, urllib2 does import fine in this environment and throws a urllib2.URLError: ""urlopen error unknown url type: https"" Regular http connections work.
",pudquick,shazow
306,2012-01-24 16:26:42,"@pudquick Please open a urllib3 bug, I'll try to get this fixed this weekend. 
",shazow,pudquick
304,2011-12-10 18:29:54,"@jokull Something much like this is done in the HTTP cache Varnish by setting a grace period where Varnish will return stale values if the back-end is not responding.

The only note I would like to add is that cache headers should be respected and not to just blindly cache all GET requests.

Also other nice properties for a cache would be:
- Ordering of GET string variables before caching so things like http://example.com?a=1&b=2 is the same as http://example.com?b=2&a=1
- Excluding GET string variables that don't affect content like Google Analytics variables
",megaman821,jokull
304,2012-06-29 07:04:40,"@queeup cachecore is a storage interface for the cache and doesn't deal specifically with HTTP caching. For example, if you have code that looks at the Cache-Control headers to store GET responses, you could store those responses in cachecore.

If you are looking for a tool to handle the HTTP protocol details for caching, I wrote a wrapper based on the httplib2 algorithms. If you do try it out please let me know any issues you find - https://bitbucket.org/elarson/httpcache
",ionrock,queeup
304,2012-06-29 18:47:36,"@ionrock: this looks perfect. I think I'm going to try to merge this into the codebase ;)
",kennethreitz,ionrock
304,2012-06-29 18:56:54,"@kennethreitz That is great news. Please let me know if I can help. I'm happy to fork and try to merge it myself. I also plan on adding the etag and if-\* header support, which I'm happy to submit as a patch later if need be. 
",ionrock,kennethreitz
304,2012-06-29 18:58:21,"@ionrock Fantastic! Start watching the #700 pull request, where i'll be working on it. It'll be using the cachecore caching interfaces.
",kennethreitz,ionrock
295,2012-05-15 13:55:06,"@kennethreitz, If I wanted to take a look at implementing this and contributing it to requests, where would you suggest I start. I don't want to step on any toes :).
",cpatrick,kennethreitz
295,2013-01-26 21:50:38,"@sigmavirus24 

I don't see how making it possible to stream a request by passing generator/iterator as `data` param's value could magically change how another part of api (`files` param) works.
",piotr-dobrogost,sigmavirus24
295,2013-01-26 22:18:24,"@piotr-dobrogost I missed this [line](https://github.com/kennethreitz/requests/commit/ef8563a#L1R348) when I originally skimmed the commit. This is making me wonder how difficult streaming multipart file uploads could be.
",sigmavirus24,piotr-dobrogost
290,2017-02-01 09:38:22,"@wanaryytel Please do not comment on old issues (this issue was originally opened in *2011*, more than five years ago) to suggest you are seeing vague problems like ""timeouts not working"". The last comment here was made almost five years ago, and any problem you are seeing now is going to be entirely unrelated to this one.

If you are having an issue, either look for newer issues that replicate your problem or open a new one.

I should note, by the way, that ""I am seeing the same problem"" is using a few more words to say ""+1"", and is not hugely helpful. I highly recommend being a bit clearer about what problem you are seeing, and how you are triggering it. The more information you can give us, the more likely it is that you will see a resolution to your problem.",Lukasa,wanaryytel
285,2011-11-23 20:13:21,"@mailgun Multiple values for singe key **are** allowed. See [`test_urlencoded_post_query_multivalued_and_data`](https://github.com/kennethreitz/requests/blob/develop/test_requests.py#L390)
",piotr-dobrogost,mailgun
281,2012-04-20 21:08:35,"@slingamn

From https://github.com/sashahart/cookies

> This doesn't compete with the cookielib (http.cookiejar) module in the Python standard library, which is specifically
> for implementing cookie storage and similar behavior in an HTTP client such as a browser.

As to

> rather than an external cookie library that seems to have no real community and only one maintainer.

I wouldn't judge based on community's size or number of maintainers. Wouldn't Requests community become this library's community the moment Request starts using it?
",piotr-dobrogost,slingamn
281,2012-04-20 21:15:09,"@piotr-dobrogost 

I actually need cookie storage for my use case. Support for `LWPCookieJar` would be ideal. Cookies are persistent by nature; it seems arbitrary to have a cookie library that can only persist cookies across the lifetime of a single Python process.

Also, what would the relationship of `cookies` be to `requests` exactly? `cookies` doesn't necessarily seem stable or mature enough to warrant a formal dependency on it as an external library. But the alternative to that is forking it and maintaining it inside the `requests` tree, which seems drastic (even assuming license-compatibility).
",slingamn,piotr-dobrogost
281,2012-04-20 21:19:52,"@kennethreitz 

What do you mean by _The module is practically non-existant._?
",piotr-dobrogost,kennethreitz
281,2012-04-20 21:28:08,"@dhagrow poke, people are interested in your work :-)
",slingamn,dhagrow
281,2012-04-22 08:22:41,"@laurentb 

Can you provide a test such that `cookielib` has the incorrect behavior and `Cookies` has the right one?

There's still the issue with `Cookies` not supporting persistence to files.
",slingamn,laurentb
281,2012-04-23 03:43:56,"@dhagrow Thanks very much! I will look into updating your branch.
",slingamn,dhagrow
281,2012-05-04 09:13:41,"@dhagrow 

> I should note also that urllib3 appears to parse headers incorrectly. I had to dig down to the httplib and rfc822 level in order to get a properly parsed cookie header.

Maybe the problem you had is the same as one in https://github.com/shazow/urllib3/issues/3 (Use MultiDict for headers)?
",piotr-dobrogost,dhagrow
281,2012-05-04 13:16:33,"@piotr-dobrogost

That looks like the one, yes.
",dhagrow,piotr-dobrogost
276,2011-11-18 11:29:09,"Per my conversation with @maraujop:

I'm happy with OAuth support being merged wherever, so long as it's merged. I think OAuth is a sufficiently common use-case that requests would be better for having support in the box, so let's say I'm +0 on OAuth in requests, but +1 on it being anywhere that is pip-installable.

First this patch needs to land in requests, though.
",idan,maraujop
276,2011-11-18 11:32:33,"@jjmaestro, @maraujop: worth noting that this patch has nothing to do with OAuth, I'm merely saying that it lays a good foundation for OAuth support inside or outside of requests.
",idan,jjmaestro
276,2011-11-18 11:32:33,"@jjmaestro, @maraujop: worth noting that this patch has nothing to do with OAuth, I'm merely saying that it lays a good foundation for OAuth support inside or outside of requests.
",idan,maraujop
276,2011-11-18 15:48:30,"@idangazit sure, I can see how this is not OAuth but I read the main issue #275 (where you link your gist about OAuth) then read the Feature Request for OAuth in requests and saw your mention, clicked and kept reading...

Since everything ties together and since this is the place to discuss the potential foundations of OAuth I thought that it would be very interesting to discuss this with @maraujop and to also talk about your implementation of header-based Oauth.

@maraujop's hook is the de-facto OAuth support in requests (definitely the best available) and it's already pip-installable:

pip install requests-oauth

So what about merging this pull request and then putting your gist into a requests-oauth pull request? That way we get the foundations of a better auth support in requests and we get your nice header-based OAuth thus improving the best hook available. Later, you guys can test the shit out the hook and improve it to the point where @kennethreitz can consider if it makes sense to add it to the requests core :)

Win-Win for me!
",jjmaestro,maraujop
272,2011-11-17 09:54:27,"Also as @juanriaza says, this is not an API specific thing, Rdio fails the same way and it was working before.
",maraujop,juanriaza
269,2011-11-18 19:40:01,"@piotr-dobrogost: exactly. That is exactly why `allow_redirects` was added — and is set to `False` for POST requests. If the user explicitly sets it to `True`, then the redirect will be followed.
",kennethreitz,piotr-dobrogost
269,2011-11-20 13:11:31,"@kennethreitz: You mean `allow_redirects`?

You should read [this piece](http://codesearch.google.com/#OAMlx_jo-ck/src/net/url_request/url_request.cc&l=701) of the Google Chrome code:



This is also how all other browsers are doing it. I think it makes more sense to follow that instead of doing it different than everybody else.

Also, as said, some websites/servers even break otherwise because they expect this behavior. (They return 302 instead of a 303 because 303 was too less supported a while ago and browsers do the expected behavior on 302 anyway.)

Also, it was reported [here](https://github.com/szechuen/CCC-Presale/issues/3#issuecomment-2759174) that Requests indeed did it that way a while ago, at least in 0.7.4. It just broke since 0.8.0.
",albertz,kennethreitz
269,2011-11-29 22:13:11,"@dstufft: It's quite common to send 302 redirects after a successful POST. Perhaps you could argue that sites doing this are technically broken, but it's how many sites currently work. And it's how many popular frameworks, such as Django, suggest on handling successful POSTs.

I'm all for doing things ""the right way"". But I get the feeling that `strict_mode` will be set to `False` more often than not.
",andymccurdy,dstufft
269,2012-02-12 10:17:31,"@piotr-dobrogost thanks for the research. That's compelling.
",kennethreitz,piotr-dobrogost
269,2012-03-07 16:08:10,"I've been bitten by this for exactly the reasons @andymccurdy mentions, that frameworks like Django give 302 redirects. Could we at least add a note in the docs on this?
",Wilfred,andymccurdy
265,2013-01-29 09:12:24,"@teslachiang: Can you paste your full traceback, please? =)
",Lukasa,teslachiang
265,2013-01-29 13:05:19,"@teslachiang also, could you post the URL? 
",sigmavirus24,teslachiang
265,2013-01-29 16:37:28,"@untitaker: Just because the RFC allows it doesn't stop it being a bug. =)
",Lukasa,untitaker
265,2013-01-29 16:38:29,"@Lukasa If this is not a misbehavior, how would this be a bug then?
",untitaker,Lukasa
239,2011-12-22 21:52:21,"@tamiel how do we fix this?
",kennethreitz,tamiel
239,2012-04-30 20:32:11,"@acdha setting:



before I make any requests still results in the same error, but I think this isn't an option for my implementation as all the requests I'm making will require a redirect =/ 
",dalanmiller,acdha
239,2012-04-30 20:43:11,"@dalanmiller How are you processing your responses? I was previously using `async.map` with a response hook and it _appears_ to be more stable using a simple loop over `async.imap`:


",acdha,dalanmiller
239,2012-04-30 22:29:34,"@acdha 

I was just using a for loop through a url list and doing a request.get on each with my settings and such. 



I tried using your paste and it works for about 50 requests in my 900 length list, until I start to get ""max retries errors exceeded with url"" for the rest.  This is a pretty standard error though for hitting the same domain repeatedly though, no? 
",dalanmiller,acdha
239,2012-07-03 09:18:10,"For me the 'Too many open files' error occurred after downloading exactly 1k files. My solution was to disable keep-alive property, ever getting requests in chunks (@acdha thank you for the hint). `lsof -p PID | wc -l` shows a non-increasing number of connections during the execution.



[1] chunking: http://stackoverflow.com/a/312464
",lmillefiori,acdha
239,2012-09-22 10:43:29,"@kennethreitz What's the urllib3's issue number?
",piotr-dobrogost,kennethreitz
239,2012-11-28 19:37:42,"@silvexis could very well be related to the urllib3 bug, now I'm just wishing someone had answered @piotr-dobrogost :P
",sigmavirus24,silvexis
239,2012-11-28 19:37:42,"@silvexis could very well be related to the urllib3 bug, now I'm just wishing someone had answered @piotr-dobrogost :P
",sigmavirus24,piotr-dobrogost
239,2013-08-10 13:59:03,"@Lukasa this was definitely fixed in urllib3 as I was part of the discussion. With an inclination towards being conservative in my estimate, I would say it's there since requests 1.2.x if not 1.1.x.
",sigmavirus24,Lukasa
239,2013-11-29 16:33:20,"@tardyp also, how did you install requests? I think all of the OS package maintainers strip out urllib3. If they don't keep that up-to-date and you're using an old version, that could be the cause instead. If you're using pip, then feel free to open a new issue to track this with instead of adding discussion onto this one.
",sigmavirus24,tardyp
239,2013-11-29 16:35:17,"I installed with pip, but I use python 2.6, I've seen fix on python2.7 for
this bug. Do you monkeypatch for older version?

Pierre

On Fri, Nov 29, 2013 at 5:33 PM, Ian Cordasco notifications@github.comwrote:

> @tardyp https://github.com/tardyp also, how did you install requests? I
> think all of the OS package maintainers strip out urllib3. If they don't
> keep that up-to-date and you're using an old version, that could be the
> cause instead. If you're using pip, then feel free to open a new issue to
> track this with instead of adding discussion onto this one.
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/issues/239#issuecomment-29526302
> .
",tardyp,tardyp
239,2013-11-29 16:53:04,"@tardyp please open a new issue with as much detail as possible including whether the requests you're making have redirects and whether you're using gevent. Also, any details about the operating system and an example of how to reproduce it would be fantastic.
",sigmavirus24,tardyp
239,2014-11-25 19:12:35,"@polvoazul There's no way this is the same issue, which was originally reported in 2011, so I don't think reopening is correct. However, if you're running the current release of requests (2.4.3) and can reproduce the problem, opening a new issue would be correct.
",Lukasa,polvoazul
239,2016-01-23 10:21:51,"@Lukasa  i need you help 。 i use eventlet + requests，that always create so many sock that can't identify protocol 。 my requests is 2.4.3, is eventlet + requests cause this problem? 
",mygoda,Lukasa
239,2016-01-23 12:49:44,"I'm sorry @mygoda, but it's impossible to know. If you aren't constraining the number of requests that can be outstanding at any one time then it's certainly possible, but that's an architectural problem outside the remit of requests.
",Lukasa,mygoda
239,2016-01-23 13:10:58,"@Lukasa  thank you。  i think my issue is similar with [this](https://gist.github.com/tamiel/1512329)。 my project is [pyvmomi](https://github.com/vmware/pyvmomi). that connection is long-connection. i always confused why can hold so many can't identify protocol  sock
",mygoda,Lukasa
239,2016-03-17 03:04:01,"@mygoda you use awesome periods。
",kennethreitz,mygoda
237,2011-11-06 17:45:31,"@osuchw, that makes all of my digest auth endpoints fail. What service are you using?
",kennethreitz,osuchw
237,2011-11-07 19:19:56,"@mfeif, Requests v0.7.6 was just released that fixes this :)
",kennethreitz,mfeif
237,2013-04-23 05:59:34,"Any word on this? I don't see another open bug that covers this as indicated by @mfeif. 
I'm bumping up against infinite recursion with 401 responses using digest auth. 


",jmakeig,mfeif
237,2013-04-23 13:40:48,"@jmakeig could you file a new issue for this?
",sigmavirus24,jmakeig
232,2011-10-31 17:04:23,"Yea, thoughts like ""man, that @kennethreitz is such a jerk!"". ;)

---

A weak Monday morning attempt to recapture my IRC arguments:

There are, IMO, two types of failure modes in this kind of library: network failures, such that you never connect to or hear back from the HTTP server; and HTTP failures, such that you successfully get back a non-200 response.

Kenneth's default is to suppress exceptions, return a valid Request object, and require the user to explicitly ask about exceptions via `Request.raise_for_<whatever>`. (IIRC he does this for both types of failures, but had a thought to create a 2nd `Request.raise_*` method so users could tell the failure types apart.)

My opinion is that this is un-Pythonic and surprising; somebody new to the library but not new to Python will be confused upon getting a successful-but-empty `Request` object, even if the server was down/DNS was down/bad URL/etc.

Furthermore, one will get a ""farther down the road"" traceback (e.g. `None has no attribute 'blah'` or `IndexError`s or etc etc) when trying to use the `Request` object as if it had succeeded, e.g. using its `.content` or `.raw` attributes, which is more confusing and harder to debug than an obvious, at-connection-time `NetworkError` or whatnot.

Such error suppression needs to have a serious, obvious benefit to offset the negative effects of forcing them to appear in strange ways farther down the stack. I don't see such a benefit here, other than Kenneth's noted use case of making N requests in a row/simultaneously, and not wanting connection errors to abort the entire run.

In that situation, my opinion is one should use an opt-in, ""please suppress errors, I will check for them myself"" setting, and that it should not be the default behavior.
",bitprophet,kennethreitz
232,2011-11-02 11:47:24,"Interesting.

FWIW, I certainly expected exceptions to be raised automatically; didn't even think to look for raise_for_\* methods!  +1 to @bitprophet's suggestion of making this behaviour opt-in.

Making it easy to catch all possible exceptions raised by Requests (without doing an ""except:"" thing) would be helpful, though.  Either by some fiddling to rebase the exceptions (so I can do ""except Requests.Error:"") or by some nicer technique if you can come up with one...
",rboulton,bitprophet
232,2011-11-10 17:34:06,"I'm new to the Python scene, but I agree with @bitprophet. Requests that have no chance of bringing back any meaningful data should fail fast and fail loudly, especially when it comes to malformed URLs. 
",joequery,bitprophet
223,2011-11-29 16:29:05,"@zoranzaric: `params` is used for query url parameters. `data` is used for body data.
",kennethreitz,zoranzaric
223,2011-11-29 16:39:05,"@kennethreitz ok yeah snap... with `data` it works... thanks and sorry for the trouble!
",zoranzaric,kennethreitz
223,2011-11-29 16:40:08,"@zoranzaric, no worries! A lot of other libraries are quite inconsistent, so I completely understand the confusion. That's the whole reason I started Requests :)
",kennethreitz,zoranzaric
223,2012-05-02 10:51:58,"@aknuds1 what version are you using? This is what I see in trunk:


",slingamn,aknuds1
223,2012-05-02 10:59:03,"@slingamn Ah, maybe it's fixed in trunk then. I'm just using the version installed via pip, i.e. 0.11.2.
",aknuds1,slingamn
223,2012-05-02 16:31:38,"@slingamn It's the desired output, yes, but why is there a difference towards WCF when I define the content-length header myself?

I mean, httpbin.org reports that the request has defined content-length as ""0"", even though the same request directed at WCF fails with error 411.
",aknuds1,slingamn
223,2012-05-04 03:37:32,"OK, there's good news and there's bad news. This is from my 2.7.2 `httplib.py`:



The relevant piece of code is the `if body and` --- `httplib` is explicitly declining to set `Content-length` for an empty body.

The RFCs are not entirely clear to me. http://www.w3.org/Protocols/rfc2616/rfc2616-sec4.html says:

> For compatibility with HTTP/1.0 applications, HTTP/1.1 requests containing a message-body MUST include a valid Content-Length header field unless the server is known to be HTTP/1.1 compliant.

so it depends on whether a zero-length body counts as ""containing a message-body"". But http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html says:

> Applications SHOULD use this field to indicate the transfer-length of the message-body, unless this is prohibited by the rules in section 4.4.

so it seems like adding the header wouldn't hurt.

@aknuds1 do you think it would be appropriate to file a bug against Python?
",slingamn,aknuds1
223,2012-05-04 06:46:48,"@slingamn Yes, it seems appropriate to me to file a bug against Python, considering this is a real world problem. I suspect it's actually IIS 7.5 that rejects the requests without defined content-length, rather than WCF, since these rejected requests never show up in my WCF log. I've enabled logging of all levels and malformed messages etc, so I'm quite sure these client errors would be logged had they been raised by WCF (as opposed to IIS), the way that for instance invalid service operation invocations are.

I found a StackOverflow [question](http://stackoverflow.com/questions/5915131/can-i-send-an-empty-http-post-webrequest-object-from-c-sharp-to-iis) on this problem re. IIS, which confirms that this server does indeed require the content-length header on POST requests.

Would you like to file the bug against Python, or should I?
",aknuds1,slingamn
223,2012-05-04 08:27:22,"@aknuds1 can you do it? :-) And if you could post a link to the ticket here, that'd be awesome.
",slingamn,aknuds1
223,2012-05-04 17:36:58,"@slingamn Which HTTP methods do you think content-length should be defined for? POST and PUT? I've only seen it mentioned so far that content-length should be defined for requests that intend to place something on the server, and I'm hardly the HTTP expert myself.
",aknuds1,slingamn
223,2012-09-05 12:35:26,"@nettok something along the lines of:



But of course if you're sending data, it wouldn't be zero. If you're sending a body, e.g.,


",sigmavirus24,nettok
223,2012-09-21 14:25:29,"Also, just realized this had bitten me a while back [here](https://github.com/sigmavirus24/github3.py/blob/master/github3/models.py#L120). If @kennethreitz still wants a PR attaching Content-Length to everything, I'd be happy to do so.
",sigmavirus24,kennethreitz
223,2012-10-18 17:28:17,"@kennethreitz where should I branch from, develop or adapters?
",sigmavirus24,kennethreitz
223,2012-11-27 01:59:30,"@kennethreitz Looks like the content-length is being set in case of GET request as well. Where the data being passed is going as query params in the URL. This could be a 400 Bad Request case (which is what is happening for me).

Following is an example of headers sent:
{'Content-Length': u'33', 'Content-Type': 'application/x-www-form-urlencoded', 'Accept-Encoding': 'gzip, deflate, compress', 'Accept': '_/_', 'User-Agent': 'python-requests/0.14.2 CPython/2.7.3 Darwin/10.8.0'}

And this was for a GET request.
",amalakar,kennethreitz
223,2012-11-27 02:12:24,"Can you provide the call @amalakar? This must be a mistake on my part and I'd like to fix it.
",sigmavirus24,amalakar
223,2012-11-27 02:16:24,"To clarify @amalakar, testing against HTTPBIN, with `requests.get('http://httpbin.org/get', params={'foo': 'bar'})` I get in the response Headers like:



which is a bit bizarre but probably a bug on HTTPBIN's end. (The fact that Content-Length is `u''` is what I find bizarre.)
",sigmavirus24,amalakar
223,2012-11-27 10:14:02,"Yup, the server is doing the wrong thing here. (Though, in its defense, your request is semantically meaningless, as @piotr-dobrogost has pointed out. =D )
",Lukasa,piotr-dobrogost
185,2011-10-08 02:00:41,"I agree that a theoretical discussion can be had about this, and I'd probably be on your side, @sergedomk. However, we cannot ignore that, _in reality_ servers can and do use data in the request body even for DELETE requests, and other standard tools (like `curl`) support this.

If you want an example, here's the first one that comes to mind: http://docs.gnip.com/w/page/23733233/Rules%20Methods%20Documentation#deleterules
",apetresc,sergedomk
179,2012-07-27 06:13:25,"@sigmavirus24 That'd be wonderful! Looking forward to it :)
",kennethreitz,sigmavirus24
179,2012-07-31 15:47:13,"@sigmavirus24, hmm, what operating system are you running? what version of python?

can you try `make simple`
",kennethreitz,sigmavirus24
179,2012-07-31 15:48:58,"Oh, that's interesting. I remember what you're talking about @sigmavirus24 , but it's not the exact same failure case. The intermittent error I was seeing was associated with httpbin.org's /redirect loop occasionally returning 503s. [Relevant url](https://github.com/kennethreitz/requests/pull/746#issuecomment-7308242).

EDIT: too slow twice in a row. I'll leave this here for posterity.
",Lukasa,sigmavirus24
179,2012-07-31 16:13:54,"Slackware 13.37 (x86_64) w/ python 2.6.6 but it was my error that the repository wasn't entirely up-to-date. I hope to finish `_encode_files` by the end of today to make sure it works properly. `_encode_data` already uses/accepts a list of k/v tuples and shouldn't really need to be changed unless I'm missing something.

Sorry to bother you @Lukasa.
",sigmavirus24,Lukasa
179,2012-08-02 14:29:02,"@kennethreitz Anything other than the following that need this functionality?

In `Session.request`:
- `headers` **done**
- `cookies`
- `proxies` **done**
- `config`
- `cert`
- `params` **done**
",sigmavirus24,kennethreitz
179,2012-08-02 21:14:47,"@sigmavirus24, `params`, `data` :)
",kennethreitz,sigmavirus24
179,2012-09-03 20:52:54,"""files"" could benefit from the magic treatment too. Happy to chip in test cases & fix on top of @sigmavirus24 work if it helps.
",CraigJPerry,sigmavirus24
179,2012-09-11 03:18:34,"@kennethreitz I'm pretty sure you can close this. I'm almost entirely certain that this is resolved as per the last PR of mine you merged (the one in reference to #817)
",sigmavirus24,kennethreitz
171,2013-12-19 13:13:51,"@ssbarnea This issue is more than two years old: it has nothing to do with the problem you're currently experiencing. =) Can you open a new issue to track your problem please?
",Lukasa,ssbarnea
171,2013-12-19 17:24:22,"@ssbarnea 
It most probably a buggy server doing wrong stuff when spoken to in specific SSL/TLS versions like described in #1567 (and several more).
Try to force a different version like it is explained [here](http://lukasa.co.uk/2013/01/Choosing_SSL_Version_In_Requests/) (some more kudos to @Lukasa)
",t-8ch,Lukasa
171,2013-12-19 17:24:22,"@ssbarnea 
It most probably a buggy server doing wrong stuff when spoken to in specific SSL/TLS versions like described in #1567 (and several more).
Try to force a different version like it is explained [here](http://lukasa.co.uk/2013/01/Choosing_SSL_Version_In_Requests/) (some more kudos to @Lukasa)
",t-8ch,ssbarnea
171,2013-12-19 17:46:32,"@ssbarnea You'll find that the limitation isn't in Requests but in Python. Currently no shipped versions of Python have support for any version of TLS more recent than V1.0 as you can see in the table [here](http://docs.python.org/3.3/library/ssl.html#ssl.wrap_socket).

Python 3.4 (not yet shipped, no guarantees that Requests is compatible with it) allows support for TLS1.2 and TLS1.1, so you can try with that. Otherwise, you'll have to allow TLSv1 to allow Requests through.
",Lukasa,ssbarnea
167,2011-10-09 16:36:22,"@FSX: attaching a user-agent to a request. 

Use cases: When a site blocks non-browsers from making requests. Maybe also testing?
",kennethreitz,FSX
166,2011-09-17 17:00:41,"@mrtazz, can you send another one?
",kennethreitz,mrtazz
165,2011-09-16 10:39:26,":+1: @cactus.
",kennethreitz,cactus
165,2011-09-16 21:29:22,"I certainly see where you are coming from @j0hnsmith.
My example was indeed simplistic (geared towards that one simple example you had presented).
Glad you found a work around that is tenable given your requirements and use case. 

I think the argument against passing in a response object is that all the other method parameters modify the request and request behavior, and not the response. Providing a custom response object could have other repercussions inside the request handling, and might require more runtime checking to make sure expected methods are present. That could be a pain.

I think using a decorator to add a response hook is a pretty clean way to go. nice.
",cactus,j0hnsmith
165,2013-01-31 17:32:08,"@nfx this pull request wasn't excepted. There never was such a feature.
",sigmavirus24,nfx
161,2011-11-15 23:34:59,"@kennethreitz I think this problem is still there, I'm getting a lot TooManyRedirects errors.

The fix by @jerem worked for me, but had to adapt it a little bit to the latest Requests version.
",michielgardner,kennethreitz
161,2015-09-06 13:38:46,"@imrek without any detail, my best advice to you is to ask for help on [StackOverflow](https://stackoverflow.com/questions/tagged/python-requests) and provide more detail there, e.g.,
- What url are you trying to make a request to?
- What does your code look like?
- etc.
",sigmavirus24,imrek
156,2012-03-31 16:03:57,"@Savier: `_detected_encoding()` is checking the bytes of the response for an encoding. Chardet does a damn good job to :)
",kennethreitz,Savier
138,2011-08-23 17:04:04,"@bjourne: Check out #90 for an example. Your use case is a bit different, but this is the general idea.
",kennethreitz,bjourne
136,2011-08-31 05:18:55,"@densh, I've considered this before, but I like being able to 'import requests' directly from `test_requests.py` without having have requests installed into a site-packages w/ `setup.py develop`.

If there's an elegant way to accomplish both at the same time, I'm game.
",kennethreitz,densh
136,2011-08-31 07:42:33,"@kennethreitz: You shouldn't really assume PYTHONPATH=`pwd` anyway (since different flavors of python won't like running the same bytecode) - so if you want to create the subdir, either inject path to `ls -d build/lib-*/` depending on python version or more commonly let the user decide by setting PYTHONPATH. I guess one could say that the latter is already in play (disregarding moving tests) :-)
",jbergstroem,kennethreitz
115,2011-08-23 08:19:51,"@kennethreitz: It would have been good if you had written here that you've done the merge here: https://github.com/kennethreitz/requests/commit/8fbb1e6d97cda90d588d4263a18906a52d147fba#requests/utils.py
",jedie,kennethreitz
115,2011-08-23 13:28:42,"@monkeython excellent point. Let's just stick to headers. This is an HTTP library after all, not HTML :)
",kennethreitz,monkeython
114,2011-08-15 15:37:46,"@jedie: I can make it an OrderedDict, so the order won't be lost. 

Making the raw request headers available is interesting. Same with the response. 

**Random ideas:** 

Raw property on the headers would be useful: `r.headers.raw` and `r.request.headers.raw` perhaps.
",kennethreitz,jedie
96,2011-08-12 19:29:32,"Just ran into this bug today, thanks for the fix @jeremys
",kyleconroy,jeremys
91,2011-08-13 17:03:33,"@jerem: This is fixed now!
",kennethreitz,jerem
90,2011-08-06 13:12:01,"Thanks @moliware, this is what I did.
",janrito,moliware
90,2011-08-06 15:02:52,"@moliware: Excellent! I'm going to add this to the upcoming cookbook :)
",kennethreitz,moliware
79,2011-06-24 16:22:58,"I believe this is correct behaviour, and a good example of why using fragments in this way is a really bad idea.

> Fragment identifiers have a special role in information retrieval systems as the primary form of client-side indirect referencing, allowing an author to specifically identify aspects of an existing resource that are only indirectly provided by the resource owner.  As such, the fragment identifier is not used in the scheme-specific processing of a URI; instead, the fragment identifier is separated from the rest of the URI prior to a dereference, and thus the identifying information within the fragment itself is dereferenced solely by the user agent, regardless of the URI scheme. - [RFC 3986](http://tools.ietf.org/html/rfc3986)

**Edit**: Damn, @kennethreitz, you fast!
",jgorset,kennethreitz
71,2011-06-23 07:42:34,"It should also be noted that `data` is not an available keyword on the `get` function, which should be the primary method of making GET requests. 

@jgorset, are you using `request.request` directly? I view that as an internal-only function.
",kennethreitz,jgorset
71,2011-06-23 07:45:30,"@kennethreitz,

if `request` is an internal function, I don't think it should be defined in the `api` module. I'd argue that it's [very useful](https://github.com/jgorset/facepy/blob/master/facepy/graph_api.py#L106), though.
",jgorset,kennethreitz
71,2011-06-23 07:48:19,"@jgorset, can you hop on freenode #python-requests? 
",kennethreitz,jgorset
70,2011-07-05 15:25:35,"@merwok: support both from the same codebase, ideally (this is what I do for [tablib](https://github.com/kennethreitz/tablib)). 
",kennethreitz,merwok
70,2012-01-13 22:29:50,"@jezdez nothing wrong with some motivation :)
",kennethreitz,jezdez
64,2013-04-23 07:17:33,"You should probably talk to @kennethreitz about whether he's prepared to dual-license Requests, either in general or in specific cases.
",Lukasa,kennethreitz
64,2013-04-23 07:24:42,"Looks like GPLv2 projects are out of luck then @ThomasWaldmann. Sorry!
",Lukasa,ThomasWaldmann
61,2011-06-17 06:57:10,"In all honesty, @kennethreitz, it is I who should've updated the docs when I committed that. ;-)
",jgorset,kennethreitz
61,2011-06-22 01:05:11,"@jgorset: feel free to :)
",kennethreitz,jgorset
47,2013-02-07 13:30:03,"@kcr: The biggest problem requests-kerberos has is a lack of tests. If you want to contribute a test suite (or even just a couple of tests), that will greatly improve our ability to keep it up to date. =)
",Lukasa,kcr
47,2013-02-07 16:03:53,"@Lukasa I suspect we'll be able to repurpose at least the setup parts of the Zephyr test suite we're working on over the next couple of weekends, we'll definitely be in touch about that :)
",eichin,Lukasa
47,2013-02-07 16:26:06,"@eichin, link for the curious?
",sigmavirus24,eichin
34,2011-05-20 17:00:13,"@jgorset already done :)

**EDIT:** You already know :P
",kennethreitz,jgorset
30,2012-01-24 12:44:51,"@kennethreitz that is to check server cert against a CA bundle or i'm missing something?

I've found traces of key/cert pair in packages, but not a mention of this in requests itself. Looks like there's some non-obvious way to actually pass client stuff to server as an auth token.
",wiz,kennethreitz
30,2012-01-24 13:50:09,"@cjw296: this pretty link: http://cl.ly/2U003k373O1L0b400y08
",kennethreitz,cjw296
26,2011-04-22 07:47:21,"@jgorset thanks! I'll try to take a look at this and add some tests later today
",kennethreitz,jgorset
23,2011-05-16 07:23:23,"Nice work, @kennethreitz!
",jgorset,kennethreitz
23,2011-05-16 08:07:15,"Thanks @kennethreitz!
",esaurito,kennethreitz
12,2011-05-19 22:48:14,"I think @mcilrain might have been proposing something along the lines of...



I'll venture a guess that he was not aware that requests saves cookies it receives and attaches them to subsequent requests, though, which (besides being really awesome) complicates things because cookies set in this way would lack traits like expiry and domain.

It's entirely possible to create cookies programatically as it is, however it's... uh... a bit tedius:



I didn't specify 17 arguments to `Cookie` for fun, by the way; they're all required. It's pretty clear (and [documented](http://docs.python.org/library/cookielib.html#cookielib.Cookie), too) that developers are not expected to construct their own cookies.

I can see how being able to do that might be really useful, though.
",jgorset,mcilrain
3933,2017-03-21 20:19:21,"Thanks for this report! It's very detailed, which is a good start. I'd like to confirm some things:

1. You used 3.5.2 and 3.5.3, with libssl 1.0.1 and 1.1.0 respectively. You said, however, that ""upgrading libssl didn't fix the problem"". Did that mean just moving to the latest libssl in those releases, or moving 1.0.1 to 1.1.0 on 3.5.2.
2. Does bug 2 go away when you remove pyopenssl?
3. Does this reproduce with a simpler scenario? That is, if you change the response to, for example, be a simple 200 OK? What about if you change the SSL method to SSLv23_METHOD?
4. Does this reproduce on different distributions? Your docker image is Debian stretch: if you use, say, Fedora Rawhide, does that problem go away?

I'm not super well at the moment so I probably won't be looking at this in the next day or two, so if anyone else wants to dive in then feel free. Also, I'm going to CC some folks associated with PyOpenSSL: @hynek @alex @reaperhulk.",Lukasa,alex
3933,2017-03-21 20:19:21,"Thanks for this report! It's very detailed, which is a good start. I'd like to confirm some things:

1. You used 3.5.2 and 3.5.3, with libssl 1.0.1 and 1.1.0 respectively. You said, however, that ""upgrading libssl didn't fix the problem"". Did that mean just moving to the latest libssl in those releases, or moving 1.0.1 to 1.1.0 on 3.5.2.
2. Does bug 2 go away when you remove pyopenssl?
3. Does this reproduce with a simpler scenario? That is, if you change the response to, for example, be a simple 200 OK? What about if you change the SSL method to SSLv23_METHOD?
4. Does this reproduce on different distributions? Your docker image is Debian stretch: if you use, say, Fedora Rawhide, does that problem go away?

I'm not super well at the moment so I probably won't be looking at this in the next day or two, so if anyone else wants to dive in then feel free. Also, I'm going to CC some folks associated with PyOpenSSL: @hynek @alex @reaperhulk.",Lukasa,hynek
3933,2017-03-21 20:19:21,"Thanks for this report! It's very detailed, which is a good start. I'd like to confirm some things:

1. You used 3.5.2 and 3.5.3, with libssl 1.0.1 and 1.1.0 respectively. You said, however, that ""upgrading libssl didn't fix the problem"". Did that mean just moving to the latest libssl in those releases, or moving 1.0.1 to 1.1.0 on 3.5.2.
2. Does bug 2 go away when you remove pyopenssl?
3. Does this reproduce with a simpler scenario? That is, if you change the response to, for example, be a simple 200 OK? What about if you change the SSL method to SSLv23_METHOD?
4. Does this reproduce on different distributions? Your docker image is Debian stretch: if you use, say, Fedora Rawhide, does that problem go away?

I'm not super well at the moment so I probably won't be looking at this in the next day or two, so if anyone else wants to dive in then feel free. Also, I'm going to CC some folks associated with PyOpenSSL: @hynek @alex @reaperhulk.",Lukasa,reaperhulk
3931,2017-03-20 19:57:00,"We consume a lot of URL shorteners and redirects. Unlike many requests users, we care about the entire journey -- not just the destination.  The current API makes it slightly inconvenient to iterate over the responses in order when needed. 

I often end up using a function that does the following, and wanted to suggest handling this upstream with something like...

    @property
    def response_chain(self):
        '''iterate over the responses in order, from the original request to ultimate response'''
        if self.history:
            for response in self.history:
                yield response
        if self.response:
            yield self.response

",jvanasco,property
3923,2017-03-14 18:41:10,"2 years after the original #2431, and after #3745
I encountered the strange behavior of `r.iter_lines()`, found this and tried to complete it.
Rebased once more and added more tests using the breakdown of @ianepperson https://github.com/kennethreitz/requests/pull/2431#issuecomment-72333964",vbarbaresi,ianepperson
3922,2017-03-14 15:40:47,"These CI failures aren't your fault. @kennethreitz, looks like our CI setup has busted again: want to take a look?",Lukasa,kennethreitz
3906,2017-03-03 09:01:36,"Aha, ok, so we're getting there. An explanation:

Requests, when using PyOpenSSL, needs to implement some logic to do non-blocking I/O. We do this by looking for the best available selector on the platform and using that. We check for the available selectors by looking at which names are defined in the `select` namespace.

It seems that this Python distribution is defining the `epoll` selector, but cannot actually *use* it. This is probably because the underlying OS defines the `epoll` syscall but cannot actually use it, and returns `ENOSYS`.

Now, we can work on this by extending our `selectors` module to actually attempt to create a selector on startup and looking to see if that works (/cc @SethMichaelLarson), but in the short term you can resolve this by running a small monkeypatch just after you import requests.

In your code, if you add:



Before you start running your main code, that should re-introduce the older behaviour. If `poll` works on your OS, you may want to use `PollSelector` instead of `SelectSelector`.

Longer term, you should consider reporting a bug upstream to suggest that they remove the definitions for syscalls that don't actually work on their platform: I imagine they'll run into trouble elsewhere.",Lukasa,SethMichaelLarson
3906,2017-03-04 14:11:32,"@sigmavirus24 Yeah, so I said a few comments ago that the selectors module can detect this case by actually trying to *instantiate* the selectors, at least for some of them where the selector itself is an FD (we can't do it so easily for poll/select). That's why I tagged @SethMichaelLarson: this is a bit his baby.",Lukasa,SethMichaelLarson
3897,2017-02-28 21:59:09,"This PR supersedes #3338 with the branch brought up to date, as well as a few minor changes, including logic tweaks based on updates to `super_len` since the original PR.

This will forcibly prevent the transmission of headers that include both Content-Length and Transfer-Encoding, as well as simplify redundant code we use for both `prepare_content_length` and `prepare_body`. All the credit here goes to @davidsoncasey, the original creator of this PR.",nateprewitt,davidsoncasey
3897,2017-03-01 17:33:33,"@davidsoncasey, glad we're able to get all of your work merged, thanks again!

@Lukasa, 3.0-HISTORY is updated :)",nateprewitt,Lukasa
3888,2017-02-24 17:40:34,"Hi there @atleta,

First, RFC 2616 has been made obsolete by RFCs [7230][], [7231][], [7232][], [7233][], [7234][], and [7235][].

As you'll see when you familiarize yourself with those RFCs, the *default* encoding for Headers is no longer ISO-8859-1 (a.k.a., Latin-1).

I suspect that if you have a specific encoding you want to do you should disallow redirects and take control over URL encoding yourself. @Lukasa may disagree, though.

[7230]: https://tools.ietf.org/html/rfc7230
[7231]: https://tools.ietf.org/html/rfc7231
[7232]: https://tools.ietf.org/html/rfc7232
[7233]: https://tools.ietf.org/html/rfc7233
[7234]: https://tools.ietf.org/html/rfc7234
[7235]: https://tools.ietf.org/html/rfc7235",sigmavirus24,Lukasa
3886,2017-02-23 22:53:28,"> The fact that this counts as regressing code review is stupid but there we are.

How do you mean, @Lukasa?

This looks generally fairly good, but I'm not sure I'm entirely clear. We're changing the ordering of the response history? Is there a good reason why? Has it been wrong all this time?",sigmavirus24,Lukasa
3885,2017-02-22 20:59:35,"@Lukasa I see a line like this already in the code base



as a top level definition. I've attempted to mock those, but my attempts so far have had some code smells. I've added a new commit that attempts to mock those in the least smelly way, but it's not ideal. Can you help me iron these out so they aren't so strange?

My approaches so far:
1. I set `current_time` in the class and mock it using `mocker.patch.object`. If I set it as a staticmethod or a classmethod, I'm not able to replace the function with a lambda w/o any args or with one anonymous arg. I'd rather not set it as an instance method of the class as that's a bit strange.
1. I use `mocker.patch` and set `current_time` at the global level of requests.structures. The only problem then is I have to import requests.structures (or just requests.structures.current_time), and pyflakes complains about an unused import.

Would appreciate any suggestions.",davidfontenot,Lukasa
3884,2017-02-22 08:58:01,"Thanks for this! I think this is absolutely worth doing but there's an element of style consideration here, so I'm going to let @kennethreitz make the call on this one.",Lukasa,kennethreitz
3874,2017-02-14 16:38:40,"Yep @sigmavirus24, `response` and `history` were added to 3.0.0 in commits by Kenneth.

Edit: 66eedec to be specific.",nateprewitt,sigmavirus24
3874,2017-02-21 18:13:30,"I think it's good to go @jvanasco, we were just waiting for confirmation from @kennethreitz. @Lukasa we can probably just merge though since this isn't actually a change, yeah?",nateprewitt,kennethreitz
3868,2017-02-11 21:17:14,"@Lukasa added a changelog entry to 3.0-HISTORY. Also the Travis build fails, but it doesn't appear like it's because of my change... not sure how to investigate...",vmalloc,Lukasa
3856,2017-02-08 11:14:52,"Oh boy, you are going to get @Lukasa screaming after you soon :) From this email https://lwn.net/Articles/643399/ it is intentional that requests take all sort of types for the ""param"" value. If something needs a fix, that probably would be Requests documentation.

Also, if you have not already, read #3855 ",JordanP,Lukasa
3851,2017-02-03 17:14:31,"@kennethreitz, it looks like the Pipfile was rebuilt yesterday with the latest version of `pipenv`. We'll need to remove the version we had pinned now that kennethreitz/pipenv#90 is resolved.",nateprewitt,kennethreitz
3840,2017-02-14 08:20:43,"> Perhaps these exceptions could be extended to include the `self` response object.

@jvansco That shouldn't really be necessary. If the user sets `stream=True` that exception will only fire when accessing the body content, at which point the response object will already be in their hands. It's just not very hard to arrange a situation where any problems processing the body occur at a controlled and defined time.",Lukasa,jvansco
3837,2017-01-28 18:03:54,"I've ran into an issue that runs tangent to issue #441.

We have a content indexer that is powered by `requests`.  Some of the larger URL shorteners exhibit some weird behaviors depending on the user agent string.

* (variant of #441) depending on the user-agent header, a particular shortener will either send a proper 301 response OR a HTTP 200 containing a meta-refresh value of 0.

         <head><meta name=""referrer"" content=""always""><noscript><META http-equiv=""refresh"" content=""0;URL={URL}""></noscript><title>{URL}</title></head><script>window.opener = null; location.replace(""{URL}"")</script>

* (new?) a handful of url shorteners may send a HTTP-200 response with a `location` header to the redirect.  (yes I know, it breaks spec and makes no sense.

To handle both of these (and several other scenarios) I have a novel suggestion.  The callback hooks could be used to allow developers to catch and follow these types of redirect oddities within a single configured request.

A developer would be able to handle edge cases like the above by with a hook:

      def bad_shortener_callback(r):
          r_location = r.headers.get('location')
          if r.status_code == 200 and r_location!= r.url:
                r.is_redirect = True
                r.redirect_location_override = r_location
          return r

In order to make this work, a slight change would be needed: 

The redirect url is pulled via this line:

https://github.com/kennethreitz/requests/blob/f72684e13c5074a671506d29c1b5638156680ea7/requests/sessions.py#L116

    url = resp.headers['location']

I propose this change to sessions.py

    -url = resp.headers['location']
    +url = resp.redirect_location

 and then extending models.py accordingly

    + redirect_location_override = None

    @property
    def redirect_location(self):
          if self.redirect_location_override is not None:
             return self.redirect_location_override
         return self.headers['location']

if this is acceptable, I would be happy to issue a PR that includes tests.
",jvanasco,property
3837,2017-01-29 20:55:18,"regarding the comment from @lukasa on general utility (not the suggested implementation): i disagree about broad utility of following non-location redirects.  i know this means allowing users to operate on ""html"", but please consider that many consumers will eventually consume the HTML redirect (which could be a meta-refresh, rel=""canonical"", type=""og:url"" or several others).  it's not a niche use, but a common one.

regarding the implementation details comment from @sigmavirus24:  I do agree. stashing headers was a way to not suggest a larger patch.

i was a bit surprised the redirect handling used a `while` loop to create a generator that is immediately consumed, and then just overrides the request.   i had looked through the commits and tickets, and it seems like approach was dictated by earlier api behaviors that no longer exist (or I haven't seen).",jvanasco,lukasa
3835,2017-01-26 21:18:43,@Lukasa I believe there were issues with ONLY doing 401 as noted in #3772. The related RFCs allow for the WWW-Authenticate header in a non-401 response which the client SHOULD respond to if received. I think the conclusion we came to is to support 4XX response codes.,nateprewitt,Lukasa
3830,2017-01-24 23:13:07,"This fixes the issue that was encountered this morning during the 2.13.0 release.

The source specified in the original Pipfile was for an incorrect pypi endpoint. This didn't matter until pipenv 2.6 was released, specifically [cb22a12](https://github.com/kennethreitz/pipenv/commit/cb22a129eae8c8e800e603c38bf1fe04d420fbde), which started using the provided `source` value.

@kennethreitz I used `pipenv lock` with pipenv 3.0.0 to generate the new lock file which I'm assuming is what we want here.",nateprewitt,kennethreitz
3827,2017-01-24 12:44:51,For now we'll just pin it so the builds keep working: @kennethreitz should feel free to remove the pin and investigate when he has some time.,Lukasa,kennethreitz
3817,2017-01-13 20:55:32,"Ok, @alex and @reaperhulk really need to sign off on this before I consider merging it. ;)",Lukasa,alex
3817,2017-01-13 20:55:32,"Ok, @alex and @reaperhulk really need to sign off on this before I consider merging it. ;)",Lukasa,reaperhulk
3817,2017-01-14 10:00:21,"Ok, all commenters, *please* read some of the discussion on #3213 before making further comments here. You'll find it helps you avoid retreading some ground. ;)

@reaperhulk, my main concern here is that we are fundamentally looking at adding this patch because ""importing PyOpenSSL is slow"". My question is: do you consider that a bug?",Lukasa,reaperhulk
3807,2017-01-11 01:12:05,"Hey @gilessbrown, thanks for opening this issue! This behaviour was exposed by a change made in (327512f) which removed exception handling for this case. It currently only exists in the 2.12.x releases, so using Requests 2.11.1 should work for you.

While you're seeing this behaviour in Requests, it actually seems to be how we're handling chunked responses without a body in urllib3. shazow/urllib3#990 introduced an attempt to catch this problem but the check is just slightly off. While it verifies the existence to `fp`, it doesn't check that `fp` isn't `None`.

I think the simple fix here is to change [the check](https://github.com/shazow/urllib3/blob/master/urllib3/response.py#L525) in urllib3 to `return getattr(self._fp, 'fp', None) is not None` which should give us what we actually want. It verifies that `fp` both exists, and is not the default `None` value.

If @Lukasa or @sigmavirus24 are in agreement, we can address this over in urllib3.",nateprewitt,Lukasa
3807,2017-01-11 01:12:05,"Hey @gilessbrown, thanks for opening this issue! This behaviour was exposed by a change made in (327512f) which removed exception handling for this case. It currently only exists in the 2.12.x releases, so using Requests 2.11.1 should work for you.

While you're seeing this behaviour in Requests, it actually seems to be how we're handling chunked responses without a body in urllib3. shazow/urllib3#990 introduced an attempt to catch this problem but the check is just slightly off. While it verifies the existence to `fp`, it doesn't check that `fp` isn't `None`.

I think the simple fix here is to change [the check](https://github.com/shazow/urllib3/blob/master/urllib3/response.py#L525) in urllib3 to `return getattr(self._fp, 'fp', None) is not None` which should give us what we actually want. It verifies that `fp` both exists, and is not the default `None` value.

If @Lukasa or @sigmavirus24 are in agreement, we can address this over in urllib3.",nateprewitt,sigmavirus24
3789,2016-12-23 17:33:05,@Lukasa Please let me know your thought on this. I think it should address your concern in my previous Pull Request.,moin18,Lukasa
3789,2016-12-26 18:09:14,"@sigmavirus24  Not sure why it is showing the status as ""Changes requested"" even though they are fixed in commit: https://github.com/kennethreitz/requests/pull/3789/commits/965eb3d1525e3d384623c0267654c2f07acc4aea

May be it due to additional import of `sys` (needed but was missing in previous PR)",moin18,sigmavirus24
3789,2017-01-10 20:36:31,@sigmavirus24  I have fixed the comments you mentioned. Please can you review them once more? :),moin18,sigmavirus24
3789,2017-01-10 20:55:01,"@moin18 would you be willing to rebase this rather than merging master into this branch? Alternatively, @Lukasa how do you feel about squash merging this?",sigmavirus24,Lukasa
3770,2016-12-15 02:06:09,"So we are not doing client auth here.

And the openssl client fails for me because there's no local certificate in my bundle for the issuer of that revoked certificate:



> tl;dr it seems that revoked ssl certs aren't being rejected, which could be an issue

This does seem, at this point, true. That said, I believe this is because requests (and openssl) don't by default turn on OCSP which is what we'd need to use to *detect* a revoked certificate.

There are lots of opinions about OCSP and certificate revocation, but one prevailing one is that it doesn't work very well (which isn't an excuse to not support it). Either way, I believe to support it, we need some work in the standard library ssl library and in cryptography to allow us to add that support to PyOpenSSL.

I'm not 100% confident in any of this, though, so I'd advise we wait until @Lukasa is up or someone like @reaperhulk can take a look.

---

Thanks for reporting this @mendaxi ",sigmavirus24,reaperhulk
3770,2016-12-15 02:06:09,"So we are not doing client auth here.

And the openssl client fails for me because there's no local certificate in my bundle for the issuer of that revoked certificate:



> tl;dr it seems that revoked ssl certs aren't being rejected, which could be an issue

This does seem, at this point, true. That said, I believe this is because requests (and openssl) don't by default turn on OCSP which is what we'd need to use to *detect* a revoked certificate.

There are lots of opinions about OCSP and certificate revocation, but one prevailing one is that it doesn't work very well (which isn't an excuse to not support it). Either way, I believe to support it, we need some work in the standard library ssl library and in cryptography to allow us to add that support to PyOpenSSL.

I'm not 100% confident in any of this, though, so I'd advise we wait until @Lukasa is up or someone like @reaperhulk can take a look.

---

Thanks for reporting this @mendaxi ",sigmavirus24,Lukasa
3761,2016-12-09 15:25:22,"@Lukasa I should have learned by now that when I feel the need for emojis, my changes are overzealous. Ready for another peek :)",nateprewitt,Lukasa
3760,2016-12-09 14:44:18,r4r @Lukasa @nateprewitt ,sigmavirus24,Lukasa
3760,2016-12-09 14:44:18,r4r @Lukasa @nateprewitt ,sigmavirus24,nateprewitt
3760,2016-12-09 14:53:29,"@Lukasa when you get another second to merge master into Proposed/3.0.0, I'll throw up the other patch.",nateprewitt,Lukasa
3758,2016-12-09 14:28:28,"@sigmavirus24 you're suggesting using `warnings.warn` to actively notify the user about the deprecation, correct? I can add that and the test here in a moment if you're not already working on it.",nateprewitt,sigmavirus24
3757,2016-12-12 14:26:32,"@Lukasa, I think this is ready for a peek whenever you've got a moment :)",nateprewitt,Lukasa
3753,2016-12-08 04:00:47,"I've got a feeling this may fall under Requests' policy of ""All header values must be a string, bytestring, or unicode."", since the Authorization field generated is a header, and the `auth` keyword is even discussed in the [Custom Headers](http://docs.python-requests.org/en/master/user/quickstart/#custom-headers) docs.

@Lukasa may have different feelings on this, but trying to permit non-string values here seems to have the same ambiguity issues we've tried to avoid elsewhere. Just like how `None` hasn't been permitted because `None` is cast to a string, when the user much more likely wants `""""`, which will generate different auth values.",nateprewitt,Lukasa
3752,2016-12-07 20:52:07,"This sounds like this is a bug in the standard library, actually: it seems like they introduced an infinite recursion when setting options directly. @tiran?",Lukasa,tiran
3745,2016-12-02 19:31:54,"This is really quite unexciting: #2431 had a tentative +1 from @Lukasa, pending a rebase and retargeting against 3.0.0, but it’s been sat unloved for nearly eight months. 😢 

This patch does the required rebase and targets the 3.0.0 branch.",alexwlchan,Lukasa
3745,2016-12-04 23:08:55,"I had a go at addressing the review markups, but I feel like more testing is required.

I feel like there’s enough subtle behaviour here that we’d be better off with a whole stack of tests (probably using `@pytest.mark.parametrize`) that tests a whole variety of cases here. Which I may or may not get around to writing sometime this week. At a minimum, the cases described in [this comment](https://github.com/kennethreitz/requests/pull/2431#issuecomment-72333964).",alexwlchan,pytest
3739,2016-11-30 23:52:35,"Hey @obestwalter, I talked to @Lukasa about this a few weeks ago. To quote a bit of his response:

>Requests moved away from Travis because it made a lot of live network requests in its test suite and Travis had frequent network outages that caused real problems with the test run. 

While a lot of these issues have been resolved, I think the general sentiment is Travis generates a fair amount of noise for the repo owner and has historically had quirks. Until recently, Requests ran CI on a Jenkins server, but that's since been removed.

As for tox, I think most of Requests' contributors are running a local tox.ini file for testing changes. If you run that in conjunction with something like [pyenv](https://github.com/yyuu/pyenv), you have an easy local equivalent of a Travis environment which has worked for the most part the last several months.

Note this may no longer be the state of things, I just wanted save Lukasa from having to reiterate what he's conveyed recently via other channels.

Here's the last couple of Kenneth's comments on the state of Requests' CI/Travis in 2016 too. [[Feb](https://github.com/kennethreitz/requests/pull/2991#issuecomment-178813436)] [[Apr](https://github.com/kennethreitz/requests/pull/3096#issuecomment-211701179)]",nateprewitt,Lukasa
3739,2016-12-01 08:18:37,@obestwalter All of these decisions are fundamentally @kennethreitz's: the other maintainers have no objection to re-adding Travis support and a toxfile.,Lukasa,kennethreitz
3738,2016-11-30 21:22:10,"This reverts commit 34af72c87d79bd8852e8564c050dd7711c6a08d6.

This commit was added by @tiran to try to avoid the IDNA-encoding logic that we added in v2.12.0. I believe that the *other* workarounds we merged for that are sufficient, and this change broke docker-py and probably broke others.

@tiran, can you confirm that your code continues to function with this change reverted? I'd like to be able to merge this and ship a v2.12.3 if at all possible.

Resolves #3735.",Lukasa,tiran
3738,2016-12-01 08:04:03,"@mshahpalerra As to the ETA of v2.12.3, it will be as soon as I can get confirmation from @tiran that the problem he was originally trying to fix is still fixed in this branch. If it isn't, then it will be as soon as I can work with him to build a new patch that avoids this specific issue.",Lukasa,tiran
3738,2016-12-22 15:05:36,@Islebodn could you provide an example http+unix:// URI/filepath that you're having issues with in 2.12.3? This should be fixed even without #3713 but perhaps we missed something.,nateprewitt,Islebodn
3738,2016-12-22 17:43:46,">> @Islebodn could you provide an example http+unix:// URI/filepath that you're having issues with in 2.12.3? This should be fixed even without #3713 but perhaps we missed something.
>Correct, this should be fixed in 2.12.4.
Thank you very much for fast response. i really appreciate it.

I did some tests with more version of pyhton-requests and I found out that 2.12.3 and 2.14.1 does not work well with upper case letters in path. The url is


I've just tested and upper case letters works well in such situation with 2.10.0 and 2.11.1",lslebodn,Islebodn
3735,2016-12-01 08:22:34,"In the short term, I think we need to back out the breakage: kicking things over with @graingert convinced me that ultimately, whether we ever intended docker-py's usage of Requests to actually work, it *has* worked for more than three years. If we're going to break that, we should break it on purpose, rather than by accident.

So this issue remains worth discussing because we should solidify our position on this kind of thing for 3.0.0. Either we'll want to add explicit support for having HTTP-like schemes, or we'll want to add support for custom schemes to register URL processing handlers, or we'll want to drop support entirely as we (accidentally) did in v2.12.2. We should discuss what of those we think we want to do, and make sure there is a transition plan in place for projects like docker-py.",Lukasa,graingert
3734,2016-11-30 15:10:21,"Reported by @graingert.

The patch filed by @tiran in #3713 seems to have broken docker-py. They're using a custom URL scheme (`http+docker`), which we previously applied URL preparation to but now do not. This has therefore busted what they were up to in v2.12.2.

I'm not immediately sure that we have a good way out of this. Prior to this patch docker-py users were *probably* at risk of encountering issues because the HTTP requests to the Docker API can be routed across unix domain sockets, which may entirely fail to contain hostnames and also fail to IDNA-encode. So I'm not sure that we don't need docker-py to route around this a different way.",Lukasa,tiran
3734,2016-11-30 15:10:21,"Reported by @graingert.

The patch filed by @tiran in #3713 seems to have broken docker-py. They're using a custom URL scheme (`http+docker`), which we previously applied URL preparation to but now do not. This has therefore busted what they were up to in v2.12.2.

I'm not immediately sure that we have a good way out of this. Prior to this patch docker-py users were *probably* at risk of encountering issues because the HTTP requests to the Docker API can be routed across unix domain sockets, which may entirely fail to contain hostnames and also fail to IDNA-encode. So I'm not sure that we don't need docker-py to route around this a different way.",Lukasa,graingert
3734,2016-11-30 15:17:26,"What is unclear to me is exactly *why* this skip is happening. @graingert, if you can run your example under pdb can you set a breakpoint at `requests.sessions.Session.request` and run `print url` when it breaks?",Lukasa,graingert
3717,2016-11-23 03:48:05,"Minor example update and rewording in response to the continued discussion after merging #3704. @afeld, if you're inclined to comment, does this change still address your original concerns?",nateprewitt,afeld
3716,2016-11-22 20:13:55,"@PatriotRDX so this is a bit harder because this is a reflection of the README.rst at the time of release. `pip install requests` is the correct command for installing requests on your machine. If you require an older version, the standard usage of pip has you specify a specific version number (`requests==2.2.1`) or a range (`requests>=2.6.0`). You can find more documentation on pip [here](https://pip.pypa.io/en/stable/).

This should be specified by whichever package you're trying to use in either the requirements.txt file, or setup.py. If it isn't, that should probably be brought up with the package maintainer.

@Lukasa or Kenneth may have different opinions on retroactively updating older release docs, but I'm not sure it's likely.",nateprewitt,Lukasa
3707,2016-11-18 00:55:01,"Hey @githubnovice, I'm unfortunately unable to reproduce this issue with the current information. Could you please provide the version of Requests you're using, the exact request (with parameters), and possibly the traceback you're seeing?

These are the examples I tried.


",nateprewitt,githubnovice
3707,2016-11-18 01:07:46,"## Hi Nate: thanks for the fast response!

TypeError                                 Traceback (most recent call last)
/home/gladwig/GNS3/Python/mkint.py in <module>()
    105
    106 if **name** == ""**main**"":
--> 107     main()

/home/gladwig/GNS3/Python/mkint.py in main()
     94     #print json.dumps(sla, indent=4, separators=(',', ': '))
     95     #return
---> 96     put_l2interface_state(s,controller_url,""gigabitethernet1"",False)
     97     l2 = get_l2interface_state(s,controller_url,""gigabitethernet1"")
     98     print ""g1 = ""

/home/gladwig/GNS3/Python/mkint.py in put_l2interface_state(session, c_url, interface, enable)
     61     url = c_url + 'api/v1/interfaces/' + interface +'/state'
     62     pdb.set_trace()
---> 63     resp = session.put(url, json=schema) #(should work!)
     64     #headers = {'Content-type': 'application/json'}
     65     #resp = session.put(url,data=json.dumps(schema),headers=headers)

/usr/lib/python2.7/dist-packages/requests/sessions.pyc in put(self, url, data, *_kwargs)
    505         """"""
    506
--> 507         return self.request('PUT', url, data=data, *_kwargs)
    508
    509     def patch(self, url, data=None, **kwargs):

TypeError: request() got an unexpected keyword argument 'json'

The version is: gladwig@ovs-linux-3:~/GNS3/Python$ pip show requests
Name: requests
Version: 2.12.1
Summary: Python HTTP for Humans.
The variables before the call are:
-> resp = session.put(url, json=schema) #(should work!)
(Pdb) print url
https://192.168.1.181:55443/api/v1/interfaces/gigabitethernet1/state
(Pdb) print schema
{'if-name': 'gigabitethernet1', 'enabled': False}
(Pdb)

From: Nate Prewitt [mailto:notifications@github.com]
Sent: Thursday, November 17, 2016 7:56 PM
To: kennethreitz/requests requests@noreply.github.com
Cc: Geoff Ladwig gladwig@gladworx.com; Mention mention@noreply.github.com
Subject: Re: [kennethreitz/requests] r=request.put(url,json={...}), put doesn't accept json as argument while post does? (#3707)

Hey @githubnovicehttps://github.com/githubnovice, I'm unfortunately unable to reproduce this issue with the current information. Could you please provide the version of Requests you're using, the exact request (with parameters), and possibly the traceback you're seeing?

These are the examples I tried.

> > > import requests
> > > 
> > > resp = requests.put('http://httpbin.org/put', json={""key"": ""value""})
> > > 
> > > resp.json()

{u'url': u'http://httpbin.org/put', ... u'headers': {u'Content-Length': u'16', u'Accept-Encoding': u'gzip, deflate', u'Accept': u'_/_', u'User-Agent': u'python-requests/2.12.0', u'Host': u'httpbin.org', u'Content-Type': u'application/json'}, u'json': {u'key': u'value'}, u'data': u'{""key"": ""value""}'}

> > > resp = requests.Session().put('http://httpbin.org/put', json={""key"": ""value""})
> > > 
> > > resp.json()

{u'url': u'http://httpbin.org/put', ... u'headers': {u'Content-Length': u'16', u'Accept-Encoding': u'gzip, deflate', u'Accept': u'_/_', u'User-Agent': u'python-requests/2.12.0', u'Host': u'httpbin.org', u'Content-Type': u'application/json'}, u'json': {u'key': u'value'}, u'data': u'{""key"": ""value""}'}

—
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHubhttps://github.com/kennethreitz/requests/issues/3707#issuecomment-261417796, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AIiYHe1b5pcybkaEAEMr_azFPMAh8xm2ks5q_PeHgaJpZM4K19xY.
",gladwig2,githubnovicehttps
3706,2016-11-19 12:08:31,"Ok so that is probably why curl succeeded in validating: it isn't using OpenSSL for its cert validation, but Requests is.

My _suspicion_ is that the problem here is that the certificate serial number in the `Authority Key Identifier` field does not appear to match the serial number in the issuer certificate, but I'm not _sure_ of that. I'm going to page in @reaperhulk here to validate my answer.
",Lukasa,reaperhulk
3705,2016-11-17 19:23:56,"@j-Christ, Jython 2.7 should support dumps without issue. I'd suggest checking your local directory or python path for another json.py file.
",nateprewitt,j-Christ
3701,2016-11-20 09:42:06,"No problem. =) I'd like to make @reaperhulk aware of this issue at this time, as this seems to be related to pyopenssl and cryptography in a way I don't fully understand yet.
",Lukasa,reaperhulk
3695,2016-11-17 15:37:08,"> An alternative solution to this is to say that if the idna module fails to encode we'll just go ahead and try to encode as ASCII. If that works then we shrug our shoulders and say everything is probably ok, and if it fails we catch that and throw InvalidURL. @sigmavirus24, how does that sound?

([comment](/kennethreitz/requests/issues/3683#issuecomment-261240279))

Do we want to hold here until a decision is made on this so we're not having to back things out?
",nateprewitt,sigmavirus24
3686,2016-11-15 18:59:25,"Thanks for opening this issue, @afaicode. This is currently being tracked in #3683 which will be the official issue for further discussion. I believe @Lukasa has stated underscores in host names are not behaviour we likely want to support, but you're welcome to raise objections in the appropriate issue.
",nateprewitt,Lukasa
3685,2016-11-15 18:33:58,"In #3682, @leth reported an error which I've just encountered:



Because the source of #3682 appears to be unrelated to this Python 3 only bug, I'm opening a new issue.

I have the same issue as leth, and requests_toolbelt is unusable due to this bug. I don't think I can wait for due time to pass. Should requests_toolbelt be trapping this exception?",jaraco,leth
3685,2016-11-15 18:42:52,"@jaraco I believe what you're seeing is shazow/urllib3#1025. It has been patched on master for urllib3 but was caught after 1.19 released. This may be enough of an issue to warrant bumping urllib3 to 1.19.1 and adding it to a 2.12.1 release? Until then, you'll likely need to use 2.11.1 unless @sigmavirus24 has a solution via requests-toolbelt.
",nateprewitt,sigmavirus24
3683,2016-11-21 14:09:24,"Just my 1c:  I am all for standards!  But I am also pragmatic -- in my case with this upgrade of `requests` I started to experience this behavior, whenever any other tool (browsers, etc) is working just fine.  Hostname in my case is not to be immediately renamed and underscore version immediately abandoned since this is a project which was out there for years, referenced in many scientific papers, etc.  So what should we do now?  we apparently can't simply adhere to standard 100%, we can't simply rename and forward (once again -- url is already spread out), we can't demand downgrades of requests... 

Actually! that restriction (no `_`) applies to **hostnames** not **DNS** records (see e.g. http://stackoverflow.com/a/2183140/1265472). Here we are actually talking about URLs and I do not think that those must contain only hostnames! (needless to say that not every URL points to a different host ;) )  See  https://tools.ietf.org/html/rfc3986#section-3.2.2 which defines `reg-name` (not a hostname) and here is relevant description:


which refers to ""domain names"" (once again which might be hostnames or some other DNS records).  So as far as I see it, it is actually `requests` which by using `idna` to verify that `reg-name` is a standard compliant **hostname**, unnecessarily imposes restriction that URLs must contain only hostnames in `reg-name`.
What do you think? ;)

CC: @chaselgrove ",yarikoptic,chaselgrove
3669,2016-11-10 15:33:34,"This is a continuation of the original PR (#3595) to address how we set Session cookies in Requests. The PR had enough disagreement on how this should be addressed that I felt it was better to move this into a discussion.

As noted in #3595, Requests currently allows the user to provide cookies as a dictionary. While we've allowed this for individual requests, because their domain is (mostly) scoped, there are some real security concerns for supporting this for Sessions.

The issues
=========

1. If the user sets Session's `cookies` attribute to a `dict`, it will happily accept it, but crashes when you try to send anything. To correct this issue, the cookies need to be supplied as a `CookieJar` instance as noted in the [documentation](http://docs.python-requests.org/en/master/user/advanced/#session-objects). This is unideal because we allow users to use an idiom supported everywhere else in the API, and then fail later with a fairly unhelpful exception.

2. #3595 automated this suggested process from the documentation, as we do elsewhere in Requests, but it allows you to easily set session-wide cookies that aren't bounded by a domain. This means it's easy to unintentionally send sensitive cookie information to unintended recipients.

3. The currently proposed solution of using `add_dict_to_cookiejar` or `cookiejar_from_dict` provides no extra protection against the security issues raised in #3595.

Solutions(?)
==========

1. I think this is a two step process. For Requests 2.x, I'm suggesting we add a warning to the user when they set Session `cookies` attribute to a `dict`. This points them at the documentation so they can at least *try* to do the right things. Otherwise, they're none the wiser until their program crashes with an error in a separate module and only minor clues on the cause. In 3.0.0, I think this warning should be raised an actual exception since we don't support cookies as dicts for Sessions.

2. (and also 3.) If we're going to require the extra step of making the user explicitly use `add_dict_to_cookiejar` or `cookie_from_dict`, then let's make them useful. They currently don't provide any benefit over the dictionary approach in #3595. We can emulate some of the functionality of how a browser handles cookies by allowing users to supply cookie parameters on a per-dictionary basis. This is similar to cookies set by individual page requests.



This is a relatively simple changed as shown in an initial mockup in e2a4f9f & 4454849. This would need to be accompanied by a documentation update (something like 3e4e5b7) to explain how to do this properly.


----
@sigmavirus24, when you've got a moment, could you confirm this issue properly encapsulates your concerns and I didn't leave anything out? As for the possible solutions, are any of these amenable? I know your work in #2714 will help this a little bit with the default cookie policy change, but we still need to provide a way for the user to easily adhere to the safeguards we're putting in place.",nateprewitt,sigmavirus24
3662,2016-11-11 23:31:43,"Hello, @lukasa!

Your idea about byte strings looks very good and fully matches the white spaces in spec.

But.

There are two ways to release your idea:
1) Save user/pass in bytes. Looks not good, because in fact we always need to check type of variable, before use it.
2) Convert user/pass to strings in **init**(). Looks not good, because we lose the original values.

And last, I think 95% peoples will be write code like this:



To my mind, it looks not 'for humans'.
Without this patch we can write this for same result:



But we can change only one line of code:



After that the same code will look as:



It looks for Humans :)

What do you think about all this?

Sorry for my grammar.
",klimenko,lukasa
3661,2016-10-31 16:02:55,"I am experiencing a race condition with a (Squid) proxy server, best described by @rdharrison2 in https://github.com/kennethreitz/requests/issues/2364#issuecomment-186214402:

> In my case I'm making a large number of periodic https requests to a apache2 server with connection keep-alive timeout 5s. There is a race between the server closing a timed out persistent connection and the client making a request on the same connection. Given a bit of network latency the client can often attempt to send a request on the socket before it gets notified it as been closed.

Rather than implementing a retry logic, I would like to ensure that my requests `Session` keep-alive ""timeout"" is lower than that of proxy's. But I cannot seem to find out how.

Either I'm blind or it's just not supported?",tuukkamustonen,rdharrison2
3655,2016-10-31 19:15:34,"Alright @Lukasa, I think everything in that last round should be addressed. A couple of notes on some decisions I made in these tweaks.

Python 2.7 decided it needs to return a long from `tell` instead of an int like every other version. I've adjusted for that by casting it to int because we can't test for long in Py3.

I didn't declare the ""sentinel object"" as a global because I couldn't come up with a scenario where it was equivocal to anything in the sessions module. Simply setting it to object accomplishes what we were looking for but perhaps there's a caveat I'm missing here?

~~**Tests.** I went a little crazy. I have a few ""lifecycle"" tests, but found a snag when adding `test_rewind_body_failed_tell`. I realized things were failing in different ways (`_body_position` was never being set with the custom test objects) but the error messages produced were still ""correct"". In order to prevent accidental regressions, I add specific tests for setting `_body_position` and failure conditions in `rewind_body` by manually setting `_body_position`. Let me know if you want these collapsed back.~~

Edit: I went ahead and expanded the test objects to simplify the testing.
",nateprewitt,Lukasa
3655,2016-11-01 14:06:07,"Here's another round of changes @Lukasa.

I moved the `rewind_body` to a util function but when I started looking at moving tests over to `test_utils` they're going to bring a lot of import overhead with them. I can chop them up into individual `_body_position` and `rewind_body` tests and divide them up accordingly. Otherwise, we can leave them where they are.
",nateprewitt,Lukasa
3651,2016-10-27 01:58:38,"when the http response like this:
`HTTP/1.1 401 Unauthorized\r\n
Server: DNVRS-Webs\r\n
Content-Type: text/html\r\n
WWW-Authenticate: Basic realm=""DVRNVRDVS""\r\n
WWW-Authenticate: Digest realm=""DVRNVRDVS"", domain=""::"", qop=""auth"", nonce=""2c4158c9159934737e69aa698494ae04"", opaque="""", algorithm=""MD5"", stale=""FALSE""\r\n`
I have print the response headers ,`{'Server': 'DNVRS-Webs', 'Content-Type': 'text/html', 'WWW-Authenticate': 'Basic realm=""DVRNVRDVS"", Digest realm=""DVRNVRDVS"", domain=""::"", qop=""auth"", nonce=""2c4158c9159934737e69aa698494ae04"", opaque="""", algorithm=""MD5"", stale=""FALSE""'}`,is this a bug? @nakajima @alex @mkomitee @thedaniel 
",beruhan,alex
3651,2016-10-27 01:58:38,"when the http response like this:
`HTTP/1.1 401 Unauthorized\r\n
Server: DNVRS-Webs\r\n
Content-Type: text/html\r\n
WWW-Authenticate: Basic realm=""DVRNVRDVS""\r\n
WWW-Authenticate: Digest realm=""DVRNVRDVS"", domain=""::"", qop=""auth"", nonce=""2c4158c9159934737e69aa698494ae04"", opaque="""", algorithm=""MD5"", stale=""FALSE""\r\n`
I have print the response headers ,`{'Server': 'DNVRS-Webs', 'Content-Type': 'text/html', 'WWW-Authenticate': 'Basic realm=""DVRNVRDVS"", Digest realm=""DVRNVRDVS"", domain=""::"", qop=""auth"", nonce=""2c4158c9159934737e69aa698494ae04"", opaque="""", algorithm=""MD5"", stale=""FALSE""'}`,is this a bug? @nakajima @alex @mkomitee @thedaniel 
",beruhan,mkomitee
3651,2016-10-27 01:58:38,"when the http response like this:
`HTTP/1.1 401 Unauthorized\r\n
Server: DNVRS-Webs\r\n
Content-Type: text/html\r\n
WWW-Authenticate: Basic realm=""DVRNVRDVS""\r\n
WWW-Authenticate: Digest realm=""DVRNVRDVS"", domain=""::"", qop=""auth"", nonce=""2c4158c9159934737e69aa698494ae04"", opaque="""", algorithm=""MD5"", stale=""FALSE""\r\n`
I have print the response headers ,`{'Server': 'DNVRS-Webs', 'Content-Type': 'text/html', 'WWW-Authenticate': 'Basic realm=""DVRNVRDVS"", Digest realm=""DVRNVRDVS"", domain=""::"", qop=""auth"", nonce=""2c4158c9159934737e69aa698494ae04"", opaque="""", algorithm=""MD5"", stale=""FALSE""'}`,is this a bug? @nakajima @alex @mkomitee @thedaniel 
",beruhan,nakajima
3651,2016-10-27 01:58:38,"when the http response like this:
`HTTP/1.1 401 Unauthorized\r\n
Server: DNVRS-Webs\r\n
Content-Type: text/html\r\n
WWW-Authenticate: Basic realm=""DVRNVRDVS""\r\n
WWW-Authenticate: Digest realm=""DVRNVRDVS"", domain=""::"", qop=""auth"", nonce=""2c4158c9159934737e69aa698494ae04"", opaque="""", algorithm=""MD5"", stale=""FALSE""\r\n`
I have print the response headers ,`{'Server': 'DNVRS-Webs', 'Content-Type': 'text/html', 'WWW-Authenticate': 'Basic realm=""DVRNVRDVS"", Digest realm=""DVRNVRDVS"", domain=""::"", qop=""auth"", nonce=""2c4158c9159934737e69aa698494ae04"", opaque="""", algorithm=""MD5"", stale=""FALSE""'}`,is this a bug? @nakajima @alex @mkomitee @thedaniel 
",beruhan,thedaniel
3636,2016-10-25 17:05:55,"Thanks for this @jortel!

I think that @jeremycline is planning a related PR that focuses on changes in urllib3 first. @jeremycline, how does this relate to your plan and what are your thoughts about this approach versus yours?
",Lukasa,jeremycline
3634,2016-10-24 14:29:27,"@Lukasa I've adjusted the test.
",jeremycline,Lukasa
3634,2016-10-25 00:40:36,"@Lukasa no worries. I haven't really worked with pytest and didn't see examples of patching in the existing tests so I got lazy :frowning_face:.
",jeremycline,Lukasa
3627,2016-10-18 17:18:25,"This is a minor fix in the same vein as #3591. This function calls `update` on the CookieJar which only exists on `RequestsCookieJar` not the standard library `cookielib.CookieJar`. While this function will now be somewhat trivial, this will ensure it maintains backwards compatibility. It may be worth discussing removal in the future.

I have a test as well @Lukasa but it seems a bit overkill. I can include it if you'd like though.
",nateprewitt,Lukasa
3627,2016-10-27 18:42:01,"Hey @Lukasa, just pinging on this when you've got a moment. This should be the last problematic use of `update` on CookieJars in the codebase.
",nateprewitt,Lukasa
3625,2016-10-21 07:20:34,"Ok, rather than block on @sigmavirus24 having time to swing back to this, I'll merge this now. @sigmavirus24 can provide feedback whenever he gets time back. Thanks for the work @mie00!
",Lukasa,sigmavirus24
3620,2016-10-14 09:56:36,"Fixes #3616. This adds support for IDNA 2008 by vendoring the idna module, with the kind permission of @kjd.

For those keeping track, changes like this are another reason that Requests should stay out of the stdlib. ;) See also: [CPython issue 17305](https://bugs.python.org/issue17305).
",Lukasa,kjd
3617,2016-10-13 12:28:59,"Congratulations @jeremycline!
",Lukasa,jeremycline
3616,2016-10-12 16:21:01,"As has been noted on the [Python Security SIG mailing list](https://mail.python.org/pipermail/security-sig/2016-October/000122.html), Python currently only includes the deprecated IDNA 2003 codec in the standard library. This is problematic, because IDNA 2003 is forbidden for some ccTLDs, such as `.de`: instead, the newer IDNA 2008 standard should be used. That standard is implemented in the PyPI package `idna`.

If we're going to support IDNA 2008, I would like it to not be optional: it leads to a fairly substantial and difficult to debug change, so it should be supported fully. To do that and keep in touch with Kenneth's wishes regarding dependencies, we'd have to vendor it.

I'd like contributors and packagers, particularly @eriol, @ralphbean, and @sigmavirus24 to weigh in with their thoughts here. What are your thoughts here? I'd also, if possible, like to hear from @kjd: while `idna` is licensed under a BSD-like license that should be broadly compatible with our own Apache 2.0 and so should present no legal blockers to vendoring, I'd still rather do it with the original author's permission than without it.
",Lukasa,ralphbean
3616,2016-10-12 16:21:01,"As has been noted on the [Python Security SIG mailing list](https://mail.python.org/pipermail/security-sig/2016-October/000122.html), Python currently only includes the deprecated IDNA 2003 codec in the standard library. This is problematic, because IDNA 2003 is forbidden for some ccTLDs, such as `.de`: instead, the newer IDNA 2008 standard should be used. That standard is implemented in the PyPI package `idna`.

If we're going to support IDNA 2008, I would like it to not be optional: it leads to a fairly substantial and difficult to debug change, so it should be supported fully. To do that and keep in touch with Kenneth's wishes regarding dependencies, we'd have to vendor it.

I'd like contributors and packagers, particularly @eriol, @ralphbean, and @sigmavirus24 to weigh in with their thoughts here. What are your thoughts here? I'd also, if possible, like to hear from @kjd: while `idna` is licensed under a BSD-like license that should be broadly compatible with our own Apache 2.0 and so should present no legal blockers to vendoring, I'd still rather do it with the original author's permission than without it.
",Lukasa,eriol
3616,2016-10-12 16:21:01,"As has been noted on the [Python Security SIG mailing list](https://mail.python.org/pipermail/security-sig/2016-October/000122.html), Python currently only includes the deprecated IDNA 2003 codec in the standard library. This is problematic, because IDNA 2003 is forbidden for some ccTLDs, such as `.de`: instead, the newer IDNA 2008 standard should be used. That standard is implemented in the PyPI package `idna`.

If we're going to support IDNA 2008, I would like it to not be optional: it leads to a fairly substantial and difficult to debug change, so it should be supported fully. To do that and keep in touch with Kenneth's wishes regarding dependencies, we'd have to vendor it.

I'd like contributors and packagers, particularly @eriol, @ralphbean, and @sigmavirus24 to weigh in with their thoughts here. What are your thoughts here? I'd also, if possible, like to hear from @kjd: while `idna` is licensed under a BSD-like license that should be broadly compatible with our own Apache 2.0 and so should present no legal blockers to vendoring, I'd still rather do it with the original author's permission than without it.
",Lukasa,sigmavirus24
3616,2016-10-12 16:21:01,"As has been noted on the [Python Security SIG mailing list](https://mail.python.org/pipermail/security-sig/2016-October/000122.html), Python currently only includes the deprecated IDNA 2003 codec in the standard library. This is problematic, because IDNA 2003 is forbidden for some ccTLDs, such as `.de`: instead, the newer IDNA 2008 standard should be used. That standard is implemented in the PyPI package `idna`.

If we're going to support IDNA 2008, I would like it to not be optional: it leads to a fairly substantial and difficult to debug change, so it should be supported fully. To do that and keep in touch with Kenneth's wishes regarding dependencies, we'd have to vendor it.

I'd like contributors and packagers, particularly @eriol, @ralphbean, and @sigmavirus24 to weigh in with their thoughts here. What are your thoughts here? I'd also, if possible, like to hear from @kjd: while `idna` is licensed under a BSD-like license that should be broadly compatible with our own Apache 2.0 and so should present no legal blockers to vendoring, I'd still rather do it with the original author's permission than without it.
",Lukasa,kjd
3616,2016-10-12 21:47:35,"Hey, @ralphbean has asked me to take over being the point of contact for Fedora packaging interactions.

I'm fine with this and I'm also fine with it being non-optional.
",jeremycline,ralphbean
3595,2016-09-23 15:51:56,"Right now you can provide a dictionary to the `cookies` param almost everywhere in Requests and it will be converted into a RequestsCookieJar (aka ""just work""). The one place this isn't happening is with Session, which will accept a dictionary without complaint but then fail when you try to send a PreparedRequest. 

I discussed wanting to make this part of the API a bit more predictable with @Lukasa and this was a suggested fix for the problem. I did fair amount of testing outside of the included test and I don't believe this will adversely affect functionality. The one case I found is where someone is providing a CookieJar-like object that doesn't inherit from `cookielib.CookieJar`, and doesn't have an `__iter__` method. This seems like an unlikely case and wasn't _really_ supported before, so I wouldn't consider this breaking.
",nateprewitt,Lukasa
3595,2016-09-30 13:47:26,"@sigmavirus24, if we're planning on enforcing cookies requiring domains with your work in #2714, then I'd agree this PR may not be needed. If we don't intend to deprecate the use of dict->cookies though, I do think this should work the same everywhere. The documentation is already a bit opaque on this, and my original suggestion to @Lukasa was to update it to reflect how updating cookies currently works with a Session. If we want to make a note of this change in the documentation, I think simply adding ""As of 2.12"" at the beginning should be sufficient.

I can make the changes for `self.cookies` to `self._cookies` if there's a consensus to move forward with this. I'm not sure I'm clear on the answer to that at present though. I'll wait for a go ahead from you and @Lukasa before I make any further changes.
",nateprewitt,Lukasa
3595,2016-09-30 13:47:26,"@sigmavirus24, if we're planning on enforcing cookies requiring domains with your work in #2714, then I'd agree this PR may not be needed. If we don't intend to deprecate the use of dict->cookies though, I do think this should work the same everywhere. The documentation is already a bit opaque on this, and my original suggestion to @Lukasa was to update it to reflect how updating cookies currently works with a Session. If we want to make a note of this change in the documentation, I think simply adding ""As of 2.12"" at the beginning should be sufficient.

I can make the changes for `self.cookies` to `self._cookies` if there's a consensus to move forward with this. I'm not sure I'm clear on the answer to that at present though. I'll wait for a go ahead from you and @Lukasa before I make any further changes.
",nateprewitt,sigmavirus24
3595,2016-10-17 06:34:43,"@sigmavirus24 @Lukasa, if we don't want to address this issue by allowing dictionaries, perhaps we should at least raise an exception when someone tries to assign one? I'm not particularly keen on this approach, but the current point of failure doesn't generate a helpful error.
",nateprewitt,Lukasa
3595,2016-10-17 06:34:43,"@sigmavirus24 @Lukasa, if we don't want to address this issue by allowing dictionaries, perhaps we should at least raise an exception when someone tries to assign one? I'm not particularly keen on this approach, but the current point of failure doesn't generate a helpful error.
",nateprewitt,sigmavirus24
3595,2016-10-17 07:22:11,"@nateprewitt I think @sigmavirus24 was tentatively in favour of this change, at least as a temporary stopping-off point before something better. He was just asking for a change in the descriptor protocol.
",Lukasa,sigmavirus24
3594,2016-09-20 16:24:26,"Merging change from #3576 into the proposed/3.0.0 branch as per @sigmavirus24's request.
",nateprewitt,sigmavirus24
3576,2016-09-17 09:04:26,"@sigmavirus24 Would you like to take a look at this?
",Lukasa,sigmavirus24
3563,2016-09-10 17:25:14,"@Lukasa, here's a bit of a hiccup I'd appreciate some input on. Currently all data reads in Requests end up using `itercontent`. 

Currently, regardless of whether or not the content is streamed, we end up initially running it through the `generate` function inside `itercontent`. This function assumes we're using Transfer-Encoding: chunked if it's called, and raises `ChunkedEncodingError` wrapping urllib3's `ProtocolError`. This is clearly incorrect because we're not chunked for responses with Content-Length but I'm not sure how best to catch this. 

We could parse the exception string for 'IncompleteRead', but that seems clunky to me. We could also add a conditional inside to raise `ProtocolError` on `stream=False` and `ChunkedEncodingError` on `stream=True` but that may be subtle enough to confuse since it's the same base error. The last option I can think of is removing `ChunkedEncodingError` completely and just raising the unmodified `ProtocolError` instead. Thoughts?
",nateprewitt,Lukasa
3554,2016-09-05 18:51:18,"This implements the proposed solution to #3538 by falling back from Unicode to ISO-8859-1 for raw reason decoding.

This is a relatively trivial fix, and I wasn't sure if you wanted to waste bandwidth fixing, @mitsuhiko. I had a brief chat with @Lukasa who said to toss this up, but I'll gladly drop it in favor of #3538, if you had plans to update.
",nateprewitt,mitsuhiko
3554,2016-09-05 18:51:18,"This implements the proposed solution to #3538 by falling back from Unicode to ISO-8859-1 for raw reason decoding.

This is a relatively trivial fix, and I wasn't sure if you wanted to waste bandwidth fixing, @mitsuhiko. I had a brief chat with @Lukasa who said to toss this up, but I'll gladly drop it in favor of #3538, if you had plans to update.
",nateprewitt,Lukasa
3554,2016-09-05 20:19:27,"@sigmavirus24 are you happy with this?
",Lukasa,sigmavirus24
3552,2016-09-05 16:53:35,"Glad I could help and thanks @Lukasa 
",Trodis,Lukasa
3548,2016-09-01 09:16:31,"@shazow @Lukasa @mcescalante @borbamartin

Hi guys;

I just tried to extract data by using the URL of the webservices; moreover, I used the requests package however it returned the error ""TypeError: **str** returned non-string (type SysCallError)"" according to that; I just read the section for the issue at GitHub and upgraded the requests package. However, it returned another error. Before updating the code worked for an hour and stopped and gave the error as I shared ""TypeError: **str** returned non-string (type SysCallError)"" however right now it does not work; moreover jit just returns the traceback:

Traceback (most recent call last):
  File ""C:\Users\may\Desktop\Documents\BOUN-CSE\Master Thesis\Project Code Files\pubchem\PubchemMapping.py"", line 35, in <module>
    activities = new_client.activity.filter(target_chembl_id__in=[target['target_chembl_id'] for target in targets])
  File ""C:\Python27\lib\site-packages\chembl_webresource_client\query_set.py"", line 114, in next
    self.chunk = self.query.get_page()
  File ""C:\Python27\lib\site-packages\chembl_webresource_client\url_query.py"", line 390, in get_page
    res = session.post(self.base_url + '.' + self.frmt, data=data, timeout=self.timeout)
  File ""C:\Python27\lib\site-packages\requests\sessions.py"", line 522, in post
    return self.request('POST', url, data=data, json=json, *_kwargs)
  File ""C:\Python27\lib\site-packages\requests_cache\core.py"", line 126, in request
    *_kwargs
  File ""C:\Python27\lib\site-packages\requests\sessions.py"", line 475, in request
    resp = self.send(prep, **send_kwargs)
  File ""C:\Python27\lib\site-packages\requests_cache\core.py"", line 97, in send
    response, timestamp = self.cache.get_response_and_time(cache_key)
  File ""C:\Python27\lib\site-packages\requests_cache\backends\base.py"", line 70, in get_response_and_time
    if key not in self.responses:
  File ""C:\Python27\lib_abcoll.py"", line 388, in **contains**
    self[key]
  File ""C:\Python27\lib\site-packages\requests_cache\backends\storage\dbdict.py"", line 163, in **getitem**
    return pickle.loads(bytes(super(DbPickleDict, self).**getitem**(key)))
  File ""C:\Python27\lib\copy_reg.py"", line 50, in _reconstructor
    obj = base.__new__(cls, state)
TypeError: ('dict.**new**(HTTPHeaderDict): HTTPHeaderDict is not a subtype of dict', <function _reconstructor at 0x03039570>, (<class 'requests.packages.urllib3._collections.HTTPHeaderDict'>, <type 'dict'>, {'date': ('date', 'Thu, 01 Sep 2016 08:24:03 GMT'), 'transfer-encoding': ('transfer-encoding', 'chunked'), 'vary': ('vary', 'Accept'), 'server': ('server', 'gunicorn/19.1.1'), 'x-chembl-retrieval-time': ('x-chembl-retrieval-time', '0.00829792022705'), 'connection': ('connection', 'Keep-Alive'), 'cache-control': ('cache-control', 's-maxage=30000000, max-age=30000000'), 'x-chembl-in-cache': ('x-chembl-in-cache', 'True'), 'content-type': ('content-type', 'application/json')}))

Can you help to solve the problem please ?

All the Best;
Aziz
",MehmetAzizYirik,shazow
3548,2016-09-01 09:16:31,"@shazow @Lukasa @mcescalante @borbamartin

Hi guys;

I just tried to extract data by using the URL of the webservices; moreover, I used the requests package however it returned the error ""TypeError: **str** returned non-string (type SysCallError)"" according to that; I just read the section for the issue at GitHub and upgraded the requests package. However, it returned another error. Before updating the code worked for an hour and stopped and gave the error as I shared ""TypeError: **str** returned non-string (type SysCallError)"" however right now it does not work; moreover jit just returns the traceback:

Traceback (most recent call last):
  File ""C:\Users\may\Desktop\Documents\BOUN-CSE\Master Thesis\Project Code Files\pubchem\PubchemMapping.py"", line 35, in <module>
    activities = new_client.activity.filter(target_chembl_id__in=[target['target_chembl_id'] for target in targets])
  File ""C:\Python27\lib\site-packages\chembl_webresource_client\query_set.py"", line 114, in next
    self.chunk = self.query.get_page()
  File ""C:\Python27\lib\site-packages\chembl_webresource_client\url_query.py"", line 390, in get_page
    res = session.post(self.base_url + '.' + self.frmt, data=data, timeout=self.timeout)
  File ""C:\Python27\lib\site-packages\requests\sessions.py"", line 522, in post
    return self.request('POST', url, data=data, json=json, *_kwargs)
  File ""C:\Python27\lib\site-packages\requests_cache\core.py"", line 126, in request
    *_kwargs
  File ""C:\Python27\lib\site-packages\requests\sessions.py"", line 475, in request
    resp = self.send(prep, **send_kwargs)
  File ""C:\Python27\lib\site-packages\requests_cache\core.py"", line 97, in send
    response, timestamp = self.cache.get_response_and_time(cache_key)
  File ""C:\Python27\lib\site-packages\requests_cache\backends\base.py"", line 70, in get_response_and_time
    if key not in self.responses:
  File ""C:\Python27\lib_abcoll.py"", line 388, in **contains**
    self[key]
  File ""C:\Python27\lib\site-packages\requests_cache\backends\storage\dbdict.py"", line 163, in **getitem**
    return pickle.loads(bytes(super(DbPickleDict, self).**getitem**(key)))
  File ""C:\Python27\lib\copy_reg.py"", line 50, in _reconstructor
    obj = base.__new__(cls, state)
TypeError: ('dict.**new**(HTTPHeaderDict): HTTPHeaderDict is not a subtype of dict', <function _reconstructor at 0x03039570>, (<class 'requests.packages.urllib3._collections.HTTPHeaderDict'>, <type 'dict'>, {'date': ('date', 'Thu, 01 Sep 2016 08:24:03 GMT'), 'transfer-encoding': ('transfer-encoding', 'chunked'), 'vary': ('vary', 'Accept'), 'server': ('server', 'gunicorn/19.1.1'), 'x-chembl-retrieval-time': ('x-chembl-retrieval-time', '0.00829792022705'), 'connection': ('connection', 'Keep-Alive'), 'cache-control': ('cache-control', 's-maxage=30000000, max-age=30000000'), 'x-chembl-in-cache': ('x-chembl-in-cache', 'True'), 'content-type': ('content-type', 'application/json')}))

Can you help to solve the problem please ?

All the Best;
Aziz
",MehmetAzizYirik,Lukasa
3548,2016-09-01 09:16:31,"@shazow @Lukasa @mcescalante @borbamartin

Hi guys;

I just tried to extract data by using the URL of the webservices; moreover, I used the requests package however it returned the error ""TypeError: **str** returned non-string (type SysCallError)"" according to that; I just read the section for the issue at GitHub and upgraded the requests package. However, it returned another error. Before updating the code worked for an hour and stopped and gave the error as I shared ""TypeError: **str** returned non-string (type SysCallError)"" however right now it does not work; moreover jit just returns the traceback:

Traceback (most recent call last):
  File ""C:\Users\may\Desktop\Documents\BOUN-CSE\Master Thesis\Project Code Files\pubchem\PubchemMapping.py"", line 35, in <module>
    activities = new_client.activity.filter(target_chembl_id__in=[target['target_chembl_id'] for target in targets])
  File ""C:\Python27\lib\site-packages\chembl_webresource_client\query_set.py"", line 114, in next
    self.chunk = self.query.get_page()
  File ""C:\Python27\lib\site-packages\chembl_webresource_client\url_query.py"", line 390, in get_page
    res = session.post(self.base_url + '.' + self.frmt, data=data, timeout=self.timeout)
  File ""C:\Python27\lib\site-packages\requests\sessions.py"", line 522, in post
    return self.request('POST', url, data=data, json=json, *_kwargs)
  File ""C:\Python27\lib\site-packages\requests_cache\core.py"", line 126, in request
    *_kwargs
  File ""C:\Python27\lib\site-packages\requests\sessions.py"", line 475, in request
    resp = self.send(prep, **send_kwargs)
  File ""C:\Python27\lib\site-packages\requests_cache\core.py"", line 97, in send
    response, timestamp = self.cache.get_response_and_time(cache_key)
  File ""C:\Python27\lib\site-packages\requests_cache\backends\base.py"", line 70, in get_response_and_time
    if key not in self.responses:
  File ""C:\Python27\lib_abcoll.py"", line 388, in **contains**
    self[key]
  File ""C:\Python27\lib\site-packages\requests_cache\backends\storage\dbdict.py"", line 163, in **getitem**
    return pickle.loads(bytes(super(DbPickleDict, self).**getitem**(key)))
  File ""C:\Python27\lib\copy_reg.py"", line 50, in _reconstructor
    obj = base.__new__(cls, state)
TypeError: ('dict.**new**(HTTPHeaderDict): HTTPHeaderDict is not a subtype of dict', <function _reconstructor at 0x03039570>, (<class 'requests.packages.urllib3._collections.HTTPHeaderDict'>, <type 'dict'>, {'date': ('date', 'Thu, 01 Sep 2016 08:24:03 GMT'), 'transfer-encoding': ('transfer-encoding', 'chunked'), 'vary': ('vary', 'Accept'), 'server': ('server', 'gunicorn/19.1.1'), 'x-chembl-retrieval-time': ('x-chembl-retrieval-time', '0.00829792022705'), 'connection': ('connection', 'Keep-Alive'), 'cache-control': ('cache-control', 's-maxage=30000000, max-age=30000000'), 'x-chembl-in-cache': ('x-chembl-in-cache', 'True'), 'content-type': ('content-type', 'application/json')}))

Can you help to solve the problem please ?

All the Best;
Aziz
",MehmetAzizYirik,borbamartin
3548,2016-09-01 09:16:31,"@shazow @Lukasa @mcescalante @borbamartin

Hi guys;

I just tried to extract data by using the URL of the webservices; moreover, I used the requests package however it returned the error ""TypeError: **str** returned non-string (type SysCallError)"" according to that; I just read the section for the issue at GitHub and upgraded the requests package. However, it returned another error. Before updating the code worked for an hour and stopped and gave the error as I shared ""TypeError: **str** returned non-string (type SysCallError)"" however right now it does not work; moreover jit just returns the traceback:

Traceback (most recent call last):
  File ""C:\Users\may\Desktop\Documents\BOUN-CSE\Master Thesis\Project Code Files\pubchem\PubchemMapping.py"", line 35, in <module>
    activities = new_client.activity.filter(target_chembl_id__in=[target['target_chembl_id'] for target in targets])
  File ""C:\Python27\lib\site-packages\chembl_webresource_client\query_set.py"", line 114, in next
    self.chunk = self.query.get_page()
  File ""C:\Python27\lib\site-packages\chembl_webresource_client\url_query.py"", line 390, in get_page
    res = session.post(self.base_url + '.' + self.frmt, data=data, timeout=self.timeout)
  File ""C:\Python27\lib\site-packages\requests\sessions.py"", line 522, in post
    return self.request('POST', url, data=data, json=json, *_kwargs)
  File ""C:\Python27\lib\site-packages\requests_cache\core.py"", line 126, in request
    *_kwargs
  File ""C:\Python27\lib\site-packages\requests\sessions.py"", line 475, in request
    resp = self.send(prep, **send_kwargs)
  File ""C:\Python27\lib\site-packages\requests_cache\core.py"", line 97, in send
    response, timestamp = self.cache.get_response_and_time(cache_key)
  File ""C:\Python27\lib\site-packages\requests_cache\backends\base.py"", line 70, in get_response_and_time
    if key not in self.responses:
  File ""C:\Python27\lib_abcoll.py"", line 388, in **contains**
    self[key]
  File ""C:\Python27\lib\site-packages\requests_cache\backends\storage\dbdict.py"", line 163, in **getitem**
    return pickle.loads(bytes(super(DbPickleDict, self).**getitem**(key)))
  File ""C:\Python27\lib\copy_reg.py"", line 50, in _reconstructor
    obj = base.__new__(cls, state)
TypeError: ('dict.**new**(HTTPHeaderDict): HTTPHeaderDict is not a subtype of dict', <function _reconstructor at 0x03039570>, (<class 'requests.packages.urllib3._collections.HTTPHeaderDict'>, <type 'dict'>, {'date': ('date', 'Thu, 01 Sep 2016 08:24:03 GMT'), 'transfer-encoding': ('transfer-encoding', 'chunked'), 'vary': ('vary', 'Accept'), 'server': ('server', 'gunicorn/19.1.1'), 'x-chembl-retrieval-time': ('x-chembl-retrieval-time', '0.00829792022705'), 'connection': ('connection', 'Keep-Alive'), 'cache-control': ('cache-control', 's-maxage=30000000, max-age=30000000'), 'x-chembl-in-cache': ('x-chembl-in-cache', 'True'), 'content-type': ('content-type', 'application/json')}))

Can you help to solve the problem please ?

All the Best;
Aziz
",MehmetAzizYirik,mcescalante
3545,2016-09-20 19:05:55,"@Lukasa whenever you've got a chance, could I get your thoughts on this?
",nateprewitt,Lukasa
3543,2016-08-27 11:36:36,"@kennethreitz This one is for you, my friend.
",Lukasa,kennethreitz
3539,2016-08-26 12:32:36,"@Lukasa I was certain this was described elsewhere, am I wrong?
",sigmavirus24,Lukasa
3535,2016-08-24 18:04:53,"This is a follow up on @jseabold's work in #3339. These last minor changes should fix the issues with return values of `seek` between Python 2 and Python 3.
",nateprewitt,jseabold
3535,2016-08-26 01:20:32,"Ok, cool, I like this. @sigmavirus24?
",Lukasa,sigmavirus24
3535,2016-09-01 22:31:59,"@sigmavirus24, wanted to ping in case this fell off the queue. No rush, just checking in :)
",nateprewitt,sigmavirus24
3535,2016-09-05 15:45:58,"I don't think I've got anything else, so I think we're good to merge from my end. Let me know if you have anything else @lukasa.

We can probably start discussing #3545 too.
",nateprewitt,lukasa
3527,2016-08-21 22:21:13,"Thanks for this @michelsch92!

Some notes. Firstly, I'm afraid to say that #3518 was already resolved by #3527. I'm sorry about that, the timing was just a bit unlucky.

Secondly, the response helpers, I think we don't want to put them into the core requests library itself. Generally speaking we try to keep the surface area of the API down, because the more methods and properties that there are on the object the harder it gets to understand. However, @sigmavirus24 may be interested in having those properties in a headers helper object in the requests-toolbelt.

Unfortunately, that means this patch won't be accepted at this time. This has nothing to do with the quality of your work: altogether it was high quality work, and we'd have been pleased to accept it. Please do keep looking around for ways to contribute, because you're most welcome here!
",Lukasa,sigmavirus24
3526,2016-08-21 17:02:52,"This seems reasonable to me. @Lukasa thoughts?
",sigmavirus24,Lukasa
3516,2016-08-17 03:28:33,"Hi list
i got problem Missing dependencies for SOCKS support. with requests module version 2.11, 

but with requests module version 2.10 the requests module works fine

my enviroment :
declare -x VIRTUAL_ENV=""/home/hadn/python3""
declare -x VTE_VERSION=""3803""
declare -x WINDOWID=""33554439""
declare -x WINDOWPATH=""1""
declare -x XAUTHORITY=""/run/gdm/auth-for-hadn-0O4FTR/database""
declare -x XDG_CURRENT_DESKTOP=""GNOME-Classic:GNOME""
declare -x XDG_MENU_PREFIX=""gnome-""
declare -x XDG_RUNTIME_DIR=""/run/user/1000""
declare -x XDG_SEAT=""seat0""
declare -x XDG_SESSION_DESKTOP=""gnome-classic""
declare -x XDG_SESSION_ID=""2""
declare -x XDG_VTNR=""1""
declare -x XMODIFIERS=""@im=ibus""
declare -x _system_arch=""x86_64""
declare -x _system_name=""CentOS""
declare -x _system_type=""Linux""
declare -x _system_version=""7""
declare -x all_proxy=""socks://proxy.hcm.fpt.vn:80/""
declare -x ftp_proxy=""http://proxy.hcm.fpt.vn:80/""
declare -x http_proxy=""http://proxy.hcm.fpt.vn:80/""
declare -x https_proxy=""http://proxy.hcm.fpt.vn:80/""

(python3)[hadn@rad-hadn4 ~]$ /home/hadn/Laravel/learning_bootstrap_v4/film/film_le.sh
Traceback (most recent call last):
  File ""/home/hadn/Laravel/learning_bootstrap_v4/film/film_le_crawler_page_1.py"", line 24, in <module>
    html_content = session.get(url)
  File ""/home/hadn/python3/lib/python3.4/site-packages/requests/sessions.py"", line 483, in get
    return self.request('GET', url, *_kwargs)
  File ""/home/hadn/python3/lib/python3.4/site-packages/requests/sessions.py"", line 471, in request
    resp = self.send(prep, *_send_kwargs)
  File ""/home/hadn/python3/lib/python3.4/site-packages/requests/sessions.py"", line 581, in send
    r = adapter.send(request, *_kwargs)
  File ""/home/hadn/python3/lib/python3.4/site-packages/requests/adapters.py"", line 384, in send
    conn = self.get_connection(request.url, proxies)
  File ""/home/hadn/python3/lib/python3.4/site-packages/requests/adapters.py"", line 287, in get_connection
    proxy_manager = self.proxy_manager_for(proxy)
  File ""/home/hadn/python3/lib/python3.4/site-packages/requests/adapters.py"", line 183, in proxy_manager_for
    *_proxy_kwargs
  File ""/home/hadn/python3/lib/python3.4/site-packages/requests/adapters.py"", line 43, in SOCKSProxyManager
    raise InvalidSchema(""Missing dependencies for SOCKS support."")
requests.exceptions.InvalidSchema: Missing dependencies for SOCKS support.
",alochym01,im
3516,2016-08-17 10:23:22,"@alochym01 If you don't want it, why is it set in your environment? Regardless, if you use a session with `trust_env=False` that problem will no longer occur, though you'll lose your environment HTTP proxy. 

However, there does seem like there is a bug here: requests will prefer `all_proxy` to a scheme-specific proxy. I think that's a bad idea. @sigmavirus24, should we re-order that?
",Lukasa,sigmavirus24
3514,2016-08-15 09:08:00,"@kennethreitz, this is all we need from a boring administrative perspective. If you merge this PR, you only need to do three things to release:
1. Update the version number to 2.11.1.
2. Put the date you do this in the changelog instead of the XX.
3. Push the release.

It'd be good to get this release out this week: it should put the nastiest bugs of 2.11 behind us and get people back to working again. =)
",Lukasa,kennethreitz
3514,2016-08-15 10:20:06,"@kennethreitz let's :shipit:
",sigmavirus24,kennethreitz
3513,2016-08-14 20:08:58,"Various doc updates for clarity. Some of these may not match @kennethreitz's aesthetic sensibilities, which may require an inversion of the proposed change. I was mainly trying to make things consistent, input on preferences would be greatly appreciated.

Some notes:
- I think at least the faq.rst changes _need_ to be merged because there is still live documentation stating that Requests supports 3.1 and 3.2. I can open a separate PR if we want those changes quicker.
- What was ScraperWiki, is now found at quickcode.io and appears to either be a paid or account-walled service. Wayback Machine is showing the site as a free service when the documentation was created, so I'm not sure if Kenneth still wants this included. At the very least, the URL should be updated.
- ~~Does Requests support pypy 2.2 officially? faq.rst says yes, but it's other versions were wrong.~~
- While I realize it's common to use `class.method` as a pattern when describing code, I felt like `r.json` is ambiguous when referenced next to `r.text`. Calling exactly `r.json` won't function as described in the provided `ValueError` example. Explicitly defining the call as `r.json()` reduces the chance of misinterpretation.
- api.rst has a pared down set of Exceptions and classes. I assume this is to keep extraneous information from bloating the page to a point of being unhelpful. I do think it may be worth including `BaseAdapter` for easy reference since anyone looking into a custom _non-HTTP_ Transport Adapter will likely need to start there.
",nateprewitt,kennethreitz
3507,2016-08-11 23:54:54,"Hey @ppolewicz, I believe you can find answers addressing this in #3479, primarily @Lukasa's comment [here](https://github.com/kennethreitz/requests/issues/3479#issuecomment-238633044).

pip is currently unable to filter out libraries based on python version, but it was also [noted](https://github.com/kennethreitz/requests/issues/3479#issuecomment-238640627) in #3479 that this may be available in the _distant_ future. I'm not positive on when 3.2 was officially deprecated for Requests (it was before 2.11.0) but it was removed from the trove list in setup.py in May 2013. Unfortunately, I don't believe there's any intent to reintroduce support for 3.2, per @sigmavirus24 [comments](https://github.com/kennethreitz/requests/issues/3479#issuecomment-238640113).
",nateprewitt,Lukasa
3507,2016-08-11 23:54:54,"Hey @ppolewicz, I believe you can find answers addressing this in #3479, primarily @Lukasa's comment [here](https://github.com/kennethreitz/requests/issues/3479#issuecomment-238633044).

pip is currently unable to filter out libraries based on python version, but it was also [noted](https://github.com/kennethreitz/requests/issues/3479#issuecomment-238640627) in #3479 that this may be available in the _distant_ future. I'm not positive on when 3.2 was officially deprecated for Requests (it was before 2.11.0) but it was removed from the trove list in setup.py in May 2013. Unfortunately, I don't believe there's any intent to reintroduce support for 3.2, per @sigmavirus24 [comments](https://github.com/kennethreitz/requests/issues/3479#issuecomment-238640113).
",nateprewitt,sigmavirus24
3507,2016-08-12 00:36:36,"If @Lukasa wants I'd be happy to create a PR for thisfor 2.11.1
",TetraEtc,Lukasa
3507,2016-08-12 11:23:07,"> I understand that 3.2 is no longer supported by requests, but pip being broken as it is with the version mapping causes trouble for some people.

I'm sorry, I don't quite understand what you are referring to @ppolewicz. 

> I think a minor change of 2 LOC to prevent this would be acceptable?

@ppolewicz except that minor change implies Python 3.2 support. Python 3.2 as a product itself is unsupported. The number of users it has according to PyPI is incredibly small. I don't see a whole lot of value in changing those two lines, but @Lukasa and @kennethreitz may disagree. I'm not against it, I just don't agree that we should be implicitly supporting an unsupported version that is mostly not used (regardless of how easy it is).
",sigmavirus24,kennethreitz
3493,2016-08-10 17:22:28,"@saveman71, I only bring up the optimization because @sigmavirus24 has [brought it up before](https://github.com/kennethreitz/requests/pull/3362#discussion_r68398047). If I'm misunderstanding his original intent though, I vote we completely remove the declaration, in favor of `prepared_request.headers`. `headers` currently doesn't provide a lot of use in `resolve_redirects`.

Edit: Clarity and Link
",nateprewitt,sigmavirus24
3493,2016-08-11 17:03:14,"Alright, fab, I'm happy with this. @sigmavirus24 you ok with us merging this for 2.11.1?
",Lukasa,sigmavirus24
3493,2016-08-12 08:02:06,"Alright, all is well. @sigmavirus24, you want this in 2.11.1?
",Lukasa,sigmavirus24
3489,2016-08-09 18:17:19,"This is a continuation of @keyan's work in #2757 and addresses issue #1558 regarding pickling Request objects (specifically `PreparedRequest` objects in this PR).

These tests ensure that a standard `PreparedRequest`, a `PreparedRequest` with a file object as the body, and a `PreparedRequest` with hooks defined outside of the `locals` scope will all pickle properly.

This also works for data passed to the `json` parameter and I can add my test for that as well if it's deemed helpful.

Hooks defined inside of a method (such as in the [original test](https://github.com/kennethreitz/requests/pull/2757/files#diff-56c2d754173a4a158ce8f445834c8fe8R843) by @keyan) will fail because you can't pickle local objects. Is this a use case that needs to be handled?",nateprewitt,keyan
3489,2016-11-09 22:10:00,"@Lukasa any thoughts here? We still have edge cases where if the `data` param is a generator or file-like object, it can't be pickled. Is there any utility in adding this subset of tests, or should #1558 be closed as can't fix?
",nateprewitt,Lukasa
3486,2016-08-09 13:51:45,"I'm good with this. How about you @Lukasa? (To be honest, I would have rathered let GitHub generate the revert than have it done artisanally.)
",sigmavirus24,Lukasa
3481,2016-08-09 08:37:36,"Aha, got it.

`decode_unicode=True` was enhanced in the last release to perform the same auto-detection of encoding as `text` does for Requests. Unfortunately, that may involve a call to `apparent_encoding`, which streams in the entire request body and attempts to determine the encoding.

This was done as part of #3362 in an attempt to resolve #3359.

Frankly, this leads me to suspect that perhaps we should revert that change. If `apparent_encoding` is going to read the whole response body then there is genuinely no point in falling back to `apparent_encoding` as part of `iter_*`: any reason to use the method has been totally lost.

I wonder if we should instead revert the fix in #3362, and then do one of the following three things:
1. Use UTF-8 as an unconditional fallback if no specific encoding is declared.
2. Raise an exception if no specific encoding is declared.
3. Allow `decode_unicode` to take an encoding name which is used as a fallback.

@kennethreitz @sigmavirus24?
",Lukasa,kennethreitz
3481,2016-08-09 08:37:36,"Aha, got it.

`decode_unicode=True` was enhanced in the last release to perform the same auto-detection of encoding as `text` does for Requests. Unfortunately, that may involve a call to `apparent_encoding`, which streams in the entire request body and attempts to determine the encoding.

This was done as part of #3362 in an attempt to resolve #3359.

Frankly, this leads me to suspect that perhaps we should revert that change. If `apparent_encoding` is going to read the whole response body then there is genuinely no point in falling back to `apparent_encoding` as part of `iter_*`: any reason to use the method has been totally lost.

I wonder if we should instead revert the fix in #3362, and then do one of the following three things:
1. Use UTF-8 as an unconditional fallback if no specific encoding is declared.
2. Raise an exception if no specific encoding is declared.
3. Allow `decode_unicode` to take an encoding name which is used as a fallback.

@kennethreitz @sigmavirus24?
",Lukasa,sigmavirus24
3479,2016-08-09 17:46:55,"@huseyinyilmaz That is not possible, I'm afraid: you'd need to ask @dstufft to provide tools to do that. It's a nice idea, certainly, although it requires much more formalisation of the trove classifiers system that currently exists.
",Lukasa,dstufft
3478,2016-08-08 22:47:04,"We have not supported Python 3.2 for ... over a year? I don't think we'll accept this. @kennethreitz, thoughts?
",sigmavirus24,kennethreitz
3476,2016-08-08 15:40:17,"@cidadao You'll need to more explicitly state what your actual problem is. Let me address why.

Firstly, you say you want to POST your JSON as UTF-8 encoded data. Why is this needed? By default, the Python JSON library will automatically escape all non-ASCII unicode code points. For example:



This code produces the same result on Python 2.7, by the way. I mention this because it doesn't matter if this is encoded in UTF-8 or Latin-1, the exact output will be identical as it uses _only ASCII characters_.

Your next comment is about apparent problems with data encoding. I cannot reproduce this problem:



This is where I use the same data dictionary as above, containing a whole slew of Chinese characters. Wireshark reveals the posted body as being the byte sequence `7b 22 6b 65 79 22 3a 20 22 e6 81 92 e5 86 99 e4 be a1 e6 99 82 e7 a9 b6 e8 a8 8e e6 98 ad e5 90 8d e4 bc 9a e8 a8 bc e4 bb 95 e7 8e b2 e4 ba 88 e9 82 b8 e5 ba 81 e5 ad 90 e7 ad 89 e5 9b b3 e8 80 85 22 7d`, which decodes via UTF-8 to:



So I think I need more information. What does your data look like? What do your headers look like? What Python and Requests version are you using?
",Lukasa,cidadao
3475,2016-08-08 12:39:59,"@kennethreitz, this is all we need from a boring administrative perspective. If you merge this PR, you only need to do three things to release:
1. Update the version number to 2.11.
2. Put the date you do this in the changelog instead of the XX.
3. Push the release.

@sigmavirus24 I'm inclined to want to let all currently open PRs remain open rather than try to rush merge any: they can always wait for a 2.11.1 or 2.12 without any risk.
",Lukasa,kennethreitz
3475,2016-08-08 12:39:59,"@kennethreitz, this is all we need from a boring administrative perspective. If you merge this PR, you only need to do three things to release:
1. Update the version number to 2.11.
2. Put the date you do this in the changelog instead of the XX.
3. Push the release.

@sigmavirus24 I'm inclined to want to let all currently open PRs remain open rather than try to rush merge any: they can always wait for a 2.11.1 or 2.12 without any risk.
",Lukasa,sigmavirus24
3463,2016-07-31 07:36:07,"So I'm generally ok with this, but I'd like @sigmavirus24 and @kennethreitz to have a look.
",Lukasa,kennethreitz
3463,2016-07-31 07:36:07,"So I'm generally ok with this, but I'd like @sigmavirus24 and @kennethreitz to have a look.
",Lukasa,sigmavirus24
3445,2016-07-27 08:55:42,"This leads to confusion, as demonstrated by #3201 (ping @thanatos), but it's not a documentation issue, it's a design issue. Connections and sessions are unrelated: a connection can be used by multiple sessions, and a session can span multiple connections; thus the library shouldn't tie them together.
",Changaco,thanatos
3444,2016-07-27 06:05:09,"Thanks for this! 

It's my view that this is overkill: generally speaking, users that require this function can implement it themselves, and the majority of users don't need it. But I'll leave that decision up to @kennethreitz.
",Lukasa,kennethreitz
3444,2016-07-27 13:47:42,"@Lukasa further, in v1.0 didn't @kennethreitz completely tear out what logging requests did provide because people were constantly trying to add tons more logging on top of what we provided? I feel like were were to accept this we'd be in the same situation.

Further, what's being logged here is a very clear overload of information for any logging that I would view as reasonable for requests. This seems to be working very well in Café as it is and I see little reason to include this in Requests.

I appreciate your offer of the code @seemethere, but I'm very strongly against this. If you want review on the code anyway, I'm happy to provide that in a separate forum.
",sigmavirus24,kennethreitz
3439,2016-07-23 19:16:06,"To be clear, I assigned @kennethreitz to this because I think this is something he's going to care about and have opinions about. I'm fine with this but I'll leave it to him to merge.
",sigmavirus24,kennethreitz
3427,2016-07-20 16:39:57,"This is a combination of two passes over the code to help cleanup some of the coding style differences that have creeped in over time. It's mostly cosmetic but I think it'll help reduce inconsistencies going forward by having the code be an example for future contributors. Per @Lukasa's request, I'm going to break this up into two pull requests. One involving docstrings and the other involving pep8 fixes. I don't know that these changes are necessarily ""correct"", I simply chose what appeared to be the most common occurrence in the code. I'm sure @kennethreitz will have opinions on which choices are best.
",nateprewitt,kennethreitz
3427,2016-07-20 16:39:57,"This is a combination of two passes over the code to help cleanup some of the coding style differences that have creeped in over time. It's mostly cosmetic but I think it'll help reduce inconsistencies going forward by having the code be an example for future contributors. Per @Lukasa's request, I'm going to break this up into two pull requests. One involving docstrings and the other involving pep8 fixes. I don't know that these changes are necessarily ""correct"", I simply chose what appeared to be the most common occurrence in the code. I'm sure @kennethreitz will have opinions on which choices are best.
",nateprewitt,Lukasa
3426,2016-07-20 16:08:10,"This spawned off of [an article](https://hynek.me/articles/hasattr/) @Lukasa brought up in another PR. These changes should fix the pitfalls with using `hasattr` by converting their logic to use `getattr` instead.
",nateprewitt,Lukasa
3396,2016-07-12 14:38:03,"I want to do the following curl call in python with requests

**curl:**
`curl -b cookies -c cookies -X POST -d @auth 'https://api.a_website.com/auth'`

The above curl commanded worked.
So, I started migrating it into python 3.5.2.

**python**



However, I got the following error:

> b'{""response"":{""error_id"":""SYNTAX"",""error"":""JSON decode failed"",""error_description"":null,""service"":null,""method"":null,""error_code"":null,""dbg_info"":{""instance"":""65.bm-hbapi.prod.nym2"",""slave_hit"":false,""db"":""master"",""awesomesauce_cache_used"":false,""count_cache_used"":false,""uuid"":""c941819d5c860f9a"",""warnings"":[],""time"":39.22700881958,""start_microtime"":1468334115.9092,""version"":""1.16.707"",""slave_lag"":0,""output_term"":""not_found""}}}\n'

here is the auth file
`$ cat auth`

> {
>     ""auth"": {
>         ""username"" : ""a_username"",
>         ""password"" : ""a_password""
>     }
> }
",cchenship,auth
3390,2016-07-07 15:32:45,"👍 from me. @Lukasa ?
",sigmavirus24,Lukasa
3389,2016-07-07 10:14:24,"Yeah, this makes me somewhat uncomfortable. My _suspicion_ is that the LGPL doesn't allow what we're doing here, but IANAL. Fundamentally the decision here is with @kennethreitz. If it were me, though, I'd chat with Van Lindberg and see what he says, and then potentially take action to make chardet an optional dependency (like PyOpenSSL), rather than a bundled one.
",Lukasa,kennethreitz
3388,2016-07-07 13:19:38,"Cool, LGTM! :sparkles: @sigmavirus24?
",Lukasa,sigmavirus24
3388,2016-07-07 13:27:44,"While you're here @sigmavirus24 and @Lukasa, I threw together [another commit](https://github.com/nateprewitt/requests/commit/be31a90906deb5553c2e703fb05cf6964ee23ed5) related to this to encapsulate the type error thrown by `re` when non-strings are passed to `check_header_validity`. This is to make it clear that the behaviour is to be expected, but may be overkill with this documentation now. Any thoughts on if this would be worth opening? 
",nateprewitt,sigmavirus24
3387,2016-07-05 16:06:41,"This will fix issue #3386 with non-string/buffer header values, but there may be some more discussion to be had here. This is a passthrough for non-string/buffers which means all other datatypes will skip the regex check. I wasn't able to create the header split issue with most of the other standard datatypes I checked (lists, sets, dicts), so this _should_ be safe with the passthrough. 

I think an alternative, and possibly better solution is to cast everything that isn't a byte string as a `str`, whether that be standard, or `encode('latin-1')`. This will ensure the check still runs and doesn't modify the value in the actual `Requests.headers` value. This is probably closer to what @sigmavirus24 already did [here](https://github.com/kennethreitz/requests/pull/866/files#diff-5956087d5835a57d9ef6fff974f6fd9bR273).
",nateprewitt,sigmavirus24
3386,2016-07-05 15:07:50,"@wut0n9 This behavioural change is not in v2.10.0, it's in the current master branch.

However, that's a real bug: #3366 has regressed this. @nateprewitt, are you interested in trying to update with a fix for this?
",Lukasa,nateprewitt
3385,2016-07-05 09:47:23,"Cool, I'm happy with this as-is. I'd like @sigmavirus24 to ACK as well, if possible.
",Lukasa,sigmavirus24
3385,2016-07-05 13:45:08,"@sigmavirus24 @lucasa done, updated PR
",gugu,lucasa
3383,2016-07-01 23:42:41,"@Lukasa @sigmavirus24 

I tried running the tests on the proposed/3.0.0 branch, and found that they did not run. So I changed a couple of variable names to fix the obvious errors. If you prefer InvalidSchema and MissingSchema I can change those back, I changed the imports to match the names of the exceptions as they were defined.

After these changes, there is still one failing test. I'll try to take a look and see if I can debug it. But this was low hanging fruit.
",davidsoncasey,Lukasa
3383,2016-07-01 23:42:41,"@Lukasa @sigmavirus24 

I tried running the tests on the proposed/3.0.0 branch, and found that they did not run. So I changed a couple of variable names to fix the obvious errors. If you prefer InvalidSchema and MissingSchema I can change those back, I changed the imports to match the names of the exceptions as they were defined.

After these changes, there is still one failing test. I'll try to take a look and see if I can debug it. But this was low hanging fruit.
",davidsoncasey,sigmavirus24
3370,2016-07-01 13:24:27,"I'm happy with this, but would like @sigmavirus24 to take a look as well.
",Lukasa,sigmavirus24
3370,2016-07-01 21:58:06,"Ok, I'm still happy, but would still like @sigmavirus24 to weigh in. 
",Lukasa,sigmavirus24
3362,2016-06-28 18:05:05,"@Lukasa this looks good to me. What are your thoughts?
",sigmavirus24,Lukasa
3361,2016-06-23 06:06:56,"PreparedRequests should be suitable for accomplishing what you're trying to accomplish.

I see where you're coming from, and thank you for your contribution. However, in short, no. :)

@sigmavirus24 & @Lukasa are welcome to add additional thoughts, if desired.
",kennethreitz,Lukasa
3361,2016-06-23 06:06:56,"PreparedRequests should be suitable for accomplishing what you're trying to accomplish.

I see where you're coming from, and thank you for your contribution. However, in short, no. :)

@sigmavirus24 & @Lukasa are welcome to add additional thoughts, if desired.
",kennethreitz,sigmavirus24
3357,2016-06-21 16:19:04,"@sigmavirus24 mentioned that in auth.py, the magic method **ne** is technically not needed since != is implied from definition of **eq**. Would this be considered a useful update?
",ueg1990,sigmavirus24
3341,2016-06-17 12:52:06,"@ikus The rationale is that timeouts are not of-a-kind with the things that are set on `Sessions`. They represent a property of the transport layer and don't semantically apply to a _session_: how does one ""time out"" a session? In general they are a per-request property, and defaulting them is a property of the transport layer, which is where [Transport Adapters](http://docs.python-requests.org/en/master/user/advanced/#transport-adapters) are used to store configuration.

[This comment](https://github.com/kennethreitz/requests/issues/2011#issuecomment-64440818) provides an option for defaulting the value.
",Lukasa,ikus
3338,2016-11-08 06:19:48,"@Lukasa @nateprewitt I sort of forgot that this PR has been hanging out - is this still something we'd like to merge in? I can see about getting the conflicts fixed. Sorry I sort of lost momentum working on requests, I switched jobs and don't get time at work for open source projects any more.
",davidsoncasey,Lukasa
3338,2016-11-08 06:19:48,"@Lukasa @nateprewitt I sort of forgot that this PR has been hanging out - is this still something we'd like to merge in? I can see about getting the conflicts fixed. Sorry I sort of lost momentum working on requests, I switched jobs and don't get time at work for open source projects any more.
",davidsoncasey,nateprewitt
3338,2016-11-08 07:25:27,"@davidsoncasey, no worries, thanks for taking the time to check back in! 😀

 @Lukasa may have more on this but I think moving [these tests](https://github.com/davidsoncasey/requests/blob/60e0349ce265378b127ae51e1853533b57eaac38/tests/test_requests.py#L1241-L1270) into a separate PR against master will be the most immediate benefit. This will show that the issue is currently fixed on master and allow us to close out #3066.

The work you did for `prepare_body` and `prepare_content_length` simplifies a lot of the logic and would be great for proposed/3.0.0. We'll need to merge master into the proposed/3.0.0 branch in this repo and then have you rebase your commits on top of it. I did some local testing and got this patch merged and working relatively pain free.

I'm also happy to help with any of the reshuffling of your commits if needed, just let us know.
",nateprewitt,Lukasa
3338,2017-02-10 17:14:06,"@davidsoncasey, checking in again. It looks like @kennethreitz may want to start cleaning some of these older PRs up. There's still a lot of useful stuff in here that didn't make it into master but should be in 3.0.0. Would you be interested in bringing this branch up to date and fixing a few things? If not, I'll tidy up your commits and wrap this up.",nateprewitt,kennethreitz
3300,2016-06-11 13:00:24,"@PegasusWang Python 3 should _not_ encode `str` to `bytes`. We cannot guess what encoding you might want, and guessing just leads to _really_ subtle bugs where you send a request you think is valid, get weird 400 errors, and then spend hours of your time and ours trying to work out why and it turns out it's because Requests defaulted to using UTF-8 and your server was expecting Latin-1. You, the user, are responsible for emitting bytes from your generator.

@maxibabyx Your sample code simply cannot be right, I'm afraid: or at least, it cannot be triggering the error you've provided. The branch of code you're in can only be entered in the following situation:
1. The value of your `data` argument does not provide the `__iter__` property, or it _does_ but is either a string, list, tuple, or dict from the perspective of `isinstance`.
2. The `files` keyword argument was also passed.

Your sample code does neither of these things, meaning that when I test it on my machine I do not encounter the bug you have encountered.

Can you confirm for me please that your sample code actually does reproduce your bug, ideally by providing a URL that is publicly reachable for me to test against myself?
",Lukasa,maxibabyx
3299,2016-06-09 17:48:16,"Got it.

curl by default sends the `Expect: 100-Continue` header. Requests does not send this header. That appears to be affecting Apache's decision-making logic here: for large bodies it clearly wants that to be set so that it can validate that the request is actually wanted.

If you prevent curl from sending that header by using the command `curl https://contributors.debian.org/contributors/test_post -F source=bugs.debian.org -F data=@test.json.xz -F data_compression=xz -H ""Expect:""`, that causes curl to see the 413 as well.

Requests cannot, in its current form, support the 100-Continue response, so there is nothing we can do about this: if you'd like to use requests here you'll have to adjust your Apache configuration appropriately.
",Lukasa,test
3297,2016-06-09 05:02:07,"Urllib3 has an exception with the same name, but it is not actually related to `requests.exceptions.HTTPError`. I [searched this repository](https://github.com/kennethreitz/requests/search?utf8=%E2%9C%93&q=httperror) (well, I used grep) for HTTPError and the only place I can see where requests itself uses it is with `raise_for_status`.

I had a discussion with @Lukasa briefly on IRC about this. As far as I can tell, `requests.exceptions.HTTPError` is only used for this purpose. However, if you think you'll use it for other purposes, another alternative is to have something like an `HTTPStatusCodeError` (a subclass of HTTPError) that is specifically for `raise_for_status`.
",davidfischer,Lukasa
3296,2016-07-04 13:30:36,"Cool, that's a good spot.

Yes, there is a minor bug in `resolve_redirects`. Specifically, while `resolve_redirects` attempts to remove proxy information, it cannot actually tell the difference between a proxy that was passed in via the command-line API or from the session and one that was extracted from the environment. This is because `resolve_redirects` is passed the _computed_ `proxies` argument, not the _user's_ `proxies` argument.

With the way the code in requests is structured, this is a very difficult problem to solve. One option is to hang the original `proxies` kwarg off the `Request` object: this will allow `rebuild_proxies` to essentially re-calculate the proxies argument. Another option is to suggest that the `NO_PROXY` environment variable overrides the user proxies argument for redirects: this is out of line with what we normally do, so I'm inclined to not do this. A third option is to try to do something wacky with storing the original `proxies` kwarg value and passing it to `Session.send` so that we can pass it to `resolve_redirects`, but that seems kind of nutty.

Does anyone else have an opinion on how to go about doing this that doesn't suck as much as the three I have just mentioned? @sigmavirus24?
",Lukasa,sigmavirus24
3289,2016-06-07 14:26:27,"This looks reasonable to me aside from my one note about warning spam. @sigmavirus24, what do you think?
",Lukasa,sigmavirus24
3287,2016-06-06 02:10:11,"In requests 2.5.2 `requests.packages.__init__` added code to allow `requests.packages.urllib3` to not exist, and it would fall back to using `urllib3`.

No doubt this works in some cases, as the people on #2375 were happy with it, but it doesnt work with pyopenssl, resulting in no SNI/etc.

`requests.packages.__init__` only injects `urllib3` as `requests.packages.urllib3`.

However all of the other modules within urlllib3 will have different copies in the namespace `requests.packages.urllib3`.

`requests.packages.urllib3.contrib.pyopenssl` then writes to a separate copy of `requests.packages.urllib3.util` and `requests.packages.urllib3.connection`, and these changes are never seen by `urllib3`.

I encountered this on debian stretch & sid's `python-requests` 2.10.0-1 with `python-urllib3` 1.15.1 when running on Python 2.7.6 (Ubuntu Trusty), and haven't retested yet on a later Python 2.7.  @eriol
#3286 fixes the problem for me.

I note that Fedora's latest `python-requests` no longer unbundles urllib3.
",jayvdb,eriol
3287,2016-06-17 09:50:02,"@warsaw , as I saw you're working on a similar package and de-vendoring ( https://github.com/pypa/pip/issues/3802 ), you might like to be aware of, or assist with, this.
",jayvdb,warsaw
3287,2016-06-17 12:14:26,"@eriol +1  I think pip has done the best job of making devendorizing easier for downstream redistributors.  My deltas are very small and now that it's been in place for a while, I haven't encountered any issues with it.  It's possible I'm missing something, so pinging @dstufft for additional thoughts.
",warsaw,dstufft
3287,2016-06-18 17:32:00,"@dstuff, I very quickly tested pip, and it inherits this problem from requests.  pip de-vendoring may well be working wonderfully, but the de-vendored requests isnt initalising urlllib3's pyopenssl properly.  See https://travis-ci.org/jayvdb/requests_issue_3287/builds/138597348 , which shows only additional warnings being emitted for pip's requests, but please see the rest of this issue to see why those warnings are occurring and the impact of that.  Due to pip mostly being used to talk to one set of servers, the fallout is smaller.  My guess is that this isnt a security vulnerability , as it will cause connections to fail when they should succeed, rather than connect when it shouldnt.  But I have very little experience in this area, and not much time to understand the corner cases in these packages, so I am sure others here will know the real potential for this to be abused or not.

The pip travis testing framework for de-vendored packages is nice, but note that https://github.com/pypa/pip/blob/master/pip/_vendor/vendor.txt isnt including `requests[security]` , so that isnt accurately representing distros.  Not that it matters a great deal, as there doesnt seem to be any pip tests covering problematic sites needing better ssl support (not surprising; pip expects requests to do that).
And, fyi, the pip tests suite [are failing](https://travis-ci.org/jayvdb/pip/builds/138602638) atm.
",jayvdb,dstuff
3250,2016-05-31 21:25:26,"Hrm, this seems to be a problem with the RTD docs build. @sigmavirus24, any ideas?
",Lukasa,sigmavirus24
3236,2016-06-21 02:18:40,"@eriol I'm so sorry. I think I lost track of this while I was travelling to PyCon. I expect the same happened to @Lukasa 
",sigmavirus24,Lukasa
3223,2016-05-28 22:04:07,"So the fact that these are all `getaddrinfo` failures makes me think that this might be some sort of DNS caching. I don't think Python 3.4 does that, so I'm guessing Fedora is doing it. I also don't think there's a way that Requests can work around this. And I don't know Fedora very well but @ralphbean does. Maybe he can help us out here?
",sigmavirus24,ralphbean
3221,2016-05-26 13:22:09,"@lukasa Thanks :) i think that sentence is unnecessary in v2.10.0, so seek your advice.
",yangbeom,lukasa
3216,2016-05-24 07:02:09,"There's one small note from @sigmavirus24 but I'd be happy to merge this when that's resolved.
",Lukasa,sigmavirus24
3213,2017-01-13 16:39:50,"@reaperhulk BTW, is slow import of cryptography still an ongoing issue?",Lukasa,reaperhulk
3213,2017-01-13 20:11:53,"On January 13, 2017 1:40:44 PM CST, Dan Sully <notifications@github.com> wrote:
>Ok, so:
>
>
>
>What is the minimum OpenSSL version for the required functionality?
>1.0.1?

I think 1.0.1 is a good minimum but would rather defer to @Lukasa and @reaperhulk on that.

-- 
Sent from my Android device with K-9 Mail. Please excuse my brevity.
",sigmavirus24,reaperhulk
3196,2016-05-16 17:19:29,"Yeah, that's definitely the case, but I think the two URLs are semantically identical.

Regardless, I _also_ think that we have a question to ask ourselves (we being @sigmavirus24 and I), which is: why does `resolve_redirects` not end up calling `prepare_url`?
",Lukasa,sigmavirus24
3195,2016-05-16 11:04:51,"Cool, I'm :+1: on this for now. I'd like another :+1: from @sigmavirus24 before we merge, but I think this is good.
",Lukasa,sigmavirus24
3192,2016-05-14 01:12:25,"ping @Lukasa @sigmavirus24 

I think I addressed the comments from #3185.

Oddly, I cannot make the test suite hang indefinitely when I run it locally, but that's what's happening in jenkins. :confused: Maybe you can spot something I'm missing?
",brettdh,Lukasa
3192,2016-05-14 01:12:25,"ping @Lukasa @sigmavirus24 

I think I addressed the comments from #3185.

Oddly, I cannot make the test suite hang indefinitely when I run it locally, but that's what's happening in jenkins. :confused: Maybe you can spot something I'm missing?
",brettdh,sigmavirus24
3192,2016-05-17 07:10:39,"Ok, cool, I'm happy with this. Assuming @sigmavirus24 is as well, he may merge.
",Lukasa,sigmavirus24
3189,2016-05-12 19:44:59,"Ok, so that there is going to be the issue. The system Python uses the ancient OpenSSL that OS X ships.

I'd like to tag @reaperhulk in here to try to understand why PyOpenSSL isn't correctly installed, because that _should_ have resolved the problem.
",Lukasa,reaperhulk
3187,2016-05-12 08:36:47,"Hey everyone,

sorry for my intervention here, ""out of the blue"". I tried to understand if this issue was already discussed (but could find them) so I come here with the situation and trying to understand the solution.

The problem is the following (from what I understood) : Google App Engine, replace the implementation of urlfetch by their own, which contains some changes that breaks urllib3 and subsequently Requests.

@agfor [made a fix](https://github.com/agfor/requests/commit/da863cc) that seems to work (I didn't had any ChunkedEncodingError since I place the patch on line) and, from my understanding, doesn't break the others users, those that don't use GAE.

I didn't saw his patch as submitted here (I'm wondering why) and I don't know why this hasn't been taken into consideration here. (Please note those are real question, not critics/remarks, my intentions are good :) ).

Hence my post :)
",cnicodeme,agfor
3186,2016-05-11 19:12:34,"`Response.content` [iterates over the response data in chunks of 10240 bytes](https://github.com/kennethreitz/requests/blob/87704105af65b382b86f168f6a54192eab91faf2/requests/models.py#L741). The number 10240 was set in commit [`62d2ea8`](https://github.com/kennethreitz/requests/commit/62d2ea8).

After tracing the source code of urllib3 and httplib, I can’t see a reason for this behavior. It all ultimately goes through httplib’s [`HTTPResponse.readinto`](https://hg.python.org/cpython/file/87130512ef34/Lib/http/client.py#l469), which automatically limits the read size according to `Content-Length` or the `chunked` framing.

Therefore, it seems that, if you simply set `CONTENT_CHUNK_SIZE` to a much larger number (like 10240000), nothing should change, except `Response.content` will become more efficient on large responses.

**Update:** it seems like httplib allocates a buffer of the requested size (to be read into), so simply setting `CONTENT_CHUNK_SIZE` to a large value will cause large chunks of memory to be allocated, which is probably a bad idea.

This is not a problem for me and I have not researched it thoroughly. I’m filing this issue after investigating [a Stack Overflow question](http://stackoverflow.com/questions/37135880/python-3-urllib-vs-requests-performance) where this caused an unexpected slowdown for the poster, and a subsequent [IRC exchange](https://botbot.me/freenode/python-requests/2016-05-11/?msg=65874287&page=1) with @Lukasa. Feel free to do (or not do) whatever you think is right here.
",vfaronov,Lukasa
3185,2016-05-11 13:09:31,"ping @Lukasa 
",brettdh,Lukasa
3184,2016-05-11 04:01:14,"@Lukasa Here's an alternative fix to #3066 (discussed in PR #3181), where I refactored `PreparedRequest.prepare_body` and `PreparedRequest.prepare_content_length` so that `prepare_content_length` is always called.

My only question is in the case when the body is a stream (which it is in the case that brought up this issue) that the current position will always be 0 when the length is calculated. Previously, in cases where `prepare_auth` was not called, the content length was being calculated with `super_len`, now it would be calculated using `self.headers['Content-Length'] = builtin_str(max(0, end_pos - curr_pos))`. As far as I can tell, this will give the same result (as long as the current position is 0), and this is how it's been computed in cases where authorization is being used. But I'm curious if you have thoughts about this.
",davidsoncasey,Lukasa
3184,2016-05-11 08:06:41,"So, I think in general I like this better. However, your point about `super_len` is a real one.

I think the solution here is to rewrite `prepare_content_length` to have it always call `super_len`. `super_len` does use `tell` to do a little dance here, so I think it's probably going to be good enough: @sigmavirus24, does that seem right to you?
",Lukasa,sigmavirus24
3184,2016-05-11 16:09:34,"Ok, I think I'm happy with this, though I'd like @sigmavirus24 to review before we merge. =)
",Lukasa,sigmavirus24
3179,2016-05-05 07:22:38,"Huh. I feel like maybe we should just change `Response.content` to never be `None`. It's certainly surprising to me that it can be. @kennethreitz @sigmavirus24?
",Lukasa,kennethreitz
3179,2016-05-05 07:22:38,"Huh. I feel like maybe we should just change `Response.content` to never be `None`. It's certainly surprising to me that it can be. @kennethreitz @sigmavirus24?
",Lukasa,sigmavirus24
3176,2016-05-04 19:29:39,"This is up to @kennethreitz, but I'm disinclined to want to add this: I think I'd rather factor out the logic in `SessionRedirectMixin` as we've done with a few other bits to allow easier subclassing, rather than add further keyword arguments.
",Lukasa,kennethreitz
3176,2016-05-05 08:43:37,"@adaronen If you'd like to supply a PR that factors out this logic to a method (`resolve_*`) then we'll happily review that. =)
",Lukasa,adaronen
3174,2016-05-03 19:33:20,"Actually, before we conclude this is a urllib3 bug, we need to have a discussion about what the API promises. So let me explain what the problem is.

urllib3 attempts to ensure that connections are thrown away if a problem occurs. This is implemented using a context manager in urllib3 (`_error_catcher`) that checks certain errors and then determines whether the block terminated cleanly. If it did not, it forcibly closes the connection before returning the connection object to the pool to be reopened.

That object treats _any_ exception as an error case that leaves the connection in an indeterminate state. That is mostly a good thing, except for one particularly awkward exception: [`GeneratorExit`](https://docs.python.org/2/library/exceptions.html#exceptions.GeneratorExit). This exception is raised when the `close()` method on a generator is called, and is thrown to allow things like `finally` blocks to execute properly.

_One_ case where this is called is when a generator is garbage collected. That happens in your code, because you call `iter_content` multiple times, once for each tag you search for. When that happens you spin up several generators (`iter_content` returns a generator, and `stream` returns a generator, and `read_chunked` also returns a generator, so there's a chain of at least three generators in this case). Because you don't save the return value from `iter_content`, that generator chain gets leaked. This causes the `read_chunked` generator to throw `GeneratorExit`, which causes the urllib3 `_error_catcher` to conclude that the connection was not left in a clean state and terminates it.

There are therefore a few questions:
1. Should `_error_catcher` consider `GeneratorExit` an error case, or special case it? I'm not sure: the question is whether the connection will get cleaned up properly in situations where the generator really is leaked. Currently my assumption is that it won't, and so `GeneratorExit` really is an error case.
2. Is it safe to open multiple versions of the generators `iter_content`, `stream`, and `read_chunked`? They don't make it clear. `iter_lines` in requests is clear that it is _not_ safe to do that, but the rest are left ambiguous. We need to make a call, where I suspect the answer boils down to whether `decode_content` is `True`: if it is, there is a state object that gets lost when that generator leaks. Given that requests essentially always sets `decode_content` to `True`, I think that means it's also not safe to repeatedly call `iter_content`.

@ducu In the short term, you can fix this by saving off the result of `iter_content` somewhere on your `Summary` object and then re-using that, rather than repeatedly re-calling that method. That logic will _definitely_ work, and we can work out whether or not the alternative _should_ work.

Can I get input from @kennethreitz, @shazow, and @sigmavirus24 on this please?
",Lukasa,kennethreitz
3174,2016-05-03 19:33:20,"Actually, before we conclude this is a urllib3 bug, we need to have a discussion about what the API promises. So let me explain what the problem is.

urllib3 attempts to ensure that connections are thrown away if a problem occurs. This is implemented using a context manager in urllib3 (`_error_catcher`) that checks certain errors and then determines whether the block terminated cleanly. If it did not, it forcibly closes the connection before returning the connection object to the pool to be reopened.

That object treats _any_ exception as an error case that leaves the connection in an indeterminate state. That is mostly a good thing, except for one particularly awkward exception: [`GeneratorExit`](https://docs.python.org/2/library/exceptions.html#exceptions.GeneratorExit). This exception is raised when the `close()` method on a generator is called, and is thrown to allow things like `finally` blocks to execute properly.

_One_ case where this is called is when a generator is garbage collected. That happens in your code, because you call `iter_content` multiple times, once for each tag you search for. When that happens you spin up several generators (`iter_content` returns a generator, and `stream` returns a generator, and `read_chunked` also returns a generator, so there's a chain of at least three generators in this case). Because you don't save the return value from `iter_content`, that generator chain gets leaked. This causes the `read_chunked` generator to throw `GeneratorExit`, which causes the urllib3 `_error_catcher` to conclude that the connection was not left in a clean state and terminates it.

There are therefore a few questions:
1. Should `_error_catcher` consider `GeneratorExit` an error case, or special case it? I'm not sure: the question is whether the connection will get cleaned up properly in situations where the generator really is leaked. Currently my assumption is that it won't, and so `GeneratorExit` really is an error case.
2. Is it safe to open multiple versions of the generators `iter_content`, `stream`, and `read_chunked`? They don't make it clear. `iter_lines` in requests is clear that it is _not_ safe to do that, but the rest are left ambiguous. We need to make a call, where I suspect the answer boils down to whether `decode_content` is `True`: if it is, there is a state object that gets lost when that generator leaks. Given that requests essentially always sets `decode_content` to `True`, I think that means it's also not safe to repeatedly call `iter_content`.

@ducu In the short term, you can fix this by saving off the result of `iter_content` somewhere on your `Summary` object and then re-using that, rather than repeatedly re-calling that method. That logic will _definitely_ work, and we can work out whether or not the alternative _should_ work.

Can I get input from @kennethreitz, @shazow, and @sigmavirus24 on this please?
",Lukasa,shazow
3174,2016-05-03 19:33:20,"Actually, before we conclude this is a urllib3 bug, we need to have a discussion about what the API promises. So let me explain what the problem is.

urllib3 attempts to ensure that connections are thrown away if a problem occurs. This is implemented using a context manager in urllib3 (`_error_catcher`) that checks certain errors and then determines whether the block terminated cleanly. If it did not, it forcibly closes the connection before returning the connection object to the pool to be reopened.

That object treats _any_ exception as an error case that leaves the connection in an indeterminate state. That is mostly a good thing, except for one particularly awkward exception: [`GeneratorExit`](https://docs.python.org/2/library/exceptions.html#exceptions.GeneratorExit). This exception is raised when the `close()` method on a generator is called, and is thrown to allow things like `finally` blocks to execute properly.

_One_ case where this is called is when a generator is garbage collected. That happens in your code, because you call `iter_content` multiple times, once for each tag you search for. When that happens you spin up several generators (`iter_content` returns a generator, and `stream` returns a generator, and `read_chunked` also returns a generator, so there's a chain of at least three generators in this case). Because you don't save the return value from `iter_content`, that generator chain gets leaked. This causes the `read_chunked` generator to throw `GeneratorExit`, which causes the urllib3 `_error_catcher` to conclude that the connection was not left in a clean state and terminates it.

There are therefore a few questions:
1. Should `_error_catcher` consider `GeneratorExit` an error case, or special case it? I'm not sure: the question is whether the connection will get cleaned up properly in situations where the generator really is leaked. Currently my assumption is that it won't, and so `GeneratorExit` really is an error case.
2. Is it safe to open multiple versions of the generators `iter_content`, `stream`, and `read_chunked`? They don't make it clear. `iter_lines` in requests is clear that it is _not_ safe to do that, but the rest are left ambiguous. We need to make a call, where I suspect the answer boils down to whether `decode_content` is `True`: if it is, there is a state object that gets lost when that generator leaks. Given that requests essentially always sets `decode_content` to `True`, I think that means it's also not safe to repeatedly call `iter_content`.

@ducu In the short term, you can fix this by saving off the result of `iter_content` somewhere on your `Summary` object and then re-using that, rather than repeatedly re-calling that method. That logic will _definitely_ work, and we can work out whether or not the alternative _should_ work.

Can I get input from @kennethreitz, @shazow, and @sigmavirus24 on this please?
",Lukasa,sigmavirus24
3131,2016-04-26 14:53:18,"If you're using a `file://` URI, you're never going to talk to something over the network using HTTP (for example). I'll talk to @msabramo about fixing unixsocket as this problem will persist otherwise for several people who will not get this fix.
",sigmavirus24,msabramo
3131,2016-04-26 15:57:36,"Ah so @msabramo should be overriding that too if he continues to subclass the adapter. (I missed the usage in the `request_url` method because GitHub search hasn't indexed that yet apparently.)
",sigmavirus24,msabramo
3108,2016-04-21 15:22:47,"I'm happy with this! Go for it @kennethreitz, merge if you'd like to. =D
",Lukasa,kennethreitz
3100,2016-04-17 20:03:22,"@kennethreitz, @sigmavirus24 Thanks! 👍 
",hitstergtd,sigmavirus24
3099,2016-04-28 16:50:36,"So this is a really tricky place to be.

The reality is that there is more-or-less no cross-platform way to kill a thread except by interrupting it, which is basically what a signal is. That means, I think, that signals are the only route you really have to making this work across platforms. I'm inclined to try to ping in a Windowsy Pythony expert: @brettcannon, do you have a good suggestion here?
",Lukasa,brettcannon
3099,2016-04-28 17:13:44,"Maybe @zooba has an idea as he actually knows how Windows works. :)
",brettcannon,zooba
3099,2016-04-28 17:15:07,"Haha, I already know @zooba and @brettcannon. I can discuss with them here or internally as a solution to this would probably help them too.
",emgerner-msft,zooba
3099,2016-04-28 18:15:11,"lol @lukasa I take your point about maintenance, which was already in my mind, but on ""feature vs misfeature"" I'm afraid I'm completely opposite to you. I think anyone who _doesn't_ want a total timeout isn't thinking clearly about what they want, and I'm having difficulty imagining a situation where what you describe as a bug  ""30MB download changes to 30GB and therefore fails"" isn't in fact a beneficial feature!

You can as you say do something a bit similar (but I suspect without most of the benefits of a total timeout) using `stream=True` but I thought the point of requests was that it handled things for you...
",jribbens,lukasa
3099,2016-04-29 12:38:38,"@lukasa I suppose my thinking is that not only do I want it, in fact nearly all users would want it if they thought about it (or they don't realise it's not already there). Half of your usage scenarios above where you say it should be avoided I would say it's vital (web scraper and log aggregator) - the other two it's less necessary as there's likely to be a user waiting for the result who can cancel the download manually if they want. Anything that runs in the background without a UI and doesn't use an overall timeout is buggy in my view!
",jribbens,lukasa
3095,2016-06-21 17:17:23,"@singletoned `openssl s_client -connect <host>:<port> -showcerts` should do the trick.
",Lukasa,singletoned
3089,2016-04-14 07:36:37,"@sleshep What specifically are you trying to achieve? You appear to have a cookie header on hand and want to attach it to a request. Is that right?
",Lukasa,sleshep
3089,2016-04-15 01:54:53,"@sleshep As a project, we don't agree. This pull request does not implement the same functionality that the standard library does. It also implements only a portion of what can be found without this pull request regardless of one's perception of ease of use.

If these functions are working for you, you should keep using them, but we are not going to accept them into requests or maintain them since there are significantly more mature and robust solutions available to us that we do not have to maintain.
",sigmavirus24,sleshep
3089,2016-05-21 17:07:27,"if found a simple way to  achieve my target.



it work well.
Thanks all of you.
@kennethreitz 
@Lukasa 
@sigmavirus24 
",liuyang007,kennethreitz
3082,2016-04-08 20:40:11,"I think this works well, at least in the basic case. As you correctly pointed out, there's a question about how this will work with redirects, but that's harder for us to test at this moment (though we'll get there shortly I hope).

@kennethreitz, want a final review here?
",Lukasa,kennethreitz
3082,2016-04-11 17:43:20,"Gah, @sigmavirus24 beat me to it
",kennethreitz,sigmavirus24
3079,2016-04-08 09:24:52,"@tzickel As far as I know, BytesIO implements `tell()` on all Python versions, so the simplest thing to do is to add logic that does `tell()`, `seek()`, `tell()`, `seek()`. Doing that, incidentally, would allow us to also record the current location in the file-like-object so that we can safely rewind.

@sigmavirus24, you wrote the original `getvalue()` call: what are the odds you remember the rationale at the time?
",Lukasa,sigmavirus24
3079,2016-04-08 14:15:35,"1. Shouldn't requests have an option to make 302 and 307 act the same (instead of as 303 as it does by default) ? The HTTP 1.1 spec actually meant that 302 should be like 307 by default (yet most browsers treat it like a 303).
2. I'm still waiting for the input from @sigmavirus24 because that is important memory-wise.
",tzickel,sigmavirus24
3070,2016-03-29 17:08:43,"This is an entirely reasonable suggestion.

Honestly, I'm not averse to doing it: there are definitely worse things to do than this. I'd be open to providing a default timeout. However, I don't think we can do it until 3.0.0.

Of course @kennethreitz as keeper of the spirit of requests has got the final say on this, and should definitely weigh in.
",Lukasa,kennethreitz
3067,2016-03-28 18:36:46,"@lukasa we should add your snippet of text there to the docs. Doubt it'll reduce incoming issues (questions) around this, but it'd be great for the docs. 
",kennethreitz,lukasa
3065,2016-03-24 14:10:48,"Well that's bizarre. Does `pip freeze` show that ndg-httpsclient is installed?

@dstufft, any theories about this weirdness?
",Lukasa,dstufft
3061,2016-05-21 19:02:19,"@ryanchou1991 Do you have some result? I'm with exactly same problem. 
",moacirmoda,ryanchou1991
3061,2016-05-22 03:59:04,"@ryanchou1991 actually i got the IP making a http request before all the flow. I think the propouse is taking an valid IP from proxy, am i right? Is not a problem anymore.

The problem that I have now is to make proxymesh works with a https URL. I can't make this works. It always response a 407 error. 

Can you help me?
",moacirmoda,ryanchou1991
3061,2016-05-23 15:31:20,"@ryanchou1991 
Could you show me some examples from the snippets? Me emails with the proxymesh are not so efficiently.
",moacirmoda,ryanchou1991
3061,2016-05-25 18:36:48,"Hi @ryanchou1991 ,

Their support send me a snippet like these that you send me.

Look. I did implement your snippet and doesn't work:

https://gist.github.com/moacirmoda/122ab1557a062943161e723f8a8c37ad

If you look in the output, on the headers, there's no 'X-ProxyMesh-IP', it means that proxy doesn't work, I'm right?
",moacirmoda,ryanchou1991
3061,2016-05-25 18:45:01,"@ryanchou1991 

Look, I made even better: I access `https://www.whatismyip.com` under the proxy:

https://gist.github.com/moacirmoda/122ab1557a062943161e723f8a8c37ad

Look the output:



`177.148.142.34` is my real ip address. Means not work, right?
",moacirmoda,ryanchou1991
3060,2016-03-18 10:01:21,"So, I'm totally happy with this code change: I think it's a reasonable refactoring that helps improve some clarity, _as well_ as allow @benweatherman to perform the override you want.

I'm also delighted to see some extra tests added for this. I think this change is wonderful! Pinging @kennethreitz or @sigmavirus24 for an extra round of review before merging.
",Lukasa,kennethreitz
3060,2016-03-18 10:01:21,"So, I'm totally happy with this code change: I think it's a reasonable refactoring that helps improve some clarity, _as well_ as allow @benweatherman to perform the override you want.

I'm also delighted to see some extra tests added for this. I think this change is wonderful! Pinging @kennethreitz or @sigmavirus24 for an extra round of review before merging.
",Lukasa,sigmavirus24
3059,2016-03-17 19:56:34,"Cool, I'm happy with this. @sigmavirus24 `ProxyError` is a `ConnectionError` subclass, so I think this can go in the next minor release. Agreed?
",Lukasa,sigmavirus24
3052,2016-03-15 21:08:17,"So to be clear, I am now as I was then open to removing simplejson as an option entirely, but @kennethreitz wanted it there. =)
",Lukasa,kennethreitz
3050,2016-03-13 12:26:00,"And yeah, in first place, thanks for great library, @kennethreitz and all the contributors :+1: 
",alexanderad,kennethreitz
3049,2016-03-13 11:23:35,"This also seems reasonable to me, but @Stranger6667 is sat right next to me. ;) @sigmavirus24/@kennethreitz, mind doing an extra review for me? This is +1 from me.
",Lukasa,kennethreitz
3049,2016-03-13 11:23:35,"This also seems reasonable to me, but @Stranger6667 is sat right next to me. ;) @sigmavirus24/@kennethreitz, mind doing an extra review for me? This is +1 from me.
",Lukasa,sigmavirus24
3049,2016-03-13 11:37:35,"Hello folks seems somehow I got placed on this email alias By mistake.
Anyway I can be removed.

Thank you
-Ryan
On Sun, Mar 13, 2016 at 7:24 AM Cory Benfield notifications@github.com
wrote:

> This also seems reasonable to me, but @Stranger6667
> https://github.com/Stranger6667 is sat right next to me. ;)
> @sigmavirus24/@kennethreitz https://github.com/kennethreitz, mind doing
> an extra review for me? This is +1 from me.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/pull/3049#issuecomment-195938314
> .
",ryandebruyn,kennethreitz
3049,2016-03-13 11:37:35,"Hello folks seems somehow I got placed on this email alias By mistake.
Anyway I can be removed.

Thank you
-Ryan
On Sun, Mar 13, 2016 at 7:24 AM Cory Benfield notifications@github.com
wrote:

> This also seems reasonable to me, but @Stranger6667
> https://github.com/Stranger6667 is sat right next to me. ;)
> @sigmavirus24/@kennethreitz https://github.com/kennethreitz, mind doing
> an extra review for me? This is +1 from me.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/pull/3049#issuecomment-195938314
> .
",ryandebruyn,sigmavirus24
3048,2016-03-13 11:13:15,"@sigmavirus24 @kennethreitz Can one of you two do a separate code review of this? @Stranger6667 is here with me at PyCon SK writing some of these tests. I'm happy with these, but it'd be good if one of you two gave the ok/not-ok.
",Lukasa,kennethreitz
3048,2016-03-13 11:13:15,"@sigmavirus24 @kennethreitz Can one of you two do a separate code review of this? @Stranger6667 is here with me at PyCon SK writing some of these tests. I'm happy with these, but it'd be good if one of you two gave the ok/not-ok.
",Lukasa,sigmavirus24
3048,2016-03-15 13:51:17,"Hello @Lukasa @sigmavirus24 !
I've updated this PR :)
",Stranger6667,sigmavirus24
3045,2016-03-11 09:55:46,"So my experience of this seems to be that it occurs only when you swap languages from one of the sub pages. If you do the swap at the top level it seems to work fine.

That _seems_ to me like it's probably an RTD issue? @ericholscher am I right about that, or am I on totally the wrong track here?
",Lukasa,ericholscher
3038,2016-03-08 08:32:42,"FWIW, I don't think any of those issues actually indicate that preservation of order on the wire is important. It _is_, but not for any of those reasons. The reason it's important is that headers can in principle be split up into multiple instances if the header is a comma-separated list, and maintaining the order of those parts is important (or the header ends up taking on a different meaning).

The fundamental reason this doesn't work is because we store the headers in our `CaseInsensitiveDict`, which does not preserve order. At this point we may want to look harder at replacing our own implementation with the one from urllib3, which _does_ preserve order.

Thoughts @kennethreitz, @sigmavirus24?
",Lukasa,kennethreitz
3038,2016-03-08 08:32:42,"FWIW, I don't think any of those issues actually indicate that preservation of order on the wire is important. It _is_, but not for any of those reasons. The reason it's important is that headers can in principle be split up into multiple instances if the header is a comma-separated list, and maintaining the order of those parts is important (or the header ends up taking on a different meaning).

The fundamental reason this doesn't work is because we store the headers in our `CaseInsensitiveDict`, which does not preserve order. At this point we may want to look harder at replacing our own implementation with the one from urllib3, which _does_ preserve order.

Thoughts @kennethreitz, @sigmavirus24?
",Lukasa,sigmavirus24
3036,2016-03-07 08:33:52,"Resolves #3035.

I'm not actually sure that this approach is the right one: I'm inclined to say that, when an exception is hit from `tell()`, that we may want to assume we don't know the length at all (return length 0) and use chunked-transfer encoding. That's a particularly good idea in this case, as frequently stdin has an unknown length altogether.

Thoughts on that point @sigmavirus24 and @jkbrzt?
",Lukasa,jkbrzt
3036,2016-03-07 08:33:52,"Resolves #3035.

I'm not actually sure that this approach is the right one: I'm inclined to say that, when an exception is hit from `tell()`, that we may want to assume we don't know the length at all (return length 0) and use chunked-transfer encoding. That's a particularly good idea in this case, as frequently stdin has an unknown length altogether.

Thoughts on that point @sigmavirus24 and @jkbrzt?
",Lukasa,sigmavirus24
3036,2016-03-07 09:02:29,"@jkbrzt That's true, it can, but in this requests we also send zero-length files via chunked-transfer encoding. This is because some files report having zero length but still have content in them: again, stdin on some platforms exhibits this behaviour.

I think we broke our fall-back logic there when we added the `tell()` code, so that makes this a regression fix. Regardless, let's switch to that approach. 
",Lukasa,jkbrzt
3036,2016-03-07 09:10:24,"FWIW, doing things this way can cause the chunk size to be relatively small (each chunk will be one line in the example you've provided) because we naively iterate over the file, so @jkbrzt you may want to have some fancy detection logic for this case that _enables_ Nagle's algorithm. In the case of the test file here we sent about 40 tinygrams (fewer than 30 data bytes in each packet) which is not the most efficient way to use the network.

Requests could _in principle_ do this too, but it's not clear to me that that's useful in the general case, and detecting the appropriate situation might be hard. We could _try_ to do something smarter here (around line 431 in models.py we catch the problem of calling `super_len()` on generators, so we may want to consider treating `length == 0` and `length == None` differently, where `length == 0` causes us to wrap the file in a generator that issues multiple large reads rather than iterating.

Not sure how I feel about that though.
",Lukasa,jkbrzt
3036,2016-03-12 07:56:59,"@jkbrzt Yeah, there's probably some value in having a section in the advanced docs about TCP efficiency. Wanna write it? ;)
",Lukasa,jkbrzt
3035,2016-03-07 07:20:15,"@jkbrzt I think you're probably right. I'll have a little play today and see if I can convince myself you're definitely right, and then submit a patch.
",Lukasa,jkbrzt
3033,2016-03-05 07:11:26,"I'm really intrigued by this idea. @kennethreitz?
",Lukasa,kennethreitz
3033,2016-03-05 09:02:55,"Transifex has a really nice permissions system, you can set it to auto
update the source translation from a url and a useful API.

On Sat, 5 Mar 2016 17:11 Cory Benfield notifications@github.com wrote:

> I'm really intrigued by this idea. @kennethreitz
> https://github.com/kennethreitz?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/3033#issuecomment-192600901
> .
> 
> ## 
> 
> _Stewart Polley_
> Head of Technical Support
> stewart.polley@elvanto.com | www.elvanto.com
",ElvantoStewart,kennethreitz
3033,2016-03-05 10:48:09,"@requests/translation-team
",caizixian,requests
3029,2016-03-03 22:26:47,"Hi @vaibhavkaul,

Thanks for sending a pull request. I'm afraid we likely won't be accepting it. Here's why:
- This is drastically backwards incompatible behaviour. 
- It takes a great deal of freedom away from the user which we have intentionally placed into their hands.
- This code does not just change this for the `multipart/form-data` case.
- This also prevents people from writing API fuzzers which provide incorrect content-type headers for a totally different kind of content and drastically reduces requests flexibility.
- This affects the cases where people provide something to `data=` or `json=` with a custom content-type header where they expect their explicit decision to specify that header to be honored.

We could consider this for version 3.0.0 but I think it would not fit there either for several of the reasons listed above and also mostly due to the fact that we hope to not break things too drastically in 3.0.

I won't close this just yet because I hope @Lukasa and @kennethreitz will weigh in as well.

Cheers,
Ian
",sigmavirus24,kennethreitz
3029,2016-03-03 22:26:47,"Hi @vaibhavkaul,

Thanks for sending a pull request. I'm afraid we likely won't be accepting it. Here's why:
- This is drastically backwards incompatible behaviour. 
- It takes a great deal of freedom away from the user which we have intentionally placed into their hands.
- This code does not just change this for the `multipart/form-data` case.
- This also prevents people from writing API fuzzers which provide incorrect content-type headers for a totally different kind of content and drastically reduces requests flexibility.
- This affects the cases where people provide something to `data=` or `json=` with a custom content-type header where they expect their explicit decision to specify that header to be honored.

We could consider this for version 3.0.0 but I think it would not fit there either for several of the reasons listed above and also mostly due to the fact that we hope to not break things too drastically in 3.0.

I won't close this just yet because I hope @Lukasa and @kennethreitz will weigh in as well.

Cheers,
Ian
",sigmavirus24,Lukasa
3029,2016-03-04 13:47:45,"@kennethreitz thoughts?
",sigmavirus24,kennethreitz
3029,2016-03-04 16:25:18,"@sigmavirus24 I made the change so it doesn't affect other things besides files.

> Sometimes overriding requests default behaviour is correct for user

Yes, which which is why `sometimes` should not dictate default behavior.

> Will make the correct body with an incorrect header. 

I dont think thats true since for the body to be parseable by any standard multi-part library (cgi etc) it would need to have the correct header. Just having the data in the body does not make it correct. I would argue the body is wrong in this case for a multipart request.

My motive right now is not to get this merged in at all. Happy to close the PR. But we should be clear about what the issue is here. Specifically I would like to address things like:

> That means that one of the cases above prevents the user from doing exactly what they intend to do

and 

> then changing the behaviour to differently silently subvert expectations isn't a great direction

The specific part of code I am referring to deals [exclusively with Multipart Files](https://github.com/kennethreitz/requests/blob/master/requests/models.py#L445-L447). The whole [idea of adding a `file=` param](http://docs.python-requests.org/en/master/user/quickstart/#post-a-multipart-encoded-file) is to encode the request as a multi-part request. `Most` developers using that code path clearly want the request to be posted as multipart.

Multipart requests must [have a boundary specified in the content-type](https://www.w3.org/Protocols/rfc1341/7_2_Multipart.html), if this is missing, the whole purpose for passing a `files=` param is defeated. This is no longer a multipart request. This is not a `correct body with the wrong header`. Clearly the `requests` framework [does not allow a user to pass their own boundary](https://github.com/kennethreitz/requests/blob/master/requests/models.py#L156) (notice the missing boundary kwarg) and use that. It `always` relies on a auto generated boundary string. We should either give people the ability to be in control of the `content-type` (by letting them specify a boundary string) or do the right think in the library.

> In general the requests header dictionary lets people do stupid things. If the user wants to change content-length, they can. If they want to change transfer-encoding, they can

I agree. Devs do stupid things. But then why make [this](https://github.com/kennethreitz/requests/blob/master/requests/models.py#L434-L438) raise an exception. There should at least be a warning somewhere about the request not being encoded as a multi part request if the headers being passed include a `content-type`, even if that content type is `multipart/form-data`.

I completely agree that we should not make this change if you feel this breaks the existing API, so maybe this should be considered for a future release or maybe not. I'll leave it up to @kennethreitz.
",vaibhavkaul,kennethreitz
3025,2016-02-27 05:53:52,"@Pr0xy671 you are specifying 3 proxies. Try only doing one. Also, add a `//` to your proxy URL.

p.s. you can use `http://httpbin.org/ip` to get ip address easily. 
",kennethreitz,Pr0xy671
3025,2016-03-09 05:01:10,"@Pr0xy671 it's because you specified a proxy only for HTTPS (`{'https': …}`) but requested a HTTP page.
",jakubroztocil,Pr0xy671
3023,2016-02-18 17:34:55,"I'm going to leave this up to @kennethreitz. As you stated in #3022, this is totally unnecessary, but neither is it a maintenance burden, so I have no objection to it. Thanks for your contribution! :cake:
",Lukasa,kennethreitz
3022,2016-02-21 01:31:49,"@kennethreitz Resolved this by changing the docs (see PR #3023).

Looks like http://docs.python-requests.org/en/master/api/#exceptions is still missing ConnectTimeout and ReadTimeout. Should they be added to `__init__.py`?
",jtpereyda,kennethreitz
3016,2016-02-15 16:14:09,"@evilerliang Do the requests docs guarantee that `Session.hooks` will not be replaced by `Request.hooks`? In general, things declared for specific requests _override_ the things declared on the `Session`, rather than _supplementing_ them.
",Lukasa,evilerliang
3009,2016-03-06 19:57:56,"Extra datapoint, I've been bitten by this too while interacting with the API of a Spanish-language server. The correct fixes here are, I believe:
1. Properly decode the HTTP Reason Phrase field as described by @denis-ryzhkov in [#1181](https://github.com/kennethreitz/requests/pull/1181#issuecomment-13423623) for header values.
   - According to [RFC2616](https://tools.ietf.org/html/rfc2616#section-6.1.1), Reason Phrase is `TEXT` which defaults to `latin1` and supports other characters under RFC2047.
2. Store the `.reason` attribute as unicode if it isn't already.
3. In `.raise_for_status` use `%r` as @Lukasa suggests (since exception messages are supposed to be bytestrings).
",erydo,denis-ryzhkov
3006,2016-02-12 05:28:54,"Any idea? @Lukasa 
",caizixian,Lukasa
3006,2016-02-12 13:15:20,"I bet that is a bug on 3.5 with pyOpenSSL that we didn't know about @lukasa (with respect to `_fileobject` not existing. 
",sigmavirus24,lukasa
3006,2016-02-21 00:26:22,"@Lukasa @shazow urllib3 always has a home at http://ci.kennethreitz.org, if desired!
",kennethreitz,shazow
3004,2016-03-02 12:04:31,"@girishsortur This would not help you at all. You can already see the errno (it's -1), and the error message (""unexpected EOF""). That means that the remote server terminated the connection without cleanly shutting down the TLS connection, likely midway through the handshake.
",Lukasa,girishsortur
3002,2016-02-09 20:55:19,"So this was probably introduced by shazow/urllib3#708. Looks like we'll have to revert that @shazow.

@pneumaticdeath Why Python version are you using?
",Lukasa,shazow
2997,2016-02-05 19:52:19,"Good spot! I suspect this might be the result of the 301 cache misbehaving somewhat. @sigmavirus24, does that sound right to you?
",Lukasa,sigmavirus24
2996,2016-02-05 13:19:26,"Pinging @kennethreitz again for review on test code.

Thanks for this @Stranger6667!
",Lukasa,kennethreitz
2992,2016-02-03 15:07:23,"For general code review I'm not best placed to do it because I suggested the approach to begin with, so let's tag in @sigmavirus24 and @kennethreitz as well.
",Lukasa,kennethreitz
2992,2016-02-03 15:07:23,"For general code review I'm not best placed to do it because I suggested the approach to begin with, so let's tag in @sigmavirus24 and @kennethreitz as well.
",Lukasa,sigmavirus24
2991,2016-02-02 13:12:00,"Thanks for this! I'll handle answering some questions, but I know that @kennethreitz has got opinions about some of the changes you're making here so I'll let him weigh in to do the actual review work.

To your first question, I suspect that test is just poorly written.

To Python 3.2 support, requests explicitly considers Python 3.2 a best-effort platform. There's no reason to continue to support it formally, because Python 3.2 is no longer supported by the PSF: the last release was in 2014. The only reason we appear to be compatible with Python 3.2 is because pip needed to support it, and even pip is planning to drop support.

To Jython: we'd like to work on Jython, but it's moderately painful for us to get a CI suite for it. It's not unreasonable to run the tests against Jython though.

Re: Travis. We moved away from Travis because our test suite used to run against the real httpbin instance, and Travis' network access was sufficiently flaky that our tests failed a lot. We've therefore been using Jenkins for a while. However, now that our tests are entirely offline I'm interested in moving back to Travis. Of course, that's @kennethreitz's call.

For coverage, I see no reason to use anything but coverage.py, but again, this is @kennethreitz's area.
",Lukasa,kennethreitz
2991,2016-02-02 20:52:32,"Regarding Travis: we used to utilize Travis, but I quickly grew frustrated with it reporting false negatives for our test suite on a regular basis. I was also frustrated by them randomly removing a few older versions of Python one day, as well. I was also frustrated by all the email notifications it sent me (many regarding forks of kennethreitz/requests). I'm also not a fan of its UI. :)

That was a long time ago, and I'm sure it's improved quite a bit since then. 

Our current CI integration with my Jenkins server seems to be suiting our needs perfectly fine. I personally quite enjoy having a private Jenkins server. However, I wouldn't be against considering a switch back to Travis if there was an obvious benefit to myself, @Lukasa, or @sigmavirus24. I'm not aware of any. Are there any?
",kennethreitz,sigmavirus24
2988,2016-02-01 08:19:29,"Yeah, 4.5 seconds is probably too long.

The concern with caching the result is that the resolver doesn't give enough information for us to invalidate that cache appropriately, but we will from time to time want to invalidate it (so that long running processes don't end up so broken that they need a restart).

A simple approach here might be to use an `@memoize` decorator with a cache that invalidates entries after a certain amount of time. Given that [`functools.lru_cache`](https://docs.python.org/3/library/functools.html#functools.lru_cache) provides a `cache_clear` function, we may be able to provide a very simple wrapper function that handles this logic.
",Lukasa,memoize
2988,2017-02-21 19:27:58,@Lukasa Is this issue still open? Would be willing to take a shot at it using the `@functools.lru_cache` and time-stamping approach.,davidfontenot,functools
2987,2016-01-31 08:41:47,"Would be great to include the functionality of `requests_toolbelt.utils.formdata.urlencode` in Requests. 

http://toolbelt.readthedocs.org/en/latest/formdata.html

Thoughts? 

/cc @Lukasa @sigmavirus24 
",kennethreitz,Lukasa
2987,2016-01-31 08:41:47,"Would be great to include the functionality of `requests_toolbelt.utils.formdata.urlencode` in Requests. 

http://toolbelt.readthedocs.org/en/latest/formdata.html

Thoughts? 

/cc @Lukasa @sigmavirus24 
",kennethreitz,sigmavirus24
2987,2016-01-31 08:55:03,"No objection from me, but the tool belt is @sigmavirus24's baby so he may have a stronger position,
",Lukasa,sigmavirus24
2983,2016-01-29 01:56:38,"requests 2.9.1 
Executing of this code (from samples in documentation):

`import requests

url = 'http://192.168.88.164:8000/'
files = {'file': ('conf.txt', 'some, data,to send\r\nnanother,row,to,send\r\nyet another,row,to,send\r\n')}

r = requests.post(url, files=files)
print r.text`

Results in correctly uploaded file, but without first line.
I.e. part 'some, data,to send\r\n' is missed.

Same with actoual file upload:
`import requests

url = 'http://192.168.88.164:8000/'
files = {'file': ('conf.txt', open('conf.txt', 'rb'))}

r = requests.post(url, files=files)
print r.text`

First line disappears from uploaded content.
The same file uploaded to the same server with curl:
`curl -v -X POST -F file=@conf.txt http://192.168.88.164:8000`
do not such an issue.

Issue seems to be in incorrect encoding - extra \r\n where missed somewhere.
",amdei,conf
2982,2016-01-28 09:27:11,"This is a remarkably tricky issue, but in general I'm sympathetic to the idea that requests should normalise the URL as much as possible when provided by the user or when received in the Location header.

I appreciate that generally speaking we try not to manipulate the URLs provided by users _too_ extensively, but we do still play with them: we try to urlencode partially encoded URLs and generally just try to make sense of what the remote party has sent us. For that reason, I also think we should take a similar attitude to partial paths.

However, there are some reasons to be tentative here. The first is that this amounts to a major change, which would mean it has to go in to 3.0.0. The second is that if we're determined to start handling URLs ""properly"", we should aim to handle them _really_ properly. I believe @glyph is working on a URL handling library, but generally speaking I think we should be aiming to bring in something that lets us work with URLs more effectively so that we can do something that amounts to appropriate behaviour.

The upshot means that, if @sigmavirus24 agrees with me, we'll need to put this on the backburner until we get a library that can let us work with URLs more effectively.
",Lukasa,glyph
2982,2016-01-28 09:41:38,"Could you please refer to the @glyph 's repository so I could use it meanwhile it is not integrated into requests or participate in its development may be
",Lol4t0,glyph
2982,2016-01-28 09:42:42,"Currently I think @glyph is developing it as part of Twisted, so I'm honestly not sure where it lives. However, I'm sure @glyph would be delighted to weigh into this thread at some time today. =)
",Lukasa,glyph
2982,2016-01-28 13:35:16,"Testing rfc3986, shows that this already works there @lukasa and we had been talking about using that in the past. @glyph maybe we should try to converge our libraries
",sigmavirus24,lukasa
2973,2016-01-20 13:54:30,"/cc @dstufft (re: https://github.com/pypa/pip/issues/3384#issuecomment-173202320) although I'm starting to suspect that that is a version of requests packaged by the distro that is causing wichert's problems.
",sigmavirus24,dstufft
2972,2016-01-20 02:21:44,"@Lukasa I'm +0.5 on this. The reason I'm not a full +1 is that we might start leaking sockets this way.
",sigmavirus24,Lukasa
2969,2016-01-24 04:24:04,"Okay that went quicker than expected, based on extrapolation of past activity on my pull request. :) My changes are now integrated into a requests-toolbelt branch, conditioned against a hypothetical 2.10 version of requests containing the latest urllib3 functionality. I assume once requests cuts this release, @sigmavirus24 can then integrate the branch into mainline requests-toolbelt.

I'll leave it to you guys to determine the time-sensitivity of appengine functionality.
",mikelambert,sigmavirus24
2966,2016-01-11 17:15:47,"It's been raised repeatedly, mostly by people using Linux systems, that it's annoying that requests doesn't use the system trust store and instead uses the one that certifi ships. This is an understandable position. I have some personal attachment to the certifi approach, but the other side of that argument definitely has a reasonable position too. For this reason, I'd like us to look into whether we should use the system trust store by default, and make certifi's bundle a fallback option.

I have some caveats here:
1. If we move to the system trust store, we must do so on _all_ platforms: Linux must not be its own special snowflake.
2. We must have broad-based support for Linux and Windows.
3. We must be able to fall back to certifi cleanly.

Right now it seems like the best route to achieving this would be to use [certitude](https://github.com/python-hyper/certitude). This currently has support for dynamically generating the cert bundle OpenSSL needs directly from the system keychain on OS X. If we added Linux and Windows support to that library, we may have the opportunity to switch to using certitude.

Given @kennethreitz's bundling policy, we probably cannot unconditionally switch to certitude, because certitude depends on cryptography (at least on OS X). However, certitude could take the current privileged position that certifi takes, or be a higher priority than certifi, as an optional dependency that is used if present on the system.

Thoughts? This is currently a RFC, so please comment if you have opinions. /cc @sigmavirus24 @alex @kennethreitz @dstufft @glyph @reaperhulk @morganfainberg
",Lukasa,reaperhulk
2966,2016-01-11 17:15:47,"It's been raised repeatedly, mostly by people using Linux systems, that it's annoying that requests doesn't use the system trust store and instead uses the one that certifi ships. This is an understandable position. I have some personal attachment to the certifi approach, but the other side of that argument definitely has a reasonable position too. For this reason, I'd like us to look into whether we should use the system trust store by default, and make certifi's bundle a fallback option.

I have some caveats here:
1. If we move to the system trust store, we must do so on _all_ platforms: Linux must not be its own special snowflake.
2. We must have broad-based support for Linux and Windows.
3. We must be able to fall back to certifi cleanly.

Right now it seems like the best route to achieving this would be to use [certitude](https://github.com/python-hyper/certitude). This currently has support for dynamically generating the cert bundle OpenSSL needs directly from the system keychain on OS X. If we added Linux and Windows support to that library, we may have the opportunity to switch to using certitude.

Given @kennethreitz's bundling policy, we probably cannot unconditionally switch to certitude, because certitude depends on cryptography (at least on OS X). However, certitude could take the current privileged position that certifi takes, or be a higher priority than certifi, as an optional dependency that is used if present on the system.

Thoughts? This is currently a RFC, so please comment if you have opinions. /cc @sigmavirus24 @alex @kennethreitz @dstufft @glyph @reaperhulk @morganfainberg
",Lukasa,kennethreitz
2966,2016-01-11 17:15:47,"It's been raised repeatedly, mostly by people using Linux systems, that it's annoying that requests doesn't use the system trust store and instead uses the one that certifi ships. This is an understandable position. I have some personal attachment to the certifi approach, but the other side of that argument definitely has a reasonable position too. For this reason, I'd like us to look into whether we should use the system trust store by default, and make certifi's bundle a fallback option.

I have some caveats here:
1. If we move to the system trust store, we must do so on _all_ platforms: Linux must not be its own special snowflake.
2. We must have broad-based support for Linux and Windows.
3. We must be able to fall back to certifi cleanly.

Right now it seems like the best route to achieving this would be to use [certitude](https://github.com/python-hyper/certitude). This currently has support for dynamically generating the cert bundle OpenSSL needs directly from the system keychain on OS X. If we added Linux and Windows support to that library, we may have the opportunity to switch to using certitude.

Given @kennethreitz's bundling policy, we probably cannot unconditionally switch to certitude, because certitude depends on cryptography (at least on OS X). However, certitude could take the current privileged position that certifi takes, or be a higher priority than certifi, as an optional dependency that is used if present on the system.

Thoughts? This is currently a RFC, so please comment if you have opinions. /cc @sigmavirus24 @alex @kennethreitz @dstufft @glyph @reaperhulk @morganfainberg
",Lukasa,morganfainberg
2966,2016-01-11 17:15:47,"It's been raised repeatedly, mostly by people using Linux systems, that it's annoying that requests doesn't use the system trust store and instead uses the one that certifi ships. This is an understandable position. I have some personal attachment to the certifi approach, but the other side of that argument definitely has a reasonable position too. For this reason, I'd like us to look into whether we should use the system trust store by default, and make certifi's bundle a fallback option.

I have some caveats here:
1. If we move to the system trust store, we must do so on _all_ platforms: Linux must not be its own special snowflake.
2. We must have broad-based support for Linux and Windows.
3. We must be able to fall back to certifi cleanly.

Right now it seems like the best route to achieving this would be to use [certitude](https://github.com/python-hyper/certitude). This currently has support for dynamically generating the cert bundle OpenSSL needs directly from the system keychain on OS X. If we added Linux and Windows support to that library, we may have the opportunity to switch to using certitude.

Given @kennethreitz's bundling policy, we probably cannot unconditionally switch to certitude, because certitude depends on cryptography (at least on OS X). However, certitude could take the current privileged position that certifi takes, or be a higher priority than certifi, as an optional dependency that is used if present on the system.

Thoughts? This is currently a RFC, so please comment if you have opinions. /cc @sigmavirus24 @alex @kennethreitz @dstufft @glyph @reaperhulk @morganfainberg
",Lukasa,dstufft
2966,2016-01-11 17:15:47,"It's been raised repeatedly, mostly by people using Linux systems, that it's annoying that requests doesn't use the system trust store and instead uses the one that certifi ships. This is an understandable position. I have some personal attachment to the certifi approach, but the other side of that argument definitely has a reasonable position too. For this reason, I'd like us to look into whether we should use the system trust store by default, and make certifi's bundle a fallback option.

I have some caveats here:
1. If we move to the system trust store, we must do so on _all_ platforms: Linux must not be its own special snowflake.
2. We must have broad-based support for Linux and Windows.
3. We must be able to fall back to certifi cleanly.

Right now it seems like the best route to achieving this would be to use [certitude](https://github.com/python-hyper/certitude). This currently has support for dynamically generating the cert bundle OpenSSL needs directly from the system keychain on OS X. If we added Linux and Windows support to that library, we may have the opportunity to switch to using certitude.

Given @kennethreitz's bundling policy, we probably cannot unconditionally switch to certitude, because certitude depends on cryptography (at least on OS X). However, certitude could take the current privileged position that certifi takes, or be a higher priority than certifi, as an optional dependency that is used if present on the system.

Thoughts? This is currently a RFC, so please comment if you have opinions. /cc @sigmavirus24 @alex @kennethreitz @dstufft @glyph @reaperhulk @morganfainberg
",Lukasa,alex
2966,2016-01-11 17:15:47,"It's been raised repeatedly, mostly by people using Linux systems, that it's annoying that requests doesn't use the system trust store and instead uses the one that certifi ships. This is an understandable position. I have some personal attachment to the certifi approach, but the other side of that argument definitely has a reasonable position too. For this reason, I'd like us to look into whether we should use the system trust store by default, and make certifi's bundle a fallback option.

I have some caveats here:
1. If we move to the system trust store, we must do so on _all_ platforms: Linux must not be its own special snowflake.
2. We must have broad-based support for Linux and Windows.
3. We must be able to fall back to certifi cleanly.

Right now it seems like the best route to achieving this would be to use [certitude](https://github.com/python-hyper/certitude). This currently has support for dynamically generating the cert bundle OpenSSL needs directly from the system keychain on OS X. If we added Linux and Windows support to that library, we may have the opportunity to switch to using certitude.

Given @kennethreitz's bundling policy, we probably cannot unconditionally switch to certitude, because certitude depends on cryptography (at least on OS X). However, certitude could take the current privileged position that certifi takes, or be a higher priority than certifi, as an optional dependency that is used if present on the system.

Thoughts? This is currently a RFC, so please comment if you have opinions. /cc @sigmavirus24 @alex @kennethreitz @dstufft @glyph @reaperhulk @morganfainberg
",Lukasa,glyph
2966,2016-01-11 17:15:47,"It's been raised repeatedly, mostly by people using Linux systems, that it's annoying that requests doesn't use the system trust store and instead uses the one that certifi ships. This is an understandable position. I have some personal attachment to the certifi approach, but the other side of that argument definitely has a reasonable position too. For this reason, I'd like us to look into whether we should use the system trust store by default, and make certifi's bundle a fallback option.

I have some caveats here:
1. If we move to the system trust store, we must do so on _all_ platforms: Linux must not be its own special snowflake.
2. We must have broad-based support for Linux and Windows.
3. We must be able to fall back to certifi cleanly.

Right now it seems like the best route to achieving this would be to use [certitude](https://github.com/python-hyper/certitude). This currently has support for dynamically generating the cert bundle OpenSSL needs directly from the system keychain on OS X. If we added Linux and Windows support to that library, we may have the opportunity to switch to using certitude.

Given @kennethreitz's bundling policy, we probably cannot unconditionally switch to certitude, because certitude depends on cryptography (at least on OS X). However, certitude could take the current privileged position that certifi takes, or be a higher priority than certifi, as an optional dependency that is used if present on the system.

Thoughts? This is currently a RFC, so please comment if you have opinions. /cc @sigmavirus24 @alex @kennethreitz @dstufft @glyph @reaperhulk @morganfainberg
",Lukasa,sigmavirus24
2966,2016-01-11 17:37:49,"I think the system trust stores (or not) essentially boils down to whether you want requests to act the same across platforms, or whether you want it to act in line with the platform it is running on. I do not think that _either_ of these options are wrong (or right), just different trade offs.

I think that it's not as simple on Windows as it is on Linux or OSX (although @tiran might have a better idea). I _think_ that Windows doesn't ship with all of the certificates available and you have to do something (use WinHTTP?) to get it to download any additional certificates on demand. I think that means that a brand new Windows install, if you attempt to dump the certificate store will be missing a great many certificates.

On Linux, you still have the problem that there isn't one single set location for the certificate files, the best you can do is try to heuristically guess at where it might be. This gets better on Python 2.7.9+ and Python  3.4+ since you can use `ssl.get_default_verify_paths()` to get what the default paths are, but you can't rely on that unless you drop 2.6, 2.7.8, and 3.3. In pip we attempt to discover the location of the system trust store (just by looping over some common file locations) and if we can't find it we fall back to certifi, and one problem that has come up is that sometimes we'll find a location, but it's an old outdated copy that isn't being updated by anything. People then get really confused because it works in their browser, it works with requests, but it doesn't in pip.

I assume the fall back to certifi ensures that things will still work correctly on platforms that either don't ship certificates at all, or don't ship them by default and they aren't installed? If so, that's another possible niggle here that you'd want to think about. Some platforms, like say FreeBSD, don't ship them by default at all. So it's possible that people will have a requests using thing running just fine without the FreeBSD certificates installed, and they then install them (explicitly or implicitly) and suddenly they trust something different and the behavior of the program changes.

Anyways, the desire seems reasonable to me and, if all of the little niggles get worked out, it really just comes down to a question of if requests wants to fall on the side of ""fitting in"" with a particular platform, or if it wants to prefer cross platform uniformity.
",dstufft,tiran
2966,2016-01-11 22:22:45,"> I think that it's not as simple on Windows as it is on Linux or OSX (although @tiran might have a better idea). I think that Windows doesn't ship with all of the certificates available and you have to do something (use WinHTTP?) to get it to download any additional certificates on demand. I think that means that a brand new Windows install, if you attempt to dump the certificate store will be missing a great many certificates.

You are right about this.  I have verified it on my nearly-pristine Windows VM; in a Python prompt, I do:



and get ""21"".  Visit some HTTPS websites, up-arrow/enter in the python interpreter, and now I get ""23"".
",glyph,tiran
2966,2016-01-22 09:41:59,"Credit to @bagder for that approach, especially as I'm just going to steal it.
",Lukasa,bagder
2966,2016-01-29 14:58:56,"@mwcampbell PyOpenSSL does _not_ support this, and never will, because PyOpenSSL is a thin wrapper library around OpenSSL, and so doesn't support the relevant APIs.

However, I recently got merged into cryptography the relevant bindings for OS X (pyca/cryptography#2683), and I believe that a cryptography with that change in it has been released now. I've also used those bindings to successfully use OS X to validate a certificate chain. A similar approach can probably be used on Windows and the cryptography developers have expressed a willingness to bind the appropriate functions. Given that PyOpenSSL depends on cryptography, this is far and away the simplest route.

My current proposal is to add the relevant functionality into the urllib3 PyOpenSSL shim. I've briefly discussed this with @shazow, who was open to the idea. Then, urllib3 would allow `urllib3.contrib.pyopenssl.inject_into_urllib3()` to take a parameter (`system_trust=True`, defaulting to `False`) that will automatically use the system trust store instead of OpenSSL on the relevant platforms.

The question then would become whether the requests project can come to consensus on setting that parameter to `True`. =) We should burn that bridge when we get to it: for now, I'd like to get the building blocks in place.

In an ideal world we'd actually pull the OS-specific logic out into its own library, so that it can be meaningfully tested, but that's a pretty tricky goal. On the other hand, if we can pull it off then we have both provided a really useful service to the Python community in general _and_ potentially helped move towards #2118 by providing a PyOpenSSL `SSLContext` equivalent. This I think would be my preferred outcome.
",Lukasa,shazow
2961,2016-01-05 17:22:26,"As @merwok pointed out at https://github.com/kennethreitz/requests/pull/512#commitcomment-1296187, schemes are what they're called, not schemas. I got tripped up by this at https://github.com/CenterForOpenScience/osf.io/pull/4745#issuecomment-169038417, where ""InvalidSchema"" made me think I was hitting a database error, when in fact it was a configuration error in the format of a URL.

This PR is a redo of #2960 against `proposed/3.0.0` instead of `master`.
",whit537,merwok
2960,2016-01-05 16:30:37,"As @merwok pointed out at https://github.com/kennethreitz/requests/pull/512#commitcomment-1296187, schemes are what they're called, not schemas. I got tripped up by this at https://github.com/CenterForOpenScience/osf.io/pull/4745#issuecomment-169038417, where ""InvalidSchema"" made me think I was hitting a database error, when in fact it was a configuration error in the format of a URL.
",whit537,merwok
2953,2016-04-06 19:14:24,"We need an updated urllib3. I think @shazow is expecting to release this week.
",Lukasa,shazow
2952,2015-12-29 22:06:16,"This is a breaking change that cannot be merged for 2.x but has been fixed for 3.0.0 in #2631 by @neosab.
",sigmavirus24,neosab
2947,2015-12-28 11:20:19,"Ugh, yeah, good spot. This isn't really ideal: while what occurs here is kinda exactly what you'd expect if you have an understanding of the system, it provides a nice shiny footgun for developers who aren't expecting it, potentially allowing for header injection if they're passing user-provided data directly to a different service. We should probably remove it.

In general, I'm inclined to want to throw errors here, resisting the temptation to guess, but @sigmavirus24 or @kennethreitz may disagree and instead want us to simply strip line terminating characters from headers.
",Lukasa,kennethreitz
2947,2015-12-28 11:20:19,"Ugh, yeah, good spot. This isn't really ideal: while what occurs here is kinda exactly what you'd expect if you have an understanding of the system, it provides a nice shiny footgun for developers who aren't expecting it, potentially allowing for header injection if they're passing user-provided data directly to a different service. We should probably remove it.

In general, I'm inclined to want to throw errors here, resisting the temptation to guess, but @sigmavirus24 or @kennethreitz may disagree and instead want us to simply strip line terminating characters from headers.
",Lukasa,sigmavirus24
2941,2015-12-20 10:15:40,"Hi @alison985!

Currently our relatively minor code of conduct is [defined here](http://docs.python-requests.org/en/latest/dev/contributing/). There is interest amongst some of the maintainers in moving to something based on the [Contributor Covenant](http://contributor-covenant.org/), given that both myself and @sigmavirus24 maintain personal projects that use it, but I'm unwilling to change that by fiat: I'd want consensus from the whole team.

Regardless, I believe that the Contributor Covenant _implicitly_ applies to this project, and certainly that's the standard to which the maintainers hold themselves.
",Lukasa,sigmavirus24
2939,2015-12-19 14:34:36,"A web server MUST NOT respond with multiple location header fields. From [RFC 7320 Section 3.2.2](https://tools.ietf.org/html/rfc7230#section-3.2.2):

> A sender MUST NOT generate multiple header fields with the same field name in a message unless either the entire field value for that header field is defined as a comma-separated list [i.e., #(values)] or the header field is a well-known exception (as noted below).

There is exactly _one_ well-known exception: `Set-Cookie`. `Location` is not `Set-Cookie`, so that doesn't apply. So it would only be acceptable to send multiple `Location` headers if the syntax allowed multiple comma-separated values. `Location` is defined in [RFC 7231 Section 7.1.2](https://tools.ietf.org/html/rfc7231#section-7.1.2):

>    Location = URI-reference
> 
> The field value consists of a single URI-reference.

The server in this example is violating the HTTP specification. In the face of such violation, a user-agent is simply not required to take any specific action. In particular, while the example you provided above has a very simple course of action (both URLs are the same), if both URLs were _not_ the same requests would be unable to take any sensible action: the situation is too ambiguous to allow a correct decision.

At the level of requests actually catching this and treating it as an error condition is tricky, because requests only sees the header string `http://foo.com,http://foo.com`, which perversely does parse as a valid URL in the case of most Python libraries because in principle the comma is an allowed character in the host part of the URL. This puts us in a bit of a bind. However, `urllib3`'s `HTTPHeaderDict` structure could enforce this requirement and treat this case as an error on receiving it. That might be a sensible way to go, but it's us going out of our way to provide a slightly nicer error when a server does something it really really shouldn't do. @sigmavirus24, thoughts?
",Lukasa,sigmavirus24
2937,2015-12-18 09:56:33,"This release is scheduled for Monday. I'm aiming to set this up now so that it's an easy release to push on Monday.

@shazow, what are the odds that we can get a urllib3 patch release 1.13.1 containing the fix for shazow/urllib3#761 by Monday? I'd owe you lots of :custard: if we could get it! :heart:
",Lukasa,shazow
2937,2015-12-18 14:19:59,"@sigmavirus24 Yeah, good question, but I think I am, given that the urllib3 release would itself be a patch release. Any concerns there? @ralphbean @eriol?
",Lukasa,ralphbean
2937,2015-12-18 14:19:59,"@sigmavirus24 Yeah, good question, but I think I am, given that the urllib3 release would itself be a patch release. Any concerns there? @ralphbean @eriol?
",Lukasa,eriol
2935,2016-03-09 21:28:17,":cake: for @Kronuz and @Yhg1s
",sigmavirus24,Yhg1s
2931,2015-12-16 15:16:38,"@untitaker has confirmed this fix works for them.
",Lukasa,untitaker
2929,2015-12-16 13:50:15,"@sigmavirus24 You're probably best placed to look at this, but my suspicion is that there's an unexpected interaction between the toolbelt's MultipartEncoder and the fix we put in for calculating the actual length of file-like objects that are not seeked to the beginning. That said, I can't find anything in that.
",Lukasa,sigmavirus24
2927,2015-12-15 14:59:42,"@sigmavirus24 Can you confirm for me that you're happy with these release notes?
",Lukasa,sigmavirus24
2919,2015-12-07 08:07:42,"Yeah, so how we approach this is basically dependent on whether we think it's acceptable for `to_native_string()` to go in `encode_params`. I personally think that's the best place for it, because it allows us to avoid duplicating the large amount of knowledge `encode_params` has about how this variable is structured. However, IIRC @sigmavirus24 originally objected to that idea so I'd like to hear from him.
",Lukasa,sigmavirus24
2915,2015-12-04 08:22:53,"Feedback is welcome! :grinning: 
## Before with @kennethreitz's sphinx theme

![before_kr_sphinx_theme](https://cloud.githubusercontent.com/assets/954858/11584924/f6321290-9a1c-11e5-890c-11a20cf3ff8d.png)
## After with alabaster sphinx theme

![alabaster_theme_for_requests](https://cloud.githubusercontent.com/assets/954858/11672701/a4a17548-9dc7-11e5-8038-d2ca698b8aa9.png)
",ArcTanSusan,kennethreitz
2915,2015-12-04 08:25:56,"\o/ You are a :star: @onceuponatimeforever!

For posterity, this resolves #2779. Based on the quick screenshot provided above it looks like the only thing that gets lost here is that the Gumroad button isn't rendering out correctly.

@kennethreitz, you'll need to review this because the look of the site is an important part of your personal branding, but FWIW I'm :+1: and totally delighted about it.
",Lukasa,kennethreitz
2915,2015-12-04 08:25:56,"\o/ You are a :star: @onceuponatimeforever!

For posterity, this resolves #2779. Based on the quick screenshot provided above it looks like the only thing that gets lost here is that the Gumroad button isn't rendering out correctly.

@kennethreitz, you'll need to review this because the look of the site is an important part of your personal branding, but FWIW I'm :+1: and totally delighted about it.
",Lukasa,onceuponatimeforever
2915,2015-12-04 08:39:11,"Ok, so @onceuponatimeforever, Kenneth's concern about ad-tracking codes is well-founded, they _are_ missing. You can check this by opening up the index.html on the current website and on the newly rendered one, then using the browser debugging tools to inspect the top-level `<body>` tag. Both documents contain the `<div class=""related"">`, `<div class=""document"">`, and `<div class=""footer"">` content tags, which is great, but then the Alabaster version is missing a few things:
- Gumroad JS file: 
  
  
- Flattr JS:
  
  
- CloudFront JS:
  
  
- HelloBar:
  
  
- Google Analytics:
  
  
- Gauges
  
  
- Perfect Audience
  
  

These all need to get re-inserted back in. They previously all lived in the footer, and it should be possible to put them back into the Alabaster footer in some way.
",Lukasa,onceuponatimeforever
2915,2015-12-04 12:51:45,"Thanks @onceuponatimeforever! This looks good to me.

I'm going to sit on it to confirm that @kennethreitz is happy with it. (I'm confident he will be.)
",Lukasa,kennethreitz
2915,2015-12-04 12:51:45,"Thanks @onceuponatimeforever! This looks good to me.

I'm going to sit on it to confirm that @kennethreitz is happy with it. (I'm confident he will be.)
",Lukasa,onceuponatimeforever
2915,2015-12-27 19:07:11,"@onceuponatimeforever I think what @kennethreitz wants is for the column beneath it to be aligned with the `R` in `Requests` under the turtle logo. (At least that's roughly what it looks like where it was from the before picture)
",sigmavirus24,onceuponatimeforever
2912,2015-12-02 08:36:58,"This is not a bug in URL parsing: quite the opposite.

Requests does its best to normalise URLs where it is safe to do so. This is necessary to ensure that various stages of URL building work appropriately, and that we don't accidentally end up with invalid headers (e.g. multiple question marks or none at all when parameters are present). A trailing question mark with no parameters is superfluous (and arguably somewhat invalid), so requests kindly strips it off for you. This is entirely intentional.

For what it's worth, my opinion is that this API is poorly designed. It relies on the idea that no middle-box will rewrite that URL to remove the empty query part, which is a risk. Generally speaking, the better API would be to have a sub-resource or a proper query string (e.g. `/api/item.json?metadata=true`, or `/api/item/metadata.json`). This API design is _super_ fragile, and I guarantee that it'll be prone to breaking in mysterious ways. I doubt we're the only framework that strips it. In fact, anything that relies on Python's standard `urlsplit` function will probably do exactly what we do and ignore the query part.

**However**, both browsers and curl will, if requested, send the empty query string. Having chatted to @bagder I don't think this is a deliberate decision on the part of those entities, so much as it just happens to fall out of the way they handle query strings.

Given that we have a duty to be better than the Python standard library, that means I think we should come up with a way to make it possible, at least when using the PreparedRequest API. Sadly, `urlsplit` makes this _really_ hard for us, because it doesn't have a difference in the query portion of the URL for `http://http2bin.org/get' and`http://http2bin.org/get?`. That makes our lives really tricky because I'd rather not hand roll URL parsing if I can possibly avoid it.

I think the only way we can fix this is to change the library we use to parse URLs to one that can safely inform us of the difference between these two cases. @sigmavirus24, do you think your URL parsing library would do better here?

I think this is a reasonable request, but we can't get to it before 3.0.0 because moving to a new URL parsing library will probably break a whole lot of stuff.
",Lukasa,bagder
2912,2015-12-02 08:36:58,"This is not a bug in URL parsing: quite the opposite.

Requests does its best to normalise URLs where it is safe to do so. This is necessary to ensure that various stages of URL building work appropriately, and that we don't accidentally end up with invalid headers (e.g. multiple question marks or none at all when parameters are present). A trailing question mark with no parameters is superfluous (and arguably somewhat invalid), so requests kindly strips it off for you. This is entirely intentional.

For what it's worth, my opinion is that this API is poorly designed. It relies on the idea that no middle-box will rewrite that URL to remove the empty query part, which is a risk. Generally speaking, the better API would be to have a sub-resource or a proper query string (e.g. `/api/item.json?metadata=true`, or `/api/item/metadata.json`). This API design is _super_ fragile, and I guarantee that it'll be prone to breaking in mysterious ways. I doubt we're the only framework that strips it. In fact, anything that relies on Python's standard `urlsplit` function will probably do exactly what we do and ignore the query part.

**However**, both browsers and curl will, if requested, send the empty query string. Having chatted to @bagder I don't think this is a deliberate decision on the part of those entities, so much as it just happens to fall out of the way they handle query strings.

Given that we have a duty to be better than the Python standard library, that means I think we should come up with a way to make it possible, at least when using the PreparedRequest API. Sadly, `urlsplit` makes this _really_ hard for us, because it doesn't have a difference in the query portion of the URL for `http://http2bin.org/get' and`http://http2bin.org/get?`. That makes our lives really tricky because I'd rather not hand roll URL parsing if I can possibly avoid it.

I think the only way we can fix this is to change the library we use to parse URLs to one that can safely inform us of the difference between these two cases. @sigmavirus24, do you think your URL parsing library would do better here?

I think this is a reasonable request, but we can't get to it before 3.0.0 because moving to a new URL parsing library will probably break a whole lot of stuff.
",Lukasa,sigmavirus24
2912,2015-12-02 11:34:51,"Your understanding is correct, at least as far as the _standard library_ URL parser goes. That is not necessarily _generally_ true of URL parsers, which is why I asked @sigmavirus24 if his URL parser has the same behaviour or not. In particular, it's not clear to me that an absent query part is semantically identical to an _empty_ query part: Appendix A of RFC 3986 does appear to allow zero-length query strings.

Again, I think this is _strictly_ acceptable, but it's likely to be extremely fragile.
",Lukasa,sigmavirus24
2911,2015-12-02 08:39:45,"Hmm. I wonder if this is connection pooling related. @Bl4ckc4t, are you comfortable with wireshark?
",Lukasa,Bl4ckc4t
2911,2015-12-02 14:00:16,"@Bl4ckc4t What I'm worried about is that this may be interacting with our connection re-use logic. This seems the most likely cause of the problem.
",Lukasa,Bl4ckc4t
2911,2015-12-02 15:44:13,"@Bl4ckc4t Does the third one use a new TCP connection, or the same one?
",Lukasa,Bl4ckc4t
2911,2015-12-02 16:12:51,"Wait, @Bl4ckc4t, my understanding is that you _are_ using the `Session` each time, we're just not correctly attaching the headers. You can actually temporarily fix this problem by not using a `Session`.

I'm trying to get an exact reproduction of this problem on my own system. Right now, I'm getting connections that correctly re-use the established tunnel, which is not quite right.
",Lukasa,Bl4ckc4t
2911,2015-12-02 16:21:15,"@Bl4ckc4t Can you check whether the response to the second request sends the `Connection` header, and if so, to what value? I'm trying to work out why the connection is going away.
",Lukasa,Bl4ckc4t
2897,2016-03-01 08:29:25,"I believe this is generally going in the right direction, but now that @kennethreitz is more active he may want to see if this is something he wants. @sigmavirus24 and I are definitely :+1: on this for improving our testing, but it's up to Kenneth.
",Lukasa,kennethreitz
2896,2015-11-24 12:46:04,"@BraulioVM That's actually intentional: sending a chunked zero-length stream is fine, there won't be a problem there. That way, the code is clearer. =)

So, this change looks good to me, but I'll let @sigmavirus24 review it.
",Lukasa,sigmavirus24
2896,2015-12-02 08:09:20,"@BraulioVM Nope, just waiting for @sigmavirus24 to get enough time to swing by and take a look at it. =)
",Lukasa,sigmavirus24
2893,2015-11-20 10:16:18,"@cyberval When you say ""used as a web server"", what does that mean exactly? What would requests do?
",Lukasa,cyberval
2887,2015-11-17 23:15:58,"When we perform a chunked request, we cause a call to `HTTPConnection._new_conn` through `HTTPConnection.connect` which can raise an instance of `urllib3.exceptions.NewConnectionError`.

Code to reproduce



cc @stevelle
",sigmavirus24,stevelle
2887,2015-11-18 13:25:34,"Huh, so this raises the proper exception now. @stevelle and I specifically tested this on 2.8.1 and were seeing the exception from urllib3 bubble up. On master, I see the (expected) `requests.exceptions.ConnectionError` (which wraps the `NewConnectionError` from urllib3).

This is especially baffling because the exception was coming from the line where `endheaders` is called on `low_conn` and the difference between 2.8.1 and HEAD is:



In other words, the exception should be raised before we get to this try/except logic.

I can't imagine this would be system specific, but I'll toy around with the server that @stevelle and I were testing on yesterday to see if I can reproduce this on master there. (I'm using OSX right now, we were using Ubuntu 14.04 inside a virtualenv yesterday.)
",sigmavirus24,stevelle
2886,2015-11-17 15:21:25,"This is definitely a duplicate but I don't have the time to find the exact bug right now. The fix is already merged in urllib3 iirc but @Lukasa can correct me.
",sigmavirus24,Lukasa
2885,2015-11-17 10:10:45,"@Wiryono That does not match what the urlencoded payload you copied decodes to. In fact, your urlencoded payload corresponds to `config={'data': 'test'}`, which is a JSON-serialized representation of the nested data structure. To get that, you'd want to use:



That will work appropriately.
",Lukasa,Wiryono
2872,2015-11-10 22:28:56,"Thanks for this report. This comes because we calculate the length of the StringIO, but don't account for where it is seeked to. 

As I see it we have two options: we can either always `seek(0)` before sending a body, or we can use `tell()` to adjust the content length. 

On balance, I think the first is probably better: we already seek(0) in some cases (e.g. auth handlers), so this would be consistent. The downside is that it makes it harder for someone to upload a partial file from disk, though I suspect that's a minor use-case. 

@sigmavirus24 thoughts?
",Lukasa,sigmavirus24
2870,2015-11-10 12:49:34,"Originally reported by @fasaxc. Interested parties: @sigmavirus24 @dcramer @eriol @ralphbean @warsaw 

We added support for a new form of unbundling logic in #2567. Unfortunately, this seems not to work properly. Specifically, the presence of requests in a system makes it impossible to catch urllib3 exceptions correctly: see [this Stack Overflow question](https://stackoverflow.com/questions/33516164/why-cant-i-catch-this-python-exception) for more.

This problem seems to specifically affect the urllib3 sub-packages, not urllib3 itself. One proposed change is to add every urllib3 sub-package that gets imported to the list of modules requests explicitly imports in this stub file. This is pretty gross, but might work. Does anyone have other suggestions?
",Lukasa,fasaxc
2870,2015-11-10 12:49:34,"Originally reported by @fasaxc. Interested parties: @sigmavirus24 @dcramer @eriol @ralphbean @warsaw 

We added support for a new form of unbundling logic in #2567. Unfortunately, this seems not to work properly. Specifically, the presence of requests in a system makes it impossible to catch urllib3 exceptions correctly: see [this Stack Overflow question](https://stackoverflow.com/questions/33516164/why-cant-i-catch-this-python-exception) for more.

This problem seems to specifically affect the urllib3 sub-packages, not urllib3 itself. One proposed change is to add every urllib3 sub-package that gets imported to the list of modules requests explicitly imports in this stub file. This is pretty gross, but might work. Does anyone have other suggestions?
",Lukasa,warsaw
2870,2015-11-10 12:49:34,"Originally reported by @fasaxc. Interested parties: @sigmavirus24 @dcramer @eriol @ralphbean @warsaw 

We added support for a new form of unbundling logic in #2567. Unfortunately, this seems not to work properly. Specifically, the presence of requests in a system makes it impossible to catch urllib3 exceptions correctly: see [this Stack Overflow question](https://stackoverflow.com/questions/33516164/why-cant-i-catch-this-python-exception) for more.

This problem seems to specifically affect the urllib3 sub-packages, not urllib3 itself. One proposed change is to add every urllib3 sub-package that gets imported to the list of modules requests explicitly imports in this stub file. This is pretty gross, but might work. Does anyone have other suggestions?
",Lukasa,ralphbean
2870,2015-11-10 12:49:34,"Originally reported by @fasaxc. Interested parties: @sigmavirus24 @dcramer @eriol @ralphbean @warsaw 

We added support for a new form of unbundling logic in #2567. Unfortunately, this seems not to work properly. Specifically, the presence of requests in a system makes it impossible to catch urllib3 exceptions correctly: see [this Stack Overflow question](https://stackoverflow.com/questions/33516164/why-cant-i-catch-this-python-exception) for more.

This problem seems to specifically affect the urllib3 sub-packages, not urllib3 itself. One proposed change is to add every urllib3 sub-package that gets imported to the list of modules requests explicitly imports in this stub file. This is pretty gross, but might work. Does anyone have other suggestions?
",Lukasa,dcramer
2870,2015-11-10 12:49:34,"Originally reported by @fasaxc. Interested parties: @sigmavirus24 @dcramer @eriol @ralphbean @warsaw 

We added support for a new form of unbundling logic in #2567. Unfortunately, this seems not to work properly. Specifically, the presence of requests in a system makes it impossible to catch urllib3 exceptions correctly: see [this Stack Overflow question](https://stackoverflow.com/questions/33516164/why-cant-i-catch-this-python-exception) for more.

This problem seems to specifically affect the urllib3 sub-packages, not urllib3 itself. One proposed change is to add every urllib3 sub-package that gets imported to the list of modules requests explicitly imports in this stub file. This is pretty gross, but might work. Does anyone have other suggestions?
",Lukasa,eriol
2870,2015-11-10 12:49:34,"Originally reported by @fasaxc. Interested parties: @sigmavirus24 @dcramer @eriol @ralphbean @warsaw 

We added support for a new form of unbundling logic in #2567. Unfortunately, this seems not to work properly. Specifically, the presence of requests in a system makes it impossible to catch urllib3 exceptions correctly: see [this Stack Overflow question](https://stackoverflow.com/questions/33516164/why-cant-i-catch-this-python-exception) for more.

This problem seems to specifically affect the urllib3 sub-packages, not urllib3 itself. One proposed change is to add every urllib3 sub-package that gets imported to the list of modules requests explicitly imports in this stub file. This is pretty gross, but might work. Does anyone have other suggestions?
",Lukasa,sigmavirus24
2870,2015-11-10 14:29:56,"So 2.6.x has the old VendorAlias logic from pip. 2.7.0 had nothing but some distros (Debian specifically, more specifically @warsaw) backported the patch from @untitaker that was released in 2.8.0. So version information is **very** important here.
",sigmavirus24,warsaw
2870,2015-11-10 14:29:56,"So 2.6.x has the old VendorAlias logic from pip. 2.7.0 had nothing but some distros (Debian specifically, more specifically @warsaw) backported the patch from @untitaker that was released in 2.8.0. So version information is **very** important here.
",sigmavirus24,untitaker
2870,2015-11-10 15:19:41,"Ubuntu may have pulled in Debian's patch from @warsaw. In that case, this is still relevant. (Alternatively, Ubuntu may have continued using the VendorAlias from 2.6.x and we should check that.)
",sigmavirus24,warsaw
2870,2015-11-10 15:26:00,"If it turns out that Debian actually does have the new logic, I'd say it's a good time to think about actually giving up on vendoring urllib3.

On 10 November 2015 16:20:13 CET, Ian Cordasco notifications@github.com wrote:

> Ubuntu may have pulled in Debian's patch from @warsaw. In that case,
> this is still relevant. (Alternatively, Ubuntu may have continued using
> the VendorAlias from 2.6.x and we should check that.)
> 
> ---
> 
> Reply to this email directly or view it on GitHub:
> https://github.com/kennethreitz/requests/issues/2870#issuecomment-155449214

## 

Sent from my phone. Please excuse my brevity.
",untitaker,warsaw
2870,2015-11-10 15:42:53,"On Nov 10, 2015, at 07:20 AM, Ian Cordasco wrote:

> Ubuntu may have pulled in Debian's patch from @warsaw. In that case, this is
> still relevant. (Alternatively, Ubuntu may have continued using the
> VendorAlias from 2.6.x and we should check that.)

Ubuntu tracks the Debian version.  There are no deltas, except for some
additional fixes backported to the Trusty version.

% rmadison -u debian requests
requests   | 0.12.1-1~bpo60+1 | squeeze-backports | source
requests   | 0.12.1-1+deb7u1  | oldstable         | source
requests   | 2.0.0-1~bpo70+2  | wheezy-backports  | source
requests   | 2.4.3-6          | stable            | source
requests   | 2.4.3-6          | stable-kfreebsd   | source
requests   | 2.8.1-1          | testing           | source
requests   | 2.8.1-1          | unstable          | source

% rmadison -u ubuntu requests
 requests | 0.8.2-1          | precise/universe | source
 requests | 2.2.1-1          | trusty           | source
 requests | 2.2.1-1ubuntu0.2 | trusty-security  | source
 requests | 2.2.1-1ubuntu0.3 | trusty-updates   | source
 requests | 2.4.3-6          | vivid            | source
 requests | 2.7.0-3          | wily             | source
 requests | 2.7.0-3          | xenial           | source
 requests | 2.8.1-1          | xenial-proposed  | source
",warsaw,warsaw
2870,2015-11-10 15:51:23,"On Nov 10, 2015, at 06:24 AM, Cory Benfield wrote:

> Oh that's a good point, did this get backported into the distros?

Don't forget, @eriol is really doing most of excellent work on this package
for Debian!

Our 2.8.1-1 removed the devendorizing patch we were carrying separately, so
these days we are really only carrying a few deltas from upstream:

https://anonscm.debian.org/cgit/python-modules/packages/requests.git/tree/debian/patches

Of course, we prefer to reduce the differences between the Debian version and
upstream.
",warsaw,eriol
2870,2015-11-10 15:54:56,"Lukasa, who do you mean exactly when you say ""the whole team""? Isn't Kenneth some sort of BDFL, so his decision would suffice?

On 10 November 2015 16:51:56 CET, Barry Warsaw notifications@github.com wrote:

> On Nov 10, 2015, at 06:24 AM, Cory Benfield wrote:
> 
> > Oh that's a good point, did this get backported into the distros?
> 
> Don't forget, @eriol is really doing most of excellent work on this
> package
> for Debian!
> 
> Our 2.8.1-1 removed the devendorizing patch we were carrying
> separately, so
> these days we are really only carrying a few deltas from upstream:
> 
> https://anonscm.debian.org/cgit/python-modules/packages/requests.git/tree/debian/patches
> 
> Of course, we prefer to reduce the differences between the Debian
> version and
> upstream.
> 
> ---
> 
> Reply to this email directly or view it on GitHub:
> https://github.com/kennethreitz/requests/issues/2870#issuecomment-155460186

## 

Sent from my phone. Please excuse my brevity.
",untitaker,eriol
2870,2015-11-11 09:11:32,"@Lukasa I'm sad to confirm but yes, the problem actually exists in Debian testing (with the new import machinery).
I tried the same setup of https://github.com/kennethreitz/requests/issues/2867 using only system packages (so I took python-etcd from sid) and I can confirm it.
I looked at python-etcd code, but the problem was already explained  by @fasaxc and is about exceptions.

On Debian this is what I got:



I'm real sorry I did not noticed when we discussed https://github.com/kennethreitz/requests/pull/2567.

I'm not neither an import logic expert, but your proposal seems the only way to fix this, without unvendoring. Maybe we can ask to @brettcannon if he can take a look at this: I can volunteer to recap all the story so far.

Also, I just want to say thanks to you, @sigmavirus24 and @kennethreitz because although you can just mark this as wontfix, we started working together talking without forget that we are all human beings. 
So, thank you.
",eriol,kennethreitz
2870,2015-11-11 09:11:32,"@Lukasa I'm sad to confirm but yes, the problem actually exists in Debian testing (with the new import machinery).
I tried the same setup of https://github.com/kennethreitz/requests/issues/2867 using only system packages (so I took python-etcd from sid) and I can confirm it.
I looked at python-etcd code, but the problem was already explained  by @fasaxc and is about exceptions.

On Debian this is what I got:



I'm real sorry I did not noticed when we discussed https://github.com/kennethreitz/requests/pull/2567.

I'm not neither an import logic expert, but your proposal seems the only way to fix this, without unvendoring. Maybe we can ask to @brettcannon if he can take a look at this: I can volunteer to recap all the story so far.

Also, I just want to say thanks to you, @sigmavirus24 and @kennethreitz because although you can just mark this as wontfix, we started working together talking without forget that we are all human beings. 
So, thank you.
",eriol,brettcannon
2870,2015-11-11 09:32:34,"@eriol Thanks for investigating. =) And thanks for working with us on this. Again, I can't reiterate enough, it's really important to us to get to a place where everyone's getting a good experience. We're not there yet, but we'll keep trying.

I'd be delighted to see if someone who knows the import machinery really well can propose a better solution to this problem. If you're willing to do the legwork on ramping up @brettcannon that would be even better @eriol.

Otherwise, we should aim to take that boring manual step or see if we can avoid the problem in requests in some other way.
",Lukasa,brettcannon
2870,2015-11-17 14:14:16,"So I'm enumerating the options and asking questions to make sure I understand correctly.

> you can just patch every module in `urllib3`

As in, ensuring that `sys.modules` has both `urllib3.submodule` and `requests.packages.urllib3.submodule`? I think this is the least magic way and given that @Lukasa and I are both core developers of urllib3, we will catch any new submodules that need to be added to requests' patching logic. I'm most strongly in favor of this one.

> Another option is a custom importer that handles just this case, but I don't know if you want such a magical solution.

We had that option and it broke many a thing. I'm not in favor of going back down that route.

> insert an object into `sys.modules` who uses `__getattribute__` to direct to the proper module and handle all magical tweaks to `sys.modules`. 

Hm. This sounds like something in the vein of option 2 but a little less magic-y. It still feels like some abuse of the module system and like it might cause us problems. If we come up with something like this, I would appreciate it if @fasaxc and @eriol would commit to testing this with python-etcd and making sure things still aren't broken before shipping a release with it.

> And lastly, the simplest solution is either let Debian handle their own desire to monkeypatch vendored code or stop vendoring stuff entirely.

Breaking this into two sub-suggestions:
- ""let Debian handle their own desire to monkey patch vendored code"" No. I don't want @eriol to have to figure that out. Vendoring is a position of this project. As much as I dislike downstreams unvendoring things, it isn't the individual maintainer's fault or decision and I don't want to push this work onto their backs.
- ""stop vendoring stuff entirely"" Many people constantly badger (or even, at times, yell) this at us (I know you're not Brett) but it simply just won't happen like that (if it happens at all).

---

My 2¢:

We can take a short term solution of the first option for a `2.8.2` release and get that updated patch to @eriol, @warsaw, @ralphbean, etc. so distros can fix their packages while taking a look at the feasibility and reliability of option 3. I still prefer the first option to the third, but the third may have benefits I'm not seeing due to the bad taste left in my mouth by the `VendorAlias` (a.k.a., option 2).
",sigmavirus24,ralphbean
2870,2015-11-17 15:36:29,"On Nov 17, 2015, at 06:14 AM, Ian Cordasco wrote:

> As in, ensuring that `sys.modules` has both `urllib3.submodule` and
> `requests.packages.urllib3.submodule`? I think this is the least magic way
> and given that @Lukasa and I are both core developers of urllib3, we will
> catch any new submodules that need to be added to requests' patching
> logic. I'm most strongly in favor of this one.

It's mildly disconcerting that a library would fiddle with another library's
sys.modules namespace, but I agree that this is probably the least magical
(and thus most likely to work) way.  Maybe we can convince @brettcannon to
build a nicer foolproof <wink> module alias system for 3.6. :)

> We can take a short term solution of the first option for a `2.8.2` release
> and get that updated patch to @eriol, @warsaw, @ralphbean, etc. so distros
> can fix their packages while taking a look at the feasibility and reliability
> of option 3. I still prefer the first option to the third, but the third may
> have benefits I'm not seeing due to the bad taste left in my mouth by the
> `VendorAlias` (a.k.a., option 2).

Yep, I suspect that #1 may be the best approach, and that #3 would be
difficult to debug if things go south.  It may be that no solution is perfect,
given Python's current import semantics and implementation, in which case
doing the best you can with the least magic (i.e. most discoverable and
debuggable) would suck less.

Thanks!
",warsaw,ralphbean
2869,2015-11-10 11:42:39,"Good idea @vienno!

If we're going to un-nest this, let's un-nest it the whole way. Want to break out the next nested couplet as well and use the same logic?
",Lukasa,vienno
2869,2015-11-10 13:01:03,"Looks good to me @vienno! Would you like to add yourself to AUTHORS.rst?
",Lukasa,vienno
2869,2015-11-10 13:20:59,"Thanks so much @vienno! :sparkles: :cake: :sparkles:
",Lukasa,vienno
2868,2015-11-09 15:06:41,"If I know @jamielennox, they're working on SAML auth plugin. Part of (some of) the SAML authentication flow(s) includes cookies being sent that aren't meant to be persisted on the session.

I think what @jamielennox wants is for those cookies that are meant to be part of an intermediate step in authentication not to be persisted. After a series of redirects, we process everything and update the Session cookie jar with the cookies from individual responses in the history. I suspect @jamielennox was removing the cookies individually from the intermediate responses hoping that would prevent them from being persisted on the Session but that isn't working because we don't use those attributes when extracting cookies to a session cookie jar. (Further, this is likely a plugin for OpenStack.)

I don't think there is necessarily a bug here. We're doing the right thing for the 99% case.

@jamielennox I suspect the cookie names are predictable and you could have a CookieJar that ignores those cookie values (along the lines of a far more selective ForgetfulCookieJar which @ceaess is adding to the requests-toolbelt).
",sigmavirus24,ceaess
2863,2015-11-06 17:16:29,"So, @shazow, I think really we need to adjust the connection pooling logic in urllib3 a bit. Keying connections off the URL isn't really enough: for HTTPS connections we need to key them off the various connection state parameters as well (cert_reqs, ca_certs, ca_cert_dir, client cert parameters). That's pretty gross because it's a major API change.

Thoughts?
",Lukasa,shazow
2861,2015-11-07 10:21:08,"I honestly don't know what to do about this. To get a test that confirms we don't break this we need either to change the WSGI server used by pytest-httpbin (the only options are ones that are _really_ heavyweight and probably not suitable) or start writing new tests that don't use a httpbin at all.

The second is plenty do-able, it just requires some py.test test fixtures and some socket work. But it makes me a bit nervous given @kennethreitz's attitude to changing our test layout in the past: I'd want to move all the tests to a `test/` subdirectory so that I can add the necessary `conf.py` file, and also start writing new tests that explicitly use sockets rather than a web server.

@kennethreitz, do you have any objection to me adding some more thorough testing around here? The reality is that our chunked upload logic is _entirely_ untested, and has been since it was originally written. That is a recipe for accidental breakage, and it makes me really nervous.
",Lukasa,kennethreitz
2840,2015-10-29 02:59:53,"@Lukasa I would concur given the tests/evidence so far. Perhaps @hpk42 could shed some light on it
",koobs,hpk42
2836,2015-10-20 14:46:59,"Ah, yes, I see the problem now.

We have a similar issue with proxy settings, where proxies set in the environment can override proxies set on the session. I'm beginning to wonder if we can fix two problems in one go here, by taking the `trust_env` block of `merge_environment_settings` and moving it below the `merge_setting` lines in that function.

@causton81 This is definitely a real bug, but we need a bit of time to work out what the fix is and what branch it'll go on to (while this is strictly a bug fix, it's been in the product long enough that it's also an API compatibility break and needs to be dealt with cautiously).

@sigmavirus24 What do you think about my proposed rearrangement of `merge_environment_settings`?
",Lukasa,sigmavirus24
2833,2015-10-16 19:02:27,"@causton `httplib` should do this check by itself already. Can you try to reproduce this to see if that happens? If it doesn't, can you post your repro code here?
",Lukasa,causton
2833,2015-10-17 08:40:29,"Iiiinteresting. So @causton81, I think you're right. Diving into the `httplib` source code shows this:



So, I think you're right and this doesn't work. I think this would be best implemented in `urllib3`, frankly: in urllib3 they can take advantage of the `HTTPResponse.length` parameter that `httplib` maintains (but is weirdly refusing to use) to make sure this works in almost all cases. @shazow does that sound reasonable to you?
",Lukasa,shazow
2829,2015-10-15 14:02:13,"This seems like a sensible change to me, I'm :+1:. @sigmavirus24?
",Lukasa,sigmavirus24
2825,2015-10-14 12:24:27,"Resolves #2725.

@sigmavirus24 I'm wondering if we should add a warning to `super_len` for whenever we decide on the length of the file using `os.fstat`, if the file has not been opened in binary mode. It should be easy enough to do. Thoughts?
",Lukasa,sigmavirus24
2822,2015-10-12 16:36:11,"The tag not existing is something @Lukasa  can fix immediately
",sigmavirus24,Lukasa
2821,2015-10-12 09:52:41,"@sigmavirus24 I'd like to slip this into 2.8.1, which I plan to release tomorrow. Mind doing a quick review before then?
",Lukasa,sigmavirus24
2818,2015-10-14 20:00:43,"@netanelkl reported this so it's probably better if he explains, I haven't had time to test this
",jorilallo,netanelkl
2818,2015-12-01 22:27:48,"Yup. We're on track, but @shazow is taking a well-deserved birthday break so it'll be a little bit. 
",Lukasa,shazow
2816,2015-10-09 14:48:30,"/cc @ralphbean @eriol

OpenStack has encountered yet another breakage caused by people combining `pip` installed and system packages: specifically, updating urllib3 via `pip` when requests/urllib3 are already installed via the system packages.

@dstufft tells me that, if the system packages correctly populate the `install_requires` part of `setup.py`, `pip` would respect those requirements. Previously this wasn't possible because requests would include mid-release versions of urllib3, but as of v2.6.2 our [new release rules](http://docs.python-requests.org/en/latest/community/release-process/) mean that we only use proper urllib3 releases.

For this reason, I think it would be extremely good if downstream unbundlers (most notably Fedora and Debian), when they unbundle requests, would populate setup.py with the correct `install_requires`. For 2.8.0, that would be `urllib3==1.12`. For 2.7.0, that would be `urllib3==1.10.4`. This should ensure that `pip` won't casually install a version of urllib3 that will break the system requests package.

Thoughts?
",Lukasa,ralphbean
2816,2015-10-09 14:48:30,"/cc @ralphbean @eriol

OpenStack has encountered yet another breakage caused by people combining `pip` installed and system packages: specifically, updating urllib3 via `pip` when requests/urllib3 are already installed via the system packages.

@dstufft tells me that, if the system packages correctly populate the `install_requires` part of `setup.py`, `pip` would respect those requirements. Previously this wasn't possible because requests would include mid-release versions of urllib3, but as of v2.6.2 our [new release rules](http://docs.python-requests.org/en/latest/community/release-process/) mean that we only use proper urllib3 releases.

For this reason, I think it would be extremely good if downstream unbundlers (most notably Fedora and Debian), when they unbundle requests, would populate setup.py with the correct `install_requires`. For 2.8.0, that would be `urllib3==1.12`. For 2.7.0, that would be `urllib3==1.10.4`. This should ensure that `pip` won't casually install a version of urllib3 that will break the system requests package.

Thoughts?
",Lukasa,eriol
2816,2015-10-09 14:48:30,"/cc @ralphbean @eriol

OpenStack has encountered yet another breakage caused by people combining `pip` installed and system packages: specifically, updating urllib3 via `pip` when requests/urllib3 are already installed via the system packages.

@dstufft tells me that, if the system packages correctly populate the `install_requires` part of `setup.py`, `pip` would respect those requirements. Previously this wasn't possible because requests would include mid-release versions of urllib3, but as of v2.6.2 our [new release rules](http://docs.python-requests.org/en/latest/community/release-process/) mean that we only use proper urllib3 releases.

For this reason, I think it would be extremely good if downstream unbundlers (most notably Fedora and Debian), when they unbundle requests, would populate setup.py with the correct `install_requires`. For 2.8.0, that would be `urllib3==1.12`. For 2.7.0, that would be `urllib3==1.10.4`. This should ensure that `pip` won't casually install a version of urllib3 that will break the system requests package.

Thoughts?
",Lukasa,dstufft
2816,2015-10-11 15:21:27,":heart: Indeed, thanks so much (and thanks @warsaw and @dstufft as well!).
",Lukasa,dstufft
2812,2015-10-08 13:11:04,"@sigmavirus24 How do you feel about removing this in 3.0.0?
",Lukasa,sigmavirus24
2811,2015-10-08 13:06:18,"Also cc'ing @shazow 
",sigmavirus24,shazow
2809,2015-10-08 07:08:34,"I think the core issue here is that requests' certificate store is somewhat out of date. I propose we ship a 2.8.1 containing a cert store update, specifically the `weak.pem` bundle that ships with certifi.

@hombremuchacho Someone I presume was your colleague or another team member (@aehlke) suggested that certifi was missing a certificate you needed, so you couldn't use it as a drop-in replacement. Did you try using `certifi.old_where()`?
",Lukasa,aehlke
2808,2015-10-07 21:15:55,"I could be mistaken but I'm rather certain that we don't accept additions to the certificate bundle directly. They have to go through [certifi](/certifi/python-certifi) first. @Lukasa am I right?
",sigmavirus24,Lukasa
2807,2015-10-07 16:03:46,"It's certainly the _coded_ behaviour: if you don't use something that goes through `Session.request` then you never get the environment variables.

However, I think this is unexpected. It doesn't seem like there's any reason we couldn't move the call to `Session.merge_environment` into `Session.send`, so I'm inclined to say we should do that. Unfortunately, it's a bit backward-incompatible IMO, so I'd want to sit on this until 2.9.0 or, more likely, 3.0.0. Thoughts @sigmavirus24?
",Lukasa,sigmavirus24
2804,2015-10-05 16:13:21,"@sigmavirus24 Happy for you to merge this when you think things look good.
",Lukasa,sigmavirus24
2797,2015-10-02 08:29:38,"Updated Documentation as mentioned by @taion in #2756 :
`data` as well as `files` supersedes json
",sumitbinnani,taion
2791,2015-11-27 15:31:17,"@frenchy75 Are you sure it's the same issue? The issue from the original post was nothing to do with TLS, and it was a coding error in their software. What problem _exactly_ are you seeing?
",Lukasa,frenchy75
2786,2015-09-27 00:47:24,"Paging @shazow to make sure we don't step on any toes here with urllib3 by accident.
",sigmavirus24,shazow
2785,2015-09-26 23:40:12,"Eh, I'm 50/50 on this. I agree the default user-agent isn't ideal. I don't agree that this is a large problem. I'd like to hear @Lukasa's opinion on this but he's away on vacation (as I should be).
",sigmavirus24,Lukasa
2785,2015-09-30 15:33:26,"My general position on this is that removing the kernel version is a good idea and we should do it. Other UA strings do not include it, and it's unlikely to be of much use in any role other than attacking a Linux kernel directly, so I'd be happy to strip it.

I'm +0 on removing the Python version, though we need to confirm with @dstufft that pip's not relying on our use of it. I don't believe it's a serious attack vector, but neither is it information that it's vital to be sending in the UA string.

IMO the biggest security risk in there is actually the _requests_ version, and that's the one thing it's hard to justify removing. ;)
",Lukasa,dstufft
2773,2015-09-12 18:35:48,"@jwilk This is certainly a thing we could do. However, it's a bizarrely application specific fix that will not actually help in a lot of cases because applications that use requests will need to opt-in to that functionality. This means they need to know enough to do that, which is not likely.

Really from a security perspective we should switch to disable netrc auth by default (a change that would need to wait until a 3.0.0 release because it's backwards incompatible, though potentially something worth doing).

In the short term, you will get more security either by not using `~/.netrc` files at all (thereby removing the source of the vulnerability altogether) or by constructing AppArmor profiles that limit access to the file to those applications you have pre-authorized to use it.

@sigmavirus24 For the longer term, I'm open to swapping our default here, which is arguably somewhat insecure, though I also just think people shouldn't be writing their passwords down anywhere at all, at least not in plaintext.
",Lukasa,sigmavirus24
2771,2015-09-11 16:40:54,"This is not a totally unreasonable thing to do, though it's totally tragic that we need it: if we didn't support Python 2.6 this wouldn't be a problem. @sigmavirus24?
",Lukasa,sigmavirus24
2771,2015-11-05 10:05:49,"@shazow If I open a PR that does this for urllib3, would you merge it?
",Lukasa,shazow
2768,2015-09-10 21:11:52,"I certainly want the change in `super` removed: as pointed out in the diff comment by @jmoldow, that isn't valid. Otherwise, I suspect it makes the class somewhat more subclass-friendly, which I don't object to. 
",Lukasa,jmoldow
2763,2015-09-08 16:37:08,"Ugh, you're right, our code sets data to the empty dict. Nevermind, I rescind that feedback. This LGTM, @sigmavirus24?
",Lukasa,sigmavirus24
2763,2015-10-02 07:29:54,"The documentation also needs to be updated.
The `files` also supersedes `json` as already pointed out by @taion in comments of #2756 
",sumitbinnani,taion
2741,2015-08-25 07:26:40,"This looks reasonable enough to me.

Thoughts @sigmavirus24?
",Lukasa,sigmavirus24
2738,2015-08-21 08:32:55,"This is strictly a cryptography bug, I think: @reaperhulk?
",Lukasa,reaperhulk
2722,2015-08-14 19:34:13,"Ok, I think that approach sounds reasonable to me: I think this feature request requires relatively little code and enables a fairly useful use-case. Thoughts from the other maintainers? /cc @kennethreitz @sigmavirus24
",Lukasa,kennethreitz
2722,2015-08-14 19:34:13,"Ok, I think that approach sounds reasonable to me: I think this feature request requires relatively little code and enables a fairly useful use-case. Thoughts from the other maintainers? /cc @kennethreitz @sigmavirus24
",Lukasa,sigmavirus24
2721,2015-08-15 15:36:00,"@tlc would you like to test this before we merge it?
",sigmavirus24,tlc
2707,2016-09-23 06:43:09,"Indeed.  I cant see any problems with the output using 2.11.1, with that website.



@shawnmjones, maybe there are other websites to test with for compliance?
",jayvdb,shawnmjones
2699,2015-07-30 21:28:40,"@sigmavirus24 Sounds good to me.

Separately, urllib3 may want a fix for this, because it is _also_ affected. /cc @shazow
",Lukasa,shazow
2699,2015-08-31 07:00:46,"@sigmavirus24 Ah, crap, that doesn't work. Observe the majesty of Python:



I wonder if we should explicitly disallow anything that compares numerically equal to zero, because setting the socket to non-blocking mode is a wacky and bizarre thing to want to do. Alternatively, we could coerce anything that compares numerically equal to zero to `None` instead, which makes the concrete assertion that when you said `timeout=0` or `timeout=False` what you meant was ""don't time out"".

@shazow @kevinburke It's my assertion that we shouldn't let anyone set `0` as their timeout for either connection or read timeouts, because it doesn't mean what they think it means. We should either coerce to `None` or reject outright. Do either of you disagree?
",Lukasa,kevinburke
2688,2015-07-24 06:55:41,"Ah, I see, the problem is that there's a non-ascii character in the location that requests is installed in. That makes sense.

So the path we generate is generated by doing `os.path.join(os.path.dirname(__file__), 'cacert.pem')`. Given that requests hasn't touched this at all, it looks like it's possible to get this wrong.

I'd like you to quickly `cd` into the directory that wakatime is in, and run the following in a Python interpreter and show me the output (censoring your username):



Right now this looks to me like the SSL library is not capable of handling Windows paths properly. @tiran?
",Lukasa,tiran
2688,2015-07-24 07:12:03,"Right, so this error seems to be entirely in the stdlib. For some reason we're not able to pass a path that we got from the stdlib _to_ the stdlib. I'm hoping that when @tiran wakes up he'll confirm that I'm right, and then we can chase this up in the stdlib.
",Lukasa,tiran
2685,2015-07-22 00:26:44,"As a workaround, the `@stopit.threading_timeoutable` seems to work.
",boolbag,stopit
2684,2015-07-22 05:59:26,"@sigmavirus24 the link you posted is interesting, but I don't see that it addresses:

> For instance, we can no longer return the response to the caller then inspect status_code.

The linked issue had an insightful explanation by @jvanasco of why this feature is needed, then no response... The issue was closed.

If `requests` means to live up to **http for humans**, then... (quoting the first post):

In keeping with simple is beautiful, it would be terrific if we could do:
`requests.get(url, max_response_size=1024*1024)`
",boolbag,jvanasco
2682,2015-07-21 11:30:38,"Thanks for the suggestion!

The supported way to do this in requests is to use a HTTP adapter. I notice you've spotted this but consider it to be a 'hack'. It's not: the requests project considers the HTTP adapter a first-class part of our interface.

The change is very simple:



This is a very simple adapter and could easily be dropped in. We may even want to add it to the requests-toolbelt: a pull request there would be very welcome. (/cc @sigmavirus24)

Thanks for the suggestion!
",Lukasa,sigmavirus24
2678,2015-07-18 16:20:06,"Here's hoping http://ci.sigmavir.us/job/requests-real/ picks up on this. @kennethreitz's CI is not picking up on the new commits.
",sigmavirus24,kennethreitz
2678,2015-07-29 15:39:00,"what does @mitsuhiko think?
",kennethreitz,mitsuhiko
2676,2015-07-17 10:48:21,"The change you've made is in urllib3. @shazow may want this change, in which case we should accept it over at the main urllib3 repository. If he does not want it, we should instead move it into the requests code.
",Lukasa,shazow
2674,2015-07-17 08:33:52,"Partially resolves #1572: ""urllib3 exceptions passing through requests
API"". #1572 

Inspired from @Lukasa's previous 2605be11d82d42438ac7c3993810c955bde74cef.

This is my first PR to requests library; feel free to give me feedback. I generally have a fast response time. Also, available on IRC and Twitter (@ArcTanSusan).
",ArcTanSusan,ArcTanSusan
2674,2015-07-17 08:33:52,"Partially resolves #1572: ""urllib3 exceptions passing through requests
API"". #1572 

Inspired from @Lukasa's previous 2605be11d82d42438ac7c3993810c955bde74cef.

This is my first PR to requests library; feel free to give me feedback. I generally have a fast response time. Also, available on IRC and Twitter (@ArcTanSusan).
",ArcTanSusan,Lukasa
2674,2015-07-17 08:41:24,"Aw no, we lost my awesome work?

![image](https://cloud.githubusercontent.com/assets/1382556/8743585/e94a1910-2c67-11e5-9387-f09a00a9b26c.png)

All joking aside, this looks good to me, I'd be happy to merge it. However, it's an API change, so it _at least_ needs to go into 2.8.0 and may need to go into 3.0.0. @sigmavirus24?
",Lukasa,sigmavirus24
2670,2015-07-15 16:42:19,"We will not be turning on Travis CI. ci.kennethreitz.org just needs a proper kick. /cc @kennethreitz 
",sigmavirus24,kennethreitz
2666,2015-07-13 13:29:33,"@Lukasa how does this look to you? Looks fine to me. I don't mind a couple extra tests personally.
",sigmavirus24,Lukasa
2661,2015-07-03 16:05:07,"/cc @shazow 
",Lukasa,shazow
2656,2015-06-29 04:02:25,"Cool. LGTM. I'll let @Lukasa weigh in.

Thanks @dpursehouse 
",sigmavirus24,Lukasa
2655,2015-06-28 15:58:44,"Resolves #2653.

This is one of those annoying changes that's almost impossible to test because of the sheer complexity of our redirect handling code. This also doesn't make it any simpler, sadly.

As to what version we merge this into, I proposed it against the master branch. I don't think it belongs in 3.0.0 (`resolve_redirects` isn't really part of our public API), but it could definitely break people's stuff. Next minor release feels appropriate, but I'd like to hear what you think @sigmavirus24.
",Lukasa,sigmavirus24
2651,2015-06-25 06:59:47,"I can see some value in this. For API reasons it could only ever work with the 'list of tuples' approach, but I'd be ok with us adding support for this. @sigmavirus24?
",Lukasa,sigmavirus24
2648,2015-06-22 20:36:36,"Seems reasonable enough to me. I'm happy to take this. @sigmavirus24?
",Lukasa,sigmavirus24
2646,2015-06-21 14:22:24,"/cc @neosab @Lukasa 
",sigmavirus24,neosab
2646,2015-06-21 14:22:24,"/cc @neosab @Lukasa 
",sigmavirus24,Lukasa
2645,2015-06-19 18:29:16,"# Preamble

Please note that this a request for comments, not something that we will _definitely_ do in Requests.
# Problem

Right now we have an attribute defined on a Response object, `ok`. This attribute [currently](https://github.com/kennethreitz/requests/blob/9bbab338fdbb562b923ba2d8a80f0bfba697fa41/requests/models.py#L617..L623) calls `self.raise_for_status()` and catches the [exception](https://github.com/kennethreitz/requests/blob/9bbab338fdbb562b923ba2d8a80f0bfba697fa41/requests/models.py#L825..L837) raised. `raise_for_status` appropriately only raises an exception for status codes in the 4xx or 5xx range. This means that a Response with a status code in the 2xx and 3xx range will be ""ok"". The problem is that ""ok"" has a certain association in HTTP with 2xx responses, specifically 200 responses. This _may_ (I haven't looked to see if anyone has had problems with this) be misleading, especially if the user is combining their usage of the `ok` attribute with `allow_redirects=False`.
# Proposed Solution

Instead of using `raise_for_status` to determine the ""ok-ness"" of a response, we should compare the status code of the response directly. This will do two things:
1. It will narrow the definition of `Response.ok`
2. It will make using `Response.ok` faster. Currently we add a new stack, potentially throw and catch an exception, and then return. With a simple comparison, we could just do: `return 200 <= self.status_code < 300` which is much faster. This argument, however, doesn't hold much weight for me personally.

I'd really love @Lukasa and @kennethreitz's opinions here as well as anyone else willing to share their opinion publicly.
",sigmavirus24,kennethreitz
2645,2015-06-19 18:29:16,"# Preamble

Please note that this a request for comments, not something that we will _definitely_ do in Requests.
# Problem

Right now we have an attribute defined on a Response object, `ok`. This attribute [currently](https://github.com/kennethreitz/requests/blob/9bbab338fdbb562b923ba2d8a80f0bfba697fa41/requests/models.py#L617..L623) calls `self.raise_for_status()` and catches the [exception](https://github.com/kennethreitz/requests/blob/9bbab338fdbb562b923ba2d8a80f0bfba697fa41/requests/models.py#L825..L837) raised. `raise_for_status` appropriately only raises an exception for status codes in the 4xx or 5xx range. This means that a Response with a status code in the 2xx and 3xx range will be ""ok"". The problem is that ""ok"" has a certain association in HTTP with 2xx responses, specifically 200 responses. This _may_ (I haven't looked to see if anyone has had problems with this) be misleading, especially if the user is combining their usage of the `ok` attribute with `allow_redirects=False`.
# Proposed Solution

Instead of using `raise_for_status` to determine the ""ok-ness"" of a response, we should compare the status code of the response directly. This will do two things:
1. It will narrow the definition of `Response.ok`
2. It will make using `Response.ok` faster. Currently we add a new stack, potentially throw and catch an exception, and then return. With a simple comparison, we could just do: `return 200 <= self.status_code < 300` which is much faster. This argument, however, doesn't hold much weight for me personally.

I'd really love @Lukasa and @kennethreitz's opinions here as well as anyone else willing to share their opinion publicly.
",sigmavirus24,Lukasa
2645,2016-05-15 16:51:31,"@Itwilloutliveyou I'm sorry, I don't think I fully understand what you're getting at here: can you elaborate or rephrase?
",Lukasa,Itwilloutliveyou
2641,2015-06-15 06:39:01,"Thanks for this @duanhongyi!

However, I'm :-1: on this idea. My primary objection is that this adds implicit global state. Generally speaking I don't think libraries should maintain any internal state at all except what is truly necessary for their function. Wherever possible we should provide 'state objects' to users that the user has control over, ensuring that users own the lifetimes of objects: we already do this.

I think having a pool of sessions in the background is a bad idea. It'll lead to bug reports from users who aren't expecting it and it'll lead to complaints from users who don't like the memory profile it causes. Most importantly, it makes it _very very difficult_ to obtain reproducible behaviour out of the top-level API, because what exactly happens on a given request is actually dependent on the entire lifetime of the program up until that point. `requests.get()` may or may not work depending on whether there are cookies present in the particular `Session` you're using. It may or may not work depending on whether the remote server supports keepalive connections (a surprising number don't handle it well).

I'll let @sigmavirus24 express an opinion as well, but I'm sorry, I doubt we'll merge this.
",Lukasa,sigmavirus24
2639,2015-06-12 20:27:50,"Hmm, this one is really tricky.

I think the best fix here is actually in urllib3: we should catch `UnicodeDecodeError`. However, if we catch it, we should return `result`. The rationale is that the user has already provided us with a bytestring, so they presumably think they know what they're doing. @shazow, thoughts?
",Lukasa,shazow
2636,2015-06-12 02:10:48,"I've seen this and wasn't able to come up with a simple case to reproduce it. /cc @shazow since we discussed it
",sigmavirus24,shazow
2633,2015-06-09 10:38:31,"It absolutely can, but you have to build the URL yourself. If you ask requests to encode the parameters for you (using params) then we'll throw the anchor away.

It seems like a reasonable request to keep hold of that anchor: @sigmavirus24?
",Lukasa,sigmavirus24
2630,2015-06-05 12:45:27,"> Correct me if I'm wrong, (I posted my question right before going to sleep) but this doesn't actually run the tests with py.test. Don't we need a py.test setuptools command in the code and registered as test_class or something like that?

Probably

> Further, doesn't setuptools still use unverified TLS to download the test dependencies when someone who doesn't already have pytest installed runs python setup.py test?

I was told at DjangoCon EU that setuptools has been improved in that regard: @dstufft?
",Lukasa,dstufft
2621,2015-06-01 16:47:09,"You are entirely correct. This likely applies a bit more broadly than requests: we pulled this algorithm out of the standard library, which means it is likely affected. Similarly, I checked `service_identity`, and I believe it may also not handle this appropriately.

Can I get some input from @reaperhulk, @hynek, and @tiran about whether that assessment is right?
",Lukasa,reaperhulk
2621,2015-06-01 16:47:09,"You are entirely correct. This likely applies a bit more broadly than requests: we pulled this algorithm out of the standard library, which means it is likely affected. Similarly, I checked `service_identity`, and I believe it may also not handle this appropriately.

Can I get some input from @reaperhulk, @hynek, and @tiran about whether that assessment is right?
",Lukasa,hynek
2621,2015-06-01 16:47:09,"You are entirely correct. This likely applies a bit more broadly than requests: we pulled this algorithm out of the standard library, which means it is likely affected. Similarly, I checked `service_identity`, and I believe it may also not handle this appropriately.

Can I get some input from @reaperhulk, @hynek, and @tiran about whether that assessment is right?
",Lukasa,tiran
2621,2015-06-01 22:16:08,"Ok then, so I think the first priority here is working out whether or not this is a problem in the stdlib as well. @tiran?
",Lukasa,tiran
2621,2015-06-02 20:10:19,"Why not. @bagder?
",Lukasa,bagder
2617,2015-05-28 17:09:51,"This fixes #2613. I believe that this technically constitutes a backward-incompatible API change, so I've proposed this against 3.0.0. Let me know if you disagree @sigmavirus24 (the other option is really that this is a bugfix), and I'll propose against master.
",Lukasa,sigmavirus24
2615,2015-05-26 13:23:54,"No objection from me: @sigmavirus24?
",Lukasa,sigmavirus24
2605,2015-05-18 17:00:36,"Removing the arguments to the `Session` constructor was a deliberate decision made by @kennethreitz in 92355ada54a3a19341f7fbc65a8bc50816858c63. In this issue I cannot pretend to speak for him, but I believe in his view this was done for cleanliness (which seems reasonable to me). We have no plans to change that portion of the API at this time. Sorry! :smile:
",Lukasa,kennethreitz
2602,2015-05-14 17:41:11,"That is, assuming, @Lukasa has no objections
",sigmavirus24,Lukasa
2598,2015-05-13 06:08:49,"As discussed in the comments for ab84f9be5740d4649d734e73b84f17f85e52ffc9, this code block is necessary thanks to our shiny `json` parameter.

As a temporary workaround, this reinstates the code Kenneth hates so much so that the builds start working again. @kennethreitz feel free to replace this with something else longer term if you still hate it. :grin:
",Lukasa,kennethreitz
2595,2015-05-12 16:58:04,"Aha, this is interesting. I also cannot reproduce this, so it's actually important that we're on GAE. It seems like this is going to be caused by GAE being different to stdlib Python.

This means, unfortunately, that this _is_ a urllib3 issue (requests is unlikely to be at fault here), and in particular I think GAE is screwing this up. Unfortunately, it's screwing it up _silently_, which is doubly bad.

@shazow What's your position on GAE support in urllib3? I know requests considers it an unsupported platform...
",Lukasa,shazow
2588,2015-05-04 02:18:34,"Thanks for catching this @gutworth! :sparkles: :cake: :sparkles: 
",sigmavirus24,gutworth
2586,2015-05-03 13:43:44,"Ok, I believe we've left enough time for the chunked stuff to bake, and no further issues seem to have come out of the woodwork. At this point, I think we should cut 2.7.0 and begin the switchover to our new release process.

@shazow are you open to tagging a new urllib3 release, and if so what commit would you like us to use? If you don't really care, by default we'd use shazow/urllib3@74073791a3429d9b9f375563954f57f2181599dc.

Things to do:
- [x] Update urllib3.
- [x] Confirm that the #2455 problem sites still work.
- [x] Update changelog.
- [ ] Tag.
- [ ] Push release.
- [ ] Have a drink.
",Lukasa,shazow
2586,2015-05-03 13:51:35,"In my ideal world we'd update the `certifi` bundle as well, but I'm now strongly averse to trying to do too much with these releases. @sigmavirus24?
",Lukasa,sigmavirus24
2586,2015-05-03 14:13:02,"Hm, this downgraded the embedded version string of urllib3.
I think it is confusing to have a quite arbitrary version in the bundled library.

Thoughts:
- Wait for a @shazow to cut a new release and update the version in urllib3 itself, then pull that.
- Rewrite the version string to be actual the commit id.
",t-8ch,shazow
2576,2015-04-29 11:59:31,"@lukasa they can. I'm proposing that we provide a stricter policy by default in the near future to make requests comply better with 6265
",sigmavirus24,lukasa
2575,2015-04-28 15:44:09,"@Lukasa ofc you are right :-)



It gets another redirect with a content-length and then seems to wait for the body



Yields no redirect for me and also hangs.

cc @bagder
",t-8ch,bagder
2575,2015-04-28 15:53:05,"I once had a chat with @bagder about the -X flag, and in particular about how it doesn't actually change curl's logic. The correct flag to pass curl is -I, and when you do that your first example works:


",Lukasa,bagder
2575,2015-04-28 16:00:34,"@t-8ch I'm sure @bagder will tell you you're not the first person to make this mistake, and you won't be the last. ;)

Yeah, that's the bug.
",Lukasa,bagder
2571,2015-04-25 13:41:15,"@mutindaz so you _might_ be able to do something like



But there are some issues with doing that.
1. The site may not react well to a `HEAD` request at that resource
2. The URI in the `Location` header may not be a URL of the form `{scheme}://{host}/{path}` but could be only parts of that and so requests will barf
3. That could loop forever if there's a redirect loop.

That would be a ""simple"" way of doing it (although there are things wrong with it).

The more difficult thing to do would be to handle the request/redirect cycle yourself. For that, you'll need to begin using a session object and the `Request` objects. I don't use those frequently so I can't just hammer out a rough example for you but it would look something like (this is very much pseudo-code):



That assumes a few things:
1. That you have no concern for exposing data to a server that shouldn't see it (which is why on certain redirects we change the method and remove the body)
2. That you are certain that handling these redirects is what you want to do
3. That you don't care about preserving the history
4. That you're willing to follow an arbitrary and unknown (and potentially infinite) number of redirects

Personally, I think we could factor the url handling in [this section of code](https://github.com/kennethreitz/requests/blob/ca66267d2cf8adc67ed8857f3f57b45ffde21e01/requests/sessions.py#L119..L139) out into its own method like we have `prepare_request` but this is such a 2% use-case that I'm not confident it will be a worthwhile refactor.
",sigmavirus24,mutindaz
2567,2015-04-24 11:08:06,"@sigmavirus24 You own this logic for the most part: how does this look?
",Lukasa,sigmavirus24
2567,2015-04-24 11:15:28,"Also I suppose it'd be nice if @dcramer tries this (because of https://github.com/kennethreitz/requests/issues/2558)
",untitaker,dcramer
2567,2015-04-24 18:19:11,"From my POV this seems safe. @mitsuhiko might have better feedback
",dcramer,mitsuhiko
2567,2015-05-10 15:43:41,"@untitaker did you mean @mitsuhiko? I would hope a direct ping would suffice.

---

@Lukasa I would argue this is probably, on the whole, better than our previous meta_path hackery for several reasons:
1. It provides the same functionality
2. It's significantly simpler (simple is better than complex)
3. It allows for all of this to work in the case where requests is vendored without its vendored dependencies (see @untitaker's use of `__name__` when adding the alias to `sys.modules`)
4. It doesn't mess with other meta_path plugins (e.g., PyInstaller, the hack that @dcramer and @mitsuhiko are using, etc.)
5. It falls back in the correct order
6. It's far easier to explain to someone
7. When urllib3 is not vendored, the following works as we'd like it to:
   
   

I have yet to test this with PyInstaller, but the root of the problem there was the fact that our meta_path plugin was in the wrong place relative to the multiple plugins that PyInstaller uses. So with that removed, this should just work. I'm also confident that if @mitsuhiko and @dcramer test this with their code that hacks the meta_path, then they'll not see any problems.
",sigmavirus24,mitsuhiko
2567,2015-05-10 16:04:52,"@sigmavirus24

> @untitaker did you mean @mitsuhiko? I would hope a direct ping would suffice.

Unfortunately I don't think so. I haven't heard back from him either, perhaps
@dcramer can ping him about that?

---

@eriol BTW this should also remove the need to rewrite every import statement
inside requests itself.
",untitaker,mitsuhiko
2567,2015-05-10 17:34:27,"@sigmavirus24 I'm not disputing better at all. =) What I'd like to do is to take all reasonable precautions to reduce the risk of deploying this fix. For example, can @eriol and @ralphbean confirm that their package building functions correctly with this patch?

Basically, rushing helps nobody, and I'd like to try to begin a run of stable requests releases if at all possible. The last run of four-or-five broken releases in a row is bad, and we need to not get in the habit of doing that.
",Lukasa,ralphbean
2567,2015-05-27 13:02:45,"We're still hoping to have @eriol and/or @ralphbean take a swing at this code and confirm it's working for them.
",Lukasa,ralphbean
2567,2015-06-30 16:00:56,"@untitaker to be honest, there are some differences. @dstufft mentioned that his patch includes something that handles `from pip._vendor import requests` as well as `import pip._vendor.requests` but I'm not sure we need that because I think ours just works as it's written. Perhaps @dstufft could explain more about what he found to be necessary in his patch that didn't work with this (assuming he tried this one in pip)
",sigmavirus24,dstufft
2567,2015-07-29 15:51:39,"Since @kennethreitz just pinged me in #2678 i want to give some comments about this.  Given all the crappy solutions about this debian bundling thing, this is probably the best so far.  My original complaint was about the badly broken import hook.  This one comes with it's own insanity.

For a start: if someone imports `from requests.packages.urllib3 import submodule` and then later someome imports `from urllib3 import submodule` (with the patch in place), then `submodule.__name__` is `requests.packages.urllib3.submodule` and not `urllib3.submodule`.  This will break pickle in urllib3 as an example.  Since you are already down so far the ""we don't care about any of this"" route I think this is the closest you will get to achieving your goal.

I still argue that what you should be doing is just stop this vendoring.  None of this patching business makes any sense.

There be many more dragons.
",mitsuhiko,kennethreitz
2567,2015-07-29 16:57:09,"As far as I understand the main advantage Kenneth sees in vendoring urllib3 is that a minimal installation can be done by just unpacking a tarball.

Perhaps there is a way to provide this comfort without making urllib3 a subpackage of requests?

On 29 July 2015 17:52:10 CEST, Armin Ronacher notifications@github.com wrote:

> Since @kennethreitz just pinged me in #2678 i want to give some
> comments about this.  Given all the crappy solutions about this debian
> bundling thing, this is probably the best so far.  My original
> complaint was about the badly broken import hook.  This one comes with
> it's own insanity.
> 
> For a start: if someone imports `from requests.packages.urllib3 import
> submodule` and then later someome imports `from urllib3 import
> submodule` (with the patch in place), then submodule.**name** is
> `requests.packages.urllib3.submodule` and not `urllib3.submodule`. 
> This will break pickle in urllib3 as an example.  Since you are already
> down so far the ""we don't care about any of this"" route I think this is
> the closest you will get to achieving your goal.
> 
> I still argue that what you should be doing is just stop this
> vendoring.  None of this patching business makes any sense.
> 
> There be many more dragons.
> 
> ---
> 
> Reply to this email directly or view it on GitHub:
> https://github.com/kennethreitz/requests/pull/2567#issuecomment-125996644

## 

Sent from my phone. Please excuse my brevity.
",untitaker,kennethreitz
2565,2015-04-24 09:50:01,"Chunked is exactly what I expected.

Can you try hitting any URL that does not return chunked data and check whether you still see this problem? @sigmavirus24 @shazow Looks like the chunked-encoding logic has a problem with proxy connections.
",Lukasa,shazow
2565,2015-04-24 09:50:01,"Chunked is exactly what I expected.

Can you try hitting any URL that does not return chunked data and check whether you still see this problem? @sigmavirus24 @shazow Looks like the chunked-encoding logic has a problem with proxy connections.
",Lukasa,sigmavirus24
2559,2015-04-22 20:08:03,"/cc @kennethreitz 
",Lukasa,kennethreitz
2558,2015-04-22 21:27:34,"> Seemingly both packages are doing similar things, barring that requests tries to replace meta_path at one point.

@dstufft can speak best to why we replace it, but I believe we only filter out instances of our own hook to prevent infinite recursion when first attempting to import the name given to us.

That shouldn't be a big problem here though because the import error looks like it's probably coming directly out of Django.

Either way this will be fixed by 3e3fc76 in 2.6.1
",sigmavirus24,dstufft
2556,2015-04-22 13:19:47,"I'll probably update the history and such in this branch too.

/cc @aanand
",sigmavirus24,aanand
2556,2015-04-22 14:12:32,"LGTM, but it updates urllib3, so we need to co-ordinate with @shazow to make sure he's comfortable with pushing a new release of urllib3 as well.

Thoughts @shazow?
",Lukasa,shazow
2556,2015-04-22 14:12:45,"Courtesy nod to @shazow that we'd like to release soon so you might get bugged by downstream redistributors
",sigmavirus24,shazow
2556,2015-04-22 16:14:42,"Also, a head's up to @eriol and @ralphbean that we're planning a release in the event they would like to prepare for that.
",sigmavirus24,ralphbean
2556,2015-04-22 16:14:42,"Also, a head's up to @eriol and @ralphbean that we're planning a release in the event they would like to prepare for that.
",sigmavirus24,eriol
2556,2015-04-22 21:30:06,"@ralphbean @eriol after a discussion today, we've decided that the VendorAlias work we had in requests was causing too many problems. If you'd like to include it as a patch for Fedora/Debian we're :+1: on that, but we won't be shipping it. If you find problems with it in the future, I'll be happy to hack on it with y'all to make sure it's robust, thread-safe, etc. and will continue to work for you and for our (people using requests on Fedora/Debian) users.
",sigmavirus24,ralphbean
2556,2015-04-22 21:30:06,"@ralphbean @eriol after a discussion today, we've decided that the VendorAlias work we had in requests was causing too many problems. If you'd like to include it as a patch for Fedora/Debian we're :+1: on that, but we won't be shipping it. If you find problems with it in the future, I'll be happy to hack on it with y'all to make sure it's robust, thread-safe, etc. and will continue to work for you and for our (people using requests on Fedora/Debian) users.
",sigmavirus24,eriol
2556,2015-04-22 21:31:33,"I also want to find a sustainable long-term solution to this problem. I want to grab some of @dstufft's time (or anyone who really understands python's import machinery) to work out how we can do this long term.
",Lukasa,dstufft
2554,2015-04-21 14:57:39,"It would be nice to be able to prefix session URLs to avoid repetitive code.

Consider the following code;



This could look a lot prettier by doing;



In the mean time, I was able to achieve this by using a subclass;



@kennethreitz would such a PR be accepted? Or does anyone know of a better way this could be done?
",foxx,kennethreitz
2554,2015-04-21 15:28:58,"I agree that `prefix_url` is not the most flexible solution, and hooks/subclasses would be better suited, as suggested by @kennethreitz in #133.

However it's not immediately clear from the docs how to do this, therefore I'd like to propose some sort of docs patch to make this clearer. Would such a patch be accepted?
",foxx,kennethreitz
2554,2015-04-21 16:12:20,"Again, I'd argue this is a common use case which genuinely improves code quality, and is at least worthy of a docs patch. However if @kennethreitz votes down on this, then so be it.
",foxx,kennethreitz
2554,2015-06-05 15:59:50,"Apologies for the slow response on this. After trying several different approaches, it seems the most flexible and cleanest approach is subclassing.

Here's an example that meets our particular needs of prepending a URL;



As for documentation, I'm thinking perhaps a ""recipes"" page of patterns and user contributed subclasses for achieving common goals, which aren't appropriate for inclusion into the core.

Thoughts @kennethreitz / @sigmavirus24 ?
",foxx,kennethreitz
2552,2015-04-20 19:39:16,"I see no good reason to leave those as `None`-able defaults. I think we could get away with patching this in a minor release, because while it's _technically_ backward-incompatible, in practice I don't think anyone can be correctly relying on this behaviour. @sigmavirus24?
",Lukasa,sigmavirus24
2550,2015-04-13 15:40:17,"Beginnings of work to have a list of recommended third-party (fourth-party?) packages.

@sigmavirus24 @kennethreitz feel free to push on top of this branch to add other things if you want.
",Lukasa,kennethreitz
2550,2015-04-13 15:40:17,"Beginnings of work to have a list of recommended third-party (fourth-party?) packages.

@sigmavirus24 @kennethreitz feel free to push on top of this branch to add other things if you want.
",Lukasa,sigmavirus24
2550,2015-04-14 01:37:38,"@kennethreitz added the requests-toolbelt to the sidebar a while back. Would it be best to move it here?
",sigmavirus24,kennethreitz
2543,2015-07-06 18:02:59,"Sorry, I'm actually the proxy for someone else (@remagio) who reported the exception getting thrown over at https://github.com/edsu/twarc/issues/72 From the stack trace it looks like he is using python 2.7, but it wasn't clear what the version of requests was at play. I've asked him to check. Perhaps it's an older version that is behaving this way?
",edsu,remagio
2538,2015-04-07 11:34:33,"I'm afraid we can't support you with the `.deb`, because we don't build or maintain that package, @ralphbean does. I recommend raising a bug with debian if you're having trouble.

When you say you've tried the install methods on our website, which methods have you actually tried and how did they fail?
",Lukasa,ralphbean
2538,2015-04-07 13:26:25,"> I'm afraid we can't support you with the .deb, because we don't build or maintain that package, @ralphbean does.

Just a correction:  I do not support the debian redistribution, @eriol does.  I work on the `.rpm` packaging for Fedora and EPEL.  Regardless, thanks for the ping!
",ralphbean,ralphbean
2538,2015-04-07 13:26:25,"> I'm afraid we can't support you with the .deb, because we don't build or maintain that package, @ralphbean does.

Just a correction:  I do not support the debian redistribution, @eriol does.  I work on the `.rpm` packaging for Fedora and EPEL.  Regardless, thanks for the ping!
",ralphbean,eriol
2538,2015-04-07 13:39:56,"I think @lukasa meant to ping @eriol
",sigmavirus24,lukasa
2538,2015-04-07 13:39:56,"I think @lukasa meant to ping @eriol
",sigmavirus24,eriol
2536,2015-04-06 18:10:05,"I've done the documentation changes suggested in #2532.

I also documented a way to work around this in the docstring, but I'm not sure if it'd be good to do so. (As it may lead to hacked-together solutions) Then again, other solutions like creating a brand new `Request` might lead to worse problems. (e.g.: loss of data)

@sigmavirus24, what do you think?
",smiley,sigmavirus24
2535,2015-04-06 14:03:00,"_/me forgot to make this last night_ Sorry @Lukasa 
",sigmavirus24,Lukasa
2532,2015-04-06 17:01:04,"I assume you're talking about [this method](https://github.com/Anorov/cloudflare-scrape/blob/master/cfscrape/__init__.py#L42) in @Anorov's project. If you're doing something similar to that, I'm confused as to why you're re-using the request. You're making an entirely new request and since you're taking control at the adapter level, this is a concession you have to make.

That said, any updates to the documentation are welcome.
",sigmavirus24,Anorov
2523,2015-04-02 20:33:56,"@Lukasa , @kennethreitz - Per your suggestion, I just updated the PR with commit e65360d.
Thanks for your input.
",exvito,kennethreitz
2523,2015-04-02 20:33:56,"@Lukasa , @kennethreitz - Per your suggestion, I just updated the PR with commit e65360d.
Thanks for your input.
",exvito,Lukasa
2523,2015-04-03 13:15:09,"@exvito : @vincentxb and I are on the same team, and have story on our sprint to resolve this issue. I was working on it this morning, and am at the point of creating a unit test.
",tardyp,vincentxb
2519,2015-03-30 17:24:44,"Hi @tdussa,

In the future, please ask _questions_ on [StackOverflow](https://stackoverflow.com/questions/tagged/python-requests). Quoting from [the docs](http://docs.python-requests.org/en/latest/api/?highlight=cert).

> cert – (optional) if String, path to ssl client cert file (.pem). If Tuple, (‘cert’, ‘key’) pair.

So if you have the key file, you can use that. @t-8ch can correct me if I'm wrong, but I don't believe we (or urllib3) support sending a passphrase. I think the lack of support is specifically a limitation of the way the SSL module [loads verification data](https://docs.python.org/3/library/ssl.html?highlight=ssl#ssl.SSLContext.load_verify_locations).
",sigmavirus24,t-8ch
2518,2015-03-25 17:20:49,"@sigmavirus24 I though it could be improved a bit. Tell me what you think.
",deronnax,sigmavirus24
2517,2015-03-24 15:32:20,"Also, ping @shazow 
",sigmavirus24,shazow
2516,2015-03-24 14:09:57,"This is a very deliberate import situation. requests does not support a version of Python that lacks the standard library version. simplejson is imported first specifically because there are people who would rather it be used than the standard library's json module. As such I'm strongly -1 on including this change, but I'd still like to hear @Lukasa's thoughts.
",sigmavirus24,Lukasa
2516,2015-03-24 16:31:39,"So I looked into the bug on simplejson and I can't reproduce it. Also, I forgot to mention earlier that this change is functionally equivalent to only ever importing the standard library's `json` module (for interested parties).

I'd still like input from @Lukasa although I'm not rather convinced that this isn't a bug requests needs to be concerned about.
",sigmavirus24,Lukasa
2504,2015-03-19 11:34:03,"LGTM. The one thing I'm thinking about is whether we want to allow for forward compatibility by actually storing these kwargs in a dictionary for resolve_redirects, but I haven't decided yet. I'll let @sigmavirus24 make a call here.
",Lukasa,sigmavirus24
2497,2015-03-15 17:28:25,"In the wake of CVE-2015-2296 we should take another look at our policy for patching, releasing and publicising security vulnerabilities like this one.

Although we did a great job of responding quickly and pushing out a new release, we can definitely improve. I've received feedback from a couple of places that wanted to point out ways we could improve, and I'd like to solicit public feedback from anyone who has an interest to ensure that we're doing the best we possibly can with this sort of thing.

The goal here is for me to write up a document that explains, step-by-step, our policy with security issues. This will be posted publicly and we'll use it as a reference for any future events.

Below is the list of points people have raised:
- We probably shouldn't announce these vulnerabilities on weekends. Several people have products and projects that require them to evaluate vulnerabilities as soon as they become aware of them, and forcing those people to work on weekends does not engender positive feelings towards us.
- We released and announced before we had a CVE number. This might not have been the best thing for us to do, particularly as it will have made it a bit more difficult for people to keep track of what was going on.

If you have any other suggestions, please post them below.

I'm going to ping some specific interested parties to ensure they see this: @sigmavirus24 @dstufft @eriol @ralphbean
",Lukasa,ralphbean
2497,2015-03-15 17:28:25,"In the wake of CVE-2015-2296 we should take another look at our policy for patching, releasing and publicising security vulnerabilities like this one.

Although we did a great job of responding quickly and pushing out a new release, we can definitely improve. I've received feedback from a couple of places that wanted to point out ways we could improve, and I'd like to solicit public feedback from anyone who has an interest to ensure that we're doing the best we possibly can with this sort of thing.

The goal here is for me to write up a document that explains, step-by-step, our policy with security issues. This will be posted publicly and we'll use it as a reference for any future events.

Below is the list of points people have raised:
- We probably shouldn't announce these vulnerabilities on weekends. Several people have products and projects that require them to evaluate vulnerabilities as soon as they become aware of them, and forcing those people to work on weekends does not engender positive feelings towards us.
- We released and announced before we had a CVE number. This might not have been the best thing for us to do, particularly as it will have made it a bit more difficult for people to keep track of what was going on.

If you have any other suggestions, please post them below.

I'm going to ping some specific interested parties to ensure they see this: @sigmavirus24 @dstufft @eriol @ralphbean
",Lukasa,eriol
2497,2015-03-15 17:28:25,"In the wake of CVE-2015-2296 we should take another look at our policy for patching, releasing and publicising security vulnerabilities like this one.

Although we did a great job of responding quickly and pushing out a new release, we can definitely improve. I've received feedback from a couple of places that wanted to point out ways we could improve, and I'd like to solicit public feedback from anyone who has an interest to ensure that we're doing the best we possibly can with this sort of thing.

The goal here is for me to write up a document that explains, step-by-step, our policy with security issues. This will be posted publicly and we'll use it as a reference for any future events.

Below is the list of points people have raised:
- We probably shouldn't announce these vulnerabilities on weekends. Several people have products and projects that require them to evaluate vulnerabilities as soon as they become aware of them, and forcing those people to work on weekends does not engender positive feelings towards us.
- We released and announced before we had a CVE number. This might not have been the best thing for us to do, particularly as it will have made it a bit more difficult for people to keep track of what was going on.

If you have any other suggestions, please post them below.

I'm going to ping some specific interested parties to ensure they see this: @sigmavirus24 @dstufft @eriol @ralphbean
",Lukasa,sigmavirus24
2497,2015-03-15 17:28:25,"In the wake of CVE-2015-2296 we should take another look at our policy for patching, releasing and publicising security vulnerabilities like this one.

Although we did a great job of responding quickly and pushing out a new release, we can definitely improve. I've received feedback from a couple of places that wanted to point out ways we could improve, and I'd like to solicit public feedback from anyone who has an interest to ensure that we're doing the best we possibly can with this sort of thing.

The goal here is for me to write up a document that explains, step-by-step, our policy with security issues. This will be posted publicly and we'll use it as a reference for any future events.

Below is the list of points people have raised:
- We probably shouldn't announce these vulnerabilities on weekends. Several people have products and projects that require them to evaluate vulnerabilities as soon as they become aware of them, and forcing those people to work on weekends does not engender positive feelings towards us.
- We released and announced before we had a CVE number. This might not have been the best thing for us to do, particularly as it will have made it a bit more difficult for people to keep track of what was going on.

If you have any other suggestions, please post them below.

I'm going to ping some specific interested parties to ensure they see this: @sigmavirus24 @dstufft @eriol @ralphbean
",Lukasa,dstufft
2497,2015-03-17 20:09:25,"Alright, thanks for your feedback all! Can you please take a look at the diff attached to this issue and confirm that it's to your liking, and that you're happy with this commitment? All feedback valued! 

One particular note: I'm not sure about having a public list of who we notify downstream, but at the same time I worry about losing track of it if we don't record it somewhere. Thoughts there? @sigmavirus24?
",Lukasa,sigmavirus24
2490,2015-03-14 12:12:56,"So this is probably a stupid question, but does that server actually work? As you discussed with @mjpieters on the question, this doesn't happen with our typical test server. Is there a server we can actually reproduce this with?
",sigmavirus24,mjpieters
2489,2015-03-14 11:31:11,"Per a discussion @sigmavirus24 and I had on the mailing list, this change ensures that we correctly record cookie properties based on the original request, rather than the mutated version.
",Lukasa,sigmavirus24
2486,2015-03-12 12:49:09,"## It's the only import in init and I'm hoping @wardi can help make sure this is correct.

Sent from my Android device with K-9 Mail. Please excuse my brevity.
",sigmavirus24,wardi
2486,2015-03-13 02:28:47,"Maybe @p-p-m can share more details about how to reproduce this.
",sigmavirus24,p-p-m
2485,2015-03-12 01:55:42,"@Lukasa feel free to merge this so we can release 2.5.5 tomorrow.
",sigmavirus24,Lukasa
2477,2015-03-06 18:53:23,"Eh, fine by me. @sigmavirus24?
",Lukasa,sigmavirus24
2476,2015-03-06 15:21:31,"This is a tricky interaction with iterators. Most iterators don't quite behave the same way. What you need to do is the following



That works fine for me. I'm not confident this is a bug because `iter_lines` is allowed to make some assumptions about how it's used. I'll wait to see if @Lukasa agrees with me.
",sigmavirus24,Lukasa
2472,2015-03-04 19:54:57,"Thanks for this!

I can see no particular reason to be concerned about this patch, it seems totally reasonable to me. I'll obviously wait for @sigmavirus24 to hop in. It would also be extremely helpful to get someone who is deploying requests in anger to weigh in here.
",Lukasa,sigmavirus24
2469,2015-03-03 15:32:27,"eventlet shouldn't be affecting the import of netrc. I'm also not certain if eventlet has special case handling for requests. All eventlet should be patching is socket or httplib (or both) as far as I know, so this seems like a bug for eventlet, not us. I don't use eventlet with requests, nor does @Lukasa so neither of us can tell you that this isn't going to break anything (i.e., that the warning is okay). So if you want assurances, you'll need to search elsewhere (e.g., ask a question on StackOverflow and tag it with eventlet so that someone more familiar with eventlet will answer your question).
",sigmavirus24,Lukasa
2468,2015-03-03 07:51:21,"This looks great, I see no reason not to merge this. :cake:

@sigmavirus24, I'll let you pull the trigger.
",Lukasa,sigmavirus24
2466,2015-02-28 16:53:27,"Closes #2465 
Closes #2470 

/cc @dstufft Let me know if I should send this to pip as well
",sigmavirus24,dstufft
2466,2015-03-01 05:31:44,"Also many thanks to @dstufft for helping with the final solution.
",sigmavirus24,dstufft
2466,2015-03-01 15:04:15,"@ralphbean @eriol if you two wouldn't mind testing this on Fedora and Debian respectively, I'd greatly appreciate it. (Alternatively, if y'all have scripts that I could test this with on the respective distros, I can spin up cloud images to test with.)
",sigmavirus24,ralphbean
2466,2015-03-01 15:04:15,"@ralphbean @eriol if you two wouldn't mind testing this on Fedora and Debian respectively, I'd greatly appreciate it. (Alternatively, if y'all have scripts that I could test this with on the respective distros, I can spin up cloud images to test with.)
",sigmavirus24,eriol
2466,2015-03-03 15:08:51,"@Lukasa should I just merge this myself to push 2.5.4 out the door?
",sigmavirus24,Lukasa
2465,2015-02-28 16:30:04,"So this is sort of a bug with that import machinery and sort of a bug with python 2 and sort of a bug with chardet. On the bright side, there's a fix for it that can be performed in chardet and then vendored into requests, etc. On the not-so-bright side, this _can_ affect urllib3 as well.

I was trying to debug https://github.com/jakubroztocil/httpie/issues/315 with pdb to figure out why I was seeing a different error and ran into an issue with this import logic trying to import `requests.packages.urllib3.pdb` because on Py2, `import pdb` is treated as a implicit relative import first and then a non-relative import second. (Woo, thanks Python 2.) The temporary work-around was to add `from __future__ import absolute_import` to the top of the file I was trying to debug in. This, of course, could be applied to urllib3 and chardet both. I think the better option is to attempt to fix the import machinery stolen wholesale from pip.

@dstufft definitely understands this code better than I do, but as I understand it now: we stop trying to import it at L83 if the `__import__(name)` (which in these cases are `chardet.sys` and `urllib3.pdb`) fails. Instead, I think we need to figure out how to try one last case to actually mimic the regular import machinery. I can imagine more complex import failures, like seeing something like `chardet.os.path` fail, so something like



Does that make sense?
",sigmavirus24,dstufft
2465,2015-03-31 01:00:25,"So I read a bit of the PEP that explains the part `sys.meta_path` plays and talked to @dstufft a bit on IRC. Provided the way PyInstaller registers it's import hooks on the meta path you can see that it registers it's ""BuiltinImporter"" first. Since the first hook to return a module wins, we probably want to register our hook first. This allows Python 2's behaviour of try an implicit relative import then try an absolute import to kind of just work.

As a quick fix, I'm going to change our behaviour from appending to the meta path to inserting at the start of it. We kind of want that behaviour anyway.
",sigmavirus24,dstufft
2462,2015-03-12 01:56:09,"/cc @dstufft 
",sigmavirus24,dstufft
2456,2015-02-24 02:42:39,"@Lukasa generated the certificate with mkcert.org if I recall correctly but I'm not sure the list of certificates he used. It seems we just need to add thawte
",sigmavirus24,Lukasa
2455,2015-02-24 01:11:01,"I'm having a somewhat odd issue. A get() request seems to work fine with requests-2.5.1, but after upgrading to requests 2.5.2, the same URL leads to `CERTIFICATE_VERIFY_FAILED`. cc @mpharrigan


",rmcgibbo,mpharrigan
2455,2015-02-24 01:18:54,"@t-8ch seems related to the urllib3 patch to disable built-in hostname verification possibly.
",sigmavirus24,t-8ch
2455,2015-03-01 16:53:45,"I've been thinking about this, I think there are two statements that are true:
- There's not going to be a reasonable way around breaking this that doesn't involve retrying connections or customizing OpenSSL code to be more lenient.
- This change should happen, requests/certifi should not be in the business of adding extra certificates to the Mozilla certificate store. Mozilla removed these for a reason, because they are weak.

Assuming I'm correct about those two statements, then I think the answer is how do we go from A to B. I think the answer is that after connection we should inspect the certificate chain ourselves and if the last one is chained to one of the 1024 bit certificates that this removes, then raise a deprecation warning. In some time (requests 3.0?) drop the 1024 bit roots.

Unless maybe @reaperhulk has some idea.
",dstufft,reaperhulk
2455,2015-03-01 17:33:46,"> I don't know what NSS or PolarSSL or anyone else does) and who uses the Mozilla certificate store (or is dropping the 1024 bit roots for whatever reason). 

Let me digress into this (I know it's not the core of your point), but I believe NSS doesn't have this problem (given that Firefox uses it).

I think we should really try to bring in some people who know this issue better than us. An interesting one would be @bagder, given that cURL uses the Mozilla cert bundle AFAICT. Other SSL types might be interesting too.
",Lukasa,bagder
2455,2015-03-01 20:24:59,"**Update**

There is actually an API available in OpenSSL to tell it to use the shortest trust path, however it's only available in OpenSSL 1.0.2+. In addition there was apparently another patch applied to OpenSSL that is unreleased which will cause OpenSSL to fall back to trying shorter paths if it can't validate the path served by the the site operator.

So I think the best course of action right now is to get Python patched so that it will use the ""shortest trust path"" API if it's available. @alex has uploaded a patch to [Python's #23476](http://bugs.python.org/issue23476) which does that. I can't think of any scenario where that would break things so that seems like an easy win to me. However that's only going to ""fix"" things for people using 2.7.10 and 3.4.4, both of which are not immediately on the horizon. Even with that fix, it's still going to require a newer version of OpenSSL unless we can convince the downstream distributors to backport the OpenSSL patch to add that API, as well as backport the Python patch which will use the new API in OpenSSL if possible.
",dstufft,alex
2455,2015-03-02 08:42:48,"Thanks @bagder.

Just to update based on where we got to last night in IRC:

@alex's patch would fix the problem in 2.7.10 and 3.5 (assuming it gets merged), as well as possibly a later version of 3.4.

We can do this more generally by ORing in `X509_V_FLAG_TRUSTED_FIRST` numeric constant into the `SSLContext.verify_flags` field:



This will only work on Python versions that a) have an SSL context (2.7.9, 3.2 and onwards), and b) are using OpenSSL 1.0.2 or later. This satisfies a large number of users, but lots of users don't have access to either or both of those things.
",Lukasa,alex
2455,2015-04-23 15:22:46,"/cc @kennethreitz: this is the issue that tracks the work we need to do for certifi, btw.
",Lukasa,kennethreitz
2455,2015-04-23 18:01:11,"Alright, fuck that. Here's my new plan.

The following are the Mozilla issues that track the removal of 1024 bit certs that I could find:
- [X] [856718](https://bugzilla.mozilla.org/show_bug.cgi?id=856718)
- [x] [881553](https://bugzilla.mozilla.org/show_bug.cgi?id=881553)
- [x] [936105](https://bugzilla.mozilla.org/show_bug.cgi?id=936105)
- [x] [936304](https://bugzilla.mozilla.org/show_bug.cgi?id=936304)
- [x] [986005](https://bugzilla.mozilla.org/show_bug.cgi?id=986005)
- [x] [986014](https://bugzilla.mozilla.org/show_bug.cgi?id=986014)
- [x] [986019](https://bugzilla.mozilla.org/show_bug.cgi?id=986019)
- [x] [1155279](https://bugzilla.mozilla.org/show_bug.cgi?id=1155279)

I plan to identify every certificate that was removed by these purges and check whether any of them are in the current requests cert bundle. If they are, I plan to restore them to the new certifi bundle.

Does anyone think this plan is bad? @reaperhulk @dstufft @alex @sigmavirus24
",Lukasa,reaperhulk
2450,2015-02-19 19:21:42,"No objection from me. @sigmavirus24?
",Lukasa,sigmavirus24
2448,2015-02-17 23:27:55,"So I think the fundamental problem here is that neither requests nor urllib3 have any way of knowing what's happening with the actually underlying TCP connection. urllib3 at its best wraps sockets to enable TLS but doesn't really directly manage sockets beyond that very thoroughly.

It isn't as if urllib3 or requests is acknowledging the FIN that it receives. That's generally how the socket behaves without us having to do that. I think a way around this is to enable TCP Keep-Alive but I'm not entirely convinced this is a solution that needs to live in requests proper.

I think @Lukasa is on vacation or just generally taking a break so I'll have to look into this more tonight. Thanks for the very very detailed issue @pddenhar!
",sigmavirus24,Lukasa
2444,2015-02-11 18:58:36,"We actually will pull in tip when we do a release. Speaking of which @Lukasa when do you think we should do a release? There's extra header awesomeness that's unreleased in urllib3 (merged last night) that I'd like to grab before doing a release.
",sigmavirus24,Lukasa
2440,2015-02-06 14:02:06,"Who added this link?

Thanks for fixing this @acquarion!
",sigmavirus24,acquarion
2434,2015-02-02 19:42:24,"I wonder if this shouldn't go into urllib3 instead, thereby giving the benefit there too. @shazow? @sigmavirus24?
",Lukasa,shazow
2434,2015-02-02 19:42:24,"I wonder if this shouldn't go into urllib3 instead, thereby giving the benefit there too. @shazow? @sigmavirus24?
",Lukasa,sigmavirus24
2431,2016-04-06 19:25:35,"This PR hasn't seen any love in a long time. @ianepperson, would mind performing a rebase to make this once-again mergable? 

@sigmavirus24 @Lukasa +1? -1? I want to get this merged or closed out. 
",kennethreitz,Lukasa
2427,2015-04-06 03:32:08,"I'm comfortable with this. Sorry for the delay @luozhaoyu. I lost track of this.

Thoughts @Lukasa?
",sigmavirus24,Lukasa
2424,2015-01-25 17:22:50,"Let's CC some people whose opinions we care strongly about:

@shazow @kevinburke @dstufft @alex @sigmavirus24
",Lukasa,sigmavirus24
2424,2015-01-25 17:22:50,"Let's CC some people whose opinions we care strongly about:

@shazow @kevinburke @dstufft @alex @sigmavirus24
",Lukasa,alex
2424,2015-01-25 17:22:50,"Let's CC some people whose opinions we care strongly about:

@shazow @kevinburke @dstufft @alex @sigmavirus24
",Lukasa,shazow
2424,2015-01-25 17:22:50,"Let's CC some people whose opinions we care strongly about:

@shazow @kevinburke @dstufft @alex @sigmavirus24
",Lukasa,dstufft
2424,2015-01-25 17:22:50,"Let's CC some people whose opinions we care strongly about:

@shazow @kevinburke @dstufft @alex @sigmavirus24
",Lukasa,kevinburke
2424,2015-01-25 17:34:43,"My 2 $CURRENCY's worth:

I would be disinclined to do it. I think being outside the standard library has given us the freedom to make choices that benefit our users without being stuck behind core dev's policies for version support and release. It allows us to respectfully disagree with the priorities of core dev. And it allows us to make decisions that are ideological, which has been the lifeblood of this project for more than three years.

I think the reality is that if this module enters the standard library the current core team will move on from it. I certainly have little interest following it into the quagmire that is core dev. The most likely to steward requests in the stdlib is @sigmavirus24, and he's just one man. That loss of direction will inevitably lead to an erosion of the library's interface over time, and I think that would be a tragic thing.

The only thing that being in the standard library gives us is our time back. That's a good reason to put it there, if that's what you think it needs, but I don't think we should pretend that it will make the library or the Python ecosystem any better.
",Lukasa,sigmavirus24
2424,2015-01-25 18:11:17,"> I think the reality is that if this module enters the standard library the current core team will move on from it. I certainly have little interest following it into the quagmire that is core dev. The most likely to steward requests in the stdlib is @sigmavirus24, and he's just one man. That loss of direction will inevitably lead to an erosion of the library's interface over time, and I think that would be a tragic thing.

I would wander into the stdlib to try to help, but given the fact that exactly one of I don't know how many previous patchsets I've submitted has been accepted and one other _reviewed_ makes me wary of wanting to bother with that process. I know the core devs are entirely swamped by more important things. I also know someone else has decided randomly that they want to maintain httplib/http but they're clearly not suited for the job (yet) and I don't have the patience to work on httplib when patches that both @Lukasa and I sit around, unreviewed, and not cared about (when they fix pressing issues with the library).

I'd probably end up just forking requests to continue using it.

> requests is absolutely unsuitable for stdlib inclusion for the many reasons stated before me. The urllib3 dependency alone is a complete showstopper; we don’t want it to got to die in the stdlib.

It's always been a contention of @kennethreitz (and therefore, the project as a whole) that urllib3 is an implementation detail. Many of requests' biggest features are handled entirely by urllib3, but it doesn't mean they couldn't be reimplemented with care into truly dependency-less library.

Regarding the chardet dependency: it's been nothing but a headache to us (and to me specifically). It used to have separate codebases for py2 and py3 until I got it into a single codebase library (which has only in the last several months been merged back into chardet proper). The library is slow and a huge memory hog (which angers many people to the point of yelling at us here on the issue tracker). It's not entirely accurate and Mozilla's universalchardet that it is modeled after has all but been abandoned by Mozilla. So removing chardet would probably be a net positive anyway.

Regarding whether we should do this or not, I'm frankly unconcerned. Whatever would be in the stdlib would end up being requests in API only. The Python 3 adoption rate is slow enough that I don't think people will be meaningfully affected by this for the next N years (where N is the globally unknown number of years for 3.5 to be used in production by corporations).

And like I said, I'd probably end up just forking requests or using urllib3 directly at that point.
",sigmavirus24,sigmavirus24
2423,2015-01-24 07:53:11,"This isn't our fault I don't think: your exception comes from pip. @dstufft?
",Lukasa,dstufft
2423,2015-01-24 16:20:51,"Yeah, so I'm pretty sure this is one of those cases where we shouldn't have let @jschneier remove that import in `requests.compat` (our fault) mixed with the constant of ""`sudo pip install x` is evil; use virtualenvs"".

We _could_ add the import back, but I would then mark it for deletion in 3.0 so this problem would just pop up again. So I'm going to close this since there's no solution that won't result in further breakage of other packages down the road.
",sigmavirus24,jschneier
2422,2015-04-07 23:31:47,"@jazzfan thanks for doing that. I'll look tonight and I suspect that @Lukasa will look when he wakes up
",sigmavirus24,jazzfan
2422,2015-07-30 15:57:04,"@Lukasa I am seeing a similar issue, but the cause is from nginx immediately returning a `413 Request Entity Too Large`. RFC2616 says this about 413:

> The server _MAY close the connection_ to prevent the client from continuing the request. 

The server closing the connection is the behaviour I am seeing, and requests (actually, I think the issue is in Python's `httplib`) is not handling that case.

I have a server on EC2 that can be used for testing against. nginx is configured with a `max_client_body_size` of `1k`, and the Python server behind nginx is the example posted above in this issue (with the modification to consume the whole body). The catch is that nginx doesn't proxy the request to the Python server due to the `413` error. You can POST to `http://52.18.181.108/`.
In my case, when uploading a file larger than roughly 75MB I see this issue.

Using wireshark, I see the below behaviour, where nginx responds upfront with a 413, but Python httplib doesn't handle the response until the very end of the request (happily uploading the whole file while the server has already rejected the request). If the file is large enough, nginx will terminate the connection before httplib/urllib3 handles the response, resulting in either a `Broken Pipe`, or a `Connection reset by peer`.



cc @cournape
",sjagoe,cournape
2416,2015-01-21 02:41:48,"So we [rebuild](https://github.com/kennethreitz/requests/blob/d2d576b6b1101e2871c82f63adf2c2b534c2dabc/requests/sessions.py#L168..L176) the `Cookie` header on a redirect as a matter of not exposing the cookie to people who shouldn't see it. Without a cookiejar to rebuild _from_ we won't persist the header through the redirect. We already [sanitize authorization](https://github.com/kennethreitz/requests/blob/d2d576b6b1101e2871c82f63adf2c2b534c2dabc/requests/sessions.py#L215) on redirect to different hosts, so we _could_ do the same here.

I want @Lukasa's thoughts on this though before I continue with work for this.

I, for one, think Cookie headers are in the same class as Content-Length and Host headers. They can be set by users but they really _shouldn't_ be set by users because it can do bad things if not handled with care.
",sigmavirus24,Lukasa
2413,2015-01-19 13:35:11,"@Lukasa This is what I get for working on this way too late last night.

@arthurdarcet thanks. Done.
",sigmavirus24,arthurdarcet
2409,2015-01-16 08:53:50,"Alright, @sigmavirus24, I've thought about this overnight and I can't think of a good fix that uses the current code and doesn't commit wild layering violations. So here's my proposal.

We should rewrite the redirect cache.

The core problem is, fundamentally, that the redirect cache's implementation is misguided. I've come to this position because of closer reading of RFC 7231. It has the following two stanzas [under the 301 code](https://tools.ietf.org/html/rfc7231#section-6.4.2) that I find to be interesting:

> [A]ny future references to this resource ought to use one of the enclosed URIs. Clients with link-editing capabilities ought to automatically re-link references to the effective request URI to one or more of the new references sent by the server, where possible

and

> A 301 response is cacheable by default; i.e., unless otherwise indicated by the method definition or explicit cache controls.

This is a weird combination of text. If the 301 is supposed to cause a rewrite of the URL, why would I cache it?

My suspicion is that the correct flow in requests is actually not a 'redirect' cache, but a 301 cache, that literally returns 301 responses without contacting the network. This will lead to the correct flow in requests: everything looks exactly the same from the perspective of the upper layers of code, the response just came in really fast.

This is backed up by examining what Chrome does, which is serve 301s from its cache (click to see better):

![301](https://cloud.githubusercontent.com/assets/1382556/5773563/dc5f2bdc-9d5b-11e4-8581-5f83e76d8526.png)

I think now, as I thought then, that the 301 cache should not have been merged in its proposed form. I now think more strongly than that: we have to change it. I have three options on the table:
1. Remove the redirect cache entirely. People who want that function should use [CacheControl](https://github.com/ionrock/cachecontrol), as we have said many, many times. This will lead to the flow through requests I mentioned above, resolve this bug (and some others that are inevitably lying around), and provide additional benefits _on top_ of the standard redirect cache.
2. Replace the redirect cache with a special-case cache for 301s that operates at the `HTTPAdapter` layer. This is tricky, because caching 301s needs to obey all the rules for caching (another thing our 'redirect cache' doesn't do), including `Cache-Control` and `Vary` headers. We'd probably need to write about 30% of the CacheControl library, which raises my third option;
3. Bring CacheControl (or someone else) into the fold. Bless a requests caching solution as 'the one true way' to cache and bring it in tree, or use it as an external `pip` dependency, but include it by default. This will fix our 301 bugs and give us caching out-of-the-box by default.

Thoughts would be extremely appreciated at this time. Especially from @sigmavirus24, @kennethreitz, @shazow, @ionrock. Note also the discussion that occurred in #2095: my reading of it suggests that @johnsheehan, @dstufft, and @dknecht favoured my option 1, and @dolph favoured something closer to my option 3 (or possibly 2).
",Lukasa,kennethreitz
2409,2015-01-16 08:53:50,"Alright, @sigmavirus24, I've thought about this overnight and I can't think of a good fix that uses the current code and doesn't commit wild layering violations. So here's my proposal.

We should rewrite the redirect cache.

The core problem is, fundamentally, that the redirect cache's implementation is misguided. I've come to this position because of closer reading of RFC 7231. It has the following two stanzas [under the 301 code](https://tools.ietf.org/html/rfc7231#section-6.4.2) that I find to be interesting:

> [A]ny future references to this resource ought to use one of the enclosed URIs. Clients with link-editing capabilities ought to automatically re-link references to the effective request URI to one or more of the new references sent by the server, where possible

and

> A 301 response is cacheable by default; i.e., unless otherwise indicated by the method definition or explicit cache controls.

This is a weird combination of text. If the 301 is supposed to cause a rewrite of the URL, why would I cache it?

My suspicion is that the correct flow in requests is actually not a 'redirect' cache, but a 301 cache, that literally returns 301 responses without contacting the network. This will lead to the correct flow in requests: everything looks exactly the same from the perspective of the upper layers of code, the response just came in really fast.

This is backed up by examining what Chrome does, which is serve 301s from its cache (click to see better):

![301](https://cloud.githubusercontent.com/assets/1382556/5773563/dc5f2bdc-9d5b-11e4-8581-5f83e76d8526.png)

I think now, as I thought then, that the 301 cache should not have been merged in its proposed form. I now think more strongly than that: we have to change it. I have three options on the table:
1. Remove the redirect cache entirely. People who want that function should use [CacheControl](https://github.com/ionrock/cachecontrol), as we have said many, many times. This will lead to the flow through requests I mentioned above, resolve this bug (and some others that are inevitably lying around), and provide additional benefits _on top_ of the standard redirect cache.
2. Replace the redirect cache with a special-case cache for 301s that operates at the `HTTPAdapter` layer. This is tricky, because caching 301s needs to obey all the rules for caching (another thing our 'redirect cache' doesn't do), including `Cache-Control` and `Vary` headers. We'd probably need to write about 30% of the CacheControl library, which raises my third option;
3. Bring CacheControl (or someone else) into the fold. Bless a requests caching solution as 'the one true way' to cache and bring it in tree, or use it as an external `pip` dependency, but include it by default. This will fix our 301 bugs and give us caching out-of-the-box by default.

Thoughts would be extremely appreciated at this time. Especially from @sigmavirus24, @kennethreitz, @shazow, @ionrock. Note also the discussion that occurred in #2095: my reading of it suggests that @johnsheehan, @dstufft, and @dknecht favoured my option 1, and @dolph favoured something closer to my option 3 (or possibly 2).
",Lukasa,dolph
2409,2015-01-16 08:53:50,"Alright, @sigmavirus24, I've thought about this overnight and I can't think of a good fix that uses the current code and doesn't commit wild layering violations. So here's my proposal.

We should rewrite the redirect cache.

The core problem is, fundamentally, that the redirect cache's implementation is misguided. I've come to this position because of closer reading of RFC 7231. It has the following two stanzas [under the 301 code](https://tools.ietf.org/html/rfc7231#section-6.4.2) that I find to be interesting:

> [A]ny future references to this resource ought to use one of the enclosed URIs. Clients with link-editing capabilities ought to automatically re-link references to the effective request URI to one or more of the new references sent by the server, where possible

and

> A 301 response is cacheable by default; i.e., unless otherwise indicated by the method definition or explicit cache controls.

This is a weird combination of text. If the 301 is supposed to cause a rewrite of the URL, why would I cache it?

My suspicion is that the correct flow in requests is actually not a 'redirect' cache, but a 301 cache, that literally returns 301 responses without contacting the network. This will lead to the correct flow in requests: everything looks exactly the same from the perspective of the upper layers of code, the response just came in really fast.

This is backed up by examining what Chrome does, which is serve 301s from its cache (click to see better):

![301](https://cloud.githubusercontent.com/assets/1382556/5773563/dc5f2bdc-9d5b-11e4-8581-5f83e76d8526.png)

I think now, as I thought then, that the 301 cache should not have been merged in its proposed form. I now think more strongly than that: we have to change it. I have three options on the table:
1. Remove the redirect cache entirely. People who want that function should use [CacheControl](https://github.com/ionrock/cachecontrol), as we have said many, many times. This will lead to the flow through requests I mentioned above, resolve this bug (and some others that are inevitably lying around), and provide additional benefits _on top_ of the standard redirect cache.
2. Replace the redirect cache with a special-case cache for 301s that operates at the `HTTPAdapter` layer. This is tricky, because caching 301s needs to obey all the rules for caching (another thing our 'redirect cache' doesn't do), including `Cache-Control` and `Vary` headers. We'd probably need to write about 30% of the CacheControl library, which raises my third option;
3. Bring CacheControl (or someone else) into the fold. Bless a requests caching solution as 'the one true way' to cache and bring it in tree, or use it as an external `pip` dependency, but include it by default. This will fix our 301 bugs and give us caching out-of-the-box by default.

Thoughts would be extremely appreciated at this time. Especially from @sigmavirus24, @kennethreitz, @shazow, @ionrock. Note also the discussion that occurred in #2095: my reading of it suggests that @johnsheehan, @dstufft, and @dknecht favoured my option 1, and @dolph favoured something closer to my option 3 (or possibly 2).
",Lukasa,dstufft
2409,2015-01-16 08:53:50,"Alright, @sigmavirus24, I've thought about this overnight and I can't think of a good fix that uses the current code and doesn't commit wild layering violations. So here's my proposal.

We should rewrite the redirect cache.

The core problem is, fundamentally, that the redirect cache's implementation is misguided. I've come to this position because of closer reading of RFC 7231. It has the following two stanzas [under the 301 code](https://tools.ietf.org/html/rfc7231#section-6.4.2) that I find to be interesting:

> [A]ny future references to this resource ought to use one of the enclosed URIs. Clients with link-editing capabilities ought to automatically re-link references to the effective request URI to one or more of the new references sent by the server, where possible

and

> A 301 response is cacheable by default; i.e., unless otherwise indicated by the method definition or explicit cache controls.

This is a weird combination of text. If the 301 is supposed to cause a rewrite of the URL, why would I cache it?

My suspicion is that the correct flow in requests is actually not a 'redirect' cache, but a 301 cache, that literally returns 301 responses without contacting the network. This will lead to the correct flow in requests: everything looks exactly the same from the perspective of the upper layers of code, the response just came in really fast.

This is backed up by examining what Chrome does, which is serve 301s from its cache (click to see better):

![301](https://cloud.githubusercontent.com/assets/1382556/5773563/dc5f2bdc-9d5b-11e4-8581-5f83e76d8526.png)

I think now, as I thought then, that the 301 cache should not have been merged in its proposed form. I now think more strongly than that: we have to change it. I have three options on the table:
1. Remove the redirect cache entirely. People who want that function should use [CacheControl](https://github.com/ionrock/cachecontrol), as we have said many, many times. This will lead to the flow through requests I mentioned above, resolve this bug (and some others that are inevitably lying around), and provide additional benefits _on top_ of the standard redirect cache.
2. Replace the redirect cache with a special-case cache for 301s that operates at the `HTTPAdapter` layer. This is tricky, because caching 301s needs to obey all the rules for caching (another thing our 'redirect cache' doesn't do), including `Cache-Control` and `Vary` headers. We'd probably need to write about 30% of the CacheControl library, which raises my third option;
3. Bring CacheControl (or someone else) into the fold. Bless a requests caching solution as 'the one true way' to cache and bring it in tree, or use it as an external `pip` dependency, but include it by default. This will fix our 301 bugs and give us caching out-of-the-box by default.

Thoughts would be extremely appreciated at this time. Especially from @sigmavirus24, @kennethreitz, @shazow, @ionrock. Note also the discussion that occurred in #2095: my reading of it suggests that @johnsheehan, @dstufft, and @dknecht favoured my option 1, and @dolph favoured something closer to my option 3 (or possibly 2).
",Lukasa,dknecht
2409,2015-01-16 08:53:50,"Alright, @sigmavirus24, I've thought about this overnight and I can't think of a good fix that uses the current code and doesn't commit wild layering violations. So here's my proposal.

We should rewrite the redirect cache.

The core problem is, fundamentally, that the redirect cache's implementation is misguided. I've come to this position because of closer reading of RFC 7231. It has the following two stanzas [under the 301 code](https://tools.ietf.org/html/rfc7231#section-6.4.2) that I find to be interesting:

> [A]ny future references to this resource ought to use one of the enclosed URIs. Clients with link-editing capabilities ought to automatically re-link references to the effective request URI to one or more of the new references sent by the server, where possible

and

> A 301 response is cacheable by default; i.e., unless otherwise indicated by the method definition or explicit cache controls.

This is a weird combination of text. If the 301 is supposed to cause a rewrite of the URL, why would I cache it?

My suspicion is that the correct flow in requests is actually not a 'redirect' cache, but a 301 cache, that literally returns 301 responses without contacting the network. This will lead to the correct flow in requests: everything looks exactly the same from the perspective of the upper layers of code, the response just came in really fast.

This is backed up by examining what Chrome does, which is serve 301s from its cache (click to see better):

![301](https://cloud.githubusercontent.com/assets/1382556/5773563/dc5f2bdc-9d5b-11e4-8581-5f83e76d8526.png)

I think now, as I thought then, that the 301 cache should not have been merged in its proposed form. I now think more strongly than that: we have to change it. I have three options on the table:
1. Remove the redirect cache entirely. People who want that function should use [CacheControl](https://github.com/ionrock/cachecontrol), as we have said many, many times. This will lead to the flow through requests I mentioned above, resolve this bug (and some others that are inevitably lying around), and provide additional benefits _on top_ of the standard redirect cache.
2. Replace the redirect cache with a special-case cache for 301s that operates at the `HTTPAdapter` layer. This is tricky, because caching 301s needs to obey all the rules for caching (another thing our 'redirect cache' doesn't do), including `Cache-Control` and `Vary` headers. We'd probably need to write about 30% of the CacheControl library, which raises my third option;
3. Bring CacheControl (or someone else) into the fold. Bless a requests caching solution as 'the one true way' to cache and bring it in tree, or use it as an external `pip` dependency, but include it by default. This will fix our 301 bugs and give us caching out-of-the-box by default.

Thoughts would be extremely appreciated at this time. Especially from @sigmavirus24, @kennethreitz, @shazow, @ionrock. Note also the discussion that occurred in #2095: my reading of it suggests that @johnsheehan, @dstufft, and @dknecht favoured my option 1, and @dolph favoured something closer to my option 3 (or possibly 2).
",Lukasa,johnsheehan
2409,2015-01-16 08:53:50,"Alright, @sigmavirus24, I've thought about this overnight and I can't think of a good fix that uses the current code and doesn't commit wild layering violations. So here's my proposal.

We should rewrite the redirect cache.

The core problem is, fundamentally, that the redirect cache's implementation is misguided. I've come to this position because of closer reading of RFC 7231. It has the following two stanzas [under the 301 code](https://tools.ietf.org/html/rfc7231#section-6.4.2) that I find to be interesting:

> [A]ny future references to this resource ought to use one of the enclosed URIs. Clients with link-editing capabilities ought to automatically re-link references to the effective request URI to one or more of the new references sent by the server, where possible

and

> A 301 response is cacheable by default; i.e., unless otherwise indicated by the method definition or explicit cache controls.

This is a weird combination of text. If the 301 is supposed to cause a rewrite of the URL, why would I cache it?

My suspicion is that the correct flow in requests is actually not a 'redirect' cache, but a 301 cache, that literally returns 301 responses without contacting the network. This will lead to the correct flow in requests: everything looks exactly the same from the perspective of the upper layers of code, the response just came in really fast.

This is backed up by examining what Chrome does, which is serve 301s from its cache (click to see better):

![301](https://cloud.githubusercontent.com/assets/1382556/5773563/dc5f2bdc-9d5b-11e4-8581-5f83e76d8526.png)

I think now, as I thought then, that the 301 cache should not have been merged in its proposed form. I now think more strongly than that: we have to change it. I have three options on the table:
1. Remove the redirect cache entirely. People who want that function should use [CacheControl](https://github.com/ionrock/cachecontrol), as we have said many, many times. This will lead to the flow through requests I mentioned above, resolve this bug (and some others that are inevitably lying around), and provide additional benefits _on top_ of the standard redirect cache.
2. Replace the redirect cache with a special-case cache for 301s that operates at the `HTTPAdapter` layer. This is tricky, because caching 301s needs to obey all the rules for caching (another thing our 'redirect cache' doesn't do), including `Cache-Control` and `Vary` headers. We'd probably need to write about 30% of the CacheControl library, which raises my third option;
3. Bring CacheControl (or someone else) into the fold. Bless a requests caching solution as 'the one true way' to cache and bring it in tree, or use it as an external `pip` dependency, but include it by default. This will fix our 301 bugs and give us caching out-of-the-box by default.

Thoughts would be extremely appreciated at this time. Especially from @sigmavirus24, @kennethreitz, @shazow, @ionrock. Note also the discussion that occurred in #2095: my reading of it suggests that @johnsheehan, @dstufft, and @dknecht favoured my option 1, and @dolph favoured something closer to my option 3 (or possibly 2).
",Lukasa,ionrock
2409,2015-01-16 08:53:50,"Alright, @sigmavirus24, I've thought about this overnight and I can't think of a good fix that uses the current code and doesn't commit wild layering violations. So here's my proposal.

We should rewrite the redirect cache.

The core problem is, fundamentally, that the redirect cache's implementation is misguided. I've come to this position because of closer reading of RFC 7231. It has the following two stanzas [under the 301 code](https://tools.ietf.org/html/rfc7231#section-6.4.2) that I find to be interesting:

> [A]ny future references to this resource ought to use one of the enclosed URIs. Clients with link-editing capabilities ought to automatically re-link references to the effective request URI to one or more of the new references sent by the server, where possible

and

> A 301 response is cacheable by default; i.e., unless otherwise indicated by the method definition or explicit cache controls.

This is a weird combination of text. If the 301 is supposed to cause a rewrite of the URL, why would I cache it?

My suspicion is that the correct flow in requests is actually not a 'redirect' cache, but a 301 cache, that literally returns 301 responses without contacting the network. This will lead to the correct flow in requests: everything looks exactly the same from the perspective of the upper layers of code, the response just came in really fast.

This is backed up by examining what Chrome does, which is serve 301s from its cache (click to see better):

![301](https://cloud.githubusercontent.com/assets/1382556/5773563/dc5f2bdc-9d5b-11e4-8581-5f83e76d8526.png)

I think now, as I thought then, that the 301 cache should not have been merged in its proposed form. I now think more strongly than that: we have to change it. I have three options on the table:
1. Remove the redirect cache entirely. People who want that function should use [CacheControl](https://github.com/ionrock/cachecontrol), as we have said many, many times. This will lead to the flow through requests I mentioned above, resolve this bug (and some others that are inevitably lying around), and provide additional benefits _on top_ of the standard redirect cache.
2. Replace the redirect cache with a special-case cache for 301s that operates at the `HTTPAdapter` layer. This is tricky, because caching 301s needs to obey all the rules for caching (another thing our 'redirect cache' doesn't do), including `Cache-Control` and `Vary` headers. We'd probably need to write about 30% of the CacheControl library, which raises my third option;
3. Bring CacheControl (or someone else) into the fold. Bless a requests caching solution as 'the one true way' to cache and bring it in tree, or use it as an external `pip` dependency, but include it by default. This will fix our 301 bugs and give us caching out-of-the-box by default.

Thoughts would be extremely appreciated at this time. Especially from @sigmavirus24, @kennethreitz, @shazow, @ionrock. Note also the discussion that occurred in #2095: my reading of it suggests that @johnsheehan, @dstufft, and @dknecht favoured my option 1, and @dolph favoured something closer to my option 3 (or possibly 2).
",Lukasa,shazow
2409,2015-01-16 08:53:50,"Alright, @sigmavirus24, I've thought about this overnight and I can't think of a good fix that uses the current code and doesn't commit wild layering violations. So here's my proposal.

We should rewrite the redirect cache.

The core problem is, fundamentally, that the redirect cache's implementation is misguided. I've come to this position because of closer reading of RFC 7231. It has the following two stanzas [under the 301 code](https://tools.ietf.org/html/rfc7231#section-6.4.2) that I find to be interesting:

> [A]ny future references to this resource ought to use one of the enclosed URIs. Clients with link-editing capabilities ought to automatically re-link references to the effective request URI to one or more of the new references sent by the server, where possible

and

> A 301 response is cacheable by default; i.e., unless otherwise indicated by the method definition or explicit cache controls.

This is a weird combination of text. If the 301 is supposed to cause a rewrite of the URL, why would I cache it?

My suspicion is that the correct flow in requests is actually not a 'redirect' cache, but a 301 cache, that literally returns 301 responses without contacting the network. This will lead to the correct flow in requests: everything looks exactly the same from the perspective of the upper layers of code, the response just came in really fast.

This is backed up by examining what Chrome does, which is serve 301s from its cache (click to see better):

![301](https://cloud.githubusercontent.com/assets/1382556/5773563/dc5f2bdc-9d5b-11e4-8581-5f83e76d8526.png)

I think now, as I thought then, that the 301 cache should not have been merged in its proposed form. I now think more strongly than that: we have to change it. I have three options on the table:
1. Remove the redirect cache entirely. People who want that function should use [CacheControl](https://github.com/ionrock/cachecontrol), as we have said many, many times. This will lead to the flow through requests I mentioned above, resolve this bug (and some others that are inevitably lying around), and provide additional benefits _on top_ of the standard redirect cache.
2. Replace the redirect cache with a special-case cache for 301s that operates at the `HTTPAdapter` layer. This is tricky, because caching 301s needs to obey all the rules for caching (another thing our 'redirect cache' doesn't do), including `Cache-Control` and `Vary` headers. We'd probably need to write about 30% of the CacheControl library, which raises my third option;
3. Bring CacheControl (or someone else) into the fold. Bless a requests caching solution as 'the one true way' to cache and bring it in tree, or use it as an external `pip` dependency, but include it by default. This will fix our 301 bugs and give us caching out-of-the-box by default.

Thoughts would be extremely appreciated at this time. Especially from @sigmavirus24, @kennethreitz, @shazow, @ionrock. Note also the discussion that occurred in #2095: my reading of it suggests that @johnsheehan, @dstufft, and @dknecht favoured my option 1, and @dolph favoured something closer to my option 3 (or possibly 2).
",Lukasa,sigmavirus24
2409,2015-01-18 19:11:05,"I'm in favor of option 1. If there's a bunch of complexity to setting up CacheControl to handle this case (which is @Ionrock's prerogative) we can put the common code in requests-toolbelt as necessary. I'm fine having a dependency in there on CacheControl. If we do choose option 3, I'd much rather see CacheControl as an outside dependency. Development on that moves quickly enough that it makes sense to allow people to upgrade as necessary rather than bundling it.

Frankly, I'm worried about removing the redirect cache haphazardly though. Perhaps the best first option would be to use a `NullCache` for v2.6.0 of requests. This means people will have to specifically turn it on for it to work (and thereby cause issues). We would document how to use `urllib3`'s `RecentlyUsedContainer`. I suspect the problem we'll have is with pickling again. We can serialize a `NullCache` as a special sentinel value. Then we'll have to change https://github.com/kennethreitz/requests/blob/d2d576b6b1101e2871c82f63adf2c2b534c2dabc/requests/sessions.py#L674 to check the popped value and determine if it's the sentinel. If it isn't, we'll use a `RecentlyUsedContainer`.

It's extra work but it will provide a level of backwards compatibility until we can break this out in 3.0. In documenting how to enable this, we should add the warning that it is a slipshod implementation of the cache that has been known to cause problems. It should also be noted that it is considered deprecated functionality and will be removed in 3.0. When it's available in CacheControl, we should then start pointing towards that as a real solution that is correct and more reliable than using the redirect cache.

How does this sound to everyone?
",sigmavirus24,Ionrock
2408,2015-01-14 14:35:00,"We do not use `qop=auth-int`, we don't support it. When given multiple `qop` values like that in an auth challenge we're allowed to select one, and we have: `auth`.

You're right, however, about the fix. If we've chosen `auth` that's what we should use in our calculation. The simplest fix should be to pull the calculation of `noncebit` down into the only place it's used.

That said, can RFCreader @sigmavirus24 confirm that this is correct WRT the specs?
",Lukasa,sigmavirus24
2408,2015-01-14 14:58:30,"> That said, can RFCreader @sigmavirus24 confirm that this is correct WRT the specs?

Is this the new requests/urllib3 thing? If so, I thoroughly approve.
",sigmavirus24,sigmavirus24
2406,2016-07-21 17:16:12,"just reposting an issue I've been having related to this:

Hey guys, @selwin @ducu  @Lukasa @erdillon 

I've got an issue related to this (i think). But it's actually really easy to replicate. When making a request from a worker fails, the worker just dies, but no errors are reported nor is the job sent to the failed queue. 

my example:



When I queue `make_request` the process just quits when making the request. Whereas a traceback is thrown when I execute the script directs. Took me a while to track this down, is there a fix or something I may be doing wrong?
",dflatow,selwin
2399,2015-01-05 23:11:00,"Earlier today I was speaking with @ducu on [IRC](https://botbot.me/freenode/python-requests/2015-01-05/?msg=28835010&page=1) about a problem they're having using `rq` and `requests`. My intuition (which is really all I have to work with about this problem) is that all of the content on the response is not being consumed and people assume that doing something like:



The socket will close properly. If I remember correctly though, we've seen that if the socket isn't completely consumed, this won't allow the socket to be released (at least not immediately). I'm further suspicious this is the cause because killing one of the workers for `rq` lets the job resume. I wonder what people think about changing [`Response.close`](https://github.com/kennethreitz/requests/blob/7612015424c88cfd98f6be692ff994898aee1502/requests/models.py#L833) to be something like:



The call to self.content will consume the rest of the socket (if there's anything to consume) and then we'll release the connection (which should release the socket in this case).

Thoughts? Also I'm pretty tired so I could be wrong about some of this stuff.
",sigmavirus24,ducu
2399,2015-01-06 11:01:34,"(Note that doing this might belong in urllib3: @shazow?)
",Lukasa,shazow
2389,2014-12-23 16:51:12,"@Lukasa I'm just waiting for @Jaypipes to verify this fixes their issues.
",sigmavirus24,Jaypipes
2388,2014-12-22 23:23:49,"Thanks for this!

The logic doesn't look quite right. Here, if we get an empty section from a generator, we stop sending the chunked body altogether. That doesn't seem like the correct logic to me. We should really do one of two things:
1. Throw an exception. This means that users need to not send us empty chunks. This has the advantage of correctness, in that users are forced to choose what the correct behaviour is for empty chunks. It has the disadvantage of being annoying.
2. Skip the chunk. We cannot emit it, that would be incorrect, so the only way to 'muddle on through' is to skip it. This has the advantage of continuing to work in the majority of cases, and the disadvantage of being a bit surprising in the remainder, where we send something other than what the user provided us.

If we want to do option 2, that means changing `break` for `continue`. @sigmavirus24, thoughts?
",Lukasa,sigmavirus24
2378,2014-12-12 09:40:28,"Hello,

I'm moving some Python code from `urlopen` to `requests` (because it will be easier for me to add a db cache (SQLite) mechanism using requests-cache)

The code I want to modify looks like



I don't find any equivalent to `readlines()`

Any idea ?

I have found a generator solution like



but requests_cache doesn't seems to like this stream parameter
see https://github.com/reclosedev/requests-cache/issues/33

So there is 2 solutions (and both can be valuable).
1. fix requests-cache : that's what I'm asking to @reclosedev
2. provide a request readlines() method (or give a tip to have something like that which will return a list and not a generator)

That will be very nice.

Thanks for requests... that's a great lib for humans
",femtotrader,reclosedev
2375,2014-12-09 02:58:42,"Fedora and Debian have both recently added symlinks to their distributions of requests so people can do `from requests.packages import urllib3` but they seem to still rewrite our import statements. So the following situation is possible:
- User registers adapter for custom scheme
- User does the following:
  
  
- When they do `s.get('glance+https://...')` they see a `KeyError` because the urllib3 we're using in requests is not the one they've imported (according to `sys.modules`).

Pip works around this with the copied machinery. The tests seem to work just fine for me. I still need to test this with our vendored packages removed and urllib3/chardet installed separately.

/cc @dstufft @eriolv @ralphbean 
",sigmavirus24,ralphbean
2375,2014-12-09 02:58:42,"Fedora and Debian have both recently added symlinks to their distributions of requests so people can do `from requests.packages import urllib3` but they seem to still rewrite our import statements. So the following situation is possible:
- User registers adapter for custom scheme
- User does the following:
  
  
- When they do `s.get('glance+https://...')` they see a `KeyError` because the urllib3 we're using in requests is not the one they've imported (according to `sys.modules`).

Pip works around this with the copied machinery. The tests seem to work just fine for me. I still need to test this with our vendored packages removed and urllib3/chardet installed separately.

/cc @dstufft @eriolv @ralphbean 
",sigmavirus24,eriolv
2375,2014-12-09 02:58:42,"Fedora and Debian have both recently added symlinks to their distributions of requests so people can do `from requests.packages import urllib3` but they seem to still rewrite our import statements. So the following situation is possible:
- User registers adapter for custom scheme
- User does the following:
  
  
- When they do `s.get('glance+https://...')` they see a `KeyError` because the urllib3 we're using in requests is not the one they've imported (according to `sys.modules`).

Pip works around this with the copied machinery. The tests seem to work just fine for me. I still need to test this with our vendored packages removed and urllib3/chardet installed separately.

/cc @dstufft @eriolv @ralphbean 
",sigmavirus24,dstufft
2375,2014-12-09 14:27:43,"Hello,
I can understand how @lukasa is annoyed by unvendoring made downstream, but I can also understand why this is done. For example, in Debian, Python SSLv3 support was removed and I only had to patch (I forwared my patch upstream of course[¹]) urllib3 to fix all packages depending on urllib3.

So, yes, this is a downstream problem, but as in past (even when this issue arosed on Debian Bug Tracker) I will never add something without asking first if you, upstream developers, are ok with a downstream change. 
 As I said a lot of time before being the Debian maintainer of requests I'm one of its users: I want requests to be in the best shape in Debian. :)

I agree with @dstufft and @sigmavirus24 but, if you don't agree, I can also replace the currently used patch in Debian with this so, at least, Debian, Ubuntu and pip will use the same code.
IMHO cooperating we will arrive to the best solution for all.

[¹] http://anonscm.debian.org/viewvc/python-modules/packages/python-urllib3/tags/1.9.1-3/debian/patches/06_do-not-make-SSLv3-mandatory.patch?view=markup Yes, next time I will use a PR, fortunately @dstufft forwarded it properly! :)
",eriol,lukasa
2375,2014-12-09 14:40:29,"@eriolv question: Since the symlink is in place (it may just be Fedora that has this in place), is there any chance of the imports that import from `.packages` inside of requests could not be rewritten? 

If not, can the imports not be rewritten after we ship this patch? (After we've updated it to give proper attribution to @dstufft and pypa/pip) The crux of this issue is that sys.modules is incorrectly populated and needs to work a certain way for users to not run into surprises like this.

[/Edit - I submitted my comment too soon]
And I appreciate your collaboration @eriolv. That's why I pinged you immediately. I wanted to make you aware of this from the start and get your feedback as well as @Lukasa's and @ralphbean's
",sigmavirus24,ralphbean
2375,2014-12-09 14:40:29,"@eriolv question: Since the symlink is in place (it may just be Fedora that has this in place), is there any chance of the imports that import from `.packages` inside of requests could not be rewritten? 

If not, can the imports not be rewritten after we ship this patch? (After we've updated it to give proper attribution to @dstufft and pypa/pip) The crux of this issue is that sys.modules is incorrectly populated and needs to work a certain way for users to not run into surprises like this.

[/Edit - I submitted my comment too soon]
And I appreciate your collaboration @eriolv. That's why I pinged you immediately. I wanted to make you aware of this from the start and get your feedback as well as @Lukasa's and @ralphbean's
",sigmavirus24,eriolv
2375,2014-12-09 16:40:17,"My snarkiness was mostly the result of me waking up late and being late for work and having not had coffee, apologies all.

In reality I'm +0 on this. I don't like that we have to do it, and the unvendoring zealots have hardened my opinion towards the idea of doing them any favours on any issue whatsoever. (I don't include you in that group @eriolv, you have not displayed any zealotry that I'm aware of :wink: )

However, I acknowledge the Catch-22 of the fact that _we_ will get blamed for the zealots decision to unbundle us breaking their code. For that reason I have no intention of blocking this patch: punishing users is unacceptable.

However, I'd like _someone_ to test the change, as neither @sigmavirus24 nor @dstufft appear to have. Ideally I'd like some form of automated testing for it as well: having that would raise me to +0.5.
",Lukasa,eriolv
2372,2014-12-08 09:53:19,"Interesting. The Debian bug shows the time is in `cryptography`. Ping @alex?
",Lukasa,alex
2371,2014-12-08 17:10:48,"@nelhage did some stracing of the various examples (in the transfer
encoding: chunked case) https://gist.github.com/nelhage/dd6490fbc5cfb815f762
are the results. It looks like there's a bug in httplib which results in it
not always reading a full chunk off the socket.

On Mon Dec 08 2014 at 9:05:14 AM Kenneth Reitz notifications@github.com
wrote:

> Cake for @alex https://github.com/alex for being super helpful [image:
> :cake:]
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/2371#issuecomment-66147998
> .
",alex,nelhage
2371,2015-03-07 19:24:05,"@gardenia I haven't had a chance to absorb all of this, but thank you so much for your effort and work here. @shazow perhaps you'd find @gardenia's research interesting.
",sigmavirus24,shazow
2368,2014-12-04 05:21:59,"So that's also my first instinct @Lukasa but I don't think `getpeercert` would remove a wildcard `subjectAltName` entry randomly. Still, I'd like to know from @EthanBlackburn or @buttscicles what happens on Python 3 (preferably 3.3 or 3.4).
",sigmavirus24,buttscicles
2366,2014-12-05 14:24:29,"Hi @wttang 

For the most part, we actively discourage users from specifying they're own `Host` header. Because we discourage users from handling their own `Host` header, we do not remove it on redirect because 99% of the time there's no need for us to do so. By specifying this header yourself and not handling redirects yourself, we continue using the `Host` header.

We very well could just always attempt to remove the `Host` header but I'm -0.5 on this matter because it will almost certainly be removed by someone in the future regardless of how well we comment the surrounding lines of code. If you're making requests that require you to override things like this from requests, you should be prepared to also be handling redirects yourself.
",sigmavirus24,wttang
2366,2014-12-18 07:23:03,"@wttang It's not at all clear to me what you mean by that. www2.baidu.com works fine. Chrome or Firefox don't let you choose what host header you set, so they don't have _any_ behaviour in this situation.
",Lukasa,wttang
2366,2014-12-18 08:45:06,"@wttang Yeah, here we have a philosophical difference with cURL.

When you override our default headers we assume that you know better than us. This affects all kinds of things. For example, when you set the `Host` header we change the way we handle cookies. However, the API doesn't have the flexibility for you to be able to set the boundary for your pre-existing knowledge. You can't say ""the Host header should be this value, but only if we aren't redirected off-host, and use the old host header for cookie values, but use the new host header to validate the TLS certificate"". That's just too complicated to specify.

So instead we just pick which of those we think will apply and _always_ apply it. This means you can easily work out what a redirect will look like from your original command line, we don't mutate it under your feet to remove the special knowledge. I value that kind of transparency.

The other option is to say ""sure, you know more than us, but the second you get redirected you don't"". That will work fine too. However, it pushes the problem onto different users. Users who like our current behaviour (""leave Host set as-is"") will need to change their code, while you won't.

I don't think either position is correct or right (though if @bagder has time I'd love his input on why cURL chose its behaviour), and in that sort of situation I will always prefer the status quo.

I think this is ""working as intended"".
",Lukasa,wttang
2366,2014-12-18 08:45:06,"@wttang Yeah, here we have a philosophical difference with cURL.

When you override our default headers we assume that you know better than us. This affects all kinds of things. For example, when you set the `Host` header we change the way we handle cookies. However, the API doesn't have the flexibility for you to be able to set the boundary for your pre-existing knowledge. You can't say ""the Host header should be this value, but only if we aren't redirected off-host, and use the old host header for cookie values, but use the new host header to validate the TLS certificate"". That's just too complicated to specify.

So instead we just pick which of those we think will apply and _always_ apply it. This means you can easily work out what a redirect will look like from your original command line, we don't mutate it under your feet to remove the special knowledge. I value that kind of transparency.

The other option is to say ""sure, you know more than us, but the second you get redirected you don't"". That will work fine too. However, it pushes the problem onto different users. Users who like our current behaviour (""leave Host set as-is"") will need to change their code, while you won't.

I don't think either position is correct or right (though if @bagder has time I'd love his input on why cURL chose its behaviour), and in that sort of situation I will always prefer the status quo.

I think this is ""working as intended"".
",Lukasa,bagder
2365,2014-12-01 23:21:24,":cake: LGTM.

Ready when you are @kennethreitz.
",Lukasa,kennethreitz
2365,2014-12-01 23:22:14,"@kennethreitz wanted to get this out today and added both of us to PyPI as maintainers so I'm going to wrap this up right now.
",sigmavirus24,kennethreitz
2364,2015-07-18 05:12:13,"did you guys figure this one out? cc @dongnizh 
",chrismattmann,dongnizh
2364,2016-05-24 10:29:45,"@join-us Requests does not normally assume that plain text strings are form-encoded data. If you want them to be signaled as form-encoded, you need to pass a dictionary to data: `data={'cmd': 'date +%Y%m%d'}`. Alternatively, as you have done, you can set the header manually.
",Lukasa,join-us
2364,2016-05-24 10:41:10,"@join-us That provides no new information: curl isn't form encoding your data either. My advice in the previous post stands.
",Lukasa,join-us
2362,2014-12-01 19:51:59,"Keys to the kingdom for @Lukasa and @sigmavirus24.

This includes....
- [ ] DNSimple collab access]
- [x] PyPi collab access

I think that's it...
",kennethreitz,Lukasa
2362,2014-12-01 19:51:59,"Keys to the kingdom for @Lukasa and @sigmavirus24.

This includes....
- [ ] DNSimple collab access]
- [x] PyPi collab access

I think that's it...
",kennethreitz,sigmavirus24
2362,2014-12-01 20:44:27,"I'm not sure we need op on the channel. I've never seen any problems arise in there. 



Should allow @Lukasa and I to OP ourselves with ChanServ when necessary. Ideally not having OP on by default will be a positive impact on the channel's atmosphere 
",sigmavirus24,Lukasa
2362,2014-12-03 02:25:06,"@sigmavirus24 now has access to dnsimple. @Lukasa, can you tell me your dnsimple email address? 
",kennethreitz,Lukasa
2356,2014-11-25 19:34:12,"This bug is exactly the same as #1360, with one key difference: here, the server isn't percent-encoding percent signs. This is not valid HTTP, and we're totally allowed to fail here according to RFC 7231:

> Note: Some recipients attempt to recover from Location fields that are not valid URI references.  This specification does not mandate or define such processing, but does allow it for the sake of robustness.

However, I wonder if we can do better. Specifically, I wonder if we can update our `requote_uri` function to allow us to attempt to unquote it, and if that fails because of invalid percent-escape sequences we can just use the URL unchanged. That probably covers most of our bases, and it's gotta be better than failing hard like we do now.

@sigmavirus24, thoughts?
",Lukasa,sigmavirus24
2355,2014-11-25 00:18:28,"This adds support for unix domain sockets by using a slightly modified version of the unix domain socket adapter from the [docker-py](https://github.com/docker/docker-py) project:

https://github.com/docker/docker-py/blob/master/docker/unixconn/unixconn.py

The modifications are to make the adapter more usable in a greater variety of contexts by getting rid of the context of a `base_url` for the adapter and inventing a URL syntax that allows embedding the socket path into the URL. Basically the socket name goes in the netloc (host) part of the URL, but it's URL-encoded so that slashes become `%2F`'s so that the slashes don't interfere with separating the netloc and path parts of the URLs. This was roughly inspired by URL syntaxes that are mentioned in the following comments:
- http://daniel.haxx.se/blog/2008/04/14/http-over-unix-domain-sockets/
- http://lists.w3.org/Archives/Public/uri/2008Oct/0000.html

So for example if I had a server listening on `/tmp/profilesvc.sock`:



I can access it as follows:



In case you're curious about the motivation for adding unix domain socket support, it was inspired by:

https://github.com/jakubroztocil/httpie/issues/209

Cc: @shin-, @jakubroztocil, @monsanto, @np, @nuxlli, @matrixise, @remmelt
",msabramo,jakubroztocil
2355,2014-11-25 00:18:28,"This adds support for unix domain sockets by using a slightly modified version of the unix domain socket adapter from the [docker-py](https://github.com/docker/docker-py) project:

https://github.com/docker/docker-py/blob/master/docker/unixconn/unixconn.py

The modifications are to make the adapter more usable in a greater variety of contexts by getting rid of the context of a `base_url` for the adapter and inventing a URL syntax that allows embedding the socket path into the URL. Basically the socket name goes in the netloc (host) part of the URL, but it's URL-encoded so that slashes become `%2F`'s so that the slashes don't interfere with separating the netloc and path parts of the URLs. This was roughly inspired by URL syntaxes that are mentioned in the following comments:
- http://daniel.haxx.se/blog/2008/04/14/http-over-unix-domain-sockets/
- http://lists.w3.org/Archives/Public/uri/2008Oct/0000.html

So for example if I had a server listening on `/tmp/profilesvc.sock`:



I can access it as follows:



In case you're curious about the motivation for adding unix domain socket support, it was inspired by:

https://github.com/jakubroztocil/httpie/issues/209

Cc: @shin-, @jakubroztocil, @monsanto, @np, @nuxlli, @matrixise, @remmelt
",msabramo,remmelt
2355,2014-11-25 00:18:28,"This adds support for unix domain sockets by using a slightly modified version of the unix domain socket adapter from the [docker-py](https://github.com/docker/docker-py) project:

https://github.com/docker/docker-py/blob/master/docker/unixconn/unixconn.py

The modifications are to make the adapter more usable in a greater variety of contexts by getting rid of the context of a `base_url` for the adapter and inventing a URL syntax that allows embedding the socket path into the URL. Basically the socket name goes in the netloc (host) part of the URL, but it's URL-encoded so that slashes become `%2F`'s so that the slashes don't interfere with separating the netloc and path parts of the URLs. This was roughly inspired by URL syntaxes that are mentioned in the following comments:
- http://daniel.haxx.se/blog/2008/04/14/http-over-unix-domain-sockets/
- http://lists.w3.org/Archives/Public/uri/2008Oct/0000.html

So for example if I had a server listening on `/tmp/profilesvc.sock`:



I can access it as follows:



In case you're curious about the motivation for adding unix domain socket support, it was inspired by:

https://github.com/jakubroztocil/httpie/issues/209

Cc: @shin-, @jakubroztocil, @monsanto, @np, @nuxlli, @matrixise, @remmelt
",msabramo,matrixise
2355,2014-11-25 00:18:28,"This adds support for unix domain sockets by using a slightly modified version of the unix domain socket adapter from the [docker-py](https://github.com/docker/docker-py) project:

https://github.com/docker/docker-py/blob/master/docker/unixconn/unixconn.py

The modifications are to make the adapter more usable in a greater variety of contexts by getting rid of the context of a `base_url` for the adapter and inventing a URL syntax that allows embedding the socket path into the URL. Basically the socket name goes in the netloc (host) part of the URL, but it's URL-encoded so that slashes become `%2F`'s so that the slashes don't interfere with separating the netloc and path parts of the URLs. This was roughly inspired by URL syntaxes that are mentioned in the following comments:
- http://daniel.haxx.se/blog/2008/04/14/http-over-unix-domain-sockets/
- http://lists.w3.org/Archives/Public/uri/2008Oct/0000.html

So for example if I had a server listening on `/tmp/profilesvc.sock`:



I can access it as follows:



In case you're curious about the motivation for adding unix domain socket support, it was inspired by:

https://github.com/jakubroztocil/httpie/issues/209

Cc: @shin-, @jakubroztocil, @monsanto, @np, @nuxlli, @matrixise, @remmelt
",msabramo,shin-
2355,2014-11-25 00:18:28,"This adds support for unix domain sockets by using a slightly modified version of the unix domain socket adapter from the [docker-py](https://github.com/docker/docker-py) project:

https://github.com/docker/docker-py/blob/master/docker/unixconn/unixconn.py

The modifications are to make the adapter more usable in a greater variety of contexts by getting rid of the context of a `base_url` for the adapter and inventing a URL syntax that allows embedding the socket path into the URL. Basically the socket name goes in the netloc (host) part of the URL, but it's URL-encoded so that slashes become `%2F`'s so that the slashes don't interfere with separating the netloc and path parts of the URLs. This was roughly inspired by URL syntaxes that are mentioned in the following comments:
- http://daniel.haxx.se/blog/2008/04/14/http-over-unix-domain-sockets/
- http://lists.w3.org/Archives/Public/uri/2008Oct/0000.html

So for example if I had a server listening on `/tmp/profilesvc.sock`:



I can access it as follows:



In case you're curious about the motivation for adding unix domain socket support, it was inspired by:

https://github.com/jakubroztocil/httpie/issues/209

Cc: @shin-, @jakubroztocil, @monsanto, @np, @nuxlli, @matrixise, @remmelt
",msabramo,nuxlli
2355,2014-11-25 00:18:28,"This adds support for unix domain sockets by using a slightly modified version of the unix domain socket adapter from the [docker-py](https://github.com/docker/docker-py) project:

https://github.com/docker/docker-py/blob/master/docker/unixconn/unixconn.py

The modifications are to make the adapter more usable in a greater variety of contexts by getting rid of the context of a `base_url` for the adapter and inventing a URL syntax that allows embedding the socket path into the URL. Basically the socket name goes in the netloc (host) part of the URL, but it's URL-encoded so that slashes become `%2F`'s so that the slashes don't interfere with separating the netloc and path parts of the URLs. This was roughly inspired by URL syntaxes that are mentioned in the following comments:
- http://daniel.haxx.se/blog/2008/04/14/http-over-unix-domain-sockets/
- http://lists.w3.org/Archives/Public/uri/2008Oct/0000.html

So for example if I had a server listening on `/tmp/profilesvc.sock`:



I can access it as follows:



In case you're curious about the motivation for adding unix domain socket support, it was inspired by:

https://github.com/jakubroztocil/httpie/issues/209

Cc: @shin-, @jakubroztocil, @monsanto, @np, @nuxlli, @matrixise, @remmelt
",msabramo,np
2355,2014-11-25 00:18:28,"This adds support for unix domain sockets by using a slightly modified version of the unix domain socket adapter from the [docker-py](https://github.com/docker/docker-py) project:

https://github.com/docker/docker-py/blob/master/docker/unixconn/unixconn.py

The modifications are to make the adapter more usable in a greater variety of contexts by getting rid of the context of a `base_url` for the adapter and inventing a URL syntax that allows embedding the socket path into the URL. Basically the socket name goes in the netloc (host) part of the URL, but it's URL-encoded so that slashes become `%2F`'s so that the slashes don't interfere with separating the netloc and path parts of the URLs. This was roughly inspired by URL syntaxes that are mentioned in the following comments:
- http://daniel.haxx.se/blog/2008/04/14/http-over-unix-domain-sockets/
- http://lists.w3.org/Archives/Public/uri/2008Oct/0000.html

So for example if I had a server listening on `/tmp/profilesvc.sock`:



I can access it as follows:



In case you're curious about the motivation for adding unix domain socket support, it was inspired by:

https://github.com/jakubroztocil/httpie/issues/209

Cc: @shin-, @jakubroztocil, @monsanto, @np, @nuxlli, @matrixise, @remmelt
",msabramo,monsanto
2355,2014-11-25 00:27:35,"Probably if `requests` had this, docker-py and other projects might be able to make use of it and not have to maintain their own versions. I wonder if @shin- could use this in docker-py.
",msabramo,shin-
2355,2014-11-25 02:09:42,"So the fundamental thing to remember about requests is that it is an HTTP library which means we strive to be in compliance with RFC 7230, 7231, 7232, 7233, 7234, and 7235 and other RFCs that relate to how we have to transmit the data or communicate with the server (etc. etc. etc.). We primarily handle HTTP, we don't handle arbitrary URLs. That's a job for a library that will handle arbitrary URLs. (Hint: this was the point of urllib/urllib2)

Note, while people use requests for `unix://` or `file://`, we do not explicitly support these protocols purposefully. Our scope is very narrowly defined to 98% of our users concerns: HTTP. Those protocols are currently used through transport adapters and the adapters that provide this functionality are packaged separately.

If copying and pasting this code around is becoming a problem for projects (like httpie, docker-py, or whomever else) the solution is not to upstream the code here to be maintained by @Lukasa and me. The solution is to find people who care about maintaining it and make it its own library. If you think this code is too small for a library on its own, consider the fact that many of the auth libraries (kerberos, ntlm, etc.) are all similar in size to this code and are all packaged separately.

**tl;dr** We have no interest in making this a feature of the project or maintaining this code when the maintainers will not use it actively enough to be comfortable maintaining it.
",sigmavirus24,Lukasa
2353,2014-11-24 08:33:11,"I think this is safe to take. @sigmavirus24?
",Lukasa,sigmavirus24
2352,2014-11-22 18:11:47,"I really don't think this belongs in the quickstart section frankly. It isn't something most people will need as soon as they start using requests. It's more of an advanced usage piece of information, but even then our Advanced section is already quite large and overloaded. I'm not entirely convinced this shouldn't be it's own section though. I'd love to see more of the recipes you think belong there before we do that though and I'd would **really** like @Lukasa's input when they have a minute or two to spare.
",sigmavirus24,Lukasa
2349,2014-11-18 04:46:01,"> any way to add a test exposing this issue?

Besides the test that failed in the first place? Jenkins isn't running for a reason only @kennethreitz can determine. Had @Lukasa or I been running the tests locally before approving PRs, it probably wouldn't have reached this point.
",sigmavirus24,kennethreitz
2349,2014-11-18 04:46:01,"> any way to add a test exposing this issue?

Besides the test that failed in the first place? Jenkins isn't running for a reason only @kennethreitz can determine. Had @Lukasa or I been running the tests locally before approving PRs, it probably wouldn't have reached this point.
",sigmavirus24,Lukasa
2347,2014-11-17 04:40:53,"Given that requests doesn't handle sockets itself, I think this is a feature you should be requesting in urllib3. Before you open an issue there, I wonder if @shazow would care to express their opinion here first.
",sigmavirus24,shazow
2346,2014-11-18 04:26:41,"@daft117 Happy to help. 
",sigmavirus24,daft117
2345,2014-11-16 15:52:34,"Note to @kennethreitz, we **can not** do a release until we fix this.
",sigmavirus24,kennethreitz
2339,2014-11-14 11:03:01,"This feature really belongs in another library altogether. The problem is that the HTTP parsing logic in requests is in `httplib`, which is well down our stack and outside our control. This means we can't pull it out into common code.

_However_, @dstufft has been working on a formal HTTP parsing library which might be usable as a base for this use-case. What do you think, @dstufft?
",Lukasa,dstufft
2338,2014-11-13 17:40:24,"Yep..That is what I just tried. See below. Didn't work if I set it up right. Still get 407 error.

import requests
proxies = {
    ""http"": ""'dmorri':'@XYZ@12345'@proxy.shareddev.acxiom.net:8080"",
    ""https"": ""'dmorri':'@XYZ@12345'@proxy.shareddev.acxiom.net:8080"",
}
r = requests.get('https://epfws.usps.gov/ws/resources/epf/version', proxies=proxies)
print(r.text)
",dmorri,XYZ
2338,2014-11-13 17:40:24,"Yep..That is what I just tried. See below. Didn't work if I set it up right. Still get 407 error.

import requests
proxies = {
    ""http"": ""'dmorri':'@XYZ@12345'@proxy.shareddev.acxiom.net:8080"",
    ""https"": ""'dmorri':'@XYZ@12345'@proxy.shareddev.acxiom.net:8080"",
}
r = requests.get('https://epfws.usps.gov/ws/resources/epf/version', proxies=proxies)
print(r.text)
",dmorri,proxy
2338,2014-11-13 17:49:19,"Same error:

import requests
proxies = {
    ""http"": ""dmorri:@XYZ@12345@http://proxy.shareddev.acxiom.net:8080"",
    ""https"": ""dmorri:@XYZ@12345@http://proxy.shareddev.acxiom.net:8080"",
}
r = requests.get('https://epfws.usps.gov/ws/resources/epf/version', proxies=proxies)
print(r.text)
",dmorri,XYZ
2338,2014-11-13 17:52:29,"import requests
proxies = {
    ""http"": ""http://dmorri:@XYZ@12345@proxy.shareddev.acxiom.net:8080"",
    ""https"": ""http://dmorri:@XYZ@12345@proxy.shareddev.acxiom.net:8080"",
}
r = requests.get('https://epfws.usps.gov/ws/resources/epf/version', proxies=proxies)
print(r.text)
",dmorri,XYZ
2338,2014-11-13 17:54:06,"are you prepending an @ to your password?

## 

Kevin Burke
phone: 925.271.7005 | twentymilliseconds.com

On Thu, Nov 13, 2014 at 9:52 AM, dmorri notifications@github.com wrote:

> import requests
> proxies = {
> ""http"": ""http://dmorri:@XYZ https://github.com/XYZ@
> 12345@proxy.shareddev.acxiom.net:8080"",
> ""https"": ""http://dmorri:@XYZ https://github.com/XYZ@
> 12345@proxy.shareddev.acxiom.net:8080"",
> }
> r = requests.get('https://epfws.usps.gov/ws/resources/epf/version',
> proxies=proxies)
> print(r.text)
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/2338#issuecomment-62935509
> .
",kevinburke,XYZ
2338,2014-11-13 17:58:03,"So slightly different, but same type of issue. 

Kevin..What do you mean by pre-appending? The password has two @ symbols. as shown...I just changed to XYZ since I didn't want to post my actual pasword. But the layout is still the same. 
@xxx@11111
",dmorri,xxx
2336,2014-11-13 15:53:37,"That's an interesting question. @kevinburke, got any idea?
",Lukasa,kevinburke
2336,2014-11-13 19:47:13,"So... this is an error. 

The gist of it is, with HTTPS connections connect() gets called earlier in the urlopen() process than with a HTTP connection. Specifically, for HTTPS _new_conn will call _prepare_conn, which calls connect() on the socket, where for HTTP, the connect() call is delayed all the way until httplib.request.

urllib3 sets the connection timeout until after calling _get_conn, but before httplib.request, which explains why the timeout isn't set.

The obvious question is why isn't this an issue when you're not using a proxy? Well, prepare_conn only attempts the connect() if you're using a proxy, with the following note:



I'll discuss with @shazow and see if we can figure out a good course of action.
",kevinburke,shazow
2336,2014-12-03 15:03:39,"We haven't shipped the fix for this yet. Ping @shazow and @sigmavirus24

On Wednesday, December 3, 2014, Zhang Yibin notifications@github.com
wrote:

> I also encoutered a deadlock problem caused by this similar timeout issue.
> 
> What I did is sending a GET request to some site (like http://jsonip.com)
> using an HTTP proxy to get proxy's public ip address, the code is like:
> 
> s = requests.Session()
> s.proxies = {'http': 'http://xx.xx.xx.xx:xxx', 'https': 'http://xx.xx.xx.xx:xxx'}
> r = s.get('http://jsonip.com', timeout=5)
> r.raise_for_status()
> ...
> 
> After deadlock, the stack trace of the problmatic thread is (other threads
> are waiting for its completion):
> 
> File: ""C:\Python27\lib\threading.py"", line 783, in __bootstrap
>   self.__bootstrap_inner()
> File: ""C:\Python27\lib\threading.py"", line 810, in __bootstrap_inner
>   self.run()
> File: ""C:\Python27\lib\threading.py"", line 763, in run
>   self.__target(_self.__args, *_self.__kwargs)
> File: ""C:\Python27\lib\site-packages\concurrent\futures\thread.py"", line 73, in _worker
>   work_item.run()
> File: ""C:\Python27\lib\site-packages\concurrent\futures\thread.py"", line 61, in run
>   result = self.fn(_self.args, *_self.kwargs)
> File: ""E:\zbb\projects\crawler\new\zbb-crawler\zbb_proxymanager\app\proxy_filter.py"", line 45, in test
>   ip = self.public_ip.get(host)
> File: ""E:\zbb\projects\crawler\new\zbb-crawler\zbb_proxymanager\utils.py"", line 91, in get
>   ip = func(session, timeout)
> File: ""E:\zbb\projects\crawler\new\zbb-crawler\zbb_proxymanager\get_ip.py"", line 43, in get_ip_4
>   r = session.get('http://jsonip.com', timeout=timeout)
> File: ""C:\Python27\lib\site-packages\requests\sessions.py"", line 473, in get
>   return self.request('GET', url, *_kwargs)
> File: ""C:\Python27\lib\site-packages\requests\sessions.py"", line 461, in request
>   resp = self.send(prep, *_send_kwargs)
> File: ""C:\Python27\lib\site-packages\requests\sessions.py"", line 599, in send
>   history = [resp for resp in gen] if allow_redirects else []
> File: ""C:\Python27\lib\site-packages\requests\sessions.py"", line 192, in resolve_redirects
>   allow_redirects=False,
> File: ""C:\Python27\lib\site-packages\requests\sessions.py"", line 573, in send
>   r = adapter.send(request, **kwargs)
> File: ""C:\Python27\lib\site-packages\requests\adapters.py"", line 370, in send
>   timeout=timeout
> File: ""C:\Python27\lib\site-packages\requests\packages\urllib3\connectionpool.py"", line 513, in urlopen
>   conn = self._get_conn(timeout=pool_timeout)
> File: ""C:\Python27\lib\site-packages\requests\packages\urllib3\connectionpool.py"", line 231, in _get_conn
>   return conn or self._new_conn()
> File: ""C:\Python27\lib\site-packages\requests\packages\urllib3\connectionpool.py"", line 717, in _new_conn
>   return self._prepare_conn(conn)
> File: ""C:\Python27\lib\site-packages\requests\packages\urllib3\connectionpool.py"", line 690, in _prepare_conn
>   conn.connect()
> File: ""C:\Python27\lib\site-packages\requests\packages\urllib3\connection.py"", line 217, in connect
>   self._tunnel()
> File: ""C:\Python27\lib\httplib.py"", line 752, in _tunnel
>   (version, code, message) = response._read_status()
> File: ""C:\Python27\lib\httplib.py"", line 365, in _read_status
>   line = self.fp.readline(_MAXLINE + 1)
> File: ""C:\Python27\lib\socket.py"", line 476, in readline
>   data = self._sock.recv(self._rbufsize)
> 
> I think it's the same bug discussed in this issue, right? The version of
> Requests is 2.5.0.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/2336#issuecomment-65412519
> .

## 

## 

Kevin Burke
phone: 925.271.7005 | twentymilliseconds.com
",kevinburke,shazow
2336,2016-07-23 15:21:36,"Hooray! So, that points to a bug in the Ubuntu version of this package. @eriol, you might want to be aware of this.
",Lukasa,eriol
2334,2015-04-03 12:18:42,"@vincentxb : thanks for your feedback. You are correct. @tardyp made the same comment in #2523. I'll go ahead and extend the per-thread storage to the other state attributes.
",exvito,tardyp
2332,2014-11-12 14:58:25,"This is related to https://github.com/kennethreitz/requests/issues/2329

Just implemented @Lukasa's solution and added test.
",asnelzin,Lukasa
2331,2014-11-12 14:26:42,"Thanks for raising this!

Unfortunately, this bug lives a long way down the stack, in `httplib`. Specifically, `httplib` never checks the socket to see if there is any data to read before sending the body.

What you've actually stumbled upon, however, is one of the trickiest parts of HTTP/1.1: how do we know we have a response to read? It's not enough to poll for data after we send the headers, because the response might be in flight and we could miss it. So we'd have to poll after each data send. That's not enough either: it's possible that we could send some data, causing the server to emit the response and RST packet that reaches us _after_ we poll (so not marking the socket readable) but _before_ we send again, therefore still causing the `Connection reset by peer` error. This gets more likely in the event of threading etc. causing us to have to wait.

Handling all of this properly is extremely tricky and `httplib` just doesn't do it, so while we're using `httplib` we cannot get around it. One day I'd like to replace `httplib`, but until that happens we're between a rock and a hard place.

An interesting idea is whether we can take the socket with us when we throw these exceptions, so that the user can read from them if they want to. However, that breaks our connection re-use where we attempt to re-use socket objects: we can't do that if the user has a handle to them.

I'm tempted to call this a ""can't fix"". @sigmavirus24?
",Lukasa,sigmavirus24
2329,2014-11-11 22:35:35,"Eh, I'm on the fence. I don't think requests should be able to generate any invalid input provided to it by the user. We're in the position of making user's lives easier, and sometimes that means ignoring their mistakes.

However, this violates our general principle of leaving the user's headers where they are. That means I'm +0.5 on fixing it. The fix is easy: in `PreparedRequest.prepare_content_length`, prevent falling into the last `elif` block by adding `and self.headers.get('Content-Length') is None`.

@sigmavirus24, does that sound reasonable?
",Lukasa,sigmavirus24
2313,2014-10-29 18:13:27,"Oh, yes, I remember now.

POSTing files with unicode filenames is awkward, because you didn't say what text encoding you want us to use. There's a spec for this, which we implement, but relatively few others do it and many servers don't understand it.

My suggested workaround would be to set the filename yourself using whatever encoding you choose. Unfortunately, that doesn't work:



The problem here seems to be [this line](https://github.com/kennethreitz/requests/blob/master/requests/packages/urllib3/fields.py#L37). This unconditional call to encode will actually cause an implicit call to `str.decode` on Python 2, which breaks for non-ascii characters. @shazow, you prepared to consider that a bug?
",Lukasa,shazow
2308,2014-10-27 18:16:12,"And this is going to cause a merge conflict with #2216. If @Lukasa doesn't object, we can merge this now and I'll update #2216 soon there after with a merge of master.
",sigmavirus24,Lukasa
2306,2014-10-26 13:32:04,"@ssadler No. I wanted you to fix [the test](https://github.com/kennethreitz/requests/blob/master/test_requests.py#L1514) by passing `hooks=[]` to the `prepare` method.
",sigmavirus24,ssadler
2306,2014-10-27 00:32:38,"Excellent @ssadler ! Thanks!
",sigmavirus24,ssadler
2305,2014-10-25 14:11:26,"Hey @ssadler thanks for this, but we already had a different [PR](https://github.com/kennethreitz/requests/pull/2253) for this, which is much simpler. If you want to send a separate PR for that test failure, please do and still add yourself to the AUTHORS file. I'll merge that ASAP
",sigmavirus24,ssadler
2287,2015-04-07 13:50:06,"@Lukasa is it still worth keeping this open?
",sigmavirus24,Lukasa
2283,2014-10-15 02:25:24,"I'm not sure that not using it by default is enough - if an attacker can interrupt the TLS v1 protocol and force the connection to downgrade to ssl v3, it's enough to read the plaintext.

I am not sure how to test this but I am reading through the code to try and understand it. Would appreciate some help from @t-8ch @alex @reaperhulk @dreid .. :)
",kevinburke,reaperhulk
2283,2014-10-15 02:25:24,"I'm not sure that not using it by default is enough - if an attacker can interrupt the TLS v1 protocol and force the connection to downgrade to ssl v3, it's enough to read the plaintext.

I am not sure how to test this but I am reading through the code to try and understand it. Would appreciate some help from @t-8ch @alex @reaperhulk @dreid .. :)
",kevinburke,alex
2283,2014-10-15 02:25:24,"I'm not sure that not using it by default is enough - if an attacker can interrupt the TLS v1 protocol and force the connection to downgrade to ssl v3, it's enough to read the plaintext.

I am not sure how to test this but I am reading through the code to try and understand it. Would appreciate some help from @t-8ch @alex @reaperhulk @dreid .. :)
",kevinburke,t-8ch
2283,2014-10-15 02:25:24,"I'm not sure that not using it by default is enough - if an attacker can interrupt the TLS v1 protocol and force the connection to downgrade to ssl v3, it's enough to read the plaintext.

I am not sure how to test this but I am reading through the code to try and understand it. Would appreciate some help from @t-8ch @alex @reaperhulk @dreid .. :)
",kevinburke,dreid
2282,2014-10-14 15:17:43,"@kennethreitz you okay with this?
",sigmavirus24,kennethreitz
2273,2014-10-10 02:07:53,"Since @kennethreitz is very passionate about the docs, I want to make sure he's okay with this before we merge it @alex 
",sigmavirus24,kennethreitz
2273,2014-10-10 02:07:53,"Since @kennethreitz is very passionate about the docs, I want to make sure he's okay with this before we merge it @alex 
",sigmavirus24,alex
2272,2014-10-11 13:22:32,"I haven't run the code yet, but this can (probably) be solved in a few ways:
1. Use grequests because it looks to do The Right Thing
2. Figure out a way to make gevent monkey patch the `queue` module with it's [own](http://www.gevent.org/gevent.queue.html) (although I have yet to test that this will work)
3. (And this will be the easiest) Use a separate session for each ""thread"". The problem is that the `LifoQueue` from the standard library is **threadsafe** but that does not imply gevent-safe. This will ensure separate queues for your program.

We _could_ add something like





But we probably won't. The reason this might work is because when using `gevent.monkey.patch_all()` you're patching the threading library and so gevent will ensure that this will not hang. I don't have the time to test all of this, but connection pooling is almost certainly the problem here. @shazow do you have any thoughts on this matter?
",sigmavirus24,shazow
2268,2014-10-05 23:56:27,"Address @kevinburke's concerns
",sigmavirus24,kevinburke
2267,2014-10-06 00:17:12,"cc @buttscicles 
",sigmavirus24,buttscicles
2263,2014-10-03 18:50:43,"This looks great @daftshady, thanks! :cake: I'll let @sigmavirus24 review, but I'm :+1:.
",Lukasa,sigmavirus24
2262,2014-10-02 23:19:13,"Hello,

I've looked through open and closed issues but haven't seen this raised before. Sorry if I'm bringing up something that's already been addressed. That said, here we go:

I'm running a flask-restful app and returning JSON responses to requests. Flask-restful helpfully encodes the response body as UTF-8 and sets the Content-Type to ""application/json"". When i make a request to my server using requests, requests can't properly detect the encoding and it looks like this is because I haven't set the charset on the Content-Type header. However, according to [RFC 4627](http://www.ietf.org/rfc/rfc4627.txt) the encoding for application/json responses is either UTF-8, UTF-16, or UTF-32. The default is UTF-8, but you can detect which one of the three encodings is being used by looking at the first four bytes as mentioned in [RFC 4627](http://www.ietf.org/rfc/rfc4627.txt).

When requests receives a response with a Content-Type header of application/json shouldn't it try to detect the encoding as UTF-8, UTF-16, or UTF-32 and set the encoding on the response object accordingly? This would help greatly with using `response.text`. Right now I'm setting the encoding on the response object manually. 

Just to note: `response.json()` works perfectly so I realize this is a bit of a corner case since users will probably just deserialize the response anyway. 

Thanks for your time, and all your awesome work. I absolutely love requests.
Mike

Possibly Related Issues: #1665 #1467 
Reference to @mitsuhiko's opinion on the matter: https://github.com/mitsuhiko/flask/issues/454#issuecomment-4578200
",noseworthy,mitsuhiko
2262,2014-10-03 18:46:40,"@Opethil Our pleasure, thanks for taking the time to talk this through with us. =) Users like you are a pleasure to work with, keep it up!
",Lukasa,Opethil
2258,2014-10-04 14:04:15,"@Lukasa @kevinburke care to take another look at this?
",sigmavirus24,kevinburke
2258,2014-10-04 14:04:15,"@Lukasa @kevinburke care to take another look at this?
",sigmavirus24,Lukasa
2258,2014-10-05 16:49:29,"@willingc thank you so much for your hard work on this. :cake: (Those emoji from @kennethreitz are for you ;))
",sigmavirus24,willingc
2258,2014-10-05 17:50:15,"Thank you @sigmavirus24 for your support and mentoring :guitar: :sunrise: :evergreen_tree: , @lukasa for the review :cat: :cat2: :smile_cat:  , and @kennethreitz for the community atmosphere :camera: :v: :guitar:
",willingc,lukasa
2255,2014-09-29 06:57:07,"Yeah, this is almost certainly connection pooling. urllib3 puts the connection back when SSL errors are raised, but I really don't know if we can safely do that. @shazow, @t-8ch, what are your thoughts about not re-using connections that raise SSL errors?
",Lukasa,shazow
2255,2014-09-29 06:57:07,"Yeah, this is almost certainly connection pooling. urllib3 puts the connection back when SSL errors are raised, but I really don't know if we can safely do that. @shazow, @t-8ch, what are your thoughts about not re-using connections that raise SSL errors?
",Lukasa,t-8ch
2249,2014-09-25 20:22:05,"This _looks_ right, but it's hard for me to know because URLs are a crazy mess. I'll let @sigmavirus24 review this, he knows URLs better than I do.
",Lukasa,sigmavirus24
2249,2014-09-26 14:52:49,"So if we used a better (read: less forgiving) URL parser library we wouldn't need to split on `@` (like we do [here](https://github.com/kennethreitz/requests/pull/2249/files#diff-5956087d5835a57d9ef6fff974f6fd9bR687)). In principle this works fine. In reality, we should get something like `rfc3986` into acceptable shape for @shazow or just make the `Url` object in `urllib3` more RFC compliant (whichever works better) and use that for parsing the URL and reconstructing it. In short, you have 5 major components of the URL:



And `authority` which we're dealing with right now has 3 sub-components so a URL would look like:



`rfc3986` would split this up and allow you to replace `userinfo` with `None` and then reconstruct the URL. For a quick fix, this is great. I'd rather not have so much URL/URI parsing logic in requests though, this is an HTTP library not an HTTP + URL + ... library.
",sigmavirus24,shazow
2244,2014-09-23 14:43:52,"But yes, I suspect we could subclass `RedirectLoopError` from `TooManyRedirects` and raise that so as to save the calls to the network that would otherwise cause us to exceed the max number of retries. I'm not opposed to that. I'd like to hear other opinions though from @fcosantos and @RuudBurger (and anyone else that has one).

@kennethreitz thoughts on saving people from hitting the network more than necessary when we can detect an endless redirect loop?
",sigmavirus24,kennethreitz
2244,2014-09-23 14:43:52,"But yes, I suspect we could subclass `RedirectLoopError` from `TooManyRedirects` and raise that so as to save the calls to the network that would otherwise cause us to exceed the max number of retries. I'm not opposed to that. I'd like to hear other opinions though from @fcosantos and @RuudBurger (and anyone else that has one).

@kennethreitz thoughts on saving people from hitting the network more than necessary when we can detect an endless redirect loop?
",sigmavirus24,fcosantos
2244,2014-09-23 14:43:52,"But yes, I suspect we could subclass `RedirectLoopError` from `TooManyRedirects` and raise that so as to save the calls to the network that would otherwise cause us to exceed the max number of retries. I'm not opposed to that. I'd like to hear other opinions though from @fcosantos and @RuudBurger (and anyone else that has one).

@kennethreitz thoughts on saving people from hitting the network more than necessary when we can detect an endless redirect loop?
",sigmavirus24,RuudBurger
2241,2014-09-22 16:39:40,"Let's have a subclass of `RequestException`. My proposed name is `BodyConsumedError`: @sigmavirus24, do you have a better one?
",Lukasa,sigmavirus24
2238,2014-09-20 22:22:20,"Thanks for this!

I think we like the ability to pass non-strings to `prepare_url`. It allows people to use custom url-building classes without running into trouble. It's worth noting that you change also breaks Python 2 behaviour: previously a Python 2 `str` would get lifted to a Python 2 `unicode`, which it now doesn't do.

I think we can get around this by simply special-casing string types. The logic is really:



I think that's really the logic we want here. @sigmavirus24, thoughts?
",Lukasa,sigmavirus24
2238,2014-09-30 18:49:08,":heart: @buttscicles 

@Lukasa this looks okay to me. Thoughts?
",sigmavirus24,buttscicles
2237,2014-09-21 16:14:04,":cake: Thanks @buttscicles 
",sigmavirus24,buttscicles
2235,2014-09-19 06:18:42,"@sigmavirus24, can I get your thoughts here?
",Lukasa,sigmavirus24
2228,2014-09-16 15:26:37,"Thanks for the suggestion! However, I disagree.

In many cases, users find it valuable to add a timeout to their calls. This is entirely reasonable. However, the quickstart code is not an example of 'general use', it's intended to be illustrative and educational. We should not obscure the pedagogical point in the pursuit of some form of pure correctness.

There is a [timeout section](http://docs.python-requests.org/en/latest/user/quickstart/#timeouts) in the quickstart docs, which we believe to be sufficient. Do you disagree?

As for mentioning the default socket timeout, I hadn't spotted that. That may actually be a bug. @sigmavirus24, what do you think: should we use the default socket timeout when no value is provided? If so, how do users specify they want no timeout at all? Am I overthinking the whole thing?
",Lukasa,sigmavirus24
2228,2014-09-16 20:59:02,"I feel like `timeout=0` is probably clear enough.

I think we stopped using it when @kevinburke did his really major timeout work, and we didn't spot that urllib3 uses a sentinel object where we use the `None` pattern. Just some crossed wires.
",Lukasa,kevinburke
2222,2014-09-12 02:43:06,"@jvantuyl
Can you remember and elaborate on why you've made this change (https://github.com/kennethreitz/requests/commit/b149be5d864cc7f65ac22113db3d0e4d2ed2b49e)?
",blueyed,jvantuyl
2222,2014-09-12 02:56:16,"Also I'm fairly certain it allowed @asmeurer to use the file protocol in conda/conda.
",sigmavirus24,asmeurer
2218,2014-09-11 12:21:27,"Yeah I just found that myself and saw @Lukasa's [comment](https://github.com/requests/requests-oauthlib/pull/147#issuecomment-55255020). That's exactly how we feel and further this already exists elsewhere. Finally, URLObject's (un)license would prohibit us from actually being able to vendor it in this case. I don't believe it's compatible with Apache v2. [rfc3986](/sigmavirus24/rfc3986) would be vendorable but even so, we won't accept this functionality in requests.
",sigmavirus24,Lukasa
2216,2014-09-10 16:12:40,"Currently our documentation tells user that by creating a custom `HTTPAdapter` they can pass `max_retries=n` and that will apply to connection and read retries. This is a lie at the moment (and has been probably since 2.4.0 was released with the new version of urllib3 that added the `Retry` class).

In order to preserve backwards compatibility (and have passing tests) I remember upgrading the logic [here](https://github.com/kennethreitz/requests/blob/master/requests/adapters.py#L361) to pass `read=False` because some where we expected a plain `ReadTimeout` exception we were now getting a `MaxRetryError`. This, however, means that we have broken what we promised in the documentation (and what was likely present in even 2.3.0).

At the moment we do not support passing a `Retry` instance from `urllib3`. I'd like to propose the following changes:
- In `requests.adapters`, `DEFAULT_RETRIES` becomes a sentinel object, e.g., `DEFAULT_RETRIES = object()`.
- In `HTTPAdatper.__init__`, we check for the sentinel object and if so set `self.max_retries` to `Retry(0, read=False)`. Otherwise, we do `self.max_retries = Retry.from_int(max_retries)`

This should fix preserve the default existing behaviour that specifying `read=False` preserved but also allow users to do one of two things:
1. Import `Retry` from urllib3, construct their own `Retry` instance and then pass it as `max_retries`
2. Specify a number for both connection and read retries.

Does this sound like a fair proposal?

**Edit** I forgot to mention that @txtsd brought this to our attention in IRC. They get the credit for reporting this. Thank you @txtsd 
",sigmavirus24,txtsd
2214,2014-09-09 22:24:09,"At this point it's fairly obvious that @Lukasa and I are -1 on this feature. @kennethreitz @shazow any opinions?
",sigmavirus24,kennethreitz
2214,2014-09-09 22:24:09,"At this point it's fairly obvious that @Lukasa and I are -1 on this feature. @kennethreitz @shazow any opinions?
",sigmavirus24,shazow
2210,2014-09-07 17:03:09,"@Lukasa wrote the fix in #2207 

With this, the expected behaviour is restored (a `TooManyRedirects` error);


",sigmavirus24,Lukasa
2209,2014-09-07 15:07:43,"So the calculation of the elapsed time per-request is done in `requests/sessions.py` and hasn't changed between 2.2.0 and 2.4.0. It still does this:



This is entirely related to something that @kevinburke pointed out though. In [HTTPAdapter#send](https://github.com/kennethreitz/requests/blob/master/requests/adapters.py#L315) we do not use the `stream` parameter any longer. In 2.2.0, if stream was not truthy, at the end of the `send` method we would do `r.content` which would consume everything. I'm not sure when or why that was removed, but I'll check into that.

**Edit** This check for `stream` is also not present in [2.3.0](https://github.com/kennethreitz/requests/blob/6366d3dd190a9e58ca582955cddf7e2ac5f32dcc/requests/adapters.py#L380..L388). It is present in [2.2.1](https://github.com/kennethreitz/requests/blob/33735480f77891754304e7f13e3cdf83aaaa76aa/requests/adapters.py#L393)
",sigmavirus24,kevinburke
2209,2014-09-07 15:21:22,"So the actual change was made by @schlamar in https://github.com/kennethreitz/requests/commit/59c8d813818110aac29fd104c2fa012387c2004c. It was part of https://github.com/kennethreitz/requests/pull/1944. Now to figure out why that was moved. It's not apparent from the bug report that it was necessary to move it to `Session#send`.
",sigmavirus24,schlamar
2209,2014-09-07 15:27:38,"Final comment of the morning (as I have other things I have to do), I'm not sure there is anything preventing us from moving that consumption back to `HTTPAdapter#send`. I'm genuinely interested in @schlamar's reason for moving it though if they can remember it.

Also, @heyman thank you for changing the test style. We may not end up merging this right away, but your cooperation and bug report are much appreciated.
",sigmavirus24,schlamar
2206,2014-09-05 20:36:43,"There is a good reason not to do it on the module scope. That function is so rarely used by 98% of requests users that triggering the compile when they do `import requests` will hurt their performance for no real benefit. That said, how would you determine which was the first encoding in the body? You could do something like:



But that seems kind of arbitrary. On the one hand, returning all of them is also arbitrary but if we had written it the way you want, someone else would have wanted us to write it the way it is currently written.

To address your concern about `get_encoding_from_headers` that is now not conformant with RFC 7230 and we have plans to change that in #2086. Right now, we _could_ accept that pull request but I'm not sure if it's desirable. I'll have to think about it and wait for @Lukasa would have to weigh in too.
",sigmavirus24,Lukasa
2204,2014-09-04 19:11:06,"@kevinburke Did retry logic change to not raise MaxRetryErrors when `retries=0`?
",Lukasa,kevinburke
2203,2014-09-04 18:45:01,"@kevinburke Done. =)
",Lukasa,kevinburke
2200,2014-09-03 10:06:40,"On the topic of requests not mapping exceptions to its own exceptions...

`requests.packages.urllib3.exceptions.ProtocolError` is not caught. Reproduction: invalid status line.

Server:
`$ echo | nc -l 8000`

requests:



Output: <class 'requests.packages.urllib3.exceptions.ProtocolError'>

I would expect this to be mapped to a requests exception.

Connection refused comes out also as ProtocolError.

This is requests 2.4.0.

Ping @Lukasa , @sigmavirus24 

Also happens with connection refused. This should be covered by requests unittests in the future.


",joneskoo,Lukasa
2200,2014-09-03 10:06:40,"On the topic of requests not mapping exceptions to its own exceptions...

`requests.packages.urllib3.exceptions.ProtocolError` is not caught. Reproduction: invalid status line.

Server:
`$ echo | nc -l 8000`

requests:



Output: <class 'requests.packages.urllib3.exceptions.ProtocolError'>

I would expect this to be mapped to a requests exception.

Connection refused comes out also as ProtocolError.

This is requests 2.4.0.

Ping @Lukasa , @sigmavirus24 

Also happens with connection refused. This should be covered by requests unittests in the future.


",joneskoo,sigmavirus24
2199,2014-09-04 04:18:20,"Nevermind, I asked @r1chardj0n3s and he assuaged my concerns.
",sigmavirus24,r1chardj0n3s
2195,2014-08-30 13:24:11,"I really have no opinion on the name. I would think `with_sni` (i.e., `pip install requests[with_sni]` reads well) would be most descriptive of the use case for the largest number of users, but `pyopenssl` is fine too. I'll wait for @kennethreitz to decide.
",sigmavirus24,kennethreitz
2189,2014-08-29 18:59:37,"The only person who can fix this is @kennethreitz 
",sigmavirus24,kennethreitz
2185,2014-08-27 12:22:05,"My code



raises an exception socket.error; I would expect it to be mapped to requests.exceptions instead.



@Lukasa, since you fixed another exception handling issue already, would you care to take a look at some point? Easy enough to work around but should be fixed to maintain a human API.
",joneskoo,Lukasa
2185,2014-08-27 12:23:32,"Agreed. Either we should catch it ourselves, or urllib3 should catch this stuff from read or stream. @shazow?
",Lukasa,shazow
2184,2015-11-08 05:08:45,"@lukasa did you try the version I released a a couple weeks with a threaded server?  You shouldn't be getting test failures.  The error messages are still a problem though.  Noticed the requirements.txt in requests still has pytest-httpbin pinned to an older version so I'm wondering if you didn't see my above message from 14 days ago.  Regardless, I'm super excited you're using my library.  Cheers!
",kevin1024,lukasa
2183,2014-08-26 19:20:57,"@kennethreitz?
",Lukasa,kennethreitz
2180,2014-08-26 05:41:07,"Good catch! @sigmavirus24, you know the GitHub API best, what endpoint should we hit?
",Lukasa,sigmavirus24
2177,2014-08-23 20:17:21,"@kennethreitz, we're going to need your input on this one.
",Lukasa,kennethreitz
2176,2014-08-26 19:15:58,"I would petition that we wait until @willingc can get her `json` parameter pull request in good shape for us to review.
",sigmavirus24,willingc
2173,2014-08-22 08:45:23,"Concerning #2169, i get a problem with automatic keep alive.
@sigmavirus24 has commented this saying that was in documentation but can have been lost.
",tychotatitscheff,sigmavirus24
2169,2014-08-26 19:25:37,"I wonder if @kevinburke or someone else removed it in one of their PRs modifying the default headers we set
",sigmavirus24,kevinburke
2169,2014-08-27 15:04:17,"Seems legit.
Good luck and please let me know if you need my help for anything.
I just poke @shazow, so he can read the wrong (imao) rewrapping of `SocketError` to `ProxyError` (first post) and say me if i shall open an issue on https://github.com/shazow/urllib3
Cheers
",tychotatitscheff,shazow
2162,2014-08-12 02:10:44,"@fenume they're not ""lost"". See @lukasa's [comment](https://github.com/kennethreitz/requests/issues/1446#issuecomment-20658864) on #1446. This was intentional because a single response's CookieJar should represent the cookies sent by the server in _that_ response. If you want what used to work in version 1.2.0, I would advise you start using a session.


",sigmavirus24,lukasa
2159,2014-08-02 14:35:22,"My only hesitation is @kennethreitz's previous resistance to the idea. Perhaps with the relatively recent release of the new version of HTTP/1.1 (which removes the necessity to support HTTP/0.9) he will be more willing.
",sigmavirus24,kennethreitz
2159,2014-08-02 15:18:54,"I'm confident I could sell him on the idea. =)

> On 2 Aug 2014, at 10:35, Ian Cordasco notifications@github.com wrote:
> 
> My only hesitation is @kennethreitz's previous resistance to the idea. Perhaps with the relatively recent release of the new version of HTTP/1.1 (which removes the necessity to support HTTP/0.9) he will be more willing.
> 
> —
> Reply to this email directly or view it on GitHub.
",Lukasa,kennethreitz
2158,2014-08-01 18:20:47,"Hey @zapman449!

Thanks for your suggestion! We really appreciate the fact that you took the time to file this. Unfortunately we're not adding anything new to the requests API at this time.

I'll wait for @Lukasa to chime in before closing this though.
",sigmavirus24,Lukasa
2158,2016-04-06 07:48:57,"@shaky You seem to be using Python 3. That changes the layout of the socket object. You should change your `ip =` line to `ip = r.raw._fp.fp.raw._sock.getpeername()`.
",Lukasa,shaky
2155,2014-08-01 14:56:06,"So this actually leads to something that's been rattling around in my head for a while and which I haven't proposed yet because it would be a significant API change.

I don't like the fact that we suggest people use `r.raw` because it's an object which we don't document and it's an object provided by `urllib3` (which we've claimed in the past is more of an implementation detail). With that in mind, I've been toying with the idea of providing methods on a `Response` object which just proxy to the `urllib3` methods (`read` would just proxy to `raw.read`, etc.). This gives us extra flexibility around `urllib3` and allows us to handle (on behalf of the users) an API change in `urllib3` (which historically has almost never been a problem, so there isn't any urgency in that).

With that said, we already have enough methods on a Response object in my opinion and growing our API isn't ideal. The best API is the API from which there's nothing left to remove. So I'm continuously on the fence about this.

---

> This assumption was also at the core of my original request to default decode_content to True. Of course now that I see what a leaky abstraction this is, I'm no longer suggesting that.

For others who find this and may not be certain why this is true, allow me to explain.

There are several users of requests who turn off automatic decompression to validate the length of a response, or to do other important things with it. One consumer of the former kind is OpenStack. Many of the OpenStack clients validate the `Content-Length` header sent to the client and the actual length of the body received. To them, handling decompression is a fair trade-off to be certain they're receiving and handling a valid response.

Another consumer is Betamax (or really any tool that (re)constructs Response objects) because when it is handling the full process of making a totally valid response, it needs the content to be in the compressed format.

I'm sure there are others that neither @Lukasa or I know about that also rely heavily on this behaviour.
",sigmavirus24,Lukasa
2148,2014-07-24 21:31:13,"@sigmavirus24 @alex Fixed. Not sure what I was thinking, there.
",romanlevin,alex
2146,2014-07-23 06:14:57,"I have a background worker that makes many http requests to various APIs. I've found that every time my job runs, a few requests appear to just ""hang"" for quite a long time.
I've investigated further and it appears the requests are not actually hanging, but my python code ""hangs"" when it invokes `OpenSSL's` `SSL_CTX_load_verify_locations` function.

I forked `requests` and added some more logging. The last entry to be logged in my code is `begin: ssl_wrap_socket: load verify locations` and that corresponds to [this line](https://github.com/WiFL-co/requests/blob/master/requests/packages/urllib3/util/ssl_.py#L118).

One possible conclusion is that SSL_CTX_load_verify_locations is hanging because of parallel file access or the version of OpenSSL on my Heroku dyno has a bug. FYI the `requests` library is calling python's `load_verify_locations` which is what then calls `OpenSSL`. I am not invoking these functions manually.

[I've created a question on StackOverflow](http://stackoverflow.com/questions/24628866/why-does-my-program-hang-after-urllib3-logs-starting-new-https-connection) that provides extensive information. I am using the following dependencies:



@Lukasa has been really helpful thus far, and I've been able to narrow it down. I figured i'd throw this issue up and see if anyone else has some insight.
",scottccoates,Lukasa
2142,2014-07-21 18:46:08,"It seems urllib3 changed the timeout logic such that, we now have a failing test (`test_stream_timeout`). We now receive a `MaxRetryError` instead of a `TimeoutError` from urllib3. Either we have to adjust or this is a bug that needs to be fixed in urllib3.

Thoughts @shazow @lukasa?
",sigmavirus24,lukasa
2142,2014-07-21 18:46:08,"It seems urllib3 changed the timeout logic such that, we now have a failing test (`test_stream_timeout`). We now receive a `MaxRetryError` instead of a `TimeoutError` from urllib3. Either we have to adjust or this is a bug that needs to be fixed in urllib3.

Thoughts @shazow @lukasa?
",sigmavirus24,shazow
2142,2014-07-21 18:59:08,"Don't think it's strictly a urllib3 bug: I seem to recall @shazow had a comment in his urllib3 code wondering why `Timeout`s weren't wrapped in `MaxRetryError`s.

As for what to do, I'm not sure. Check what the wrapped exception is, maybe?
",Lukasa,shazow
2141,2014-07-18 17:16:21,"This looks good to me. We're already using the print function in several places on the advanced.rst page. Any objections @Lukasa ?
",sigmavirus24,Lukasa
2129,2014-07-22 20:23:04,"There are some nice changes in here. Looking forward to hearing what @sigmavirus24 and @Lukasa have to say :)
",kennethreitz,Lukasa
2118,2014-07-02 16:11:52,"> The requests library seems to grow more and more keyword arguments to try to provide all of the flexibility that SSL users need.

I'm not sure if you mean the fundamental `get/post/put/delete/head/options/request` methods/functions, but if that is what you're referring to, those haven't changed since before 1.0 was released. If you mean that there's an increasing amount of flexibility within adapters to fine tune this, then yes, you're correct.

> If the requests library under Python 3 started supporting a context= parameter like the Standard Library protocols, then users could fine-tune their encryption settings without requests having to become more complicated.

We're very unlikely to start supporting a keyword argument on only one version of Python. `urllib3` will discard keyword arguments on specific versions of Python, but we have yet to do that.

In general I like the idea, but so long as we will support Python 2 (unless a backport of the SSL module can be successfully provided), we will have trouble supporting this. Further, we're ignoring whether or not @kennethreitz would even want to add this keyword argument. I for one would be in favor of adding it (when we can support all Python versions equally), but I don't get to make these decisions ;).

That said, I'll have to investigate if urllib3 will support `SSLContext` objects (because I frankly don't remember). I'll look into that tonight unless @shazow chimes in before then. I'm also willing to do the work to allow urllib3 to accept them.
",sigmavirus24,kennethreitz
2118,2014-07-02 16:11:52,"> The requests library seems to grow more and more keyword arguments to try to provide all of the flexibility that SSL users need.

I'm not sure if you mean the fundamental `get/post/put/delete/head/options/request` methods/functions, but if that is what you're referring to, those haven't changed since before 1.0 was released. If you mean that there's an increasing amount of flexibility within adapters to fine tune this, then yes, you're correct.

> If the requests library under Python 3 started supporting a context= parameter like the Standard Library protocols, then users could fine-tune their encryption settings without requests having to become more complicated.

We're very unlikely to start supporting a keyword argument on only one version of Python. `urllib3` will discard keyword arguments on specific versions of Python, but we have yet to do that.

In general I like the idea, but so long as we will support Python 2 (unless a backport of the SSL module can be successfully provided), we will have trouble supporting this. Further, we're ignoring whether or not @kennethreitz would even want to add this keyword argument. I for one would be in favor of adding it (when we can support all Python versions equally), but I don't get to make these decisions ;).

That said, I'll have to investigate if urllib3 will support `SSLContext` objects (because I frankly don't remember). I'll look into that tonight unless @shazow chimes in before then. I'm also willing to do the work to allow urllib3 to accept them.
",sigmavirus24,shazow
2118,2014-07-02 16:39:48,"> Further, we're ignoring whether or not @kennethreitz would even want to add this keyword argument.

By **no means** did I intend to ignore @kennethreitz’s wishes! Alas. Opening this issue was, I had thought, precisely the means by which Mr. Reitz’s wishes could become known. Would there have been a more appropriate forum for asking my question?

> In this I agree with you.

Thank you, @Lukasa! Sorry if I worded the issue awkwardly and it required multiple read-throughs.

>  I think requests should continue its proud tradition of solving the 80% use-case … I'd never accept adding a `ciphers` kwarg to the main API … This would allow us to write something like the `SSLAdapter`

So the keyword arguments to `get()` and the other functions are not “kitchen sink” collections of everything that _could_ be specified, but a smaller collection of settings, and users are intended to step back and create adapters for more difficult cases? Then you are correct that what I probably need is an adapter that accepts an `SSLContext` for building connections!

The `urllib3` library accepts an `ssl_version` keyword? That, I fear, is a first step towards insanity, and a course which can be stopped by turning to `SSLContext`. Because the next logical step after `ssl_version` (which is really setting what OpenSSL calls the “protocol” version, from what I can see?) is adding a `ciphers` keyword, and then a `dh_params` keyword, and then `ecdh_curve`, and so forth.

In fact, what I really probably want is an adapter that does not even know that `SSLContext` exists, but simply accepts that I have gotten ready an object with a `wrap_socket()` method that it can use when it is ready to negotiate an encrypted connection. That way, as long as an SSL library that I want to use in the future also offers a `wrap_socket()` method (think of PyOpenSSL, or that new `cryptography` project), then it would automatically work if dropped in to the transport object.
",brandon-rhodes,kennethreitz
2118,2014-07-02 17:23:36,"Hooray for three person issue conversations! They're always so easy to follow. =) @brandon-rhodes, you've provided lots of great options for us. I'd like to try to enumerate them as I understand them, and then provide feedback on each of them. Please step in if you think I've left anything out or misunderstood something.
1. Make it possible to pass urllib3 a `socket`.
   
   I don't like this idea much. In principle it's do-able, but it violates the abstraction layer that urllib3 provides. Our favourite feature of urllib3 is that it performs connection pooling, and for it to do that it needs to be able to transparently create new connections (to conserve resources). This means we can't say ""here, use this connection object"" because urllib3 owns all the connection objects. This idea is sadly unworkable.
2. Provide urllib3 with an `SSLContext`.
   
   This is workable, as urllib3 can use it as a kind of connection factory. However, urllib3 right now does an excellent job of transparently interworking between PyOpenSSL and the standard library's `ssl` module. This transparent interworking is only going to get stronger as we move toward [hyper](http://hyper.rtfd.org/en/development/) and HTTP/2. This means being able to provide an SSL context is something of a footgun: if you provide a stdlib `SSLContext` to `hyper`, it won't be able to make an HTTP/2 connection through it, it needs PyOpenSSL's.
   
   Additionally, the PyOpenSSL `Context` does not have the same API as the `ssl` module's one. This is incredibly annoying, and @alekstorm has done awesome work by writing a compatibility module ([backports.ssl](https://github.com/alekstorm/backports.ssl)) to paper over the differences. Again, however, I note that working around the myriad user inputs is a logistical nightmare.
3. More SSL keyword arguments.
   
   Brandon, you've expressed a discomfort with the many keyword arguments potentially required. I am sympathetic to that argument. I have no good alternative to using either a **ssl_kwargs or to have a ssl_args dictionary argument, neither of which is great. I guess a NamedTuple?
",Lukasa,alekstorm
2117,2014-07-03 10:50:41,"Recommend we open a bug report on urllib3 if @shazow agrees.
",Lukasa,shazow
2115,2014-07-02 15:31:28,"Assigned to @Lukasa for review
",sigmavirus24,Lukasa
2111,2014-06-26 18:22:57,"Unless @kennethreitz disagrees, I think this can be closed.
",sigmavirus24,kennethreitz
2110,2014-06-24 16:59:57,"I wonder if this has anything to do with Connection Pooling. @shazow?
",Lukasa,shazow
2105,2014-06-21 01:00:08,"@robvdl it does not say that it ""does not support Python 3.4"" it is merely an omission that we do in fact support 3.4. I'd like to wait for @kennethreitz to add a Python 3.4 builder to the Jenkins server before adding it however.
",sigmavirus24,kennethreitz
2095,2014-06-12 12:47:33,"Thanks for this!

I have mixed feelings. In general, this seems really helpful, but it also seems like it could introduce difficult-to-trace bugs when working with unhelpful upstream websites. Altogether I'm tempted to err in favour of the change though.

It'll need a unit test though: mind writing one?

In the meantime, @kennethreitz, you interested in this change?
",Lukasa,kennethreitz
2095,2014-06-12 13:02:11,"I'm :-1: on this change if only because I can imagine consumers like CloudFlare and Runscope relying on the fact that we follow the redirects each time. Sure this provides a speed-up and performance enhancement but I'm not convinced this won't make some people's lives much harder and less pleasant. Before we move on we should at least gather a lot more feedback from the people who rely so heavily on requests.

@dolph, @johnsheehan, @bryanhelmig, @dstufft, @mtourne, @dknecht can any of you weigh in or have someone more appropriate way in please?
",sigmavirus24,dolph
2095,2014-06-12 13:02:11,"I'm :-1: on this change if only because I can imagine consumers like CloudFlare and Runscope relying on the fact that we follow the redirects each time. Sure this provides a speed-up and performance enhancement but I'm not convinced this won't make some people's lives much harder and less pleasant. Before we move on we should at least gather a lot more feedback from the people who rely so heavily on requests.

@dolph, @johnsheehan, @bryanhelmig, @dstufft, @mtourne, @dknecht can any of you weigh in or have someone more appropriate way in please?
",sigmavirus24,dstufft
2095,2014-06-12 13:02:11,"I'm :-1: on this change if only because I can imagine consumers like CloudFlare and Runscope relying on the fact that we follow the redirects each time. Sure this provides a speed-up and performance enhancement but I'm not convinced this won't make some people's lives much harder and less pleasant. Before we move on we should at least gather a lot more feedback from the people who rely so heavily on requests.

@dolph, @johnsheehan, @bryanhelmig, @dstufft, @mtourne, @dknecht can any of you weigh in or have someone more appropriate way in please?
",sigmavirus24,dknecht
2095,2014-06-12 13:02:11,"I'm :-1: on this change if only because I can imagine consumers like CloudFlare and Runscope relying on the fact that we follow the redirects each time. Sure this provides a speed-up and performance enhancement but I'm not convinced this won't make some people's lives much harder and less pleasant. Before we move on we should at least gather a lot more feedback from the people who rely so heavily on requests.

@dolph, @johnsheehan, @bryanhelmig, @dstufft, @mtourne, @dknecht can any of you weigh in or have someone more appropriate way in please?
",sigmavirus24,mtourne
2095,2014-06-12 13:02:11,"I'm :-1: on this change if only because I can imagine consumers like CloudFlare and Runscope relying on the fact that we follow the redirects each time. Sure this provides a speed-up and performance enhancement but I'm not convinced this won't make some people's lives much harder and less pleasant. Before we move on we should at least gather a lot more feedback from the people who rely so heavily on requests.

@dolph, @johnsheehan, @bryanhelmig, @dstufft, @mtourne, @dknecht can any of you weigh in or have someone more appropriate way in please?
",sigmavirus24,johnsheehan
2095,2014-06-12 13:02:11,"I'm :-1: on this change if only because I can imagine consumers like CloudFlare and Runscope relying on the fact that we follow the redirects each time. Sure this provides a speed-up and performance enhancement but I'm not convinced this won't make some people's lives much harder and less pleasant. Before we move on we should at least gather a lot more feedback from the people who rely so heavily on requests.

@dolph, @johnsheehan, @bryanhelmig, @dstufft, @mtourne, @dknecht can any of you weigh in or have someone more appropriate way in please?
",sigmavirus24,bryanhelmig
2095,2014-06-12 17:38:04,"@ericfrederich It does control whether we follow redirects. `resolve_redirects` returns a generator, so if `allow_redirects` is false that generator doesn't get consumed and no redirects get followed.

The key thing to note is that requests is not, in fact, a browser, and there are times users are going to want more control. With that said, the bigger problem is that we're strongly resistant to adding more fields to the `Session`. That's why I wanted @kennethreitz's insight, just to see if he thinks this is a reasonable exception.
",Lukasa,kennethreitz
2093,2014-06-12 13:08:44,"HTTPbin changes exactly how often? That's exactly how often you would have to run the tests against the site or a local copy. You can also control with what precision betamax will match a request in a recorded cassette so you can send multiple posts with varying bodies and be sure you'll get the right response.

As for trust, @dgouldin might be able to tell us how much he trusts them. I use them, and the equivalent tool in Ruby (VCR) is trusted quite heavily by quite a large number of people including several corporations I can think of off the top of my head.
",sigmavirus24,dgouldin
2087,2014-06-07 08:56:07,"As a companion to #2086, this updates all the other references to RFC 2616 in our codebase and documentation to their new locations. Gonna let @sigmavirus24 review this rather than merge it myself (which I'd normally do) just so he can sanity-check.
",Lukasa,sigmavirus24
2086,2014-06-07 06:54:12,"For a long time we've had a fallback value in `response.encoding` of `ISO-8859-1`, because RFC 2616 told us to. RFC 2616 is now obsolete, replaced by RFCs 7230, 7231, 7232, 7233, 7234, and 7235. The authoritative RFC on this issue is RFC 7231, which has this to say:

> The default charset of ISO-8859-1 for text media types has been removed; the default is now whatever the media type definition says.

The media type definitions for `text/*` are most recently affected by RFC 6657, which has this to say:

> In accordance with option (a) above, registrations for ""text/*"" media types that can transport charset information inside the corresponding payloads (such as ""text/html"" and ""text/xml"") SHOULD NOT specify the use of a ""charset"" parameter, nor any default value, in order to avoid conflicting interpretations should the ""charset"" parameter value and the value specified in the payload disagree.

I checked the registration for `text/html` [here](https://www.iana.org/assignments/media-types/media-types.xhtml#text). Unsurprisingly, it provides no default values. It does allow a charset parameter which overrides anything in the content itself.

I propose the following changes:
1. Remove the ISO-8859-1 fallback, as it's no longer valid (being only enforced by RFC 2616). We should _definitely_ do this.
2. Consider writing a module that has the appropriate fallback encodings for other `text/*` content and use them where appropriate. This isn't vital, just is a ""might be nice"".
3. Begin checking HTML content for meta tags again, in order to appropriately fall back. This is controversial, and we'll want @kennethreitz to consider it carefully.
",Lukasa,kennethreitz
2086,2014-06-08 14:56:11,"> Remove the ISO-8859-1 fallback, as it's no longer valid (being only enforced by RFC 2616). We should definitely do this.

I agree that we should remove the fallback. I do wonder how we should handle `Response#text` in the event that the server does not specify a charset (in anyway, including the meta tags of the body). Should we disable `Response#text` conditionally either through an exception or something else? Not doing so will rely more heavily on chardet, which I have decreasing confidence in given the number of new encodings it does not detect.

> Consider writing a module that has the appropriate fallback encodings for other text/\* content and use them where appropriate. This isn't vital, just is a ""might be nice"".

Given that this is not guaranteed to be included in requests, I'm fine with adding it to the toolbelt, that said. I'm also okay with making this a separate package so users can just use that with out having to install the rest of the toolbelt. That, however, is a separate discussion.

> Begin checking HTML content for meta tags again, in order to appropriately fall back. This is controversial, and we'll want @kennethreitz to consider it carefully.

We still have a method to do this in `utils`, right? I don't like the idea in the slightest, but it won't cost extra effort. That said, we have to make sure any charset provided in the media type takes precedence.
",sigmavirus24,kennethreitz
2086,2014-06-08 16:18:06,"Agreed from a correctness perspective, but I wonder if @kennethreitz is going to want it from a usability perspective.
",Lukasa,kennethreitz
2071,2014-05-28 01:27:48,"@Lukasa this is the actual behaviour. I tested on Python 2 and 3 and this only happens on Python 3 due to [these lines](https://github.com/kennethreitz/requests/blob/6c72509f5bb0e9cd8fad64a44ba99687ed044771/requests/models.py#L434..L439) I'm pretty sure we don't want this behaviour by default. I'm going to work on this some tonight and post my progress before heading to sleep.
",sigmavirus24,Lukasa
2065,2014-05-26 14:25:46,":+1: I'm in favor of this change. The only thing that would be better would be if @kennethreitz added support to Jenkins for 3.2, 3.4, and pypy 2.2 :)
",sigmavirus24,kennethreitz
2064,2014-05-26 13:37:14,"I could be wrong, but I think I remember @dstufft saying that pip support for 3.2 wasn't a high priority. That aside, I'm not quite sure why we support using simplejson. If @mgeisler hadn't linked to #710 I would have guessed it was to support Python 2.5 (which we no longer do).
",sigmavirus24,dstufft
2061,2014-05-23 16:40:54,"@shazow, are you open to doing part 2?
",Lukasa,shazow
2050,2014-05-18 01:42:48,"This is great work @Hasimir! I really appreciate your effort in putting this together.

Personally, I think this is better suited to a blog post and not the documentation. I'll leave this open, though, until @Lukasa wakes up. :)
",sigmavirus24,Lukasa
2049,2014-05-17 22:42:52,"Basically identical to @sigmavirus24's suggested code - thanks for that pointer. Just added docstring.

There aren't tests for particular Adapter implementations, and I've not added any at this stage.
I have tested it with the following (which achieves what I want):


",codedstructure,sigmavirus24
2048,2014-05-17 20:42:42,"I think there's no good reason not to do this, and it would definitely make this easier. @sigmavirus24?
",Lukasa,sigmavirus24
2045,2014-05-16 14:26:47,"Thanks so much for this! :cake:

This looks like a good catch. I think the generator created in `Response.iter_content` should probably be looking for Timeout errors, both from urllib3 and from the socket module, and should catch and wrap them. @sigmavirus24?
",Lukasa,sigmavirus24
2039,2014-05-13 06:17:12,"I'd like to continue discussion here, if you don't mind.

@cmasc, can you replace the `onedrive/api_v5.py` file (that you've edited earlier) with the one from here: https://gist.githubusercontent.com/mk-fg/c3de257baa034181b6dc/raw/b1b891f1946ecc2c0a6ee921fcc5764a29a1d44b/gistfile1.txt

Diff for these changes there is:



It should print exact arguments passed to requests, file size before it gets passed and same body len afterwards, plus disable TLSv1Adapter (which I don't see how might affect things much, but to avoid mess with default requests behavior altogether).

If this updated version will hang on upload (OneDrive servers you get redirected to still hang on TLS 1.2) - please try this api_v5.py version: https://gist.githubusercontent.com/mk-fg/9e20e898ea029d77bb3c/raw/e9288643371d1a66b9550e2f566ef3881b2a395b/gistfile1.txt (it leaves TLSv1Adapter in)

Also, can you run the following python script:



It should print requests module version, I wonder if it's pre-1.0.0, where more serious monkey-patching was involved to avoid TLS 1.2.
",mk-fg,cmasc
2039,2014-05-14 07:42:30,"@cmasc 
Looks like github is snipping out somewhat important `<open file ...>` bits from the output you're pasting, would be more useful if you'd wrap these things in  python
import requests
r = requests.post('http://httpbin.org/post', files={'file': (filename, open(path_to_file, 'r'), 'application/octet-stream')})
print r.status_code
print r.json()['headers']
print len(r.request.body)
```
",mk-fg,cmasc
2039,2014-05-14 09:40:01,"@cmasc 
I believe issue should go away in the new version (14.05.3), which should be available on pypi or installable from github master.
",mk-fg,cmasc
2038,2014-05-12 18:47:22,":+1: :cake:

@kennethreitz Just so you're aware, the reason this has been opened is because of shazow/urllib3#385, which is a nastly TLS-related bug on Python 3.4.1. This will be a big problem for `pip` in addition to our users, so we'll want to think hard about pushing a new release. Let me know if you decide to do that soon and @sigmavirus24 and I will tidy up the code ready for a release.
",Lukasa,kennethreitz
2038,2014-05-12 18:47:22,":+1: :cake:

@kennethreitz Just so you're aware, the reason this has been opened is because of shazow/urllib3#385, which is a nastly TLS-related bug on Python 3.4.1. This will be a big problem for `pip` in addition to our users, so we'll want to think hard about pushing a new release. Let me know if you decide to do that soon and @sigmavirus24 and I will tidy up the code ready for a release.
",Lukasa,sigmavirus24
2038,2014-05-12 18:59:41,"@Lukasa @sigmavirus24 alright, let's get a release ready. :)
",kennethreitz,sigmavirus24
2029,2014-05-01 09:54:44,"I've honestly got no idea. @t-8ch, does the underlying ssl library not time out based on the socket timeout?
",Lukasa,t-8ch
2027,2014-04-29 15:51:43,"Hi @gauravp2003,

Much of the documentation around requests, including the [README](https://github.com/kennethreitz/requests#contribute), suggests that the issue tracker is for bugs or feature requests, not questions. If you have questions, the should be asked on [StackOverflow](https://stackoverflow.com/questions/tagged/python-requests). @Lukasa, someone else, or I will answer it there.

Thanks for your interest in requests,
",sigmavirus24,Lukasa
2026,2014-04-29 01:07:15,"@zakcalrissian how did you install requests? Your traceback seems to say that you have version 2.3.0 installed when no such version has been released.
",sigmavirus24,zakcalrissian
2025,2014-05-02 19:07:59,"@willingc and I are working on this.
",sigmavirus24,willingc
2022,2016-06-06 22:24:43,"@Lekinho I found that making a short test-script that tested the domain I was having problems with helped.

it was just:


",jvanasco,Lekinho
2022,2016-06-07 07:10:27,"@Lekinho You can extract pyopenssl from requests in your code:


",MadSpindel,Lekinho
2022,2016-06-07 08:27:13,"@Lekinho If you're still encountering this problem with Python 2.7.11 it's highly likely that the remote server doesn't support the TLS settings being used by requests. Is the server in question available on the public internet? If so, can you provide me with the URL?
",Lukasa,Lekinho
2022,2016-06-07 19:27:48,"It looks like @Lekinho deleted their github account?  For the next person who has issues - it's possible that their upgrade of OpenSsl or Python broke some compiled c bindings.  Whenever I have an upgrade like that, I trash my virtualenv or all packages and then build a new one.
",jvanasco,Lekinho
2021,2014-04-26 13:29:49,"LGTM. Merge when ready @kennethreitz 
",sigmavirus24,kennethreitz
2018,2014-04-26 22:50:42,"To be clear for those who aren't sure, the way @ouroborus' suggestion differs from the current logic is that we take the proxies from the request, then apply proxies from the environment, then finally apply proxies from the `Session`.

I'm open to re-ordering the precedence of the priorities. @sigmavirus24, thoughts?
",Lukasa,sigmavirus24
2013,2014-04-23 01:39:47,"Hey @cheecheeo, thanks for opening this!

Your code is fine and the idea is valid. The problem is that this makes the rest of the library inconsistent and it is _not_ desired behaviour. We have no intent to implement this and we have never intended for the a Request's representation to be used like this.

I'll wait for @Lukasa to weigh in, but I'm pretty sure we will not be accepting this contribution.

Cheers
",sigmavirus24,Lukasa
2010,2014-04-18 16:30:32,"As @kennethreitz and I discussed at PyCon, this is a first pass at a rewrite of part of the Philosophy section of the docs, attempting to explain Requests' slightly unusual management style. I'd like some feedback.

Some ideas:
- should @shazow be mentioned? I felt like he should but I wasn't sure how best to explain the relationship.
- is there anything major that I've missed?
",Lukasa,kennethreitz
2010,2014-04-18 16:30:32,"As @kennethreitz and I discussed at PyCon, this is a first pass at a rewrite of part of the Philosophy section of the docs, attempting to explain Requests' slightly unusual management style. I'd like some feedback.

Some ideas:
- should @shazow be mentioned? I felt like he should but I wasn't sure how best to explain the relationship.
- is there anything major that I've missed?
",Lukasa,shazow
2006,2014-04-16 20:03:11,"This is an interesting idea @untitaker! It does feel like it'd be useful.

I wonder if we should trial it in [the toolbelt](https://github.com/sigmavirus24/requests-toolbelt) first, see if it's useful. @sigmavirus24?
",Lukasa,sigmavirus24
2002,2014-04-14 21:32:06,"Yeah I'm :+1: for removing this. @Lukasa thoughts? 

I know it would have to wait for requests 3.0 but it might still be good to have a wishlist for 3.0 
",sigmavirus24,Lukasa
2002,2014-04-14 21:50:16,"I just chatted with Kenneth about this, he definitely doesn't like it. I think we should be leaving this on a wishlist for 3.0, but @kennethreitz might disagree.
",Lukasa,kennethreitz
1999,2014-04-12 18:37:01,"This looks great to me, thanks! :cake: @kennethreitz, are you happy with this?
",Lukasa,kennethreitz
1999,2014-04-12 18:41:52,"Sorry, wrong answer in gmail. Thanks for approve!

2014-04-12 21:37 GMT+03:00 Cory Benfield notifications@github.com:

> This looks great to me, thanks! [image: :cake:]@kennethreitzhttps://github.com/kennethreitz,
> are you happy with this?
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/pull/1999#issuecomment-40288137
> .

## 

Nasridinov Renat, mavladi@gmail.com
",ap-Codkelden,kennethreitzhttps
1996,2014-04-07 21:18:43,"Hmm, yup, from looking at the code it's clear that we no longer guess the MIME type of the uploaded file. I wonder if that was deliberate.

Looks like it went away in af4fb8cedca7c331b8c914a40c477a2cb02055e1 (part of #1640) when we switched to explicitly using `RequestField` objects. Assuming we're happy to guess MIME types I'm happy to put it back. @kennethreitz, should we be guessing MIME types on multipart upload parts?
",Lukasa,kennethreitz
1996,2014-10-10 19:09:49,"Hey @kennethreitz I was rummaging through issues and we never did get an answer from you. Was it intentional that content type guessing was removed? There's an API for the user to specify it now explicitly so we just might want to document this.
",sigmavirus24,kennethreitz
1995,2014-04-07 06:14:08,"I'm happy to do this as well. @kennethreitz, do you want to do this?

Would be nice if we could have a bit of the docs that talks about building the most secure possible form of requests, including stuff like installing OpenSSL from Homebrew and then building against that.
",Lukasa,kennethreitz
1995,2014-07-01 00:54:26,"@kennethreitz any update on this? 
",sigmavirus24,kennethreitz
1993,2014-04-05 07:48:56,"@richierichrawr That's really not a problem, we were all new once. Ours is not a community that vilifies mistakes. =)

Yeah, welcome to the fun world of GitHub. =D We use Sphinx for our documentation, which provides lots of excellent features that GitHub does not understand.
",Lukasa,richierichrawr
1991,2014-04-05 14:52:14,"We should remove all reference of the mirrors. PyPI is at the point now where the mirrors are entirely unnecessary thanks to the use of a CDN by the infrastructure team. @dstufft can speak more to this.
",sigmavirus24,dstufft
1989,2014-04-03 00:05:43,"Just a doc change. If anyone thinks they deserve separate items let me know. /cc @dstufft
",sigmavirus24,dstufft
1987,2014-04-01 13:01:06,"I'm basically in favour of this, and I'm happy to do the legwork required for it, but I want to know if @kennethreitz wants it or not.
",Lukasa,kennethreitz
1980,2014-03-29 09:05:48,"Wow, this is a serious chunk of work: thankyou!

I have to get around to reviewing this, which I haven't done yet, but it seems like it's a good enhancement. I'm not going to speculate on how @kennethreitz will want to handle the API change: whether he'll want extra properties or to hold off until Semver lets us make this backwards incompatible change.
",Lukasa,kennethreitz
1980,2014-03-29 15:31:21,"I haven't looked at this at all and don't have any opinion yet. I just think the important context here is that (if I remember correctly) @kennethreitz was explicit in his desire to only handle the case where there's a single link for a given relation type. I'll have to find the corresponding issues/PRs to double check though. 

In general I'm always :+1: for conforming to RFCs. As a side note, if this is rejected, I would be more than happy to accept this as a feature for the [toolbelt](/sigmavirus24/toolbelt).
",sigmavirus24,kennethreitz
1979,2014-03-31 15:40:55,"Aha, ok.

I don't think auth handlers can handle an auth challenge that occurs _after_ a redirect: I think we lose track of the auth handler. @sigmavirus24 does this match your reading of the code?

If that's the case, do we consider that behaviour a bug?
",Lukasa,sigmavirus24
1976,2014-03-26 13:38:02,"In related news, @kennethreitz is there an email address that the Jenkins server emails when tests fail? If not, could you add @Lukasa and/or me to the notifications? Assuming Jenkins runs on pushes to master (i.e., when you merge a PR) I would have seen this and fixed it sooner.
",sigmavirus24,kennethreitz
1973,2014-03-24 20:55:44,"Hmm, @shazow, thoughts?
",Lukasa,shazow
1972,2014-03-23 16:39:33,"@sigmavirus24 Sure updated now
",avidas,sigmavirus24
1972,2014-03-23 16:49:03,":+1: :cake: Thanks. Assigning @kennethreitz since @Lukasa and I agree this is ready to merge.
",sigmavirus24,kennethreitz
1972,2014-04-04 16:05:28,"Will you merge this @kennethreitz ?
",avidas,kennethreitz
1969,2014-08-17 02:27:24,"FWIW, it looks like the stdlib in 2.7 will support SNI (and a whole host of other things), just as soon as someone reviews and merges @dreid and my patch.
",alex,dreid
1968,2014-03-20 00:46:38,"Thanks for the info @lukasa.

Do we all agree the current info on the
http://docs.python-requests.org/en/latest/ page is misleading, as it
(a) calls 2.3.0 ""released"", and (b) calls it ""latest"" (in the url)? From
that I'd conclude 2.3.0 was the latest release.

On Wednesday, March 19, 2014, Ian Cordasco
<notifications@github.com<javascript:_e(%7B%7D,'cvml','notifications@github.com');>>
wrote:

> @Lukasa https://github.com/Lukasa I didn't trigger this but I prefer it
> this way. Certain things have been fixed/broken in 2.3.0. That behaviour is
> not available/visible in 2.2.1 and so the documented behaviour is
> accurately documented. :)
> 
> ## 
> 
> Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/issues/1968#issuecomment-38116629
> .
",skivvies,lukasa
1968,2014-03-20 00:55:32,"If you change the current latest version to something like ""2.3.0-dev"" (and
then update the docs, ideally substituting ""release"" with ""version"" for
docs on not-yet-released versions) it'd be clearer, and more semantic too.

On Wednesday, March 19, 2014, _pants@getlantern.org wrote:

> Thanks for the info @lukasa.
> 
> Do we all agree the current info on the
> http://docs.python-requests.org/en/latest/ page is misleading, as it
> (a) calls 2.3.0 ""released"", and (b) calls it ""latest"" (in the url)? From
> that I'd conclude 2.3.0 was the latest release.
> 
> On Wednesday, March 19, 2014, Ian Cordasco notifications@github.com
> wrote:
> 
> > @Lukasa https://github.com/Lukasa I didn't trigger this but I prefer
> > it this way. Certain things have been fixed/broken in 2.3.0. That behaviour
> > is not available/visible in 2.2.1 and so the documented behaviour is
> > accurately documented. :)
> > 
> > ## 
> > 
> > Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/issues/1968#issuecomment-38116629
> > .
",skivvies,lukasa
1968,2014-03-23 11:34:08,"This is really up to @kennethreitz, but I suspect he's going to want to avoid the extra procedural overhead.
",Lukasa,kennethreitz
1967,2014-03-20 14:36:30,"So in that situation we should have dropped the connection, which is what I'd expect if we'd exhausted it on the read (which we do).

@shazow, want to run your eye over this quickly?
",Lukasa,shazow
1963,2014-03-23 14:54:08,"@kennethreitz since you're around, care to weigh in on this one?
",sigmavirus24,kennethreitz
1962,2014-03-15 16:36:18,"@Lukasa I was considering calling `list(r.history)` instead of `tuple(r.history)` but I cannot see a case there where it wouldn't be a list.
",sigmavirus24,Lukasa
1962,2014-03-23 14:54:20,"@kennethreitz thoughts?
",sigmavirus24,kennethreitz
1956,2015-04-20 05:46:59,"@onceuponatimeforever Ah, good catch, we just merged a fix for this a few days ago. =)
",Lukasa,onceuponatimeforever
1951,2014-03-12 19:26:15,"This pull request falls into three commits.

The first is a refactoring of the `get_environ_proxies` method to make it possible to evaluate whether a given URL is in the NO_PROXY list.

The second is a refactoring of the `resolve_redirects` method to move rebuilding the `Authorization` header to its own method.

Both of these commits are intended to set the stage for the third, which is a new method that re-evaluates proxy configuration on a redirect, and re-evaluates proxy authorization as well.

I don't want this merged yet, but it's a pretty sizeable change and I'd like to get eyes on it as early as possible. The key problem right now is that I don't have tests, though it should be possible for me to test this fairly easily. I'll want to add tests before we merge this.

@sigmavirus24, do you mind taking a look?
",Lukasa,sigmavirus24
1949,2014-03-12 18:36:54,"Can I get review from @sigmavirus24 and @ionrock?
",Lukasa,sigmavirus24
1948,2014-03-12 11:20:34,"And, in fact, @ceaess has already provided a Pull Request that fixes this in #1935, which will be in the next release.
",Lukasa,ceaess
1947,2014-03-12 07:02:13,"@kennethreitz I'm open to doing the work here if this is something you think you'd like.
",Lukasa,kennethreitz
1945,2014-03-10 22:05:45,"Thanks for this!

I have no strong opinions, it looks fine. =) +0.5. @sigmavirus24?
",Lukasa,sigmavirus24
1945,2014-03-11 13:36:46,"I have a couple concerns. The first is that you're introducing yet another way for the user to set the CA bundle. You did not state that in your description. I assume you want this in order to specify the path that you would otherwise specify to `verify` as a plain string.

That noted, I'm not comfortable with verify having a trinary value (its current state) let alone having 4 possible values (`True`, `False`, `'/path/to/bundle'`, and dictionary of **connection** options). Let's be clear, you are attaching these to the connection. We've held in the past that these are Transport Adapter concerns. Whilst being less invasive and a less severe API change, I'm still -0. I still feel that if you have all of the information for each URL you could structure it in such a way that a custom transport adapter could use it and attach it to the connection.  The current API affords for what you want, just not in the simplest way possible. Especially given Kenneth's hesitancy to tie requests too closely to urllib3's API, I'm hesitant to be positive about this.

If @kennethreitz likes the change though my only request would be that you use `#get` on the verify dictionary like so:



This is far better than first checking for the value first and then assigning it. Each of those assigned values will be `None` if in fact the parameter is not provided or the dictionary is simply empty.
",sigmavirus24,kennethreitz
1945,2014-10-10 15:16:45,"@schlamar in this case, if I were you, I would do the following:
1. Use @t-8ch's [StackOverflow answer](http://stackoverflow.com/a/22794281/1953283) to create a new Adapter
2. Register the adapter on your session like so:


",sigmavirus24,t-8ch
1944,2014-03-10 07:24:49,"This is my proposal which fixes #1939 completely.

@sigmavirus24 missed one possible exception in #1940 (ChunkedEncodingError) and didn't handle RuntimeError correctly (because this means that the content is already consumed).

This also addresses the issue that a decoding error is already thrown in Adapter.send by moving the content reading part at the end of Session.send.
",schlamar,sigmavirus24
1944,2014-03-10 15:54:51,"This looks reasonable to me. I'll let @sigmavirus24 take a look too.
",Lukasa,sigmavirus24
1944,2014-03-24 16:26:40,"I suspect that `r.transfer_encoding` would be very useful for `pip`. I seem to recall that they've had some trouble with servers sending `.tar.gz` files with `Content-Encoding: gzip` set, causing `r.content` to save the ungzipped tar file. @dstufft, does that ring a bell?
",Lukasa,dstufft
1942,2014-03-07 14:32:54,"Looking into YCM, they use pythonfutures and [requests-futures](https://github.com/ross/requests-futures).  I wonder if @ross has any insight into this. Granted I'm not sure they actually use requests-futures, but they're at least using requests with concurrent futures and he might be able to help us all out.
",sigmavirus24,ross
1942,2014-03-07 14:58:22,"Fair enough, I'll take it up with @Valloric again. :)
",ashemedai,Valloric
1941,2014-03-06 16:31:13,"Not that I am aware of.

@sigmavirus24, is this worth putting in the toolbelt? `flatten_and_form_encode`?
",Lukasa,sigmavirus24
1940,2014-03-05 03:08:13,"This fixes #1939 and part of @zackw's rehaul of redirection.

To better test this, I'd like to add Betamax as a test dependency. Also, as a separate PR I plan to kill the usage of `RuntimeError` in `Response#content`. That's awful and the community consensus is that nothing should ever raise that. 
",sigmavirus24,zackw
1940,2014-03-08 02:32:36,"> The actual point of raising the exception is here: https://github.com/kennethreitz/requests/blob/master/requests/adapters.py#L391

@schlamar we cannot catch that in `Session#resolve_redirects`. To catch it in the adapter would be a serious change in behaviour. I wonder why we consume all of the content in the adapter though. If there's a decoding error, we don't even attempt to return a Response to the user. That may be fodder for an entirely different issue though. If @Lukasa is satisfied with this as an immediate solution to what was reported, I am too. But I want to investigate that other piece in the adapter as well.
",sigmavirus24,Lukasa
1940,2014-03-08 12:18:57,"> satisfied with this as an immediate solution to what was reported

This is no solution to #1939 because it is already failing in `Adapter.send` (before resolving redirects). See the traceback in this issue. The analysis of @mechanical-snail is not quite correct here.  

It would work if we move the `if not stream: r.content` at the end of `Session.send` (which is IMO the correct place, the stream argument should only come in play after the redirects are resolved). But that means that the stream parameter to Adapter.send is obsolete, so this would be a API breaking change.
",schlamar,mechanical-snail
1940,2014-03-08 14:53:49,"Reviewing the issue, you're once again correct @schlamar. That aside @mechanical-snail is not wrong that this could in fact also be a problem. Their issue won't be fixed by this but it may fix the case where streaming is used.
",sigmavirus24,mechanical-snail
1939,2014-03-07 14:09:14,"Interesting. It's very likely that this is a duplicate of https://github.com/shazow/urllib3/issues/206 / https://github.com/kennethreitz/requests/issues/1472 because there is also an 301 redirect in the curl output. @shazow 
",schlamar,shazow
1935,2014-02-28 16:12:05,"Hey @Lukasa can we have some :eyes: on this? I paired with @cjstapleton on this PR this morning. Let us know what you think.
",sigmavirus24,Lukasa
1935,2014-02-28 16:12:05,"Hey @Lukasa can we have some :eyes: on this? I paired with @cjstapleton on this PR this morning. Let us know what you think.
",sigmavirus24,cjstapleton
1935,2014-02-28 20:55:10,"Thanks for this, @cjstapleton! It looks great. =) I've made a couple of tiny stylistic notes inline, but the substance of this change is perfect.

It's worth noting that this implements _part_ of #1801. I don't think that should be a reason not to merge this, but it's worth being aware of.

Otherwise, this is good to go. When you make those changes I'll flag this as ready to merge. Thanks! :cake:
",Lukasa,cjstapleton
1935,2014-03-01 15:33:37,":cake: Thanks for this @cjstapleton 
",sigmavirus24,cjstapleton
1934,2014-02-28 14:30:45,"@Lukasa this is essentially a documentation change. I included the other information about cacert.pem but I'm not sure it is entirely necessary in the NOTICE file. I'll happily remove it if you share the same doubts.
",sigmavirus24,Lukasa
1930,2014-02-26 07:29:07,"> Session.send now offers a new mode, `iter_redirects=True`, in which
> it returns an iterator over redirects instead of the first response. [...] If both allow_redirects=False and iter_redirects=True are specified, allow_redirects=False wins.

Aah! Nope, that's not the way to do this. That last sentence should be a bit of a clue: your arguments to a function should never be fighting each other.

More importantly, function argument should not change the type of the returned value. This makes it substantially more difficult to reason about the function. If you really want to do this then you're going to have to have a new function call here, rather than a new parameter.

@kennethreitz We're going to want your insight here. =)
",Lukasa,kennethreitz
1930,2014-03-12 20:36:13,"Alright enough fighting guys :)

---

@zachw, would you like to have a Skype or Google Hangout sometime this week or next? Perhaps I could really connect get a good idea for what you're trying to accomplish, and see if I can come up with a supplemental API that would work well :)
",kennethreitz,zachw
1930,2014-03-12 20:41:36,"@kennethreitz I'm @zackw, not @zachw :-)

Could probably sit down and talk sometime tomorrow.  I'm in US/Eastern and can do either Skype or Google (but Skype is preferred).  I will also find time to file bugs for all the things I was trying to fix, as requested earlier.
",zackw,zachw
1926,2014-02-19 16:13:25,"I don't have the time to find the spec but I think Headers are supposed to be encoded as latin strings (and that's how basic authentication and digest authentication is specified for the server). If Python cannot coerce your unicode credentials to Latin we should raise an exception. UnicodeEncodeError is a good one in my opinion. I don't like the idea of adding yet another exception.

I agree that this doesn't give the user a great deal of information, but at the same time, this is a very accurate message.

@Lukasa should we be checking credentials ahead of time? The problem with that is the fact that we gleam authentication credentials from the Session too. There's no way to check this except in the preparation of the request. If we inherit from the UnicodeDecodeError, we could raise a new exception with the original message attached. It might be more informative to the user, `AuthenticationEncodeError`? Or perhaps a more generic `HeaderEncodeError`?
",sigmavirus24,Lukasa
1921,2014-02-14 22:16:24,"Assigning to @Lukasa for review and once past that, I will assign it to Kenneth.
",sigmavirus24,Lukasa
1914,2014-02-13 07:40:03,"Thanks for raising this!

We actually quite like this behaviour, though: it means you can examine the exception object we wrapped. I'm going to let @sigmavirus24 weigh in here, but I think we're happy as-is.
",Lukasa,sigmavirus24
1913,2014-02-11 21:24:06,"Wow, this is a substantial change, and an impressive bit of work. I'm not going to dive into code review at this moment for two reasons: I don't have time, and I think we'll want @kennethreitz involved early in the discussion on this pull request.

Nevertheless, there'll definitely be a couple of code review comments if we decide to move ahead with this. I'm mostly interested to see what Kenneth has to say about the idea though.
",Lukasa,kennethreitz
1913,2014-02-12 08:49:35,"I've glanced through the code review comments stuff so far, and wanted to talk more generally about how this relates to our API freeze.

@zackw has said in one of his comments that he believes these changes would be welcome extensions to the API for anyone who has to manually handle redirects. That's probably true. However, our API freeze policy does not say that we are freezing the API ""except when it'll be useful for people if it was extended"". By default, the Requests answer to API changes will always be ""no"". The reason this issue is still open and being discussed is because we think there is potentially enough value here that we want to look at it in depth. Please don't assume that we hate your PR, @zackw, we're just starting from a very conservative position.

Next, there is a question about how much affordance we should give to people who circumvent Requests' redirection policy. The general Requests policy on this sort of thing (see also: `PreparedRequest` objects) is that if you don't like the way Requests does it you should do it yourself. We have made concessions here in the past, but not many and always under substantial duress.

Again, I'm going to hold off more dramatic code review until @kennethreitz gives an idea of whether he's likely to want this change at all.
",Lukasa,kennethreitz
1912,2014-02-11 20:08:25,"@zackw actually this is the right direction. On any response that did have history previously we were returning tuples and on any without we were returning an empty list. The fact of the matter is that history on a response should be immutable. For that to be the case, it should be a tuple. I also don't quite understand your arguments. If you're manually processing redirections or using a hook you will now get a tuple after this change, as you should have in the first place. That does raise a good point that hook authors will expect a list though (if they're writing hooks dealing with manual processing of redirecitons).

@Lukasa that makes this (sort of) a backwards incompatible change.
",sigmavirus24,Lukasa
1907,2014-03-26 09:45:46,"_Use by default SSL CA certificate bundle from the platform_ is what OP proposes in this issue and that's something I totally agree with. It feels so natural to me that I find it difficult someone could oppose this, yet alone argue that not doing this is better experience. Yet this is what I see here and that's why I decided to share my opinion with you.
@Lukasa states

> I don't think we should change which certificates we use based on whether or not you install an optional dependency. Our current behaviour WRT pyopenssl is to enable additional features without changing our current behaviour. That's what you should be aiming for here.

This argument is misguided here as pretty much everyone agrees that when you're dealing with security you should **by default** be as secure as you are able to be, given the environment you operate in. This means using system CA certificates by default (and fallback to bundled ones if it's not possible or very hard to do) and not bundled ones. I think one might argue that security begs for policy of graceful 
degradation.

Also @Lukasa states

> Surely you'd want to ensure that all your users have the exact same certs on all platforms, to avoid annoying platform-specific bugs?

Surely, if the only goal is to _avoid annoying platform-specific bugs_ then yes. But if the goal is to make requests secure by default (and this is in line with _HTTP library for humans_ motto) then surely you would want to ensure that all your users have _the best certs available on their systems_.

Additional bonus is that you save packagers from having to patch this (mis)feature.

I'm curious what @dstuff and @t-8ch think.
",piotr-dobrogost,t-8ch
1907,2014-03-26 09:45:46,"_Use by default SSL CA certificate bundle from the platform_ is what OP proposes in this issue and that's something I totally agree with. It feels so natural to me that I find it difficult someone could oppose this, yet alone argue that not doing this is better experience. Yet this is what I see here and that's why I decided to share my opinion with you.
@Lukasa states

> I don't think we should change which certificates we use based on whether or not you install an optional dependency. Our current behaviour WRT pyopenssl is to enable additional features without changing our current behaviour. That's what you should be aiming for here.

This argument is misguided here as pretty much everyone agrees that when you're dealing with security you should **by default** be as secure as you are able to be, given the environment you operate in. This means using system CA certificates by default (and fallback to bundled ones if it's not possible or very hard to do) and not bundled ones. I think one might argue that security begs for policy of graceful 
degradation.

Also @Lukasa states

> Surely you'd want to ensure that all your users have the exact same certs on all platforms, to avoid annoying platform-specific bugs?

Surely, if the only goal is to _avoid annoying platform-specific bugs_ then yes. But if the goal is to make requests secure by default (and this is in line with _HTTP library for humans_ motto) then surely you would want to ensure that all your users have _the best certs available on their systems_.

Additional bonus is that you save packagers from having to patch this (mis)feature.

I'm curious what @dstuff and @t-8ch think.
",piotr-dobrogost,dstuff
1906,2014-02-07 13:37:31,"I agree that it's a real problem. I do not necessarily agree that Requests needs a workaround for every bug in any of our dependencies. 

What is not clear to me at this time is how severe this bug is, how widespread it is, and how easy it is to work around. It's also not easy for me to find those things out today: I'm at a conference and won't have time near a laptop. It would be useful if @sigmavirus24 could take a look: otherwise I'll have to dig into it tomorrow. 

The key thing to know is that OpenSSL has had other bugs in the past that we haven't worked around in the core Requests code. Most notably, 0.9.8f (I think) that ships on OS X 10.8 has a similar bug in it that we have never worked around. 

We are not required to fix every bug in all of our dependencies, especially implicit ones. We _may_ fix this one, but I don't know yet. 

Note that another workaround is to use a different version of OpenSSL. Just saying. 
",Lukasa,sigmavirus24
1906,2014-03-07 14:36:06,"I wonder if someone who works @heroku could give us a hand. They might have some insight. @catsby can you give us some info about the SSL configuration on a default Heroku box? Can you point us at someone who can?
",sigmavirus24,heroku
1906,2014-03-07 14:36:06,"I wonder if someone who works @heroku could give us a hand. They might have some insight. @catsby can you give us some info about the SSL configuration on a default Heroku box? Can you point us at someone who can?
",sigmavirus24,catsby
1905,2014-02-07 06:25:27,"I understand fixing AppEngine is not something you intend to pursue. In case it helps anyone else, though, I think I have found the cause: line 172 [here](https://github.com/kennethreitz/requests/pull/1892/files#diff-28e67177469c0d36b068d68d9f6043bfR172). It's my impression that [get_netrc_auth essentially fails on AppEngine](https://github.com/kennethreitz/requests/pull/1709/files#diff-5956087d5835a57d9ef6fff974f6fd9bL97), and without that, authentication will not survive redirects, resulting in infinite recursive redirects to any endpoint that requires Auth (not just POSTs). 

I'm not sure, but the absence of the feature mentioned here: https://github.com/kennethreitz/requests/pull/1892#issuecomment-33730591 , which at the time seemed like a ""nice-to-have"", may be a factor. 

@kennethreitz had seemed [concerned](https://github.com/kennethreitz/requests/pull/1892#issuecomment-33821403) that making this design decision for the developer might have adverse consequences... I wonder if there are others outside of the GAE ecosystem who would prefer Authentication tokens survive across redirects. 

Perhaps an option to allow this would assuage the GAE community as well as those in similar situations?
",rattrayalex,kennethreitz
1905,2014-02-07 07:26:47,"There are absolutely people who would prefer that we keep the auth tokens on over redirects. However, both @sigmavirus24 and I very strongly believe that being insecure by default is dangerous. @kennethreitz has a more nuanced view than we do, and I acknowledge that. However, I think that #1892 is a good change.

However, this bug cannot be #1892 because we haven't released a version with #1892 in it yet. =)
",Lukasa,kennethreitz
1905,2014-02-07 07:26:47,"There are absolutely people who would prefer that we keep the auth tokens on over redirects. However, both @sigmavirus24 and I very strongly believe that being insecure by default is dangerous. @kennethreitz has a more nuanced view than we do, and I acknowledge that. However, I think that #1892 is a good change.

However, this bug cannot be #1892 because we haven't released a version with #1892 in it yet. =)
",Lukasa,sigmavirus24
1905,2015-11-24 18:47:12,"/CC @jonparrott
",webmaven,jonparrott
1903,2014-02-04 10:35:02,"That's not a problem at all. =) In the future, if you're worried about whether a question is a bug or simply a misunderstanding, you can email either myself or @sigmavirus24. =) We both publish our email addresses on GitHub, and we're always happy to take questions.
",Lukasa,sigmavirus24
1899,2014-02-02 17:31:11,"This has been proposed in the past and @kennethreitz vastly dislikes most of pep8 from my experience. I've never been able to tell exactly what style he likes to follow but it's very much his call. Until he can get around to replying to this, I would hold off on your efforts so you do not expend too much energy.

FWIW, As the maintainer of Flake8 as well as this project, I appreciate your desire to make this project more compliant.
",sigmavirus24,kennethreitz
1899,2014-02-02 17:38:34,"Ok, let's wait @kennethreitz's rely. It is interesting that he does't like PEP8 style.
",cli248,kennethreitz
1899,2014-02-02 17:51:50,"Haha, that reminds me Shakespeare's famous remark, **There are a thousand Hamlets in a thousand people's eyes**

I will wait @kennethreitz's reply and see what I should do next. 

BTW: I think @kennethreitz should come up with a style guide for `requests` if he doesn't like pep8. 
",cli248,kennethreitz
1899,2014-02-06 19:52:33,"While it's not a style guide for requests specifically, @kennethreitz 's guide linked above documents many of his opinions on python style in general. Though he does seem to say nicer things about PEP8 there: http://docs.python-guide.org/en/latest/writing/style/#pep-8
",rattrayalex,kennethreitz
1899,2014-02-07 04:52:45,"Ah! Not a lot of @kennethreitz in that ;-P
(tl;dr, none of the recent blames are his)
Thanks!
",rattrayalex,kennethreitz
1897,2014-02-02 09:40:12,"Thanks for this!

However, I'm on the fence. So far we've made a conscious decision not to _officially_ document this support in Requests, because we don't adequately test it. We don't have an SNI environment in our CI system, we've regressed this support in the past and we'll probably do it again.

We should combine that with the fact that Requests is meant to be _for humans_. This SNI support is a frankly-fairly-ugly workaround for a serious failing in Python 2. I'm happy to suggest this as a workaround for people everywhere else on the internet (and I do), but I'm reluctant to add it to the core documentation.

However, if the rest of the team wants to I'm happy to let them, so I'll leave this open until @kennethreitz and @sigmavirus24 take a look.
",Lukasa,kennethreitz
1897,2014-02-02 09:40:12,"Thanks for this!

However, I'm on the fence. So far we've made a conscious decision not to _officially_ document this support in Requests, because we don't adequately test it. We don't have an SNI environment in our CI system, we've regressed this support in the past and we'll probably do it again.

We should combine that with the fact that Requests is meant to be _for humans_. This SNI support is a frankly-fairly-ugly workaround for a serious failing in Python 2. I'm happy to suggest this as a workaround for people everywhere else on the internet (and I do), but I'm reluctant to add it to the core documentation.

However, if the rest of the team wants to I'm happy to let them, so I'll leave this open until @kennethreitz and @sigmavirus24 take a look.
",Lukasa,sigmavirus24
1897,2014-02-04 10:34:12,"Excellent, I'm happy with that.

Normally I'd merge documentation changes myself, but this changes the layout a little bit and I want to chat with @kennethreitz a bit before we go ahead and merge. It will definitely get merged though. =D
",Lukasa,kennethreitz
1896,2014-01-31 13:32:01,"Thanks for this!

I'm -0.5 on this change, because I believe it to be a security risk. It is upsettingly easy to create a situation where such a module is installed that defines a `where()` function that returns arbitrary certificates, and such behaviour would not be noticed. Monkeypatching makes this behaviour harder (though not impossible).

I'm going to wait until @sigmavirus24 weighs in. =)
",Lukasa,sigmavirus24
1893,2014-01-30 17:34:26,"One day, crate.io will return as pypi.python.org. Unless @dstufft explodes in a ball of caremad.
",Lukasa,dstufft
1892,2014-01-29 19:15:37,"This should be a fix for #1885.

@sigmavirus24, can you give me some code review? =)
",Lukasa,sigmavirus24
1892,2014-01-30 20:38:17,"@kennethreitz we absolutely cannot continue reusing authorizations on redirects to sites that are not the same host. With that in mind we almost certainly need to issue a CVE. I'll happily work on that though.

We've been leaking credentials and we need to at least address that. Whether we repopulate the auth after stopping the leak or not is more of a feature decision. I'm sure one of the security experts, like @dstufft would back up @Lukasa and I on that.
",sigmavirus24,dstufft
1892,2014-01-31 14:20:32,"@Sirenity it isn't a big deal but requests should have far more than 8 million downloads last I checked (we're probably upwards of 9 million) and that's only from PyPI. We have no way of measuring how many downloads installations came from distribution repositories (i.e., people installing aptitude, yum, or pacman).
",sigmavirus24,Sirenity
1888,2014-01-28 17:20:23,"This seems like a reasonable fix to me. Unfortunately, the unit test is not a particularly good one because most people don't use the SNI build of Requests in their development systems. Additionally, we don't do it in our CI server (though we probably should, I'll bring it up with @kennethreitz). Leave it there for now: if we can get it into the CI server it'll be fine, otherwise we'll need another test.
",Lukasa,kennethreitz
1885,2014-01-27 17:54:29,"Thanks for this!

Right now we don't strip authentication information of any kind on redirects. We could, but we can't ask the user for new credentials, we can only really go to `~/. Thoughts @sigmavirus24?
",Lukasa,sigmavirus24
1885,2014-01-27 23:33:24,"Yeah I talked to some of the security guys at Bendyworks about this and they agree with you @eriolv. So a simple check of 



Have we decided about `Proxy-Authorization`?
",sigmavirus24,eriolv
1885,2014-01-31 05:28:05,"@eriolv do you have experience with CVEs? This was reported to debian but is not a debian specific issue. This is our responsibility to report to MITRE, right?
",sigmavirus24,eriolv
1885,2014-01-31 12:47:08,"Thanks for managing everything @eriolv. I'm going to guess none of you have submitted a CVE so I'm going to request one from MITRE since the CVE suggests just asking them.

FWIW, if you would like you can tell Endi that we're going to address the Proxy-Authorization case before the next release but in a separate PR probably.
",sigmavirus24,eriolv
1884,2014-01-27 10:01:29,"Thanks for this! Jython is weird. =)

I think before we merge this we should set up our CI server with a Jython build. If we're going to support Jython, let's do it properly. @kennethreitz do you want to do that yourself, or would you rather one of @sigmavirus24 or I did it?
",Lukasa,kennethreitz
1884,2014-01-27 10:01:29,"Thanks for this! Jython is weird. =)

I think before we merge this we should set up our CI server with a Jython build. If we're going to support Jython, let's do it properly. @kennethreitz do you want to do that yourself, or would you rather one of @sigmavirus24 or I did it?
",Lukasa,sigmavirus24
1884,2014-01-27 14:37:47,"@kennethreitz controls the Jenkins CI server so I would guess he'd be able to set up Jython if desired. FWIW, I would like to support Jython, but I'm curious about some things in the PR. None are really merge blockers.

The only thing that _is_ a merge blocker to me is failing HTTPS tests. If we're going to support Jython, given that we're the only package that does SSL as correctly as possible, I would want that to be fully functional before we ship anything advertising that we support Jython. Otherwise, the greatest draw to requests (beyond its API) is invalid and horribly broken. That raises the question:

How far out is SSL support?
",sigmavirus24,kennethreitz
1883,2014-01-26 19:01:39,"The default cipher suites set in the pyopenssl contrib are now slightly out of date. SSLLabs has updated their recommendations and now suggests not using RC4 (see: [Is BEAST still a threat?](http://blog.ivanristic.com/2013/09/is-beast-still-a-threat.html).

This particular suite has been lifted from the [current master of twisted](https://github.com/twisted/twisted/blob/trunk/twisted/internet/_sslverify.py#L995), but provides a whitelist that prefers forward secrecy for key exchange, AES-GCM for encryption (with fallbacks for CBC and 3DES if AES is unavailable), and disables null auth, etc. This was derived by @hynek, so if there questions about it we can drag him in to answer.
",reaperhulk,hynek
1882,2014-01-25 19:33:34,"Thanks for this!

This is actually a behaviour in urllib3. I'm going to summon @shazow into this conversation, but I believe this is a deliberate design decision. We can often reuse the socket object to reconnect to the same host without the overhead of recreating the object. This makes this deliberate. I wonder if we can silence the warning.
",Lukasa,shazow
1877,2014-01-23 23:34:26,"Considering you're the first person in over 2 years to request this, it seems as though this is not something that is popular enough to belong in the core of requests. A feature like this would need to be requested a whole lot more for us to even consider adding this method.

On the other hand though, you could totally get this into the [requests-toolbelt](https://github.com/sigmavirus24/requests-toolbelt) which is designed for exactly this kind of thing. This is something that would be awesome to have and if you can put together a PR to handle the parsing, that would be awesome! I'd love to merge it over there provided you also add tests around it.

I'll leave this open to make sure that @Lukasa has a chance to weigh in, but I have a hunch he'll have the same opinion as me.
",sigmavirus24,Lukasa
1876,2014-01-23 03:17:22,"@kennethreitz :cake: :+1: :shipit: 
",sigmavirus24,kennethreitz
1870,2014-01-19 04:48:47,"We can add more info to discuss the inclusion of @lukasa's `SSLAdapter`. Are there any objections to adding this information to requests' documentation?
",sigmavirus24,lukasa
1869,2014-01-31 14:34:29,"Let's do it unless @kennethreitz has an object to restricting the library to HTTP/1.1
",sigmavirus24,kennethreitz
1867,2014-01-16 08:39:35,"From #1493. Provides a way to use the `Response` object as a context manager without adding the conceptual overhead to the `Response` class.

I'm still on the fence about whether we should document this or just make `Response`s context managers, but this seems the least controversial option so lets do this first.

Review: @sigmavirus24, @pepijndevos.
",Lukasa,pepijndevos
1867,2014-01-16 08:39:35,"From #1493. Provides a way to use the `Response` object as a context manager without adding the conceptual overhead to the `Response` class.

I'm still on the fence about whether we should document this or just make `Response`s context managers, but this seems the least controversial option so lets do this first.

Review: @sigmavirus24, @pepijndevos.
",Lukasa,sigmavirus24
1865,2014-01-15 13:51:50,"In 3.4 the standard library's previously woeful `ssl` module is being vastly improved, and one of the nice things it is getting is [support for Certificate Revocation Lists](http://docs.python.org/3.4/library/ssl.html#ssl.SSLContext.load_verify_locations). I'd like us to investigate supporting CRLs in versions of Python where they are available. If we can do this, it'll continue the ongoing trend of Requests being the single most secure way to do HTTP in Python.

It's highly probable that this will need to be a two-pronged approach: urllib3 will probably want to support some genericised CRL options, with Requests pasting over the genericness with a built-in ""correct"" behaviour. For that reason, I want to start a discussion with the relevant people to work out how we can do this.

Requests people: @kennethreitz, @sigmavirus24
urllib3 people: @shazow 
Our SSL guy: @t-8ch.

People who might be interested in either helping or providing insight and code review (to make sure we don't get this wrong) are @tiran (heavily involved with the standard library's `ssl` module) and @alex (currently on a big crypto kick). If either of you two are actually not interested that's totally fine. =)

Initial views: how should we split this work up, and how safe can Requests' defaults be?
",Lukasa,kennethreitz
1865,2014-01-15 13:51:50,"In 3.4 the standard library's previously woeful `ssl` module is being vastly improved, and one of the nice things it is getting is [support for Certificate Revocation Lists](http://docs.python.org/3.4/library/ssl.html#ssl.SSLContext.load_verify_locations). I'd like us to investigate supporting CRLs in versions of Python where they are available. If we can do this, it'll continue the ongoing trend of Requests being the single most secure way to do HTTP in Python.

It's highly probable that this will need to be a two-pronged approach: urllib3 will probably want to support some genericised CRL options, with Requests pasting over the genericness with a built-in ""correct"" behaviour. For that reason, I want to start a discussion with the relevant people to work out how we can do this.

Requests people: @kennethreitz, @sigmavirus24
urllib3 people: @shazow 
Our SSL guy: @t-8ch.

People who might be interested in either helping or providing insight and code review (to make sure we don't get this wrong) are @tiran (heavily involved with the standard library's `ssl` module) and @alex (currently on a big crypto kick). If either of you two are actually not interested that's totally fine. =)

Initial views: how should we split this work up, and how safe can Requests' defaults be?
",Lukasa,alex
1865,2014-01-15 13:51:50,"In 3.4 the standard library's previously woeful `ssl` module is being vastly improved, and one of the nice things it is getting is [support for Certificate Revocation Lists](http://docs.python.org/3.4/library/ssl.html#ssl.SSLContext.load_verify_locations). I'd like us to investigate supporting CRLs in versions of Python where they are available. If we can do this, it'll continue the ongoing trend of Requests being the single most secure way to do HTTP in Python.

It's highly probable that this will need to be a two-pronged approach: urllib3 will probably want to support some genericised CRL options, with Requests pasting over the genericness with a built-in ""correct"" behaviour. For that reason, I want to start a discussion with the relevant people to work out how we can do this.

Requests people: @kennethreitz, @sigmavirus24
urllib3 people: @shazow 
Our SSL guy: @t-8ch.

People who might be interested in either helping or providing insight and code review (to make sure we don't get this wrong) are @tiran (heavily involved with the standard library's `ssl` module) and @alex (currently on a big crypto kick). If either of you two are actually not interested that's totally fine. =)

Initial views: how should we split this work up, and how safe can Requests' defaults be?
",Lukasa,tiran
1865,2014-01-15 13:51:50,"In 3.4 the standard library's previously woeful `ssl` module is being vastly improved, and one of the nice things it is getting is [support for Certificate Revocation Lists](http://docs.python.org/3.4/library/ssl.html#ssl.SSLContext.load_verify_locations). I'd like us to investigate supporting CRLs in versions of Python where they are available. If we can do this, it'll continue the ongoing trend of Requests being the single most secure way to do HTTP in Python.

It's highly probable that this will need to be a two-pronged approach: urllib3 will probably want to support some genericised CRL options, with Requests pasting over the genericness with a built-in ""correct"" behaviour. For that reason, I want to start a discussion with the relevant people to work out how we can do this.

Requests people: @kennethreitz, @sigmavirus24
urllib3 people: @shazow 
Our SSL guy: @t-8ch.

People who might be interested in either helping or providing insight and code review (to make sure we don't get this wrong) are @tiran (heavily involved with the standard library's `ssl` module) and @alex (currently on a big crypto kick). If either of you two are actually not interested that's totally fine. =)

Initial views: how should we split this work up, and how safe can Requests' defaults be?
",Lukasa,t-8ch
1865,2014-01-15 13:51:50,"In 3.4 the standard library's previously woeful `ssl` module is being vastly improved, and one of the nice things it is getting is [support for Certificate Revocation Lists](http://docs.python.org/3.4/library/ssl.html#ssl.SSLContext.load_verify_locations). I'd like us to investigate supporting CRLs in versions of Python where they are available. If we can do this, it'll continue the ongoing trend of Requests being the single most secure way to do HTTP in Python.

It's highly probable that this will need to be a two-pronged approach: urllib3 will probably want to support some genericised CRL options, with Requests pasting over the genericness with a built-in ""correct"" behaviour. For that reason, I want to start a discussion with the relevant people to work out how we can do this.

Requests people: @kennethreitz, @sigmavirus24
urllib3 people: @shazow 
Our SSL guy: @t-8ch.

People who might be interested in either helping or providing insight and code review (to make sure we don't get this wrong) are @tiran (heavily involved with the standard library's `ssl` module) and @alex (currently on a big crypto kick). If either of you two are actually not interested that's totally fine. =)

Initial views: how should we split this work up, and how safe can Requests' defaults be?
",Lukasa,shazow
1865,2014-01-15 13:51:50,"In 3.4 the standard library's previously woeful `ssl` module is being vastly improved, and one of the nice things it is getting is [support for Certificate Revocation Lists](http://docs.python.org/3.4/library/ssl.html#ssl.SSLContext.load_verify_locations). I'd like us to investigate supporting CRLs in versions of Python where they are available. If we can do this, it'll continue the ongoing trend of Requests being the single most secure way to do HTTP in Python.

It's highly probable that this will need to be a two-pronged approach: urllib3 will probably want to support some genericised CRL options, with Requests pasting over the genericness with a built-in ""correct"" behaviour. For that reason, I want to start a discussion with the relevant people to work out how we can do this.

Requests people: @kennethreitz, @sigmavirus24
urllib3 people: @shazow 
Our SSL guy: @t-8ch.

People who might be interested in either helping or providing insight and code review (to make sure we don't get this wrong) are @tiran (heavily involved with the standard library's `ssl` module) and @alex (currently on a big crypto kick). If either of you two are actually not interested that's totally fine. =)

Initial views: how should we split this work up, and how safe can Requests' defaults be?
",Lukasa,sigmavirus24
1861,2014-01-13 07:16:20,"Thanks for this!

In principle this looks fine. My only worry is that technically this changes what our multipart requests look like on the wire, and I worry about breaking currently functional code. I'm interested to see if @sigmavirus24 is as worried about that as me.
",Lukasa,sigmavirus24
1861,2014-01-13 09:00:08,"@Lukasa 

> My only worry is that technically this changes what our multipart requests look like on the wire

I don't think it changed anything. A proper guess of file content type shouldn't break anything (I hope).

ping @sigmavirus24 for a code review.
",lepture,sigmavirus24
1861,2014-01-13 10:19:40,"I mean, in the literal sense it definitely changed something.

This:



 is not equal to this:



You're right, this _shouldn't_ break things, but this is the web we're talking about, it definitely has a chance to. =) That's why I want @sigmavirus24 to take a look.
",Lukasa,sigmavirus24
1861,2014-01-14 21:20:33,"@shazow how accurate is this detection?
",kennethreitz,shazow
1861,2014-01-16 08:26:06,"Actually, that's a point. @dstufft, can you give us an idea of what `pip`s release cycle looks like from the perspective of supporting 2.6?
",Lukasa,dstufft
1860,2014-01-12 20:28:11,"Fixes #1859

Credit: @lukasa
",sigmavirus24,lukasa
1860,2014-01-12 20:34:24,"That's a good point @Lukasa. I'm guessing @gazpachoking might have a good idea. I'm just too tired to think this hard right now =P
",sigmavirus24,gazpachoking
1858,2014-01-11 10:00:52,"This _should_ resolve #1856. /cc @sigmavirus24 @t-8ch.
",Lukasa,t-8ch
1858,2014-01-11 10:00:52,"This _should_ resolve #1856. /cc @sigmavirus24 @t-8ch.
",Lukasa,sigmavirus24
1858,2014-01-12 14:26:45,"And the other place it's used is here: https://github.com/kennethreitz/requests/blob/ac4e05874a1a983ca126185a0e4d4e74915f792e/requests/models.py#L456 and notice that there's an optional param to that method that is never used if it is ever passed. We should either use it or remove it (not necessarily in this PR though).

Finally, given that the call to `HTTPAdapter#proxy_headers` is wrapped inside an `if proxy:` block, the param it sends to `get_auth_from_url` should never be `None` or `''`. And `prepare_auth` on the `PreparedRequest` is called after `prepare_url` which should blow up if `url` is not a valid `url`. I think we're safe making `get_auth_from_url` a bit less paranoid. As with all of my reviews though, this is totally up to the discretion of @kennethreitz and @Lukasa 
",sigmavirus24,kennethreitz
1858,2014-01-12 14:58:02,"@kennethreitz :shipit: :+1:
",sigmavirus24,kennethreitz
1858,2014-01-13 12:42:17,"@Lukasa @sybeck2k found a problem with this test: https://github.com/kennethreitz/requests/pull/1862/files#diff-56c2d754173a4a158ce8f445834c8fe8R705 can you fix it here too?
",sigmavirus24,sybeck2k
1857,2014-01-10 18:14:42,"After the recent discussion about SSL compression both in shazow/urllib3#109
and kennethreitz/requests#1583 (this issue was about performance reason, it got me to look at other projects, performance is _not_ the reason for this issue) I looked at the behaviour of other popular open source projects. It turned out that the following projects disable the compression by default for [security reasons](https://en.wikipedia.org/wiki/CRIME_(security_exploit\)):
- Nginx, July 2012 ([version 1.2.2](http://nginx.org/en/CHANGES-1.2), [version 1.3.2, the development version to 1.4](http://nginx.org/en/CHANGES-1.4)
- Apache2, ([version 2.2.25](https://httpd.apache.org/docs/2.2/mod/mod_ssl.html#sslcompression), [version 2.4.4](https://httpd.apache.org/docs/2.4/mod/mod_ssl.html#sslcompression))
- The upcoming version 2.7 of the Pound reverse proxy (the `CHANGELOG` file in the source archive: http://www.apsis.ch/pound/Pound-2.7b.tgz)
- [Curl 7.28.1](http://curl.haxx.se/changes.html#7_28_1) July 2012
- CPython >= 3.4 (http://hg.python.org/cpython/rev/98eb88d3d94e)

This are the only projects I have looked at. I am sure we can find more if necessary.
As the stdlib `ssl` module does not allow us to change this parameter before 3.3
I propose to raise the issue on the CPython bug tracker, so that SSL
compression will be disabled by default (with the possibility to manually
enable it on 3.3 and later).

The current handlich of CPython 3.4 and up only disables compression on openssl
1.0 and up, as the relevant constant has not introduced before. However the
nginx changelog claims to also disable compression on earlier versions. I will
look into this.

This issue is meant to gather feedback and momentum before raising the issue with CPython (and maybe also the other implementations)

/cc @lukasa @sigmavirus24 @shazow @alex @jmhodges
",t-8ch,lukasa
1857,2014-01-10 18:14:42,"After the recent discussion about SSL compression both in shazow/urllib3#109
and kennethreitz/requests#1583 (this issue was about performance reason, it got me to look at other projects, performance is _not_ the reason for this issue) I looked at the behaviour of other popular open source projects. It turned out that the following projects disable the compression by default for [security reasons](https://en.wikipedia.org/wiki/CRIME_(security_exploit\)):
- Nginx, July 2012 ([version 1.2.2](http://nginx.org/en/CHANGES-1.2), [version 1.3.2, the development version to 1.4](http://nginx.org/en/CHANGES-1.4)
- Apache2, ([version 2.2.25](https://httpd.apache.org/docs/2.2/mod/mod_ssl.html#sslcompression), [version 2.4.4](https://httpd.apache.org/docs/2.4/mod/mod_ssl.html#sslcompression))
- The upcoming version 2.7 of the Pound reverse proxy (the `CHANGELOG` file in the source archive: http://www.apsis.ch/pound/Pound-2.7b.tgz)
- [Curl 7.28.1](http://curl.haxx.se/changes.html#7_28_1) July 2012
- CPython >= 3.4 (http://hg.python.org/cpython/rev/98eb88d3d94e)

This are the only projects I have looked at. I am sure we can find more if necessary.
As the stdlib `ssl` module does not allow us to change this parameter before 3.3
I propose to raise the issue on the CPython bug tracker, so that SSL
compression will be disabled by default (with the possibility to manually
enable it on 3.3 and later).

The current handlich of CPython 3.4 and up only disables compression on openssl
1.0 and up, as the relevant constant has not introduced before. However the
nginx changelog claims to also disable compression on earlier versions. I will
look into this.

This issue is meant to gather feedback and momentum before raising the issue with CPython (and maybe also the other implementations)

/cc @lukasa @sigmavirus24 @shazow @alex @jmhodges
",t-8ch,shazow
1857,2014-01-10 18:14:42,"After the recent discussion about SSL compression both in shazow/urllib3#109
and kennethreitz/requests#1583 (this issue was about performance reason, it got me to look at other projects, performance is _not_ the reason for this issue) I looked at the behaviour of other popular open source projects. It turned out that the following projects disable the compression by default for [security reasons](https://en.wikipedia.org/wiki/CRIME_(security_exploit\)):
- Nginx, July 2012 ([version 1.2.2](http://nginx.org/en/CHANGES-1.2), [version 1.3.2, the development version to 1.4](http://nginx.org/en/CHANGES-1.4)
- Apache2, ([version 2.2.25](https://httpd.apache.org/docs/2.2/mod/mod_ssl.html#sslcompression), [version 2.4.4](https://httpd.apache.org/docs/2.4/mod/mod_ssl.html#sslcompression))
- The upcoming version 2.7 of the Pound reverse proxy (the `CHANGELOG` file in the source archive: http://www.apsis.ch/pound/Pound-2.7b.tgz)
- [Curl 7.28.1](http://curl.haxx.se/changes.html#7_28_1) July 2012
- CPython >= 3.4 (http://hg.python.org/cpython/rev/98eb88d3d94e)

This are the only projects I have looked at. I am sure we can find more if necessary.
As the stdlib `ssl` module does not allow us to change this parameter before 3.3
I propose to raise the issue on the CPython bug tracker, so that SSL
compression will be disabled by default (with the possibility to manually
enable it on 3.3 and later).

The current handlich of CPython 3.4 and up only disables compression on openssl
1.0 and up, as the relevant constant has not introduced before. However the
nginx changelog claims to also disable compression on earlier versions. I will
look into this.

This issue is meant to gather feedback and momentum before raising the issue with CPython (and maybe also the other implementations)

/cc @lukasa @sigmavirus24 @shazow @alex @jmhodges
",t-8ch,alex
1857,2014-01-10 18:14:42,"After the recent discussion about SSL compression both in shazow/urllib3#109
and kennethreitz/requests#1583 (this issue was about performance reason, it got me to look at other projects, performance is _not_ the reason for this issue) I looked at the behaviour of other popular open source projects. It turned out that the following projects disable the compression by default for [security reasons](https://en.wikipedia.org/wiki/CRIME_(security_exploit\)):
- Nginx, July 2012 ([version 1.2.2](http://nginx.org/en/CHANGES-1.2), [version 1.3.2, the development version to 1.4](http://nginx.org/en/CHANGES-1.4)
- Apache2, ([version 2.2.25](https://httpd.apache.org/docs/2.2/mod/mod_ssl.html#sslcompression), [version 2.4.4](https://httpd.apache.org/docs/2.4/mod/mod_ssl.html#sslcompression))
- The upcoming version 2.7 of the Pound reverse proxy (the `CHANGELOG` file in the source archive: http://www.apsis.ch/pound/Pound-2.7b.tgz)
- [Curl 7.28.1](http://curl.haxx.se/changes.html#7_28_1) July 2012
- CPython >= 3.4 (http://hg.python.org/cpython/rev/98eb88d3d94e)

This are the only projects I have looked at. I am sure we can find more if necessary.
As the stdlib `ssl` module does not allow us to change this parameter before 3.3
I propose to raise the issue on the CPython bug tracker, so that SSL
compression will be disabled by default (with the possibility to manually
enable it on 3.3 and later).

The current handlich of CPython 3.4 and up only disables compression on openssl
1.0 and up, as the relevant constant has not introduced before. However the
nginx changelog claims to also disable compression on earlier versions. I will
look into this.

This issue is meant to gather feedback and momentum before raising the issue with CPython (and maybe also the other implementations)

/cc @lukasa @sigmavirus24 @shazow @alex @jmhodges
",t-8ch,sigmavirus24
1857,2014-01-10 18:14:42,"After the recent discussion about SSL compression both in shazow/urllib3#109
and kennethreitz/requests#1583 (this issue was about performance reason, it got me to look at other projects, performance is _not_ the reason for this issue) I looked at the behaviour of other popular open source projects. It turned out that the following projects disable the compression by default for [security reasons](https://en.wikipedia.org/wiki/CRIME_(security_exploit\)):
- Nginx, July 2012 ([version 1.2.2](http://nginx.org/en/CHANGES-1.2), [version 1.3.2, the development version to 1.4](http://nginx.org/en/CHANGES-1.4)
- Apache2, ([version 2.2.25](https://httpd.apache.org/docs/2.2/mod/mod_ssl.html#sslcompression), [version 2.4.4](https://httpd.apache.org/docs/2.4/mod/mod_ssl.html#sslcompression))
- The upcoming version 2.7 of the Pound reverse proxy (the `CHANGELOG` file in the source archive: http://www.apsis.ch/pound/Pound-2.7b.tgz)
- [Curl 7.28.1](http://curl.haxx.se/changes.html#7_28_1) July 2012
- CPython >= 3.4 (http://hg.python.org/cpython/rev/98eb88d3d94e)

This are the only projects I have looked at. I am sure we can find more if necessary.
As the stdlib `ssl` module does not allow us to change this parameter before 3.3
I propose to raise the issue on the CPython bug tracker, so that SSL
compression will be disabled by default (with the possibility to manually
enable it on 3.3 and later).

The current handlich of CPython 3.4 and up only disables compression on openssl
1.0 and up, as the relevant constant has not introduced before. However the
nginx changelog claims to also disable compression on earlier versions. I will
look into this.

This issue is meant to gather feedback and momentum before raising the issue with CPython (and maybe also the other implementations)

/cc @lukasa @sigmavirus24 @shazow @alex @jmhodges
",t-8ch,jmhodges
1857,2014-01-10 18:18:09,"Fwiw, urllib3 _just_ merged a PR which disables this by default in Py32+ and PyOpenSSL, big thanks to @dbrgn for bringing it up and writing the patch. https://github.com/shazow/urllib3/pull/309
",shazow,dbrgn
1857,2014-01-10 18:48:17,"I'm 100% in favour of this. 2.7 is clearly accepting some SSL related changes in the next version (see [20207](http://bugs.python.org/issue20207) from @alex), so it doesn't seem unreasonable to make a push for this as well.

Unfortunately, it doesn't save requests/urllib3 _entirely_, because both projects support 2.6 and 2.6 is deader-than-dead.
",Lukasa,alex
1856,2014-01-10 15:56:40,"Thanks for raising this issue!

It's hard to see how unquoting before parsing the URL is really helping us. We can almost certainly defer the unquote step until after parsing the URL, applying unquote only to the username and password. @sigmavirus24, can you think of any reason that's not going to be ok?
",Lukasa,sigmavirus24
1856,2014-01-11 09:10:51,"Ok, so `urlparse` chokes like a champ on a sample URL, and so does urllib3's superior `parse_url` function:





Thoughts? /cc @shazow
",Lukasa,shazow
1853,2014-01-09 16:13:16,"Also, @t-8ch, our resident TLS guru, can you confirm that this is/isn't related to specific installs of OpenSSL?
",Lukasa,t-8ch
1849,2014-01-07 23:15:06,"This isn't necessary since @kennethreitz pulls in a fresh copy of urllib3 before each release I think. But thanks, this will hopefully ensure he pulls in the right version again.
",sigmavirus24,kennethreitz
1847,2014-01-07 19:57:50,"@kennethreitz has previously stated that he views requests as a product and anything else as an implementation detail, so I assumed he'd want to know about this to address this in requests, if at all possible.
",alex,kennethreitz
1846,2014-01-07 17:30:05,"Alright, I'm happy with this. You can ignore TravisCI (I have no idea why it's still running), the relevant CI results are these (all of which passed):

http://ci.kennethreitz.org/job/requests-pr/PYTHON=2.6/167/
http://ci.kennethreitz.org/job/requests-pr/PYTHON=2.7/167/
http://ci.kennethreitz.org/job/requests-pr/PYTHON=3.3/167/
http://ci.kennethreitz.org/job/requests-pr/PYTHON=pypy-1.8/167/

Waiting on @sigmavirus24 to review.
",Lukasa,sigmavirus24
1844,2014-01-09 13:08:04,"@Lukasa Looks good to me. I had the reporter of the bugs test it out and he was able to successfully install stuff without error. So once @kennethreitz cuts a new release (hopefully soon!) pip 1.5.1 can vendor that release and close out those bugs.
",dstufft,kennethreitz
1843,2014-01-07 08:51:59,"I'm obviously tempted to argue that `pip` should be able to spot this and handle it (that is, any compliant server would signal `Content-Encoding: gzip` and `Content-Type: application/x-tar`), but that's not a very helpful way to approach this problem.

In principle we could do that, it would be very easy from our side. @kennethreitz, the API is your call: are you happy to do this?
",Lukasa,kennethreitz
1843,2014-01-07 09:30:30,"Yeah, that was basically the decision I want @kennethreitz to make: do we just say ""tough, use the raw response"", or do we provide the kwarg.
",Lukasa,kennethreitz
1843,2014-01-07 09:32:28,"The current intended API design is to use .raw for this.

## 

Kenneth Reitz

> On Jan 7, 2014, at 3:52 AM, Cory Benfield notifications@github.com wrote:
> 
> I'm obviously tempted to argue that pip should be able to spot this and handle it (that is, any compliant server would signal Content-Encoding: gzip and Content-Type: application/x-tar), but that's not a very helpful way to approach this problem.
> 
> In principle we could do that, it would be very easy from our side. @kennethreitz, the API is your call: are you happy to do this?
> 
> —
> Reply to this email directly or view it on GitHub.
",kennethreitz,kennethreitz
1842,2014-01-06 20:57:50,"@sigmavirus24 and I are in favour of not supporting very old patch releases of 2.6. For perspective, 2.6.0 was released in 2008 and 2.6.2 was released in April 2009.

Obviously, we'll take the fix in urllib3, but I see no reason for Requests to rush into supporting a five year old release of Python. Anyone who felt the need to be on Python 2.6 should have been taking security patches and be at 2.6.9. 
",Lukasa,sigmavirus24
1842,2014-01-06 21:07:06,"Agreed, was mostly creating this so people can hopefully find it on Google
if they run into this problem.

## 

Kevin Burke | Twilio
phone: 925.271.7005 | kev.inburke.com

On Mon, Jan 6, 2014 at 12:58 PM, Cory Benfield notifications@github.comwrote:

> @sigmavirus24 https://github.com/sigmavirus24 and I are in favour of
> not supporting very old patch releases of 2.6. For perspective, 2.6.0 was
> released in 2008 and 2.6.2 was released in April 2009.
> 
> Obviously, we'll take the fix in urllib3, but I see no reason for Requests
> to rush into supporting a five year old release of Python. Anyone who felt
> the need to be on Python 2.6 should have been taking security patches and
> be at 2.6.9.
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/issues/1842#issuecomment-31686263
> .
",kevinburke,sigmavirus24
1842,2014-01-07 12:26:22,"I'm fine with releasing relatively soon but that responsibility lies entirely on @kenmethreitz's shoulders.
",sigmavirus24,kenmethreitz
1834,2013-12-30 17:38:18,"PyPI is a strange beast that is luckily being slowly replaced by [pypa/warehouse](https://github.com/pypa/warehouse). In the past I had issues where I created a package name manually and then metadata from `setup.py` didn't properly appear. I suspect this might be something similar. Regardless only @kennethreitz can change this and it isn't an actual bug in requests.
",sigmavirus24,kennethreitz
1831,2013-12-28 11:20:24,"This in principle looks fine, though I don't know about about intersphinx to be sure. =) Let's see what @sigmavirus24 thinks.
",Lukasa,sigmavirus24
1827,2013-12-24 10:27:53,"Thanks for this!

I like this solution, I think it's the correct approach. We can quite safely overload these kwargs, and I think doing so provides the nicer interface.

Once again, it's annoying that we can't easily test this, but I'm happy with this solution as-is. @sigmavirus24?
",Lukasa,sigmavirus24
1818,2013-12-20 02:33:58,"This sounds like a bug in urllib3. @shazow opinions?
",sigmavirus24,shazow
1818,2013-12-20 02:40:49,"Actually following the conversation on #171 this is not a bug in requests but in Python as @Lukasa explained.
",sigmavirus24,Lukasa
1813,2013-12-18 22:15:35,"@orokusaki In principle there's no reason. I suggest raising the issue on urllib3 and seeing what @shazow thinks. =)
",Lukasa,shazow
1813,2013-12-18 22:56:48,"Ah, so it sounds like maybe a `requests==3.0.0` type of feature, if it were to be added. Thanks for checking on that for me @shazow.

@Lukasa @kennethreitz If more things were delegated to urllib3, it would simplify the requests codebase even more. If either of you have suggestions pertaining a change like this, I'd be more than happy to take a stab at it.
",orokusaki,kennethreitz
1811,2013-12-18 15:25:45,"We have discussed this issue with users several times in the past. A search of the issues that are closed would show that. Our opinion has not changed, nor has our reasoning. By vendoring urllib3 we have a very specific version that we have tested against that may include unreleased bug fixes and does not put pressure on @shazow. It also allows us to checkout a specific version of the repository and just work on it with what existed at the time. It's reliable and it will be how we work for the foreseeable future. I'll leave this open until @Lukasa can make his way around to it, but I'm strongly in favor of closing this.
",sigmavirus24,shazow
1811,2013-12-18 15:25:45,"We have discussed this issue with users several times in the past. A search of the issues that are closed would show that. Our opinion has not changed, nor has our reasoning. By vendoring urllib3 we have a very specific version that we have tested against that may include unreleased bug fixes and does not put pressure on @shazow. It also allows us to checkout a specific version of the repository and just work on it with what existed at the time. It's reliable and it will be how we work for the foreseeable future. I'll leave this open until @Lukasa can make his way around to it, but I'm strongly in favor of closing this.
",sigmavirus24,Lukasa
1810,2013-12-18 14:48:06,"This shouldn't affect any of the tests @kennethreitz and should be merged before the next release.
",sigmavirus24,kennethreitz
1808,2013-12-18 04:55:25,"Sure cookies are persisted to the session, but those new cookies are not added
to the next prepared request. We need to update that new request's CookieJar
with the new cookies.

/cc @teleyinex
This allows me to fetch the URL you specified quite quickly.

TODO:
- [ ] Find a way to reliably test this. Perhaps introduce [Betamax](https://github.com/sigmavirus24/betamax) to make this test reproducible.
",sigmavirus24,teleyinex
1806,2013-12-18 13:40:25,"I'm waiting for @Lukasa to weigh in before closing this.
",sigmavirus24,Lukasa
1805,2013-12-17 15:32:09,"Urgh, this is a cookies problem. Observe:



@gazpachoking, @sigmavirus24, I choose you!
",Lukasa,gazpachoking
1805,2013-12-17 15:32:09,"Urgh, this is a cookies problem. Observe:



@gazpachoking, @sigmavirus24, I choose you!
",Lukasa,sigmavirus24
1802,2013-12-16 14:08:23,"I agree with 100% of what @Lukasa said. In fact, even curl would give you far more information to debug with. _Furthermore_, there are headers that we compute that we don't even set on the `PreparedRequest` object so adding this method to that would be useless too quite frankly. Unfortunately your best bet is something like `mitmproxy` or `wireshark` (locally) or a service like `Runscope` or `httpbin`.

With that said, the both of us are in agreement and I'm certain @kennethreitz will agree with both of us, so I'm going to close this.
",sigmavirus24,kennethreitz
1801,2013-12-16 04:10:00,"Per discussion with @sigmavirus24 in IRC, this PR kills the Timeout class and adds support for connect timeouts via a simple tuple interface: 



Sometimes you try to connect to a dead host (this happens to us all the time, because of AWS) and you would like to fail fairly quickly in this case; the request has no chance of succeeding. However, once the connection succeeds, you'd like to give the server a fairly long time to process the request and return a response. 

The attached tests and documentation have more information. Hope this is okay!
",kevinburke,sigmavirus24
1801,2013-12-16 08:56:58,"In principle I'm +0.5 on this. It's a nice extension. However, as memory serves, @kennethreitz considered this approach and ended up not taking it. It might be that now that someone has written the code he'll be happy with it, but equally, it might be that he doesn't like the approach.

@sigmavirus24 Yeah, `TimeoutSauce` is used for the urllib3 `Timeout` object, because we have our own `Timeout` object (an exception).
",Lukasa,kennethreitz
1801,2013-12-16 14:05:32,"@Lukasa As I understood it @kennethreitz was more concerned with the addition (and requirement) of the `Timeout` class from urllib3. And thanks for clearing up the naming, I still think there has to be a better name for it. (I'm shaving a yak, I know)
",sigmavirus24,kennethreitz
1801,2014-04-05 17:31:03,"I believe connection timeouts would be retried 3 times as they represent a
failure to connect to the server, but not at my computer at the moment.

On Saturday, April 5, 2014, Cory Benfield notifications@github.com wrote:

> That's weird, it works fine on Python 2.7. Seems like a Python 3 bug,
> because I can reproduce your problem in 3.4. @kevinburkehttps://github.com/kevinburke,
> are you aware of any timeout bugs in urllib3?
> 
> ## 
> 
> Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/pull/1801#issuecomment-39640149
> .

## 

## 

Kevin Burke | Twilio
phone: 925.271.7005 | kev.inburke.com
",kevinburke,kevinburkehttps
1801,2014-05-10 15:50:59,"@lukasa https://github.com/shazow/urllib3/pull/326 ; though now that I read it more carefully, if the OS itself is trying each DNS record in turn then there's not much that can be done. That pull request lets you specify the number of times you would like to retry a connection failure, whether a timeout or an error.
",kevinburke,lukasa
1800,2013-12-15 03:16:53,"@dan-blanchard has taken over maintenance of [chardet](erikrose/chardet). We've merged charade into chardet so that chardet will now work on python2 and python3 without needing separate versions. I'd like to get rid of charade since it's been a responsibility that I had neglected and has been more trouble than it has been worth.

We should start using chardet proper again when they have it ready. (In all candor it should already be ready)
",sigmavirus24,dan-blanchard
1799,2013-12-14 20:14:48,"I'm something in the region of -0 on this. @sigmavirus24?
",Lukasa,sigmavirus24
1796,2013-12-14 09:41:16,"Thanks for this @martinblech!

Unfortunately, I'm opposed to this change. Not anything to do with the code itself, but I don't think adding this parameter to the `Response` object adds sufficient value to justify including it. If people _really_ want this information they can get it (by running `charade`/`chardet` themselves), and for a significant majority of people this information is simply unnecessary. Requests got where it was today by hiding complexity from users, and I'm in favour of that trend continuing. @sigmavirus24?
",Lukasa,sigmavirus24
1795,2013-12-14 01:01:56,"Ah, good spot. We have mixed feelings about auto detection. It's hard to do right, and no matter how well we do there will always be edge cases which cause problems.

Realistically, there's not a lot we can do about that beyond warn people. Auto detection _usually_ works well enough for people. I'm interested to see what @sigmavirus24 thinks. 
",Lukasa,sigmavirus24
1794,2013-12-13 11:06:27,"This fix is a subset of the fix in #1793. You and @sigmavirus24 should work out which fix we apply (maybe a merged set of the fixes).
",Lukasa,sigmavirus24
1793,2013-12-12 19:04:51,"@Lukasa if you'd like to add tests for this you can. I'm not exactly certain if we already had tests for pickling objects and didn't have the time to check right now. I'll check when I get home though (unless you've already added tests).
",sigmavirus24,Lukasa
1793,2013-12-14 04:30:42,"@Lukasa any further comments?
",sigmavirus24,Lukasa
1790,2013-12-12 06:57:48,"Hey, thanks for thi @kracekumar!

I don't think this is a good idea, however. The key reason is that auto-converting outgoing data is upsettingly magic. As much as possible, what you put in one parameter to the request function (e.g. `headers`) should not affect what happens to something that came in on another parameter (e.g. `data`, or `auth`).

It's also not quite the same as what we do on `Response`s. For a `Response`, we don't automatically convert to JSON unless explicitly asked to by the user: that is, they have to actually call `Response.json()`. That fits the Zen of Python (""Explicit is better than implicit""). For that reason as well, I'd rather that we stick to the current behaviour.

Let's leave this open to see if @sigmavirus24 agrees.
",Lukasa,sigmavirus24
1789,2013-12-11 16:30:35,"Yeah, @t-8ch seems to have the right analysis. In CPython, it takes 29milliseconds per call to `requests.get()`, of which 22ms is `getaddrinfo()`. In PyPy, it takes 47 ms per call to `requests.get()`, of which 35ms is `getaddrinfo()` That accounts for 13 ms of the 18ms difference. I imagine the remaining portion of that time difference is probably because the profiler doesn't play well with PyPy (I seem to recall that being a problem, though @alex will surely be able to correct me if it isn't).
",Lukasa,alex
1789,2013-12-11 16:36:22,"Do you have a benchmark of just `getaddrinfo()`? /cc @fijal
",alex,fijal
1787,2013-12-19 08:30:06,"Hang on, it looks to me like urllib3 doesn't catch `socket.timeout` in `Response.read()`. That feels clearly and obviously wrong to me. @shazow, (and @kevinburke since you implemented this), am I missing something here?
",Lukasa,shazow
1787,2013-12-19 08:30:06,"Hang on, it looks to me like urllib3 doesn't catch `socket.timeout` in `Response.read()`. That feels clearly and obviously wrong to me. @shazow, (and @kevinburke since you implemented this), am I missing something here?
",Lukasa,kevinburke
1779,2013-12-05 18:29:19,"I'm not really sure. It's a small change but we would then have to expose it to the user and that's what I'm not exactly a fan of. Certainly they can find it on their own now but that's not the same as exposing it to them which would be encouraging its use. How commonly do requests users set their own User-Agent string and want it to include all of that information? Frankly I'm not convinced it is all that frequently. That said if @kennethreitz wants this I'm perfectly okay with it granted that it handles strings correctly (as @Lukasa already mentioned).
",sigmavirus24,kennethreitz
1776,2013-12-04 12:46:50,"This is just @gazpachoking's PR #1729 in a mergable state and with the PR feedback that @Lukasa left on it.
",sigmavirus24,gazpachoking
1776,2013-12-04 12:46:50,"This is just @gazpachoking's PR #1729 in a mergable state and with the PR feedback that @Lukasa left on it.
",sigmavirus24,Lukasa
1775,2013-12-03 22:37:33,"@kennethreitz can you turn off the Travis integration hook altogether? This is annoying. I'd be willing to guess that they both ran this but Travis ran it last and got precendence over the other.
",sigmavirus24,kennethreitz
1775,2013-12-04 01:39:39,"Two-Factor Auth might invalidate old tokens but really _shouldn't_. That also only effects it if Jenkins is using Basic Authentication to access the API. If it is, then that is most certainly the issue. If you made a token however, then that's really bizarre and should be mentioned to the API team via the contact form. We could also just ping @izuzak here ;)
",sigmavirus24,izuzak
1775,2013-12-04 20:05:26,"> We could also just ping @izuzak here ;)

Pong! Just wanted to drop a note about those statuses. Notice the difference when you request statuses for the commit in the context of sigmavirus24/requests:

https://api.github.com/repos/sigmavirus24/requests/statuses/617bc8cc1c1b34b11954cf9e4caec3a25a07167b

and in the context of kennethreitz/requests

https://api.github.com/repos/kennethreitz/requests/statuses/617bc8cc1c1b34b11954cf9e4caec3a25a07167b

So, statuses were set by Travis, but just for sigmavirus24/requests (and not for kennethreitz/requests), which was expected, right?

Also, as far as I know, turning on 2FA should not invalidate existing OAuth tokens. [Let us know](https://github.com/contact) if you suspect that indeed did happen.
",izuzak,izuzak
1773,2013-12-03 12:44:55,"Fix issue @tardyp reported on #239

We have not been able to reproduce on a simple testcase. We can only make it happen on our complex environment; involving buildbot, twisted and txrequests.

This fix no longer exhibits the issue. Please, give us a feedback whether this solutions is correct.
",vincentxb,tardyp
1772,2013-12-03 16:06:09,"@sigmavirus24 - Pushed the changes we discussed. Let me know if you have anything else!

Thanks!
",mdbecker,sigmavirus24
1769,2013-12-02 10:46:54,"Hi there!

First, the answer. Yes, this is a known issue, and a fix is already present in master.

Now, some notes. Asking this question on Stack Overflow was exactly the right thing to do. When you do that, please wait more than 30 minutes before moving to this issue tracker. Both I and @sigmavirus24 regularly look at the Python-Requests tag on Stack Overflow, so you'd have got our attention before long. Asking the question both there and here is unnecessary duplication.

Secondly, the question 'is this a known issue' is one you should be able to answer just as well as us. It is a known issue: issue #1745 tracked it. A [very simple GitHub search](https://github.com/kennethreitz/requests/search?o=desc&q=cookie+AttributeError&s=created&type=Issues) could have shown you this issue quite quickly. I appreciate that it's not always obvious, but if everyone took five minutes to browse some old related issues I'd save hours of my life over the long term.
",Lukasa,sigmavirus24
1766,2013-11-29 16:36:47,"#1765 was already a PR, why did you open this one? We could have added tests after the fact. Can you rebase this to work off of #1765 so that @bicycle1885 gets some credit?
",sigmavirus24,bicycle1885
1764,2013-11-28 18:36:29,"There are a few nasty bugs that we've fixed since the last release, including the broken SNI support. It'd be good to get another release out. I think a few things have changed dramatically enough that this is probably a minor release (e.g. takes us to 2.1.0). Thoughts? /cc @kennethreitz @sigmavirus24

Still to do:
- [ ] Merge outstanding PRs (definitely ~~#1713, #1657, #1768, and #1766~~; maybe #1729, #1628 and #1743 depending on how @kennethreitz feels).
- [ ] Update changelog to match those PRs.

If nothing else Runscope could do with this release, so it'd be nice to be in good shape.
",Lukasa,kennethreitz
1764,2013-11-28 18:36:29,"There are a few nasty bugs that we've fixed since the last release, including the broken SNI support. It'd be good to get another release out. I think a few things have changed dramatically enough that this is probably a minor release (e.g. takes us to 2.1.0). Thoughts? /cc @kennethreitz @sigmavirus24

Still to do:
- [ ] Merge outstanding PRs (definitely ~~#1713, #1657, #1768, and #1766~~; maybe #1729, #1628 and #1743 depending on how @kennethreitz feels).
- [ ] Update changelog to match those PRs.

If nothing else Runscope could do with this release, so it'd be nice to be in good shape.
",Lukasa,sigmavirus24
1764,2013-11-29 16:59:48,"Looking over the PRs listed above now:
- #1713 :+1: merge it
- #1657 :+1: merge it
- #1729 :-1: do not merge it (I don't fee it is ready yet. @Lukasa and I left feedback that hasn't been addressed)
- #1628 :-1: do not merge it (neither @kennethreitz nor I are very fond of this)
- #1766 :+1: merge it
- #1743 +0 I don't see anything awful about allowing for separate timeouts, it is just the API under question. I've proposed a different way of handling the same feature. I'm not sure this _has_ to be in 2.1 though 
",sigmavirus24,kennethreitz
1764,2013-12-04 01:49:40,"@kennethreitz this PR wasn't finished. There's still stuff missing from the Changelog
",sigmavirus24,kennethreitz
1752,2013-11-22 07:51:22,"This is effecting Pip pretty aggressively. 

Also being tracked by PyPy: https://bugs.pypy.org/issue1645

/cc @Lukasa @sigmavirus24 
",kennethreitz,Lukasa
1752,2013-11-22 07:51:22,"This is effecting Pip pretty aggressively. 

Also being tracked by PyPy: https://bugs.pypy.org/issue1645

/cc @Lukasa @sigmavirus24 
",kennethreitz,sigmavirus24
1744,2013-11-18 16:25:03,"When will the fix be available ?

From: Cory Benfield [mailto:notifications@github.com]
Sent: 18 November 2013 15:47
To: kennethreitz/requests
Cc: Adrian Upton
Subject: Re: [requests] Upgrade to 2.0.1 breaks existing code wherever cookies are passed as keyword arguments. (#1744)

@auworksharehttps://github.com/auworkshare If you're using cookies you got from a successful login, you should really just be using a Session to persist your state for you. I'm assuming you're just taking the cookies from the Response object, which are in a CookieJar. This means that @sigmavirus24https://github.com/sigmavirus24 is correct, this is a duplicate of #1711https://github.com/kennethreitz/requests/issues/1711.

—
Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/issues/1744#issuecomment-28708654.
",auworkshare,auworksharehttps
1744,2013-11-18 16:25:03,"When will the fix be available ?

From: Cory Benfield [mailto:notifications@github.com]
Sent: 18 November 2013 15:47
To: kennethreitz/requests
Cc: Adrian Upton
Subject: Re: [requests] Upgrade to 2.0.1 breaks existing code wherever cookies are passed as keyword arguments. (#1744)

@auworksharehttps://github.com/auworkshare If you're using cookies you got from a successful login, you should really just be using a Session to persist your state for you. I'm assuming you're just taking the cookies from the Response object, which are in a CookieJar. This means that @sigmavirus24https://github.com/sigmavirus24 is correct, this is a duplicate of #1711https://github.com/kennethreitz/requests/issues/1711.

—
Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/issues/1744#issuecomment-28708654.
",auworkshare,sigmavirus24https
1743,2013-11-18 08:57:00,"I'm 100% sitting on the fence here. This is an API change, so we need to see if @kennethreitz likes it or not. =)
",Lukasa,kennethreitz
1733,2013-11-09 22:50:42,"Looks fine in principle to me, but I'd want @sigmavirus24 to take a look. =)
",Lukasa,sigmavirus24
1732,2013-11-09 08:57:36,"`urllib3` had a bug, we accidentally pulled it in, we didn't catch it because we don't test with SNI support (should we have that as a separate build?). Happily the awesome @t-8ch has already provided us with a patch, which is already merged, so we don't necessarily have an action on this: it'll be quietly fixed in the next release. Just raising this issue so that people who look on the issue tracker see it.
",Lukasa,t-8ch
1727,2013-11-10 09:35:43,"@sigmavirus24 I really want to add a regression test for this, but we have essentially got no testing infrasrtucture for  proxies. Do you have any bright ideas/will Betamax help with this, or do I need to look into spinning up an open HTTP proxy?
",Lukasa,sigmavirus24
1723,2013-11-04 09:33:02,"Thanks for raising this issue! It looks like @daftshady is already taking a pass at fixing this, so let's see what gets produced. =)
",Lukasa,daftshady
1723,2013-11-04 16:05:44,"@hwkns Yes, something has changed. Specifically, we merged #1338. This added the `to_native_string` method that @daftshady is using in the fix.

Previously, we'd have had to special-case this parameter, which seemed insane to me, especially as it's very easy to simply not use it (e.g. call requests.get or requests.post). However, we can now treat this parameter the way we treat header keys, which is to say that people who use this library have all sorts of ways of handling their internal strings, so we should just fix it up as best we can.

In your particular case I think you're overthinking the issue. If the method name can't come from outside your application (and I hope to god it can't), then there's no reason for you not to just use native strings. You gain nothing by forcing them to unicode, and your code will be cleaner. However, I'm still +1 on taking a fix for this, because it's now so easy to do.
",Lukasa,daftshady
1717,2013-10-30 21:17:56,"Yeah, just a transient problem. As discussed previously, I'm generally happy with this kind of change. It's up to @sigmavirus24 and @kennethreitz to agree/disagree with this.
",Lukasa,kennethreitz
1717,2013-10-30 21:17:56,"Yeah, just a transient problem. As discussed previously, I'm generally happy with this kind of change. It's up to @sigmavirus24 and @kennethreitz to agree/disagree with this.
",Lukasa,sigmavirus24
1717,2013-10-31 00:29:12,"It seems @kennethreitz has warmed to the idea so I'll withdraw my objections once the tests are made a bit better. You should check to make sure the url isn't changed in the future.
",sigmavirus24,kennethreitz
1713,2013-10-30 00:41:20,"I thought @gazpachoking or I had commented the old way of merging these fairly explicitly to talk about accepting a CookieJar. I just can't seem to find what I remember writing. Something like `merge_cookies` or `merge_session_cookies`. 
",sigmavirus24,gazpachoking
1710,2013-10-29 11:05:21,"Closed and reopened to get a rebuild.

The code changes here look totally fine to me, so in principle this is acceptable to merge. I'm also in principle happy with this change: it makes using unusual protocols in Transport Adapters not too big a problem.

However, it's a significant enough diversion that I want to hear from @sigmavirus24 and @kennethreitz first. =)
",Lukasa,kennethreitz
1710,2013-10-29 11:05:21,"Closed and reopened to get a rebuild.

The code changes here look totally fine to me, so in principle this is acceptable to merge. I'm also in principle happy with this change: it makes using unusual protocols in Transport Adapters not too big a problem.

However, it's a significant enough diversion that I want to hear from @sigmavirus24 and @kennethreitz first. =)
",Lukasa,sigmavirus24
1709,2013-10-28 17:21:17,"I am strongly +0 on this. Requests does not support Google App Engine, and there has never been any expectation that it will function correctly on GAE. I'm not opposed to this change, because it's minor and should have no functional effect, but I equally do not want to begin to establish a tradition of establishing special-cases for GAE.

I'll let @sigmavirus24 and @kennethreitz provide their opinions instead.
",Lukasa,kennethreitz
1709,2013-10-28 17:21:17,"I am strongly +0 on this. Requests does not support Google App Engine, and there has never been any expectation that it will function correctly on GAE. I'm not opposed to this change, because it's minor and should have no functional effect, but I equally do not want to begin to establish a tradition of establishing special-cases for GAE.

I'll let @sigmavirus24 and @kennethreitz provide their opinions instead.
",Lukasa,sigmavirus24
1705,2013-10-26 08:21:55,"Thanks for saving us again @shazow and @t-8ch.

Also, cross repo automatic issue closing? Very cool.
",Lukasa,shazow
1705,2013-10-26 08:21:55,"Thanks for saving us again @shazow and @t-8ch.

Also, cross repo automatic issue closing? Very cool.
",Lukasa,t-8ch
1702,2013-10-24 20:24:11,"In response to some of the discussion in #1698. Looking for review from @sigmavirus24. =)
",Lukasa,sigmavirus24
1700,2013-10-24 14:01:45,"TODO: 
- [x] get :+1: from @lukasa and @sigmavirus24 
- [x] update changelog

Fixes #1659
",kennethreitz,lukasa
1700,2013-10-24 14:01:45,"TODO: 
- [x] get :+1: from @lukasa and @sigmavirus24 
- [x] update changelog

Fixes #1659
",kennethreitz,sigmavirus24
1700,2013-10-24 14:02:41,"/cc @Lukasa @sigmavirus24 
",kennethreitz,Lukasa
1700,2013-10-24 14:02:41,"/cc @Lukasa @sigmavirus24 
",kennethreitz,sigmavirus24
1696,2013-10-22 20:51:27,"First contribution, half expecting to see that I screwed something up. Thanks a lot @Lukasa for your assistance in IRC.
",canibanoglu,Lukasa
1694,2013-10-20 15:24:17,"Looks like it's 404ing. @kennethreitz?
",Lukasa,kennethreitz
1693,2013-10-20 17:26:08,"I might go through and update all of these to use `#format` instead in the library. It is the ""future"" after all. Any objections @Lukasa @kennethreitz ?
",sigmavirus24,kennethreitz
1690,2013-10-19 01:34:06,"There are no plans for such and I believe it is currently completely untenable. There is no support like this in any other library like requests that I know of and the place to add anything like this would be in shazow/urllib3. I'm not even entirely certain it would be possible to do in urllib3, especially since I find it hard to believe this is even possible with httplib (which urllib3 uses).

I'll leave this open so @Lukasa and @kennethreitz can comment, but not only am I :-1: on the idea in general, I'm also pretty sure it is not technically possible at the moment.
",sigmavirus24,kennethreitz
1690,2013-10-19 01:34:06,"There are no plans for such and I believe it is currently completely untenable. There is no support like this in any other library like requests that I know of and the place to add anything like this would be in shazow/urllib3. I'm not even entirely certain it would be possible to do in urllib3, especially since I find it hard to believe this is even possible with httplib (which urllib3 uses).

I'll leave this open so @Lukasa and @kennethreitz can comment, but not only am I :-1: on the idea in general, I'm also pretty sure it is not technically possible at the moment.
",sigmavirus24,Lukasa
1685,2014-11-07 13:35:55,"@stas I want to address one thing:

> Requests users should be aware of those methods, because in most of the cases the methods are not called implicitly.

Leaving PyPy aside for a moment, those methods shouldn't _need_ to be called explicitly. If the socket objects become unreachable in CPython they will get auto gc'd, which includes closing the file handles. This is not an argument to not-document those methods, but it is a warning to not focus overmuch on them.

We are meant to use a CI, but it appears to be unwell at the moment, and only @kennethreitz is in a position to fix it. He'll get to it when he has time. Note, however, that benchmark tests are extremely difficult to get right in a way that doesn't make them extremely noisy.
",Lukasa,kennethreitz
1685,2014-11-08 16:56:23,"Alright, so the problem explicitly appears to be with the way we handle memory interacting with the PyPy JIT. It might be a good idea to summon in a PyPy expert: @alex?
",Lukasa,alex
1674,2013-10-20 16:42:06,"@jkatzer can you tell us what `self.encoding` is when encountering this issue? Furthermore, can you tell us what `guess_json_utf(self.content)` returns? Something @mjpieters or @sburns contributed seems to be causing the issue here.

The stack trace seems to imply that `self.encoding` is `None` or some other falsey value (e.g., `''`) so we use `guess_json_utf` with `self.content`. `self.content` at that point is the raw bytes object we get from urllib3. So we use `self.content.decode(encoding)` which seems to be what's causing this issue. Judging by the stack trace (again) it seems that `guess_json_utf` is returning `utf8`.

One other note is that on requests master (on python 2.7), when I use `r.json()` the title of this issue comes back replaced like so: `u'""\u010d"" - UTF-8 UnicodeDecodeError'` which if I remember correctly is how the stdlib replaces errors and is a consequence of us always using `errors='replace'`. This suggests that the call to `str.decode` on line 692 needs an `errors='replace'` parameter passed in since that's what we do for `self.text`. 

Objections? I feel like using that particular option is a bad idea but we'd break the API were we to change it now.
",sigmavirus24,sburns
1674,2013-10-20 16:42:06,"@jkatzer can you tell us what `self.encoding` is when encountering this issue? Furthermore, can you tell us what `guess_json_utf(self.content)` returns? Something @mjpieters or @sburns contributed seems to be causing the issue here.

The stack trace seems to imply that `self.encoding` is `None` or some other falsey value (e.g., `''`) so we use `guess_json_utf` with `self.content`. `self.content` at that point is the raw bytes object we get from urllib3. So we use `self.content.decode(encoding)` which seems to be what's causing this issue. Judging by the stack trace (again) it seems that `guess_json_utf` is returning `utf8`.

One other note is that on requests master (on python 2.7), when I use `r.json()` the title of this issue comes back replaced like so: `u'""\u010d"" - UTF-8 UnicodeDecodeError'` which if I remember correctly is how the stdlib replaces errors and is a consequence of us always using `errors='replace'`. This suggests that the call to `str.decode` on line 692 needs an `errors='replace'` parameter passed in since that's what we do for `self.text`. 

Objections? I feel like using that particular option is a bad idea but we'd break the API were we to change it now.
",sigmavirus24,mjpieters
1674,2013-10-21 07:44:13,"It _used_ to fall back to using `self.text` if decoding failed, but [@kennethreitz removed that at some point](https://github.com/kennethreitz/requests/commit/1451ba0c6d395c41f86da35036fa361c3a41bc90), without explanation. `guess_json_utf` assumes that the content is correctly encoded to _a_ UTF codec, and the exception handling would handle the edge cases where a non-RFC-compliant JSON response is to be handled.
",mjpieters,kennethreitz
1671,2013-10-21 10:12:15,"@sigmavirus24 Should we be filing this under our long list of ""Do we trust the Host: header"" bugs?
",Lukasa,sigmavirus24
1669,2013-10-13 08:59:50,"Following #1642, I think we should treat urllib3's pooled connections better when we manage them ourselves. This PR provides the following extra functionality:
- Clean up connections when we hit problems during chunked upload, rather than leaking them.
- Return connections to the pool when a chunked upload is successful, rather than leaking it.

The diff is a little unclear, so let me clarify: I wrapped most of the chunked code in a `try` block, then added a catch-everything exception handler that will attempt to close the connection if we hit it. Otherwise, we'll return the presumably-fine connection to the pool.

I'd like some code review on this. @sigmavirus24, can you confirm this is doing what I think it is, and can you give me some suggestions for how best to test it? @shazow, if you have time, can you confirm that I'm obeying the semantics of the urllib3 connection pool? (e.g. should I check if we get a `Connection: close` header on the response?
",Lukasa,shazow
1669,2013-10-13 08:59:50,"Following #1642, I think we should treat urllib3's pooled connections better when we manage them ourselves. This PR provides the following extra functionality:
- Clean up connections when we hit problems during chunked upload, rather than leaking them.
- Return connections to the pool when a chunked upload is successful, rather than leaking it.

The diff is a little unclear, so let me clarify: I wrapped most of the chunked code in a `try` block, then added a catch-everything exception handler that will attempt to close the connection if we hit it. Otherwise, we'll return the presumably-fine connection to the pool.

I'd like some code review on this. @sigmavirus24, can you confirm this is doing what I think it is, and can you give me some suggestions for how best to test it? @shazow, if you have time, can you confirm that I'm obeying the semantics of the urllib3 connection pool? (e.g. should I check if we get a `Connection: close` header on the response?
",Lukasa,sigmavirus24
1669,2013-10-13 10:27:12,"@lukasa For sure. I recall some streaming-related issues open for urllib3 but I'm on superbad internet right now so I can't look for them. Just take a quick look for them before you dive in, then have at it. :)
",shazow,lukasa
1668,2013-10-13 01:30:49,"

In trying to download a large file, the above code exposes two apparent problems:
- Regardless of the read size, the CPU gets pegged
- Periodically one of the streams closes early

Reported for @zedshaw since he's apparently blocked from the repo.
",sigmavirus24,zedshaw
1668,2013-10-14 03:01:49,"@zedshaw if you can see this and can provide more info, please email me. I hope we can figure out who blocked you and why.
",sigmavirus24,zedshaw
1664,2013-10-11 14:11:44,"Hi there! Thanks for raising this issue!

Firstly, you've opened this question in two places, here and Stack Overflow. Stack Overflow was the correct location for this question, you didn't need to open it here. =)

Secondly, this is not actually a direct Requests question, it's a question about [requests-futures](https://github.com/ross/requests-futures), which is a separate project managed by the awesome @ross. In particular, vanilla Requests doesn't provide any form of background callback.

However, for standard Requests, you can simple continue to use the same `Session` object. `Session` performs connection pooling, so as long as the previous connection is still open (the server's choice) we'll re-use it.
",Lukasa,ross
1663,2013-11-11 14:12:13,"Looks like the change where @daftshady wasn't expecting to be receiving CookieJar objects instead of dictionaries. There's a fix for this somewhere and should be in 2.0.2 then again, I can't be certain without confirmation from @dmakhno as to what `self.context.cookies` actually is.
",sigmavirus24,daftshady
1662,2013-10-10 22:14:33,"Taking a page out of @mattspitz's book.

https://github.com/kennethreitz/requests/pull/1439
# dreamsdocometrue!
",voberoi,mattspitz
1662,2013-10-10 22:44:20,"I should add, this is partly in jest since I know @mattspitz.

My contribution is probably not as substantial as others', in which case #dreamsdonotcometrueaseasilyasihoped.
",voberoi,mattspitz
1659,2013-10-24 10:57:58,"/cc @dstufft
",kennethreitz,dstufft
1659,2013-10-24 11:42:22,"@agl, any input you can provide would be much appreciated!
",kennethreitz,agl
1659,2013-10-24 12:07:18,"Tentative new cert is available here:

http://ci.kennethreitz.org/job/ca-bundle/lastSuccessfulBuild/artifact/certs.pem

+1 from @dstufft, @agl, or @tiran would be appreciated :)

Once I receive a +1, a new release will be cut.
",kennethreitz,tiran
1659,2013-10-24 12:07:18,"Tentative new cert is available here:

http://ci.kennethreitz.org/job/ca-bundle/lastSuccessfulBuild/artifact/certs.pem

+1 from @dstufft, @agl, or @tiran would be appreciated :)

Once I receive a +1, a new release will be cut.
",kennethreitz,agl
1657,2013-10-08 01:01:47,"@Lukasa I assigned this to you for code review, but don't spend too much time on it yet. I've got work to do.
",sigmavirus24,Lukasa
1657,2013-11-27 14:41:57,"@Lukasa @kennethreitz can we get this merged soon? I just realized I'll probably need this for github3.py so that I don't have to do [this](https://github.com/sigmavirus24/github3.py/commit/9a1b4d892a31bd12ee477f39caada76670cfc062#diff-8a0b7651f0a27fb4490e3f2d87206eedR67) ;)
",sigmavirus24,kennethreitz
1654,2013-10-05 14:49:11,"@untitaker I was not objecting to the changes themselves. I have far more to maintain than just requests and I do far more offline than anyone really knows or cares to know about. @kennethreitz has an ostensibly similar schedule (which is why he has minions) and having exact language to review in an email makes our lives far easier. Stressing that point with @riyadparvez will only make future pull requests to this and other projects on his behalf better and perchance allow them to be merged in a quicker fashion.
",sigmavirus24,kennethreitz
1652,2013-10-04 13:05:54,"This looks fine to me. You could potentially just use `if` instead of `elif` here: might be slightly cleaner. @sigmavirus24, thoughts?
",Lukasa,sigmavirus24
1648,2013-11-17 20:14:55,"> There's a difference between servers not understanding a chunked Transfer-Encoding and servers not wanting to respect it. I suspect the latter is the case in @bryanhelmig's case. Alternatively, the application could have been written by someone who doesn't understand or know about Transfer-Encoding and so requires a Content-Length.

I actually think this is very likely the case in most of my examples. We (@zapier) attempt to upload files to over a dozen different APIs and the few of them that require Content-Length (seem to) timeout with chunked Transfer-Encoding.

> As a side note, the fact that many servers don't understand Transfer-Encoding is just a wild assertion based on no evidence that I've seen. Until any evidence is provided, I'm choosing to ignore it.

I could put together a test suite of how various services respond to Content-Length/Transfer-Encoding, but I feel like even if it is incorrectly implemented APIs, that shouldn't really advise the design of python-requests. Easier still, I could just name names based on my experience fighting this for the last week, but again, if they are API/server bugs, what is the use of such information to python-requests?

> I suggest that we instead trigger a warning and then do the right thing in certain well documented situations.

Agree on default behavior, but sometimes reality trumps the ""right thing"" (especially when you have no control over a potentially broken server). It might be nice to document a technique to override (even if it advocates for the user to do a lot of work, like writing a custom Adapter).
",bryanhelmig,zapier
1647,2013-10-04 10:21:17,"Ah, yes, ok. The problem is that we pass the full URL to `proxy_from_url`. This ends up calling `urllib3.util.parse_url` which is a bit stupid when it comes to splitting the path.

@shazow Do you want to make `proxy_from_url` better, or do you want Requests to try to sanitise the URL it sends to you?
",Lukasa,shazow
1646,2013-10-03 19:57:03,"Definitely still a urllib3 bug. =) @shazow is playing silly buggers, I think. I've put a reproducible script in the urllib3 issue. So you don't feel like you're being messed around I'm going to leave this issue open, but I'm convinced it's upstream.
",Lukasa,shazow
1640,2013-10-01 00:57:09,"This would have to wait for 3.0 since it's a significant API change unless @kennethreitz or @Lukasa disagree.
",sigmavirus24,kennethreitz
1640,2013-10-01 00:57:09,"This would have to wait for 3.0 since it's a significant API change unless @kennethreitz or @Lukasa disagree.
",sigmavirus24,Lukasa
1640,2013-10-01 08:11:00,"This in principle looks great @abarnert, thanks so much!

We wouldn't need to wait until 3.0, because this change isn't backwards incompatible: adding the ability to pass a 4-tuple can be done in a minor release (e.g. 2.1.0) according to [semver](http://semver.org/).

The real question is whether we think this API extension is the right way to handle it. As you pointed out in #1640, we already poorly document this corner of the API so that definitely has to be fixed.

@kennethreitz: Are you happy with this extension to the API, or would you like to reconsider the multipart file API entirely?
",Lukasa,kennethreitz
1638,2013-10-01 01:57:29,"This is default python `cookielib` behaviour:



This is not an issue in requests. Perhaps we did something different with the Host header previously and possibly ignored it, but that version is so ancient that I don't see this as a real issue. If @kennethreitz and @Lukasa agree they can close this issue.
",sigmavirus24,kennethreitz
1638,2013-10-01 01:57:29,"This is default python `cookielib` behaviour:



This is not an issue in requests. Perhaps we did something different with the Host header previously and possibly ignored it, but that version is so ancient that I don't see this as a real issue. If @kennethreitz and @Lukasa agree they can close this issue.
",sigmavirus24,Lukasa
1636,2013-09-28 14:50:19,"This should resolve the problem from #1631.

@atomontage, can you try testing with this version of Requests to confirm that it resolves your problem?
",Lukasa,atomontage
1634,2013-09-28 15:02:05,"I'm -1 on this personally but @Lukasa and @kennethreitz might have different opinions.
",sigmavirus24,kennethreitz
1634,2013-09-28 15:02:05,"I'm -1 on this personally but @Lukasa and @kennethreitz might have different opinions.
",sigmavirus24,Lukasa
1632,2013-09-28 15:01:13,"Further more @matt-hickford in order to use requests for local files you must define your own file adapter. We have been asked for it several times and I believe we all three maintain the opinion that trying to use requests for local files makes absolutely no sense.
",sigmavirus24,matt-hickford
1628,2013-09-26 12:22:29,"@jkbr Agreed, but that falls into the problem I suggested: if that host does a relative redirect this change will wipe your previous Host header.
",Lukasa,jkbr
1627,2013-09-26 07:52:25,"Thanks for this!

I'm not in the best place to review this, having not contributed to this section of our codebase, but I can point out a few things. @sigmavirus24 would do a better job of reviewing this change.

Firstly, according to RFC 2617, ['If the ""algorithm"" directive's value is ""MD5-sess"", then A1 is calculated only once - on the first request by the client following receipt of a WWW-Authenticate challenge from the server.  It uses the server nonce from that challenge...'](http://tools.ietf.org/html/rfc2617#page-13). This is something that is very difficult for our current authentication handler system to do, but appears to mean that we are severely limited in our ability to properly execute 'MD5-sess'.

Secondly, the `build_digest_header` method is now _massive_ and quite difficult to follow. I think we should be aiming to refactor this method, either as part of this pull-request or more generally.
",Lukasa,sigmavirus24
1625,2013-09-25 14:33:23,"It adds the kbutton to clone requests repository into Koding easily. You can find the details in http://kbutton.org

Thank you so much! :)
github: @f
twitter: @fkadev
Fatih Kadir Akin
",fka,f
1625,2013-09-25 14:33:23,"It adds the kbutton to clone requests repository into Koding easily. You can find the details in http://kbutton.org

Thank you so much! :)
github: @f
twitter: @fkadev
Fatih Kadir Akin
",fka,fkadev
1625,2013-09-25 14:49:09,"I'm totally +0 on this, it's up to @kennethreitz. =)
",Lukasa,kennethreitz
1622,2013-09-25 07:48:23,"This is a good catch, and thanks so much for the bug report. The problem is almost certainly in the underlying urllib3 implementation of the proxy behaviour though. Let's not raise an issue there just yet until we can isolate the problem, but I think the fix will have to be in that library.

Hey @schlamar, can you take a look at this?
",Lukasa,schlamar
1622,2013-10-11 11:08:20,"@schlamar Sorry I let this slip past me. The answer there is 'maybe'. @t-8ch was one of the people strongly pushing for explicit proxy schemes: do you have an opinion?
",Lukasa,t-8ch
1621,2013-09-25 01:42:32,"In a similar vein to the migrating to 1.x docs, this section details changes that can break existing code in order to ease the migration to 2.x. I didn't talk about any of the added APIs since those don't really break existing code. Compared with the last major release, there's not much to say.

This is largely based on @Lukasa's [blog post](http://lukasa.co.uk/2013/09/Requests_20/) and the [changelog](https://github.com/kennethreitz/requests/blob/master/HISTORY.rst). I did not know how folks feel about linking to a blog post from the docs so I didn't add the link.
",davidfischer,Lukasa
1618,2013-09-24 11:21:37,"Thanks for raising this issue!

I seem to recall we discussed this in the past, and decided that what we do here is a user-level issue. Some users may absolutely want their authentication headers to redirect off-host. Does that sound right to you @kennethreitz?
",Lukasa,kennethreitz
1617,2013-09-24 02:45:49,"@laruellef yeah I was just about to go looking for that issue. It almost certainly seems related to #1577. We're still waiting on the urllib3 PR that @Lukasa alluded to in that issue so I'm going to close this while we wait for that to work itself out so we can provide that interface to you.

That said, if you run into timeout issues again, and setting it lower doesn't resolve your issue, then open an issue. Otherwise it might be safe to assume it's related to #1577.
",sigmavirus24,Lukasa
1617,2013-09-24 02:52:47,"oh, but wait,
the proposed work around is not working here... :-(
ie
https://github.com/kennethreitz/requests/issues/1577#issuecomment-23794576

Fred~

On Mon, Sep 23, 2013 at 7:46 PM, Ian Cordasco notifications@github.comwrote:

> @laruellef https://github.com/laruellef yeah I was just about to go
> looking for that issue. It almost certainly seems related to #1577https://github.com/kennethreitz/requests/issues/1577.
> We're still waiting on the urllib3 PR that @Lukasahttps://github.com/Lukasaalluded to in that issue so I'm going to close this while we wait for that
> to work itself out so we can provide that interface to you.
> 
> That said, if you run into timeout issues again, and setting it lower
> doesn't resolve your issue, then open an issue. Otherwise it might be safe
> to assume it's related to #1577https://github.com/kennethreitz/requests/issues/1577
> .
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/issues/1617#issuecomment-24971338
> .
",laruellef,Lukasahttps
1615,2013-09-21 13:13:43,"Thanks for raising this issue!

I don't think I agree with your assessment of what the 'expected behaviour' of this should be. You've set a timeout of zero seconds. That implies that the connection process should time out after zero seconds, e.g. immediately.

I'm a bit mixed here. On the one hand, we're doing exactly what the code asked for. On the other hand, I'd argue that falsy values for timeout should be treated the same as `None`. @sigmavirus24, thoughts?
",Lukasa,sigmavirus24
1612,2013-09-24 02:21:19,"@sanmayaj do I understand correctly that you expect requests to string-ify everything you send in? Why can you not do this yourself on your inputs?

> Be liberal in what you receive and conservative in what you send.

You should be assuming that everything is a string (or byte) that leaves your computer via requests. This includes headers too. Personally I'm -1 on this. Up to @kennethreitz and @Lukasa though
",sigmavirus24,kennethreitz
1612,2013-09-24 02:21:19,"@sanmayaj do I understand correctly that you expect requests to string-ify everything you send in? Why can you not do this yourself on your inputs?

> Be liberal in what you receive and conservative in what you send.

You should be assuming that everything is a string (or byte) that leaves your computer via requests. This includes headers too. Personally I'm -1 on this. Up to @kennethreitz and @Lukasa though
",sigmavirus24,Lukasa
1606,2013-09-17 07:53:39,"Hi @borfig! Thanks for doing this work!

I have no idea if this'll get accepted or not. We're very reluctant to add new things to the Requests functional API at this stage, so this boils down to whether or not @kennethreitz believes this is a feature worth exposing at that level. I simply don't know what he'll decide. =)

Regardless of what Kenneth decides, the patch itself appears to be in good shape, so if he wants the feature this patch is a good solid implementation of it.
",Lukasa,kennethreitz
1606,2013-09-17 08:30:06,"Hi, you are welcome :)

I have submitted the relevant urllib3 patch (shazow/urllib3@dfd9e0dc7ecbe4bb7abe723b5fd0e01688096d02) 3 months ago, but I still prefer to use requests.

The use-case of this feature is for testing purposes:
this allows us (@ravello) to test exact replicas of our production HTTPS service, without creating and signing certificates for each replica.
",borfig,ravello
1602,2013-09-14 03:34:06,"As a side note: the Travis builds will continue to fail on this branch until someone (:eyes: @kennethreitz) deploys the latest version of httpbin. :wink:
",sigmavirus24,kennethreitz
1598,2013-09-13 13:13:55,"Turns out we were doing this wrong. Kindly pointed out by @redspider. Bizarrely, we're half doing this right, but just didn't fix the whole thing up. This should. =D
",Lukasa,redspider
1589,2013-09-11 14:45:08,"I actually need to backtrack on the JSON encoding: we have logic for it in `Response.json()`. On balance, I think that belongs there unless we add more specific behaviour to `Response.text`, in which case it should be moved. The relevant funky details are that you can use any of the UTF- encodings, I think. Don't hold me to that though.
1. You and I disagree on the definition of 'information that is present'. =) I find RFC 2616's statement to be clear and unambiguous: if you serve, for example, `text/plain` with no charset specified, the headers actually do say to use ISO-8859-1. This is because RFC 2616 defines what they say, and that definition clearly states that `text/X` means ISO-8859-1 unless explicitly overridden in the `Content-Type` header.
   
   We should note here that I'm not arguing that HTML has a default of ISO-8859-1, I'm saying that _any_ MIME type beginning `text/` should have that default. HTML is a notable example but it's far from the only one.
   
   As for what the default should be in a given context, that context is defined by the `Content-Type` header, which is what we're examining anyway. No big deal. =)
2. Re: My sample workflow above. I believe it to be correct(-ish) within the parameters of the problem. The only time that it isn't correct is if the headers actually _do_ specify ISO-8859-1. I didn't handle that case because I wrote the code in 10 seconds. =)

My proposal is to add the following logic (in Python, but not directly related to any part of the Requests code):



Does this seem like a sensible set of logic to you?

Final note: I can't guarantee that a pull request that I'm happy with will get incorporated. Requests is ""one man one vote"": Kenneth is the man, he has the vote. I'm already tempted to say that the entire discussion above is an overreach, and that Kenneth will believe that Requests simply should stop caring about `Content-Type` beyond whether it has a `charset` value (that is, removing the ISO-8859-1 default and not replacing it with anything else).

In fact, let's pose him that exact question (there's no way he has time to read the entire discussion above). I'll also get Ian's opinion:

**BDFL Question:**
@kennethreitz: Currently Requests will use `ISO-8859-1` as the default encoding for anything with `Content-Type` set to `text/<something>` and without a `charset` declaration (as specified by RFC 2616). This can cause problems with HTML that uses `<meta>` tags to declare a non-ISO-8859-1 encoding. We can do one of three things:
1. Be smarter. If the `Content-Type` is `text/html` or one of a similar set of families, we can search for a relevant `<meta>` tag.
2. Stay the same.
3. Remove the ISO-8859-1 default and stop pretending we know about `Content-Type`s at all.

Preferences? @sigmavirus24, I'd like your opinion too. =)
",Lukasa,kennethreitz
1589,2013-09-11 14:45:08,"I actually need to backtrack on the JSON encoding: we have logic for it in `Response.json()`. On balance, I think that belongs there unless we add more specific behaviour to `Response.text`, in which case it should be moved. The relevant funky details are that you can use any of the UTF- encodings, I think. Don't hold me to that though.
1. You and I disagree on the definition of 'information that is present'. =) I find RFC 2616's statement to be clear and unambiguous: if you serve, for example, `text/plain` with no charset specified, the headers actually do say to use ISO-8859-1. This is because RFC 2616 defines what they say, and that definition clearly states that `text/X` means ISO-8859-1 unless explicitly overridden in the `Content-Type` header.
   
   We should note here that I'm not arguing that HTML has a default of ISO-8859-1, I'm saying that _any_ MIME type beginning `text/` should have that default. HTML is a notable example but it's far from the only one.
   
   As for what the default should be in a given context, that context is defined by the `Content-Type` header, which is what we're examining anyway. No big deal. =)
2. Re: My sample workflow above. I believe it to be correct(-ish) within the parameters of the problem. The only time that it isn't correct is if the headers actually _do_ specify ISO-8859-1. I didn't handle that case because I wrote the code in 10 seconds. =)

My proposal is to add the following logic (in Python, but not directly related to any part of the Requests code):



Does this seem like a sensible set of logic to you?

Final note: I can't guarantee that a pull request that I'm happy with will get incorporated. Requests is ""one man one vote"": Kenneth is the man, he has the vote. I'm already tempted to say that the entire discussion above is an overreach, and that Kenneth will believe that Requests simply should stop caring about `Content-Type` beyond whether it has a `charset` value (that is, removing the ISO-8859-1 default and not replacing it with anything else).

In fact, let's pose him that exact question (there's no way he has time to read the entire discussion above). I'll also get Ian's opinion:

**BDFL Question:**
@kennethreitz: Currently Requests will use `ISO-8859-1` as the default encoding for anything with `Content-Type` set to `text/<something>` and without a `charset` declaration (as specified by RFC 2616). This can cause problems with HTML that uses `<meta>` tags to declare a non-ISO-8859-1 encoding. We can do one of three things:
1. Be smarter. If the `Content-Type` is `text/html` or one of a similar set of families, we can search for a relevant `<meta>` tag.
2. Stay the same.
3. Remove the ISO-8859-1 default and stop pretending we know about `Content-Type`s at all.

Preferences? @sigmavirus24, I'd like your opinion too. =)
",Lukasa,sigmavirus24
1586,2013-09-09 15:02:00,"Thanks for raising this issue!

Quickly, before I dive into this: the correct solution when Requests is getting in your way like this is to use `Response.raw`. =) 

Before we get into discussing this feature request further, I'm interested to know how this problem bit you. We should only decompress the file if the file is served with a `Content-Encoding` header set to `gzip`. My best guess is that a `.tar.gz` file should be served with no `Content-Encoding` header and `Content-Type: x-gzip`. Do we know if this is a common behaviour with gzipped raw files, or if this was an unusual behaviour from your webserver? /cc @sigmavirus24 
",Lukasa,sigmavirus24
1584,2013-10-15 00:13:58,"@BernardoLima how is your question relevant to this issue? Your question should be asked on [StackOverflow](http://stackoverflow.com/questions/tagged/python-requests). I have a strong suspicion as to why your Post is not working. I'll answer you either on StackOverfow or by email (if you choose to email me privately). 
",sigmavirus24,BernardoLima
1582,2013-09-08 15:11:47,"This is a good idea @matt-hickford. I see three ways of doing this:
1. Use a `Proxy` class in the `Proxies` dictionary. Definitely do-able, but adds quite a bit of complication into the API. Possible. +0
2. Use proxy-type Auth handlers. Not good. -1
3. Use Transport Adapters.

I'm generally in favour of TAs. In particular, Transport Adapters are really the only thing that knows anything about HTTPS over proxy (the CONNECT verb), which needs to be differently handled from other kinds of messages. To that end, it seems more natural to provide proxy authentication solutions at the Transport Adapter level.

Potentially TAs could take pluggable auth modules, just like individual requests? /cc @sigmavirus24 
",Lukasa,matt-hickford
1582,2013-09-08 15:11:47,"This is a good idea @matt-hickford. I see three ways of doing this:
1. Use a `Proxy` class in the `Proxies` dictionary. Definitely do-able, but adds quite a bit of complication into the API. Possible. +0
2. Use proxy-type Auth handlers. Not good. -1
3. Use Transport Adapters.

I'm generally in favour of TAs. In particular, Transport Adapters are really the only thing that knows anything about HTTPS over proxy (the CONNECT verb), which needs to be differently handled from other kinds of messages. To that end, it seems more natural to provide proxy authentication solutions at the Transport Adapter level.

Potentially TAs could take pluggable auth modules, just like individual requests? /cc @sigmavirus24 
",Lukasa,sigmavirus24
1577,2013-09-05 08:16:44,"So, @laruellef, it looks like @kevinburke is doing some work on the `urllib3` side to add better timeout control. When that gets sorted we'll probably try to plumb it through to Requests. I think waiting for that issue (shazow/urllib3#231) to be resolved is the correct next step here.

Thanks for raising this, and keep track of the `urllib3` issue!
",Lukasa,kevinburke
1574,2013-09-03 15:03:50,"@ntx Thanks for raising this issue! I agree, this is a fairly graceless way to fail. I think the correct place for this fix is in [urllib3](https://github.com/shazow/urllib3), which implements our connection-layer logic. A simple exception handler should suffice over there. Can I recommend you open an issue on that repository?
",Lukasa,ntx
1573,2015-04-16 19:36:29,"I'm afraid that I don't know of any way. @reaperhulk?
",Lukasa,reaperhulk
1573,2016-02-24 07:36:00,"@botondus I think I found a simpler way to achieve this with request library. I am documenting this for other people who are facing the issue.

I assume that you have a .p12 certificate and a passphrase for the key.

### Generate certificate and private key.



Well, we are not done yet and we need to generate the key that doesn't require the PEM password every time it needs to talk to the server.

### Generate key without passphrase.



Now, you will have `certificate.pem` and `plainkey.pem`, both of the files required to talk to the API using requests.

Here is an example request using these cert and keys.



Hope this helps:

cc @kennethreitz @Lukasa @sigmavirus24 
",vinitkumar,kennethreitz
1568,2013-09-02 15:06:27,"Thanks for this @alekibango, and thanks for providing a PR!

It's worth noting that this was an issue we already knew about: see #1426. We work very closely with @shazow on his urllib3 project because we build Requests on top of it, and we regularly take newer versions of urllib3 with releases of Requests. I'm really glad you opened this issue, but in future it is worth quickly searching the GitHub issue tracker to see if someone else raised it before you. It would have saved you a little time! :smile:

Thanks again for this!
",Lukasa,shazow
1567,2013-09-02 08:05:34,"So I think this is an SSL problem, because I have no problems running this query on my Windows box. What OS are you using?

Also, let's ping @t-8ch for his expertise here. =)
",Lukasa,t-8ch
1565,2013-09-01 17:12:19,"Pointed out by @dstufft in #1560.
",Lukasa,dstufft
1564,2013-08-31 02:04:08,"Part of the rational for my argument against this is in the issue you provided. To quote ""Tim B"":

> Few mainstream browsers support gzip transfer-encoding.

requests never attempts to mimic anything that isn't in a browser and isn't very common. On the other hand, we also strive to mimic curl(/libcurl) and the fact that curl supports it (according to Tim B in the same comment) would be a point in your favor except that Tim then goes on to say:

> …however, there's no webserver that will serve this.

So I think it is safe to say that we can satisfy the largest number of our uses without it.

To not give you all of the details, however, would be dishonest of me. The Content-Encoding decompression occurs in `urrlib3` if I remember correctly. The correct place to request this would then be `urrlib3`. 

That said, I'm leaving this open so @Lukasa and @kennethreitz can feel free to correct me if I have mistated anything above.
",sigmavirus24,kennethreitz
1564,2013-08-31 02:04:08,"Part of the rational for my argument against this is in the issue you provided. To quote ""Tim B"":

> Few mainstream browsers support gzip transfer-encoding.

requests never attempts to mimic anything that isn't in a browser and isn't very common. On the other hand, we also strive to mimic curl(/libcurl) and the fact that curl supports it (according to Tim B in the same comment) would be a point in your favor except that Tim then goes on to say:

> …however, there's no webserver that will serve this.

So I think it is safe to say that we can satisfy the largest number of our uses without it.

To not give you all of the details, however, would be dishonest of me. The Content-Encoding decompression occurs in `urrlib3` if I remember correctly. The correct place to request this would then be `urrlib3`. 

That said, I'm leaving this open so @Lukasa and @kennethreitz can feel free to correct me if I have mistated anything above.
",sigmavirus24,Lukasa
1562,2013-08-31 01:30:19,"@Lionchj the issue is that by default a python package does not provide any way to access the documentation post installation. Distributions like Debian and Arch package our documentation in a way that it becomes accessible but by default we have no way of doing the same in Python. If the situation were different I would be 100% for this.

Thank you for considering this though. It was extremely thoughtful and maybe if you feel adventurous you could send a message to the Distutils mailing list about this. In the meantime I'm going to close this and you can feel free to re-open it when the tools support it in a meaningful way.

Also, please don't be discouraged to send other pull requests, we're more than happy to have them.
",sigmavirus24,Lionchj
1561,2013-08-30 15:10:18,"It does. See [L244 of urllib3/response.py](https://github.com/shazow/urllib3/blob/master/urllib3/response.py#L244). I'm not totally convinced this is a bug. For the purposes of the comment, the phrase ""the actual key"" should more correctly read ""the key that was added to the dictionary"". The CID is functioning as intended.

@sigmavirus24, I don't feel like urllib3 is doing the wrong thing here. Do you agree?
",Lukasa,sigmavirus24
1558,2013-08-28 11:34:51,"@ssbarnea To the best of my knowledge we have no plans to release any further 1.2.X point releases. The next planned release is 2.0.0. I'm potentially open to you making a patch against master, but I'd want to check with @kennethreitz first.
",Lukasa,kennethreitz
1558,2013-08-28 12:09:55,"Go for it



Kenneth Reitz

On Wed, Aug 28, 2013 at 7:34 AM, Cory Benfield notifications@github.com
wrote:

> ## @ssbarnea To the best of my knowledge we have no plans to release any further 1.2.X point releases. The next planned release is 2.0.0. I'm potentially open to you making a patch against master, but I'd want to check with @kennethreitz first.
> 
> Reply to this email directly or view it on GitHub:
> https://github.com/kennethreitz/requests/issues/1558#issuecomment-23407592
",kennethreitz,kennethreitz
1558,2013-09-01 18:16:13,"I'm not (quite frankly) convinced that a `Request` object should be entirely pickleable. It is an ephemeral object that we use to construct the `PreparedRequest`. `PreparedRequest` objects do not store a reference to it, so I'm 90% sure it does not need to be pickleable. We should probably make it pickleable anyway though it is a seriously minor introduction of code and it will cover us for the future.

I'm wondering if @shazow would be opposed to making the `urllib3` response object pickleable. If he were to approve, then I think it would make sense to also make the Response object pickleable. 
",sigmavirus24,shazow
1553,2013-08-25 06:16:06,"It would be nice if requests could support automatic proxy detection on Windows.

Both urllib2 and urllib accomplish this by reading windows registry via urllib.getproxies() on Windows and using _scproxy on Mac.

I opened an issue in [urllib3](https://github.com/shazow/urllib3/issues/232) but @shazow doesn't think it belongs there.
",oliverjanik,shazow
1550,2013-08-26 07:48:02,"That's true, it does. Still, I think it'd be better to set it higher up if possible, to make the `PreparedRequest` look as much like what went out on the wire as possible. @kennethreitz may disagree though.
",Lukasa,kennethreitz
1547,2013-10-07 18:04:32,"@Creepr can you import uuid from the interactive prompt?
",sigmavirus24,Creepr
1547,2013-10-07 18:18:57,"@Creepr It looks like something is wrong with your cyguuid-1.dll.

The following bit of code runs in uuid correctly for me.  You can just execute directly in Python to test.


",ahmadia,Creepr
1547,2013-10-08 00:56:06,"No need to apologize @Creepr. I'm really interested in what the final result is that you two find. I just get a ton of email and this is more of a Cygwin issue than a requests issue.
",sigmavirus24,Creepr
1545,2013-08-20 19:16:10,"Hi @Aaron1011! That's a perfectly good idea. However, 100% code coverage has never been an explicit goal of this project. In the absence of a particular code coverage target, I'm not convinced that adding coverage by default is a good plan. I'll defer to @kennethreitz's judgement in this case.

(Side note: You can do your own code coverage testing very easily by installing the [pytest-cov](https://pypi.python.org/pypi/pytest-cov) plugin for py.test. Then testing requests is done by the command `py.test --cov-report term --cov=requests test_requests.py`. This creates a fair amount of noise because it tests our coverage of our vendored libraries (charade and urllib3), which is irrelevant here. Excluding those we're at roughly 75% test coverage, which is pretty solid. Sample output from master:


",Lukasa,kennethreitz
1545,2013-08-20 19:19:20,"In general I'm +1 on coverage reports for all projects. This is 
@kennethreitz's final call though.
",sigmavirus24,kennethreitz
1545,2013-09-01 18:28:42,"@kennethreitz any opinions on this?
",sigmavirus24,kennethreitz
1545,2013-09-24 02:51:09,"Pinging @kennethreitz 
",sigmavirus24,kennethreitz
1542,2013-08-20 02:15:42,"@kennethreitz any idea when Heroku will upgrade to python 2.7.5? I thought it was already using that.
",sigmavirus24,kennethreitz
1542,2014-05-20 16:57:43,"@cvolawless can you try making the same request as if it were a curl request? If you receive the same response as requests, you should contact Heroku support. If you get a different response feel free to open a new issue so @Lukasa and I can be of use. :)
",sigmavirus24,Lukasa
1542,2014-05-20 16:59:21,"@cvolawless also, if you can't discuss the issue publicly, @Lukasa and I both have our emails on our profile pages, so you can send us an email as an alternative for private (confidential) support.
",sigmavirus24,Lukasa
1534,2013-08-17 01:50:45,"This probably doesn't help but a few things:
- Requests is ill-suited for the `file://` scheme.
- The `_original_response` object can be faked in a couple ways but only one way if you're interested in python 2 and 3 support. If the latter is your purpose then checkout sigmavirus24/betamax. I'm using the latter method there. If you have questions, feel free to ask me outside of this message.

I also wouldn't be opposed to documenting the method, but for the most part, I don't see this being a bug. Maybe @Lukasa or @kennethreitz 
",sigmavirus24,kennethreitz
1534,2013-08-17 01:50:45,"This probably doesn't help but a few things:
- Requests is ill-suited for the `file://` scheme.
- The `_original_response` object can be faked in a couple ways but only one way if you're interested in python 2 and 3 support. If the latter is your purpose then checkout sigmavirus24/betamax. I'm using the latter method there. If you have questions, feel free to ask me outside of this message.

I also wouldn't be opposed to documenting the method, but for the most part, I don't see this being a bug. Maybe @Lukasa or @kennethreitz 
",sigmavirus24,Lukasa
1534,2013-08-17 01:55:36,"Heh, @kennethreitz is the one who told me to file it :) I actually have it mocked and it's pretty janky. Simple fix for upstream though and the `file://` url is actually working pretty well, requests seems to handle it fine. The only ""gotchas"" are this one which requires a pretty janky hack to get around, and an error because `file://` urls don't have a hostname but another small hack can fix that too.

FWIW: https://github.com/dstufft/pip/commit/aba6f37d5619b26a4dbc4b2ceb01e9dd2899f5ad
",dstufft,kennethreitz
1534,2013-08-17 02:12:01,"The fix is simple without a doubt and since it seems @kennethreitz seems okay with it, I'll put together a PR with it.

That said, the best workaround is not that janky frankly but you're right in that it shouldn't be necessary. My project needs it though :/
",sigmavirus24,kennethreitz
1532,2013-08-15 07:48:12,"Yeah, I think you're right. @kennethreitz, are you open to me doing this for 2.0?
",Lukasa,kennethreitz
1523,2013-08-12 11:06:28,"So it's worth noting that the default response of this project to any request to change the API is No. An API change request starts with -100 points and needs to justify itself to that level before inclusion.

Sadly, I don't think this is sufficiently useful to justify inclusion. It would get a decent amount of use, but it's not hard to do yourself and not super common (like JSON decoding). I just don't think the API would benefit particularly from its inclusion.

I'll wait until @sigmavirus24 and/or @kennethreitz comments on this, but I'm -1. Sorry. =(

NB: The transformation for anyone that needs it is



Any RFC-2616 compliant Content-Type header should come through that just fine, including no header or an empty header.
",Lukasa,kennethreitz
1523,2013-08-12 11:06:28,"So it's worth noting that the default response of this project to any request to change the API is No. An API change request starts with -100 points and needs to justify itself to that level before inclusion.

Sadly, I don't think this is sufficiently useful to justify inclusion. It would get a decent amount of use, but it's not hard to do yourself and not super common (like JSON decoding). I just don't think the API would benefit particularly from its inclusion.

I'll wait until @sigmavirus24 and/or @kennethreitz comments on this, but I'm -1. Sorry. =(

NB: The transformation for anyone that needs it is



Any RFC-2616 compliant Content-Type header should come through that just fine, including no header or an empty header.
",Lukasa,sigmavirus24
1522,2013-08-29 15:43:57,"@pikumar can you reproduce the issue with the hanging request?
I tried to reproduce this but I can't.

Maybe some sort of personal firewall on your workstation interferes?
I have seen a firewall that ninja-encrypted all outgoing traffic to port 993...
",t-8ch,pikumar
1515,2013-08-08 11:09:08,"@yfengz This will probably take some time, see https://github.com/shazow/urllib3/pull/68
",schlamar,yfengz
1507,2013-07-31 07:26:36,"Mm. I agree that you can see many situations in which modifying the Request itself creates unexpected behaviour. Conversely, though, I can definitely see situations in which `prepare_request`'s allocation hurts you: specifically, situations when you have only one `Session`. Then you have to absorb two allocations per request instead of one, which seems like a nasty waste of time for the GC.

My engineering inclination is to leave the `Request.copy` method in, and say that by default we don't do any extra allocations, and if you want to you have to do it yourself. This turns your code into:



The problem is, I think that the API isn't as good here. I'm pretty torn. My intuition is that the _general_ use-case does not have the same unprepared `Request` being sent to different `Session`s, so we should optimise for that case. I'm prepared to be wrong here, though. @sigmavirus24, thoughts?

**EDIT**: I should note that while the cost of extra allocations is usually not that high, it's inescapable, while the bug introduced by failing to copy a `Request` can be easily worked around.
",Lukasa,sigmavirus24
1507,2013-08-01 01:12:45,"Here's my hang-up. It consists of two parts:
1. > 90% of our users won't ever need this as such, our existing strategy is to use Request objects as organizational throw away objects, as such we never present them to the user directly as part of a Response. What we do allow is for a user to replicate what we do in order to send a PreparedRequest. So as the API is concerned, the most import objects are the Session and Response object. Next most important are PreparedRequest objects because those are directly exposed via the Response object and finally the Request object. I fail to see how we can not just explicitly document for the user that the Request object will be mutated. We're not using a purely functional language so there's no reasonable expectation that the Request will not be mutated.
2. The second part, contingent on that first, is that when you prepare a request, you're not going to get it back and so it may be mutated. If you're using this advanced API then you should have read the docs where we can explicitly document that those objects will be mutated. Does this hamper your use case? Yes. Does it meet the needs of what is likely > 90% of our users? More emphatically, yes.

I haven't skimmed your PR because it seems (from the conversation that I've read via email) that it's very much in flux. That said, if you can sell @Lukasa and me on why we need Ruby-ish methods here, we can probably sell @kennethreitz and frankly you haven't sold me.
",sigmavirus24,kennethreitz
1505,2013-07-30 17:29:10,"You're quite right, there is not. In no small part this is because this functionality is not exposed (at least not cleanly) in versions of Python before 3.3: additionally, AFAIK, urllib3 does not expose the functionality (@t-8ch knows the TLS support of urllib3 way better than I do, he'll be able to tell you).
",Lukasa,t-8ch
1498,2013-07-28 06:43:35,"This should improve the user experience from #1397.

@sigmavirus24, @kennethreitz: I'm not really happy with the nested try...except blocks here. Suggestions for a better style? Nothing leaps out at me.
",Lukasa,kennethreitz
1498,2013-07-28 06:43:35,"This should improve the user experience from #1397.

@sigmavirus24, @kennethreitz: I'm not really happy with the nested try...except blocks here. Suggestions for a better style? Nothing leaps out at me.
",Lukasa,sigmavirus24
1497,2013-07-28 06:17:58,"This is for 2.0, and is in response to @t-8ch's suggestion that proxies should have explicit schemes instead of guessing (which is stupid).

@sigmavirus24, @kennethreitz: I'm not entirely happy with the validation code being here: let me know if you have a better idea for where it should go.
",Lukasa,kennethreitz
1497,2013-07-28 06:17:58,"This is for 2.0, and is in response to @t-8ch's suggestion that proxies should have explicit schemes instead of guessing (which is stupid).

@sigmavirus24, @kennethreitz: I'm not entirely happy with the validation code being here: let me know if you have a better idea for where it should go.
",Lukasa,t-8ch
1497,2013-07-28 06:17:58,"This is for 2.0, and is in response to @t-8ch's suggestion that proxies should have explicit schemes instead of guessing (which is stupid).

@sigmavirus24, @kennethreitz: I'm not entirely happy with the validation code being here: let me know if you have a better idea for where it should go.
",Lukasa,sigmavirus24
1493,2013-07-25 18:14:22,"Thanks for raising this issue @julie777!

I'm strictly +0 on this. I can see the utility of it, but I don't know that a context manager on `Response` is semantically the right way to approach this issue. If @kennethreitz is +1 on this, though, I think it'd make a good addition to V2.0.
",Lukasa,kennethreitz
1475,2013-07-20 17:04:19,"This PR is a work in progress. It addresses some problems we've got with Exceptions. Currently it contains two fixes:
- Resolves the misspelling of 'scheme', reimplementing the fix proposed in #1187.
- Attempts to provide a fix of #1294.

This second part is of particular interest. I want to know what @sigmavirus24 thinks of this possible fix.
",Lukasa,sigmavirus24
1473,2013-07-20 20:05:32,"@Lukasa some eyes on this early on will be helpful too.
",sigmavirus24,Lukasa
1473,2013-08-01 01:38:26,"Cookies are collected on redirects successfully.

I think @gazpachoking worked on that successfully. This was a similar problem 
that wasn't able to be handled the same way.
",sigmavirus24,gazpachoking
1472,2013-07-19 15:20:41,"I'm prepared to believe this was broken by commit 6d6252aa9fc4bb38d7f68073b41092c31ddb146b, and more specifically [this commit](https://github.com/shazow/urllib3/commit/d00d3052382de69382e21af8d10524e5aefadde8). 

I'm pretty sure this failure is in urllib3, not Requests: can you open this issue over there, and cc @schlamar on it?
",Lukasa,schlamar
1459,2013-07-15 13:56:58,"/cc @Lukasa @sigmavirus24 

What would Requests 2.0 include?
- #1338 
- ...
",kennethreitz,Lukasa
1459,2013-07-15 13:56:58,"/cc @Lukasa @sigmavirus24 

What would Requests 2.0 include?
- #1338 
- ...
",kennethreitz,sigmavirus24
1457,2013-07-13 08:44:36,"@drepo This works just fine on my machine. I'm inclined to suggest that you've got an SSL-related problem. What OS are you using?
",Lukasa,drepo
1457,2013-08-02 01:32:16,"@drepo then please clarify what you mean by this:

> All right, now I have managed to get the last code posted by @t-8ch (using urllib3) to work properly and I am not getting any error even on https. So I guess my installation is now fine.

Because it sounds to me that the installation of requests is fine. The last code block @t-8ch posted involved trying to get `requests.__file__` and `requests.__dict__` which you couldn't previously do. Can you print that output now?
",sigmavirus24,drepo
1456,2013-07-13 09:15:04,"Done. By the way, i've seen some status codes that did not match with the ones in the IANA list, but i don't if @kennethreitz did that intentionnaly or if it's just some errors. 

Eg:

`uri_too_long` is set to 122 in requests but the IANA set it to 414 (103 to 199 are unassigned)
",phndiaye,kennethreitz
1449,2013-07-10 02:36:35,"I know this isn't exactly helpful but thank you for such an excellent issue report. One thing I'd ask to see, out of some level of curiosity, is which version of openssl you're running. While @t-8ch and @lukasa are 100% correct, I wonder of this isn't an SSL issue being masked by proxies.
",sigmavirus24,lukasa
1449,2013-07-10 03:03:28,"As a member of a debug team of a broad software stack, I understand the 
agony of trying to find the root cause of bugs from customers that don't 
submit detailed bug reports. It took a good chunk of time out of my 
lunch break to prepare it, but I just thought that making it easier on 
you guys would speed things along. I really appreciate the requests 
library and how easy it makes Python web interactions. I'd like to thank 
you guys for all the work you do to keep this open source project active 
and going in the right direction. If I do find a work around, I'll be 
sure to post it here so others can find it.

Feel free to close this if you deem it necessary.

On 7/9/2013 7:36 PM, Ian Cordasco wrote:

> I know this isn't exactly helpful but thank you for such an excellent 
> issue report. One thing I'd ask to see, out of some level of 
> curiosity, is which version of openssl you're running. While @t-8ch 
> https://github.com/t-8ch and @lukasa https://github.com/lukasa are 
> 100% correct, I wonder of this isn't an SSL issue being masked by proxies.
> 
> —
> Reply to this email directly or view it on GitHub 
> https://github.com/kennethreitz/requests/issues/1449#issuecomment-20718442.
",kylestev,lukasa
1447,2013-07-09 11:31:04,"The documentation for response hooks says ""If the callback function returns a value, it is assumed that it is to replace the data that was passed in. If the function doesn’t return anything, nothing else is effected""

Now i am trying to return a value(int in my case) from my hook function and it throws an exception. This will be valid in all the cases when the return value is an object that DOESNOT have the raw() method defined for it.

Here is some code



And here is the exception:



The code in sessions.py @line 446 is trying to extract cookies after the dispatch_hook..From source



Either the documentation needs to change or the handling needs to be re-worked. What is the best way to handle this ?
",ahmedtalhakhan,line
1445,2013-07-09 09:11:50,"Mm, I guess I can see wanting to do this. I'm still pretty much +0 on it though, so I'll defer to @sigmavirus24 and @kennethreitz.
",Lukasa,kennethreitz
1445,2013-07-09 09:11:50,"Mm, I guess I can see wanting to do this. I'm still pretty much +0 on it though, so I'll defer to @sigmavirus24 and @kennethreitz.
",Lukasa,sigmavirus24
1441,2013-07-04 09:41:43,"As pointed out by @lukesneeringer in #1395, the kwargs in the non-urllib3 branch of the `iter_content` code are urllib3 specific. This PR will finish the split I originally made in #1425.

/cc @sigmavirus24 and @lukesneeringer who have both done work that has been rendered unnecessary (though not useless) by this change.
",Lukasa,lukesneeringer
1441,2013-07-04 09:41:43,"As pointed out by @lukesneeringer in #1395, the kwargs in the non-urllib3 branch of the `iter_content` code are urllib3 specific. This PR will finish the split I originally made in #1425.

/cc @sigmavirus24 and @lukesneeringer who have both done work that has been rendered unnecessary (though not useless) by this change.
",Lukasa,sigmavirus24
1436,2013-06-26 09:51:50,"Actually, on second thought, I'm no longer convinced we should change this.

I've moved that code in a local copy of Requests, and while everything works fine it just looks _wrong_ to me. `Session.send()` doesn't really mess about with keyword arguments, but this change would do that.

I'm now tempted to say that if you've circumvented `Session.request()` (in order to prepare Requests yourself), you must make sure you do whatever you need from `Session.request()`. In this case, that's merging cookies and getting data from environment variables.

I'm now +0 on this, so I'll wait until we get input from @kennethreitz and @sigmavirus24.
",Lukasa,kennethreitz
1436,2013-06-26 09:51:50,"Actually, on second thought, I'm no longer convinced we should change this.

I've moved that code in a local copy of Requests, and while everything works fine it just looks _wrong_ to me. `Session.send()` doesn't really mess about with keyword arguments, but this change would do that.

I'm now tempted to say that if you've circumvented `Session.request()` (in order to prepare Requests yourself), you must make sure you do whatever you need from `Session.request()`. In this case, that's merging cookies and getting data from environment variables.

I'm now +0 on this, so I'll wait until we get input from @kennethreitz and @sigmavirus24.
",Lukasa,sigmavirus24
1436,2013-06-26 12:22:12,"Hmm, I'm prepared to be convinced by that line of argument. I'd still like at least one of @kennethreitz and @sigmavirus24 to weigh in, but I think that's reasonable. 
",Lukasa,kennethreitz
1436,2013-06-26 12:22:12,"Hmm, I'm prepared to be convinced by that line of argument. I'd still like at least one of @kennethreitz and @sigmavirus24 to weigh in, but I think that's reasonable. 
",Lukasa,sigmavirus24
1430,2013-06-24 07:55:07,"I'm pretty sure that the API is doing the wrong thing here.

The '+' character has a syntactic meaning in the query string. Spaces may be encoded either to the literal `+` or to `%20`. For this reason, a non-space `+` character _must_ be encoded to `%2B`. Given that the HN API is clearly using the `%20` representation of a space, I think someone needs to tell them that they can't use the `+` like that.

Unfortunately, this makes it a lot harder for us to help you on this problem. You do have some options. One would be to break open the flow, building `PreparedRequests` yourself and adjusting their URLs. You could potentially use @sigmavirus24's [uritemplate](https://github.com/sigmavirus24/uritemplate) library to help with that.
",Lukasa,sigmavirus24
1429,2013-06-22 14:05:03,"Warn about including `cacerts.pem`.

@andrewgross: This is my idea about better placement. Do you think this wording of the warning would have been helpful?
",Lukasa,andrewgross
1426,2013-06-18 21:32:03,"`urlparse` from the stdlib keeps the braces.

cc @shazow 
",t-8ch,shazow
1425,2013-06-18 17:26:37,"In shazow/urllib3#186 I proposed to add a `.stream()` generator to the urllib3 response object. This avoids some of the nasty problems with the `.read()` method on that object (apparent zero-sized reads when there's still data, all sorts of stuff). It's also a much more natural way to stream data.

This PR would update urllib3 to include that PR, and updates our streaming methods to use it where possible.

@michaelhelmick: Do you want to try this branch with your twitter issue? Use one-byte chunk sizes.
",Lukasa,michaelhelmick
1425,2013-06-18 17:28:53,"I'll give it a shot in a half hour! Thanks!

Sent from my iPhone

> On Jun 18, 2013, at 1:26 PM, Cory Benfield notifications@github.com wrote:
> 
> In shazow/urllib3#186 I proposed to add a .stream() generator to the urllib3 response object. This avoids some of the nasty problems with the .read() method on that object (apparent zero-sized reads when there's still data, all sorts of stuff). It's also a much more natural way to stream data.
> 
> This PR would update urllib3 to include that PR, and updates our streaming methods to use it where possible.
> 
> @michaelhelmick: Do you want to try this branch with your twitter issue? Use one-byte chunk sizes.
> 
> You can merge this Pull Request by running
> 
>   git pull https://github.com/Lukasa/requests stream
> Or view, comment on, or merge it at:
> 
>   https://github.com/kennethreitz/requests/pull/1425
> 
> Commit Summary
> 
> Update urllib3 to cffbd6b317
> Use the new urllib3 stream generator.
> File Changes
> 
> M requests/models.py (17)
> M requests/packages/urllib3/response.py (24)
> M requests/packages/urllib3/util.py (15)
> Patch Links:
> 
> https://github.com/kennethreitz/requests/pull/1425.patch
> https://github.com/kennethreitz/requests/pull/1425.diff
",michaelhelmick,michaelhelmick
1420,2013-06-13 08:16:58,"Hi @jase1987! Thanks so much for this pull request. :cake:

Unfortunately, I think it's really very unlikely that we'll accept this. This has nothing to do with the PR itself: the code is in great shape. However, I don't think we want the feature.

Matrix parameters are really very, very infrequently used. The [URI RFC](http://pretty-rfc.herokuapp.com/RFC3986) does not include them, On top of that, the article you linked to doesn't just say they stopped being supported in '01: they hadn't been supported up until then _either_. I haven't done exhaustive research here, but it's quite possible they were never part of the URI (or URL) standards.

You've also only tackled a subset of the problem. Insanely, matrix parameters can apply to any (and all) portions of the path element, e.g. `http://example.com/res/categories;name=foo/objects;name=green/?page=1`. This is very difficult to cleanly represent in Requests' standard functional API.

Given that they're not part of the standard, are very infrequently used, and are difficult to do 'properly', I think we won't want to put this in mainline Requests. With that said, I encourage you to maintain a downstream fork of Requests that includes them if that would be useful to you.

Also, I'm very sorry that you have to interact with such an unhelpful REST API. =)

I'm going to leave this open until @kennethreitz takes a look, but I'd be surprised if he wanted to add support for matrix parameters.

Again, thanks so much for the work!
",Lukasa,kennethreitz
1419,2013-06-13 08:03:08,"I'm +0 on this, actually. I like having clearer class names, but having the full class name makes it way more obvious how to import the class. If it were me, I'd probably leave it as is, but @kennethreitz will probably disagree with me. =)
",Lukasa,kennethreitz
1418,2013-06-11 19:19:40,"This is a duplicate of #1289. Right?
According to the bugreport you linked it's an issue with the SSL/TLS version requests is using to connect to the server.
@Lukasa wrote a blogpost about customizing the version, requests uses: http://lukasa.co.uk/2013/01/Choosing_SSL_Version_In_Requests/

I modified the code from the blog a bit, so that it works with the current version of requests:
(Try to remove `s.mount()` to break it again)



Btw, I also couldn't connect from my arch box.
And also congratulations for open sourcing your project!
",t-8ch,Lukasa
1418,2013-06-11 21:21:55,"I suspect (I'm not sure) the server is buggy. It sees a TLS version it doesn't recognize and then stops responding, while the client is waiting for a respones, even an error would be better.
I have the same problem on archlinux with openssl 1.0.1.e. Rereading your last bugreport it seems It worked for me back then, with an older version of openssl.
This older version presumably used a lower TLS version, which didn't freak out the server.
So the version is rather too new :smile:

@Lukasa You may want to update your blogpost to include the `block` argument, which is needed nowadays.
",t-8ch,Lukasa
1408,2013-06-07 10:21:18,"Hi @wasw100, thanks for raising this pull request!

Unfortunately, it's not clear to me whether we want to continue supporting adding Morsels to `RequestsCookieJars`. Can I get a call on this @kennethreitz?
",Lukasa,kennethreitz
1408,2013-06-07 10:43:22,"I agree that it's useful, I just wonder how useful it is. Requests does not include plenty of useful code on the grounds that it complicates the API and increases the amount of code we need to maintain. I'm on the fence about this: I don't mind keeping it and I don't mind throwing it away. In situations like that, I'd usually throw it away.

That's why I'm interested to see what @kennethreitz thinks.
",Lukasa,kennethreitz
1405,2013-06-04 12:46:34,"There have been talks in urllib3#140 about a new class `Verification` where users can embed their own verification logic, as urllib3's ssl api is becoming a but unwieldy.
Until this gets implemented it may be possible to change the `assert_hostname` of `urllib3` to disable hostname verification if it's value is the empty string `''`.

cc @shazow
",t-8ch,shazow
1401,2013-06-03 09:03:26,"Hi @foxx, thanks for opening this issue! Let me address these in no particular order.
1. Implementing .read(). Requests explicitly provides the underlying transport library's notion of a response in the `Response.raw` property. If you believe that `decode_content=True` should be the default behaviour (and I agree that it should), I recommend you open a pull request to that effect on [urllib3](https://github.com/shazow/urllib3). I'm sure @shazow will be happy to take a look at it. =)
2. Changing the default read size is a bit of an odder one. Given the point I made in **1**, we probably can't do it in Requests itself. I would also argue that any method named `read()` should behave like a file-like object, which doesn't have a minimum read value. When I get internet in my new house, I'm going to take a look at implementing the idea in shazow/urllib3#186, which should be able to paper over this issue.
3. Default `iter_content()` size is a sore topic around here: see #844, which has still not been completely resolved. Your input is welcome there. =)
4. Where is the memory leak in `iter_content()`?
",Lukasa,shazow
1394,2013-05-31 13:50:51,"@shazow and I have explored adding this several times. Every time, our conclusion is that it's simply too much work with little performance benefit.

Waiting for SPDY now :)
",kennethreitz,shazow
1390,2016-06-09 08:33:40,"@kennethreitz The answer to that is _maybe_. The problem with using Deferreds is that Deferreds don't make things magically async: they're literally just fancy callback containers. We can almost certainly _use_ Deferreds to build an abstraction layer on top of multiple concurrency models if that's a route we want to go.

This is a problem that really needs to be approached _very_ carefully, because as long as Python 2.7 is around we cannot unconditionally expect that the standard library will have an event loop inside it that we can use. That represents a concern: we need a way to execute _without_ an event loop. This is why Deferreds are attractive: they can execute in just such a manner because they're _literally_ just callback holders.

So one approach here would be to write an entirely callback-based HTTP/1.1 + HTTP/2 client library that uses Deferreds (probably on top of @njsmith's [h11](https://github.com/njsmith/h11) and @python-hyper's [h2](https://github.com/python-hyper/h2)).

We could then try to think about how we can integrate that into the various different event loops, though again, I'm not 100% certain of how we'd do that best. _Probably_ we'd have to write (sigh) an event-loop abstraction layer that basically defines certain functions as returning deferreds (""data_from_network""), and that then plugs into the relevant event loops. In practice, though I've said ""event loops"", we really only need two implementations to get started: one that implements the Twisted/asyncio ""protocol"" notion, and one that is synchronous and blocking (so that we continue to work in the absence of an event loop). In fact, we could even just start with the second one.

This is one of those things that it'd be really interesting to get someone like @glyph to weigh in on, in part because @glyph is strongly incentivised to help us get this right (no more treq!) and in part because he's a lot smarter than me. Certainly, however, I'd love to have a version of requests that you can either use synchronously in its current form without noticing the difference _or_ that you can use on top of ${EVENT_LOOP}. Requests provides enough goodness that we should really reimplement as little of it as possible on different platforms.
",Lukasa,glyph
1390,2016-06-09 08:33:40,"@kennethreitz The answer to that is _maybe_. The problem with using Deferreds is that Deferreds don't make things magically async: they're literally just fancy callback containers. We can almost certainly _use_ Deferreds to build an abstraction layer on top of multiple concurrency models if that's a route we want to go.

This is a problem that really needs to be approached _very_ carefully, because as long as Python 2.7 is around we cannot unconditionally expect that the standard library will have an event loop inside it that we can use. That represents a concern: we need a way to execute _without_ an event loop. This is why Deferreds are attractive: they can execute in just such a manner because they're _literally_ just callback holders.

So one approach here would be to write an entirely callback-based HTTP/1.1 + HTTP/2 client library that uses Deferreds (probably on top of @njsmith's [h11](https://github.com/njsmith/h11) and @python-hyper's [h2](https://github.com/python-hyper/h2)).

We could then try to think about how we can integrate that into the various different event loops, though again, I'm not 100% certain of how we'd do that best. _Probably_ we'd have to write (sigh) an event-loop abstraction layer that basically defines certain functions as returning deferreds (""data_from_network""), and that then plugs into the relevant event loops. In practice, though I've said ""event loops"", we really only need two implementations to get started: one that implements the Twisted/asyncio ""protocol"" notion, and one that is synchronous and blocking (so that we continue to work in the absence of an event loop). In fact, we could even just start with the second one.

This is one of those things that it'd be really interesting to get someone like @glyph to weigh in on, in part because @glyph is strongly incentivised to help us get this right (no more treq!) and in part because he's a lot smarter than me. Certainly, however, I'd love to have a version of requests that you can either use synchronously in its current form without noticing the difference _or_ that you can use on top of ${EVENT_LOOP}. Requests provides enough goodness that we should really reimplement as little of it as possible on different platforms.
",Lukasa,njsmith
1390,2016-06-09 08:33:40,"@kennethreitz The answer to that is _maybe_. The problem with using Deferreds is that Deferreds don't make things magically async: they're literally just fancy callback containers. We can almost certainly _use_ Deferreds to build an abstraction layer on top of multiple concurrency models if that's a route we want to go.

This is a problem that really needs to be approached _very_ carefully, because as long as Python 2.7 is around we cannot unconditionally expect that the standard library will have an event loop inside it that we can use. That represents a concern: we need a way to execute _without_ an event loop. This is why Deferreds are attractive: they can execute in just such a manner because they're _literally_ just callback holders.

So one approach here would be to write an entirely callback-based HTTP/1.1 + HTTP/2 client library that uses Deferreds (probably on top of @njsmith's [h11](https://github.com/njsmith/h11) and @python-hyper's [h2](https://github.com/python-hyper/h2)).

We could then try to think about how we can integrate that into the various different event loops, though again, I'm not 100% certain of how we'd do that best. _Probably_ we'd have to write (sigh) an event-loop abstraction layer that basically defines certain functions as returning deferreds (""data_from_network""), and that then plugs into the relevant event loops. In practice, though I've said ""event loops"", we really only need two implementations to get started: one that implements the Twisted/asyncio ""protocol"" notion, and one that is synchronous and blocking (so that we continue to work in the absence of an event loop). In fact, we could even just start with the second one.

This is one of those things that it'd be really interesting to get someone like @glyph to weigh in on, in part because @glyph is strongly incentivised to help us get this right (no more treq!) and in part because he's a lot smarter than me. Certainly, however, I'd love to have a version of requests that you can either use synchronously in its current form without noticing the difference _or_ that you can use on top of ${EVENT_LOOP}. Requests provides enough goodness that we should really reimplement as little of it as possible on different platforms.
",Lukasa,python-hyper
1390,2016-06-09 08:37:22,"Oh, and @shazow, this is something you might care about too, in part because we might be able to make urllib3 do most of the heavy lifting here.
",Lukasa,shazow
1390,2016-06-11 13:23:36,"Heh @njsmith, this is a problem because you don't know what Requests looks like internally. =)

The internals of Requests itself already _are_ very, very close to the design of h2/h11: that is, they almost entirely twiddle data around in in-memory objects. Unlike in aiohttp where it seems like every single function/class in the library touches or handles I/O in some way, the vast majority of requests is function calls.

The functions that either directly or transitively do I/O or that would need to be adjusted in some way are the following:
- `requests.api.*`: all of these transitively wrap `Session.request`.
- `requests.sessions.Session.request`: this transitively calls `Session.send`, which actually does do I/O
- `requests.sessions.SessionRedirectMixin.resolve_redirects`: this both reads and writes (using `Session.send`), and so is clearly an I/O based function, but mostly just uses the primitives we'd have to work out at the lower level.
- `requests.sessions.Session.send`: This is the _only_ top-level requests function that actually does I/O (excepting file reads/writes which don't count because Unix).
  `requests.adapters.HTTPAdapter.send`: This does the bulk of the low-level I/O, though it calls one helper function it defines on itself that is I/O-adjacent, namely...
- `requests.adapters.HTTPAdapter.get_connection`: This obtains a connection object from urllib3's connection pool. This is going to be different in concurrent and synchronous modes because the way we wait for connections to show up will be different.

In this case, for requests, I'd ideally like to touch _only_ those functions. That ensures that the rest of Requests goes on functioning exactly as it does today. This reduces the churn in Requests and reduces the odds of us introducing bugs in terrible places.

From my perspective, the easiest way to do this is to just write `@asyncio.coroutine` on top of all of them and throw some `yield from`s in there, then wrap it all in `asyncio.run_until_complete` if people want to use the blocking APIs.

However, if we decide we need a separate blocking backend this _all_ gets way harder. In that kind of model, your approach may work slightly better, because it may be quite painful (and indeed impossible?) to use `Deferred`s in such a way that the top-level user of requests doesn't have to think about them in synchronous code. In that kind of model, we actually do need to write things quite differently, and at _that_ point I'm willing to discuss tearing the whole library apart to rebuild it.

Otherwise, I'm extremely reluctant to do a big rewrite: there's just no way that we'll persist all the useful functionality that the library provides in its current form. =(
",Lukasa,asyncio
1390,2016-06-14 14:28:12,"- @KingOfPoptart
",smorin,KingOfPoptart
1390,2016-07-09 02:34:19,"Since @Lukasa was so kind in his assessment of my abilities I figure I should weigh in pretty substantively.  (As always, my estimate last month of ""a couple of days"" was hilariously optimistic, given that I had another conference and a week of vacation to do after that...)

Let me first address some of the questions around the possibility of using `Deferred` directly to facilitate asynchrony.

There's a stand-alone version of Deferred available from &lt;https://pypi.python.org/pypi/deferred&gt;.  I've recently been added as a package index owner, and we (Twisted generally) plan to actively maintain this going forward, and hopefully split it out of Twisted entirely.  This is a much smaller dependency than asyncio or one of its various clones, mostly due to the fact that it does not do any I/O; it's _just_ the callback abstraction.  Right now it still includes stuff like `DeferredFilesystemLock` but we are probably going to remove that and leave it in Twisted proper.

There are two problems that `Deferred` could potentially solve within `requests`.  Let me start with the simpler one: providing a front-end for Twisted - replacing `treq`.  Whatever asynchronous solution you end up going with, it should be easy enough to have a `requests.frontend.twisted` which translates whatever not-yet-available-result.

The other, more substantive one, is providing a common abstraction between the different layers within requests to compose with each other.  @Lukasa laid out some of these layers [in this comment](https://github.com/kennethreitz/requests/issues/1390#issuecomment-225361421).  My understanding is pretty superficial, but at the highest level, `requests.get` wraps `Session.request` and expects it to block, and then returns its result.  What you would be using `Deferred` for here would be to have a single return type which could stand in for ""expects it to block"", and anything that currently blocks could simply start returning a `Deferred`, which would be fired by the external I/O machinery feeding bytes into it somehow, h11-style.

If you have to continue supporting Python 2, `Deferred` is definitely a friendlier abstraction for this than a `Future`-alike, since `asyncio.Future`'s extremely spare API is specifically designed to be a primitive that you interact with indirectly, behind the syntactic convenience of `await`.  (I think it's still a better internal abstraction )

The question then is, having retrofitted everything internally to use this new non-blocking abstraction, how do you still support blocking code?  This is actually a bit more generic than just `Deferred`, because any non-blocking approach will have the same problem, but I'll discuss it in the language of `Deferred` because it provides an easier shorthand :).  This breaks down into two sub-problems: how do you deal with external callers calling into e.g. `requests.get`, and how do you facilitate plugging in a blocking implementation of something that lives in the middle of the stack?

For the former, the answer is really simple: you just have a wrapper that lives at the edge, which presents a blocking API, and when called, dispatches into an event loop which just does I/O until the `Deferred` returned by the lower level has fired.  Doing this in a natively asynchronous program is problematic because the whole point is you want to share an event loop and you don't want to use it re-entrantly (and Twisted, in particular, has never really been retrofitted all the way to make loops easy to instantiate, although we'll get there one day).  But your ""event loop"" can just be a little function that does blocking `socket.recv` / `socket.send` and stuffs the results into the underlying I/O layer; you don't need full-blown event-driven concurrency in this layer.

To plug something in in the middle is a bit more complex in implementation, but essentially the same conceptually: at every possible public integration point, you have a wrapper for the layer below like the one I just described, and you have a wrapper for the layer above which just calls the blocking thing and then wraps the result in an already-fired `Deferred`; this is sort of what [`succeed`](https://twistedmatrix.com/documents/16.2.0/api/twisted.internet.defer.html#succeed) is for.

The original pioneer of the [synchronous `Deferred` approach](https://github.com/radix/synchronous-deferred), @radix, would tell you to use his newer library, [effect](https://github.com/python-effect/effect), to thread the non-blocking needle through the core of `requests`.  And he might even be right!  This is really a matter of taste; functionally the way you'd be writing code and dealing with the edges of the system where things become blocking or attach to a concrete event loop is similar.  Effect has a bit more action-at-a-distance which can be a little tricky to reason about - which is by design, the separation of intents and performers is one of its architectural features - but which also facilitates dispatching to different backends which requests might need.

I'll probably have more thoughts on this later, but I should probably yield the floor at this point - we've still got 11 months before this needs to roll out, right? :)
",glyph,radix
1390,2016-08-06 12:00:30,"Thanks guys that you discuss this topic because it is very important for me too. btw If you would like to use require with asyncio python 3 only. Here is simple solution to wrap `require.<something>` call with `loop.run_in_executor` http://stackoverflow.com/questions/22190403/how-could-i-use-requests-in-asyncio. I haven't tried all edge case but it works at least for regular POST for me. Only one problem that other very popular library https://github.com/getsentry/responses from @dcramer doesn't work with asyncio right now, so here is still some work ever for this such straightforward solution.
",hyzhak,dcramer
1388,2013-05-25 08:03:59,"@Arfrever Unfortunately we rushed the last two releases out due to a pair of nasty bugs, one affecting everyone on 3.3.2 and one affecting anyone using OAuth (both bad). Presumably @kennethreitz was midway through working on something when he published the last two releases, and my badgering him to release fast caused him to lose track of it.

However, the library itself functions fine, only the tests fail to run. Is it sufficient for you to simply change the tests locally?
",Lukasa,kennethreitz
1384,2013-05-24 07:22:17,"Hi, I'm quite new to the world of Python, so might be missing something. When I submitted [this pull request](https://github.com/kennethreitz/requests/pull/1382) the other day, @sigmavirus24's response was surprising. Surely, putting a checkout of a third party library in your repository is not the right way to manage dependencies?

Can't you use [requirements.txt](https://github.com/kennethreitz/requests/blob/master/requirements.txt) to include urllib3 or at least make it a git submodule?
",tasuk,sigmavirus24
1380,2013-05-22 21:02:57,"This is easily worked around by doing:



Technically we could return any object that behaves like a dict without it being a dict, so I don't see this as much of a problem. What would be fantastic is if there were a `__serialize__` method we could declare on objects so that modules like `json` could just reference that as the author of the object intends.

I'll defer to @Lukasa on this one though. I personally don't find this so offensive frankly but I'm not against changing it either.
",sigmavirus24,Lukasa
1374,2013-05-21 19:12:48,"@papeye: Oh yeah, my fix is totally wrong. Do you want to open a Pull Request with your fix in it? =)
",Lukasa,papeye
1373,2013-05-21 08:52:21,"`filepost.encode_multipart_formdata` says all fieldnames should be unicode and currently that is not the case - https://github.com/shazow/urllib3/blob/master/urllib3/filepost.py#L55 - since requests encodes to bytes here - https://github.com/kennethreitz/requests/blob/master/requests/models.py#L108.

Discovered after plenty of fun in https://github.com/requests/requests-oauthlib/pull/43. This should fix https://github.com/kennethreitz/requests/issues/1371.

Tested in 2.6, 2.7 and 3.3.

cc @Lukasa, @michaelhelmick, @sigmavirus24
",ib-lundgren,Lukasa
1373,2013-05-21 08:52:21,"`filepost.encode_multipart_formdata` says all fieldnames should be unicode and currently that is not the case - https://github.com/shazow/urllib3/blob/master/urllib3/filepost.py#L55 - since requests encodes to bytes here - https://github.com/kennethreitz/requests/blob/master/requests/models.py#L108.

Discovered after plenty of fun in https://github.com/requests/requests-oauthlib/pull/43. This should fix https://github.com/kennethreitz/requests/issues/1371.

Tested in 2.6, 2.7 and 3.3.

cc @Lukasa, @michaelhelmick, @sigmavirus24
",ib-lundgren,sigmavirus24
1373,2013-05-21 08:52:21,"`filepost.encode_multipart_formdata` says all fieldnames should be unicode and currently that is not the case - https://github.com/shazow/urllib3/blob/master/urllib3/filepost.py#L55 - since requests encodes to bytes here - https://github.com/kennethreitz/requests/blob/master/requests/models.py#L108.

Discovered after plenty of fun in https://github.com/requests/requests-oauthlib/pull/43. This should fix https://github.com/kennethreitz/requests/issues/1371.

Tested in 2.6, 2.7 and 3.3.

cc @Lukasa, @michaelhelmick, @sigmavirus24
",ib-lundgren,michaelhelmick
1371,2013-05-21 06:24:55,"Firstly, thanks for opening this issue! This exact bug was found about 8 hours before you opened this, in requests/requests-oauthlib#43. The problem is that all field and file names passed to `encode_multipart_formdata` should be Unicode. As you identified, #1279 caused all of these field and file names to be bytes, not unicode.

@ib-lundgren has proposed a fix across in requests-oauthlib, which involves changing L108 as follows:



If you would like to fix this yourself, you can write a test and apply the fix, then send a Pull Request. Alternatively,@ib-lundgren will probably do that at some point soon. =)
",Lukasa,ib-lundgren
1369,2013-05-20 19:21:58,"@jellyflower Thanks for reporting this bug. It's actually one we already knew about: see requests/requests-oauthlib#43.

I wonder if 3.3.2 has introduced a bug in Requests. @sigmavirus24, I'm leaving this for you. =)
",Lukasa,sigmavirus24
1366,2013-05-19 01:14:35,"/cc @idan
",michaelhelmick,idan
1363,2013-05-16 17:39:33,"@dave-shawley it most certainly does! Thank you kind sir!

:+1: for @kennethreitz when he comes around these parts again
",sigmavirus24,kennethreitz
1356,2013-05-12 01:11:39,"Thanks @sigmavirus24 for the help!

This was causing problems when I was doing threaded gets to a single host. I can get around it by subclassing the adapter and overriding the creation of the connection pool (where I ran into the problem from #1357), but this seems like it should be something you can do without needing to sub class the adapter.
",Zoramite,sigmavirus24
1356,2013-05-12 01:38:27,":cake: Awesome! Sorry it took so long, but this pull request now looks perfect IMO. Hopefully @Lukasa will agree.
",sigmavirus24,Lukasa
1353,2013-05-07 13:31:47,"For everyone else interested, this stems from [this discussion](http://stackoverflow.com/questions/16390243/closing-python-requests-connection-from-another-thread/16400574?noredirect=1#comment23529567_16400574). I'm personally :-1: on the idea, but I'd like to hear @kennethreitz and @Lukasa chime in.
",sigmavirus24,kennethreitz
1353,2013-05-07 13:31:47,"For everyone else interested, this stems from [this discussion](http://stackoverflow.com/questions/16390243/closing-python-requests-connection-from-another-thread/16400574?noredirect=1#comment23529567_16400574). I'm personally :-1: on the idea, but I'd like to hear @kennethreitz and @Lukasa chime in.
",sigmavirus24,Lukasa
1349,2013-05-05 03:13:58,"@Lukasa concerns?
",sigmavirus24,Lukasa
1341,2013-05-01 18:37:33,"Requested by @sigmavirus24.
",cdunklau,sigmavirus24
1340,2013-05-01 16:46:51,"This PR removes Python 3.1 and 3.2 from the trove classifiers in `setup.py` since @kennethreitz decided to drop support for those versions in the rejected PR #1326. 

It also introduces a rudimentary ""tox.ini"" which makes local tests with multiple Python versions trivial:


",ambv,kennethreitz
1339,2013-04-30 20:05:30,"Fixes #649 and #1329 by making Session.headers a CaseInsensitiveDict,
and fixing the implementation of CID. Credit for the brilliant idea
to map `lowercased_key -> (cased_key, mapped_value)` goes to
@gazpachoking, thanks a bunch.

Changes from original implementation of CaseInsensitiveDict:
1.  CID is rewritten as a subclass of `collections.MutableMapping`.
2.  CID remembers the case of the last-set key, but `__setitem__`
   and `__delitem__` will handle keys without respect to case.
3.  CID returns the key case as remembered for the `keys`, `items`,
   and `__iter__` methods.
4.  Query operations (`__getitem__` and `__contains__`) are done in
   a case-insensitive manner: `cid['foo']` and `cid['FOO']` will
   return the same value.
5.  The constructor as well as `update` and `__eq__` have undefined
   behavior when given multiple keys that have the same `lower()`.
6.  The new method `lower_items` is like `iteritems`, but keys are
   all lowercased.
7.  CID raises `KeyError` for `__getitem__` as normal dicts do. The
   old implementation returned `None`
8.  The `__repr__` now makes it obvious that it's not a normal dict.

See PR #1333 for the discussions that lead up to this implementation
",cdunklau,gazpachoking
1338,2013-04-30 19:56:33,"_Note_: This PR follows a discussion on IRC. The logs can be found [here](https://botbot.me/freenode/python-requests/msg/2942059/).
### Summary

This fixes a problem that @https://github.com/va1en0k _would_ have had in #1335 if the current Case-Insensitive Dict didn't behave so weirdly (see #1333 for progress on that).

Right now, Requests unconditionally encodes all header values as bytes on all Python versions. Kenneth and I are (tentatively) in agreement that header values should actually be native strings on all platforms. This pull request introduces code that explicitly encodes or decodes string objects to satisfy that requirement.
### Notes

Firstly, **this is a breaking API change on Python 3**. On Python 2 the behaviour will be the same unless the user has changed `sys.defaultencoding`, and if they have they deserve what's coming to them. On Python 3, all header keys on `PreparedRequest`s will go from being bytes to unicode objects, and so anyone who has correctly programmed to this interface will have to change their code.

Secondly, this code currently uses ASCII as the codec in both directions. I think this is OK for now. I'm pretty sure that header values should actually be Latin-1, but I can understand if we want to keep the current behaviour in place. I'm open to feedback here.
",Lukasa,https
1338,2013-05-01 19:21:16,"Yeah that fixes it. Travis just has your 2.6 build queued so @kennethreitz might be wary until it completes (not that he need be).
",sigmavirus24,kennethreitz
1338,2013-06-20 20:20:46,"@adian0: Indeed. Try replacing the `'User-Agent'` and `'Content-Type'` header keys with `b'User-Agent'` and `b'Content-Type'`. If that works, then the problem is one that will be fixed by this pull request.
",Lukasa,adian0
1338,2013-06-21 14:09:20,"The cause of the behavior reported by @adian0 seems to be the same I described in #1250.
",paparomeo,adian0
1338,2013-06-21 14:10:29,"@pmcnr Agreed, and this PR should resolve it. =)
",Lukasa,pmcnr
1336,2013-04-30 08:27:45,"`HTTPDigestAuth.handle_401()` needs to persist cookies. Thanks for reporting this! I'm marking this as contributor friendly, anyone who wants it should take it. I'm looking at @gazpachoking and @sigmavirus24, they're the cookie masters here.
",Lukasa,gazpachoking
1336,2013-04-30 08:27:45,"`HTTPDigestAuth.handle_401()` needs to persist cookies. Thanks for reporting this! I'm marking this as contributor friendly, anyone who wants it should take it. I'm looking at @gazpachoking and @sigmavirus24, they're the cookie masters here.
",Lukasa,sigmavirus24
1336,2013-06-06 08:36:55,"Sorry @ericL, both @sigmavirus24 and I have just moved house and don't have domestic internet connections. In addition, I also get lousy mobile data reception in my new house. I can only get EDGE, which means I can't really tether to my mobile phone either.

I think we'll just have to sit on this for a little while until @sigmavirus24 and I have better access and more time.
",Lukasa,ericL
1336,2013-07-19 20:15:28,"Started working on this with @Iross over [here](https://github.com/sigmavirus24/requests/tree/fix_401_cookies)
",sigmavirus24,Iross
1334,2013-06-05 23:59:33,"I'm not sure I follow. Per http://www.ietf.org/rfc/rfc2965.txt hostname matching should be case-insensitive. Is that not how it's behaving today? A quick test with uppercase hosts seems to work.

I'm curious the conditions under which #1385 are needed. For example, this prints 200 for me with no changes to requests:



My patch addresses uppercase schemes in the Location header, so it only affects redirects. I did try what @ViktorHaag has proposed, but I seem to remember having issues with a pool manager?
",rcarz,ViktorHaag
1334,2013-06-06 08:48:10,"@rcarz That's odd, vanilla requests throws exceptions when I do a get on `HTTP://www.google.com/`. TravisCI does too.

I'm not sure you're right, @Anorov. As @rcarz points out, this change here only affects redirects. You can see this by looking at the Travis CI output for this change, where a test with upper-case scheme fails.

#1385 is more comprehensive than this fix. I need to sit down and confirm whether #1385 covers all cases this fix does.
",Lukasa,Anorov
1333,2013-04-29 16:59:19,"Another concern, brought up by @gazpachoking: The original implementation preserves the case of the header name first inserted, this implementation does not.
",cdunklau,gazpachoking
1333,2013-04-29 21:56:55,"@piotr-dobrogost There was some back and forth on IRC about it, and I was swayed. The undefined behavior is preferred because:
1. Simplicity. (KISS)
2. More dict-like in some circumstances.

But I can't remember the finer details. @gazpachoking, @Lukasa can you chime in?
",cdunklau,gazpachoking
1333,2013-04-29 21:56:55,"@piotr-dobrogost There was some back and forth on IRC about it, and I was swayed. The undefined behavior is preferred because:
1. Simplicity. (KISS)
2. More dict-like in some circumstances.

But I can't remember the finer details. @gazpachoking, @Lukasa can you chime in?
",cdunklau,Lukasa
1329,2013-04-26 21:03:27,"Ok, so based on a suggestion from @Lukasa in IRC, I'm considering switching `.sessions.Session.headers` over to `.structures.CaseInsensitiveDict`, but it looks like its `.update` method doesn't do what I'd like. However, I don't really know what the sane behavior would be for edge cases where the input mapping has two keys that have the same `.lower()`...

This needs some extra thinking, I'll ponder it over the weekend.
",cdunklau,Lukasa
1325,2014-08-06 19:08:26,"@num1: The issue is not ""one extra parameter"". The issue is that we get many feature requests all of which want ""one extra parameter"". At some point we either draw a line or end up having many thousands of parameters. 

Requests will never be perfect for all use cases. It is impossible to follow the full breadth of both _de jure_ and _de facto_ HTTP usage _and_ have a simple API. It cannot be done. 

The API is what matters to us. This means features that are only useful to a minority of developers but that add complexity to the API for all of them tend not to be accepted. At the moment, that is the case for this feature.
",Lukasa,num1
1325,2014-08-07 21:05:29,"@jamshid I hope next time you'll investigate the [actual behaviour](https://github.com/kennethreitz/requests/blob/master/requests/sessions.py#L134..L149) of requests. For the status codes that we [may](http://tools.ietf.org/html/rfc7231#section-6.4.2) change the method, we do because this is the unfortunate de facto behaviour that @lukasa referred to. We do **not** change them on 307 or 308 responses, ergo we already follow the new set of RFCs for HTTP/1.1.
",sigmavirus24,lukasa
1324,2013-04-25 03:20:04,"So the code which causes this break was introduced partially because of Transport Adapters and partially because the re-factor broke the old code. This is a far more elegant way of managing cookies (in my opinion) than we used to use and I'm really not 100% comfortable changing it back. However, this is a major break in the API (although entirely unintended and not exactly forseen) so I am guessing that @kennethreitz will be entirely in favor of finding a solution to return to the old behaviour.

This especially is a pain because @gazpachoking and I put a decent amount of effort into restoring cookie persistance and making it elegant. Perhaps he would be interested in tackling this? _Nudge, nudge_
",sigmavirus24,gazpachoking
1324,2013-04-25 03:20:04,"So the code which causes this break was introduced partially because of Transport Adapters and partially because the re-factor broke the old code. This is a far more elegant way of managing cookies (in my opinion) than we used to use and I'm really not 100% comfortable changing it back. However, this is a major break in the API (although entirely unintended and not exactly forseen) so I am guessing that @kennethreitz will be entirely in favor of finding a solution to return to the old behaviour.

This especially is a pain because @gazpachoking and I put a decent amount of effort into restoring cookie persistance and making it elegant. Perhaps he would be interested in tackling this? _Nudge, nudge_
",sigmavirus24,kennethreitz
1323,2013-04-25 07:07:45,"@sigmavirus24: Changes made. =)
",Lukasa,sigmavirus24
1321,2013-05-01 18:43:50,"@cdunklau is this fixed by your pull request?
",sigmavirus24,cdunklau
1321,2013-05-03 18:09:52,"I think #1343 + #1339 will make this pull obsolete to a degree. The other issue is that previously we were merging the arguments in a case insensitive way which was only correct for headers. This is better handled by @gazpachoking's PR.
",sigmavirus24,gazpachoking
1320,2013-04-24 14:41:08,"> This is because there is already a default adapter for 'http://' in the form of requests.adapters.HTTPAdapter. Depending on the (seemingly random) order of keys in the s.adapters dictionary, for some combinations of keys it will work, for others it won't.

**EDIT** None of the information in this comment is correct. There's nothing to see here except my embarrassment.

This has nothing to do with dictionary order. When we look an adapter we're looking for an adapter based on protocol, not hostname. We use `urlparse` to get the scheme (or protocol) and then we look for that in the adapters dictionary. With this in mind you get



And we do `self.adapters.get(uri.scheme)` I believe. You would have to monkey patch `get_adapter` to get the behaviour you want.

That's how we do it now. As for the docs, I have no clue why that example is there because it is just plain wrong. Setting up an adapter for that though would probably be convenient for quite a few people though. One concern I have, though, is that it is a change that sort of breaks the API despite being documented as working that way.

@Lukasa ideas?
",sigmavirus24,Lukasa
1320,2013-04-24 15:52:20,"I am sincerely sorry @ambv. That'll teach me to work from memory ever again. 

Here are my thoughts about this with the code above:
- We could collect a list of matching adapters instead of returning the first one we find. The problem is then deciding which adapter to use
- We could maintain two separate adapters registries: 1) user-created 2) default. The user-created adapters would be the first to be searched through and if there's a match in them we could then return that. If none of those match we would then search the default adapters and if nothing matches from there raise the `InvalidSchema` error. To preserve the API we could make `adapters` a property. The `@adapters.setter` method would then only set adapters on the user-created dictionary. The returned information would then be best represented as a list of two-tuples where the user-created items come first and is then followed by the default. This gives an intuitive idea of the overall ordering of discovery of adapters. This, however, would break the case where someone tries to do `session.adapters['http://']`
- We could create our own `AdaptersRegistry` object which behaves like a dictionary, i.e., has the `__setitem__`, `__getitem__`, `get`, `set`, &c., methods, and does the search for us. Then we just maintain that as the `adapters` attribute.

I could be vastly over-thinking the problem though.
",sigmavirus24,adapters
1320,2013-04-24 18:12:26,"I think we're totally over-engineering this. If we were going to do this properly we'd implement a trie and cause it to mimic the dictionary interface (not hard).

The reality is, we don't need to. We can assert that the number of transport adapters plugged into any session is likely to be small. If that's the case, we should just do:



This way we don't have to maintain a new data structure. Unless @kennethreitz wants to, of course, in which case I'll happily whip up a pure-Python trie. =)
",Lukasa,kennethreitz
1320,2013-04-25 06:58:08,"One of the valid use cases for adapters is unit testing. Tests should run as fast as possible, spending time sorting adapters in place every time is wasteful. I don't like the approach taken in #1323 because `get_adapter()` is called for every single request.

I'd like @kennethreitz to weigh in here whether he considers session.adapters a public API. For what it's worth this attribute is not listed in the ""Developers Interface"" section of the docs here: http://www.python-requests.org/en/latest/api/#request-sessions
",ambv,kennethreitz
1312,2013-04-14 18:30:16,"`HTTPResponse` is actualy from urllib3 in this case, not urllib2. The simple work around would be to check urllib3 for similar issues and if none exist (open or closed) open a new one to see if @shazow likes the idea of making the Responses usable via select. You might be better off (in the interim) looking into the Respones attributes for something that might allow you to use select. It won't be as elegant as doing `Response.raw` but it shouldn't be worse than `Response.raw.attribute`.
",sigmavirus24,shazow
1311,2013-04-13 16:34:19,"cc @Lukasa for code-review
",sigmavirus24,Lukasa
1308,2013-04-12 14:05:02,"@lukasa thanks, this solves my problem.
",medecau,lukasa
1308,2013-04-12 15:45:31,"@damiengermonville you might then need to petition over at shazow/urllib3 for an easier way to pass in the ciphers to the VerifiedHTTPSConnection object. We have no clue what your code looks like, but I imagine this might make it easier. It's also something I don't believe @shazow would be that adverse to, but I'm pretty sick at the moment so it's probable that I'm wrong. :)
",sigmavirus24,shazow
1307,2013-04-11 21:27:46,"@omatic-hacker is it possible to do `'тест'.encode('cp-1252').decode('utf-8')` or perhaps to `utf-16` or `utf-32` in the event that 8 isn't big enough? That would then be a string and might post correctly.
",sigmavirus24,omatic-hacker
1298,2013-04-09 20:03:03,"A documentation Pull Request, from @Lukasa? How unexpected! :grin: 

There comes a point where I should stop writing blog articles about Requests-y things and start actually adding some documentation to Requests. This will add the Transport Adapter to the API documentation and a description of them to the Advanced docs.
",Lukasa,Lukasa
1297,2013-04-09 18:51:25,"Requests had logging before 1.0 and it was torn out during the refactor. Take from that what you will. I'm entirely indifferent on the proposal. It's up to @kennethreitz no matter what.
",sigmavirus24,kennethreitz
1294,2013-04-06 15:08:59,"So while we could introspect those exceptions, we really only re-wrap 
exceptions so that the user doesn't have to do:



Those errors are strictly rising out of the SSL library being used so their 
being re-wrapped as SSLErrors is perfectly valid. What we would rely on is 
urllib3 doing the introspection but I doubt @shazow would want that there 
either. So that you can consider a ""Won't fix"" item.
",sigmavirus24,shazow
1294,2013-06-30 03:32:09,"@kennethreitz @Lukasa @GP89 is there any interest in this issue any longer? I'm not keen on the solution I had started to work out and the issue wasn't really ours in the first place as can be seen by the discussion on e4e7eb8
",sigmavirus24,kennethreitz
1294,2013-06-30 03:32:09,"@kennethreitz @Lukasa @GP89 is there any interest in this issue any longer? I'm not keen on the solution I had started to work out and the issue wasn't really ours in the first place as can be seen by the discussion on e4e7eb8
",sigmavirus24,Lukasa
1294,2014-10-23 14:12:38,"That's really bizarre @jgillmanjr I'm going to keep my neck out of that bug report but I trust @eriolv will be a great help to you.
",sigmavirus24,eriolv
1289,2013-04-04 19:00:40,"@t-8ch can explain this in detail but upgrading to 1.2.0 should fix all of your problems.
",sigmavirus24,t-8ch
1289,2013-07-22 07:25:40,"@AvivLander Have you tried the solution proposed in #1418?
",Lukasa,AvivLander
1289,2013-07-22 14:36:40,"@AvivLander Glad you managed to get to the bottom of this.
",Lukasa,AvivLander
1289,2013-12-28 11:30:07,"Riiiight, now that makes sense.

This is an unforeseen problem to do with how exception tracebacks are being reported in Python 3. [PEP 3134](http://www.python.org/dev/peps/pep-3134/) introduced this 'chaining exceptions' reporting that you can see in @bodiam's traceback. The purpose of this error reporting is to highlight that some exceptions occur in `except` blocks, and to work out what chain of exceptions was hit. This is potentially very useful: for instance, you can hit an exception after destroying a resource and then attempt to use that resource in the `except` block, which hits another exception. It's helpful to be able to see both exceptions at once.

The key is that the `TypeError` raised as the first exception is _unrelated_ to the subsequent ones. In fact, that's the standard control flow in `urllib3`. This means that the real exception that's being raised here is the `request.exceptions.ConnectionError` exception that wraps the `urllib3.exceptions.MaxRetryError` exception being raised in `urllib3`.

This is _not_ a Requests bug, it's just an ugly traceback introduced by Python 3. We can try to reduce the nastiness of it somewhat by refactoring the method in `urllib3` (though I don't believe it's possible: @shazow?), but that'll only remove the `TypeError` from the chain: the rest will stay in place.
",Lukasa,shazow
1289,2016-04-26 19:20:41,"Thank you @Daniel Hawkins  for response.

 My concern is not about Jira API.. its about the same  connection aborted issue error 10054.. we are discussing in this thread.
requests.exceptions.ConnectionError: ('Connection aborted.', error(10054, 'An existing connection was forcibly closed by the remote host'))

through python i am calling those APIs.

i can provide code if required
",sandeepnassa,Daniel
1287,2013-04-04 16:28:28,"@gazpachoking any insight? I'm going to take a look at this now
",sigmavirus24,gazpachoking
1287,2013-04-04 20:18:35,"So as I mentioned to @gazpachoking on IRC, using pdb, I seem to have traced this to an issue in cookielib which makes absolutely no sense. If this was working prior to 1.x there should be no reason for it to have stopped working.

For future adventurers though here are some relevant snips:


",sigmavirus24,gazpachoking
1286,2013-04-02 19:55:53,"Over the last few months there have been multiple issues raised about problems with string encodings. At best Requests is being inconsistent (#1250), and at worst perfectly valid combinations of input are failing (#1279).

I think it's time for Requests to enforce a very consistent behaviour when it comes to string encodings. We need to establish what format the data will be in when we pass it to urllib3, and where we're going to transform it. We need to be consistent so that when users inevitably encounter errors, we can make it very clear what they should have passed in.

@shazow has said that urllib3 wants the following input: bytes in the body, and native strings everywhere else. This defines our interface to urllib3.

To keep in sync with the above, we need to do the following things.
- On Python 2, any unicode strings anywhere on a `PreparedRequest` other than the body should be encoded as UTF-8. The body should also be encoded as UTF-8 if it is a unicode string (unlikely, but unless we can guarantee it won't be we should do the right thing).
- On Python 3, any bytestrings in a PreparedRequest should be decoded to unicode strings. Because we cannot make any intelligent guess, we will use Python's (stupid, stupid, _stupid_) locale-based decoding. This will almost-certainly throw exceptions if an incorrect encoding has been used, alerting the user.

Thoughts?
",Lukasa,shazow
1286,2013-04-03 19:45:10,"Ok, so I've taken a first-pass at this. Right now this doesn't do anything about coercing the `body` parameter to bytes, which it needs to.

I want lots of feedback on this. In particular, at some point @kennethreitz is going to need to step in and make an API decision. This change is a massive API change and affects the output of the PreparedRequest. I think it's worth it, but if @kennethreitz doesn't then I'm happy to stop work on this.
",Lukasa,kennethreitz
1273,2013-04-03 13:28:35,"I agree with @lukasa. In fact to expand a bit on his point about the last statement I'll say this: 

We emulate browser behaviour in some instances because in spite of the relevant RFC some servers will only behave ""correctly"" where ""correctly"" is how browsers expect it to behave. In those cases, RFC be damned, that's how we have to behave but at no point is requests a replacement for a browser, or a programmatic browser. Were that the case, we would have to keep a session history tracing back to the very first request. We provide histories for individual requests because being able to know that a redirect occurred is very important. It isn't so important on a session.

I was and remain frankly -0 on this because this existed well into 1.x (I think until 1.1.0) but when we asked if anyone was using it, no one replied until now. If it isn't used, it is just causing code-smell and will only cause confusion for future editors of and contributors to the project.

@Lukasa's solution is elegant and works very well. Even easier might be a hook like so:



Admittedly I don't have the time to test that, but I'm 90% sure it will work. :)
",sigmavirus24,lukasa
1267,2013-03-28 03:56:26,"@Lukasa if you have a chance, some code review would be great. Thanks
",sigmavirus24,Lukasa
1265,2013-04-08 16:48:41,"Thanks. I'll dig into this later this week but @t-8ch might have some insight on this that could wrap it up sooner.
",sigmavirus24,t-8ch
1265,2013-04-08 16:59:49,"Hm. I do recall there being an issue with versions of OpenSSL older than 1.0.0 but I don't recall if it was directly related to this exact message. Again @t-8ch would know more about it if he has the chance to take a look.
",sigmavirus24,t-8ch
1261,2013-03-23 01:21:55,"The tests caught this purely by coincidence with #1259. 

The issue is that we're now passing optional keyword arguments and typical hooks don't expect them causing exceptions to be raised.

In essence, this is a big breaking change and we will have to make it very clear to users that this is in the next version. requests/requests-kerberos depends on this but this is going to break a good amount of people's code.

Admittedly it only rears its head when we try and set the defaults via `kwargs` in `Session.send`. Again that's due to #1259/#1258 but @geoff-kruss is right in asserting that those defaults should be passed.

Assuming we keep the addition that causes this, we need a plan how to announce it.
",sigmavirus24,geoff-kruss
1261,2013-03-25 14:53:08,"So I'm personally in favor of keeping the existing behaviour and trying to make it as clear as possible to users that the behaviour has been changed.

Also, I forgot to include @mkomitee in on this ticket.
",sigmavirus24,mkomitee
1256,2013-03-22 11:00:11,"@LukaTes just saw that, I was going to delete this request...
",pradyunsg,LukaTes
1255,2013-03-22 02:48:01,"**tl;dr** It can not be easily added as far as I know and likely won't be considered by Kenneth given we're in a feature freeze.

Something about the API you show in the second snippet is oddly attractive but not at all what is within the realm of possibility for requests. I think a generator is your best bet in this instance but without knowing exactly what you're doing (beyond this) I can't speak as to the need for threading (although frankly I'm unsure of how that would be necessary for a generator).

With respect to the feature freeze please see #1165. As for adding this, all we do in requests is prepare a request to be sent using urllib3. You may be able to do something like this using urllib3 but the initial API will not be as simple as ours. It's a great library though and I think it has more functionality than what requests exposes to be frank. @shazow could definitely confirm or deny the ability to do this there though.
",sigmavirus24,shazow
1252,2013-03-21 12:48:28,"So here's my first guess (but I'll obviously look into this more): I think that OAuth1 might be generating unicode and opening the file in binary form will cause this issue due to how httplib sends the message body. Also this may be related to #1250.

@Lukasa, opinions?
",sigmavirus24,Lukasa
1252,2013-03-27 18:58:09,"So, really, this is because oauthlib is converting all the headers to unicode objects. We're then concatenating these unicode objects with the bytes of the file. Python tries to implicitly decode the bytes into unicode using the locale default codec, and obviously fails.

We can do the 'easy' fix and have `requests-oauthlib` just encode the headers using Latin-1, but that defers the problem. Alternatively, we can do the 'right' thing and take control of header encoding ourselves. I don't know if Kenneth is up for that, though. @kennethreitz, thoughts?
",Lukasa,kennethreitz
1252,2013-03-27 21:55:35,"Cc @shazow
",t-8ch,shazow
1252,2013-03-28 15:10:49,"So this can not block 1.2.0 unless @kennethreitz really wants it to. 

Perhaps to satisfy @michaelhelmick and company we should add a notice to the release that we realize that this is broken and a fix is being worked on in shazow/urllib3
",sigmavirus24,kennethreitz
1252,2013-03-28 16:45:27,"@michaelhelmick I hope you got better sleep than I did. :) And yes, as soon as this gets fixed, I would be certain to bug @kennethreitz about a bump to 1.2.1

And there's no need to apologize.
",sigmavirus24,kennethreitz
1242,2013-03-11 08:59:47,"I hate web server authors. :angry: 

This is a good patch. =) My only question is whether we need the overhead of a regular expression here? @sigmavirus24?
",Lukasa,sigmavirus24
1235,2013-03-10 00:31:38,"I still don't like requests performing this check either honestly. This is never going to affect the headers that we create and users can send whatever they like, especially by modifying a PreparedRequest object. That would also be the only place to add this check if @kennethreitz wants to add it, but I don't see a real need for it.
",sigmavirus24,kennethreitz
1233,2013-03-02 21:05:11,"/cc @sigmavirus24 @slingamn 

Anything pending for a new release? A ton of stuff has happened.

_Note_ I updated the title to reflect what is generally believed to be the correct version we should cut. --Ian
",kennethreitz,slingamn
1233,2013-03-02 21:05:11,"/cc @sigmavirus24 @slingamn 

Anything pending for a new release? A ton of stuff has happened.

_Note_ I updated the title to reflect what is generally believed to be the correct version we should cut. --Ian
",kennethreitz,sigmavirus24
1233,2013-03-03 02:44:05,"A damn lot has happened. Did you want multidicts ready for this? I'm still trying to think of a good way to handle the merging of keyword arguments. Also, I'm missing one error class from Flask, but my branch isn't really ready for a merge at all.

Also pinging the jet-lagged @Lukasa 
",sigmavirus24,Lukasa
1228,2013-03-02 04:14:47,"Maybe. Especially if you did it within the last week or two.

In that case, I'm going to assume that the SESSION ID cookie is set when you originally do a GET on the login page (unless you're setting it manually which it doesn't seem as if you are). A POST to it might not set it properly at the start. In other words, you might need that cookie set in the header when making the post, otherwise it will prevent you from logging in. However, if the above code works (not letting us handle the redirects) that would seem to disprove my idea.

Maybe @Lukasa has some insight into this. I'm 99% sure the logic for this is correct in `resolve_redirects`. :/
",sigmavirus24,Lukasa
1227,2013-03-01 23:58:04,"I'm +1 on this. It definitely seems worth adding to allow for extra information and it isn't so much a change in the API although @Lukasa and @kennethreitz may disagree.
",sigmavirus24,kennethreitz
1227,2013-03-01 23:58:04,"I'm +1 on this. It definitely seems worth adding to allow for extra information and it isn't so much a change in the API although @Lukasa and @kennethreitz may disagree.
",sigmavirus24,Lukasa
1227,2013-03-02 09:47:53,"I'm +0 on this. As you said @dmedvinsky, it's a minor refactor. I have no objection to it being in the library, and it's an internal API really, so not too problematic. I'm happy to go with whatever @kennethreitz feels is right here.
",Lukasa,kennethreitz
1222,2013-02-28 22:11:06,"Ah @t-8ch sorry for the confusion. I must have misread @Lukasa's response to #1221 last night (or had some rather wild dreams of what it said). Thanks for the correction.
",sigmavirus24,Lukasa
1220,2013-02-28 02:05:25,"Hmm. Actually, I think I'm -0.5 on reintroducing the old behaviour. It doesn't cost much and doesn't clutter the API much. We're in [feature freeze](http://docs.python-requests.org/en/latest/dev/todo/#feature-freeze), but if @kennethreitz decides to we might put this back. I'm leaving this for him. =)
",Lukasa,kennethreitz
1219,2013-02-27 23:04:22,"There is nothing wrong with this PR, but it won't be accepted. Requests is in [feature freeze](http://docs.python-requests.org/en/latest/dev/todo/#feature-freeze), and that goes doubly for changes that affect the API. As @sigmavirus24 pointed out in #1208, there are plenty of ways to specify this value.
",Lukasa,sigmavirus24
1218,2013-02-26 17:13:07,"I believe this is a duplicate and related to shazow/urllib3#139

@Lukasa thoughts?
",sigmavirus24,Lukasa
1218,2013-02-26 17:35:29,"No worries. We're just waiting for that pull request to land before we can support this. I believe @t-8ch has made the case that you can use `http://` for your proxy and it should work, but I've no practical knowledge of how proxies work in all candor.
",sigmavirus24,t-8ch
1215,2013-02-24 20:48:44,"I didn't like that pull request anyway. Like we said on that issue, the user should go way out of their way to break things. @kennethreitz do you mind if I revert and push directly to requests? It's really not much of a difficult fix.
",sigmavirus24,kennethreitz
1211,2013-02-27 18:44:38,"My particular issue was a bug in my code.  This still sends GET requests.
Looks like I missed the documentation for this, so maybe a documentation
issue but this report is otherwise invalid.
On Feb 27, 2013 1:41 PM, ""Philip James"" notifications@github.com wrote:

> @bluefoxicy https://github.com/bluefoxicy @Lukasahttps://github.com/Lukasais this still an open issue? It looks like its being resolved for the
> opener offline.
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/issues/1211#issuecomment-14191018
> .
",bluefoxicy,Lukasahttps
1209,2013-03-01 23:56:44,"@ciphercast you're on the list now too. (Don't worry the list is as ambiguous as it seems ;))
",sigmavirus24,ciphercast
1209,2013-07-19 08:02:11,"@maxbane @ciphercast There is a requests HTTPS proxy ready version to test, can you give it a try: https://github.com/shazow/urllib3/pull/170#issuecomment-21234629

@sigmavirus24 Anyone else on your list? :)
",schlamar,ciphercast
1201,2013-02-19 15:34:49,"@kennethreitz this seems sensible to me. I won't send a Pull Request though until you say so.
",sigmavirus24,kennethreitz
1198,2014-02-09 09:25:32,"Whether or not the fix belongs in urllib3 is totally down to @shazow. Given that urllib3 by default _does_ retry (3 times IIRC), it may be that he wants to keep urllib3's behaviour as is. Pinging him to get his input.

We vendor urllib3 to avoid some dependency issues. Essentially, it means we're always operating against a known version of urllib3. This has been discussed at excruciating length in #1384 and #1812 if you want the gritty details.
",Lukasa,shazow
1198,2014-02-09 13:50:08,"Phew gritty but informative. @shazow these are just a few thoughts I had -- raising a RequestError rather than MaxRetryError as above. Really I think I better understand the MaxRetryError after checking out [urlopen](https://github.com/shazow/urllib3/blob/951ea12ba18103e5434e751246c0895e54fef211/urllib3/connectionpool.py#L382).

Double edit: Really even just a kwarg so one can `raise MaxRetryError(retries=0)` and alter the message on `retries==0`.
",ksnavely,shazow
1198,2014-10-05 17:37:27,"@kevinburke thoughts?
",sigmavirus24,kevinburke
1198,2014-10-05 19:47:08,"Need a little more time

## 

Kevin Burke
phone: 925.271.7005 | twentymilliseconds.com

On Sun, Oct 5, 2014 at 10:37 AM, Ian Cordasco notifications@github.com
wrote:

> @kevinburke https://github.com/kevinburke thoughts?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/1198#issuecomment-57945403
> .
",kevinburke,kevinburke
1193,2013-02-14 04:23:44,"In #1188 @brandon-rhodes pointed out that we throw some uninformative exceptions when problems arise in urllib3. This would catch the exception and wrap it in a Requests exception, then rethrow it with its original traceback.

I'm on the fence about whether we should do one of the following:
1. Create a new Requests exception for this case (`requests.exceptions.TransportError`?)
2. Import urllib3's exceptions into `requests.exceptions` and export them as our own? (Not great)

Opinions welcome (@sigmavirus24?)
",Lukasa,brandon-rhodes
1193,2013-02-14 04:23:44,"In #1188 @brandon-rhodes pointed out that we throw some uninformative exceptions when problems arise in urllib3. This would catch the exception and wrap it in a Requests exception, then rethrow it with its original traceback.

I'm on the fence about whether we should do one of the following:
1. Create a new Requests exception for this case (`requests.exceptions.TransportError`?)
2. Import urllib3's exceptions into `requests.exceptions` and export them as our own? (Not great)

Opinions welcome (@sigmavirus24?)
",Lukasa,sigmavirus24
1193,2013-02-17 22:55:30,"So it turns out that there is no simple way to get around this Syntax Error. `six` has a good solution, but we currently don't vendor it in. The way I see it, we have three options:
1. Use the version of `six.py` in urllib3. This is bad: `six.py` is an implementation detail in urllib3, which is itself an implementation detail of Requests. -1
2. Add the bit of `six.py` that we need to our `compat.py`. This has a minimal effect on our code, but is a bit weird. +0
3. Vendor in a copy of `six.py`. This is the cleanest, but adds a whole new Python file from which we only need one line. Probably a small perf hit too. +0

@kennethreitz, @sigmavirus24: I don't know which is best from options 2 and 3. Thoughts?
",Lukasa,kennethreitz
1192,2013-02-14 03:57:17,"Exactly my view. =) I thought I'd open this PR so we can see what it entails, but I think we'll probably not fix what isn't broken. Just want to see what @kennethreitz thinks.
",Lukasa,kennethreitz
1191,2013-02-14 03:34:10,"Fixes #1189 as confirmed by @Lukasa
",sigmavirus24,Lukasa
1189,2013-02-14 00:11:57,"/cc @sigmavirus24 
",Lukasa,sigmavirus24
1189,2013-02-14 03:15:22,"@Lukasa you can also try sigmavirus24/requests @fix1189
",sigmavirus24,fix1189
1188,2013-02-14 04:34:07,"@shazow, we need your help!
",kennethreitz,shazow
1188,2013-02-15 17:36:31,"Sorry I'm off the grid until Tuesday.
- Sent from rogue wilderness wifi
  On Feb 13, 2013 8:34 PM, ""Kenneth Reitz"" notifications@github.com wrote:

> @shazow https://github.com/shazow, we need your help!
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/issues/1188#issuecomment-13532802.
",shazow,shazow
1185,2013-02-13 04:05:15,"Fixes #1183 

/cc @mkomitee
",sigmavirus24,mkomitee
1183,2013-02-12 18:41:56,"Ah, I see what you mean. We resolve redirects before dispatching hooks. If that came before the redirect resolution, that would mean you could catch the redirect, correct?

Semantically I think it should be there anyway. If a redirect is resolved, every request except the first has the hook called on it (because each subsequent request calls `send` with `allow_redirects=False`). It should before the resolution unless @kennethreitz or @Lukasa disagree.
",sigmavirus24,kennethreitz
1183,2013-02-13 02:36:08,"@mkomittee. Assuming that we move the call to `dispatch_hook` it would occur before being redirected. So what would happen is the following:



So after this change, if there is a 302 returned, (assuming the auth handler doesn't call `dispatch_hooks` itself) then Session's send method will handle it.
",sigmavirus24,mkomittee
1182,2013-02-12 04:13:22,"@Miarevo I'm guessing it was either a change in our proxy handling or a change in urllib3's. Nice and specific eh? :-P Frankly though, @Lukasa has been more engaged in the proxy work lately so he might have an idea.
",sigmavirus24,Lukasa
1182,2013-02-13 16:30:36,"@t-8ch we do very little work with proxies. If I remember our code correctly, we just pass it to urllib3 and they handle all of that for us. I guess we _could_ validate it before passing it to urllib3 though. I'll defer to @kennethreitz and @Lukasa about whether we should be doing that or not though.
",sigmavirus24,kennethreitz
1181,2013-02-12 08:28:31,"I don't recall off the top of my head where I've seen this, but I believe latin1 (or something similar) is recommended for header encodings. @mitsuhiko do you know?
",kennethreitz,mitsuhiko
1180,2013-02-11 00:40:52,"Reported by Filefly on IRC and written with help from @gazpachoking.
",sigmavirus24,gazpachoking
1177,2013-02-10 10:18:04,"I have added the requested changes, but I'm not sure I understand you, @piotr-dogrobost. Using `builtin_str` will still be of type `str` in Python 3, so will be a unicode object. Do you think we should convert to `bytes` here (causing `str` in Python 2 and `bytes` in Python 3)?
",shezi,piotr-dogrobost
1171,2013-02-07 20:04:09,"@shazow any clues?
",sigmavirus24,shazow
1166,2013-02-06 13:41:46,"We could use @kanzure's project which relies on HTTPretty or we could use mock like I do in github3.py. If you chose the latter, I'll be happy to work on the tests.
",sigmavirus24,kanzure
1163,2013-02-05 12:28:54,"I suspect it got pulled out during the big refactor. I'm not opposed to having it back, depends what @kennethreitz thinks.
",Lukasa,kennethreitz
1163,2013-02-05 17:21:38,"Hm, the docs didn't indicate that. The problem is that the entire Configuration API was deliberately removed in the refactor. As such, there isn't really a `safe_mode` either, just the in-between that always existed. A request is a request and any exception that occurs during that is raised. I doubt @kennethreitz would like to re-add this (given there is likely not a very elegant way of doing so without adding the Configuration API back), but I'll re-open it to be fair.
",sigmavirus24,kennethreitz
1155,2013-04-16 18:48:49,"Don't forget @Lukasa. He's done and is doing far more than me.
",sigmavirus24,Lukasa
1155,2013-04-16 18:50:39,"(+ @Lukasa!)
",ihodes,Lukasa
1155,2013-04-18 21:44:07,"> Is requests attempting to have any kind of parity with werkzeug's structures?

I haven't seen any explicit mention of this. And yes I read Kenneth's blog post but @core is anything except active. (I'd be happy to work on it with them though. :-P) And really I think forcing a brutally sane API is more important especially since we will be exposing these structures to users through `PreparedRequest` and `Response` objects.

> I know that from my perspective it would be great if I could pass a Werkzeug `MultiDict` and have it work in Requests as-is.

The problem with this is trying to catch every single case. We rely on the object being iterable with two-tuples or being able to be coerced into that format. Certainly `MultiDict.items()` returns that, but it doesn't return to us what the user expects. Right now we have to test if the object we have is an instance of `MultiDict` and while that seems ok now, that's a slippery slope. What do we next have to test to assure that fractured APIs are allowed?

> just seems like we'd be diverting too far from simple,

I agree.

We don't currently use a `CaseInsensitiveDict` on `PreparedRequests` so using a plain `OrderedMultiDict` there is fine. As for returning headers on a response object, perhaps I was too ambitious in saying that it was necessary. I'm okay leaving it as it is for now until a better consensus can be reached.
",sigmavirus24,core
1151,2013-01-30 18:54:33,"Yeah, my intuition was correct:



@kennethreitz are you okay with req (or the Request object) being mandatory as a parameter, e.g., `Session.send`'s signature becomes: `def send(prepared_request, request, **kwargs):`?
",sigmavirus24,kennethreitz
1151,2013-01-31 14:51:39,"ping @kennethreitz, thoughts on changing `Session.send`'s signature?
",sigmavirus24,kennethreitz
1148,2013-01-28 17:16:44,"per @kennethreitz's request
",sigmavirus24,kennethreitz
1138,2013-01-24 18:45:37,"@sigmavirus24 Your point about streaming responses is correct; I'm not sure that a simple 'end - start' timer is useful for streaming responses _anyway_, but it IS useful for everything else (which is what I need).

As far as whether it's better as a method or a property - I honestly don't care. It's such a small change that if it gets rewritten to be a single property instead of a method, hooray! All I want is the ability to _get_ this information, from requests, without having to patch it myself every time I use it. @kennethreitz didn't seem to be convinced that this functionality even _belongs_ in requests, so I'm hoping that even if this particular commit doesn't get accepted, the general idea is considered to be worthwhile and some kind of implementation makes its way in.
",clee,kennethreitz
1138,2013-01-25 04:09:10,"Btw, @kennethreitz, as we discussed on IRC, :+1:. :shipit: 
",sigmavirus24,kennethreitz
1125,2013-01-23 02:49:05,"@sigmavirus24, all good?
",kennethreitz,sigmavirus24
1125,2013-01-23 03:43:04,"Seems it to me. I'm convinced it does what he says.

Kenneth Reitz notifications@github.com wrote:

> @sigmavirus24, all good?
> 
> ---
> 
> Reply to this email directly or view it on GitHub:
> https://github.com/kennethreitz/requests/pull/1125#issuecomment-12579122
",sigmavirus24,sigmavirus24
1115,2013-01-19 01:40:44,"The documentation describes request.json as a function, but in `'0.14.2' it is a property. This appears to be changed in master, which is correct and future compatible? The`@property` decorator produces nice syntactic sugar, and I would support the continued use of it to make accessing the json body even easier.
",corydolphin,property
1102,2013-01-15 11:30:17,"It seems likely that `requests/cacerts.pem` is affected by the recent compromise of TURKTRUST certificates. However, I'm not sure what exactly we need to do to fix this. Removing both of the root certs we currently have in there is probably wrong.

Maybe @saschpe can advise?
",slingamn,saschpe
1099,2013-01-23 13:31:08,"@kennethreitz this is definitely a pull request you should merge. It complements #1117 which you already merged.
",sigmavirus24,kennethreitz
1092,2013-01-10 07:04:55,"@kennethreitz Are you closing/reopening in order to restart the Travis build?
",jianli,kennethreitz
1084,2013-01-26 17:48:24,"@cbare you're correct about that call to `request` not being complete.

So let me just walk through the steps of the request before submitting a pull request to fix that.

In a normal case (not chunked encoding), the user calls `requests.post(url, data={'key': 'value'}, files={'foo': open('foo', 'rb')})`. In this case, the `Request` object is created and prepared turning into a `PreparedRequest` which is what we receive as `req` in `resolve_redirects`. This is stored in `req.body`. Since this is prepared, we can do this (in `resolve_redirects`):



This works because when `data` receives a string, it sends that.

The problematic case is when the user is using chunked encoding (I think). The problem is, I'm not entirely sure what happens with chunked encoding at the moment. Maybe @kennethreitz can explain how that works because I haven't presently looked at it at all.

I could be wrong and it could all be handled as one case though.
",sigmavirus24,kennethreitz
1082,2013-01-03 19:23:39,"Related issues: #390, #400, #409, #421, #424.
#400 seems to have the most information.

@kennethreitz
Why was `test_unicode_headers` test (see #400)  and other valuable tests removed?
",piotr-dobrogost,kennethreitz
1082,2013-02-11 12:49:00,"@kennethreitz
This issue is still reproducible with current `requests==1.1.0` and `python==2.7.3`.

I have created a pull-request #1181 that fixes the issue, please check it.
",denis-ryzhkov,kennethreitz
1075,2012-12-31 01:08:30,"Personally, I'm -1 on adding `netparse` as a requirement. Not sure what @kennethreitz will think though.
",sigmavirus24,kennethreitz
1070,2012-12-26 02:26:13,"Yeah, I'm not sure if we need to re-pass the parameters. This is a good catch @alefnula. If you want to ready a commit that removes this, feel free to. I'm sure @Lukasa or @kennethreitz will weigh in after the holidays.
",sigmavirus24,kennethreitz
1070,2012-12-26 02:26:13,"Yeah, I'm not sure if we need to re-pass the parameters. This is a good catch @alefnula. If you want to ready a commit that removes this, feel free to. I'm sure @Lukasa or @kennethreitz will weigh in after the holidays.
",sigmavirus24,Lukasa
1065,2012-12-23 10:44:14,"Given the discussion on #1033, I removed the code support for OS certificate bundles entirely, and indicated where packagers can modify the code to support their environments.

Travis will fail, but that's just because it's red ever since #1064. The tests pass locally.

cc @sagarun @saschpe
",slingamn,sagarun
1065,2012-12-23 10:44:14,"Given the discussion on #1033, I removed the code support for OS certificate bundles entirely, and indicated where packagers can modify the code to support their environments.

Travis will fail, but that's just because it's red ever since #1064. The tests pass locally.

cc @sagarun @saschpe
",slingamn,saschpe
1060,2012-12-22 18:49:16,"Agreed @lukasa
",sigmavirus24,lukasa
1056,2012-12-22 12:14:23,"So, I've got a branch with a possible fix for this issue [here](https://github.com/Lukasa/requests/tree/proxy). @python273 and @leoluk, would you like to try downloading it and making the same request? If it works, I'll submit a Pull Request.
",Lukasa,leoluk
1053,2013-01-19 13:00:57,"Taking another look at this, I think this fix might belong in urllib3. The `urllib3.poolmanger.ProxyManager` class has a `_set_proxy_headers` method which should probably set the Host header. Does that sound right, @shazow? If it does, I can offer you a PR with the fix.
",Lukasa,shazow
1053,2013-02-09 03:19:25,"@socketubs, I think @Lukasa is close to fixing this for you. (Just to keep you up to date)
",sigmavirus24,socketubs
1047,2012-12-19 19:00:56,"I think that the JSON response method is missing the `@property` decorator.

Calling it like `r.json` obviously returns `<bound method Response.json of <Response [200]>>`.

https://github.com/kennethreitz/requests/blob/master/requests/models.py#L522
",jpadilla,property
1041,2012-12-19 03:32:25,"Pending shazow/urllib3#132 we can add that line I mentioned after calling `release_conn` and `r.raw.close()`. This should solve the issue completely.

The other thing is that `Response.close()` doesn't return anything since `release_conn` doesn't return anything. If we want, we can also stick `self.raw.close()` in there. Any objections @kennethreitz ?
",sigmavirus24,kennethreitz
1041,2012-12-19 03:52:38,"See the pull request I referenced. @shazow and I are discussing whether this 
behaviour should implicitly be part of the `release_conn` method.
",sigmavirus24,shazow
1035,2012-12-18 14:40:31,"Actually I may have spoken too soon. Looks like @kennethreitz just tagged v1.0.3 which means it is probably on PyPI. Try doing `pip install -U requests` and let us know if this is resolved (which it should be).
",sigmavirus24,kennethreitz
1025,2012-12-17 22:26:15,"See @kennethreitz's comment here: https://github.com/kennethreitz/requests/issues/1008
",sigmavirus24,kennethreitz
1021,2012-12-17 19:10:39,"@pankajn17 

How did you get this pretty stack with source context?
",piotr-dobrogost,pankajn17
1017,2012-12-17 17:58:08,"Yup, that looks wrong.

@kennethreitz, I haven't ramped up on the adaptors stuff yet, but it feels like [line 130 of adapters.py](https://github.com/kennethreitz/requests/blob/master/requests/adapters.py#L130) should read `conn = ProxyManager.(self.poolmanager.proxy_from_url(proxy))`. You'd need to import `ProxyManager` too.

If I get up to speed tonight I'll branch and raise a PR, but I thought I'd put this here as I might not.
",Lukasa,kennethreitz
1008,2013-01-18 15:30:09,"I broke it while enabling SNI in urllib3.
It will be fixed by shazow/urllib3#130 (ping @shazow)
Try removing the `ssl_wrap_socket()` function from `requests.packages.urllib3.util.py`
",t-8ch,shazow
1008,2013-01-28 01:50:56,"Per @sigmavirus24, I'm watching this as a possible root cause of a similar-looking problem in Gittip, hitting Twitter: https://github.com/zetaweb/www.gittip.com/issues/531.
",whit537,sigmavirus24
1001,2012-12-12 17:05:10,"@kennethreitz,

If I'm not mistaken, the Travis CI builds were failing before this pull request. Correct?
",matthewlmcclure,kennethreitz
997,2012-12-12 12:51:17,"Yes, this is definitely a bug. =) Good spot, and excellent detail in your report, thanks so much!

I think Requests' documentation is documenting the behaviour we want. We can get this by adding a function call in line 558 (or some inline code) that puts the scheme into the proxy URL if it's missing. Thus, to carry on the example from [here](http://docs.python-requests.org/en/latest/user/advanced/#proxies), we'd change `""10.10.1.10:3128""` to `""http://10.10.1.10:3128""` and `""10.10.1.10:1080""` into `""https://10.10.1.10:1080""`.

The only worry here is that people might have code that passes their HTTPS traffic through an HTTP proxy, which this would break (unless the expressly indicate the scheme in their proxy URL). @kennethreitz, is this a problem?
",Lukasa,kennethreitz
995,2012-12-11 16:24:07,"We could expose this in the refactor as a connection parameter.

@shazow, what would need to change in urllib3?
",kennethreitz,shazow
991,2012-12-06 16:51:13,"Per @kennethreitz's implied intent in 5c1bc201c46297d39d591e5d30e0e65a989bd22c, this change implements the Apache 2.0 license across requests, in every location I could find a license specified.
",lyndsysimon,kennethreitz
989,2012-12-06 20:52:20,"Change your line:



to:



You'll find this works. =)

@sigmavirus24: IIRC, you worked on the streaming stuff last. Why is the default iter_lines chunk size so large (10240 bytes)? Is there a design decision I don't know about there?
",Lukasa,sigmavirus24
989,2012-12-06 21:48:26,"@lukasa I think you have me mistaken for someone but if I remember correctly 
1024 was considered because that's 1MB. It isn't unreasonable, most web stuff 
is much larger than that and it is configurable.
",sigmavirus24,lukasa
989,2012-12-06 21:52:26,"Also @gdamjan it was already decided if I remember correctly so you can close this if you feel your needs were met.

At this point, with the refactor coming up we could change it to half the current size since we can really announce the breaking changes then. But that still wouldn't fix his problem. To try to phrase this how @kennethreitz will see it, 90% of people's cases will be sufficiently met by this default, and probably 10% will be affected negatively. He likes to ignore that 10% if possible. (Paraphrasing from one of his talks.)
",sigmavirus24,kennethreitz
988,2012-12-05 16:58:31,"Not sure @kennethreitz will approve of the extra parameter. At best you might be able to add that as a configuration parameter. Even so, as I already said, the majority of the trace work only makes sense if it is performed in urllib3. If it doesn't exist there, you'll be hard pressed to see it introduced in requests.
",sigmavirus24,kennethreitz
980,2012-12-01 14:39:18,"I renamed my Github account from @gwrtheyrn to @dbrgn.
",dbrgn,dbrgn
980,2012-12-01 14:39:18,"I renamed my Github account from @gwrtheyrn to @dbrgn.
",dbrgn,gwrtheyrn
975,2013-01-23 13:44:30,"@lukasa I am holding your feet to the fire on this one. ;)
",sigmavirus24,lukasa
973,2012-11-28 16:39:09,"poke @kennethreitz 
",sigmavirus24,kennethreitz
968,2012-11-27 19:03:55,"@kenneth: You'd have to fork it as a separate project, relicensed under the LGPL, and then vendor that forked project into Requests. Not the end of the world, but a hassle.
",Lukasa,kenneth
967,2012-11-27 18:36:24,"@idan?
",kennethreitz,idan
964,2012-11-27 11:27:21,"Good point about its location. Clearly I'm so familiar with the documentation I just assume everyone does what I do and skip straight to Quickstart.

Regarding the tone, I think we have a legitimate difference of opinion here. My reading of that section is that the tone isn't pontifical but informative. It does not say ""Do not use the GPL"", it says ""Don't use the GPL _without thinking_"", which is very different.

That said, I'm happy for @kennethreitz to weigh in here if he feels differently.
",Lukasa,kennethreitz
962,2012-11-26 19:04:46,"@kennethreitz, before you merge this, can we get a consensus on whether we officially support 3.1 and 3.3? If we do, it might be worth getting TravisCI to test on them so we don't regress.
",Lukasa,kennethreitz
962,2012-11-26 19:18:14,"I'm fairly confident that @travisci never added python 3.3 support although I haven't received any answer either way. My reasoning for claiming the supported versions were up to and including 3.3 was [setup.py](https://github.com/kennethreitz/requests/blob/develop/setup.py#L67). If that is wrong, I don't want to be right ;)
",sigmavirus24,travisci
962,2012-11-26 20:24:54,"@Lukasa I should have added that I'm +10 on waiting for @kennethreitz to weigh in. ;)
",sigmavirus24,kennethreitz
961,2012-11-25 21:39:01,"Bizarrely, I can't repro this. Using OS X, but otherwise the exact same setup. @sigmavirus, can you repro this?
",Lukasa,sigmavirus
961,2012-11-25 22:31:39,"@Lukasa: Thanks for the positive feedback.  I aims to please!

I'm definitely a requests newbie, but so far I love it.  And I have to say, the response here (on a Sunday, no less!) was freaking awesome.  Thanks, @all!
",inactivist,all
960,2012-11-25 20:41:08,"@kennethreitz 

How about creating new project requests-hooks with various hooks for requests? I could maintain it if you want.
",piotr-dobrogost,kennethreitz
960,2012-11-25 23:39:31,"@piotr-dobrogost both ;)

Also, if a separate repository is not acceptable to @kennethreitz, my next choice would be to add a sort of ""Recipes"" section to the docs which could include hook recipes.
",sigmavirus24,kennethreitz
956,2012-11-24 21:44:17,"This is an attempt to solve issue #910 (and hopefully #896 by association).

This solution is unlikely to cause a regression except in one specific case, where the form data _is_ urlencoded but doesn't have the correct Content-Type header set. If we think that's a likely scenario, I can go back and make the test for that case less restrictive. The flip side is that we will not mistakenly include data we shouldn't.

There aren't any tests for OAuth functionality in Requests, so I have no idea if this makes things worse or better. My local tests seem to suggest that everything is ok though, so let's assume it's all awesome. =D

NB: I'm aware that we've moved the OAuth stuff to requests/requests-oauth. I'll raise an equivalent PR over there if we decide to use this one, but there are a few open issues here that make raising this PR here useful as well.

@idan: Does this look right to you?
@michaelhelmick: Does my branch resolve your issues?
",Lukasa,idan
956,2012-11-24 21:44:17,"This is an attempt to solve issue #910 (and hopefully #896 by association).

This solution is unlikely to cause a regression except in one specific case, where the form data _is_ urlencoded but doesn't have the correct Content-Type header set. If we think that's a likely scenario, I can go back and make the test for that case less restrictive. The flip side is that we will not mistakenly include data we shouldn't.

There aren't any tests for OAuth functionality in Requests, so I have no idea if this makes things worse or better. My local tests seem to suggest that everything is ok though, so let's assume it's all awesome. =D

NB: I'm aware that we've moved the OAuth stuff to requests/requests-oauth. I'll raise an equivalent PR over there if we decide to use this one, but there are a few open issues here that make raising this PR here useful as well.

@idan: Does this look right to you?
@michaelhelmick: Does my branch resolve your issues?
",Lukasa,michaelhelmick
956,2012-11-26 08:31:02,"I'll merge once @idan approves :)
",kennethreitz,idan
953,2012-11-24 13:21:17,"I recall @kennethreitz first said he did not like the patch, then after a while he said he actually could accept it but then nothing more happened. If you compare our pull requests you notice that your change to `Session._send_request()` is not enough as this is not being called when `return_response` param is false (https://github.com/kennethreitz/requests/blob/v0.14.2/requests/sessions.py#L237) The change must be made on `Request.send()` level which always is called and that's what I did in my patch.
",piotr-dobrogost,kennethreitz
952,2012-11-23 10:54:15,"Currently refactoring the codebase for v1.0. Cleaning things up.

Plan:
- Replace the current `prefetch` parameter with `stream`. 
- Make all uploads stream.

Is there any downside to streaming all uploads? @mitsuhiko quick opinion?

---

Related: #895
",kennethreitz,mitsuhiko
951,2012-11-24 11:24:35,"@Lukasa @kennethreitz #939 solves these problems without any major disadvantage I can think of. Furthermore it has the slight advantage of letting the package maintainers easily add separate versions of any other package `requests` may depend on in the future without having to follow this same forking approach everytime. But this is just my humble opinion, of course :-)
",ghost,Lukasa
944,2012-11-18 12:15:12,"It's my understanding that the general intent is that `requests` will stop supporting Python 3.2 and move to supporting Python 3.3, in no small part because of this syntax.

@kennethreitz, do you want this to be fixed now, or should we just leave this until it fixes itself in 3.3 and so consider this a ""won't fix""?
",Lukasa,kennethreitz
944,2012-11-23 10:24:57,"Yeah, it'll be super simple. All the actual work is done by @idan and team :)
",kennethreitz,idan
929,2012-11-08 13:59:45,"Long short story, the associated commit aims at fixing #916 by refactoring the setup script and moving dependencies out of the package itself... These are now downloaded from PyPI whenever necessary before installation (via `install_requires`) while keeping compatibility with Python 2 and Python 3 in the same codebase (`chardet` is now resolved via two external requirements file).

The only downside of this is that one cannot import `requests` in `setup.py` (since that would require importing `charder`, which is not available at that time). Thus, the version is repeated both in `setup.py` and `requests/__init__.py`. I'll have a go at that once I find some more time, but for now this seems like acceptable for me.

**Note:** The package's version had been kept as `0.14.1` in the previous release. I decided to amend that to `0.14.2` instead of `0.14.3.dev0` or something like that, since maybe @kennethreitz wants to manage that himself. :-)
",ghost,kennethreitz
924,2012-11-25 02:00:46,"I'm unaware. @shazow?
",kennethreitz,shazow
910,2012-11-24 11:35:35,"I seem to remember this having come up before, though I can't find a good example of this issue.

@idan, you're way more familiar with OAuth than me. Is the principle supposed to be that OAuth doesn't include the body for signing purposes, or is it just specifically not supposed to sign the body for `multipart/form-data`?
",Lukasa,idan
907,2012-10-23 06:23:34,"See #889 for motivation.

My understanding is that this was added in order to fix tests, which were trying to POST `__file__` and failing on the second run, once `__file__` began to refer to a .pyc instead of a .py?

It looks like postbin/httpbin no longer choke on .pyc files, so we should be able to omit this.

cc @sigmavirus24
",slingamn,sigmavirus24
907,2012-10-23 13:55:15,"This wasn't exactly my decision/idea but yeah, I don't think the problem was solely with HTTPbin. I don't remember exactly since it was happening in July/early August, but I was seeing it with Amazon S3 if I remember correctly (I was using it via the GitHub Downloads API). But my memory is too hazy to be 100% certain. _shrug_ It's @kennethreitz's call anyway. I'm swamped with other stuff at the moment (I'm not even working on my own projects).
",sigmavirus24,kennethreitz
907,2012-11-29 01:26:14,"Personally I am sympathetic towards the creation of ""native"" packages for Requests. On my own machines, I use the Fedora distribution's RPMs of Requests (courtesy of @sagarun), not pip or easy_install.

However:
1. An unavoidable aspect of packaging is maintaining a (hopefully small) set of downstream patches
2. Requests does not have an obligation to interoperate perfectly with every packaging system; if Requests has become _unusually_ difficult to package (and the issue here doesn't seem to qualify), then we can work with the packagers to improve it
",slingamn,sagarun
905,2012-11-04 19:59:26,"@cynikalX - you can use foxx branch of requests - last time I checked, https through proxy worked there (the only trick is that after cloning his repository you have to switch to the branch on which he did the changes).

Another option is using an old requests branch that works on top of urllib2. Last option is to use some mitm-capable proxy like fiddler, which is aware that is has to communicate with other proxies and does the HTTP CONNECT.

I also agree that HTTPS and SOCKS proxy support could be introduced separately.
",dahpgjgamgan,cynikalX
905,2012-11-26 18:05:26,"@machinae two things:

First, @kennethreitz is working on a rewrite using transport adapters which should alleviate or fix this issue.

Second, if you want the patch at shadow/urllib3 to get through so it can be used here, go and work on that issue. It's already known to be am issue that affects plenty of people but complaining won't help anyone. I'm sure @shazow would love help on that pull request but until he gets some, it isn't going anywhere fast.
",sigmavirus24,kennethreitz
905,2012-11-26 18:05:26,"@machinae two things:

First, @kennethreitz is working on a rewrite using transport adapters which should alleviate or fix this issue.

Second, if you want the patch at shadow/urllib3 to get through so it can be used here, go and work on that issue. It's already known to be am issue that affects plenty of people but complaining won't help anyone. I'm sure @shazow would love help on that pull request but until he gets some, it isn't going anywhere fast.
",sigmavirus24,shazow
900,2012-10-22 12:46:44,"I made one last change to correct an issue where you could have a crash if Python was built without SSL support.

Also, I've submitted all of these changes to the urllib3 project, and it looks like @shazow is interested in merging, so you should be good there.
",dandrzejewski,shazow
896,2013-01-25 16:11:27,"@slingamn I've been super busy, but I guess the fixes are currently in requests but we're waiting on requests_oauthlib 0.3.0 to be released? I think..
",michaelhelmick,slingamn
889,2012-10-11 02:43:42,"@sagarun might know about dist_rpm and the LICENSE issue.

Did we set PYTHONDONTWRITEBYTECODE in order to fix a test that was trying to upload `__file__`? I can submit a pull request that posts a normal text file instead, or works out a relative path to the original Python source file.
",slingamn,sagarun
875,2012-11-27 01:04:51,"@Lukasa @shazow @kennethreitz opinions on this? It seems reasonable to me, but I'm not as familiar with urllib3 honestly.
",sigmavirus24,kennethreitz
875,2012-11-27 01:04:51,"@Lukasa @shazow @kennethreitz opinions on this? It seems reasonable to me, but I'm not as familiar with urllib3 honestly.
",sigmavirus24,shazow
875,2012-11-27 01:04:51,"@Lukasa @shazow @kennethreitz opinions on this? It seems reasonable to me, but I'm not as familiar with urllib3 honestly.
",sigmavirus24,Lukasa
871,2012-09-30 11:18:32,"@piotr-dobrogost I was thinking the same thing.
",splee,piotr-dobrogost
869,2012-09-26 20:52:11,"Those are all just warnings. Requests vendorizes a package called chardet which is specifically for python2. It doesn't break python3, but it gives those horrible and ugly warnings that you saw. A package maintainer complained about the same thing (#832). #858 is also related but only in that @craigholm was hoping to find a way to conditionally import/install chardet.
",sigmavirus24,craigholm
868,2012-09-26 19:18:24,"https://github.com/sigmavirus24/requests/commit/4dd3d1a1a2534f2996a368ebe26114bf974e15f9#commitcomment-1904600

For reference, I'm going to copy the comments into here.

---

@piotr-dobrogost

Explicit type checking is code smell. We should be checking for iterability not types. See http://stackoverflow.com/questions/1952464/

---

@sigmavirus24

I was following the style of the rest of requests, if wanted I'll clean up this and the other places I saw it.

---

@piotr-dobrogost

@kennethreitz What do you think?

---

@kennethreitz 

iterability is always preferred, but not always possible in requests because of the way certain types work (strings, dicts)

---

@sigmavirus24 

@kennethreitz correct me if I'm wrong, but I believe you're against this. And if I remember correctly I had submitted an earlier PR that tried to do something similar to what @piotr-dobrogost is suggesting in `_encode_data` which failed when passed a dictionary. For reference in python 2 and 3, the following works



Which may not be entirely obvious to some and certainly might seem like unintended behavior. Likewise,



We could always do the complement though (which is far less obvious):



---

@piotr-dobrogost 

All we should care here is that we want iterable. It's up to the user to pass iterable he thinks is appropriate. If someone wants to pass ""random"" iterable then it's not our business to know better and reject it. Yes, there is more type checking in the Requests and I think it should be cleaned as well.

---

@piotr-dobrogost 

@shazow What's your opinion?

---

@shazow 

I would vote for making an is_foo helper somewhere nearby which contains the ""is iter and not str/dict"" logic or whatever we want it to be. Then re-use that where appropriate. Also name foo something descriptive. is_plural?

As for the logic, I'd be +0 on {iter and not str/dict}. I imagine people may want to pass in arbitrary generators and whatnot.

---

@sigmavirus24 

I think @piotr-dobrogost would just rather the logic `hasattr(foo, '__iter__')` rather than excluding `str`s and `dict`s.

---

@shazow 

In the context of hooks, shouldn't the check just be if it's callable? If not, then extend?

(My previous suggestion was more towards the general style of Requests, rather than this hook-specific example.)

---

@sigmavirus24 

Right, I was following the general style of requests when writing this. But yeah, you're right, if it's callable then it should be append otherwise we should try to extend it.

---

@shazow 

Further: Shouldn't the logic for register_hook and deregister_hook live in the same place as dispatch_hook? If that was the case, then the {check for `__call__` or extend} decision would be very evident. :)

---

@piotr-dobrogost 

Just in case I asked this http://stackoverflow.com/questions/12590494/.

> In the context of hooks, shouldn't the check just be if it's callable? If not, then extend?

Either we check all for callability or none. Why should the first one be better than the rest? :)

---

@sigmavirus24 

So you just want something like:



?

How about a compromise on:



Because calling it with None would cause some issues. If you would rather check that each element of the iterable is callable then simply:



That will take care of checking for callability and allow for lists, tuples, sets, etc to be passed.

---

@sigmavirus24 

@piotr-dobrogost, @shazow, @kennethreitz, does anyone mind if we take this to an issue instead of continuing to discuss everything here? An issue would make this more visible to future contributors/users/etc. of requests than it is here.

---

@piotr-dobrogost 

+1

> That will take care of checking for callability and allow for lists, tuples, sets, etc to be passed.

Not only this, this will allow for nested iterables (trees) to be passed as well! :)

---

Well that was painful, I'm all up for fixing this in particular and going through the rest of requests to see if we can get rid of some of these explicit checks unless anyone disagrees.
",sigmavirus24,shazow
868,2012-09-26 19:18:24,"https://github.com/sigmavirus24/requests/commit/4dd3d1a1a2534f2996a368ebe26114bf974e15f9#commitcomment-1904600

For reference, I'm going to copy the comments into here.

---

@piotr-dobrogost

Explicit type checking is code smell. We should be checking for iterability not types. See http://stackoverflow.com/questions/1952464/

---

@sigmavirus24

I was following the style of the rest of requests, if wanted I'll clean up this and the other places I saw it.

---

@piotr-dobrogost

@kennethreitz What do you think?

---

@kennethreitz 

iterability is always preferred, but not always possible in requests because of the way certain types work (strings, dicts)

---

@sigmavirus24 

@kennethreitz correct me if I'm wrong, but I believe you're against this. And if I remember correctly I had submitted an earlier PR that tried to do something similar to what @piotr-dobrogost is suggesting in `_encode_data` which failed when passed a dictionary. For reference in python 2 and 3, the following works



Which may not be entirely obvious to some and certainly might seem like unintended behavior. Likewise,



We could always do the complement though (which is far less obvious):



---

@piotr-dobrogost 

All we should care here is that we want iterable. It's up to the user to pass iterable he thinks is appropriate. If someone wants to pass ""random"" iterable then it's not our business to know better and reject it. Yes, there is more type checking in the Requests and I think it should be cleaned as well.

---

@piotr-dobrogost 

@shazow What's your opinion?

---

@shazow 

I would vote for making an is_foo helper somewhere nearby which contains the ""is iter and not str/dict"" logic or whatever we want it to be. Then re-use that where appropriate. Also name foo something descriptive. is_plural?

As for the logic, I'd be +0 on {iter and not str/dict}. I imagine people may want to pass in arbitrary generators and whatnot.

---

@sigmavirus24 

I think @piotr-dobrogost would just rather the logic `hasattr(foo, '__iter__')` rather than excluding `str`s and `dict`s.

---

@shazow 

In the context of hooks, shouldn't the check just be if it's callable? If not, then extend?

(My previous suggestion was more towards the general style of Requests, rather than this hook-specific example.)

---

@sigmavirus24 

Right, I was following the general style of requests when writing this. But yeah, you're right, if it's callable then it should be append otherwise we should try to extend it.

---

@shazow 

Further: Shouldn't the logic for register_hook and deregister_hook live in the same place as dispatch_hook? If that was the case, then the {check for `__call__` or extend} decision would be very evident. :)

---

@piotr-dobrogost 

Just in case I asked this http://stackoverflow.com/questions/12590494/.

> In the context of hooks, shouldn't the check just be if it's callable? If not, then extend?

Either we check all for callability or none. Why should the first one be better than the rest? :)

---

@sigmavirus24 

So you just want something like:



?

How about a compromise on:



Because calling it with None would cause some issues. If you would rather check that each element of the iterable is callable then simply:



That will take care of checking for callability and allow for lists, tuples, sets, etc to be passed.

---

@sigmavirus24 

@piotr-dobrogost, @shazow, @kennethreitz, does anyone mind if we take this to an issue instead of continuing to discuss everything here? An issue would make this more visible to future contributors/users/etc. of requests than it is here.

---

@piotr-dobrogost 

+1

> That will take care of checking for callability and allow for lists, tuples, sets, etc to be passed.

Not only this, this will allow for nested iterables (trees) to be passed as well! :)

---

Well that was painful, I'm all up for fixing this in particular and going through the rest of requests to see if we can get rid of some of these explicit checks unless anyone disagrees.
",sigmavirus24,kennethreitz
868,2012-09-26 19:18:24,"https://github.com/sigmavirus24/requests/commit/4dd3d1a1a2534f2996a368ebe26114bf974e15f9#commitcomment-1904600

For reference, I'm going to copy the comments into here.

---

@piotr-dobrogost

Explicit type checking is code smell. We should be checking for iterability not types. See http://stackoverflow.com/questions/1952464/

---

@sigmavirus24

I was following the style of the rest of requests, if wanted I'll clean up this and the other places I saw it.

---

@piotr-dobrogost

@kennethreitz What do you think?

---

@kennethreitz 

iterability is always preferred, but not always possible in requests because of the way certain types work (strings, dicts)

---

@sigmavirus24 

@kennethreitz correct me if I'm wrong, but I believe you're against this. And if I remember correctly I had submitted an earlier PR that tried to do something similar to what @piotr-dobrogost is suggesting in `_encode_data` which failed when passed a dictionary. For reference in python 2 and 3, the following works



Which may not be entirely obvious to some and certainly might seem like unintended behavior. Likewise,



We could always do the complement though (which is far less obvious):



---

@piotr-dobrogost 

All we should care here is that we want iterable. It's up to the user to pass iterable he thinks is appropriate. If someone wants to pass ""random"" iterable then it's not our business to know better and reject it. Yes, there is more type checking in the Requests and I think it should be cleaned as well.

---

@piotr-dobrogost 

@shazow What's your opinion?

---

@shazow 

I would vote for making an is_foo helper somewhere nearby which contains the ""is iter and not str/dict"" logic or whatever we want it to be. Then re-use that where appropriate. Also name foo something descriptive. is_plural?

As for the logic, I'd be +0 on {iter and not str/dict}. I imagine people may want to pass in arbitrary generators and whatnot.

---

@sigmavirus24 

I think @piotr-dobrogost would just rather the logic `hasattr(foo, '__iter__')` rather than excluding `str`s and `dict`s.

---

@shazow 

In the context of hooks, shouldn't the check just be if it's callable? If not, then extend?

(My previous suggestion was more towards the general style of Requests, rather than this hook-specific example.)

---

@sigmavirus24 

Right, I was following the general style of requests when writing this. But yeah, you're right, if it's callable then it should be append otherwise we should try to extend it.

---

@shazow 

Further: Shouldn't the logic for register_hook and deregister_hook live in the same place as dispatch_hook? If that was the case, then the {check for `__call__` or extend} decision would be very evident. :)

---

@piotr-dobrogost 

Just in case I asked this http://stackoverflow.com/questions/12590494/.

> In the context of hooks, shouldn't the check just be if it's callable? If not, then extend?

Either we check all for callability or none. Why should the first one be better than the rest? :)

---

@sigmavirus24 

So you just want something like:



?

How about a compromise on:



Because calling it with None would cause some issues. If you would rather check that each element of the iterable is callable then simply:



That will take care of checking for callability and allow for lists, tuples, sets, etc to be passed.

---

@sigmavirus24 

@piotr-dobrogost, @shazow, @kennethreitz, does anyone mind if we take this to an issue instead of continuing to discuss everything here? An issue would make this more visible to future contributors/users/etc. of requests than it is here.

---

@piotr-dobrogost 

+1

> That will take care of checking for callability and allow for lists, tuples, sets, etc to be passed.

Not only this, this will allow for nested iterables (trees) to be passed as well! :)

---

Well that was painful, I'm all up for fixing this in particular and going through the rest of requests to see if we can get rid of some of these explicit checks unless anyone disagrees.
",sigmavirus24,piotr-dobrogost
868,2012-09-26 19:18:24,"https://github.com/sigmavirus24/requests/commit/4dd3d1a1a2534f2996a368ebe26114bf974e15f9#commitcomment-1904600

For reference, I'm going to copy the comments into here.

---

@piotr-dobrogost

Explicit type checking is code smell. We should be checking for iterability not types. See http://stackoverflow.com/questions/1952464/

---

@sigmavirus24

I was following the style of the rest of requests, if wanted I'll clean up this and the other places I saw it.

---

@piotr-dobrogost

@kennethreitz What do you think?

---

@kennethreitz 

iterability is always preferred, but not always possible in requests because of the way certain types work (strings, dicts)

---

@sigmavirus24 

@kennethreitz correct me if I'm wrong, but I believe you're against this. And if I remember correctly I had submitted an earlier PR that tried to do something similar to what @piotr-dobrogost is suggesting in `_encode_data` which failed when passed a dictionary. For reference in python 2 and 3, the following works



Which may not be entirely obvious to some and certainly might seem like unintended behavior. Likewise,



We could always do the complement though (which is far less obvious):



---

@piotr-dobrogost 

All we should care here is that we want iterable. It's up to the user to pass iterable he thinks is appropriate. If someone wants to pass ""random"" iterable then it's not our business to know better and reject it. Yes, there is more type checking in the Requests and I think it should be cleaned as well.

---

@piotr-dobrogost 

@shazow What's your opinion?

---

@shazow 

I would vote for making an is_foo helper somewhere nearby which contains the ""is iter and not str/dict"" logic or whatever we want it to be. Then re-use that where appropriate. Also name foo something descriptive. is_plural?

As for the logic, I'd be +0 on {iter and not str/dict}. I imagine people may want to pass in arbitrary generators and whatnot.

---

@sigmavirus24 

I think @piotr-dobrogost would just rather the logic `hasattr(foo, '__iter__')` rather than excluding `str`s and `dict`s.

---

@shazow 

In the context of hooks, shouldn't the check just be if it's callable? If not, then extend?

(My previous suggestion was more towards the general style of Requests, rather than this hook-specific example.)

---

@sigmavirus24 

Right, I was following the general style of requests when writing this. But yeah, you're right, if it's callable then it should be append otherwise we should try to extend it.

---

@shazow 

Further: Shouldn't the logic for register_hook and deregister_hook live in the same place as dispatch_hook? If that was the case, then the {check for `__call__` or extend} decision would be very evident. :)

---

@piotr-dobrogost 

Just in case I asked this http://stackoverflow.com/questions/12590494/.

> In the context of hooks, shouldn't the check just be if it's callable? If not, then extend?

Either we check all for callability or none. Why should the first one be better than the rest? :)

---

@sigmavirus24 

So you just want something like:



?

How about a compromise on:



Because calling it with None would cause some issues. If you would rather check that each element of the iterable is callable then simply:



That will take care of checking for callability and allow for lists, tuples, sets, etc to be passed.

---

@sigmavirus24 

@piotr-dobrogost, @shazow, @kennethreitz, does anyone mind if we take this to an issue instead of continuing to discuss everything here? An issue would make this more visible to future contributors/users/etc. of requests than it is here.

---

@piotr-dobrogost 

+1

> That will take care of checking for callability and allow for lists, tuples, sets, etc to be passed.

Not only this, this will allow for nested iterables (trees) to be passed as well! :)

---

Well that was painful, I'm all up for fixing this in particular and going through the rest of requests to see if we can get rid of some of these explicit checks unless anyone disagrees.
",sigmavirus24,sigmavirus24
860,2012-09-18 22:00:20,"Pretty sure it is described in the docs but I'm glad to help. Care to close the issue for @kennethreitz? 
",sigmavirus24,kennethreitz
857,2013-01-26 19:54:24,"@jkl1337 we don't set the options on the sockets and I'm not sure if urllib3 does either (although if it does, it would probably be a good idea to submit the idea to @shazow). That aside, it would be ideal if we could use `select` but that would break Windows support.

We could definitely catch and re-throw the `socket.error`. That's easy. The problem it seems (to me) is that reading from a broken socket isn't raising any exception at all.
",sigmavirus24,shazow
851,2012-09-24 14:41:50,"Hmm, maybe this belongs in urllib3. @shazow ?
",kennethreitz,shazow
850,2012-09-12 18:05:39,"@securityforus it all depends where you branch from.
",sigmavirus24,securityforus
847,2012-09-10 13:49:51,"I'm pretty sure this support would have to be added to urllib3 (or if it's already there, the vendorized package would have to be updated). @shazow might be a better person to talk to about this. In the meantime, I'll see if it's part of urllib3.
",sigmavirus24,shazow
847,2012-09-10 15:48:13,"It could be done on the requests side since `file` scheme won't be doing any network-based work nor does it require any connection pooling. For those reasons I'm also uncomfortable adding it into urllib3. :/ What do you think, @kennethreitz?
",shazow,kennethreitz
847,2013-06-19 19:19:23,"That's an interesting bug. Could you perhaps trying using @kanzure's 
requestions? I suspect that's probably getting a little long in the tooth but 
it probably works around that issue if I remember it correctly. I forget, 
however, whether it uses transport adapters.
",sigmavirus24,kanzure
844,2012-09-07 03:54:10,"This ticket is intended to aggregate previous discussion from #539, #589, and #597 about the default value of `chunk_size` used by `iter_content` and `iter_lines`.

cc @mponton @gwrtheyrn @shazow

Issues:
1. The default read size of `iter_content` is 1 byte; this is probably inefficient
2. Requests does not expose the ability to read chunked encoding streams in the ""correct"" way, i.e., using the provided octet counts to tell how much to read.
3. However, this would not be suitable as the default implementation of `iter_content` anyway; not all websites are standards-compliant and when this was tried it caused more problems than it solved.
4. The current default read size for `iter_lines` is 10kB. This is high enough that iteration over lines can be perceived as unresponsive --- no lines are returned until all 10kB have been read.
5. There is no ""correct"" way to implement `iter_lines` using blocking I/O, we just have to bite the bullet and take a guess as to how much data we should read.
6. There's apparently some nondeterminism in `iter_lines`, I think because of the edge case where a read ends between a `\r` and a `\n`.
7. `iter_lines` is backed by `iter_content`, which operates on raw byte strings and splits at byte boundaries. I think there may be edge cases where we could split the body in the middle of a multi-byte encoding of a Unicode character.

My guess at a solution:
1. Set the default `chunk_size` to 1024 bytes, for both `iter_content` and `iter_lines`.
2. Provide a separate interface (possibly `iter_chunks`) for iterating over chunks of pages that are known to correctly implement chunked encoding, e.g., Twitter's firehose APIs
3. We may need our own implementation of `splitlines` that is deterministic with respect to our chunking boundaries, i.e., remembers if the last-read character was `\r` and suppresses a subsequent `\n`. We may also need to build in Unicode awareness at this level, i.e., decode as much of the body as is valid, then save any leftover invalid bytes to be prepended to the next chunk.

Comments and thoughts are much appreciated. Thanks for your time!
",slingamn,shazow
844,2012-09-07 03:54:10,"This ticket is intended to aggregate previous discussion from #539, #589, and #597 about the default value of `chunk_size` used by `iter_content` and `iter_lines`.

cc @mponton @gwrtheyrn @shazow

Issues:
1. The default read size of `iter_content` is 1 byte; this is probably inefficient
2. Requests does not expose the ability to read chunked encoding streams in the ""correct"" way, i.e., using the provided octet counts to tell how much to read.
3. However, this would not be suitable as the default implementation of `iter_content` anyway; not all websites are standards-compliant and when this was tried it caused more problems than it solved.
4. The current default read size for `iter_lines` is 10kB. This is high enough that iteration over lines can be perceived as unresponsive --- no lines are returned until all 10kB have been read.
5. There is no ""correct"" way to implement `iter_lines` using blocking I/O, we just have to bite the bullet and take a guess as to how much data we should read.
6. There's apparently some nondeterminism in `iter_lines`, I think because of the edge case where a read ends between a `\r` and a `\n`.
7. `iter_lines` is backed by `iter_content`, which operates on raw byte strings and splits at byte boundaries. I think there may be edge cases where we could split the body in the middle of a multi-byte encoding of a Unicode character.

My guess at a solution:
1. Set the default `chunk_size` to 1024 bytes, for both `iter_content` and `iter_lines`.
2. Provide a separate interface (possibly `iter_chunks`) for iterating over chunks of pages that are known to correctly implement chunked encoding, e.g., Twitter's firehose APIs
3. We may need our own implementation of `splitlines` that is deterministic with respect to our chunking boundaries, i.e., remembers if the last-read character was `\r` and suppresses a subsequent `\n`. We may also need to build in Unicode awareness at this level, i.e., decode as much of the body as is valid, then save any leftover invalid bytes to be prepended to the next chunk.

Comments and thoughts are much appreciated. Thanks for your time!
",slingamn,mponton
844,2012-09-07 03:54:10,"This ticket is intended to aggregate previous discussion from #539, #589, and #597 about the default value of `chunk_size` used by `iter_content` and `iter_lines`.

cc @mponton @gwrtheyrn @shazow

Issues:
1. The default read size of `iter_content` is 1 byte; this is probably inefficient
2. Requests does not expose the ability to read chunked encoding streams in the ""correct"" way, i.e., using the provided octet counts to tell how much to read.
3. However, this would not be suitable as the default implementation of `iter_content` anyway; not all websites are standards-compliant and when this was tried it caused more problems than it solved.
4. The current default read size for `iter_lines` is 10kB. This is high enough that iteration over lines can be perceived as unresponsive --- no lines are returned until all 10kB have been read.
5. There is no ""correct"" way to implement `iter_lines` using blocking I/O, we just have to bite the bullet and take a guess as to how much data we should read.
6. There's apparently some nondeterminism in `iter_lines`, I think because of the edge case where a read ends between a `\r` and a `\n`.
7. `iter_lines` is backed by `iter_content`, which operates on raw byte strings and splits at byte boundaries. I think there may be edge cases where we could split the body in the middle of a multi-byte encoding of a Unicode character.

My guess at a solution:
1. Set the default `chunk_size` to 1024 bytes, for both `iter_content` and `iter_lines`.
2. Provide a separate interface (possibly `iter_chunks`) for iterating over chunks of pages that are known to correctly implement chunked encoding, e.g., Twitter's firehose APIs
3. We may need our own implementation of `splitlines` that is deterministic with respect to our chunking boundaries, i.e., remembers if the last-read character was `\r` and suppresses a subsequent `\n`. We may also need to build in Unicode awareness at this level, i.e., decode as much of the body as is valid, then save any leftover invalid bytes to be prepended to the next chunk.

Comments and thoughts are much appreciated. Thanks for your time!
",slingamn,gwrtheyrn
837,2012-09-04 06:58:52,"Take #4.
Thanks @sigmavirus24
",alicebob,sigmavirus24
820,2012-08-29 19:31:04,"@idan?
",kennethreitz,idan
819,2012-08-28 05:39:39,"@idan @dgouldin @gulopine code review? :)
",kennethreitz,idan
819,2012-08-28 05:39:39,"@idan @dgouldin @gulopine code review? :)
",kennethreitz,gulopine
819,2012-08-28 05:39:39,"@idan @dgouldin @gulopine code review? :)
",kennethreitz,dgouldin
803,2012-08-21 14:44:19,"Part of this should actually be a PR on shazow/urllib3 and the other part would depend on @shazow accepting it unless I'm speaking out of turn.
",sigmavirus24,shazow
800,2012-08-24 15:06:28,"Anyone can handle this?
What the bot said is wrong. It is not merged. The bot gives a wrong commit.
cc @Lukasa @kennethreitz
",ayanamist,kennethreitz
800,2012-08-24 15:06:28,"Anyone can handle this?
What the bot said is wrong. It is not merged. The bot gives a wrong commit.
cc @Lukasa @kennethreitz
",ayanamist,Lukasa
799,2012-12-16 00:56:55,"@Schwanksta The respective issue on urllib3 has been closed. :)
",shazow,Schwanksta
799,2014-01-07 18:57:08,"I did not intend for that message to seem like I was chastising you, or to seem abrasive: I'm sorry.

It's worth noting that, as I mentioned, this comes up very frequently. This means that a lot of developer time is spent responding to duplicate issues and comments on previous issues. Any GitHub comment is responded to as if it is urgent, because they often are. This means a comment or GitHub issue is an instant cost of our time (as I'm sure you can appreciate from your own projects).

This is what I meant when I said there were ""lower-noise"" ways of raising this issue. For example, Stack Overflow would have been a suitable place to ask. Alternatively, emailing one of the triagers would have been just as appropriate: both @sigmavirus24 and I have our email addresses published very clearly.

My response was born in part from frustration at repeatedly having to follow up on issues opened or commented on that could have been resolved by more care. I'm sure you just made a mistake, and as I said, I'm sorry for not taking care with my language.
",Lukasa,sigmavirus24
799,2014-01-07 19:33:44,"I appreciate that, Cory. I was also very frustrated, as it seems like I've
been dealing with tiny, obstructive bugs from every dependency lately, and
I just spent a week rebuilding a new solution as an alternative to an
existing one that was broken and blocking me. Right when I was about to
check-off the last feature, I run into a mechanical issue with SSL, where
I'm a bit fuzzy when it comes to implementation on _ssl and urllib, and how
requests wraps one or both. I'd be more careful in the afternoon, but when
I just couldn't get it done before having to walk out, this morning, I got
hasty.

On Tue, Jan 7, 2014 at 1:57 PM, Cory Benfield notifications@github.comwrote:

> I did not intend for that message to seem like I was chastising you, or to
> seem abrasive: I'm sorry.
> 
> It's worth noting that, as I mentioned, this comes up very frequently.
> This means that a lot of developer time is spent responding to duplicate
> issues and comments on previous issues. Any GitHub comment is responded to
> as if it is urgent, because they often are. This means a comment or GitHub
> issue is an instant cost of our time (as I'm sure you can appreciate from
> your own projects).
> 
> This is what I meant when I said there were ""lower-noise"" ways of raising
> this issue. For example, Stack Overflow would have been a suitable place to
> ask. Alternatively, emailing one of the triagers would have been just as
> appropriate: both @sigmavirus24 https://github.com/sigmavirus24 and I
> have our email addresses published very clearly.
> 
> My response was born in part from frustration at repeatedly having to
> follow up on issues opened or commented on that could have been resolved by
> more care. I'm sure you just made a mistake, and as I said, I'm sorry for
> not taking care with my language.
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/pull/799#issuecomment-31767192
> .
",dsoprea,sigmavirus24
795,2012-08-19 18:40:51,"So I realized in changing github3.py's download creation function that when @jkbr changed the encoding of files (`_encode_files`) he inserted the file tuples first. So making a call like:



Will actually have the structure (sent to urllib3) of



Whereas before that it would be:



And this changes the previously expected/exhibited behavior. I kept that construction assuming it had been approved, but I can't find that and I'm not sure if anyone was aware of this. I can change the ordering very easily, but I was wondering if I should or if this is ok.
",sigmavirus24,jkbr
795,2012-08-19 19:11:20,"Ah ok, I'll just wait for word from @kennethreitz before issuing the pull request.
",sigmavirus24,kennethreitz
794,2012-08-19 10:31:15,"Ehh...standards are hard.

The comma (along with some other characters) is defined in [RFC 3986](http://pretty-rfc.herokuapp.com/RFC3986#reserved) as a reserved character. This means the comma has defined meaning at various parts in a URL, and if it is not being used in that context it needs to be percent-encoded.

That said, the [query parameter](http://pretty-rfc.herokuapp.com/RFC3986#query) doesn't give the comma any special syntax, so in query parameters we probably shouldn't be encoding it. That said, it's not entirely Requests' fault: the parameters are encoded using [`urllib.urlencode()`](http://docs.python.org/library/urllib#urllib.urlencode), which is what is percent-encoding the query parameters.

This isn't easy to fix though, because some web services use `,` and some use `%2C`, and neither is wrong. You might just have to handle this encoding yourself.

@kennethreitz, can you think of anyone who would have a better insight into this problem?
",Lukasa,kennethreitz
791,2012-08-18 17:50:27,"Accept lists of `(key, val)` tuples everywhere.

This should close/solve issue #179. Unfortunately @jkbr had duplicated some of the effort so I had to remove his contributions there, but all the tests pass.

Also, I apologize in advance for the likely very ugly history. I wanted to keep up-to-date while working on this so there are a few (possibly unnecessary) merge commits.
",sigmavirus24,jkbr
791,2012-08-18 20:31:57,"@kennethreitz @Lukasa should I start a new branch off of develop and cherry pick my commits on top of it to avoid the merges? Besides, one of the merges would not have been able to be automatic, so it would have required a merge commit anyway.
",sigmavirus24,kennethreitz
790,2012-08-18 20:17:20,"So, I know nothing about OAuth, but it looks like we just shouldn't be checking the Content Type header. The OAuth code appears to check for the presence of a file anyway, so it doesn't seem like it matters.

Thoughts @kennethreitz?
",Lukasa,kennethreitz
790,2012-08-20 06:33:45,"So nobody cares?
@kennethreitz has so many activities today.
",ayanamist,kennethreitz
790,2012-08-20 07:03:14,"I'm on the go, but very generally:

If you're passing in a dict, requests will render the dict as www-form-urlencoded in the HTTP body, and set the appropriate content type. These parameters are taken as input when building the base signing string (http://tools.ietf.org/html/rfc5849#section-3.4.1.3.1), but only if the content-type header is set to ""application/x-www-form-urlencoded"", otherwise the request body is ignored for the purpose of signing.

Since the act of passing a body dict payload in requests is a clear signal of your intention to have form-urlencoded body, the content-type is set automatically.

If the body is anything but a dict (or a manually-generated, valid x-www-form-urlencoded body), oauth should be ignoring the body for the purposes of signing and should not be setting a content-type (since you're doing something special).

That's how it should work. If it works any other way, then we need to look into it.  

On Monday, August 20, 2012 at 9:33 AM, 老A wrote:

> So nobody cares?
> @kennethreitz (https://github.com/kennethreitz) has so many activities today.  
> 
> —
> Reply to this email directly or view it on GitHub (https://github.com/kennethreitz/requests/issues/790#issuecomment-7861749).  
",idan,kennethreitz
783,2012-08-16 19:56:51,"Obviously, whether this gets pulled is up to @kennethreitz, but I would suspect that answer will be that it won't be.

Requests is not intended for formatting or displaying data, but for obtaining it. Even the `Response.json` method only really exists as a courtesy, with any fault-tolerant JSON handling being done by the end user (see #691). Requests generally does not deal with the data beyond decoding the response and handing it back to the programmer.

As @kennethreitz [has said before](https://github.com/kennethreitz/requests/issues/769#issuecomment-7622419), Requests is an HTTP library, not anything else. This is **not** my decision, but I think it's likely that the answer will be that this pull request is not accepted. Don't take my word for it though, wait for Kenneth to look at it. I might be talking total rubbish. =)
",Lukasa,kennethreitz
775,2012-08-14 11:39:53,"Thanks for understanding. I didn't anticipate this problem when I changed the default, unfortunately.

@kennethreitz maybe `iter_content` and `iter_lines` should still work after the content is fetched, for example, `iter_content` could return an iterator containing a single item, the entire response body?
",slingamn,kennethreitz
773,2012-08-14 03:54:05,"@radomir: @Lukasa is right, this change was great. I was just worried that the testing infrastructure for Requests was broken somehow. But `test_requests_ext.py` is intentionally excluded, so that explains it. Thanks for your contribution!
",slingamn,radomir
771,2012-09-07 04:03:29,"Unfortunately this is quite challenging to debug, because the relevant code extends across Requests, urllib3, and the standard-library `httplib`. Also, it's hard to reproduce :-/

I think some hints about how to inspect the problem are in the debugging patch mentioned above. Basically, try and identify every time a socket is created, and figure out who is creating it and storing it, and output its `fileno` whenever possible, and then try and correlate that with output from `lsof` and/or `strace` describing the actions taken by the program on its file descriptors. An interactive debugger (pdb, ipdb, pudb) might help.

@keves this is hard, but we'd be really appreciative if you could investigate the issue :-)

One thing I thought was interesting from the debugging output was:



Seemed like there should have been a newline in between the ""returning"" and ""adding"" messages. I think I saw a similar missing newline when I was playing with google.com's redirection, but I didn't have a corresponding leak.
",slingamn,keves
771,2012-09-07 11:18:39,"I would be glad to investigate the issue on my own, however it will take me
some time to get into that as I'm pretty overwhelmed with work at the
moment and as you say, this isn't the simplest of bugs :/

What I can do, in the meantime, is offer ssh access to a Linux VM where
this constantly reproduces. If anyone feels like playing with this, let me
know and I will set it up. If not, I'm hoping to have some time to look
into it myself in the near future. Tracking the socket from its creation
sounds like a good idea, and maybe I could monkey-patch the socket module
to print a stack-trace when a new socket is created and then match it
against the lsof output and find out the offender.

On Fri, Sep 7, 2012 at 7:03 AM, Shivaram Lingamneni <
notifications@github.com> wrote:

> Unfortunately this is quite challenging to debug, because the relevant
> code extends across Requests, urllib3, and the standard-library httplib.
> Also, it's hard to reproduce :-/
> 
> I think some hints about how to inspect the problem are in the debugging
> patch mentioned above. Basically, try and identify every time a socket is
> created, and figure out who is creating it and storing it, and output its
> fileno whenever possible, and then try and correlate that with output
> from lsof and/or strace describing the actions taken by the program on
> its file descriptors. An interactive debugger (pdb, ipdb, pudb) might help.
> 
> @keves https://github.com/keves this is hard, but we'd be really
> appreciative if you could investigate the issue :-)
> 
> One thing I thought was interesting from the debugging output was:
> 
> returning to pool: www.mouser.com adding new connection for il.mouser.com
> 
> Seemed like there should have been a newline in between the ""returning""
> and ""adding"" messages. I think I saw a similar missing newline when I was
> playing with google.com's redirection, but I didn't have a corresponding
> leak.
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/issues/771#issuecomment-8355455.
",eranrund,keves
771,2012-10-15 07:30:02,"@keves I was unclear, sorry. There's an action item for you in this comment: [https://github.com/kennethreitz/requests/issues/771#issuecomment-8678971]
",slingamn,keves
771,2012-10-21 23:10:29,"@keves yeah, similar in the sense that it's a subtle bug in `httplib` related to the content-length header.

It's possible that this is in fact the root cause of [http://bugs.python.org/issue15633]. I think reporting this to Python upstream will clarify what exactly the problem is, but we'll likely have to add a workaround to Requests in any case (because the release cycle for Python upstream is slow, and because the fix will never hit Python 2.6).

It's up to you whether you want to report this on [http://bugs.python.org/issue15633] or on a new ticket.
",slingamn,keves
764,2012-08-06 22:57:23,"This adds unit tests corresponding to the issues @dhagrow identified on #760, then fixes them.
",slingamn,dhagrow
759,2012-08-06 14:14:21,"This _is_ a `urllib3` issue. It's also a stupid Python version problem.

In Python 2, the correct code would read:



In Python 3, that's a NameError because the `long` type was removed. @shazow, does this fix look useful?

Whack this function definition into `util.py`:



Then just replace the test with:


",Lukasa,shazow
755,2012-08-03 16:23:50,"@enginous if you pull develop and merge, your PR should pass unless there's a way to kick @travisbot to merge it against the current head.
",sigmavirus24,enginous
754,2012-08-03 15:43:44,"Found by @enginous https://github.com/kennethreitz/requests/commit/204649521bf8d165690b78a349d0f7c10d03bb99#requests/models.py-P27
",sigmavirus24,enginous
749,2013-05-03 22:30:30,"I actually prefer the former because the people who need it are the ones who know they'll need it and they're few enough that it won't cause too many complaints. However, if we're going to be consistent with the existing API, the latter would be the way to implement it. Currently, urllib3 (and requests) will provide the API to send HTTPS requests without the ssl module present but will fail in the case where the ssl module is not available. In other words, the API is there and you can use it but it just won't work and that is indicated by an exception.

As to re-opening this, that's up to @kennethreitz. The issue of course (with all of this) is that we're currently under a feature freeze (#1165, #1168) so I'm not sure if the work is even worth doing because it might not be accepted.
",sigmavirus24,kennethreitz
749,2013-05-03 23:36:10,"Let me be a bit clearer: what I meant by the second alternative was ""Try to use SNI if the optional deps are available, but still use SSL as we always have it they aren't."". This shouldn't break anything existing up to now.

As for reopening (or not) the issue, IMHO, this issue shouldn't be taken lightly. requests can become useless without SNI support for many scenarios. In particular, multiple domains on a single IPv4 host without SNI become impossible. And additional IPv4 addresses are out of the questions for many users (due to their cost).

In any case, this is half feature, half bug, though I'll await @kennethreitz's reply regarding reopening the issue. 
",hobarrera,kennethreitz
749,2013-08-29 15:45:28,"@lukasa Thanks :smile: I guess I could add myself to `AUTHORS.txt` some time in the future.
",t-8ch,lukasa
746,2012-07-27 15:23:51,"Looks good to me. I'm uncomfortable merging pull requests on @kennethreitz 's behalf, so I'll let him do it.

That TravisCI failure is worrying me, because it seems to come and go, and not be related to the contents of a particular build. /pensiveface
",Lukasa,kennethreitz
746,2012-07-27 17:43:44,"Ah, good idea. =)

If they're 5xx's though, it's probably on httpbin.org's side (aka. it's @kennethreitz 's problem, not mine. =D ).
",Lukasa,kennethreitz
743,2012-07-27 06:02:49,"@idan @dgouldin @gulopine?
",kennethreitz,idan
743,2012-07-27 06:02:49,"@idan @dgouldin @gulopine?
",kennethreitz,gulopine
743,2012-07-27 06:02:49,"@idan @dgouldin @gulopine?
",kennethreitz,dgouldin
741,2012-07-26 23:47:05,"Way more than syntactic sugar, but yes, that's where the problem lies.

We have plans to fix this I believe, right @shazow?
",kennethreitz,shazow
738,2012-07-25 12:44:57,"These are all genuine Python 3 Syntax Errors that are in libraries included within `requests`. The errors in `chardet` don't affect functionality, as in Python 3 `chardet2` is imported instead, which does not have these problems. It would be nice if we could somehow exclude the module entirely in Python 3 though.

`oauthlib` is a separate problem. Looking at the code in `auth.py`, the Syntax Errors raised when importing `oauthlib` cause the import to fail, and that's handled by the except block [here](https://github.com/kennethreitz/requests/blob/develop/requests/auth.py#L22). In the Python 3 case, that except block allows you to use the other Auth methods in that file, but _will_ cause OAuth1 to fail. (For detail, it fails with a NameError because `Client` was not successfully imported.)

Interestingly, all of the Syntax Errors in `oauthlib` are caused by unicode literals, which means they will stop being errors in Python 3.3. It's @kennethreitz 's call, but I know that he's planning to drop Python 3.2 as a supported version sometime after the release of 3.3, when this problem will go away.
",Lukasa,kennethreitz
737,2012-07-24 22:21:30,"I've just started looking into this, and I can explain the behaviour of the second case. The list of tuples you provided to `data` has `dict()` called on it. Dictionaries (obviously) don't allow duplicate keys, but your list of tuples has duplicate keys, so the last item in the iterable takes the value for that key. Exactly the same behaviour is shown if you provide a list of tuples to `data` without the `files` argument, so I'd assume that this is intended behaviour.

I'm significantly less sure about your first case. The actual exception is occurring in urllib3, which does not expect to be passed a list. If I had to hazard a guess, I'd say that the fix should be applied to requests, not urllib3, because requests already has edge case code for lists in [`_encode_params()`](https://github.com/kennethreitz/requests/blob/develop/requests/models.py#L306). It's a fix I'm prepared to write, but I would want @kennethreitz to confirm that this is a bug, and not actually correct behaviour, first.
",Lukasa,kennethreitz
731,2012-07-18 13:40:00,"I checked it out on my machine and it looks good, to me, so you don't need to do anything else. @kennethreitz will pop by at some point when he has time and will handle merging into the development branch, but it'll merge nicely so it shouldn't be a big deal. If you want to, you could add your name to the Authors file, but that's really up to you. As for trunk, it should make it into the next minor version of requests.
",Lukasa,kennethreitz
729,2012-07-14 22:30:26,"Co-Authored By: Timnit Gebru tgebru@gmail.com @tgebru
Co-Authored By: Sarah Gonzalez smar.gonz@gmail.com @smargonz
Co-Authored By: Leila Muhtasib muhtasib@gmail.com @muhtasib

This is a fix for Issue #661.
",vickimo,tgebru
729,2012-07-14 22:30:26,"Co-Authored By: Timnit Gebru tgebru@gmail.com @tgebru
Co-Authored By: Sarah Gonzalez smar.gonz@gmail.com @smargonz
Co-Authored By: Leila Muhtasib muhtasib@gmail.com @muhtasib

This is a fix for Issue #661.
",vickimo,muhtasib
729,2012-07-14 22:30:26,"Co-Authored By: Timnit Gebru tgebru@gmail.com @tgebru
Co-Authored By: Sarah Gonzalez smar.gonz@gmail.com @smargonz
Co-Authored By: Leila Muhtasib muhtasib@gmail.com @muhtasib

This is a fix for Issue #661.
",vickimo,smargonz
728,2012-07-14 21:15:30,"This is a fix for issue #695. 

Additional co-authors:
@smargonz
@vickimo
@tgebru
",muhtasib,tgebru
728,2012-07-14 21:15:30,"This is a fix for issue #695. 

Additional co-authors:
@smargonz
@vickimo
@tgebru
",muhtasib,smargonz
728,2012-07-14 21:15:30,"This is a fix for issue #695. 

Additional co-authors:
@smargonz
@vickimo
@tgebru
",muhtasib,vickimo
728,2012-07-16 16:28:33,"@smargonz and I noticed this hasn't been implemented yet. Would you like us to implement it?  Let us know if you have any design considerations you'd like us to follow.
",muhtasib,smargonz
727,2012-07-14 20:12:25,"Modified code to use the current fix versus the old fix, which was broken.

Co-Authored By: Timnit Gebru tgebru@gmail.com @tgebru
Co-Authored By: Sarah Gonzalez smar.gonz@gmail.com @smargonz
Co-Authored By: Leila Muhtasib muhtasib@gmail.com @muhtasib

This is a fix for issue #541 and #547.
",vickimo,tgebru
727,2012-07-14 20:12:25,"Modified code to use the current fix versus the old fix, which was broken.

Co-Authored By: Timnit Gebru tgebru@gmail.com @tgebru
Co-Authored By: Sarah Gonzalez smar.gonz@gmail.com @smargonz
Co-Authored By: Leila Muhtasib muhtasib@gmail.com @muhtasib

This is a fix for issue #541 and #547.
",vickimo,muhtasib
727,2012-07-14 20:12:25,"Modified code to use the current fix versus the old fix, which was broken.

Co-Authored By: Timnit Gebru tgebru@gmail.com @tgebru
Co-Authored By: Sarah Gonzalez smar.gonz@gmail.com @smargonz
Co-Authored By: Leila Muhtasib muhtasib@gmail.com @muhtasib

This is a fix for issue #541 and #547.
",vickimo,smargonz
722,2012-07-15 05:18:45,"After trying out a few alternatives, I talked with @kennethreitz briefly about this on IRC, and he came up with the obvious solution:

https://github.com/gulopine/requests/commit/cf24d37caf567c0e89122dc4943315c974f5b3b8

I think I screwed up my fork, so I don't know if I can submit a proper pull request, much less attach it to this ticket. That commit should illustrate the point, though.
",gulopine,kennethreitz
713,2012-07-10 21:44:55,"This would be wonderful.

I believe @durin42 has some thoughts about this as well.
",kennethreitz,durin42
713,2013-01-23 14:03:33,"@durdin42 yessir! http://docs.python-requests.org/en/latest/user/advanced/#streaming-uploads
",kennethreitz,durdin42
711,2012-07-11 11:25:55,"HAL is now an [Internet-Draft](http://tools.ietf.org/html/draft-kelly-json-hal-03). This is likely to become _the_ hypermedia guideline for expressing connections between resources (what the Link header does) with many more features. 

Also @mitsuhiko expressed his hate for embedding ""new things"" into headers. I think it might have something to do with cachability or intermediaries that mess with headers.

See more:
- [Backbone.HAL](https://github.com/mikekelly/backbone.hal)
- [frenetic](http://dlindahl.github.com/frenetic/)
",jokull,mitsuhiko
700,2012-06-29 04:16:38,"Going to implement conditional gets first, content will come at a later date.

/c @dstufft
",kennethreitz,dstufft
700,2012-06-29 04:16:39,"Going to implement conditional gets first, content will come at a later date.

/c @dstufft
",kennethreitz,dstufft
694,2012-06-22 12:27:55,"So, I have a theory. It's just a theory, and it'd be great if someone who knows more about HTTPS proxies could try to back me up here (@kennethreitz, any ideas?), but here goes.

[According to the Squid wiki](http://wiki.squid-cache.org/Features/HTTPS), SSL/TLS can be tunnelled through Squid using the HTTP CONNECT method or, in some tiny subset of cases, insert itself as a man-in-the-middle in your HTTPS connection. The issue with presenting itself as a man-in-the-middle is that it'll cause browsers to alert about mismatching certificates (and it's a little unethical if the proxy is controlled by someone who isn't you).

I'd be prepared to be £5 (maybe more) that your browser is using the CONNECT verb to handle the connection, and AFAIK `requests` doesn't do that. A quick reading of `urllib3.request.RequestMethods` shows that `urllib3` [doesn't handle the CONNECT verb](https://github.com/shazow/urllib3/blob/master/urllib3/request.py#L41), which suggests that if you want the functionality you'd need to submit a pull request to `urllib3`.

**tl;dr**: `requests` can't use the only HTTPS proxying method your setup of Squid allows. Probably.
",Lukasa,kennethreitz
692,2012-06-26 12:16:16,"Yes this would be very good, I'm currently bumping up against this, I'm wondering why git sub-modules are not used here

Note it looks like this was fixed in 8f0ac668da796b04df9901db58f5d4215be308dd 

But it's yet to be pushed to PyPi. @kennethreitz can you do this?
",graingert,kennethreitz
684,2012-06-29 16:51:16,"url = 'https://api.twitter.com/1/statuses/home_timeline.json' 
params = {'count': 2}

You'll have to grab your own app key/secret, token key/secret if that's okay? haha 

## 

Mike Helmick
@mikehelmick

On Friday, June 29, 2012 at 12:49 PM, Marty Alchin wrote:

> The response was None? I can't think of why that would happen. If it was just a signing issue, you should get a response from Twitter saying so. I'll do some more testing tonight and see if I can reproduce it. To make sure I'm doing the same you are, can you provide a the `url` and `params` you're using?
> 
> ---
> 
> Reply to this email directly or view it on GitHub:
> https://github.com/kennethreitz/requests/pull/684#issuecomment-6667600
",michaelhelmick,mikehelmick
677,2012-06-18 22:34:13,"The latter commit fixes what @lukasa noticed on #503
",sigmavirus24,lukasa
677,2012-06-21 01:07:39,"The comment I deleted stands: I ran the tests on my machine with python 3000 and nose and got the same errors on my upstream branch as I get on my branch. I'll dig in to see if I can figure out where these tests are breaking.

**Update**: I tried running the tests individually, e.g.,



Only the last one passes with python3 on my machine on my branch tracking develop

**Update #2** After getting the tests working (thanks again @kennethreitz), I have a feeling the failure is related to nose and not requests or my changes. From what I can tell, the exception is occurring because of my having OpenDNS installed and running. This causes an error to be raised in the test for invalid content which causes the exception in nosetests. @jpellerin, @kumar303, any clues about the message above?
",sigmavirus24,kumar303
677,2012-06-21 01:07:39,"The comment I deleted stands: I ran the tests on my machine with python 3000 and nose and got the same errors on my upstream branch as I get on my branch. I'll dig in to see if I can figure out where these tests are breaking.

**Update**: I tried running the tests individually, e.g.,



Only the last one passes with python3 on my machine on my branch tracking develop

**Update #2** After getting the tests working (thanks again @kennethreitz), I have a feeling the failure is related to nose and not requests or my changes. From what I can tell, the exception is occurring because of my having OpenDNS installed and running. This causes an error to be raised in the test for invalid content which causes the exception in nosetests. @jpellerin, @kumar303, any clues about the message above?
",sigmavirus24,jpellerin
671,2012-09-22 10:11:24,"@kennethreitz Any comments why it was closed? I believe that failing tests have nothing to do with this pull request.
",piotr-dobrogost,kennethreitz
665,2012-06-07 23:54:19,"If this [comment is true](https://github.com/kennethreitz/requests/issues/239#issuecomment-6180202) and requests.async has been removed and is now GRequests, then we want to make sure that people are being pointed in the right place :). 

If this isn't the case then disregard this request.

Thank you for all your work @kennethreitz
",dalanmiller,kennethreitz
664,2012-06-10 20:14:25,"This is fixed in shazow/urllib3@1c22dec48bc57199d71b86476dbed8f027877824, feel free to pull @kennethreitz.

Also worth noting that this bug is partly due to the improperly encoded param. It does not manifest if we do this instead:


",shazow,kennethreitz
664,2012-06-19 13:20:52,"Thanks @shazow @kennethreitz!
",lsemel,kennethreitz
658,2012-07-03 11:19:27,"@simukis Well, the user is passing the string to `requests` and **not** to a standard library.
",schlamar,simukis
658,2012-07-03 18:26:59,"what @simukis said
",kennethreitz,simukis
655,2012-06-06 06:35:08,"That's... interesting. @shazow ?
",kennethreitz,shazow
655,2012-06-15 18:31:38,"@shazow, any insight here?
",kennethreitz,shazow
655,2012-06-15 18:36:15,"@ms4py Can you reproduce this with plain urllib3? If so, could you open a bug there please?
",shazow,ms4py
655,2012-06-25 16:40:40,"@ms4py merged. It'll be included in the next release of requests. :cake:
",kennethreitz,ms4py
632,2012-05-29 20:49:41,"Hi @kennethreitz,

Any update on this pull request? I understand if you are busy with other things.
Jus trying to ""bug the maintainer"" :) 

Also let me know if this requires any other change.

Thanks,
Arup  
",amalakar,kennethreitz
627,2012-05-22 13:57:29,"-- edit --

This relates to finding of @Lukas, who found, that **hooks are not passed to a new request on redirect**, which is not described elsewhere (yet?).

As soon as this hook related problem is resolved, there seem to be solution using hook, removing problem making header Authorization from final redirecting request (as sigmavirus24 has found).

-- end of edit --

I use web app, which first asks user to authenticate (usign digest auth) and then redirects him to temporarily valid url at AWS s3 storage.

Using requests fails to handle this correctly, it reports 401.

Asking for easy pages does not make these problems.

Using the app over web browser works well.

I created test utility::



test_redirect_to_awspage fails by 401.

And here is simple web app, written in cherrypy::



Assuming you have cherrypy installed, just run the script which will provide what is needed for testing locally.
",vlcinsky,Lukas
627,2012-11-26 22:11:01,"@vlcinsky in my impatience I installed cherrypy in a virtualenv and ran your script. If you instead return the response for the awspage function instead of calling assertions, I get 



What's interesting is that both the Signature query string and Authorization header can not both be specified. There-in lies your problem.

You can probably fix this with a hook that will remove the auth header on redirect. @kennethreitz @Lukasa @piotr-dobrogost would any of you be able to help him write the hook?
",sigmavirus24,kennethreitz
627,2012-11-26 22:11:01,"@vlcinsky in my impatience I installed cherrypy in a virtualenv and ran your script. If you instead return the response for the awspage function instead of calling assertions, I get 



What's interesting is that both the Signature query string and Authorization header can not both be specified. There-in lies your problem.

You can probably fix this with a hook that will remove the auth header on redirect. @kennethreitz @Lukasa @piotr-dobrogost would any of you be able to help him write the hook?
",sigmavirus24,piotr-dobrogost
627,2012-11-26 22:11:01,"@vlcinsky in my impatience I installed cherrypy in a virtualenv and ran your script. If you instead return the response for the awspage function instead of calling assertions, I get 



What's interesting is that both the Signature query string and Authorization header can not both be specified. There-in lies your problem.

You can probably fix this with a hook that will remove the auth header on redirect. @kennethreitz @Lukasa @piotr-dobrogost would any of you be able to help him write the hook?
",sigmavirus24,Lukasa
627,2012-11-26 22:49:03,"Oh, this has been interesting. Turns out that hooks are _not_ passed to a new request on redirect, as you can see [here](https://github.com/kennethreitz/requests/blob/develop/requests/models.py#L294). In the meantime, I can hack around this. @kennethreitz, is this a bug?
",Lukasa,kennethreitz
627,2012-11-27 12:06:10,"-- edit note--
This description was modified to include more general observation about web browser behaviour, so not only Firefox, but also Chromium and IE
-- end of edit note

@all
API of request is nice in it's simplicity. Using hook is a solution, but is it really necessary?

## Findings from my short investigation

### How are browsers behaving - using Authorization header only if redirecting into the same domain

Using Wireshark I looked at how Firefox, Chromium and partially also IE are behaving in regard to use of header Authorization in redirected requests after previous 302 (redirect) using obtained address from Location.

If redirect points into the same domain, it does reuse existing Authorization header in the request.

If redirect points out of current domain, header Authorization is never reused.

Note, that if the same IP has different domain name, then it is considered different domain. I used one domain ""localhost"" and another ""zen"" (name of my computer), both having IP 127.0.0.1.

Note about IE test: On MS Windows I had problem to track network traffic with Wireshark on localhost, which is typically invisible to Wireshark. I could only prove, that leaving to external domain from localhost was not including Authorization header.  However, it is very likely, it behaves exactly the same way as was observed with Firefox and Chromium.

### RFC2617: Authorization header allowed even without preceding 401

http://tools.ietf.org/html/rfc2617#page-4 tells:

> A user agent that wishes to authenticate itself with an origin
> server--usually, but not necessarily, after receiving a 401
> (Unauthorized)--MAY do so by including an Authorization header field
> with the request. A client that wishes to authenticate itself with a
> proxy--usually, but not necessarily, after receiving a 407 (Proxy
> Authentication Required)--MAY do so by including a Proxy-
> Authorization header field with the request.  Both the Authorization
> field value and the Proxy-Authorization field value consist of
> credentials containing the authentication information of the client
> for the realm of the resource being requested. The user agent MUST
> choose to use one of the challenges with the strongest auth-scheme it
> understands and request credentials from the user based upon that
> challenge.

So it seems, there is no strict rule, defining, if agent (browser) shall first get challenge via 401, and then be allowed to provide Authorization header.

### Developers often expect, redirection will be able to reuse Authorization

As redirect goes often within the same domain, it is natural, one expects, it will be able to authorize even on the redirected url.

I found some links about Android and iOS related bugs and implementations, which finally allowed to redirect and authorize using the same credentials. I lost the links to these pages, but it seem to follow the pattern observed in Firefox, Chromium and IE on desktop.

## Proposed solutions

### NoAutoAuthorization: Do not automatically use Authorization on first redirect, only if challenged

**Do not use this**

If there is request to be redirected, go there, but do not automatically include Authorization header.

If there comes response, challenging for authentication, do so, but not sooner.

Drawback: this generates one request more in cases, where Authorization is really requested for target url.

### HomeAutoAuthorization: Do use Authorization for redirects within the same domain

**Use this one, as it follows behaviour seen on desktop browsers**

If there is redirection, use Authorization header with first request to new url if Location aims into the same domain as original request, otherwise do not use it.

This is sort of optimized NoAutoAuthorization solution and this one I would prefer.

### FailOverAuthorization: Always use Authorization, retry without it if failed

**Do not use this**, desktop browser never attempted to reuse known credentials (could this be even sort of security risk?)

Always use Authorization header with first request to new location, but if it fails, try once more without header Authorization.

This seems a bit messy approach.

### HooksWhereNeeded: Add/Remove Authorization case by case

**Do not use this**, try to follow behaviour of desktop browsers.

In fact, this is solution, which extends any of previous ones.

The best is to avoid this, as it makes use more complex. But it is fixture which comes handy when needed.

## Final thoughts
- Existing behaviour is not simply wrong, as RFC2617 does allow Authorization header to be placed even without challenge
- However, **behaviour observed with desktop browsers Firefox, Chromium and IE shows use of  _HomeAutoAuthorization_ approach. Following the same pattern with Requests would be very intuitive and would also make use quite simple** even in the case, reported originally with this bug report.

Question: shall anybody file a bug report for currently discovered problem with hooks being sometime ignored? 
",vlcinsky,all
627,2012-11-27 14:28:16,"I love it when people quote RFC's so thank you for doing your research and giving us the necessary information. The problem is your use case seems to be in the 10% and knowing what little I do about Kenneth's design philosophy means that your proposed solutions are likely to not be implemented. As you said, requests is technically correct in how it behaves since the specification is permissive in its language. 

Furthermore, I think treating this issue as the hooks issue is the better option to avoid creating more issues than needed. We're just waiting on @kennethreitz to affirm that the exclusion of hooks from the redirected requests is not intentional.
",sigmavirus24,kennethreitz
627,2012-11-27 14:40:50,"@piotr-dobrogost :-)
My question regarding filing a bug has two sides:

#### Shall it be filed at all?

In case, the problem is almost fixed and about to be committed and adopted into code, then fileing a new issue is not worth to do.

#### Who is the best person to write such a bug report

I do my best to describe things, I am at least familiar with or where I can provide good quality description.

But with hook being ignored - I did not even try to use one so far and I would be talking about completely new topic to me.

I agree with @sigmavirus24 we could keep this issue as discussed bug report **hooks are not passed to a new request on redirect**

I will edit my original description to point to this aspect, if anyone is interested in detailing, do it.

@all this is my first experience with Github issue cooperation incorporating more then two persons - I love it. So fast, so effective. It is like chatting while being quite productive.
",vlcinsky,all
626,2012-05-22 04:40:27,"Beautiful! We'll see what @travisbot says :)
",kennethreitz,travisbot
624,2015-01-28 00:12:15,"@deusExCore start a fork and start translating the documentation in place. When we can grab his attention, we'll have @kennethreitz add you to the requests organization and help you move your fork to github.com/requests/requests-docs-tr and then he, or I will have to set up DNS to resolve that for you
",sigmavirus24,deusExCore
624,2015-06-01 19:09:32,"@csparpa please start translating and as soon as it's mostly done is when we usually import your fork into the @requests org
",sigmavirus24,requests
624,2015-06-01 19:43:39,"Got it, thanks ;)

Claudio Sparpaglione
http://csparpa.github.io
On 1 Jun 2015 9:10 pm, ""Ian Cordasco"" notifications@github.com wrote:

> @csparpa https://github.com/csparpa please start translating and as
> soon as it's mostly done is when we usually import your fork into the
> @requests https://github.com/requests org
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/624#issuecomment-107673876
> .
",csparpa,requests
624,2015-06-14 16:43:13,"So now's the time when @Lukasa or @kennethreitz should add you (@csparpa) to the translators team. They also need to set up a repository for the translation and import it from your fork. I'll set up the stuff so that it appears on python-requests.org, once they've taken care of that. (I'm not an owner/admin on the @requests org so I can't do that stuff for you.)
",sigmavirus24,Lukasa
624,2015-06-14 16:43:13,"So now's the time when @Lukasa or @kennethreitz should add you (@csparpa) to the translators team. They also need to set up a repository for the translation and import it from your fork. I'll set up the stuff so that it appears on python-requests.org, once they've taken care of that. (I'm not an owner/admin on the @requests org so I can't do that stuff for you.)
",sigmavirus24,requests
624,2015-08-24 10:26:40,"@reg4in I'm afraid we already have a translation [here](http://docs.python-requests.org/de/latest/). Help might be required to keep it up-to-date though!
",Lukasa,reg4in
619,2012-05-19 20:36:43,"@kennethreitz: How do you feel about that last commit as a first draft of a solution to #482?
",Lukasa,kennethreitz
616,2012-05-15 23:59:16,"Please do not merge this! This is intended as a ""design review"" :-) It needs more comments and tests before it is production ready.

This implements explicit disposal of sockets in urllib3 and Requests that does not rely on instantaneous garbage collection via reference counts (e.g., for PyPy). It should fix #520 without removing the `Response.request` member, or otherwise breaking the API (or the reference cycle, depending on whether you're a glass-half-full or glass-half-empty person).

It's split into three commits: one is a test case, one adds explicit disposal, one defaults `prefetch=True` and changes the tests to match.

@shazow maybe you could take a look at the urllib3 changes?

Thanks everyone.
",slingamn,shazow
614,2012-05-15 15:35:37,"@idangazit, any comments on this?
",kennethreitz,idangazit
601,2012-05-09 07:15:07,"Currently, as @djco and @arfrever note, the Makefile doesn't make it into the Pypi tarball, and a plain invocation of `nosetests` doesn't run the right tests. So if you've downloaded Requests and you want to run the tests, it's not clear how. This is a reasonable thing to want to do.

Also, we should document which tests contributors should run before they submit their pull requests. (Ideally we would encourage contributors to run `make pyflakes`.)
",slingamn,arfrever
601,2012-05-09 07:15:07,"Currently, as @djco and @arfrever note, the Makefile doesn't make it into the Pypi tarball, and a plain invocation of `nosetests` doesn't run the right tests. So if you've downloaded Requests and you want to run the tests, it's not clear how. This is a reasonable thing to want to do.

Also, we should document which tests contributors should run before they submit their pull requests. (Ideally we would encourage contributors to run `make pyflakes`.)
",slingamn,djco
600,2012-05-10 18:20:02,"@slingman apparently not ;)
",kennethreitz,slingman
592,2012-05-07 23:47:14,"Another wrinkle is that the bytes from that page do successfully decode as ISO-8859-1 (Latin-1). If they failed, there might be a case for trying `chardet` and re-decoding with chardet's guess. But in this case, we don't really seem to have a good indication not to trust the header. @ensigntropic, thoughts?
",slingamn,ensigntropic
578,2012-05-18 04:15:19,"@kennethreitz Any opinion on the error handling that @dhagrow mentions? I'm working on a patch, but I don't really think I should make that decision.
",joshimhoff,kennethreitz
571,2012-05-02 06:33:07,"Here are several atomic, largely-unrelated changes, ordered roughly from least to most controversial. Feel free to take whichever ones you like :-)
- Restore something I accidentally removed from the .gitignore
- Small documentation fix for the new cookie API
- Don't crash if we can't import chardet --- assume utf8 instead
- Cache response.text, since computing it can require a call into chardet
- Change setup.py to provide guidance for packagers (I think this is part of what @jnrowe was asking for)
- Have Travis run all the tests, not just `test_requests.py`
",slingamn,jnrowe
568,2012-04-29 23:29:05,"@pythonben It's just because I'm a n00b :-)
",esaurito,pythonben
559,2012-05-03 09:05:46,"Hmm, is there a way to kick Travis for arbitrary branches?

@travisbot kick
",slingamn,travisbot
559,2012-05-06 23:31:58,"@kennethreitz want to merge this? Looks correct, is tested, Travis likes it, it closes #526.
",slingamn,kennethreitz
556,2012-04-23 02:14:12,"@idangazit can you make this PR again?
",kennethreitz,idangazit
539,2012-05-07 00:19:21,"@kennethreitz Well, of course it will be. You're reading ""chunks"" of 1 byte at a time. However, when we talked about changing the default `chunk_size`, it was for `iter_lines()`, not `iter_chunk()` (see my pull request). The reason was to allow `iter_lines()` to behave like most people using requests would expect it to behave: Read lines as they come in. If you do use `iter_lines()` to read from a streaming API like the Twitter API, and it was used with an `chunk_size` of 10K, you may have to wait a freaking long time before you ever get any line.

That said, I'm open to alternatives, but unless you start polling the socket in non-blocking mode, the `read(chunk_size)` will block until it get `chunk_size` or the connection is closed AFAIK. Please let me know if I'm missing something.

@piotr-dobrogost If you know of another implementation to read lines as they come in from a socket while using blocking I/O, please feel free to let me (or us) know. I'm not trying to sell my quick fix to anyone, I'm simply trying to find a way to have `iter_lines()` behave like (I hope) it was meant to behave.

@kennethreitz Also, if you feel `chunk_size` should not be set to 1 in the distribution, please change it back, I don't want to impose on anybody. I can specify it in my code but I'm pretty sure you'll get regular ""issues"" opened about this thing by people wondering why their ""lines"" are not coming in when sent by the server as expected. You already had two soon after the initial code change, @gwrtheyrn and I.
",mponton,gwrtheyrn
536,2012-05-04 19:42:57,"@sashahart 

Thanks for dropping in here and sharing your knowledge with us. It's good to have cookie expert on board.

As to the backslash I'm not done yet :) - I've just posted to ongoing thread at https://github.com/sashahart/cookies/issues/1

> As author of cookies.py and a close follower of the thrilling world of RFC cookie value syntax, I think letting cookielib handle it is a very sound choice for code focused on the client side of HTTP.

I think the main reason cookielib is used is because it implements cookie jars. However as you know its parsing is not perfect - that's why you created Cookies. The perfect solution would be to use your Cookies and write implementation of cookie jars.
",piotr-dobrogost,sashahart
522,2012-03-31 05:25:59,"Blocking OAuthLib, right @idangazit ?
",kennethreitz,idangazit
520,2012-03-31 05:31:36,"@shazow any thoughts?
",kennethreitz,shazow
520,2012-05-04 22:08:03,"@bluehorn's test continues to reproduce in trunk for me. Thanks!

Here's something else interesting. If I change the test like so:



I get the same result, 20 leaked `ESTABLISHED` connections. Are we sure that keep-alive is working?
",slingamn,bluehorn
520,2012-05-06 20:44:25,"Well, we wouldn't special-case behavior inside Requests itself based on platforms. CPython would do the right thing whether or not the client called `dispose()` (which is what CPython users probably expect --- instantaneous scope-based GC). PyPy users would have to ensure that they called `dispose()`.

As for why it would matter whether the session is ephemeral or not, @craigholm makes a good point on #458: a client that does not support persistent connections ""must"" send `Connection: close`. If we're creating the session just to destroy it, that's essentially not supporting persistent connections. (Technically, you can get the session as `resp.request.session` and reuse it, but if we get rid of the reference cycle, that would also go away.)
",slingamn,craigholm
520,2012-05-07 22:26:32,"Intuitively, a property access is not supposed to have side effects; `@property` should transparently abstract away the computation of a ""derived member"", or transparently implement lazy evaluation. Closing a socket is quite a serious side effect.

Changing a default or creating even a small API break would definitely be unfortunate, but I'm having trouble seeing another way to protect the client from socket leaks.
",slingamn,property
503,2012-06-17 14:13:19,"@jups23 that's a good idea. @kennethreitz, what do you think about me removing all of the >>> prefixes from all code examples to allow easy copy/paste?
",kylerob,kennethreitz
498,2012-03-19 02:08:20,"@singingwolfboy Your pull request in #493 had a lot of fixes for this. If you're up for it, you could merge against the latest urllib3 which supports AppEngine and just make the requests-specific changes without introducing Py25 support. :)
",shazow,singingwolfboy
491,2012-03-16 16:38:00," @ronnypfannsch ?
",kennethreitz,ronnypfannsch
484,2012-03-14 14:27:47,"I found _LegalChars in monkeys.py file is not include ""@"" and it will output as Cookie: key=""@abc"" instead of Cookie: key=@abc

so that the server can not recognize the key of cookies. I think maybe more characters should be including in _LegalChars.
",listeng,abc
482,2013-10-27 00:10:11,"@Fighter42 I don't know why you commented 11 times but this is already taken care of [here](http://docs.python-requests.org/en/latest/user/advanced/#http-verbs)
",sigmavirus24,Fighter42
482,2013-11-20 14:29:47,"@Lukasa the emails I received from @RAINCEN had the same message that @Fighter42 posted about 20 times (of which I deleted all but one). I have to wonder if these are just spam accounts or if perhaps this is related to [recent attempts to brute force passwords on accounts](https://github.com/blog/1698-weak-passwords-brute-forced). Regardless, it's probably advisable to just ignore everyone who makes similar comments on this issue. It's been closed for so long.
",sigmavirus24,Fighter42
478,2012-03-10 14:22:11,"Also @kennethreitz my apologies that this wasn't branched from the 'develop' branch, by the time I had realised it was too late. On the plus side, it seems master and develop were in sync at the time of forking.
",foxx,kennethreitz
478,2012-03-14 16:59:54,"@vly I can confirm that currently the code does not currently have proxy authentication support (however afaik, proxy authentication is not currently in the original code either).

It does still however work for remote authentication (i.e. using auth= to authenticate to the site).

@kennethreitz can you confirm if you would be happy to merge the code in its current state, with proxy authentication being done at a later date?
",foxx,kennethreitz
478,2012-03-15 13:28:08,"@vly Excellent stuff - it would be great to have proxy authentication support, however this patch was contributed as open source from work done for a clients specific requirement - which sadly doesn't require proxy authentication. I'm hoping @kennethreitz will accept the patch without, and either myself or someone else will add proxy authentication support at a later date.
",foxx,kennethreitz
478,2012-03-15 13:57:34,"@shazow any comments on this? I know there was another SOCKS implementation submitted recently, right?
",kennethreitz,shazow
478,2012-03-15 14:10:03,"@shazow Let me know if you would consider merging in my changes, and if so, then I will create a new fork of urllib3 with the necessary modifications (as the patch in its current state _might_ not cleanly apply to the urllib3 repo, or at the very least would only have the new features usable via undocumented attributes).
---edited---
",foxx,shazow
478,2012-03-15 14:24:19,"@shazow Or - you could accept the patch as 'undocumented features', then apply your own changes in the future to use them in a documented fashion (as they should apply cleanly - they just won't be accessibly without setting some undocumented attributes). Then once this has been done, Python Requests would be modified to use the documented approach.

This would mean the features will be accessible quicker, without being blocked on many other issues. I was thinking of just applying the patches myself to urllib3, but it would appear you have quite a few other issues that _might_ block this, as well as a design decision needed, which would probably take up too much time for me to do myself.
",foxx,shazow
478,2012-03-15 15:17:31,"@foxx Hey there! @wolever has been helping me code review and manage with this particular feature request. Could you take a look at this @wolever?

Superquick glance at it looks alright. Our long-term redesign ambitions will make this code simpler but that's a way's out so let's not wait for that. Cleanup into urllib3's codebase and some unit tests in urllib3's suite would be great. :)
",shazow,wolever
478,2012-03-15 15:53:55,"@shazow Sounds good man. @wolever can you make sure you are happy with this code and approach, if so I will make the necessary fork/mods. ---edited---
",foxx,wolever
478,2012-03-18 16:21:19,"@wolever Any update on your thoughts for this patch?? 
",foxx,wolever
478,2012-03-19 06:49:16,"After some thought, I believe this should be possible (and, in fact, desirable) to implement this proxy support entirely in urllib3 by passing the `proxy_url=` string directly to the `HTTP[S]ConnectionPool` classes. This would imply that:
- `PoolManagerManager.__init__` should also accept a `proxy=` keyword
- `proxy_from_url` would need to have the signature `proxy_from_pool(proxy_url)` and it would need to return a `PoolManager(proxy_url=proxy_url)`.
- The `ProxyManager` would be deprecated, as it can only handle one specific kind of proxy (regular HTTP proxies).
- A bunc hof code will be added to `HTTPConnectionPool`. In general this would be suboptimal… But there are plans to do some large-scale refactoring of urllib3 anyway, and we can make the `HTTPConnectionPool` pretty then :)

The possible `proxy_url` schemes would then be something like:
- `http[s]` — for ""regular"" HTTP proxies
- `http[s]+connect` — for `CONNECT` HTTP proxies (ie, https://github.com/shazow/urllib3/pull/56)
- `socks{4,5}` — for SOCKS4/SOCKS5 proxies

Care will need to be taken so as not to confuse the scheme of the _proxy_ with the scheme of the _connection_ (ex, it should be possible to make an HTTP connection over an `https` or `https+connect` proxy, or an HTTPS connection over an `http` or `http+connect` proxy)… But apart from that, I feel like this will be a straight-forward change, especially given that you've got the tricky bits implemented already.

Below is a quick sketch of what I'm thinking of for `HTTPConnectionPool`. It's not exhaustive by any means, but between it, your SOCKSs stuff and @senko's CONNECT proxy stuff, I think this will be the best way of supporting proxies in urllib3 for now.


",wolever,senko
477,2012-03-12 02:23:42,"Thanks @piotr-dobrogost, I've updated with a (hopefully) better comment.
",maxcountryman,piotr-dobrogost
475,2012-03-08 21:25:34,"Here's a potential fix for #436 - @RonnyPfannschmidt is this what you had in mind?
",umbrae,RonnyPfannschmidt
468,2012-03-14 08:49:20,"I'm very surprised about this. If you re-read carefully <a href=""https://github.com/kennethreitz/requests/issues/65"">requests issue #65</a>, you will remember that it was me the first one trying to integrate OAuth into requests. I actually integrated the feature into requests' core the first time. But at that time, authentication needed to be polished, so you requested me to take it out and wait for the hook system.

After the hook system landed in, I redid my work turning it into a hook. I asked you what to do with it and at that time you were ""leaning more towards having a requests-oauth module"". After days without answer and people sending me requests to release it, I decided to create a separate repository for it.

It took you quite some time to list the module in the docs and I haven't seen you mention it publicly anywhere. Usually people list OAuth support as a first level feature in an http library. One day, out of the blue, @idangazit appeared and started complaining about an issue with cookies in my hook. I fixed it in a day. But he started talking about refactoring everything into core and adding Header authentication (which I didn't support at that time). I remember telling him: ""I'm open to all the pull requests you want to send me"".

Suddenly, he got assigned the task to integrate OAuth into requests' core. Several questions pop into my head: why him? why not integrating requests-oauth? How many requests libraries will benefit from a universal OAuth signing library (oauthlib)? why not reusing requests-oauth code base somehow? 

All I can say is that OAuth1.0 has more details and specifics to cover than the ones you can do in a universal signing library. Those ones take time to polish. requests-oauth2 could be merged into requests right away, as the concept of a universal library for this version of the protocol doesn't make much sense. It's orders of magnitude easier to handle and initialize.

Finally, meanwhile I was working on requests-oauth, I think I've found and submitted 4 or 5 issues in requests, for which some had patches. I also started the keepalive branch. But not a single line of code I did is included in this project.

I'm not sure why I should help in this initiative when It's my project the one that has been helping coders handle OAuth with requests for months. I've put a lot of time and effort in it already, Shouldn't it be the other way? 

When I did this specifically for requests, was because this is the future of Python's https libraries. The only thing oauthlib is covering now, that I'm missing is the provider part. I believe they are working on the provider for OAuth1.0. This is an important gap to cover in the community as there is not a single good OAuth1 Python provider.

However, more and more, OAuth2 is conquering the scene and apps like <a href=""https://github.com/hiidef/oauth2app"">oauth2app</a> do a decent job providing it. Sites like Github, started serving their API with OAuth2, facebook only uses OAuth2 at the moment, Twitter is planning the movement. OAuth1 will be with us for some time, probably years, but most likely it will fade away and luckily it's not the future of OAuth.

Cheers,
Miguel
",maraujop,idangazit
468,2012-03-14 15:07:29,"Other people actively involved in this are: @dgouldin, @pydanny, @ghickman, and @ib-lundgren. Dozens are passively involved. 

Here's a log of the stuff that went down at PyCon: https://github.com/pydanny/pycon2012-oauth-sprint/blob/master/index.rst

@ib-lundgren is a grad student that is hoping to do his thesis on OAuthlib, actually :)
",kennethreitz,pydanny
468,2012-03-14 15:07:29,"Other people actively involved in this are: @dgouldin, @pydanny, @ghickman, and @ib-lundgren. Dozens are passively involved. 

Here's a log of the stuff that went down at PyCon: https://github.com/pydanny/pycon2012-oauth-sprint/blob/master/index.rst

@ib-lundgren is a grad student that is hoping to do his thesis on OAuthlib, actually :)
",kennethreitz,ghickman
468,2012-03-14 15:07:29,"Other people actively involved in this are: @dgouldin, @pydanny, @ghickman, and @ib-lundgren. Dozens are passively involved. 

Here's a log of the stuff that went down at PyCon: https://github.com/pydanny/pycon2012-oauth-sprint/blob/master/index.rst

@ib-lundgren is a grad student that is hoping to do his thesis on OAuthlib, actually :)
",kennethreitz,dgouldin
468,2012-03-14 15:07:29,"Other people actively involved in this are: @dgouldin, @pydanny, @ghickman, and @ib-lundgren. Dozens are passively involved. 

Here's a log of the stuff that went down at PyCon: https://github.com/pydanny/pycon2012-oauth-sprint/blob/master/index.rst

@ib-lundgren is a grad student that is hoping to do his thesis on OAuthlib, actually :)
",kennethreitz,ib-lundgren
429,2012-02-14 21:56:53,"See issue #369. Once changes made by @mgiuca are merged your problem should be fixed as well.
",piotr-dobrogost,mgiuca
429,2012-03-31 06:32:51,"@mgiuca we could really use your help with a new collaboration with @mitsuhiko to merge requests and werkzeug. Would you like to help?
",kennethreitz,mitsuhiko
429,2012-04-01 09:54:45,"> (...) a new collaboration with @mitsuhiko to merge requests and werkzeug. 

Was the intention of merging published somewhere?
",piotr-dobrogost,mitsuhiko
426,2012-02-15 00:15:15,"@kennethreitz Thanks for merging. I forgot to add my name to AUTHORS. Could you please do that (Matt Giuca)?
@foxx I don't think we are talking about the same issue. I'll respond on the talk page for Issue #429.
",mgiuca,kennethreitz
416,2012-02-08 01:51:51,"@garnaat mentioning you here so you're aware of this issue.
",gtaylor,garnaat
416,2012-02-08 16:50:10,"Because we're not dealing with multi-part uploads, if I understand correctly.

Also, @kennethreitz mentioned wanting to add generator support at the same time, so the 'data' kwarg doesn't care what you feed it, as long as you give it something. Maybe it's a file, maybe it's a huge string, it probably shouldn't care.
",gtaylor,kennethreitz
399,2012-01-30 17:13:52,"Perhaps sessions are leaving connections open when they should be killed. @shazow ?
",kennethreitz,shazow
386,2012-01-25 14:55:47,"@shazow have (_very_ lightly) been discussing the idea of merging our projects in the mid-future. If that comes to fruition, that will be the time to merge test suites.

In the meantime, I'm fine with requests ""trusting"" that `urllib3` is tested and reliable.
",kennethreitz,shazow
386,2012-01-25 15:25:36,"> @shazow have (very lightly) been discussing the idea of merging our projects in the mid-future

That's really interesting.

> In the meantime, I'm fine with requests ""trusting"" that urllib3 is tested and reliable.

I was not implying it's not the case. I think it would be easier to have one test platform used by both libs. Taking into account that requests already embeds urllib3 there's no reason not to use its test platform if it supports all features requests' tests need. This way we could start taking advantage of this right away and in addition there would be less work to do when merging projects :)
",piotr-dobrogost,shazow
378,2012-01-23 00:55:58,"@kennethreitz, what do you think? :)
",johtso,kennethreitz
369,2012-02-11 19:30:01,"@stringfellow that's a bug. It's double-encoding. This happened when I added Python 3 support. I'm working on it.

@gfxmonk as I said, someone's working on a patch to make the escaping optional.
",kennethreitz,gfxmonk
369,2012-02-12 10:24:12,"@gfxmonk it's not a URL, it's an IRI. Quoting needs to be optimized and optional.
",kennethreitz,gfxmonk
369,2012-02-14 02:13:48,"Hi. I'm the author of the post @gfxmonk posted above. I've had a look at the requests library and fixed both of the bugs in this thread. My branch [py3-uri-encoding](https://github.com/mgiuca/requests/tree/py3-uri-encoding) contains both fixes, as well as test cases.

There are two separate issues: the double-encoding and the quoting of reserved characters. (Perhaps we should have two bugs and two branches, but I was short on time.)

First, the double-encoding is only a problem on Python 3 for me. In Python 2, it works great -- it unquotes and then quotes the path, which means that any illegal characters (such as '<') become quoted (to ""%3C""), while any already-quoted characters (such as ""%3E"") remain quoted. It works on Unicode characters too. However, in Python 3, it has completely different behaviour: it simply encodes the path to UTF-8 (not necessary, since Python 3's quote automatically does this if given a Unicode string) and then quotes it. This results in double-quoting: '<' becomes ""%3C"", but ""%3E"" becomes ""%253E"".

My fix was basically to remove the Python 3 specific code and make it work the same as Python 2. I had to make a quick fix to utils.requote_path, and then it worked. On Python 3, it now just does the unquote and the requote, and that works.

As for the quoting of reserved characters, that is a more tricky issue. It is essentially the same problem as I outlined in my [blog post](http://unspecified.wordpress.com/2012/02/12/how-do-you-escape-a-complete-uri/). For some reason, all of this only concerns the path (I'd have applied it to the full URI, but for now, I'm just addressing how the path is handled). The current behaviour is to quote every character in the path except for '/', '%' and most of the unreserved characters. That violates [RFC 3986](http://tools.ietf.org/html/rfc3986#section-2.2):

> URIs that differ in the replacement of a reserved character with its
> corresponding percent-encoded octet are not equivalent.  Percent-
> encoding a reserved character, or decoding a percent-encoded octet
> that corresponds to a reserved character, will change how the URI is
> interpreted by most applications.  Thus, characters in the reserved
> set are protected from normalization and are therefore safe to be
> used by scheme-specific and producer-specific algorithms for
> delimiting data subcomponents within a URI.

You should NOT be tampering with any reserved characters. But I understand the desire to a) encode illegal characters (such as spaces and non-ASCII characters) so they form a valid URI, and b) unencode unreserved characters (such as '-') so they are easier to read without changing the semantics of the URI.

This took a bit of work, but I have rewritten requote_path so that it behaves the following way:
1. Unquote only the unreserved characters, leaving reserved and illegal characters as-is.
2. Quote only illegal characters, leaving reserved and unreserved characters as-is.

Note that I have also changed how the tilde ('~') character is treated -- for some reason Python treats it as a reserved character, which is wrong. It is unreserved, so it ought to be unquoted.

That means that percent-encoded unreserved characters will end up being unquoted (which is legal, because they are equivalent either way), illegal characters will end up being quoted (which is required for a valid URI), and reserved characters will remain exactly as they were. This solves the original problem of this bug report (I can successfully pull down that eBay URL) because the special characters in the path '=' and '$' are left unquoted.

There isn't any point in making this ""optional"" -- the current behaviour is a violation of the RFC and is erroneously tampering with valid URIs. If you must make it optional, you should make the correct behaviour the default, and add an option to quote all reserved characters in the path.

I have included comprehensive test cases for all of the behaviour described above.

P.S. As you mentioned you were interested, in my [blog post](http://unspecified.wordpress.com/2012/02/12/how-do-you-escape-a-complete-uri/) I described exactly how Chrome performs URI cleaning. It is largely the same as I have implemented here, but it has a few wonky characters that don't get escaped properly.
",mgiuca,gfxmonk
362,2012-02-01 17:06:41,"@shazow, we need to send `CONNECT` first.
",kennethreitz,shazow
335,2012-02-23 21:29:47,"@Lukasa Tried using a new virtualenv, then installing requests via pip and everything worked perfectly fine like you did. 

Going to try uninstalling / reinstalling requests.

Thank you for all your help! 

p.s.

A `pip uninstall requests` and then a `pip install requests --upgrade` did the trick! Now working! Thank you @Lukasa, @KennethReitz, @piotr-dobrogost. 
",dalanmiller,KennethReitz
306,2011-12-13 16:19:18,"@piotr-dobrogost: urllib3 is, as always, not an issue. 

Probally worth noting to @shazow though.
",kennethreitz,shazow
295,2011-11-29 14:35:22,"Correct. This is currently a bit of a fundamental limitation of the current architecture, but is something that I'd like to revisit on the longterm roadmap. 

Requests no longer utilizes Poster internally. 

I believe @mitsuhiko has some thoughts on this.
",kennethreitz,mitsuhiko
295,2012-08-22 20:03:35,"The bulk of your changes are in `urllib3`, so we should probably see if @shazow is interested in your changes. (NB: the reason the tests pass is because requests doesn't test `urllib3`. =D )
",Lukasa,shazow
295,2012-08-22 21:08:59,"Yes and no. The encode_multipart_formdata() function is in urllib3, but the call to it is in requests in models.py (there's another call to it in urllib3, but that's unused by requests as far as i can see, as the body is already encoded by the time it gets that far). Requests is encoding the body before handing the request off to urllib3. It just happens to be reusing that function by importing it from urllib3. There's no reason they have to share the same copy of the function other than sanity. :)

Anyway, i'm just pointing out that, in the case that @shazow doesn't want these sorts of changes in urllib3, the patch can be changed to move that functionality local to requests. Btw, does anyone know the reason why requests doesn't just hand off the unencoded files to urllib3 but rather encodes them itself?

In any case, i'm sort of only posting this as a ""could we do something like this?"" suggestion rather than a pull request. There's a few flaws in my approach that even I noticed. I don't really have enough knowledge of the code and all the interactions to make this change. It's also almost certainly a performance regression for small files.
",zigmonty,shazow
295,2012-08-22 21:31:21,"I believe we'll have to do chunked encoding, yes.

As for httplib, @shazow would be a better person to ask ;)
",kennethreitz,shazow
293,2011-11-29 14:37:57,"This may be an unexpected side effect of @shazow's hated for the urlparse module :)
",kennethreitz,shazow
286,2011-11-24 20:53:49,"Format of cookies is defined in RFC 2109. In section 4.1 value part of cookie is defined as `token | quoted-string`. `token` is defined in section 2.2 of RFC 2068 as `1*<any CHAR except CTLs or tspecials>` and _tspecials_ are defined as `tspecials = ""("" | "")"" | ""<"" | "">"" | ""@"" | "","" | "";"" | "":"" | ""\"" | <""> | ""/"" | ""["" | ""]"" | ""?"" | ""="" | ""{"" | ""}"" | SP | HT`. According to this `:` is a special character and any cookie's value with this character should be quoted. The behavior of Requests which you observe is in line with the specification. It's worth noting that as every cookie's value can be quoted the server should accept any such cookie. Btw. `[` and `]` are special so they should be absent from `_LegalChars`'s value I guess but it's a question to @kennethreitz
",piotr-dobrogost,kennethreitz
276,2011-11-17 15:57:33,"@idangazit I can see you referenced this pull request in the main [OAUTH Feature request](https://github.com/kennethreitz/requests/issues/65).

After reading the full thread thoroughly I can see how @maraujop [already thought about doing a pull request and then decided better](https://github.com/kennethreitz/requests/issues/65#issuecomment-2257820) to move into a separate hook to avoid a potential raise in issues, bugs, etc, in requests until the OAUTH functionality gets stable enough to consider pulling it in.

I think your code is pretty cool as it brings the HEADER authentication into the mix (although less supported by third parties, it's definitely more standard-friendly and much compact to code!). But, like @maraujop and probably like @kennethreitz, I think your code would be better merged and tested within @maraujop's [requests-oauth](https://github.com/maraujop/requests-oauth) hook.

I'm sure he will be happy to add such an awesome feature to the hook!

Cheers,
",jjmaestro,kennethreitz
276,2011-11-17 15:57:33,"@idangazit I can see you referenced this pull request in the main [OAUTH Feature request](https://github.com/kennethreitz/requests/issues/65).

After reading the full thread thoroughly I can see how @maraujop [already thought about doing a pull request and then decided better](https://github.com/kennethreitz/requests/issues/65#issuecomment-2257820) to move into a separate hook to avoid a potential raise in issues, bugs, etc, in requests until the OAUTH functionality gets stable enough to consider pulling it in.

I think your code is pretty cool as it brings the HEADER authentication into the mix (although less supported by third parties, it's definitely more standard-friendly and much compact to code!). But, like @maraujop and probably like @kennethreitz, I think your code would be better merged and tested within @maraujop's [requests-oauth](https://github.com/maraujop/requests-oauth) hook.

I'm sure he will be happy to add such an awesome feature to the hook!

Cheers,
",jjmaestro,maraujop
276,2011-11-17 15:57:33,"@idangazit I can see you referenced this pull request in the main [OAUTH Feature request](https://github.com/kennethreitz/requests/issues/65).

After reading the full thread thoroughly I can see how @maraujop [already thought about doing a pull request and then decided better](https://github.com/kennethreitz/requests/issues/65#issuecomment-2257820) to move into a separate hook to avoid a potential raise in issues, bugs, etc, in requests until the OAUTH functionality gets stable enough to consider pulling it in.

I think your code is pretty cool as it brings the HEADER authentication into the mix (although less supported by third parties, it's definitely more standard-friendly and much compact to code!). But, like @maraujop and probably like @kennethreitz, I think your code would be better merged and tested within @maraujop's [requests-oauth](https://github.com/maraujop/requests-oauth) hook.

I'm sure he will be happy to add such an awesome feature to the hook!

Cheers,
",jjmaestro,idangazit
276,2011-11-17 16:27:49,"I actually integrated OAuth into requests [in an early branch within my fork of requests](https://github.com/maraujop/requests/tree/feature/oauth). I implemented it as part of requests because back then (version 0.5.1) there were no hooks and of course, no pluggable auth.

However, now that requests comes with a nicer architecture for third party contributions, I really think that keeping OAUTH out of request will definitely avoid pollution of the bug tracker with unnecesarry issues. After it's stable enough, we can decide if it's core enough to deserve a place within the main project.

@idangazit as we talked today in IRC I am more than happy to merge your implementation of header-OAUTH into my hook. I think having a nice, clean and complete implementation of OAUTH will truly benefit us all! 

Cheers,
Miguel
",maraujop,idangazit
276,2011-11-18 15:48:30,"@idangazit sure, I can see how this is not OAuth but I read the main issue #275 (where you link your gist about OAuth) then read the Feature Request for OAuth in requests and saw your mention, clicked and kept reading...

Since everything ties together and since this is the place to discuss the potential foundations of OAuth I thought that it would be very interesting to discuss this with @maraujop and to also talk about your implementation of header-based Oauth.

@maraujop's hook is the de-facto OAuth support in requests (definitely the best available) and it's already pip-installable:

pip install requests-oauth

So what about merging this pull request and then putting your gist into a requests-oauth pull request? That way we get the foundations of a better auth support in requests and we get your nice header-based OAuth thus improving the best hook available. Later, you guys can test the shit out the hook and improve it to the point where @kennethreitz can consider if it makes sense to add it to the requests core :)

Win-Win for me!
",jjmaestro,kennethreitz
276,2011-11-18 15:48:30,"@idangazit sure, I can see how this is not OAuth but I read the main issue #275 (where you link your gist about OAuth) then read the Feature Request for OAuth in requests and saw your mention, clicked and kept reading...

Since everything ties together and since this is the place to discuss the potential foundations of OAuth I thought that it would be very interesting to discuss this with @maraujop and to also talk about your implementation of header-based Oauth.

@maraujop's hook is the de-facto OAuth support in requests (definitely the best available) and it's already pip-installable:

pip install requests-oauth

So what about merging this pull request and then putting your gist into a requests-oauth pull request? That way we get the foundations of a better auth support in requests and we get your nice header-based OAuth thus improving the best hook available. Later, you guys can test the shit out the hook and improve it to the point where @kennethreitz can consider if it makes sense to add it to the requests core :)

Win-Win for me!
",jjmaestro,idangazit
272,2011-11-17 11:42:09,"@idangazit

I've moved <a href=""http://oauth.net/core/1.0/#auth_header"">OAuth bits to headers</a>, but OAuth isn't working for a simple Twitter status POST example, so I must be doing something wrong, but I cannot catch the error.



Can you spot anything wrong here? I've debugged it as much as possible.

Even though I'm not sure this will fix the cookies error, I think it's a good try. If you want to look at the code, I can put it in a branch.

Miguel
",maraujop,idangazit
251,2011-11-11 22:50:37,"Includes [Verbose logging example does not work](https://github.com/dasevilla/requests/commit/2d8440ea98301131edd537c4f2010b749f005e3b#commitcomment-712251), from @dasevilla and should close [issue #250](https://github.com/kennethreitz/requests/pull/250)
",joequery,dasevilla
234,2011-11-01 01:52:18,"via @kennethreitz
",shanemcd,kennethreitz
232,2011-10-31 13:57:45,"@bitprophet has some further thoughts on connection issues that I may or may not agree with.
",kennethreitz,bitprophet
202,2011-10-14 01:55:17,"@mitsuhiko
",kennethreitz,mitsuhiko
185,2011-10-08 04:35:06,"Oh, interesting. I didn't know about `requests.request`. That certainly mitigates my issue, and makes me agree with you even more.

I'm okay with closing this issue if @kennethreitz is.
",apetresc,kennethreitz
179,2012-07-31 15:41:36,"So I started with `_encode_files` but realized that I cannot accurately test those changes since we seem to be having problems with the tests failing as the repository is now (at least they fail on my machine and I know @Lukasa is aware of this). Once those stop failing, I'll get back to work on this.
",sigmavirus24,Lukasa
179,2012-09-03 20:56:02,"@capped if you pull from git://github.com/sigmavirus24/requests and play around on fix_key_val_args branch it should work fine. Let me know if you run into any problems please. The changes are currently in PR #833 and now that you mention it, they should also close this issue entirely.
",sigmavirus24,capped
161,2011-11-15 23:34:59,"@kennethreitz I think this problem is still there, I'm getting a lot TooManyRedirects errors.

The fix by @jerem worked for me, but had to adapt it a little bit to the latest Requests version.
",michielgardner,jerem
153,2011-09-25 23:45:40,"Thanks, @mrtazz!
",kennethreitz,mrtazz
143,2011-08-30 09:58:41,"I wonder if it's possible to build a form within the requests library.
I want to do that to implement the download in github api (python-github3).

Here is the doc for the api v3 download :
curl \
-F ""key=downloads/octocat/Hello-World/new_file.jpg"" \
-F ""acl=public-read"" \
...
-F ""file=@new_file.jpg"" \
https://github.s3.amazonaws.com/

Any advice on how to do that with the requests library ?

Bye
",kigeia,new
141,2011-09-19 19:44:02,"New proposal, as discussed with @robmadole:

All locally passed dictionaries will merge with the session-level dictionaries. If a value is set to None, that key will be removed before being passed to the request method.
",kennethreitz,robmadole
65,2011-12-28 09:22:38,"update: @idan is working on an oauth module that is library-agnostic. It'll be fully integrated into requests when it is complete.
",kennethreitz,idan
30,2011-12-19 00:46:01,"@jcwdev see https://github.com/eldarion/braintree_python/commit/634f1f3e4952de2240e110d68c81c1fca87f3a2d#comments. 

I honestly don't think it will take much effort. I'll accept a working pull requests that fits this workflow as soon as it happens :)
",kennethreitz,jcwdev
30,2012-01-24 12:40:08,"@wlz you can provide any CA Bundle.
",kennethreitz,wlz
