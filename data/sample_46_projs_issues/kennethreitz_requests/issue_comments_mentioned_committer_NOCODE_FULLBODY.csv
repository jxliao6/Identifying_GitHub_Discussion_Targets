issue_num,datetime,body,login,mention_login
3940,2017-03-26 18:39:39,"@Lukasa Thanks for your comments!

I was able to resolve this by explicitly specifying the certificate like below:
os.environ[""REQUESTS_CA_BUNDLE""] = ""F:/emulator_certificate.cer""

I wanted to avoid checking-in this certificate in my file system for running my automated tests. Is there a way in which Python can look into my Windows Certification Manager store to see if this certificate is installed and use it rather than me having to explicitly export it and provide it as above?
",rnagpal,Lukasa
3940,2017-03-27 04:24:40,"@Lukasa Thanks for the suggestion!
I went ahead and used wincertstore and it works without me having to extract and specify the certificate.",rnagpal,Lukasa
3939,2017-03-24 10:49:09,"Thanks @Lukasa here is the code that failing

",a-sharma11,Lukasa
3939,2017-03-24 11:01:17,"@Lukasa Yes it should be the same in this case(hardcoded shown below). Also I made one simple request using postman which should have only triggered this once and got the same error.
`self.url = base_url + ""/api/platform/v1/session""`

and `base_url` is const and in this case name of the other linked container",a-sharma11,Lukasa
3928,2017-03-23 16:55:33,@kennethreitz perfectly reasonable to do it with 3.0,sigmavirus24,kennethreitz
3924,2017-03-15 08:37:08,"@Lukasa 
Python version: 
Requests version:
Other packages:


Here is my relative code: , and I also use gevent in my app.
",i2it,Lukasa
3924,2017-03-15 08:39:49,"@Lukasa Ok, I'll have a try.",i2it,Lukasa
3922,2017-03-14 15:42:26,"Yep, this is a bug in the last release of pipenv. There's a patch underway, thanks @Lukasa!",nateprewitt,Lukasa
3919,2017-03-08 17:03:16,"@Lukasa, are you sure this is fixed in 3.6? The redirect URL being passed back is encoded as utf-8 but we're decoding it as ISO-8859-1 which is what's causing the issues. I'm getting a `TooManyRedirect` exception when trying to use Python3.6 but the request works if I pass the properly decoded Location header: `http://uae.souq.com/ae-ar/ابل-ايفون-7-مع-فيس-تايم-32-جيجا-الجيل-الرابع-ال-تي-اي-ذهبي-11526690/i/`.",nateprewitt,Lukasa
3913,2017-03-07 09:35:52,"Cool, this looks good. I'll wait for the build to go green and merge. Thanks so much @StyXman! :sparkles:",Lukasa,StyXman
3908,2017-03-04 20:56:49,"@Lukasa: Interesting—I'd have thought it's using the system's but apparently it's bundled one.



Would that be the cause?  FWIW, the calls to https://localhost:8080 validate successfully when using `REQUESTS_CA_BUNDLE=cert.pem`, it's everything else that fails.",dbazile,Lukasa
3908,2017-03-04 21:04:50,"@sigmavirus24: They're not in `cert.pem`.

But it's not the same behavior that I saw in Python 3.5 (which checked `REQUESTS_CA_BUNDLE` first and fell back on the system's trust store for everything else).

Is there a way to configure this previous behavior without `cat`ing the entire system's trust chain into cert.pem?",dbazile,sigmavirus24
3908,2017-03-04 21:12:13,"I think you're right; Python 3.5 didn't bundle OpenSSL but 3.6 does:



I think my solution is to grab and concat all the certs my code will be using into an actual bundle and use that instead.

Thank you much, @Lukasa @sigmavirus24!",dbazile,Lukasa
3908,2017-03-04 21:12:13,"I think you're right; Python 3.5 didn't bundle OpenSSL but 3.6 does:



I think my solution is to grab and concat all the certs my code will be using into an actual bundle and use that instead.

Thank you much, @Lukasa @sigmavirus24!",dbazile,sigmavirus24
3906,2017-03-04 13:47:22,"@Lukasa, while I think this should be closed (since it's not a Requests bug), I also wonder if the selectors code in urllib3 should attempt to fallback to less efficient methods in cases like this and only then let the OS error bubble up. Thoughts?",sigmavirus24,Lukasa
3906,2017-03-04 14:11:32,"@sigmavirus24 Yeah, so I said a few comments ago that the selectors module can detect this case by actually trying to *instantiate* the selectors, at least for some of them where the selector itself is an FD (we can't do it so easily for poll/select). That's why I tagged @SethMichaelLarson: this is a bit his baby.",Lukasa,sigmavirus24
3904,2017-03-01 21:50:01,@Lukasa I was just going to that direction. Thanks.,brangi,Lukasa
3904,2017-03-02 00:51:39,"I'm going to close this, as this isn't an issue with Requests. Cheers @brangi @lutzhorn!",sigmavirus24,lutzhorn
3901,2017-03-01 20:05:39,"Nice first commit, thanks @nedbat!",Lukasa,nedbat
3899,2017-03-01 08:23:34,"Requests will avoid aggressively handling your string if you do what @patallen suggests, which should resolve your problem, but @lutzhorn is right: the server should accept both, and is at fault here. :smile:",Lukasa,lutzhorn
3899,2017-03-01 09:31:49,"Hi All

thank you for your help
@lutzhorn is true, and i have reported the problem as well, but i'm sure that is a bit difficult to solve from the server part, is Asterisk the open source PBX and have a very large list of issues.
@TetraEtc yes but the URL is into the internal network and have no external access

@Lukasa and @patallen , thank you i will use your suggestion, hope is work well

",ogonbat,lutzhorn
3899,2017-03-01 09:31:49,"Hi All

thank you for your help
@lutzhorn is true, and i have reported the problem as well, but i'm sure that is a bit difficult to solve from the server part, is Asterisk the open source PBX and have a very large list of issues.
@TetraEtc yes but the URL is into the internal network and have no external access

@Lukasa and @patallen , thank you i will use your suggestion, hope is work well

",ogonbat,TetraEtc
3899,2017-03-01 09:31:49,"Hi All

thank you for your help
@lutzhorn is true, and i have reported the problem as well, but i'm sure that is a bit difficult to solve from the server part, is Asterisk the open source PBX and have a very large list of issues.
@TetraEtc yes but the URL is into the internal network and have no external access

@Lukasa and @patallen , thank you i will use your suggestion, hope is work well

",ogonbat,Lukasa
3897,2017-03-01 17:24:33,"@nateprewitt nice work on this, thanks for getting this over the finish line",davidsoncasey,nateprewitt
3892,2017-02-27 16:23:12,"This looks good now, I think. @vpfautz if you feel like it, this can be patched in [urllib3](https://github.com/shazow/urllib3) too.

Also thanks for catching this! I shouldn't be typing without spell check in my IDE :)",nateprewitt,vpfautz
3888,2017-02-24 17:53:04,"@nateprewitt Nah, I think these are different issues. This is because requests *does* urlencode the redirect header, but it also *character* encodes it, wrongly.

But yeah, I'm prepared to believe that that example is a good example of how we get it wrong. Happily, ISO-8859-1 is round-trippable.",Lukasa,nateprewitt
3885,2017-02-23 21:15:26,@Lukasa thanks for your help with this. As for squashing up all of the commits this PR has accumulated--who should do that? Do the maintainers of this lib do that? Or I can squash locally and push to a new PR (or just force push).,davidfontenot,Lukasa
3881,2017-02-19 21:34:15,"Thanks for this request!

So I agree with @sigmavirus24. While the `response.url` field is a handy convenience, in general we make sure that the request that triggered a given response is always available at `response.request`, and can be introspected. 

If you didn't find that obvious, I'd be delighted to merge a pull request with clarifying documentation. ",Lukasa,sigmavirus24
3879,2017-02-17 08:37:48,@Lukasa what the highest version that python 2.6.6 supports?,duyanghao,Lukasa
3879,2017-02-17 08:43:20,"@Lukasa i have googled a lot for the error: `ConnectionError: [Errno 2] server failed`,but nothing has been found!
Do you have any idea about this strange error?",duyanghao,Lukasa
3879,2017-02-17 08:50:19,"the second exception:`ConnectionError: [Errno 3] name does not exist` maybe be related to domain name resolving,but the first exception:`ConnectionError: [Errno 2] server failed` is unrelated to the second exception and has nothing to do with domain name resolving.

@Lukasa do you have any idea about the first exception?",duyanghao,Lukasa
3879,2017-02-20 03:03:29,"@Lukasa Why Linux errno 2 `ENOENT` means having problems resolving the domain name,as far as i know,the `ENOENT`  means `No such file or directory`.
",duyanghao,Lukasa
3879,2017-02-20 08:22:29,"@Lukasa it is a mutil-process server,and the error occurs sometimes,so i am afraid it can't be straced easily!",duyanghao,Lukasa
3879,2017-02-20 08:40:16,"@Lukasa anyway,thanks for your response!

still,actually,there is another error:
12 SSLError: [Errno 185090050] _ssl.c:330: error:0B084002:x509 certificate routines:X509_load_cert_crl_file:system lib：
2017-02-20 10:59:31,890 ERROR: Exception on /v1/images/1b6c5dbe537403c916f6ea668632a05099a09cdde9a668543e4da2eab5a6b179/layer [GET]
Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/flask/app.py"", line 1687, in wsgi_app
    response = self.full_dispatch_request()
  File ""/usr/lib/python2.6/site-packages/flask/app.py"", line 1360, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/usr/lib/python2.6/site-packages/flask/app.py"", line 1358, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/usr/lib/python2.6/site-packages/flask/app.py"", line 1344, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/usr/lib/python2.6/site-packages/docker-registry/docker_registry/toolkit.py"", line 264, in wrapper
    return f(*args, **kwargs)
  File ""/usr/lib/python2.6/site-packages/docker-registry/docker_registry/images.py"", line 35, in wrapper
    return f(*args, **kwargs)
  File ""/usr/lib/python2.6/site-packages/docker-registry/docker_registry/images.py"", line 56, in wrapper
    return f(*args, **kwargs)
  File ""/usr/lib/python2.6/site-packages/docker-registry/docker_registry/lib/mirroring.py"", line 140, in wrapper
    flask.request.path, stream=stream, source=source
  File ""/usr/lib/python2.6/site-packages/docker-registry/docker_registry/lib/mirroring.py"", line 42, in lookup_source
    stream=stream
  File ""/usr/lib/python2.6/site-packages/requests/api.py"", line 55, in get
    return request('get', url, **kwargs)
  File ""/usr/lib/python2.6/site-packages/requests/api.py"", line 44, in request
    return session.request(method=method, url=url, **kwargs)
  File ""/usr/lib/python2.6/site-packages/requests/sessions.py"", line 279, in request
    resp = self.send(prep, stream=stream, timeout=timeout, verify=verify, cert=cert, proxies=proxies)
  File ""/usr/lib/python2.6/site-packages/requests/sessions.py"", line 374, in send
    r = adapter.send(request, **kwargs)
  File ""/usr/lib/python2.6/site-packages/requests/adapters.py"", line 213, in send
    raise SSLError(e)
SSLError: [Errno 185090050] _ssl.c:330: error:0B084002:x509 certificate routines:X509_load_cert_crl_file:system lib


",duyanghao,Lukasa
3877,2017-02-16 13:17:30,"@sigmavirus24 That PR was merged on the grounds that it was consistent with the docstrings that relied on that method. If we're changing one, we should change them all. =)",Lukasa,sigmavirus24
3877,2017-02-16 13:18:53,"@Lukasa which docstrings are those? I was under the impression that the docstring format was what was borrowed from, not the content of other docstrings.",sigmavirus24,Lukasa
3875,2017-02-15 05:24:11,"Thanks, @nateprewitt! Turns out AWS4Auth is adding headers as unicode strings, but Python 2.7 httplib.py or `requests` do not expect that and fail when concatenating the (binary?) body string. There's an easy fix to AWS4Auth for it:
https://github.com/sam-washington/requests-aws4auth/issues/29",jamshid,nateprewitt
3874,2017-02-21 18:13:30,"I think it's good to go @jvanasco, we were just waiting for confirmation from @kennethreitz. @Lukasa we can probably just merge though since this isn't actually a change, yeah?",nateprewitt,Lukasa
3874,2017-02-21 18:13:30,"I think it's good to go @jvanasco, we were just waiting for confirmation from @kennethreitz. @Lukasa we can probably just merge though since this isn't actually a change, yeah?",nateprewitt,jvanasco
3873,2017-02-14 15:47:28,"Great! @Lukasa, when you have a free moment, would you mind merging master in proposed/3.0.0 so I can rebase my changes for #3338 onto this?",nateprewitt,Lukasa
3872,2017-02-14 14:08:20,"@Lukasa thanks for the feedback!

I understand the concerns for performance (I care more about being able to do one less request than the security aspects to be frank). I think we could use a disk-based format that makes the lookups fast and memory-efficient?",sylvinus,Lukasa
3869,2017-02-12 07:33:08,"This is an annoyance, but looks right. Thanks @nateprewitt!",Lukasa,nateprewitt
3868,2017-02-11 21:22:03,@Lukasa of course. No rush. Thanks a lot!,vmalloc,Lukasa
3868,2017-02-12 07:40:38,@Lukasa will rebase and push again.,vmalloc,Lukasa
3866,2017-02-13 17:17:08,@jvanasco yes but please make that commit message easier on the eyes :),sigmavirus24,jvanasco
3859,2017-02-13 13:01:30,@PatrickDChampion @Lukasa and I both have our emails on our profiles so you can do just that. =),sigmavirus24,Lukasa
3855,2017-02-07 14:28:40,"Further there are already some flawed and unofficial hints in https://github.com/python/typeshed. We won't endorse those, but they already exist, so there's less work for you to do.

Also, in the spirit of being explicit, I wholeheartedly agree with @Lukasa.",sigmavirus24,Lukasa
3845,2017-02-06 21:38:06,"<strike>@Lukasa `If you want even more control you can pass a urllib3 Retry object as shown in urllib3's documentation instead of the integer value.` - This does not seem to work. Is this broken?

The code doesn't seem to handle this correctly. Reference: https://github.com/kennethreitz/requests/blob/master/requests/adapters.py#L110

I'll try to set it on the adapter after initialisation with:

and see if that works...</strike>

It seems to be passes through correctly. However I still get the errors. Very odd...

Edit: Turns out `POST` requests are not automatically retried...",lukas-gitl,Lukasa
3842,2017-01-31 20:17:49,"@Lukasa Many thanks. Seeing that is really seems you know what is going on here 3 qs:

1. What other Python cURL lib can I use here that you might know of that works?
2. Why would requests.put() work perfectly not specifying the data param, hence not specifying the source location of file?
3. Not being sure how cURL does this, is it not possible to also add the cURL `-T, --upload-file <file>` like option in the requests library?",Twoflower2,Lukasa
3841,2017-01-31 08:54:26,Thanks @TetraEtc!,Lukasa,TetraEtc
3840,2017-02-14 14:17:32,"Thanks for the reply,
@jvanasco Inexplicably this error doesn't happen again since yesterday... Obviously it was a bug in the space-time continuum... And I still don't understand why there was no problem with urllib3 but this will remain a mystery.
@Lukasa the problem is that I use a library which use `requests`, and I can't catch this exception without a global catch.",azotlikid,Lukasa
3840,2017-02-14 14:17:32,"Thanks for the reply,
@jvanasco Inexplicably this error doesn't happen again since yesterday... Obviously it was a bug in the space-time continuum... And I still don't understand why there was no problem with urllib3 but this will remain a mystery.
@Lukasa the problem is that I use a library which use `requests`, and I can't catch this exception without a global catch.",azotlikid,jvanasco
3840,2017-02-14 17:01:56,"> at which point the response object will already be in their hands. 

@Lukasa you are 100% correct.  I've been staring at the inner workings too long.  my apologies!",jvanasco,Lukasa
3840,2017-02-14 17:04:05,";) no need for apologies @jvanasco, this is why we work in groups: it's easy for any one of us to miss the wood for the trees.",Lukasa,jvanasco
3839,2017-01-31 15:02:14,@Lukasa Thanks!,cmanallen,Lukasa
3837,2017-01-29 20:55:18,"regarding the comment from @lukasa on general utility (not the suggested implementation): i disagree about broad utility of following non-location redirects.  i know this means allowing users to operate on ""html"", but please consider that many consumers will eventually consume the HTML redirect (which could be a meta-refresh, rel=""canonical"", type=""og:url"" or several others).  it's not a niche use, but a common one.

regarding the implementation details comment from @sigmavirus24:  I do agree. stashing headers was a way to not suggest a larger patch.

i was a bit surprised the redirect handling used a `while` loop to create a generator that is immediately consumed, and then just overrides the request.   i had looked through the commits and tickets, and it seems like approach was dictated by earlier api behaviors that no longer exist (or I haven't seen).",jvanasco,sigmavirus24
3836,2017-01-27 20:55:23,Thanks @nateprewitt!,Lukasa,nateprewitt
3835,2017-01-26 21:23:36,I believe @nateprewitt is correct. It seemed to me that the conclusion of the discussion was to support 4XX response codes.,mmedal,nateprewitt
3835,2017-01-27 11:51:25,"Hrm. @mmedal, can you rebase on top of the current master? We seem to be having some build problems and I want to check whether they're ones we've seen before and fixed, or new ones.",Lukasa,mmedal
3835,2017-01-27 19:43:49,@Lukasa this is a bug in pipenv (kennethreitz/pipenv#90) that may not have an immediate solution. It may be best to pin Requests' dependency for a bit. `pipenv==3.1.9` should work for now.,nateprewitt,Lukasa
3835,2017-01-27 20:33:02,@nateprewitt I'll merge a PR that does that. 😁,Lukasa,nateprewitt
3832,2017-01-25 15:40:26,"LGTM, thanks @nateprewitt!",Lukasa,nateprewitt
3825,2017-01-20 13:00:06,"Thanks for the quick response @Lukasa 

Please have a look again at the first comment, which I edited shortly after publishing.

Basically: Even if I we discussed and agreed on the inconvenience of the 'leaky bucket' strategy (which will probably not happen), it's clear that this would be a major functionality change and therefore it would make no sense to force its introduction.

However, I think that a parameter could be added to provide such a functionality. Otherwise, as I said, there is no way to actually limit the amount of outgoing connections without taking care of it in a higher level.",csala,Lukasa
3825,2017-01-20 13:05:01,"@Lukasa No, no, what I mean is something else.

I mean another parameter, call it `fail_if_empy` to indicate that, if `block` is `False` and the connection pool is empty, an Exception will be raised instead a new connection created.

Functionally, what I would like to achieve is being able to use the connection pool to make sure that the number of concurrent outgoing connections will never exceed a given number ( `pool_maxsize`)",csala,Lukasa
3825,2017-01-20 13:16:06,"@Lukasa Well, that's not entirely true. Basically because that part of the code can never be reached because of how the queue works and because `pool_timeout` is not being used.

At the moment, if `block` is set to `True`, the `pool.get` call will never raise an exception because it will just block the `get` attempt indefinitely until a connection is available.

However, it's true that if the `pool_timeout` is indicated and reached, it would work as I expect. And it could even be set to 0 to make it fail immediately.

But, on the other side, without such a `fail_if_empty` parameter, you cannot have it following the 'leaky_bucket' strategy along with a timeout!
If there was this parameter, you could make it wait for some time and, if no connection was available after the given timeout, go on and create a new one, which currently is not possible.",csala,Lukasa
3825,2017-01-20 13:22:10,"@Lukasa Alright, I'll go for that bit.",csala,Lukasa
3824,2017-01-18 21:21:02,"@Lukasa thank you so much for helping me on this issue.
Ok, so I understand that I'm now using an installed PyOpenSSL lib on this system, rather than the PyOpenSSL that comes embedded with ""requests"", which explains the difference with my other systems.

If I get it right (provided I'm using Python2): all strings in my code are of type 'str' by default and I guess that's what you call native string (is it?). So it looks like all my headers (name or value) are already native python string. Still, just in case, I tried to change my code to add `.encode('ascii')`on all my header strings => no luck, still the same error.

So I've reduced my testing scenario down to the basic following test in python command line:

So it turns out not to be a header issue, but rather the data itself returned from my `Crypto.xor` function, which is very basic:


I hope you'll get to see what's wrong in this piece of code that PyOpenSSL doesn't like... thanks again for your help.",Arno0x,Lukasa
3821,2017-01-14 12:50:19,"@Lukasa hidden in the original message is:

",sigmavirus24,Lukasa
3821,2017-01-14 14:56:57,"@sigmavirus24 
I am surfing the internet, and run this code , so connect is not the problem.
I am still getting the error:
requests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None))
That's weird! ",bifeng,sigmavirus24
3819,2017-01-15 18:58:19,"@Lukasa thanks for the commet! I already propose the changes on that repository.

",llazzaro,Lukasa
3817,2017-01-14 10:22:03,"It'd also help if I took @Lukasa's advice and actually read the whole issue that spawned this.

I don't consider the current import performance to be a bug, but if someone wants to try to improve it on cryptography (which is probably the dominant source of import time for pyopenssl) I'm happy to review. The biggest single source of time is likely when it loops over the lib object to build a new conditional lib object. That could be optimized if cffi supported a means of conditional binding, but we're likely talking only ~10ms?",reaperhulk,Lukasa
3817,2017-01-14 12:48:17,"@Lukasa @reaperhulk so it's worth noting that `slow` will change based on how many things are installed + how many possible entry-points exist. `pkg_resources`, if I remember correctly, will scan all of `site_packages` for entry-points. So if you're testing `pyOpenSSL` in a fresh virtual environment, you're import speed (given that cryptography apparently scans entry-points at import) is going to be faster than @dsully since they seem to be installing a lot of things in one `site_packages` directory.

Granted, this is a fundamental flaw of how `pkg_resources` works, but I think it's still a legitimate problem. I haven't looked at what cryptography uses `pkg_resources` to find at import, but there would only be ""import time"" benefits to avoiding that scan rather than any real performance benefit to not doing it when cryptography is imported.

I'd also like a better understanding of `slow` from @dsully + maybe a better description of how much is being installed into their site-packages directory.",sigmavirus24,Lukasa
3817,2017-01-16 20:04:17,"@reaperhulk Yes - removing the pkg_resources import until it's needed will certainly help. I'll create a PR over there to move the import.

@sigmavirus24 nails it - we have a lot installed in site-packages, and are extremely sensitive about CLI tools start up time. Every millisecond counts in the eyes of our users. I do agree this is a fundamental flaw in pkg_resources. (My weekend was pretty packed, so I wasn't able to reply until now).

I do feel that requests automatically trying to use pyopenssl if it's installed without any way to opt-out is surprise functionality. I do have a work around for now, so this PR can be dropped.",dsully,sigmavirus24
3815,2017-01-13 12:56:53,Thanks @Lukasa.,Th30n,Lukasa
3814,2017-01-16 21:16:58,@Lukasa One question: won't implementation of `ssl_wrap_socket` override custom CA certificates by calling load_verify_locations inside?,Kentzo,Lukasa
3814,2017-01-16 21:45:29,"@Lukasa That's how I did it so far:

",Kentzo,Lukasa
3814,2017-01-16 22:18:41,"@Lukasa Something like this:


?",Kentzo,Lukasa
3812,2017-01-12 17:12:51,Thanks @inglesp!,Lukasa,inglesp
3811,2017-01-11 21:53:20,"@Lukasa Hmm, I was hoping that would work. I ran the following and got the same response.

import requests
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.poolmanager import PoolManager
from requests.packages.urllib3.util.ssl_ import create_urllib3_context
CIPHERS = (
    'ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:ECDH+HIGH:'
    'DH+HIGH:ECDH+3DES:DH+3DES:RSA+AESGCM:RSA+AES:RSA+HIGH:RSA+3DES:!aNULL:'
    '!eNULL:!MD5'
)
baseurl='https://webapps.kdads.ks.gov/LSOBP18'
class DESAdapter(HTTPAdapter):
    def init_poolmanager(self, connections, maxsize, block=False,*args, **kwargs):
        context = create_urllib3_context(ciphers=CIPHERS)
        kwargs['ssl_context'] = context
        self.poolmanager = PoolManager(num_pools=connections,
                                       maxsize=maxsize,
                                       block=block,
                                       *args, **kwargs)
s=requests.Session()
s.mount('https://10.192.8.89', DESAdapter())
s.get(baseurl, verify=False)",2tim,Lukasa
3809,2017-01-11 09:49:57,Thanks for this @JungWinter! :sparkles: :cake: :sparkles:,Lukasa,JungWinter
3807,2017-01-11 15:15:10,"> I'd say the server is at fault here.

I completely agree the server isn't compliant here, but we also just said we'd *like* to be tolerant of things like this in #3794, which is even more out of spec. The server is definitely returning garbage though, so maybe we choose not to address this.

@sigmavirus24 I'm currently able to reproduce this on Python 2.7.12 and 3.5.2 on Mac OS 10.12.2, Ubuntu 12.04, and [Travis](https://travis-ci.org/nateprewitt/requests/builds/190981913).

The repro won't fail if you start it at the second hop (the https url), so it seems to require this specific set of responses. It's definitely related to the transfer-encoding because chaining a similar set of responses ((http)302->(https)302->(separate server)200) from httpbin won't trigger the failure. 

This ""bug"" was introduced in Requests 2.7.0 (urllib3 1.10.4) but masked by two separate `try/except AssertionError` blocks. The first was removed in 2.8.0 in c6c8d64 but this didn't expose the issue because the `content` exception block was still catching it. 327512f removed the second safeguard which is why 2.12.x is now showing this. I backported 327512f and was able to confirm this only started happening after the chunk transfer work in urllib3 1.10.4. Something causes the underlying socket closed before we can read it for the redirect, but I wasn't able to immediately find what.

At this point this is probably too much digging for a pretty uncommon edge case, but I'll let you two decide.",nateprewitt,sigmavirus24
3807,2017-01-11 18:07:00,"Hi @Lukasa, your analysis of it being caused by the double consumption of raw (first by the `copyfileobj`, and then by the `response.content` in `resolve_redirects`) matches what I saw in the debugger so I'm pretty confident your work-around of setting `._content` will work for me.  Thanks for your deep dive into this.",gilessbrown,Lukasa
3803,2017-01-09 19:41:40,"Yup, this is strictly a Python syntax problem and not a Requests issue. Thanks @nateprewitt. :D",Lukasa,nateprewitt
3802,2017-01-09 11:46:32,"I agree with @Lukasa. While I know a significant number of people use this function, it's not a public API that we support and even so, this is the one of the least optimal ways of accomplishing what you want.",sigmavirus24,Lukasa
3797,2017-01-24 15:51:24,@Lukasa Sorry to bother on closed issue but I'm wondering why should this fix not being updated in Request?,RobGThai,Lukasa
3794,2016-12-29 17:47:44,"@nateprewitt yes, they should fix that there, but we should also be able to tolerate this.",sigmavirus24,nateprewitt
3794,2016-12-29 19:22:28,"@Lukasa no, it won't because that's where the error is being raised from. httplib raises a IncompleteRead error of 0 bytes because the connection is terminated without sending anything.",nateprewitt,Lukasa
3794,2017-01-14 12:42:39,Good catch @nateprewitt ,sigmavirus24,nateprewitt
3790,2016-12-30 03:55:14,"@drpoggi, why instead of trying to solve the problem with slow code - you are trying to shift the responsibility for organizational activities?
",djbaldey,drpoggi
3790,2016-12-30 04:45:45,"@drpoggi, I understand all of this. Therefore, I suggest to remove the trying from urllib3 module initialization. I don't think that the most important component and it is more important than speed loading the module. I am sure that those who need urllib3 with TLS - they can write these strings in the app after loading the module. And it will be right.
Upd:
In addition, in the internal corporate network - [""advanced features TLS""](#issuecomment-269605327) may be banned or make mistakes.
",djbaldey,drpoggi
3790,2017-01-01 15:16:41,"Needless to say, I agree with @Lukasa. Being able to provide the best possible security out-of-the-box (without futher user configuration) is absolutely imperative. We will not sacrifice that.

If you are doing `apt-get install -y python-requests` and it's installing `PyOpenSSL` for you, that means the packagers understand this *and* they understand that the system Python you're using doesn't allow for the best possible security for you and every other user. Now, let's try to get to the bottom of thos.

@djbaldey can you determine how long it takes on your system to do `import cryptography`?",sigmavirus24,Lukasa
3789,2016-12-26 18:21:05,"@moin18 that will be in place until sigmavirus24 has a chance to review your changes again. Today is still a ""holiday"" for a lot of people working in the US, so sigmavirus24 likely won't respond immediately. Things look pretty cleaned up now, so I think you can leave these changes and he'll respond when he has a spare moment :)",nateprewitt,moin18
3789,2017-01-10 20:55:01,"@moin18 would you be willing to rebase this rather than merging master into this branch? Alternatively, @Lukasa how do you feel about squash merging this?",sigmavirus24,moin18
3789,2017-01-17 15:06:01,@sigmavirus24 @Lukasa cleaned up the commit history. Merged all commits into single commit.,moin18,Lukasa
3789,2017-01-17 15:06:01,@sigmavirus24 @Lukasa cleaned up the commit history. Merged all commits into single commit.,moin18,sigmavirus24
3787,2016-12-23 15:12:58,"@Lukasa : In that case I will be creating a separate patch for urllib3 discarding the urllib3 related changes from this PR. Apart from that, does the `requests` library related changes looks good to you?",moin18,Lukasa
3786,2016-12-22 19:56:51,"@moin18 , No my api endpoint is fine. When I hit the url from browser it returns correct data. 

@nateprewitt , Thank you. Firstly my requests version is 

Here is my encoded try 
 
without encoding 

Also tried with encoding whole url  but request didn't made. 
",Andriuskislas,moin18
3786,2016-12-22 19:56:51,"@moin18 , No my api endpoint is fine. When I hit the url from browser it returns correct data. 

@nateprewitt , Thank you. Firstly my requests version is 

Here is my encoded try 
 
without encoding 

Also tried with encoding whole url  but request didn't made. 
",Andriuskislas,nateprewitt
3786,2016-12-23 01:36:34,"@Andriuskislas I would bet that @nateprewitt is correct about redirects being the root cause here. You can also tell requests not to follow them by passing `allow_redirects=False` to your request. It's plausible the server you're talking to is redirecting based on something like User-Agent. Either way, @nateprewitt's suggestion of asking a question on StackOverflow is correct. This isn't a general support forum, but a place for defects, which I don't think you've found.",sigmavirus24,nateprewitt
3785,2016-12-21 18:11:31,"@Lukasa it looks like this is a modification in the appengine module in urllib3. The warning update may need to go over there, otherwise we'll overwrite it the next time we update urllib3.",nateprewitt,Lukasa
3785,2016-12-21 23:10:48,@Adusei would you be interested in submitting this patch to [urllib3](https://github.com/shazow/urllib3) so this can be fixed at the source?,nateprewitt,Adusei
3785,2016-12-23 21:07:21,"Great, things should be set for future releases then. Thanks for taking a look @Adusei!",nateprewitt,Adusei
3783,2016-12-22 00:46:46,@moin18 The problem I ran up against was that `idna` is part of `packages` so whenever `packages` is loaded (e.g. in `models.py`) `idna` will be loaded at that point. So the fix would be for `idna` not to be loaded at that point but somehow still be loaded when needed :/,DanielGibbsNZ,moin18
3781,2016-12-21 13:40:05,"Thanks for this patch @mplonka. However, we would generally prefer to update the bundled packages ourselves at the time of release: that way, we have some confidence that we have done the sensible thing. Can you pare this patch down to just the makefile change, please?",Lukasa,mplonka
3781,2016-12-21 13:48:30,"Thanks for reviewing this PR, @Lukasa .

As per your request, here's the revert commit.",mplonka,Lukasa
3781,2017-01-18 16:00:10,"@Lukasa it seems that idna hasn't been updated yet, but a new version has been released in the mean time. Will idna be updated to fix the Jython issue?",LordGaav,Lukasa
3780,2016-12-22 22:46:14,"@Lukasa: `idna` is also imported in `urllib3/contrib/pyopenssl.py`. Here: https://github.com/kennethreitz/requests/blob/master/requests/packages/urllib3/contrib/pyopenssl.py#L46

In order to remove the reference of `idna` via garbage collector, corresponding changes in urllib3 are also required. The idea is to do lazy loading of idna package only on demand basis. 

Let me know if I am on correct path. If yes, I will raise the bug and pull request to change this behavior in urllib3 as well.

Sample diff is available here: https://github.com/kennethreitz/requests/pull/3787",moin18,Lukasa
3776,2016-12-22 10:40:54,"Hi @Lukasa. Thanks for looking into that.
Wouldn't it be wise to do some sort of escaping of those comments in https://github.com/Lukasa/mkcert itself? Are you OK with me submitting a PR there?",mplonka,Lukasa
3776,2016-12-22 12:02:39,"@mplonka I don't really see any reason to do the escaping there. PEM isn't well specced but so far we have only one extremely unusual implementation that chokes. I don't really see any reason to destroy that output for that, given that it's clearly intended to be human readable. ",Lukasa,mplonka
3774,2016-12-19 04:18:39,"@Lukasa thank you for your reply, but your code doesn't work for me.  
here are my code(minor modified on yours):



same error msg appears:



and under 2.11.1, there was another error which i can fix it by removing the ssl_context key:

",bigbagboom,Lukasa
3774,2016-12-20 01:07:33,"@Lukasa yes,  i installed PyOpenSSL but I don't think it's the cause. I uninstalled it and the error changed to the following:

now the pip freeze is:
C:\Users\bigbagboom>pip freeze
alabaster==0.7.9
anaconda-client==1.4.0
anaconda-navigator==1.1.0
argcomplete==1.0.0
astroid==1.4.8
astropy==1.1.2
Babel==2.3.4
beautifulsoup4==4.4.1
bitarray==0.8.1
blaze==0.9.1
bokeh==0.11.1
boto==2.39.0
Bottleneck==1.0.0
certifi==2016.9.26
cffi==1.9.1
chest==0.2.3
click==6.6
cloudpickle==0.1.1
clyent==1.2.1
colorama==0.3.7
comtypes==1.1.2
conda==4.0.5
conda-build==1.20.0
conda-env==2.4.5
conda-manager==0.3.1
configobj==5.0.6
cryptography==1.7.1
cycler==0.10.0
Cython==0.23.4
cytoolz==0.7.5
dask==0.8.1
datashape==0.5.1
decorator==4.0.10
dill==0.2.4
docutils==0.13.1
dynd===c328ab7
entrypoints==0.2.2
et-xmlfile==1.0.1
fastcache==1.0.2
Flask==0.11.1
Flask-Cors==2.1.2
future==0.16.0
gevent==1.1.2
greenlet==0.4.11
h5py==2.5.0
HeapDict==1.0.0
idna==2.1
imagesize==0.7.1
ipykernel==4.5.2
ipython==5.1.0
ipython-genutils==0.1.0
ipywidgets==4.1.1
isort==4.2.5
itsdangerous==0.24
jdcal==1.3
jedi==0.9.0
Jinja2==2.8
jsonschema==2.5.1
jupyter==1.0.0
jupyter-client==4.4.0
jupyter-console==4.1.1
jupyter-core==4.2.1
lazy-object-proxy==1.2.2
llvmlite==0.9.0
locket==0.2.0
lxml==3.6.0
MarkupSafe==0.23
matplotlib==1.5.1
mccabe==0.5.3
menuinst==1.3.2
mistune==0.7.3
mpmath==0.19
multipledispatch==0.4.8
nbconvert==4.3.0
nbformat==4.2.0
ndg-httpsclient==0.4.2
networkx==1.11
nltk==3.2
nose==1.3.7
notebook==4.1.0
ntlm-auth==1.0.2
numba==0.24.0
numexpr==2.6.1
numpy==1.11.2
odo==0.4.2
openpyxl==2.4.1
ordereddict==1.1
pandas==0.18.0
partd==0.3.2
path.py==0.0.0
patsy==0.4.0
pep8==1.7.0
pickleshare==0.7.4
Pillow==3.4.2
ply==3.8
prompt-toolkit==1.0.9
psutil==5.0.0
py==1.4.31
pyasn1==0.1.9
pycosat==0.6.1
pycparser==2.17
pycrypto==2.6.1
pyexcel==0.3.3
pyexcel-io==0.2.4
pyexcel-xls==0.2.0
pyexcel-xlsx==0.2.3
pyflakes==1.3.0
Pygments==2.1.3
pylint==1.6.4
pyparsing==2.0.3
pyreadline==2.1
pytest==2.8.5
python-dateutil==2.5.1
python-ntlm3==1.0.2
pytz==2016.10
pywin32==220
PyYAML==3.11
pyzmq==16.0.2
QtAwesome==0.3.3
qtconsole==4.2.1
QtPy==1.1.2
requests==2.12.0
requests-ntlm==1.0.0
requests-toolbelt==0.7.0
rope-py3k==0.9.4.post1
scikit-image==0.12.3
scikit-learn==0.17.1
scipy==0.17.0
simplegeneric==0.8.1
singledispatch==3.4.0.3
six==1.10.0
snowballstemmer==1.2.1
sockjs-tornado==1.0.1
sphinx-rtd-theme==0.1.9
spyder==2.3.8
SQLAlchemy==1.0.12
statsmodels==0.6.1
sympy==1.0
tables==3.3.0
texttable==0.8.7
toolz==0.7.4
tornado==4.4.2
traitlets==4.3.1
unicodecsv==0.14.1
urllib3==1.19.1
wcwidth==0.1.7
Werkzeug==0.11.11
win-unicode-console==0.5
wrapt==1.10.8
xlrd==0.9.4
XlsxWriter==0.8.4
xlwings==0.7.0
xlwt==1.1.2
xlwt-future==0.8.0

And for reference, here is some output(successful) with curl 7.50.3:
",bigbagboom,Lukasa
3774,2016-12-20 04:27:46,"my error was gone by adding 'DES-CBC3-SHA' in the CIPHERS string. sigh...

Thank you @Lukasa  and @ernestoalejo .

and it's also ok with following code:
s.mount('https://10.192.8.89:8080/yps_report',DESAdapter()) ",bigbagboom,Lukasa
3772,2016-12-16 18:46:56,"@nateprewitt So we have two role models to look at here:

1. Browsers - If you read the logs for our IRC channel, you'll see that Browsers are doing what we do (although probably not as loosely as we do it)

2. Curl/Wget - They disregard the WWW-Authenticate headers on anything that's not a 401. In the case of the user on IRC they were receiving a 403 with WWW-Authenticate headers and we were authenticating while Curl and Wget were not.

Frankly, I'm not sold on restricting this to 401s only, but I do think we should restrict it to 4xx codes. Having these headers sent back on a 30x, for example, could lead to interesting behaviour. On the one hand, we have to answer the challenge, right? On the other we're being redirected, potentially to a totally different domain. I think the only right answer in non 4xx cases is to not authenticate. Unfortunately, we will answer any challenge we receive at the moment and that's problematic.",sigmavirus24,nateprewitt
3772,2016-12-16 18:47:14,"Also @nateprewitt, please let someone else pick this up.",sigmavirus24,nateprewitt
3772,2016-12-16 20:28:51,"Sorry, I should have clarified further on Authorization. I agree we shouldn't be sending Digest Authorization without an appropriate challenge first. What I'm saying is that the section quoted above seems to also support that responding to an appropriate challenge in a non-401 response is permissible which is what we what we currently do.

The scoping here is obviously in the hands of you and @Lukasa,  I just wanted to make sure we didn't completely constrict that to 401 hastily as the original post noted.",nateprewitt,Lukasa
3772,2016-12-16 20:50:23,"> I agree we shouldn't be sending Digest Authorization without an appropriate challenge first. 

We also *literally* can't.

> I just wanted to make sure we didn't completely constrict that to 401 hastily as the original post noted.

I appreciate that :) I think @Lukasa is starting to feel more and more (as I do) that we should be very strict in our interpretation of specifications. That said, (as you point out) the spec is lenient, so my position is that we should be strict in the spirit of the specification. I can't see any reason why someone would challenge with a 200 response, so it should be safe to restrict it to 4xx responses.",sigmavirus24,Lukasa
3772,2017-01-07 06:57:50,Hey @Lukasa will work on this issue,iamrajhans,Lukasa
3770,2016-12-16 00:00:49,"@Lukasa to be clear, @mendaxi is using CLIENT_AUTH with an SSLContext object. It's failing because it's the wrong configuration to validate a certificate from a server. :)",sigmavirus24,Lukasa
3768,2016-12-19 01:39:13,@yoyoprs I believe @Lukasa wanted you two to `import socks` and then check that the version was the same as what `pip` reports.,sigmavirus24,Lukasa
3766,2016-12-13 21:41:16,"@Lukasa I rescinded my last comment on `num_401_calls` because while what I was testing worked, it's not reproducible in the wild. I still think there may be value in testing the hooks individually but I don't think it's a blocker here. These tests cover the use cases we're concerned about in #1979.",nateprewitt,Lukasa
3766,2016-12-14 09:41:08,"Cool, tests are good. Thanks for the work @nateprewitt!",Lukasa,nateprewitt
3758,2016-12-09 08:45:11,LGTM. Thanks for this @nateprewitt! :sparkles:,Lukasa,nateprewitt
3748,2016-12-05 10:32:23,"@Lukasa - Great ! but when I did telnet proxy followed by get urlname, it actually returns me the content.it works that way. But I tried using requests.get(), urllib.urlopen() and httplib, it all fails, sounds a bit interesting. ",arunchandramouli,Lukasa
3748,2016-12-05 11:55:51,"@Lukasa The Proxy / Normal request doesn't request anything at all , it keeps hanging unless I issue a timeout. Where as Telnet GET returns the url content. I couldn't share due to privacy and client protocol issues.",arunchandramouli,Lukasa
3748,2016-12-05 12:24:06,"@Lukasa requests.exceptions.ReadTimeOut:HTTPSConnectionPool(host='***',port=443).Read Timed out.

api.py -> sessions.py -> adapters.py (normal flow for HTTP/HTTPS connection)
I couldn't copy the trace from the server",arunchandramouli,Lukasa
3743,2016-12-02 00:49:55,Nicely caught @nateprewitt ,sigmavirus24,nateprewitt
3743,2016-12-02 02:30:10,"Thanks @nateprewitt 
",timothyjlaurent,nateprewitt
3739,2016-12-01 11:46:33,"Hello @Lukasa and @nateprewitt,

thanks for the explanation. Why is the tox.ini then not part of the project? Is there a problem to get it working for everyone involved? My naive thinking would be that once you have tox properly configured for the project it works for everyone involved. 

So, if there are any concrete problems/bugs that prevent your project from adopting tox properly, just let us know here: https://github.com/tox-dev/tox (yes, we finally moved to Github :)).",obestwalter,Lukasa
3739,2016-12-01 11:46:33,"Hello @Lukasa and @nateprewitt,

thanks for the explanation. Why is the tox.ini then not part of the project? Is there a problem to get it working for everyone involved? My naive thinking would be that once you have tox properly configured for the project it works for everyone involved. 

So, if there are any concrete problems/bugs that prevent your project from adopting tox properly, just let us know here: https://github.com/tox-dev/tox (yes, we finally moved to Github :)).",obestwalter,nateprewitt
3738,2016-11-30 21:24:39,"@Lukasa, I think it would be beneficial readding the test after this reversion. It still provides a useful check going forward and will pass regardless of the tightened scheme check.",nateprewitt,Lukasa
3738,2016-12-01 06:00:01,"@Lukasa sorry for being a hound but do you have an eta on this 2.12.3 release? We just lost a day of dev / test trying to figure out what broke docker-py until we found this. Thx 

- MS",mshahpalerra,Lukasa
3738,2016-12-01 06:04:02,@mshahpalerra just pin your requests dep to an older  2.11.x version and you'll be ok. @Lukasa please release a new version of requests ASAP or this will be another left-pad.,marcosnils,Lukasa
3738,2016-12-01 10:44:47,"ACK

Custodia's test suite for my http+unix adapter is passing with @Lukasa 's branch.",tiran,Lukasa
3738,2016-12-01 10:45:23,"Ok, let's ship a 2.12.3 then. Thanks @tiran!",Lukasa,tiran
3738,2016-12-22 14:03:52,"> Custodia's test suite for my http+unix adapter is passing with @Lukasa 's branch.

I it really amazing that `http+unix` t will work with python-requests-3.0. Unfortunately, python-requests-3.0 has not been released yet. But 2.12.3 is released and already in some distributions: fedora-26 and debian unstable and might get to debian testing in few days.

2.12.0 introduced IDNA-encoding logic which broke `http+unix://` 2.12.3 with this patch reverted the fix due to #3735 but did not provide any alternative solution. And therefore `http+unix://` is broken again.

Is there a reason why cannot be fixed in 2.12.* as well?",lslebodn,Lukasa
3738,2016-12-22 18:34:52,"Hmm, so you're right. This is an issue in that this path will pass through IDNA without throwing an exception, but IDNA is forcing it to lowercase. This is valid behaviour for a hostname which should be case insensitive but doesn't work for paths (which really shouldn't be IDNA encoded in the first place). This is a bug, but the way unix sockets have worked in the past is kind of incorrect too. It supplies a path in place of the hostname we're expecting.

I don't have an immediate solution for this other than suggesting you store the socket file in a lower-cased path. We'll have to wait for @Lukasa's opinion on this and I'll take a deeper dive if it's deemed needed.",nateprewitt,Lukasa
3738,2016-12-22 19:12:43,"So, I did some more investigating and this actually is completely unrelated to IDNA.

shazow/urllib3#911 introduced a forced lowercasing of all host names in urllib3 which was released in urllib3 1.17. The version of urllib3 used in Requests prior to 2.12.0 was 1.16 which is why we hadn't hit this before. urllib3's behaviour here is completely correct because host names should be able to be lowercased without affecting the URIs usability.

As I said above, passing a path has worked in the past but is technically not in line with how we're parsing the URI. Again, I think we'll have to wait for @Lukasa to weigh in here. ",nateprewitt,Lukasa
3738,2016-12-22 19:23:26,"> urllib3's behaviour here is completely correct because host names should be able to be lowercased without affecting the URIs usability.

Right.

> As I said above, passing a path has worked in the past but is technically not in line with how we're parsing the URI. Again, I think we'll have to wait for @Lukasa to weigh in here.

Since there is no specification around UNIX socket URIs, I would think we'd treat that as a path, not a hostname. *shrug*",sigmavirus24,Lukasa
3738,2016-12-23 09:33:24,"@nateprewitt I don't think we do store `/tmp/path/out.socket` as the `host`. I think we only store it if its urlencoded when we receive it, which seems more reasonable to me. Though admittedly I'm not the URI expert (that'd be @sigmavirus24).",Lukasa,sigmavirus24
3738,2016-12-23 09:33:24,"@nateprewitt I don't think we do store `/tmp/path/out.socket` as the `host`. I think we only store it if its urlencoded when we receive it, which seems more reasonable to me. Though admittedly I'm not the URI expert (that'd be @sigmavirus24).",Lukasa,nateprewitt
3738,2016-12-23 16:45:08,"@Lukasa, you're right, I was conflating behaviours in my explanation. %-encoding the path is a hack around `prepare_url` because Requests refuses URLs without a host at the model level, rather than adapter. The host is mandated for http/https schemes but becomes a grey area with the use of ""http+"".

If we want to support ""http+{unix,docker,???}"", which seems poorly defined but in use, we'd probably need to relax the hard stop at no host until we get to the adapter. This would allow someone to pass `/tmp/path/out.socket` and we can treat it correctly as a path, without needing a work around.  Then we can avoid modifying correctly functioning code to accommodate the hack. If we don't intend to support ""http+"" in 3.0.0 then this is probably a moot point.",nateprewitt,Lukasa
3738,2016-12-23 18:22:21,"The % encoding of the path to the Unix socket file or abstract namespace is mandatory. How else would you distinguish between the file path and the HTTP path? Some implementations probe ever path segment until a socket file is found and then magically treat the remaining path segments as HTTP path. That's a dangerous hack. The socket path may include non-ASCII chars or a NULL byte for abstract namespace.

Am 23. Dezember 2016 17:45:25 MEZ, schrieb Nate Prewitt <notifications@github.com>:
>@Lukasa, you're right, I was conflating behaviours in my explanation.
>%-encoding the path is a hack around `prepare_url` because Requests
>refuses URLs without a host at the model level, rather than adapter.
>The host is mandated for http/https schemes but becomes a grey area
>with the use of ""http+"".
>
>If we want to support ""http+{unix,docker,???}"", which seems poorly
>defined but in use, we'd probably need to relax the hard stop at no
>host until we get to the adapter. This would allow someone to pass
>`/tmp/path/out.socket` and we can treat it correctly as a path, without
>needing a work around.  Then we can avoid modifying correctly
>functioning code to accommodate the hack. If we don't intend to support
>""http+"" in 3.0.0 then this is probably a moot point.
>
>-- 
>You are receiving this because you were mentioned.
>Reply to this email directly or view it on GitHub:
>https://github.com/kennethreitz/requests/pull/3738#issuecomment-269016139

Sent from my Android phone with K-9 Mail.",tiran,Lukasa
3738,2016-12-23 18:27:37,"@tiran thanks, for clarifying! I'll defer to those of you that are much more well versed in this space than I am :)

I'll open the issue in urllib3 with Lukasa's originally suggested fix.",nateprewitt,tiran
3735,2016-11-30 23:06:24,"Thanks @Lukasa for chiming in and looking into this!

Being able to rely on `requests` to make docker-py work has been a great boon for us, especially the ability to handle different transport formats (HTTP/s, UNIX sockets, and more recently Windows named pipes) through the same interface is a powerful feature for us.

If you see any way we could keep this flexibility while making things easier on the requests library to support our use-case(s), I'd be happy to hear them and consider any changes.

Thanks again for your help.",shin-,Lukasa
3734,2016-11-30 15:18:22,@Lukasa can do,graingert,Lukasa
3734,2016-11-30 15:21:09,@Lukasa it's 'http+docker://localunixsocket/v1.24/images/create' but that gets handled by the mounted `docker.transport.unixconn.UnixAdapter`,graingert,Lukasa
3734,2016-11-30 15:37:01,@Lukasa someone might come along and register the gtld localunixsocket,graingert,Lukasa
3734,2016-11-30 15:39:57,"@graingert That's fine, it won't matter. The adapter is selected based on a longest-prefix match, so the docker-py adapter will still be selected and knows it doesn't have to do a DNS lookup.",Lukasa,graingert
3734,2016-11-30 15:42:10,@graingert has pointed out that in situations like this we should probably throw an exception if we were supposed to add any query parameters to the request.,Lukasa,graingert
3734,2016-11-30 16:02:18,@Lukasa should this handling be moved to the adaptor?,graingert,Lukasa
3718,2016-11-23 21:38:02,"@Lukasa Do you mean a test that for an empty `requests.Response()` object, the `content` property is None?",nsoranzo,Lukasa
3718,2016-11-23 23:58:51,"@nateprewitt I'd be happy to change the test to:

or anything that makes:

return `None`. I think most programmers would expect this and that the traceback is a regression.",nsoranzo,nateprewitt
3718,2016-11-24 11:22:33,"@Lukasa I'll try to explain why I think that this is the expected behaviour:

- `requests.Response` class is clearly part of the `requests` API and creating such objects is useful when dealing with connection exceptions
- the only `__init__` method of `requests.Response()` has no parameters
- using a property of a legitimately created object shouldn't raise an error, unless there is a very good reason

>  Number one is to change this patch such that we tolerate the actual problem: namely, that response.raw is None.

So, is

a good solution for you?",nsoranzo,Lukasa
3718,2016-11-24 12:36:10,"> I think I'm leaning towards number one, if only because we've handled this kind of problem in the past (admittedly with an overbroad except block).

This makes sense to me. We can add the requirement that raw not be None in 3.0

@nsoranzo adding the explicit check for None works for me.",sigmavirus24,nsoranzo
3718,2016-11-24 12:57:00,@sigmavirus24 I've added the explicit check for None and also a test as requested by @Lukasa.,nsoranzo,Lukasa
3718,2016-11-24 12:57:00,@sigmavirus24 I've added the explicit check for None and also a test as requested by @Lukasa.,nsoranzo,sigmavirus24
3717,2016-11-23 10:17:29,Thanks @afeld and @nateprewitt!,Lukasa,afeld
3717,2016-11-23 10:17:29,Thanks @afeld and @nateprewitt!,Lukasa,nateprewitt
3716,2016-11-22 20:00:40,"@nateprewitt thanks this helped. However, it seems like [this documentation](https://pypi.python.org/pypi/requests/2.2.1) should be updated, since it's just flat out wrong.",PatriotRDX,nateprewitt
3716,2016-11-22 20:04:49,"@nateprewitt the documentation here says to run `pip install requests` in Terminal. However, this only installs 2.12.1. Not to mention the documentation here is exactly the same as the documentation for 2.12.1.",PatriotRDX,nateprewitt
3716,2016-11-22 20:25:24,@nateprewitt there is no way to update it. @PatriotRDX we expect users to have some understanding of how to specify requirements themselves and use pip.,sigmavirus24,nateprewitt
3716,2016-11-22 20:35:43,"> we expect users to have some understanding of how to specify requirements themselves and use pip.

@sigmavirus24 wow that's brutal, only time I've used Python has been today.",PatriotRDX,sigmavirus24
3716,2016-11-22 20:48:53,"@nateprewitt thank you for your help. I still get an error when running 2.12.1, so I'll just stick to 2.2.1 for now. Thank you again.",PatriotRDX,nateprewitt
3716,2016-11-22 20:54:28,@lutzhorn my issue was solved by @nateprewitt at the beginning of this thread. I don't have any other problems that need to be solved.,PatriotRDX,lutzhorn
3716,2016-11-22 20:54:28,@lutzhorn my issue was solved by @nateprewitt at the beginning of this thread. I don't have any other problems that need to be solved.,PatriotRDX,nateprewitt
3716,2016-11-22 20:58:09,"@lutzhorn it doesn't matter, as long as it works, it works. I don't need 2.12.1, this is only one small program I'm testing out.",PatriotRDX,lutzhorn
3716,2016-11-22 21:08:28,"@nateprewitt, @lutzhorn, @sigmavirus24 thank you all for your help. Have a nice day.",PatriotRDX,lutzhorn
3716,2016-11-22 21:08:28,"@nateprewitt, @lutzhorn, @sigmavirus24 thank you all for your help. Have a nice day.",PatriotRDX,sigmavirus24
3716,2016-11-22 21:08:28,"@nateprewitt, @lutzhorn, @sigmavirus24 thank you all for your help. Have a nice day.",PatriotRDX,nateprewitt
3716,2016-11-23 14:04:42,"@drpoggi thank you for the extra information on my other issue. I've got it working with 2.2.1 so it doesn't matter to me, however, someone else may find it useful.",PatriotRDX,drpoggi
3714,2016-11-21 19:48:00,Thanks @Lukasa !  Posted an issue in (what I hope is) the main urllib3 repo.  ,aseering,Lukasa
3713,2016-11-21 17:13:39,"This looks entirely reasonable to me, thanks @tiran!",Lukasa,tiran
3713,2016-11-21 19:13:54,"Cool, thanks @tiran! All tests pass, so it seems like it's good to go. Thanks so much! :sparkles: :cake: :sparkles:",Lukasa,tiran
3709,2016-11-20 03:54:15,"@drpoggi Can you explain how users can create the same output with r.content.  The output from r.content is different from r.json().  Also how do you provide preferred indentation level and separators. As far as I know it doesn't take any arguments. You can provide preferred indentation level and separators to r.sjson through key value arguments. You can look at the documentation for the dumps function in the json module to give custom arguments.  The whole point of creating the function was so that users can get the json object in a human readable form with simple readable code. Also how do you know there is a freeze in requests. Was there a post somewhere, or is it in the documentation somewhere?

EDIT: Also thanks for the feedback.
",rmhasan,drpoggi
3709,2016-11-20 04:05:51,"@rmhasan, thanks for your interest in contributing to Requests. You can find information about the freeze and other contributing topics in Requests [Contributor's Guide](http://docs.python-requests.org/en/master/dev/contributing). The specific information you're looking for is under [feature requests](http://docs.python-requests.org/en/master/dev/contributing/#feature-requests).
",nateprewitt,rmhasan
3709,2016-11-20 04:14:48,"@nateprewitt Thanks. 
",rmhasan,nateprewitt
3706,2016-11-21 17:29:20,Thanks @Lukasa @reaperhulk .Its indeed issue with the certificate. I used a certificate  without AKI and it works fine.,kmala,Lukasa
3705,2016-11-17 19:29:34,"@nateprewitt Thanks!  That was the issue...somewhere in the jython path was a json package by patrickdlogan@[...].com, and requests was finding that one instead.
",j-christ,nateprewitt
3704,2016-11-17 18:19:37,"@nateprewitt Yeah, thought about that...might make sense to give a value of `128` or something reasonable in the example, and just say that it can be changed. I'm no expert on this, so whatever y'all think!
",afeld,nateprewitt
3704,2016-11-18 20:10:14,"@afeld if you think this change is agreeable and have the inclination, feel free to throw the PR together. Otherwise, I'll try to open something next week.
",nateprewitt,afeld
3701,2016-11-20 09:39:20,"@Lukasa I had the same error, read your comments, reinstalled both libraries, pyopenssl and cryptography, and it worked. no more error. Thanks for your help. :) 
",camara-,Lukasa
3701,2016-11-21 11:35:22,I had the same issue on a new MBP with a 'vanilla' macOS Sierra.  @Lukasa I uninstalled and re-installed pyOpenSSL and Cryptography and it all worked fine.  Thanks.,markyj,Lukasa
3700,2016-11-17 18:07:31,"Hi @kennethreitz , I was wondering why my pull request got reverted.
",rmhasan,kennethreitz
3698,2016-11-16 20:59:22,"Yup, I think @nateprewitt has it: if you can't get a repro that doesn't involve requests_mock I think this issue is almost certainly with their code.
",Lukasa,nateprewitt
3698,2016-11-17 00:16:10,"@Makman2 I went **way** down the rabbit hole and believe I've found an answer. This is in fact the product of a combination of changes in 2.12 and the way you're mocking objects in your tests.

You're currently mocking responses with a Response object that only the status_code initialized.
This is a really simple repro of your issue.



`send()` in `requests.Session` always calls `.content` on streams to ensure they're consumed. When this is called on the empty response, it raises an AttributeError previously caught. This exception was removed in 327512f, oddly enough as a fix for requests_mock, because it was deemed too broad. The implication of this is we now require all adapters to support `raw` in their responses which conflicts somewhat with our documentation. I'll leave it to @Lukasa to determine the next steps.

As for getting your tests working with Requests-2.12, I'd suggest simply setting `Response.raw = io.BytesIO()`, or some file-like object, [here](https://github.com/coala/coala-bears/blob/master/tests/general/InvalidLinkBearTest.py#L48).
",nateprewitt,Lukasa
3695,2016-11-19 20:13:09,"I guess we should probably clarify what we're attempting to do here. The above comment was in reference to the the inability to use IPv6 with Requests 2.12+. Personally, I think that definitely needs to be addressed, the other complaints I have no opinion on.

The fallback approach though essentially gives carte blanche to anything that can be ASCII encoded, which seems to defeat some benefits of doing IDNA encoding. They've special cased `.` delimiters in the IDNA library, I'm assuming for things like subdomains, which allows IPv4 addresses to pass through (unintentionally?). However, they don't appear to have taken into account IPv6 style addresses with `:` delimiters or wrapping brackets.

From a cursory reading of [RFC5891](https://tools.ietf.org/html/rfc5891), it seems like IDNA is only intended for domain names. That would suggest to me we shouldn't be passing IP addresses into this function to begin with.

If we're ONLY attempting to handle IPv6, we can check to skip `host` values that are entirely numeric or delimiters. Otherwise, if we're entertaining allowing things like underscores then maybe the passthrough is the only viable option to avoid constantly programming around use cases.

**Edit**: Sorry @Lukasa I just realized I essentially reiterated your entire thought process from the other thread. Vendoring in a dependency is likely not what we want to do. `any([c.isalpha() for c in host])` is probably kludgy enough to keep out of the main path for Requests. So I guess that leaves the bypass.
",nateprewitt,Lukasa
3695,2016-11-25 13:17:57,"Alright, I'm going to merge this now. @sigmavirus24 feel free to give feedback regardless, we can always revisit this. =) Thanks @nateprewitt!",Lukasa,sigmavirus24
3695,2016-11-25 13:17:57,"Alright, I'm going to merge this now. @sigmavirus24 feel free to give feedback regardless, we can always revisit this. =) Thanks @nateprewitt!",Lukasa,nateprewitt
3691,2016-11-16 08:39:04,"Thanks @Cleod9! :sparkles: :cake: :sparkles:
",Lukasa,Cleod9
3690,2016-11-16 08:38:31,"Thanks @galgeek!
",Lukasa,galgeek
3688,2016-11-15 20:30:20,"Thanks @nateprewitt! :sparkles: :cake: :sparkles:
",Lukasa,nateprewitt
3687,2016-11-16 06:28:10,"Go for it @nateprewitt
",nlevitt,nateprewitt
3687,2016-11-23 09:31:18,"I have the same problem, which I reported in kjd/idna#32, but it seems more to be an issue in requests than in idna.

@Lukasa's logic sounds right to me.",quantenschaum,Lukasa
3685,2016-11-15 18:42:52,"@jaraco I believe what you're seeing is shazow/urllib3#1025. It has been patched on master for urllib3 but was caught after 1.19 released. This may be enough of an issue to warrant bumping urllib3 to 1.19.1 and adding it to a 2.12.1 release? Until then, you'll likely need to use 2.11.1 unless @sigmavirus24 has a solution via requests-toolbelt.
",nateprewitt,jaraco
3685,2016-11-15 19:44:29,"> This may be enough of an issue to warrant bumping urllib3 to 1.19.1 and adding it to a 2.12.1 release?

@nateprewitt Requests release procedures are clearly documented. Please don't make up policy that we don't follow for us.
",sigmavirus24,nateprewitt
3685,2016-11-15 19:53:38,"Sorry @sigmavirus24, I wasn't attempting to insinuate anything about policy. I was simply asking a question about severity. Thanks for pointing that documentation out, I had previously missed the sentence on vendoring changes.
",nateprewitt,sigmavirus24
3685,2016-11-15 20:11:39,"> I'm not really feeling like rushing up to a new urllib3 release for this bug. So I'm going to say that downgrading to 2.11.1 is the best course of action for now, and the next major release of Requests will fix this issue. 😊

@Lukasa @sigmavirus24 
The last major release was almost 3 years ago. I'm hoping you meant minor? I see procedure allows vendor updates in minor releases. 

Assuming you did is there anything interested parties can do to speed up the release? Minor releases seem to come ever 3-4 months. Which is a bit long to wait for 1 line of code and miss the improvements in 2.12.0. Its certainly not a pleasant situation, but having just done a release is now not the cheapest time to do another one? Given that there are minimal changes to requests to deal with.

http://docs.python-requests.org/en/master/community/release-process/
",tunezaq,Lukasa
3685,2016-11-15 20:11:39,"> I'm not really feeling like rushing up to a new urllib3 release for this bug. So I'm going to say that downgrading to 2.11.1 is the best course of action for now, and the next major release of Requests will fix this issue. 😊

@Lukasa @sigmavirus24 
The last major release was almost 3 years ago. I'm hoping you meant minor? I see procedure allows vendor updates in minor releases. 

Assuming you did is there anything interested parties can do to speed up the release? Minor releases seem to come ever 3-4 months. Which is a bit long to wait for 1 line of code and miss the improvements in 2.12.0. Its certainly not a pleasant situation, but having just done a release is now not the cheapest time to do another one? Given that there are minimal changes to requests to deal with.

http://docs.python-requests.org/en/master/community/release-process/
",tunezaq,sigmavirus24
3685,2016-11-15 23:31:25,"@Lukasa My bad, I had thought the change was in a released version.

Last attempt :), would you consider a hotfix (cherry-pick) release to get the change into a `1.19.1` urllib3? I wasn't able to find a documented release process for the project, but that would allow this issue to be resolved very timely. I see urllib3 releases a bit more often, I would guess by the time its released again requests may not be in a place for a minor release. Seems like this is a ""critical window"" where the issue can be resolved now or in a couple months.
",tunezaq,Lukasa
3684,2016-11-15 17:51:46,"Thanks @nateprewitt!
",Lukasa,nateprewitt
3683,2016-11-16 00:23:26,"Just a quick question @sigmavirus24 / @Lukasa 

Services like Amazon's SES, and Sengrid, use CNAMES with underscores in them for DKIM authentication. 

http://docs.aws.amazon.com/ses/latest/DeveloperGuide/easy-dkim-dns-records.html

As you've mentioned before, these are fine for DNS, but not for Hostnames.

When you say Hostnames, I'm assuming you're talking about DNS records that are used for browsers, or similar tools, to access data, not for records that are used for stuff like the aforementioned DKIM records?
",TetraEtc,Lukasa
3683,2016-11-16 00:23:26,"Just a quick question @sigmavirus24 / @Lukasa 

Services like Amazon's SES, and Sengrid, use CNAMES with underscores in them for DKIM authentication. 

http://docs.aws.amazon.com/ses/latest/DeveloperGuide/easy-dkim-dns-records.html

As you've mentioned before, these are fine for DNS, but not for Hostnames.

When you say Hostnames, I'm assuming you're talking about DNS records that are used for browsers, or similar tools, to access data, not for records that are used for stuff like the aforementioned DKIM records?
",TetraEtc,sigmavirus24
3683,2016-11-16 08:36:09,"@TetraEtc Exactly so. Underscores in DNS names are reasonably common (e.g. SRV records have mandatory underscores in them), but for _hostnames_ (that is, anything in the host part of a URL) they are not allowed.
",Lukasa,TetraEtc
3683,2016-11-17 01:39:05,"Unfortunately, I don't think #3695 will address this problem. #3695 is solely for bypassing hostnames that were IDNA encoded before the request was made. I haven't seen any comment by @Lukasa about changing how we are now handling underscores in hostnames. @sigmavirus24's advice to raise a bug report with GitLab is the way to go.
",nateprewitt,Lukasa
3683,2016-11-17 01:39:05,"Unfortunately, I don't think #3695 will address this problem. #3695 is solely for bypassing hostnames that were IDNA encoded before the request was made. I haven't seen any comment by @Lukasa about changing how we are now handling underscores in hostnames. @sigmavirus24's advice to raise a bug report with GitLab is the way to go.
",nateprewitt,sigmavirus24
3683,2016-11-17 12:58:21,"@mfriedenhagen None of those tools tolerate IDNs. In this case it's a matter of picking what you want: do users want us to correctly tolerate internationalized domain names, or do they want to do their own IDN handling? I'm prepared to believe that most users do not want to do their own IDN handling.

@rsaikali So that seems to be a bigger issue. And at this point we're in a bit of a bind because by default there aren't good tools to spot this specific problem (namely, we have an IP address here, not a hostname, so skip IDNA), which means we'd have to vendor the ipaddress module to get that to work. Alternatively, we could use some heuristics to detect possible IP addresses in hostnames and skip IDNA encoding for those as well.

I think that's definitely a problem though.

An alternative solution to this is to say that if the `idna` module fails to encode we'll just go ahead and try to encode as ASCII. If that works then we shrug our shoulders and say everything is probably ok, and if it fails we catch that and throw `InvalidURL`. @sigmavirus24, how does that sound?
",Lukasa,sigmavirus24
3683,2016-11-21 14:38:50,"@Lukasa Thank you very much for the rapid reply!!! very much appreciated.  And hopefully now ppl would see your statement in bold (feel free to remove my reply to not shadow it), and not e.g. `Yup, there's no plan to tolerate underscores: they're not valid in hostnames, and people should avoid using them.` just from few days back.  Also if label `Propose Close` was replaced with `Fix pending` or something like that -- could help as well.  another way is to adjust initial bug report description with pointer to a tentative fix.
Cheers!",yarikoptic,Lukasa
3682,2016-11-15 14:38:40,"@Lukasa I installed `ycmd` which includes `requests` in windows 7. I copied it to windows 10.
",moyotar,Lukasa
3682,2016-11-15 15:35:31,"@sigmavirus24 I don't really understand what you mean. It's about my python version? If yes, it's python27.
",moyotar,sigmavirus24
3682,2016-11-16 16:49:46,"@Lukasa @sigmavirus24 Up to now, I don't kown why I got this error and have no ideas to solve it. I'd be really appreciate if you can give me some help.
",moyotar,Lukasa
3682,2016-11-16 16:49:46,"@Lukasa @sigmavirus24 Up to now, I don't kown why I got this error and have no ideas to solve it. I'd be really appreciate if you can give me some help.
",moyotar,sigmavirus24
3682,2016-11-16 18:35:44,"I agree with @Lukasa. You've also provided us minimal information. I couldn't reproduce this if I tried because I don't know what instructions you followed and while we've asked for more detail you haven't provided it. We can't help you if you won't let us.
",sigmavirus24,Lukasa
3682,2016-11-17 02:56:36,"@sigmavirus24 Sorry.I don't understand what info you need. The error output is as the pic. I followed [ycmd's building](https://github.com/Valloric/ycmd#building) to install it.
",moyotar,sigmavirus24
3682,2016-11-17 04:46:52,"@drpoggi Thanks! Now I decide to reinstall `ycmd` to get it work.
",moyotar,drpoggi
3681,2016-11-16 02:06:02,"thanks you @Lukasa ,I use `shadowsocks` and the version of my requests is `2.10.0` while it's used to be `2.9.1` and updated with the install of google map api;
btw, I can get a correct json format result if I hardcode the url `https://maps.googleapis.com/maps/api/geocode/json?address=1600+Amphitheatre+Parkway,+Mountain+View,+CA&key=AIzaSyBYdEWfcUNHUP3zLtc17nX-3aE4uPUkFnY` in my web browser.
",LancelotHolmes,Lukasa
3679,2016-11-14 17:58:18,"Thanks @mlissner! :sparkles:
",Lukasa,mlissner
3679,2016-11-14 18:05:04,"I'm fine with removing the second mention if folks want, but I agree with @Lukasa too. I didn't realize for ages that a request not timing out meant my program would hang until I killed it. It's a pretty critical issue in my opinion, so being super-duper, repetitively explicit doesn't seem like a bad thing to me.
",mlissner,Lukasa
3678,2016-11-14 16:55:31,"LGTM, let's do this! Thanks @nateprewitt! :sparkles: :cake: :sparkles:
",Lukasa,nateprewitt
3672,2016-11-12 04:27:43,"Good spot, @Cleod9! `domain` is definitely the keyword we want here. A PR for this would be very welcome.
",nateprewitt,Cleod9
3670,2016-11-10 19:37:44,"FYI, requestsexceptions looks like it doesn't monkey-patch `requests.exceptions`: https://github.com/openstack-infra/requestsexceptions/blob/master/requestsexceptions/__init__.py.

Either way, force reinstalling indicates that this is emphatically not our problem (as @Lukasa has already pointed out).
",sigmavirus24,Lukasa
3670,2016-11-10 19:42:03,"Also, @ssbarnea, http://stackoverflow.com/questions/32986626/python-requests-importerror-cannot-import-name-headerparsingerror seems to be a different (although similar) issue. From the StackOverflow issue it looks like you have some left over bits of old versions of requests on your disk.
",sigmavirus24,ssbarnea
3666,2016-11-03 10:39:45,"Thanks @hootnot! :sparkles: :cake: :sparkles:
",Lukasa,hootnot
3664,2016-11-03 10:03:59,"@hootnot I think in this case this should be considered a documentation issue, rather than explicitly naming the parameters. Mind opening a docs PR?
",Lukasa,hootnot
3663,2016-11-03 10:06:51,"Thanks @nateprewitt! :sparkles: :cake: :sparkles:
",Lukasa,nateprewitt
3662,2016-11-02 15:54:19,"@Lukasa Gladly, but I'm a bit overburdened at the moment. I'll have spare time in 2-3 weeks, if it's still open, I'll take a peek.
",sztomi,Lukasa
3662,2016-11-12 08:42:57,"@klimenko It does look better that way, but it's unfortunately just moving the problem. Now anyone whose server is expecting a non-UTF-8 encoded username is going to get tripped up, and so we'll have to re-open this issue when someone says ""my server wanted Latin1 and now doesn't get it"".

It's better to use bytestrings because that way we avoid making a guess that is wrong. If the users still want the helpful automatic choice, they can pass a unicode string, but if they want to do something more specific we have an escape hatch for them.
",Lukasa,klimenko
3662,2016-11-16 16:06:58,"@rmhasan thanks for the interest in contributing! It may be important to note that PR #3673 is already open to address this. You may want to keep an eye on the outcome of that before spending time working on a solution.
",nateprewitt,rmhasan
3662,2016-11-17 03:05:09,"@nateprewitt I will keep an eye on it, thanks.
",rmhasan,nateprewitt
3661,2016-10-31 16:32:46,"@Lukasa Thanks for the quick reply. I guess using `max_retries` would make the code a bit more fault-tolerant in generic sense.

However, to fail-fast at least when developing, running tests or continuous integration, I would rather not retry but simply avoid using those connections that are in risk of being closed (I know proxy server's keep-alive timeout so I could just set my value to a second or two lower than that). Do you see this as totally stupid thing to do?
",tuukkamustonen,Lukasa
3660,2016-10-31 13:59:12,"Cool, so this looks really good. You'll note we have some tests with proxies in `test_lowlevel.py`, so if you feel like adding more testing in this area I'll happily merge it. However, for this specific issue, I think this test is sufficient.

Thanks @pawelmhm! :sparkles: :cake: :sparkles:
",Lukasa,pawelmhm
3659,2016-10-31 12:49:50,"This issue looks easily fixed too: the `proxy_headers` method on the HTTPAdapter requires truthy values for both `username` and `password`. I recommend that that be changed to only require a truthy value for `username`, along with a test. Would you like to make that change @pawelmhm?
",Lukasa,pawelmhm
3655,2016-11-02 08:34:01,"Want to do your final tidying up now, @nateprewitt? I don't think I'm going to have any further feedback at this time.
",Lukasa,nateprewitt
3655,2016-11-03 10:25:22,"Thanks for this @nateprewitt!
",Lukasa,nateprewitt
3651,2016-10-27 03:04:24,"@drpoggi is likely correct. Further, without extra information, it's hard for any of the people you tagged in this bug to help you (let alone the maintainers of this project).

If you have a _question_ please ask it on [StackOverflow](https://stackoverflow.com).
",sigmavirus24,drpoggi
3651,2016-10-29 12:24:06,"here is a example,`import requests
from requests.auth import *
req = requests.get('http://61.91.211.158:81/onvif/snapshot/101',timeout=3)
print req.headers`,but it raise timeout exception,I have used wireshark to capture the packets,It response quikly.`HTTP/1.1 401 Unauthorized
Date: Sat, 29 Oct 2016 19:06:59 GMT
Server: DNVRS-Webs
Cache-Control: no-cache
Content-Type: text/html
Connection: keep-alive
Keep-Alive: timeout=60, max=99
WWW-Authenticate: Basic realm=""DVRNVRDVS""
WWW-Authenticate: Digest realm=""DVRNVRDVS"", domain=""::"", qop=""auth"", nonce=""71225169d5e6c3e80672d195911b6d23"", opaque="""", algorithm=""MD5"", stale=""FALSE""`,why requests will raise timeout exception? ,and when I remove timeout param It will get 401 status code,but will wait a long long time.@Lukasa @sigmavirus24 @drpoggi 
",beruhan,Lukasa
3651,2016-10-29 12:24:06,"here is a example,`import requests
from requests.auth import *
req = requests.get('http://61.91.211.158:81/onvif/snapshot/101',timeout=3)
print req.headers`,but it raise timeout exception,I have used wireshark to capture the packets,It response quikly.`HTTP/1.1 401 Unauthorized
Date: Sat, 29 Oct 2016 19:06:59 GMT
Server: DNVRS-Webs
Cache-Control: no-cache
Content-Type: text/html
Connection: keep-alive
Keep-Alive: timeout=60, max=99
WWW-Authenticate: Basic realm=""DVRNVRDVS""
WWW-Authenticate: Digest realm=""DVRNVRDVS"", domain=""::"", qop=""auth"", nonce=""71225169d5e6c3e80672d195911b6d23"", opaque="""", algorithm=""MD5"", stale=""FALSE""`,why requests will raise timeout exception? ,and when I remove timeout param It will get 401 status code,but will wait a long long time.@Lukasa @sigmavirus24 @drpoggi 
",beruhan,sigmavirus24
3651,2016-10-29 12:24:06,"here is a example,`import requests
from requests.auth import *
req = requests.get('http://61.91.211.158:81/onvif/snapshot/101',timeout=3)
print req.headers`,but it raise timeout exception,I have used wireshark to capture the packets,It response quikly.`HTTP/1.1 401 Unauthorized
Date: Sat, 29 Oct 2016 19:06:59 GMT
Server: DNVRS-Webs
Cache-Control: no-cache
Content-Type: text/html
Connection: keep-alive
Keep-Alive: timeout=60, max=99
WWW-Authenticate: Basic realm=""DVRNVRDVS""
WWW-Authenticate: Digest realm=""DVRNVRDVS"", domain=""::"", qop=""auth"", nonce=""71225169d5e6c3e80672d195911b6d23"", opaque="""", algorithm=""MD5"", stale=""FALSE""`,why requests will raise timeout exception? ,and when I remove timeout param It will get 401 status code,but will wait a long long time.@Lukasa @sigmavirus24 @drpoggi 
",beruhan,drpoggi
3649,2016-10-27 13:06:36,"@gardiac2002 Unfortunately, GitHub doesn't allow assigning issues to anyone other than contributors or the person who opened the issue. However, no-one on the team is currently working on this, so you are welcome to work on it and open a pull request.
",Lukasa,gardiac2002
3649,2016-10-27 13:49:31,"@Lukasa thank you, I am going to take a look at the issue :)
",gardiac2002,Lukasa
3649,2016-11-03 15:57:50,"It looks like this is now resolved thanks to @gardiac2002's work in #3652.
",nateprewitt,gardiac2002
3635,2016-10-26 06:49:53,"@Lukasa  Actually , I can't change the proxy authorization, It's not my proxy server. For now, I use the urllib.request  instead. Is it possible this authorization work with requests? 
",Arion-Dsh,Lukasa
3627,2016-10-27 19:53:16,"Thanks @nateprewitt! :sparkles:
",Lukasa,nateprewitt
3625,2016-10-17 16:13:30,"@mie00 The question isn't ""can `seek()` throw an `IOError`"", it's ""can `seek()` throw an `IOError` in a situation where `tell()` does not"". I feel like the odds of that are pretty low. I'm inclined to want to worry about that when we know it's an issue.
",Lukasa,mie00
3625,2016-10-21 07:20:34,"Ok, rather than block on @sigmavirus24 having time to swing back to this, I'll merge this now. @sigmavirus24 can provide feedback whenever he gets time back. Thanks for the work @mie00!
",Lukasa,mie00
3616,2016-10-12 20:19:01,"+1 for me. I agree with @sigmavirus24 on making it non-optional.
",eriol,sigmavirus24
3615,2016-10-12 10:11:41,"Thanks @TetraEtc! :sparkles: :cake: :sparkles:
",Lukasa,TetraEtc
3612,2016-10-07 20:15:09,"@drpoggi is correct. There's not much to be done here within requests (or six for that matter). It's usually best to not to shadow the names of standard library modules.
",sigmavirus24,drpoggi
3611,2016-10-06 13:16:52,"@TetraEtc is correct. Cheers!
",sigmavirus24,TetraEtc
3606,2016-09-29 07:09:18,"Looks good to me @bbamsch, thanks for fixing this up!
",Lukasa,bbamsch
3605,2016-10-26 19:47:57,"@Lukasa I am getting intermittent `EOF occurred in violation of protocol (_ssl.c:590)` error on Ubuntu, should I just upgrade Ubuntu openssl package? Looks like the latest openssl version on Ubuntu is 1.0.1g, would installing that version help with my issue? I am not explicitly using ssl in my code, just using requests to access https URLs. 
",IntelBob,Lukasa
3605,2017-01-12 07:16:10,"Hi @Lukasa 

I the same error , and it stuck me for couple of days :+1: 
`urllib2.URLError: <urlopen error EOF occurred in violation of protocol (_ssl.c:590)>`
my python version is 2.7.10,  and I use urllib2 
below is my related code:
`cookieprocessor = urllib2.HTTPCookieProcessor()
            opener = urllib2.build_opener(cookieprocessor)
            urllib2.install_opener(opener)
            request = urllib2.Request(url,postParams)
            if sys.version_info < (2, 7, 9):
                file = urllib2.urlopen(request)
            else:
                ctx = ssl.create_default_context()
                ctx.check_hostname = False
                ctx.verify_mode = ssl.CERT_NONE
                print (""---"")
                file = urllib2.urlopen(request, context=ctx, timeout=30)
            fileInfo = file.read()`

Please help me...",szyl111,Lukasa
3604,2016-09-28 11:32:51,"Thanks @iamprakashom! :sparkles: :cake: :sparkles:
",Lukasa,iamprakashom
3604,2016-09-28 11:38:27,"Thank you  very much @Lukasa  for merging my PR. :smile: 
",iamprakashom,Lukasa
3603,2016-09-28 07:16:48,"Yup, that looks wrong. I recommend the first link you posted @bbamsch. Would either of you be interested in submitting a pull request to fix that?
",Lukasa,bbamsch
3603,2016-09-28 07:41:05,"@Lukasa, If I understand right, first link you are referring to is http://urllib3.readthedocs.io/en/1.5/pools.html ?
",iamprakashom,Lukasa
3602,2016-09-27 14:00:07,"@Lukasa Thanks for your responses so far. Curl did not use my macos keychain, when I didn't pass the certificate as an argument, curl would not download anything either.
I followed the crt link in the output above, downloaded that certificate, checked the output too and so on to get something that might just be the root certificate. It still does not work but next I first try to find a human who might have the proper root certificate.
",do3cc,Lukasa
3602,2016-10-05 09:02:17,"Hi, I got an answer and the certificate files. 
It was still failing while adding all certs to my browser made my browser accept the website. Googling for how to validate the cert via ssl gave me this page http://stackoverflow.com/questions/25482199/verify-a-certificate-chain-using-openssl-verify and after that I realized that it is not enough to pass the root certificate to `verify` but also the intermediate certificates. 

Our org uses its own root certificate and intermediate certificates. To verify the https connection, requests needs the complete certificate chain. 
Creating a cert file where I pasted the root certificate and intermediate certificates did just that.

@Lukasa Thank you so much for your quick help!
",do3cc,Lukasa
3600,2016-09-27 07:06:25,"Thanks @frewsxcv!
",Lukasa,frewsxcv
3597,2016-09-26 05:32:35,"@Lukasa 
I've attempted a patch of this issue in PR #3598 
Would appreciate your input on the patch.

:smile:
",bbamsch,Lukasa
3595,2016-10-17 07:22:11,"@nateprewitt I think @sigmavirus24 was tentatively in favour of this change, at least as a temporary stopping-off point before something better. He was just asking for a change in the descriptor protocol.
",Lukasa,nateprewitt
3595,2016-10-18 16:31:02,"@sigmavirus24 I agree with you about there being a risk of unintentionally sending cookies over the wire with session cookies, but I think that isn't entirely coupled to this issue. Instead of allowing people to pass a dictionary, we currently have them dump a dictionary into a RequestsCookieJar which has no domain info either. So this is an extra step for the sake of purity, that currently provides the user no extra safety or benefit.

I can see how this modification may reenforce bad behaviour that should probably be removed in 3.0.0, so in the meantime, I'll propose this. I think adding a section in the documentation detailing _how_ to do this correctly would go a long way. We should also at the very least add a warning when setting Session's cookies attribute with a dict, directing them to the documentation.

Something like this could go in the docs with an explanation of why using the domain property is important. An iterable-based version of `set` or `set_cookie` would probably be a useful helper for 3.0.0 too.


",nateprewitt,sigmavirus24
3595,2016-10-24 08:15:36,"Like @nateprewitt said, there is benefits here we can not ignore.
When the user want to use `add_dict_to_cookiejar` this will add the dict to `''`domain. This is most likely not the desired scenario. 

For example:


",OrDuan,nateprewitt
3595,2016-11-09 21:39:13,"Alright @Lukasa, @sigmavirus24,

I took a swing at my [last comment](https://github.com/kennethreitz/requests/pull/3595#issuecomment-254563756). It comes in two pieces. 

3e4e5b7: This adds documentation for adding dictionaries with domains. It's currently based off of the solution below, but if it's decided that isn't a good fit, we can change the code to my comment above. This at least provides direction for the user to do the right thing, rather than identical behaviour to a dictionary, with more code.

e2a4f9f & 4454849: I added in a `**kwargs` option to the `add_dict_to_cookiejar` and `cookiejar_from_dict` to allow cookie parameters. This will allow you to pass dictionary-wide cookie params to use the current proposed solution for session cookies ""safely"". This behaviour resembles what the browser does with set-cookie on a per request basis, allows for _fairly_ easy extensibility and it's a minimal code change. It definitely needs some updated doc strings but I wanted to get feed back before proceeding.

Lastly, I think that a warning, pointing users to the documentation, when setting the session cookie to a dict is low overhead and will prevent issues like #3624.

This is becoming a much longer running discussion than I anticipated after discussing opening this patch with Lukasa. It may be worth closing this PR and moving it into an issue, but I'll defer to your judgements on how you want this cataloged.
",nateprewitt,Lukasa
3595,2016-11-09 21:39:13,"Alright @Lukasa, @sigmavirus24,

I took a swing at my [last comment](https://github.com/kennethreitz/requests/pull/3595#issuecomment-254563756). It comes in two pieces. 

3e4e5b7: This adds documentation for adding dictionaries with domains. It's currently based off of the solution below, but if it's decided that isn't a good fit, we can change the code to my comment above. This at least provides direction for the user to do the right thing, rather than identical behaviour to a dictionary, with more code.

e2a4f9f & 4454849: I added in a `**kwargs` option to the `add_dict_to_cookiejar` and `cookiejar_from_dict` to allow cookie parameters. This will allow you to pass dictionary-wide cookie params to use the current proposed solution for session cookies ""safely"". This behaviour resembles what the browser does with set-cookie on a per request basis, allows for _fairly_ easy extensibility and it's a minimal code change. It definitely needs some updated doc strings but I wanted to get feed back before proceeding.

Lastly, I think that a warning, pointing users to the documentation, when setting the session cookie to a dict is low overhead and will prevent issues like #3624.

This is becoming a much longer running discussion than I anticipated after discussing opening this patch with Lukasa. It may be worth closing this PR and moving it into an issue, but I'll defer to your judgements on how you want this cataloged.
",nateprewitt,sigmavirus24
3595,2016-11-10 10:20:53,"I can get behind moving this to an issue for discussion. It's clear @sigmavirus24 has a bigger goal for this API than I originally did, and so I'd like us to move to a forum where he can participate effectively despite his current high workload.
",Lukasa,sigmavirus24
3592,2016-09-20 14:57:34,"@sigmavirus24 

I am not sure why this was **closed**.

I cannot provide the code because it contains proprietary credentials.

So, what would cause requests would make repeated ""Starting new HTTPS connection"" followed by repeated method calls?
",jefftune,sigmavirus24
3592,2016-09-20 15:01:47,"@Lukasa 

Thanks for the feedback, I will look into **Multithreaded use of Requests**
",jefftune,Lukasa
3592,2016-09-20 15:34:53,"@sigmavirus24 

Understood, thank you
",jefftune,sigmavirus24
3591,2016-09-23 09:32:49,"Ok, per my note above I'm going ahead and merging this. Thanks @nateprewitt!
",Lukasa,nateprewitt
3578,2016-09-17 05:48:55,"@rbcarson Can you confirm for me: does adding `import encodings.idna` to your `task` function in your example also fix the problem?
",Lukasa,rbcarson
3578,2016-09-17 06:51:09,"@Lukasa Yes, it does.
",rbcarson,Lukasa
3577,2016-09-20 10:39:48,"Thanks @Lukasa 
However, setting `chunk_size` to `1` or `None` did not change the results in my case. It seems that my issue is related to https://github.com/kennethreitz/requests/issues/2020 . Requests somehow handles chucked-encoding differently as curl does.
The following example shows different results GET from my log-server using curl and requests.





We can see that `iter_content` get the correct data as well as CRLF but chunks them in a different way. Since `iter_lines` internally called `iter_content`, the line split differently accordingly. 
",haocs,Lukasa
3576,2016-09-19 18:18:51,"@Lukasa I would like to pull it and run the tests locally. I suspect we might have a locale mismatch which is why this _appears_ to work for @nateprewitt and not for me. This is why I was using bytes explicitly previously.
",sigmavirus24,Lukasa
3576,2016-09-19 18:18:51,"@Lukasa I would like to pull it and run the tests locally. I suspect we might have a locale mismatch which is why this _appears_ to work for @nateprewitt and not for me. This is why I was using bytes explicitly previously.
",sigmavirus24,nateprewitt
3576,2016-09-19 18:27:11,"Hey @sigmavirus24, while this could be a locale mismatch, there are 5 other tests in the same file using unicode characters in unicode strings like this (including \xf6). I _think_ I figured out why we were getting different results, and [detailed it](https://github.com/kennethreitz/requests/pull/3557#issuecomment-246908451), along with most of my other findings in #3557. I had @Lukasa confirm he could reproduce this issue before I opened the PR. Is this patch not working on your system?
",nateprewitt,Lukasa
3576,2016-09-19 18:27:11,"Hey @sigmavirus24, while this could be a locale mismatch, there are 5 other tests in the same file using unicode characters in unicode strings like this (including \xf6). I _think_ I figured out why we were getting different results, and [detailed it](https://github.com/kennethreitz/requests/pull/3557#issuecomment-246908451), along with most of my other findings in #3557. I had @Lukasa confirm he could reproduce this issue before I opened the PR. Is this patch not working on your system?
",nateprewitt,sigmavirus24
3576,2016-09-19 19:00:29,"> ...which is why **this** _appears_ to work for @nateprewitt and not for me.

Sorry @sigmavirus24, I interpreted ""this"" as meaning ""this patch"". I was just trying to clarify.
",nateprewitt,sigmavirus24
3576,2016-09-19 19:00:29,"> ...which is why **this** _appears_ to work for @nateprewitt and not for me.

Sorry @sigmavirus24, I interpreted ""this"" as meaning ""this patch"". I was just trying to clarify.
",nateprewitt,nateprewitt
3576,2016-09-20 14:48:49,"Thanks @nateprewitt. Care to update proposed/3.0.0 too then?
",sigmavirus24,nateprewitt
3573,2016-09-15 07:39:49,"@Lukasa what is the change in urllib3 that will solve this issue?
",stefanfoulis,Lukasa
3572,2016-09-15 06:34:09,"Yup, as @drpoggi said, Requests doesn't carry any patches to urllib3, we just use it as is. Please make this change to urllib3 and open a PR there. =)
",Lukasa,drpoggi
3565,2016-09-09 09:47:56,"@Lukasa - it also works if library authors _don't opt-out_ by using the Session API. Ie. if they just write `request.get` (which I suspect most of them do) instead of setting up a session and calling methods on it, this ""default session factory"" would be used.

Also, perhaps some guidance in form of documentation - ie. ""How to use requests if you're a library author"" - could be useful. It could cover things like: allow the users of your library to customize how requests are made, to set up proxies, etc.
",cdman,Lukasa
3565,2016-09-09 12:38:14,"I have to agree with @Lukasa here.

> it also works if library authors don't opt-out by using the Session API. Ie. if they just write request.get (which I suspect most of them do) instead of setting up a session and calling methods on it, this ""default session factory"" would be used.

If someone is building a library that is interacting with a service using requests and not using a Session they're performing a **severe** disservice to their users. Their users are paying the performance price for the library author not using a Session. Further, if the library is not providing a way for their users to customize the session (or provide their own ready-made one) then they're also hurting their users. Poor usage of requests is not requests responsibility to fix.

> Also, perhaps some guidance in form of documentation - ie. ""How to use requests if you're a library author"" - could be useful. It could cover things like: allow the users of your library to customize how requests are made, to set up proxies, etc.

People don't often like to be told how to do things, and certain authors will violently disagree with what I've said above. We already have people who send pull requests to change the code-style of requests from Kenneth's style to their own personal style. The last thing we need is another avenue for people to send pull requests that are based on their own opinions of how to do a thing.

Blog posts are the better place for this kind of discussion, not Requests' documentation.
",sigmavirus24,Lukasa
3563,2016-11-15 16:47:47,"@Lukasa we'll need to merge master into Proposed/3.0.0 to run the tests normally, but with 1.19 in Requests 2.12 this should be ready for a look.
",nateprewitt,Lukasa
3563,2016-11-15 17:28:16,"Thanks @Lukasa! I'll merge it in now.
",nateprewitt,Lukasa
3563,2016-11-16 13:29:47,"Ok, I think this looks good! Thanks so much @nateprewitt! :sparkles:
",Lukasa,nateprewitt
3560,2016-12-26 21:54:07,"@Lukasa could you give any pointers to debug this? I'm getting the same error when I try to use pip. I'm trying to understand what's going on, but I don't know where to start",elopio,Lukasa
3558,2016-09-07 13:59:07,"@Lukasa   But I close it in IE settings. 
sometimes I need VPN to climbe the Great Fire Wall    : ) you know, a lot of  websites I can't visit. Sometimes visit github I also need use VPN.
",coderfreedom,Lukasa
3558,2016-09-07 14:09:07,"@Lukasa 



It was my vps's IP, why????
",coderfreedom,Lukasa
3558,2016-09-07 14:19:15,"@Lukasa I use this way before. But sometimes  I need use othre lib which can't set proxies, such as `builtwith.parse('http://example.webscraping.com')`  it also through `Error: <urlopen error [Errno 10061] >`
",coderfreedom,Lukasa
3558,2016-09-07 14:31:30,"@Lukasa  Anyway, thank you for your help  : )
This problem has troubled me for months, It drives me crazy ""_""
I think it's time to use Mac OS.  : )
",coderfreedom,Lukasa
3558,2016-09-07 14:38:47,"@sigmavirus24  But another lib will still not work. 
",coderfreedom,sigmavirus24
3558,2016-09-07 14:40:12,"@Lukasa  @sigmavirus24  Does python has a global variable to carvery it???
",coderfreedom,Lukasa
3558,2016-09-07 14:40:12,"@Lukasa  @sigmavirus24  Does python has a global variable to carvery it???
",coderfreedom,sigmavirus24
3557,2016-09-06 16:07:47,"Sorry I let Python 2 slip through the cracks. I am, however, still seeing a failure in python 2.7.10 on multiple systems with this merge. The issue appears to be that we're handing a unicode string in Python 2 to the HTTPError which is producing an ""unprintable HTTPError"" message when it can't be decoded to ascii. This was also present in the original test in PR #3385, we just weren't testing the string. Encoding the actual [error message](https://github.com/kennethreitz/requests/blob/master/requests/models.py#L862-L869) string before passing it to the exception fixes this problem in Python 2.

e.g.



**Edit:** @sigmavirus24 can you confirm this is failing for you in Python 2 still and that my tests aren't borked?

**Second Edit:** I can't find anything functionally different in this PR from pre-#3557 except that we're now comparing a string object with a unicode object in the final assert statement. I'm not particularly tied to the encoding ordering, but I do think that this test should match the other unicode tests above it. Is there a reason that the encoding needs to be done during the assertion (bytes->str) instead of at the beginning (unicode->bytes)? These will always be unicode strings, so I'm not sure if we gain anything from dropping the reason phrase down to latin-1 during the final assertion (other than it needs to not be bytes). Apologies if I'm missing something obvious.

This would be [my proposed solution](https://github.com/nateprewitt/requests/commit/0cf4e700d2ce30c9422d788d705c5496d5401d3d) for fixing the failing tests. Like I said, I'm not tied to reverting but I think related tests are easier to understand if they share common pieces.
",nateprewitt,sigmavirus24
3557,2016-09-06 20:26:51,"@nateprewitt this wasn't working on Python 2 _or_ Python 3 before but this passes for me on Linux and OSX. What are you testing on?
",sigmavirus24,nateprewitt
3557,2016-09-07 15:43:30,"@sigmavirus24, here's some tox runs on blank travis instances displaying what I'm seeing locally.

Pre-#3557: https://travis-ci.org/nateprewitt/requests/builds/158050706 (87f9693)
Post-#3557: https://travis-ci.org/nateprewitt/requests/builds/158050156 (2041adb)
My Proposed Patch: https://travis-ci.org/nateprewitt/requests/builds/158051134 (0cf4e70)
",nateprewitt,sigmavirus24
3557,2016-09-14 05:13:31,"Alright @sigmavirus24, I think I've finally gotten to the bottom of the confusion here (or most of it). It looks like you wrote and commited [a separate test](https://github.com/kennethreitz/requests/blob/proposed/3.0.0/tests/test_requests.py#L1054-L1069) for proposed/3.0.0 than you submitted here, which does in fact pass on all versions. This patch as it stands on master though is functionally identical to the test beforehand and still fails for Python 2. 

We could move the test from proposed/3.0.0 over to master to fix the problem, but I feel like it may be unnecessarily verbose. You perform an identical check by simply switching `str(e)` to `e.value.args[0]`. I've updated my commit (39e8c0d) to use this instead, still with the reversion of the encodings for the reasons listed in the comments above. As I said before, not tied to that, but I'd rather keep the tests similar if possible.

I'm fairly confident this is the correct approach at this point, but please let me know if you see something I'm not. I'll open this as a PR tomorrow afternoon unless you'd like to make the changes yourself.

Here's the [Travis build](https://travis-ci.org/nateprewitt/requests/builds/159784486), if you wanna take a look.
",nateprewitt,sigmavirus24
3556,2016-09-06 14:03:49,"@Lukasa too serious
",kennethreitz,Lukasa
3556,2017-03-07 06:21:05,"@kennethreitz  Oh, I see, :) :) :). Thank you.",Celthi,kennethreitz
3555,2016-09-07 01:45:28,"@Lukasa Thank you for response.

The REST API server needs a header leading whitespace. In this system, we must use the header field `""Authorization"" : "" Basic (SECURITY-TOKEN)""` for the authorization to the system. Spaces also must be needed! :sob: 

I think this system's REST API design is so bad :sob: 
",shouh,Lukasa
3552,2016-09-05 16:47:10,"Thanks @Trodis!
",sigmavirus24,Trodis
3551,2016-09-05 16:22:13,"@Trodis Have you seen the fix in the linked PR?
",Lukasa,Trodis
3550,2016-09-06 00:20:31,"@Lukasa thks for reply,i confirm the domain name is valid,because i can get some datas.but sometimes i get the error ""ConnectionError"".
**some error info:**
`[zjhxmjl@localhost phoneregion]$ python main.py --scrapy > log.txt
Traceback (most recent call last):
  File ""/usr/lib64/python2.7/site-packages/gevent/greenlet.py"", line 534, in run
    result = self._run(_self.args, *_self.kwargs)
  File ""/home/zjhxmjl/Documents/phoneregion/scrapy.py"", line 55, in worker
    info = self.validate(phone)
  File ""/home/zjhxmjl/Documents/phoneregion/scrapy.py"", line 62, in validate
    r = requests.get(url)
  File ""/usr/lib/python2.7/site-packages/requests/api.py"", line 70, in get
    return request('get', url, params=params, *_kwargs)
  File ""/usr/lib/python2.7/site-packages/requests/api.py"", line 56, in request
    return session.request(method=method, url=url, *_kwargs)
  File ""/usr/lib/python2.7/site-packages/requests/sessions.py"", line 475, in request
    resp = self.send(prep, *_send_kwargs)
  File ""/usr/lib/python2.7/site-packages/requests/sessions.py"", line 596, in send
    r = adapter.send(request, *_kwargs)
  File ""/usr/lib/python2.7/site-packages/requests/adapters.py"", line 487, in send
    raise ConnectionError(e, request=request)
ConnectionError: HTTPConnectionPool(host='www.ip138.com', port=8080): Max retries exceeded with url: /search.asp?mobile=1459772&action=mobile (Caused by NewConnectionError('<requests.packages.urllib3.connection.HTTPConnection object at 0x3970d90>: Failed to establish a new connection: [Errno -2] Name or service not known',))
<Greenlet at 0x31ae0f0: <bound method Scrapy.worker of <scrapy.Scrapy instance at 0x307ef38>>(['1450000', '1450001', '1450002', '1450003', '1450)> failed with ConnectionError

Traceback (most recent call last):
  File ""/usr/lib64/python2.7/site-packages/gevent/greenlet.py"", line 534, in run
    result = self._run(_self.args, *_self.kwargs)
  File ""/home/zjhxmjl/Documents/phoneregion/scrapy.py"", line 55, in worker
    info = self.validate(phone)
  File ""/home/zjhxmjl/Documents/phoneregion/scrapy.py"", line 62, in validate
    r = requests.get(url)
  File ""/usr/lib/python2.7/site-packages/requests/api.py"", line 70, in get
    return request('get', url, params=params, *_kwargs)
  File ""/usr/lib/python2.7/site-packages/requests/api.py"", line 56, in request
    return session.request(method=method, url=url, *_kwargs)
  File ""/usr/lib/python2.7/site-packages/requests/sessions.py"", line 475, in request
    resp = self.send(prep, *_send_kwargs)
  File ""/usr/lib/python2.7/site-packages/requests/sessions.py"", line 596, in send
    r = adapter.send(request, *_kwargs)
  File ""/usr/lib/python2.7/site-packages/requests/adapters.py"", line 487, in send
    raise ConnectionError(e, request=request)
ConnectionError: HTTPConnectionPool(host='www.ip138.com', port=8080): Max retries exceeded with url: /search.asp?mobile=1459728&action=mobile (Caused by NewConnectionError('<requests.packages.urllib3.connection.HTTPConnection object at 0x34df390>: Failed to establish a new connection: [Errno -2] Name or service not known',))
<Greenlet at 0x31aef50: <bound method Scrapy.worker of <scrapy.Scrapy instance at 0x307ef38>>(['1450000', '1450001', '1450002', '1450003', '1450)> failed with ConnectionError

Traceback (most recent call last):
  File ""/usr/lib64/python2.7/site-packages/gevent/greenlet.py"", line 534, in run
    result = self._run(_self.args, *_self.kwargs)
  File ""/home/zjhxmjl/Documents/phoneregion/scrapy.py"", line 55, in worker
    info = self.validate(phone)
  File ""/home/zjhxmjl/Documents/phoneregion/scrapy.py"", line 62, in validate
    r = requests.get(url)
  File ""/usr/lib/python2.7/site-packages/requests/api.py"", line 70, in get
    return request('get', url, params=params, *_kwargs)
  File ""/usr/lib/python2.7/site-packages/requests/api.py"", line 56, in request
    return session.request(method=method, url=url, *_kwargs)
  File ""/usr/lib/python2.7/site-packages/requests/sessions.py"", line 475, in request
    resp = self.send(prep, *_send_kwargs)
  File ""/usr/lib/python2.7/site-packages/requests/sessions.py"", line 596, in send
    r = adapter.send(request, *_kwargs)
  File ""/usr/lib/python2.7/site-packages/requests/adapters.py"", line 487, in send
    raise ConnectionError(e, request=request)
ConnectionError: HTTPConnectionPool(host='www.ip138.com', port=8080): Max retries exceeded with url: /search.asp?mobile=1459727&action=mobile (Caused by NewConnectionError('<requests.packages.urllib3.connection.HTTPConnection object at 0x361c210>: Failed to establish a new connection: [Errno -2] Name or service not known',))
<Greenlet at 0x306b4b0: <bound method Scrapy.worker of <scrapy.Scrapy instance at 0x307ef38>>(['1450000', '1450001', '1450002', '1450003', '1450)> failed with ConnectionError`

**this is log.txt**
[log.txt](https://github.com/kennethreitz/requests/files/455727/log.txt)
",zhumingyu,Lukasa
3545,2016-09-21 16:31:14,"Ok, things are pared down. The patch is now pretty simple.

The logic for file-like objects (with seek/tell) is now in [`super_len`](https://github.com/kennethreitz/requests/blob/d7227fbb7e07af35f23a0d370ab3b01661af9e40/requests/utils.py#L78-L103). I'm removing [this chunk](https://github.com/kennethreitz/requests/blob/d7227fbb7e07af35f23a0d370ab3b01661af9e40/requests/models.py#L478-L483) since it is now redundant. I'm also adding two tests to verify that Content-Length is set to 0 appropriately.

@Lukasa, @sigmavirus24 feel free to take a peak when you have a moment.
",nateprewitt,Lukasa
3545,2016-09-21 16:31:14,"Ok, things are pared down. The patch is now pretty simple.

The logic for file-like objects (with seek/tell) is now in [`super_len`](https://github.com/kennethreitz/requests/blob/d7227fbb7e07af35f23a0d370ab3b01661af9e40/requests/utils.py#L78-L103). I'm removing [this chunk](https://github.com/kennethreitz/requests/blob/d7227fbb7e07af35f23a0d370ab3b01661af9e40/requests/models.py#L478-L483) since it is now redundant. I'm also adding two tests to verify that Content-Length is set to 0 appropriately.

@Lukasa, @sigmavirus24 feel free to take a peak when you have a moment.
",nateprewitt,sigmavirus24
3545,2016-09-22 12:04:49,"@Lukasa it looks as though @nateprewitt pushed new changes after your approval. Care to re-review?
",sigmavirus24,Lukasa
3545,2016-09-22 12:04:49,"@Lukasa it looks as though @nateprewitt pushed new changes after your approval. Care to re-review?
",sigmavirus24,nateprewitt
3540,2016-08-26 12:23:48,"Yup, that's correct. =) Thanks @TetraEtc!
",Lukasa,TetraEtc
3538,2016-08-26 07:34:26,"@Lukasa it came up because Jira when served up with french locale sends french status reasons. Originally it was an exception and when we saw this was fixed I noticed that now the reason is garbled. I don't care _that_ much about it but the current code is definitely incorrect.
",mitsuhiko,Lukasa
3538,2016-08-26 16:18:19,"@mitsuhiko we changed it because there are servers sending back utf-8. I can't search for it at the moment, but you should have little problems finding that pull request and issue.
",sigmavirus24,mitsuhiko
3538,2016-09-06 08:33:02,"Resolved by #3554 instead. Thanks @mitsuhiko!
",Lukasa,mitsuhiko
3536,2016-11-03 15:36:23,"@Lukasa tzickel's commit was merged into master with #3655, so this can probably be closed.
",nateprewitt,Lukasa
3535,2016-08-24 19:08:00,"It'd be really nice to have some more tests around this. Ideally tests that go into each of the branches. @nateprewitt, are you open to adding those?
",Lukasa,nateprewitt
3535,2016-08-25 22:24:07,"@Lukasa, tests are updated with custom classes. I think things should be good to go with tests.
",nateprewitt,Lukasa
3534,2016-09-06 08:30:11,"Yup, this looks like an error in parsing URLs. @sigmavirus24, you're the resident expert here: any thoughts?
",Lukasa,sigmavirus24
3534,2016-09-06 11:30:39,"@Lukasa I don't see the problem parsing the URL. 



Seems like that _might_ be the problem based on that message but I'm having trouble figuring out why. We [create a ProxyManager](https://github.com/kennethreitz/requests/blob/5259b374512ec4e47785f5004ec6ad30dafe906f/requests/adapters.py#L188) from urllib3 which parses the URL using `parse_url` from it's url utility module. This parses the example without issue and has always handled IPv6 addresses correctly.

If we look at the ProxyManager it has different logic for requests to an [https url versus an http url](https://github.com/kennethreitz/requests/blob/05d90b9379f57ee5f5d0beb268c495793fb5b2d7/requests/packages/urllib3/poolmanager.py#L331). 

I'm failing to see where the URL parsing would fail and why an IPv6 literal address would cause a name resolution failure.
",sigmavirus24,Lukasa
3534,2016-09-06 12:37:45,"@Lukasa it should be 



As that's the port specified in the proxy url that @lvg01 is showing us
",sigmavirus24,Lukasa
3534,2016-09-07 05:04:56,"as for port 3128, the same:

> > > socket.getaddrinfo('[abcd:0123:dcba:3210::1]',3128)
> > > Traceback (most recent call last):
> > >    File ""<stdin>"", line 1, in <module>
> > > socket.gaierror: [Errno -3] Temporary failure in name resolution

On 06-09-16 14:38, Ian Cordasco wrote:

> @Lukasa https://github.com/Lukasa it should be
> 
> import  socket
> socket.getaddrinfo('[abcd:0123:dcba:3210::1]',3128)
> 
> As that's the port specified in the proxy url that @lvg01 
> https://github.com/lvg01 is showing us
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub 
> https://github.com/kennethreitz/requests/issues/3534#issuecomment-244936907, 
> or mute the thread 
> https://github.com/notifications/unsubscribe-auth/APqn3sQEPsDAyL0KKslJrvy1Vc1lJvUwks5qnV66gaJpZM4Jr16P.
",lvg01,Lukasa
3530,2016-08-23 11:54:32,"Thanks @shellhead! :sparkles:
",Lukasa,shellhead
3529,2016-08-22 22:17:30,"OK, Thanks @Lukasa - in my case, I ended up using eventlet module to wrap the entire thing in a more ""global"" timeout.
",haydenth,Lukasa
3526,2016-08-21 22:17:28,"This looks really good to me, I'm totally happy with it! Thanks so much @shellhead! :sparkles: :cake: :sparkles:
",Lukasa,shellhead
3523,2016-08-19 13:33:35,"@Lukasa ~~No i dont, i will try and let you know~~
looks like its been auto set to this:


",ryanym,Lukasa
3523,2016-08-23 20:27:20,"@ryanym, does it work if you set the `all_proxy` scheme to `socks5` or `socks4`? Looking [here](https://github.com/kennethreitz/requests/blob/a2e41ba1f1ae81b2ed224030df908a1432ad435e/requests/packages/urllib3/contrib/socks.py#L153), seems like it's looking for the socks version. Unsure why you're getting different behavior between the two versions, though.

Edit: Did some more digging.

FWIW, it looks like part of the 2.11.0 release was to support all_proxy. So in 2.10.0, it didn't pick up the environment variable, all_proxy, and didn't bother using a proxy. In 2.11.0, it does, which causes the SOCKS version check, and the resulting error. 

I was able to get something to work by either setting all_proxy to `socks5://someproxy.com:someport` or sending the `proxies` dictionary (with the `all` key set to the same value as all_proxy environment variable) on the request.

Disclaimer: I'm a total n00b with the code base, just trying to help out. I would defer to @Lukasa for any real, competent help with this issue though :)
",shellhead,Lukasa
3523,2016-08-24 15:41:30,"@shellhead, thanks for the reply, setting `all_proxy` sheme to `socks5` or `socks4` seems  gives me a timeout


",ryanym,shellhead
3523,2016-08-26 13:28:10,"@Lukasa yes, i have `HTTP_PROXY, HTTPS_PROXY, http_proxy, https_proxy` set to `http://user:pass@someproxy.com:8080`
",ryanym,Lukasa
3523,2016-08-26 15:09:42,"@Lukasa I don't want to use any proxy settings.
r = requests.post(**\* , proxies=None) does not work.

It used to work few days ago. But problem started last night, which is failing my application.

What if I don't want to use any of the proxy configurations?

I tried os.environ.pop('http_proxy') and so on for each protocol. It didn't worked.
However, os.envron.pop('all_proxy') worked as it had the url : socks://***.

Any workaround?
",himanshub16,Lukasa
3523,2016-10-25 04:21:06,"@Lukasa , i have provided the error while the request is sent from python script .
Value Error : unable to determine the version SOCKS from socks://<ip>
",Rahul529,Lukasa
3521,2016-08-29 16:08:44,"@Lukasa I think they wanted to ask whether this issue should be closed or not.
",untitaker,Lukasa
3516,2016-08-17 09:03:24,"@Lukasa @kennethreitz can i have option to disable the SOCKS proxy???
",alochym01,kennethreitz
3516,2016-08-17 09:03:24,"@Lukasa @kennethreitz can i have option to disable the SOCKS proxy???
",alochym01,Lukasa
3516,2016-08-17 11:52:28,"@Lukasa So as I understand it, `all_proxy` is what should be used if `http(s)_proxy` is unset. And in a more general sense it should be used if `<protocol>_proxy` is unset. So based on that understanding (and assuming I'm correct) yes we definitely should re-order that. Let's file a new bug for that.
",sigmavirus24,Lukasa
3516,2016-08-18 00:57:47,"@Lukasa proxies={'all': None} not work because the network should go through proxy, so i see you did open the new issue :)
let follow the new issue :+1: 
",alochym01,Lukasa
3514,2016-08-15 18:03:32,"@kennethreitz I think @Lukasa and I can still do it for you if you want us to. Just let us know. We'd both be happy to help out.
",sigmavirus24,kennethreitz
3514,2016-08-15 18:03:32,"@kennethreitz I think @Lukasa and I can still do it for you if you want us to. Just let us know. We'd both be happy to help out.
",sigmavirus24,Lukasa
3513,2016-08-15 10:21:33,"@nateprewitt is this related to #2228?
",sigmavirus24,nateprewitt
3513,2016-08-15 13:33:03,"@sigmavirus24 It wasn't originally, I found a few minor hiccups making the `.json` change and then did a pass through the docs from there. I realize this is kind of a mess of unrelated doc changes so I can break it up into different commits or PRs if needed.

It looks like most of the discussion in #2228 is about timeouts documentation. Was there something low hanging you wanted to see integrated?
",nateprewitt,sigmavirus24
3512,2016-08-14 02:06:32,"@nateprewitt  Perfect I will chase it down there ;)
",listingmirror,nateprewitt
3511,2016-08-13 08:42:21,"Thanks @forrestchang! :sparkles: :cake: :sparkles:
",Lukasa,forrestchang
3510,2016-08-18 08:28:16,"Thanks @nateprewitt!
",Lukasa,nateprewitt
3508,2016-08-12 07:41:56,"@forrestchang Thanks, would you like to open a PR to change it?
",Lukasa,forrestchang
3508,2016-08-13 02:25:26,"@Lukasa Yes, I would like to. I have modified the mistake in `.rst` file. Should I generate it to the HTML file? 
",forrestchang,Lukasa
3508,2016-08-13 02:39:00,"Hey @forrestchang, there's no need to run the docs build. If you open a PR with the rst changes, read the docs will generate changes on merge.
",nateprewitt,forrestchang
3508,2016-08-13 02:44:43,"@nateprewitt Thanks.
",forrestchang,nateprewitt
3507,2016-08-12 11:23:07,"> I understand that 3.2 is no longer supported by requests, but pip being broken as it is with the version mapping causes trouble for some people.

I'm sorry, I don't quite understand what you are referring to @ppolewicz. 

> I think a minor change of 2 LOC to prevent this would be acceptable?

@ppolewicz except that minor change implies Python 3.2 support. Python 3.2 as a product itself is unsupported. The number of users it has according to PyPI is incredibly small. I don't see a whole lot of value in changing those two lines, but @Lukasa and @kennethreitz may disagree. I'm not against it, I just don't agree that we should be implicitly supporting an unsupported version that is mostly not used (regardless of how easy it is).
",sigmavirus24,Lukasa
3493,2016-08-10 16:10:59,"I was on it too. Did not find a way to properly add tests though since httpbin does not seem to be able to post to an endpoint and respond with a 301/302. Am I wrong on this one?

Anyway since @nateprewitt is on it I will not open my PR ;)
",saveman71,nateprewitt
3493,2016-08-10 17:04:49,"@saveman71, sorry, I didn't mean to jump on your PR. I wasn't sure if anyone had started working on it.

@Lukasa I added a test to check the header removal and empty body.
",nateprewitt,Lukasa
3493,2016-08-10 17:10:28,"@Lukasa re: [this line](https://github.com/kennethreitz/requests/pull/3493/files#diff-28e67177469c0d36b068d68d9f6043bfR151)

Do you have an opinion on moving the declaration farther up to avoid the multiple calls to the headers attribute on the prepare_request? Also as for the conversion from try/except, if that was done would you want it in a separate PR to avoid semi-unrelated changes?
",nateprewitt,Lukasa
3493,2016-08-10 17:16:37,"@nateprewitt no worries at all, I should have stated I started working on it.
As for the line thing, I do not think it is a performance issue (I believe, but I might be mistaken) because well requests is a network library and ""premature optimization is the root of all evil"". (https://www.xkcd.com/1691/)

I was suggesting that merely because it's already been like that [here](https://github.com/nateprewitt/requests/blob/20a2b5ac71f1d6ce674ca2633172ef5964ec9c38/requests/sessions.py#L192) and [here](https://github.com/nateprewitt/requests/blob/20a2b5ac71f1d6ce674ca2633172ef5964ec9c38/requests/sessions.py#L223) for example.

On a side note, :+1: for a separate PR.
",saveman71,nateprewitt
3493,2016-08-10 19:33:06,"Quick semi-unrelated question, what would be the timeline of a release on PyPI after this gets merged?

cc @Lukasa 
",saveman71,Lukasa
3491,2016-08-09 21:05:57,"@Lukasa @sigmavirus24 It looks like the [HISTORY.rst change](https://github.com/kennethreitz/requests/blame/master/HISTORY.rst#L33) specifies only integers. Perhaps that should be changed to 'non-string'?
",nateprewitt,Lukasa
3491,2016-08-09 21:05:57,"@Lukasa @sigmavirus24 It looks like the [HISTORY.rst change](https://github.com/kennethreitz/requests/blame/master/HISTORY.rst#L33) specifies only integers. Perhaps that should be changed to 'non-string'?
",nateprewitt,sigmavirus24
3491,2016-08-09 21:10:05,"@nateprewitt Yes, it should.

@jidar How do we represent True/False as strings? ""True""/""False""? Why that over ""true""/""false"", ""1""/""0"", or ""Yes""/""No""? Requests is simply refusing to guess in this case, and asking that you unambiguously decide what you want to send.
",Lukasa,nateprewitt
3491,2016-08-09 21:22:10,"@Lukasa str(value) seems acceptable.  No need for special casing anything really.  It's what the user is going to be forced to do on their end now anyway.
",jidar,Lukasa
3490,2016-08-09 19:44:51,"@Lukasa That was it! I was using the data field preformatted with `urlib.parse.urlencode`, my bad.
Felt bad when I read 2mn later in the official documentation:

> Requests allows you to send organic, grass-fed HTTP/1.1 requests, without the need for manual labor. 
> 
> There's no need to manually add query strings to your URLs, or to form-encode your POST data.

Thank you for your quick and effective answer :+1:
",saveman71,Lukasa
3490,2016-08-10 14:53:58,"Hey @Lukasa,

Yesterday I tested it rapidly and thought it worked. Today I'm not able to reproduce what worked...



The output with the patch commented is:



You can see that the header `Content-Type` is repeated in the redirect although it was not specified explicitly in the request headers as before.

The log if I activate the workaround suppressing the header for the specific request:



Sorry for the french in error messages/URLs. I, unfortunately, cannot reveal what is the website responsible for the misbehaviour (NDA).

Am I doing something wrong here?
",saveman71,Lukasa
3487,2016-08-09 15:40:18,"Thanks @drpoggi!
",Lukasa,drpoggi
3486,2016-08-09 13:53:03,"@sigmavirus24 sorry, I tried the revert button and got an error so did it by hand.
",nateprewitt,sigmavirus24
3482,2016-08-09 09:12:14,"@Lukasa You can check this out here http://52.40.67.18/testing
",rohitkhatri,Lukasa
3481,2016-08-09 08:46:07,"@Lukasa thanks for looking into it that fast!
Yes, I'm going to decode manually, which is quite an easy change for me. 😉 
",jone,Lukasa
3481,2016-08-10 03:29:09,"@maxpowa #3486 reverts this change and should be included in the upcoming 2.11.1 release. As @Lukasa suggested [above](https://github.com/kennethreitz/requests/issues/3481#issuecomment-238489871), it's recommended not to use `decode_unicode=True` for the time being. You can either manually decode the strings as a temporary fix, or continue using 2.10 if your workflow allows it.
",nateprewitt,Lukasa
3480,2016-08-09 03:03:21,"@kennethreitz thanks, tried copy pasting your en dash from above. Apparently GitHub's UI wasn't a fan of my methods.
",drpoggi,kennethreitz
3479,2016-08-08 22:47:24,"Ah, I see @nateprewitt already chimed in over here.
",sigmavirus24,nateprewitt
3479,2016-08-08 22:51:05,"I see. So, python3.2 is not supported by requests library. Than I will just close the issue and pr.

Thanks for the link @nateprewitt .
",huseyinyilmaz,nateprewitt
3478,2016-08-08 22:52:49,"@sigmavirus24 . I did not realize that pyhon3.2 is not supported. Thanks for the explanation. I will close the pr.
",huseyinyilmaz,sigmavirus24
3478,2016-08-09 02:39:45,"Requests no longer supports Python 3.2. As referenced by @sigmavirus24 this has been the case for ever a year.

If you view the setup.py for requests it specifies 3.3 and higher, or 2.6 and higher.
",TetraEtc,sigmavirus24
3477,2016-08-08 21:36:28,"Thanks @kennethreitz.
",Lukasa,kennethreitz
3477,2016-08-08 21:40:22,"Ok, thanks for the clarification. I will update my code accordingly.
@kennethreitz if you could add your note in PyPI too, would be amazing.
",lmazuel,kennethreitz
3471,2016-08-04 10:34:16,"> Are you saying requests<=0.7.4 handles multipart file uploads well by default?

No. That is not what @Lukasa is saying.

>  But, I dont think this below code uses multipart format

You're using the files parameter, so yes, it is sending a multipart message.

If you describe what you want the body to look like, we _might_ be able to help you, but you should be using [StackOverflow](stackoverflow.com) for help with this module, not the defect tracker.
",sigmavirus24,Lukasa
3470,2016-08-04 07:13:27,"@Lukasa  are you saying if urllib3 is updated, then requests could be in the position of having this option?
",yeukhon,Lukasa
3469,2016-08-03 20:04:55,"Woop, woop, thanks @roselma! :sparkles: :cake: :sparkles:
",Lukasa,roselma
3464,2016-08-01 04:35:36,"@kennethreitz Oh - in my defense I did try searching closed issues as well :).
",glyph,kennethreitz
3463,2017-02-10 21:51:23,thanks @nateprewitt ,kennethreitz,nateprewitt
3460,2016-08-02 16:26:29,"It looks like the [`RequestsCookieJar` class](http://docs.python-requests.org/en/master/api/#requests.cookies.RequestsCookieJar) is available on the main page of links in the documentation, under the [Developer Interface docs](http://docs.python-requests.org/en/master/#the-api-documentation-guide).

Were you looking to have a code example added in the Quickstart section @roselma, or would a link to the existing documentation suffice? 
",nateprewitt,roselma
3447,2016-07-28 08:18:50,"@Lukasa Thank you very much, it's working!
",gsw945,Lukasa
3444,2016-07-27 13:47:42,"@Lukasa further, in v1.0 didn't @kennethreitz completely tear out what logging requests did provide because people were constantly trying to add tons more logging on top of what we provided? I feel like were were to accept this we'd be in the same situation.

Further, what's being logged here is a very clear overload of information for any logging that I would view as reasonable for requests. This seems to be working very well in Café as it is and I see little reason to include this in Requests.

I appreciate your offer of the code @seemethere, but I'm very strongly against this. If you want review on the code anyway, I'm happy to provide that in a separate forum.
",sigmavirus24,Lukasa
3441,2016-07-26 12:26:25,"Thanks @scop!
",sigmavirus24,scop
3440,2016-07-26 04:50:04,"@nateprewitt is correct. Thanks for reaching out, though @walkerlala!
",kennethreitz,nateprewitt
3430,2016-07-22 22:20:24,"I completely agree that the up-stack application should be doing a better job of handling exceptions.  But, like you, @sigmavirus24, I'm not about to expect that from OpenStack, and given how prevalent the problem is, it seems like some relief could be found in the common code, even if it's not an actual fix to the real bug(s) here.

There are a couple of exceptions raised that IMO should include the URL but don't, and don't provide a request object (`InvalidURL` instances seem to be where that would be the most useful). They should probably just have their message strings rewritten, but my initial attempt won't try to change that.
",dhduvall,sigmavirus24
3427,2016-07-20 16:40:58,"@kennethreitz, I kinda want you to take the lead on code review for this: just flag changes you don't like and I'm sure @nateprewitt will undo it.
",Lukasa,kennethreitz
3427,2016-07-20 16:40:58,"@kennethreitz, I kinda want you to take the lead on code review for this: just flag changes you don't like and I'm sure @nateprewitt will undo it.
",Lukasa,nateprewitt
3426,2016-07-20 17:00:51,"@kennethreitz Same reason: https://hynek.me/articles/hasattr/
",Lukasa,kennethreitz
3423,2016-07-18 19:00:37,"@Lukasa Not sure to understand but if you say so... :)
That means that the traceback is wrong and the following should work (it quite hard to test here) ?


",dvasseur,Lukasa
3423,2016-07-18 19:05:44,"@Lukasa ok, thanks!
",dvasseur,Lukasa
3421,2016-07-17 16:52:14,"That's about where I am at in debugging too @Lukasa. It looks like `isclosed` is being set and will evaluate to `True` but the `closed` attribute is permanently `False`. I'm trying to figure out if this is a bug in `httplib` or `urllib3`'s use of it. Our current check in `is_fp_closed` is checking the `closed` attribute.
",nateprewitt,Lukasa
3421,2016-07-17 16:53:52,"The answer is that it doesn't really matter. =)

This behaviour is present in basically all the shipped Python 3 solutions, which means even if it's a bug we need to be bug-compatible with it. I think the best solution is simply to rely on `isclosed` if it's present, in preference to `closed`, which is then in preference to reaching in to the `fp` thing.

Are you interested in writing the PR that fixes this, @nateprewitt?
",Lukasa,nateprewitt
3421,2016-07-17 17:04:18,"Ok, for now let's close this. @nateprewitt, can you open an equivalent issue on the urllib3 repository? You can use this test code as the example (it should live in `test_socketlevel.py` in the urllib3 repo):


",Lukasa,nateprewitt
3417,2016-07-15 07:12:36,"@nateprewitt Thanks for starting this!

Unfortunately, quite a lot of people use non-urllib3 file-like objects to back requests, and I'd like to avoid breaking that if we can. Can you rearrange this to support urllib3 if possible but otherwise fallback to some other behaviour, [per my comment](https://github.com/kennethreitz/requests/issues/2939#issuecomment-166003526).
",Lukasa,nateprewitt
3417,2016-07-16 15:08:45,"Sorry, @sigmavirus24, I just pushed a new copy with partially finished changes. I have a couple of questions on how we're expecting the code to function that I was going to annotate for discussion.
",nateprewitt,sigmavirus24
3417,2016-07-16 20:25:04,"Ok, I'm happy with this. @sigmavirus24?
",Lukasa,sigmavirus24
3417,2016-07-19 14:56:03,"@nateprewitt Yeah, this is a breaking change. =)
",Lukasa,nateprewitt
3417,2016-08-22 15:33:55,"Hey @Lukasa @sigmavirus24, just wanted to check in on this. Let me know if you're waiting on anything from me.
",nateprewitt,Lukasa
3417,2016-08-22 15:33:55,"Hey @Lukasa @sigmavirus24, just wanted to check in on this. Let me know if you're waiting on anything from me.
",nateprewitt,sigmavirus24
3417,2016-08-22 16:30:29,"No problem from me. @sigmavirus24?
",Lukasa,sigmavirus24
3417,2016-08-22 16:41:45,"Looks fine as a start. Thanks @nateprewitt 

@Lukasa since we're sans CI, have you pulled this and run the tests?
",sigmavirus24,Lukasa
3417,2016-08-22 16:41:45,"Looks fine as a start. Thanks @nateprewitt 

@Lukasa since we're sans CI, have you pulled this and run the tests?
",sigmavirus24,nateprewitt
3417,2016-08-22 16:42:24,"@sigmavirus24 I haven't, would you like me to?
",Lukasa,sigmavirus24
3417,2016-08-22 16:48:05,"@nateprewitt I merged the original branch into the head of proposed/3.0.0 and it passed just fine. No need to rebase
",sigmavirus24,nateprewitt
3415,2016-07-15 07:07:43,"Thanks @nateprewitt! :sparkles: :cake: :sparkles:
",Lukasa,nateprewitt
3401,2016-07-14 08:34:57,"@kennethreitz Python 2.7 also has the `io` module: this change keeps the code working as expected on Python 2.
",Lukasa,kennethreitz
3401,2016-07-14 16:20:01,"> Either way, my above comment is generally relevant. We may want to start making our examples 3.x soon, if we want to ""move the needle"". I don't think there's any rush to do so, though.

@kennethreitz have you looked at the downloads for requests recently? I'm not sure which version is ""winning"" but regardless, I think we can write the examples to work across all supported versions without having to have an opinion on whether they're all Py2 or Py3 or moving any needle.
",sigmavirus24,kennethreitz
3391,2016-07-12 09:44:11,"For anyone with this problem:

I've fixed it by following @Lukasa 's suggestions and added this just after importing requests:



Then, where I was using `requests.get()` before, I used `sess.get()`.

Hopefully this helps, and thanks for your help @Lukasa !
",vandernath,Lukasa
3389,2016-07-07 19:38:33,"> LGPL allows re-distribution, which is what we are doing here. 

Right. It's important to understand that things that requests vendors are not modified when vendored. I work on chardet upstream to ensure it's suitable for our vendoring. Likewise @Lukasa and I work on urllib3.

We don't vendor & modify, we simply vendor to redistribute.
",sigmavirus24,Lukasa
3388,2016-07-07 13:27:44,"While you're here @sigmavirus24 and @Lukasa, I threw together [another commit](https://github.com/nateprewitt/requests/commit/be31a90906deb5553c2e703fb05cf6964ee23ed5) related to this to encapsulate the type error thrown by `re` when non-strings are passed to `check_header_validity`. This is to make it clear that the behaviour is to be expected, but may be overkill with this documentation now. Any thoughts on if this would be worth opening? 
",nateprewitt,Lukasa
3388,2016-07-07 14:56:21,"Thanks @nateprewitt 🍮 🍰 ☕ 
",sigmavirus24,nateprewitt
3387,2016-07-05 16:13:13,"@nateprewitt that pull request was _rejected_ precisely because integers are not defined behaviour within requests and haven't been for _years_.
",sigmavirus24,nateprewitt
3387,2016-07-05 16:29:13,"Sorry, @sigmavirus24, I was suggesting something along this line for the check since it wouldn't be transforming the value in this case. I was trying to use this as a template example. I realize the documentation says that headers must be strings, and that was the assumption I was operating off of when I wrote #3366. I submitted the passthrough as the initial pull request here because it avoids dealing with any defining behaviour by ignoring non-strings.

I just wanted to make sure both options were at least briefly discussed.
",nateprewitt,sigmavirus24
3387,2016-07-05 16:41:43,"Yeah, so the question boils down to whether we handle non-string headers. Clearly we've oscillated around for a while: they didn't work, and then they did, and now they don't again.

However, what I failed to note is Kenneth's original response in #865. With that in mind, I'm inclined to want to continue to respect his wishes and say that non-string headers are not acceptable.

Sorry for having you do this work @nateprewitt!
",Lukasa,nateprewitt
3386,2016-07-05 15:13:49,"Yeah, I'll get right on this @Lukasa.
",nateprewitt,Lukasa
3386,2016-07-05 16:42:22,"@sigmavirus24 has linked to the relevant earlier opinions, which suggest that actually we don't allow non-string header values. So that means this is not a bug: it was us making a revision that is within the scope of the API definition.
",Lukasa,sigmavirus24
3386,2016-07-05 16:43:14,"@Lukasa right, I'm frankly surprised this didn't break earlier. The meaning of non-bytes/str as a header value is undefined as far as I'm concerned.
",sigmavirus24,Lukasa
3385,2016-07-05 09:01:30,"@Lukasa is `reason.decode('utf-8', 'ignore')` ok?
",gugu,Lukasa
3385,2016-07-05 09:04:42,"@Lukasa done, updated PR
",gugu,Lukasa
3385,2016-07-05 09:33:51,"@Lukasa done
",gugu,Lukasa
3385,2016-07-05 13:35:22,"@gugu just use `u''`. Further, stop importing six. It's not a dependency of requests.
",sigmavirus24,gugu
3385,2016-07-05 13:45:08,"@sigmavirus24 @lucasa done, updated PR
",gugu,sigmavirus24
3385,2016-07-05 14:01:17,"Looks good to me. @Lukasa, the changes are minimal from when you gave your LGTM, so I'm merging. :)
",sigmavirus24,Lukasa
3385,2016-07-05 14:02:00,"Thanks @gugu! :sparkles: :cake: :sparkles: :fireworks: 
",sigmavirus24,gugu
3382,2016-07-01 22:01:01,"Looks like I failed to set stream=True in my test case. I was misinterpreting the call to `r.content` as the stream read. Thanks @Lukasa, string updated.
",nateprewitt,Lukasa
3382,2016-07-02 18:10:18,"Cool, I'm happy with this! Thanks @nateprewitt! :sparkles: :cake: :sparkles:
",Lukasa,nateprewitt
3370,2016-07-02 20:56:18,":+1: Looks fine to me. Thanks @nateprewitt!
",sigmavirus24,nateprewitt
3368,2016-07-01 00:10:57,"Thanks @joyzheng! :cake: 
",sigmavirus24,joyzheng
3367,2016-06-30 09:01:45,"@Lukasa 

#tcpdump -i eth0 -vnn 164.132.52.143
16:54:31.440888 IP (tos 0x0, ttl 64, id 52147, offset 0, flags [DF], proto TCP (6), length 44)
    x.x.x.x.62004 > 164.132.52.143.3128: Flags [S], cksum 0x4aa1 (correct), seq 1499401970, win 5840, options [mss 1460], length 0
16:54:31.749509 IP (tos 0x0, ttl 52, id 0, offset 0, flags [DF], proto TCP (6), length 44)
    164.132.52.143.3128 > x.x.x.x.62004: Flags [S.], cksum 0x203b (correct), seq 3958367012, ack 1499401971, win 29200, options [mss 1460], length 0
16:54:31.749549 IP (tos 0x0, ttl 64, id 52148, offset 0, flags [DF], proto TCP (6), length 40)
    x.x.x.x.62004 > 164.132.52.143.3128: Flags [.], cksum 0x9338 (correct), ack 1, win 5840, length 0
16:54:31.749663 IP (tos 0x0, ttl 64, id 52149, offset 0, flags [DF], proto TCP (6), length 80)
    x.x.x.x.62004 > 164.132.52.143.3128: Flags [P.], cksum 0xd039 (incorrect -> 0x5679), seq 1:41, ack 1, win 5840, length 40
16:54:32.057972 IP (tos 0x0, ttl 52, id 42614, offset 0, flags [DF], proto TCP (6), length 40)
    164.132.52.143.3128 > x.x.x.x.62004: Flags [.], cksum 0x37d0 (correct), ack 41, win 29200, length 0
16:54:32.058005 IP (tos 0x0, ttl 64, id 52150, offset 0, flags [DF], proto TCP (6), length 42)
    x.x.x.x.62004 > 164.132.52.143.3128: Flags [P.], cksum 0xd013 (incorrect -> 0x85fc), seq 41:43, ack 1, win 5840, length 2
16:54:32.366473 IP (tos 0x0, ttl 52, id 42615, offset 0, flags [DF], proto TCP (6), length 40)
    164.132.52.143.3128 > x.x.x.x.62004: Flags [.], cksum 0x37ce (correct), ack 43, win 29200, length 0
16:54:32.488958 IP (tos 0x0, ttl 52, id 42616, offset 0, flags [DF], proto TCP (6), length 79)
    164.132.52.143.3128 > x.x.x.x.62004: Flags [P.], cksum 0x2de6 (correct), seq 1:40, ack 43, win 29200, length 39
16:54:32.488986 IP (tos 0x0, ttl 64, id 52151, offset 0, flags [DF], proto TCP (6), length 40)
    x.x.x.x.62004 > 164.132.52.143.3128: Flags [.], cksum 0x92e7 (correct), ack 40, win 5840, length 0
16:54:32.490157 IP (tos 0x0, ttl 64, id 52152, offset 0, flags [DF], proto TCP (6), length 337)
    x.x.x.x.62004 > 164.132.52.143.3128: Flags [P.], cksum 0xd13a (incorrect -> 0xf979), seq 43:340, ack 40, win 5840, length 297
16:54:32.798673 IP (tos 0x0, ttl 52, id 42617, offset 0, flags [DF], proto TCP (6), length 40)
    164.132.52.143.3128 > x.x.x.x.62004: Flags [.], cksum 0x334e (correct), ack 340, win 30016, length 0
16:54:35.254479 IP (tos 0x0, ttl 52, id 42618, offset 0, flags [DF], proto TCP (6), length 41)
    164.132.52.143.3128 > x.x.x.x.62004: Flags [P.], cksum 0x1d45 (correct), seq 40:41, ack 340, win 30016, length 1
16:54:35.295080 IP (tos 0x0, ttl 64, id 52153, offset 0, flags [DF], proto TCP (6), length 40)
    x.x.x.x.62004 > 164.132.52.143.3128: Flags [.], cksum 0x91bd (correct), ack 41, win 5840, length 0
16:54:38.045490 IP (tos 0x0, ttl 52, id 42619, offset 0, flags [DF], proto TCP (6), length 41)
    164.132.52.143.3128 > x.x.x.x.62004: Flags [P.], cksum 0x3044 (correct), seq 41:42, ack 340, win 30016, length 1
16:54:38.045518 IP (tos 0x0, ttl 64, id 52154, offset 0, flags [DF], proto TCP (6), length 40)
    x.x.x.x.62004 > 164.132.52.143.3128: Flags [.], cksum 0x91bc (correct), ack 42, win 5840, length 0
16:54:40.817095 IP (tos 0x0, ttl 52, id 42620, offset 0, flags [DF], proto TCP (6), length 41)
    164.132.52.143.3128 > x.x.x.x.62004: Flags [P.], cksum 0x3043 (correct), seq 42:43, ack 340, win 30016, length 1
16:54:40.817125 IP (tos 0x0, ttl 64, id 52155, offset 0, flags [DF], proto TCP (6), length 40)
    x.x.x.x.62004 > 164.132.52.143.3128: Flags [.], cksum 0x91bb (correct), ack 43, win 5840, length 0
16:54:43.574452 IP (tos 0x0, ttl 52, id 42621, offset 0, flags [DF], proto TCP (6), length 41)
    164.132.52.143.3128 > x.x.x.x.62004: Flags [P.], cksum 0x2242 (correct), seq 43:44, ack 340, win 30016, length 1
16:54:43.574483 IP (tos 0x0, ttl 64, id 52156, offset 0, flags [DF], proto TCP (6), length 40)
    x.x.x.x.62004 > 164.132.52.143.3128: Flags [.], cksum 0x91ba (correct), ack 44, win 5840, length 0
16:54:46.349464 IP (tos 0x0, ttl 52, id 42622, offset 0, flags [DF], proto TCP (6), length 41)
    164.132.52.143.3128 > x.x.x.x.62004: Flags [P.], cksum 0xc140 (correct), seq 44:45, ack 340, win 30016, length 1
16:54:46.349495 IP (tos 0x0, ttl 64, id 52157, offset 0, flags [DF], proto TCP (6), length 40)
    x.x.x.x.62004 > 164.132.52.143.3128: Flags [.], cksum 0x91b9 (correct), ack 45, win 5840, length 0
16:54:48.247303 IP (tos 0x0, ttl 64, id 52158, offset 0, flags [DF], proto TCP (6), length 40)
    x.x.x.x.62004 > 164.132.52.143.3128: Flags [F.], cksum 0x91b8 (correct), seq 340, ack 45, win 5840, length 0
16:54:48.556321 IP (tos 0x0, ttl 52, id 42623, offset 0, flags [DF], proto TCP (6), length 40)
    164.132.52.143.3128 > x.x.x.x.62004: Flags [F.], cksum 0x3347 (correct), seq 45, ack 341, win 30016, length 0
16:54:48.556358 IP (tos 0x0, ttl 64, id 52159, offset 0, flags [DF], proto TCP (6), length 40)
    x.x.x.x.62004 > 164.132.52.143.3128: Flags [.], cksum 0x91b7 (correct), ack 46, win 5840, length 0
",toadzhou,Lukasa
3366,2016-06-29 21:08:00,"I understand the reasoning behind putting it in adapters, but I do feel like it should be in the prepare_headers section instead, if possible. PreparedRequests should allow users to do stupid things, like this. No need to have saftey tape at that level, imo. 

I won't be surprised if @Lukasa disagrees :)
",kennethreitz,Lukasa
3366,2016-06-29 21:32:12,"If we're treating this as a security issue and trying to prevent injection on dynamically generated headers, I think @Lukasa's internal method allows flexibility while maintaining the smallest surface area for issues. If we're just looking to prevent unintentional foot shooting, prepare_headers seems ""cleaner"" to me.
",nateprewitt,Lukasa
3366,2016-06-30 07:21:54,"Much to @kennethreitz's surprise, I agree with him. =)

I'm worried that users who are unsuspectingly using our high-level APIs aren't subject to risk, but if you're going to drop down to monkeying with PreparedRequests then you should be able to do more-or-less what you like. =) 
",Lukasa,kennethreitz
3366,2016-07-01 14:59:28,"@Lukasa Alright, there's rev 1 for a utility function.
",nateprewitt,Lukasa
3366,2016-07-02 19:13:13,"Squashed and passing. Thanks for taking the time to work through this with me @Lukasa :)
",nateprewitt,Lukasa
3366,2016-07-02 19:32:28,"Thanks @nateprewitt! :sparkles: :cake: :sparkles:
",Lukasa,nateprewitt
3366,2016-07-03 18:39:21,"@Lukasa, stealing my emoji chain hahaha
",kennethreitz,Lukasa
3365,2016-06-28 07:33:27,"Very nice @nateprewitt! Thanks! :sparkles: :cake: :sparkles:
",Lukasa,nateprewitt
3362,2016-06-28 19:21:05,"I think this looks good to me. @sigmavirus24 are you happy to merge this directly? I think I'd call the old behaviour a bug so I'd be fine with merging this for the next release.
",Lukasa,sigmavirus24
3359,2016-09-01 06:05:16,"I was reading through this and the related issues and PRs and I'd love to help out. However, I'm not sure what direction to head with it (or if it's something more suitable for the next version of `requests`).

Just so I understand correctly, we want to get it so `iter_content` and `text` both return Unicode strings. The original PR solved this by grabbing the `apparent_encoding`, which caused a problem when using streaming responses (`chardet.detect` consumes the entire stream).

I'm assuming [detecting the encoding incrementally](http://chardet.readthedocs.io/en/latest/usage.html#example-detecting-encoding-incrementally) will still lead to problems, as part of the stream will be consumed. Is this something that may be best solved via documentation, suggesting encoding is set before using `iter_content`? It seems like we'd need some sort of knowledge about the response, which may be hard to do without consuming it (or part of it).

Could we find some middle ground? For streaming responses, documentation calls out that encoding should be set. If it isn't, return bytes and let the users decode to their heart's desire (seemed like it worked well as a workaround). While nonstreaming responses can use the `apparent_encoding`, as originally suggested.

I know its a bit cold, but thoughts @Lukasa, @sigmavirus24?
",shellhead,Lukasa
3359,2016-09-01 06:05:16,"I was reading through this and the related issues and PRs and I'd love to help out. However, I'm not sure what direction to head with it (or if it's something more suitable for the next version of `requests`).

Just so I understand correctly, we want to get it so `iter_content` and `text` both return Unicode strings. The original PR solved this by grabbing the `apparent_encoding`, which caused a problem when using streaming responses (`chardet.detect` consumes the entire stream).

I'm assuming [detecting the encoding incrementally](http://chardet.readthedocs.io/en/latest/usage.html#example-detecting-encoding-incrementally) will still lead to problems, as part of the stream will be consumed. Is this something that may be best solved via documentation, suggesting encoding is set before using `iter_content`? It seems like we'd need some sort of knowledge about the response, which may be hard to do without consuming it (or part of it).

Could we find some middle ground? For streaming responses, documentation calls out that encoding should be set. If it isn't, return bytes and let the users decode to their heart's desire (seemed like it worked well as a workaround). While nonstreaming responses can use the `apparent_encoding`, as originally suggested.

I know its a bit cold, but thoughts @Lukasa, @sigmavirus24?
",shellhead,sigmavirus24
3359,2016-09-01 13:41:03,"I've been playing around with this since we reverted and I don't think that there's a nice way to avoid the exception. I'm +1 on that but I'm also thinking that an encoding param for `iter_content` that @Lukasa suggested in #3481 would be useful. It allows the user to avoid having to check and set `Response.encoding` on every Response that might be streamed.
",nateprewitt,Lukasa
3359,2016-09-11 02:23:12,"I know @sigmavirus24 brought up a concern about raising an exception if `encoding` isn't specified for the current major version. Would the suggested fix go against the 3.0 branch? @Lukasa, when you say `encoding`, I'm assuming you're referring to the response's encoding rather than the additional parameter on `iter_content`?
",shellhead,Lukasa
3359,2016-09-11 02:23:12,"I know @sigmavirus24 brought up a concern about raising an exception if `encoding` isn't specified for the current major version. Would the suggested fix go against the 3.0 branch? @Lukasa, when you say `encoding`, I'm assuming you're referring to the response's encoding rather than the additional parameter on `iter_content`?
",shellhead,sigmavirus24
3359,2016-09-23 20:03:07,"I think @shellhead's work in #3574 should have this addressed in 3.0.0.
",nateprewitt,shellhead
3358,2016-06-21 18:53:17,"Thanks @petedmarsh! :sparkles: :cake: :sparkles:
",Lukasa,petedmarsh
3355,2016-06-20 07:17:02,"@Lukasa Sorry, in my app, `key2` is supposed to be a boolean field. And many frameworks' boolean validations only check whether the `key2` string is '1' or 'true',rather than 'True'. So when the client assembles params with 'True', it will not pass the server's boolean validator.
",lufeihaidao,Lukasa
3355,2016-06-20 07:44:04,"@Lukasa OK, thanks.
",lufeihaidao,Lukasa
3354,2016-07-26 03:13:46,"@Lukasa  OK. thank you .
",ljdawn,Lukasa
3353,2016-10-25 19:02:25,"@Lukasa : no, my program hung (without any traceback) and I was wondering what could have caused the hanging. I just did a `gdb python3 <pid of my script>` and the only valuable thing I got was the pitput of `py-list`. I just rebooted my machine (it is a RPi) because I needed the script to run but if there is something i can do with the code to make it more talkative please let me know.
",wsw70,Lukasa
3340,2016-06-18 02:25:28,"This is my first use github to solve issue.
@Lukasa 
Thank you! And my bug solved now.

I use the pyenv to run the python. and I uninstall the python, then install again. It's working!   So happy~~

By the way, I tried the second method firstly, but It not working.  Then I tried the first one.I didn't know the reason. Maybe I had already installed that packages. 

Finally, Thanks very much!
",Kingmaxwang,Lukasa
3339,2016-08-24 18:03:19,"Thanks for that @jseabold, it's helpful when people come back to tidy up when they no longer have the time to dedicate to the patch. All the best with whatever you're spending your time on, and I hope you hang around!
",Lukasa,jseabold
3339,2016-08-24 18:06:53,"Thanks for contributing to Requests @jseabold!
",nateprewitt,jseabold
3338,2017-02-21 17:21:00,"@nateprewitt feel free to go ahead and tidy up and get this into master. I haven't had a chance to work on this project recently, and it would take me a little to get back up to speed on it.",davidsoncasey,nateprewitt
3337,2016-06-15 13:10:42,"@Lukasa Already installed those libs, alike, running with --upgrade tells `Requirement already up-to-date`
",PabloLefort,Lukasa
3337,2016-06-15 13:21:21,"@Lukasa OpenSSl version: `OpenSSL 1.0.1e-fips 11 Feb 2013`
",PabloLefort,Lukasa
3337,2016-06-27 14:42:02,"@Lukasa So, after some investigation, installed `tcpdump` to see whats happening with the packets.
On my local env the connection was through `TLS`, but in the server first try to connect with `TLS` and fallback to `SSL`. This raise `EOF Exception`.
Going foward, there was some firewall closing every connection. I changed it and it works like a charm!

Thanks for all.
",PabloLefort,Lukasa
3299,2016-06-09 18:03:38,"@Lukasa many thanks for the investigation!
",eriol,Lukasa
3295,2016-06-08 16:44:45,"This looks great. Thanks @Lukasa 
",sigmavirus24,Lukasa
3292,2016-06-07 15:04:04,"Sorry to raise again, Lukasa. I had tried in different places and laptops between Mainland china and Hongkong lots of time. But it still not work. Thus I think it is not the machine problem. @Lukasa 
",hedgeliu,Lukasa
3291,2016-06-07 15:16:02,"@Lukasa I'm in agreement. Let's do that in 3.0
",sigmavirus24,Lukasa
3289,2016-06-12 10:58:53,"Ok I'm good with this, leaving it up to @sigmavirus24 to merge when he's happy.
",Lukasa,sigmavirus24
3289,2016-06-17 09:50:54,"@sigmavirus24 , ping ... ?
",jayvdb,sigmavirus24
3287,2016-06-06 23:09:47,"@jayvdb thanks for testing but there is something I don't get. Tests for `DIST=trusty+sid` seem to use Python 2.7.6, but Stretch has Python 2.7.11. Even the stable release, Jessie, has 2.7.9 with several backports (for example cPython >= 2.7.9 has ssl features backported from Python3).
",eriol,jayvdb
3287,2016-06-07 05:17:42,"Hi @eriol, the `DIST=trusty+sid` job in that build is mostly Travis' trusty, with selected packages added from sid.  Travis trusty reports that it is Python 2.7.6, whereas trusty ships with 2.7.5 as far as I can see, so I am a bit confused about that, but it isnt a job I am particularly focused on, as the same problems appear in the other jobs that have a cleaner virtual env at the beginning of the test sequence.

As a consequence of building the additional testing on Travis, I feel more confident that any distro version shipping Python 2.7.9+ is fine with the current `requests.packages` unbundling code.  I still have some more test scenarios to create, though.

So the only 'problem' may be that the requests 2.10 package in stretch / sid states it `Depends: python:any (>= 2.7.5-5~)` , which is why it can be installed onto these Travis trusty environments, and probably any other debian derivatives which are still on Python 2.7.5 - 2.7.8 (are there any?).

Arch Linux also uses the unbundling, but it appears to be only providing Python 3.5.
Contrary to what I said earlier, Fedora isnt using the unbundling fallbacks in `requests.packaging.__init__`, but they are still [using symlinks](https://pkgs.fedoraproject.org/cgit/rpms/python-requests.git/tree/python-requests.spec#n115), however they are Python 2.7.10+ also, so should be safe anyway.
",jayvdb,eriol
3287,2016-06-07 14:15:46,"@eriol, I've been able to provide a better test case for this, and it shows that the problem exists even in 2.7.11 and 3.3.

https://travis-ci.org/jayvdb/requests/builds/135867398 is just #3289 with a `.travis.yml` that shows various combinations all work well with the bundled version of `urllib3`.

https://travis-ci.org/jayvdb/requests/builds/135870520 is a [very simple](https://github.com/jayvdb/requests/commit/09ae4ad78e28087fd5e041f5ce0a7bc603cd6a04) change to the `.travis.yml` that emulates what the Debian package looks like, and shows that `SubjectAltNameWarning` stops occurring on all environments that are using pyopenssl.  As explained in the opening issue, under the covers it is more than just the warning that isnt happening.  A second copy of the modules are being created and configured for pyopenssl mode, and the actual `urllib3` doesnt get into pyopenssl mode.

Finally, https://travis-ci.org/jayvdb/requests/builds/135873388 is #3286 , which fixes the problem.
That patch isnt intended to be the final solution; it is a WIP until we figure out what should be merged (I was told in #2670 to PR early), intended mostly to show what does work.  I have very quickly looked at the pip approach, and it is doing roughly the same thing so it should work.  It does require closer coupling between pip/requests and urllib3, which my patch avoided, for better and for worse.  I am not pushing to have my patch, or any other similar patch, merged, pushing `requests` further down this 'support unbundling' rabbit hole further, if the maintainers don't feel it is appropriate.  The patch is there to prove the bug exists.

My next step is going to be to check what happens with symlinks like what Fedora is doing, to see if that is a way to beat the import machinery. (I'd be surprised if it didnt work, but this problem is full of surprises).
",jayvdb,eriol
3287,2016-06-17 10:58:32,"@jayvdb Thanks for the heads-up.  That was a quick fix which seemed to work for Debian.  I'll take a look at this issue in more detail when I get a chance. 
",warsaw,jayvdb
3287,2016-06-17 12:14:26,"@eriol +1  I think pip has done the best job of making devendorizing easier for downstream redistributors.  My deltas are very small and now that it's been in place for a while, I haven't encountered any issues with it.  It's possible I'm missing something, so pinging @dstufft for additional thoughts.
",warsaw,eriol
3287,2016-06-17 12:38:42,"@jayvdb many thanks for #3289! I'm going to work on this in a few hours or at max tomorrow. Feel free to ping me again if you did not get a report from me tomorrow afternoon. But I hope to be able to work on this today.
",eriol,jayvdb
3287,2016-06-18 17:36:25,"@jayvdb fast report, working on unbundling stuff right now. Testing with your https://github.com/jayvdb/requests_issue_3287.
",eriol,jayvdb
3287,2016-06-18 21:41:34,"@dstufft many thanks for the detailed explanation!

I have just uploaded `requests 2.10.0-2` to unstable (still in upload queue, it'll show up shortly). I cherry picked the unbundling stuff from pip, [this is the patch](https://anonscm.debian.org/cgit/python-modules/packages/requests.git/commit/?h=patched/2.10.0-2&id=3311851f0cffea52cd779d01a6bf31cd8d34a37f) landed on Debian.

@jayvdb can you test again when `requests 2.10.0-2` will be in the archive? Thanks!
Also, what about renaming this bug in a more specific way? _Does not work_ seems to broad to me: we are addressing a problem using Python2 and related to SSL.

One more thing, I can bump the Python dependency to ensure a cPython2 with SSL feature from Python3, but only after we fix this. I was not aware of `trusty+sid` combo, I can understand the use, but mixing packages from different release seems dangerous to me.
",eriol,jayvdb
3287,2016-06-18 21:41:34,"@dstufft many thanks for the detailed explanation!

I have just uploaded `requests 2.10.0-2` to unstable (still in upload queue, it'll show up shortly). I cherry picked the unbundling stuff from pip, [this is the patch](https://anonscm.debian.org/cgit/python-modules/packages/requests.git/commit/?h=patched/2.10.0-2&id=3311851f0cffea52cd779d01a6bf31cd8d34a37f) landed on Debian.

@jayvdb can you test again when `requests 2.10.0-2` will be in the archive? Thanks!
Also, what about renaming this bug in a more specific way? _Does not work_ seems to broad to me: we are addressing a problem using Python2 and related to SSL.

One more thing, I can bump the Python dependency to ensure a cPython2 with SSL feature from Python3, but only after we fix this. I was not aware of `trusty+sid` combo, I can understand the use, but mixing packages from different release seems dangerous to me.
",eriol,dstufft
3286,2016-06-06 12:09:22,"So pip does [something similar](https://github.com/pypa/pip/blob/master/pip/_vendor/__init__.py#L79) but in a far more intelligent way than what is happening here.

That said, I don't think we have evidence that what pip is doing works either.

I also don't agree with your all or nothing mentality @jayvdb. It's not productive.
",sigmavirus24,jayvdb
3286,2016-06-06 22:52:58,"Hello,
sorry for the late reply, I was on trip and once returned I had to work on the backport of betamax for the stable release due the sheduled upload on OpenStack (I don't remember which one).

I definitively agree with @Lukasa here: the problem is on the Debian side.
So, as soon I complete the backport of betamax I will work on this.
This is my plan:
1. use the same pip's `vendored` function to patch requests.packages.**init**;
2. add some tests on Debian CI infrastructure over this specific issue;
3. upload this new revision to experimental suite;
4. make a call for test;

@jayvdb can you share your tests about OpenSSL & SNI support on Debian? Thanks!
",eriol,jayvdb
3286,2016-06-06 22:52:58,"Hello,
sorry for the late reply, I was on trip and once returned I had to work on the backport of betamax for the stable release due the sheduled upload on OpenStack (I don't remember which one).

I definitively agree with @Lukasa here: the problem is on the Debian side.
So, as soon I complete the backport of betamax I will work on this.
This is my plan:
1. use the same pip's `vendored` function to patch requests.packages.**init**;
2. add some tests on Debian CI infrastructure over this specific issue;
3. upload this new revision to experimental suite;
4. make a call for test;

@jayvdb can you share your tests about OpenSSL & SNI support on Debian? Thanks!
",eriol,Lukasa
3286,2016-06-06 23:04:03,"@jayvdb never mind, you were talking about https://github.com/kennethreitz/requests/issues/3287.
",eriol,jayvdb
3286,2016-06-06 23:43:22,"I agree with @Lukasa — this part of the codebase is absolutely abhorrent and should not even exist — however, we have chosen to do so to improve the user experience of our very unfortunate distro-installed users. 

We've already done more than I think we should have. Doing even more, without extremely strong beyond-a-reasonable-doubt purpose, is out of the question.
",kennethreitz,Lukasa
3250,2016-06-01 15:48:56,"@Lukasa to be clear we'd still be able to use the constant we import from `requests.models` that also appropriately helps sphinx determine what is happening.

I am not particularly opinionated about this I guess.
",sigmavirus24,Lukasa
3250,2016-06-01 15:59:29,"@Lukasa that works for me.
",sigmavirus24,Lukasa
3250,2016-06-09 04:49:21,"@Lukasa that was going to be my suggestion. Utilizing class-instance variables just for the doc build is a huge red flag for me. 
",kennethreitz,Lukasa
3236,2016-06-21 02:18:40,"@eriol I'm so sorry. I think I lost track of this while I was travelling to PyCon. I expect the same happened to @Lukasa 
",sigmavirus24,eriol
3221,2016-05-26 13:27:29,"@Lukasa make sense!! thanks =)
",yangbeom,Lukasa
3217,2016-05-24 07:00:54,"Hey @alanhamlett, thanks for reporting this!

This will be fixed when we release the next minor version of requests, which will bring in an updated version of urllib3.
",Lukasa,alanhamlett
3213,2016-07-13 20:54:54,"@sigmavirus24 as @cournape mentions you can't time it like that; try using `time` on the command line. On my machine (ran each ~6-10 times to try to get a reliable average) for user+sys times (including sys because if there's any strange calls made into the kernel as a result of the import, that should be counted):
- `time python -c """"` (CPy 2.7.11) = 130 ms (time for Python to start up)
- `time python -c ""import requests""` (2.10.0) = 240 ms (above + importing requests)
- `time python -c ""import urllib3""` (1.15.1) = 210 ms (installed separately)

So `urllib3` takes about 80 ms, then about 30 ms more for requests stuff.
",nicktimko,sigmavirus24
3213,2016-07-13 22:16:57,"@kennethreitz python does not take 120 ms to start, unless you are on a seriously broken environment. It is much closer to 20 ms on decently modern hw (< 5 years), i.e. importing requests means 3x the cost of starting python.

FWIW, on my 2011 Desktop PC (Debian):



On my 2014 macbook (OS X)



A simple `hg` (for help) on my macbook takes ~ 100 ms
",cournape,kennethreitz
3213,2017-01-13 16:27:41,"@Lukasa Unless I'm mistaken, the `.contrib.pyopenssl` ssl wrapper & context is not needed on Python 2.7.9+ and 3.4+. Given that, `requests` is always using pyopenssl when it is installed, even when the core Python libs support SNI, etc. So, changing the import to be:



Less of an opt-out, more of a not-needed.

Now, if there are cases where the pyopenssl SSLContext wrapper is desired for some reason (?) even when the core libraries are sufficient, then I'll make another pass. What do you think?",dsully,Lukasa
3213,2017-01-13 16:32:29,"@sigmavirus24 Indirectly via `cryptography`:



Imported via:

",dsully,sigmavirus24
3213,2017-01-13 16:34:12,"@Lukasa Right.. forgot about that. I don't have that particular issue on macOS, but most people do.

I'll look at coming up with a way to explicitly opt-out then.",dsully,Lukasa
3213,2017-01-13 16:37:39,"@Lukasa - yes and no. Our build environment has real dependency management for modules (https://engineering.linkedin.com/blog/2016/08/introducing--py-gradle--an-open-source-python-plugin-for-gradle), which means just because someone included PyOpenSSL in their dependency tree, doesn't mean that the code you are importing for your upstream uses it. Lots of code gets installed transitively, but not imported.",dsully,Lukasa
3213,2017-01-13 17:12:59,"@Lukasa - without moving the location of the pyopenssl loader, currently in `requests/__init__.py`, would an environment variable be acceptable?",dsully,Lukasa
3213,2017-01-13 17:42:03,"@sigmavirus24 I hear you there. If environment changes aren't ok, an explicit call? I'd have to move the current injection, since it happens in `__init__.py`",dsully,sigmavirus24
3213,2017-01-13 18:25:05,@sigmavirus24 I think so.. but I don't follow 100%. Do you mean checking the OPENSSL_VERSION to avoid the pyopenssl injection?,dsully,sigmavirus24
3213,2017-01-13 18:27:23,"@sigmavirus24 And yes, you are correct - we run Python 2.7.11 and Python 3.5 (soon to be 3.6), both compiled against a non-system shipped OpenSSL.$latest. Our build system (PyGradle) also sets build time `CPPFLAGS` and `LDFLAGS` to have modules like `cryptography` and `PyOpenSSL` link to the non-system OpenSSL as well.",dsully,sigmavirus24
3213,2017-01-13 19:19:30,"
>@sigmavirus24 I think so.. but I don't follow 100%. Do you mean
>checking the OPENSSL_VERSION to avoid the pyopenssl injection?

That's exactly what I failed to communicate. :)

-- 
Sent from my Android device with K-9 Mail. Please excuse my brevity.
",sigmavirus24,sigmavirus24
3213,2017-01-13 20:11:53,"On January 13, 2017 1:40:44 PM CST, Dan Sully <notifications@github.com> wrote:
>Ok, so:
>
>
>
>What is the minimum OpenSSL version for the required functionality?
>1.0.1?

I think 1.0.1 is a good minimum but would rather defer to @Lukasa and @reaperhulk on that.

-- 
Sent from my Android device with K-9 Mail. Please excuse my brevity.
",sigmavirus24,Lukasa
3213,2017-01-13 20:24:37,"@Lukasa - It should be set to whatever pyOpenSSL's minimum is for the functionality that is required.

0.9.8 was dropped in pyOpenSSL 16.1.0:

https://pyopenssl.readthedocs.io/en/stable/changelog.html

So 0.9.9 would be the bare minimum, but that's hard to recommend. I still say 1.0.1 or perhaps 1.0.0",dsully,Lukasa
3212,2017-01-03 12:50:37,"@Lukasa how can one check if the problem is with requests or certificate issuer?
I have one cert issued by Entrust and my browsers are quite ok with it when I browse to the URL.
But when I try to get to that URL via requests I have `[SSL: CERTIFICATE_VERIFY_FAILED]`

**Full traceback**


**installed packages**


**Cert details**
",hellt,Lukasa
3212,2017-01-03 14:06:01,"Thanks @Lukasa for this educational reply you gave! Really appreciate and sorry for troubling with this, it was my TLS-knowledge-gap =) Mostly I went to issues because in was all good in browsers and didn't work in requests. But now its clear to me what is the root cause.

I believe there is no workaround (download intermediate cert upfront) for this case, but to reconfigure the server? For the time being I will skip verification checks",hellt,Lukasa
3199,2016-05-19 19:46:29,"(Closing for @Lukasa since he said he was going to and didn't. ;))
",sigmavirus24,Lukasa
3195,2016-05-16 10:41:13,"@Lukasa added tests, please check and merge.
",kumarvaradarajulu,Lukasa
3195,2016-05-16 10:54:28,"@Lukasa addressed comments, pls check
",kumarvaradarajulu,Lukasa
3192,2016-05-17 15:45:25,"Thanks @brettdh!
",sigmavirus24,brettdh
3185,2016-05-12 21:05:00,"@brettdh Yeah, I think that would be fine. =)
",Lukasa,brettdh
3185,2016-05-13 17:09:42,"@sigmavirus24 The test server refactoring was mainly done as I added a failing functional test for the feature, in order to prevent that test failure from hanging the server (because it was waiting for a connection that wasn't coming).

Now that the test is passing, I'm happy to separate the test server refactoring into a separate PR if that helps.
",brettdh,sigmavirus24
3185,2016-05-13 19:43:47,"Cool, @sigmavirus24 do you want to re-review?
",Lukasa,sigmavirus24
3185,2016-05-16 21:31:50,"@sigmavirus24 Yeah, sorry; I forgot that I had vim configured to always trim trailing whitespace on save. Thought I had reverted those before committing, but clearly I missed one. :-)
",brettdh,sigmavirus24
3185,2016-05-16 21:56:33,"@brettdh no need to put trailing whitespace back, just change that line to use iter instead =D
",sigmavirus24,brettdh
3185,2016-05-17 07:23:06,"Ok, :+1: from me. @sigmavirus24?
",Lukasa,sigmavirus24
3185,2016-05-17 15:42:29,"Looks good to me. Thanks @brettdh!
",sigmavirus24,brettdh
3184,2016-05-11 13:55:01,"@Lukasa in that case, could we get rid of the first elif statement and just go to the case where `body is not None`? Looking at `super_len`, I think it would calculate length in the same way. So in what I'm proposing, prepare content length would look like this:



All tests pass with this change.
",davidsoncasey,Lukasa
3184,2016-05-11 18:44:36,"@sigmavirus24 Arg, good spot. Hrm. We need to have some extra logic here to get this right.
",Lukasa,sigmavirus24
3184,2016-05-11 19:18:09,"@Lukasa @sigmavirus24 it looks to me like the try/except of using `super_len` to calculate the length of the body could be moved to `prepare_content_length` - and then both the `Content-Length` and `Transfer-Encoding` headers would be set there. If content length can be calculated, then we strip the TE header, and otherwise, we strip the CL header. I think this would be the most clear, and would ensure that the headers are mutually exclusive, regardless of what a user enters.
",davidsoncasey,Lukasa
3184,2016-05-11 19:18:09,"@Lukasa @sigmavirus24 it looks to me like the try/except of using `super_len` to calculate the length of the body could be moved to `prepare_content_length` - and then both the `Content-Length` and `Transfer-Encoding` headers would be set there. If content length can be calculated, then we strip the TE header, and otherwise, we strip the CL header. I think this would be the most clear, and would ensure that the headers are mutually exclusive, regardless of what a user enters.
",davidsoncasey,sigmavirus24
3184,2016-05-12 00:02:47,"@Lukasa sounds good. I'll see if I can put something together for that. Thanks for helping work through this.
",davidsoncasey,Lukasa
3184,2016-05-17 16:36:51,"@Lukasa @sigmavirus24 I updated this PR to make the Content-Length and Transfer-Encoding headers mutually exclusive, regardless of whether a user manually provides a value, as @sigmavirus24 showed in his comment. While working on this, I stumbled upon #1648. I hadn't realized that this had been a subject of so much discussion, and that people have differing opinions about how this should work. This is obviously related to that issue, so I understand if you decide that this isn't the fix that you're looking for. It could be refactored to raise an error if a user has provided a CL header when TE is set, and vice versa, instead of silently removing the unneeded header. Let me know what you think.
",davidsoncasey,Lukasa
3184,2016-05-17 16:36:51,"@Lukasa @sigmavirus24 I updated this PR to make the Content-Length and Transfer-Encoding headers mutually exclusive, regardless of whether a user manually provides a value, as @sigmavirus24 showed in his comment. While working on this, I stumbled upon #1648. I hadn't realized that this had been a subject of so much discussion, and that people have differing opinions about how this should work. This is obviously related to that issue, so I understand if you decide that this isn't the fix that you're looking for. It could be refactored to raise an error if a user has provided a CL header when TE is set, and vice versa, instead of silently removing the unneeded header. Let me know what you think.
",davidsoncasey,sigmavirus24
3184,2016-05-18 15:31:21,"@Lukasa @sigmavirus24 I'll go ahead and alter it to raise an exception for now (perhaps `InvalidHeadersError`? Or is there an existing exception that would make sense to raise?). And then we can leave this PR open until it's decided what the best behavior is.

Also, are you planning on coming to Portland for PyCon? I didn't get a ticket in time, but I live in Portland and it would be great to meet up and discuss the project.
",davidsoncasey,Lukasa
3184,2016-05-18 15:31:21,"@Lukasa @sigmavirus24 I'll go ahead and alter it to raise an exception for now (perhaps `InvalidHeadersError`? Or is there an existing exception that would make sense to raise?). And then we can leave this PR open until it's decided what the best behavior is.

Also, are you planning on coming to Portland for PyCon? I didn't get a ticket in time, but I live in Portland and it would be great to meet up and discuss the project.
",davidsoncasey,sigmavirus24
3184,2016-05-22 20:45:24,"@Lukasa @sigmavirus24 I made a few more changes and a bit more refactoring of `prepare_body` and `prepare_content_length`. These include:
- Moving the logic set `Transfer-Encoding` header to the `prepare_content_length` method. This makes it more explicit that the headers should be mutually exclusive, and does not rely on implicitly connected logic in two methods.
- Raises an `InvalidHeaderError` if a user manually sets either header when it should not be set (let me know if you think that there's a preexisting exception that could be raised instead).
- Added tests to check different cases of when each header should be set.

I understand we may need to wait to decide if this is the desired behavior, so no need to review or merge this right away. We can leave this PR open until you've had a chance to discuss and agree upon the desired behavior.
",davidsoncasey,Lukasa
3184,2016-05-22 20:45:24,"@Lukasa @sigmavirus24 I made a few more changes and a bit more refactoring of `prepare_body` and `prepare_content_length`. These include:
- Moving the logic set `Transfer-Encoding` header to the `prepare_content_length` method. This makes it more explicit that the headers should be mutually exclusive, and does not rely on implicitly connected logic in two methods.
- Raises an `InvalidHeaderError` if a user manually sets either header when it should not be set (let me know if you think that there's a preexisting exception that could be raised instead).
- Added tests to check different cases of when each header should be set.

I understand we may need to wait to decide if this is the desired behavior, so no need to review or merge this right away. We can leave this PR open until you've had a chance to discuss and agree upon the desired behavior.
",davidsoncasey,sigmavirus24
3184,2016-05-24 02:12:15,"@sigmavirus24 @Lukasa sounds good. I'll make those small cleanup changes and squash extraneous commits.
",davidsoncasey,Lukasa
3184,2016-05-24 02:12:15,"@sigmavirus24 @Lukasa sounds good. I'll make those small cleanup changes and squash extraneous commits.
",davidsoncasey,sigmavirus24
3184,2016-06-09 16:45:56,"@Lukasa @sigmavirus24 alright, it took me a bit to get back to this, but I've cleaned it up a bit and it should be ready to merge into the proposed branch. The conlflict is in AUTHORS.rst. Let me know if there's anything else here you'd like to see changed, otherwise I'll try to find another issue to get started on.
",davidsoncasey,Lukasa
3184,2016-06-09 16:45:56,"@Lukasa @sigmavirus24 alright, it took me a bit to get back to this, but I've cleaned it up a bit and it should be ready to merge into the proposed branch. The conlflict is in AUTHORS.rst. Let me know if there's anything else here you'd like to see changed, otherwise I'll try to find another issue to get started on.
",davidsoncasey,sigmavirus24
3184,2016-06-15 07:26:48,"Ok, good by me. @sigmavirus24?
",Lukasa,sigmavirus24
3184,2016-06-16 16:34:50,"@sigmavirus24 I'll open a new PR into proposed/3.0. As far as the conflict, that was only in the contributors.rst, and I was thinking it would be a little cleaner if one of you resolved it on merge, instead of back merging master into this branch and fixing it there. I'm happy to do that if you like though.
",davidsoncasey,sigmavirus24
3181,2016-05-09 16:11:16,"@Lukasa sounds good. I'll work on getting something put together in the next couple of days.
",davidsoncasey,Lukasa
3180,2016-05-05 13:54:48,"Hi, @sigmavirus24 
1) http://stackoverflow.com/questions/11662960/ioerror-errno-22-invalid-argument-when-reading-writing-large-bytestring and it's my fault to use like that.
2) Sorry, I couldn't include the traceback with requests due to some logic.. but there is problem when you use data=data which is above 2GB size binary in OS X. If you use OSX, you can test it.

Naver mind. I will post in StackOverflow.

Thanks.
",AstinCHOI,sigmavirus24
3179,2016-05-05 13:26:21,"@Lukasa so we set it to `None` in [the descriptor](https://github.com/kennethreitz/requests/blob/master/requests/models.py#L728) in (2) [error](https://github.com/kennethreitz/requests/blob/master/requests/models.py#L739) [cases](https://github.com/kennethreitz/requests/blob/master/requests/models.py#L744). 

I don't think we can just change it to never be `None` before 3.0 but that's my hesitancy to break user-level code speaking.
",sigmavirus24,Lukasa
3179,2016-05-05 13:40:55,"@sigmavirus24 Yeah, that seems reasonable. In that case, clearly the `json()` code needs to handle it too.

That means I think this patch is fine. @messense are you interested in adding a test to validate that this works as expected?
",Lukasa,messense
3179,2016-05-05 13:40:55,"@sigmavirus24 Yeah, that seems reasonable. In that case, clearly the `json()` code needs to handle it too.

That means I think this patch is fine. @messense are you interested in adding a test to validate that this works as expected?
",Lukasa,sigmavirus24
3179,2016-05-06 07:47:57,"Hurrah, this looks great to me! I'd argue the previous behaviour was a bug so we have no API compatibility concerns to worry about here, but I want @sigmavirus24 to ACK that before merging.
",Lukasa,sigmavirus24
3178,2016-05-05 07:20:25,"@haikuginger Thanks for this! I've left some notes inline.
",Lukasa,haikuginger
3178,2016-05-22 15:51:06,"Sorry @haikuginger! GitHub doesn't ping me when you push new changes. :(
",sigmavirus24,haikuginger
3178,2016-05-22 15:51:41,"I'm :+1: on this if @Lukasa is =D
",sigmavirus24,Lukasa
3178,2016-05-22 16:02:02,"Let's do it! Thanks @haikuginger, you're doing awesome work! :sparkles: :cake: :sparkles:
",Lukasa,haikuginger
3176,2016-05-04 20:34:24,"I agree with @Lukasa. I don't think we need this as an extra parameter to this method. I also don't think we need to keep track of which headers we're sanitizing for a user. I also agree that we might be able to better serve you by refactoring things so you can override these with subclassing.
",sigmavirus24,Lukasa
3176,2016-05-05 08:31:07,"@sigmavirus24, @Lukasa thanks for the comments.
Inheriting `SessionRedirectMixin` was my first idea but then I've seen so much logic in the `resolve_redirects` function so it became a sort of code duplication.
By saying this, I agree with you guys, factoring out the logic might come very handy.
So how should we proceed?
",RonenHoffer,Lukasa
3176,2016-05-05 08:31:07,"@sigmavirus24, @Lukasa thanks for the comments.
Inheriting `SessionRedirectMixin` was my first idea but then I've seen so much logic in the `resolve_redirects` function so it became a sort of code duplication.
By saying this, I agree with you guys, factoring out the logic might come very handy.
So how should we proceed?
",RonenHoffer,sigmavirus24
3174,2016-08-23 16:37:51,"@Lukasa if we decide to cache the iterators, it would. I'm not sure we should be caching the iterators though.
",sigmavirus24,Lukasa
3174,2016-08-23 20:10:53,"@sigmavirus24 You're right. I wouldn't expect your example to work, and I can see why the surprise with `iter_content()`, because it's a function, which one expects to have side effects. But it still seems valuable to have a way to hook into the stream at whatever point it's currently at. This was a feature, however undesirable, that Requests 2.9 had which Requests 2.10 does not have... and the workaround is clumsy, requiring code in two different points in the code to be coordinated.

It really would be nice if the response object exposed some sort of interface such that one doesn't have to carry both the iterator and the response around anytime streaming is in play.
",jaraco,sigmavirus24
3172,2016-05-03 07:07:20,"Thanks and  I got it @Lukasa 
",iliul,Lukasa
3171,2016-05-03 07:44:40,"Thanks for this @luv! :sparkles: :cake: :sparkles:
",Lukasa,luv
3170,2016-05-03 15:56:32,"@Lukasa Sorry to miss my original thread, Let me respond the questions in my thread. Thanks for your help
",nahonnyate,Lukasa
3137,2016-05-03 15:58:55,"@Lukasa  cert.pfx is the SSL certificate which I need to mention in my POST/GET along with basic auth.
@baptistapedro I tried both the option and it is throwing me the following error.

I am attaching the error I encountered in both the option.

Option # 1

> > > import requests
> > > from requests.auth import HTTPBasicAuth
> > > auth = HTTPBasicAuth('auth_user', 'auth_pass')
> > > requests.post('https://my-site.com/rest_service', cert=('/Users/my_user/Desktop/auth_user.pfx', '/path/key'),  auth=auth)
> > > Traceback (most recent call last):
> > >   File ""<stdin>"", line 1, in <module>
> > >   File ""/Users/my_user/anaconda/lib/python2.7/site-packages/requests/api.py"", line 88, in post
> > >     return request('post', url, data=data, *_kwargs)
> > >   File ""/Users/my_user/anaconda/lib/python2.7/site-packages/requests/api.py"", line 44, in request
> > >     return session.request(method=method, url=url, *_kwargs)
> > >   File ""/Users/my_user/anaconda/lib/python2.7/site-packages/requests/sessions.py"", line 456, in request
> > >     resp = self.send(prep, *_send_kwargs)
> > >   File ""/Users/my_user/anaconda/lib/python2.7/site-packages/requests/sessions.py"", line 559, in send
> > >     r = adapter.send(request, *_kwargs)
> > >   File ""/Users/my_user/anaconda/lib/python2.7/site-packages/requests/adapters.py"", line 382, in send
> > >     raise SSLError(e, request=request)
> > > requests.exceptions.SSLError: [SSL] PEM lib (_ssl.c:2580)

Option # 2

> > > r = requests.post(""https://my-site.com/rest_service"", verify='/Users/my_user/Desktop/auth_user.pfx', data={}, auth=('auth_user', 'auth_pass'))
> > > Traceback (most recent call last):
> > >   File ""<stdin>"", line 1, in <module>
> > >   File ""/Users/my_user/anaconda/lib/python2.7/site-packages/requests/api.py"", line 88, in post
> > >     return request('post', url, data=data, *_kwargs)
> > >   File ""/Users/my_user/anaconda/lib/python2.7/site-packages/requests/api.py"", line 44, in request
> > >     return session.request(method=method, url=url, *_kwargs)
> > >   File ""/Users/my_user/anaconda/lib/python2.7/site-packages/requests/sessions.py"", line 456, in request
> > >     resp = self.send(prep, *_send_kwargs)
> > >   File ""/Users/my_user/anaconda/lib/python2.7/site-packages/requests/sessions.py"", line 559, in send
> > >     r = adapter.send(request, *_kwargs)
> > >   File ""/Users/my_user/anaconda/lib/python2.7/site-packages/requests/adapters.py"", line 382, in send
> > >     raise SSLError(e, request=request)
> > > requests.exceptions.SSLError: unknown error (_ssl.c:2825)
",nahonnyate,Lukasa
3137,2016-05-03 17:59:52,"@Lukasa thanks for the referred page, I just did info on that SSL .pfx file and here is the output: if it helps.

pkcs12 -info -in '/Users/my_user/Desktop/auth_user.pfx'
Enter Import Password:
MAC Iteration 2000
MAC verified OK
PKCS7 Data
Shrouded Keybag: pb***_SHA1**_-Key*****_-**_**, Iteration 2000
Bag Attributes
    localKeyID: 0\* *\* 00 00 
    friendlyName: le-*****_-**_-***_-**__-_********
    Microsoft CSP Name: Microsoft Enhanced Cryptographic Provider v1.0
Key Attributes
    X509v3 Key Usage: 80 
",nahonnyate,Lukasa
3137,2016-05-03 18:21:03,"@Lukasa tried the following as you have suggested , not sure why it is complaining about the certificate now, I am using the same certificate (.pfx version) through SOAP UI in POST request and getting proper response back

## Converted .pfx file to .pem through this

openssl pkcs12 -in '/Users/auth_user/Desktop/cert.pfx' -out cert.pem -nodes

## use the same .pem in my POST

r = requests.post('https://my-site.com/rest_service', verify='/Users/auth_user/Desktop/cert.pem', data={}, auth=('auth_user', 'auth_pass'))
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/auth_user/anaconda/lib/python2.7/site-packages/requests/api.py"", line 88, in post
    return request('post', url, data=data, *_kwargs)
  File ""/Users/auth_user/anaconda/lib/python2.7/site-packages/requests/api.py"", line 44, in request
    return session.request(method=method, url=url, *_kwargs)
  File ""/Users/auth_user/anaconda/lib/python2.7/site-packages/requests/sessions.py"", line 456, in request
    resp = self.send(prep, *_send_kwargs)
  File ""/Users/auth_user/anaconda/lib/python2.7/site-packages/requests/sessions.py"", line 559, in send
    r = adapter.send(request, *_kwargs)
  File ""/Users/auth_user/anaconda/lib/python2.7/site-packages/requests/adapters.py"", line 382, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:590)
",nahonnyate,Lukasa
3137,2016-05-04 05:12:39,"@Lukasa I also tried with urllib2 with the following option and getting the same error as before:

---

import requests
import urllib2
import base64

chimpConfig = {
    ""headers"" : {
    ""Authorization"": ""Basic "" + base64.encodestring(""auth_user:auth_pass"").replace('\n', '')
    },
    ""cert"": ""/Users/user/Desktop/ssl_suth_cert.pem"",
    ""url"": 'https://url.com/'}

#perform authentication
datas = None
timeout = 2
cert = ""/Users/user/Desktop/ssl_suth_cert.pem""
request = urllib2.Request(chimpConfig[""url""], datas,  chimpConfig[""headers""])
result = urllib2.urlopen(request)

print ""Response:"",result
print result.code

---

it is giving me the following error
urllib2.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:590)>

now if I add ""cert"" parameter in my call I am getting the following error:
only change in the above code is: added cert param:
request = urllib2.Request(chimpConfig[""url""], datas, **cert,** chimpConfig[""headers""])

error:

Traceback (most recent call last):
  File ""urlpost.py"", line 16, in <module>
    request = urllib2.Request(chimpConfig[""url""], datas, cert, chimpConfig[""headers""])
  File ""/Users/summukhe/anaconda/lib/python2.7/urllib2.py"", line 238, in **init**
    for key, value in headers.items():
AttributeError: 'str' object has no attribute 'items'
",nahonnyate,Lukasa
3137,2016-05-04 17:55:20,"@Lukasa  , thanks for your direction, after analyzing the log it seems the url was having some '\n' character which was causing the later issue.
simple rstrip() worked and Verify= False was the key as you have mentioned before. 

One more question, how do I post message through this ? if I am adding another param as ""data"", this request is throwing error ""TypeError: post() got multiple values for keyword argument 'data'""

I really appreciate your help and the community to provide the support.

## POST started working

headers = {'content-type': 'application/json'}

> > > r = requests.post(url, headers, verify=False, cert='/Users/user/Desktop/auth_user.pem', auth=('user', 'pass'))
> > > r.content
> > > 'POST REQUEST PING'
> > > r.status_code
> > > 200
> > > ///
",nahonnyate,Lukasa
3137,2016-05-04 18:38:14,"@Lukasa 

## I think we are all clean now, able to post with data-file as well.

Thanks everyone for the help, we can close the issue

payload='/Users/user/Desktop/a.xml'
r = requests.post(url,auth=('auth_user', 'pass'), data=payload, verify=False, cert='/Users/user/Desktop/sinri2683c.pem',headers=headers)
",nahonnyate,Lukasa
3136,2016-04-28 10:43:10,"Thanks so much @Natim!
",Lukasa,Natim
3131,2016-04-26 14:40:42,"@chipaca keep in mind that whatever adapter you're using will have to deal with older versions of requests and if it's not meant to touch the network should not be inheriting from the HTTPAdapter.
",sigmavirus24,chipaca
3131,2016-04-26 14:48:41,"@sigmavirus24 I'm using https://github.com/msabramo/requests-unixsocket/ which does inherit from `HTTPAdapter`. I'm not sure what you mean, though; where does ""touching the network"" become a factor?
",chipaca,sigmavirus24
3109,2016-04-22 10:42:59,"@sigmavirus24 Because it sets things on the connection pool directly, which is Not Right. We want to get a different pool if those are different.
",Lukasa,sigmavirus24
3109,2016-09-06 13:35:32,"@sigmavirus24 no problem, rebased!
",jeremycline,sigmavirus24
3109,2016-09-06 13:58:45,"wait a second, @Lukasa can this not go into a pre-3.0 release?
",sigmavirus24,Lukasa
3109,2016-09-06 14:32:52,"@sigmavirus24 Nope, we removed cert_verify which is public on the HTTPAdapter.
",Lukasa,sigmavirus24
3109,2016-09-14 15:05:18,"@sigmavirus24 Should I add a 3.0 changelog entry or is that something you'd prefer to handle?
",jeremycline,sigmavirus24
3104,2016-04-19 06:19:23,"Determined a better way of reproducing the issue consistently. @Lukasa looks fine on 2.9.0.  Move along nothing to see here.
",fredthomsen,Lukasa
3103,2016-04-18 13:42:50,"@Lukasa Thanks for pointing to the right direction.
",gastlygem,Lukasa
3100,2016-04-17 20:03:22,"@kennethreitz, @sigmavirus24 Thanks! 👍 
",hitstergtd,kennethreitz
3099,2016-04-26 17:32:21,"@emgerner-msft is correct. I am bit confused by @sigmavirus24's comment, having a ""total timeout"" without using threads or processes seems quite pedestrian and not at all ""amazing"". Just calculate the deadline at the start of the whole process (e.g. `deadline = time.time() + total_timeout`) and then on any individual operation set the timeout to be `deadline - time.time()`.
",jribbens,sigmavirus24
3099,2016-04-27 10:27:50,"Apologies if I was unclear @sigmavirus24 , you seem to have critiqued my pseudocode illustration of principle as if you thought it was a literal patch. I should point out though that `time.time()` does not work the way you apparently think - daylight savings time is not relevant, and neither is clock skew on the timescales we're talking about here. Also you have misunderstood the suggestion if you think the bug you describe would occur. Finally I am not sure what you mean by the Requests API being ""frozen"" as the API was changed as recently as version 2.9.0 so clearly whatever you mean it's not what I would normally understand by the word.
",jribbens,sigmavirus24
3099,2016-04-27 20:39:08,"@Lukasa Okay, thanks! How does the library determine the request is no longer continuing? For example, if I used the timeout decorator route and cut off in the middle of the download, when would the download actually stop? Do I need to do anything special with the streaming options?
",emgerner-msft,Lukasa
3099,2016-04-27 22:11:44,"@Lukasa Yup, tried the basic [usage snippet](https://github.com/pnpnpn/timeout-decorator#usage) and it doesn't work on Windows. I read some more of the code/examples and fiddled and it looks like if we don't use signals the package might work, but everything has to be pickable which is not the case for my application. So as far as I can tell, timeout decorator won't solve my problem. Any other ideas?
",emgerner-msft,Lukasa
3099,2016-04-28 16:47:12,"@Lukasa To be blunt, I simply don't know. I haven't used signals before, and much like I didn't realize until you told me that they'd interrupt the request I'm not sure what's appropriate. I'm also not trying to get this just to work on Windows. I need full crossplat support (Windows and Unix) and both Python 2 and Python 3 support. So much of signals looks platform specific it's throwing me. [Timer](https://docs.python.org/2/library/threading.html#timer-objects) was one of the solutions I was looking at that looked less low level and thus might take care of my constraints, but I'm not sure then how I could close the connection. I can do more reading, but this is why I was hoping to get additional guidance from you guys. :)
",emgerner-msft,Lukasa
3099,2016-04-28 17:21:52,"@Lukasa Just reading through the wall of text you just wrote above -- interesting! On the discussion of stream=True and iter_content to time downloads, what is the equivalent way of handling larger uploads?

_PS_: The paragraph above starting with 'Put another way,..' is the kind of guidance I looked for in the docs. Given the number of requests you get for maximum timeout (and your valid reasons for not doing it), maybe the best thing to do is add some of that information in the [timeout docs](http://docs.python-requests.org/en/master/user/advanced/#timeouts)?
",emgerner-msft,Lukasa
3099,2016-04-28 19:14:54,"@sigmavirus24 

> If a total timeout belongs anywhere, it would be there, but again, it would have to work on Windows, BSD, Linux, and OSX with excellent test coverage and without it being a nightmare to maintain.

Agreed!
",kennethreitz,sigmavirus24
3099,2016-04-29 17:04:57,"@sigmavirus24 Throughout this thread you have been needlessly condescending, inflammatory and rude, and I'm asking you politely, please stop.
",jribbens,sigmavirus24
3099,2016-05-04 18:05:23,"@Lukasa I looked in detail at your suggestions for how to do streaming upload and download and read the docs on these topics. If you could validate my assumptions/questions that would be great.
1. For streaming downloads if I use something like a read timeout '(e.g. 5s) and then iter_content over fairly small chunks (e.g. 1KB of data)', that means the requests library will apply the 5s timeout for each read of 1KB and timeout if it takes more than 5s. Correct?
2. For streaming uploads if I use a generator or file like object which returns chunks of data and I set the read timeout to 5s, the request library will apply the 5s timeout for each chunk I return and timeout if it takes longer. Correct?
3. If I don't use a generator for upload and simply pass bytes directly, how does the requests library decide to apply the read timeout I set? For example, if I pass a chunk of size 4MB and a read timeout of 5s, when exactly is that read timeout applied?
4. If I don't use iter_content and simply have requests download all of the content directly into the request with a read timeout of 5s, when exactly is that read timeout applied?

I have a general understanding of sockets/TCP protocol/etc but not exactly how urllib works with these concepts at a lower level or if requests does anything special besides passing the values down. I want to understand exactly how the timeouts are applied as simply getting the control flow back and applying my own timeout scheme doesn't work given the crossplat issues with terminating the thread. If there's additional reading material to answer my questions, feel free to refer me! In any case, this should hopefully be my last set of questions. :)

Thanks for your help so far.
",emgerner-msft,Lukasa
3099,2016-05-04 19:47:05,"@Lukasa 

Ah, what a mess, haha. I thought that might be the case but I was really hoping I was wrong. 

First, we desperately need a send timeout. I simply can't tell my users that their uploads can just hang infinitely and we don't have a plan to fix the problem. :/

It seems like I'm kind of in an impossible situation at this point. There's no library support for total timeout (which I do understand). There's no guarantees on exactly how the existing timeout works with various chunk sizes -- if there was, I could just sum up the time: connect timeout + read timeout \* chunk size. Being able to interrupt flow with stream mode and generators is nice, but since I don't have a solution to actually abort the threads in a cross platform manner this doesn't help either. Do you see other options to move forward? What are other users doing to solve these issues?
",emgerner-msft,Lukasa
3098,2016-10-24 03:11:21,"Content-Disposition is defined in RFC2183. It states that ""Short"" parameters should be US-ASCII. Long Parameters should be encoded as per RFC-2184. I can't see where in RFC-2184 it says that UTF-8 encoding is valid. (But I may be missing that particular line)

@Lukasa knows much more about the relevant RFCs than I do though.
",TetraEtc,Lukasa
3096,2016-04-15 05:07:40,"@piotrjurkiewicz the tests fail on 3.3 and 3.4 now http://ci.kennethreitz.org/job/requests-pr/974/

This reminds me, I need to add Python 3.5 to the CI server. I didn't realize it was missing. 
",kennethreitz,piotrjurkiewicz
3096,2016-04-18 19:18:14,"@kennethreitz the tests here seem to have hung in Jenkins. Is there anyway to add a timeout to test runs?
",sigmavirus24,kennethreitz
3096,2016-04-18 21:34:24,"@sigmavirus24 just added a build timeout plugin, configured for 5 minutes for PR builds. 

Don't worry, I'm getting increasingly frustrated with maintaining Jenkins at the moment. 
",kennethreitz,sigmavirus24
3096,2016-06-07 08:49:48,"The reason the header order is being overridden in your case is because of the way requests merges the two different dictionaries in the `Session` and the request kwargs.

By default, a requests `Session` already contains several keys:



You'll note, then, that when you send headers using the `headers` kwarg, the order of the keys in the Session is preserved in priority to the order of the keys in the `headers` kwarg. This is the expected result of using the dict on the `Session` as the base into which the request dict is merged to update.

Trying to get the entirely intuitive behaviour (where the request header defines the order in preference to the `Session` order) is somewhat frustrating. Right now the code looks like this:



We'd need to change the code to



This would lead to the exact same result as we currently have but would prioritise the _order_ of the request dict rather than the `Session` dict. The cost is that we do substantially extra computation in order to achieve this relatively minor effect.

I am open to making this change, but it does rather feel like using a sledgehammer to crack a nut. @kennethreitz @sigmavirus24?
",Lukasa,kennethreitz
3096,2016-06-07 08:49:48,"The reason the header order is being overridden in your case is because of the way requests merges the two different dictionaries in the `Session` and the request kwargs.

By default, a requests `Session` already contains several keys:



You'll note, then, that when you send headers using the `headers` kwarg, the order of the keys in the Session is preserved in priority to the order of the keys in the `headers` kwarg. This is the expected result of using the dict on the `Session` as the base into which the request dict is merged to update.

Trying to get the entirely intuitive behaviour (where the request header defines the order in preference to the `Session` order) is somewhat frustrating. Right now the code looks like this:



We'd need to change the code to



This would lead to the exact same result as we currently have but would prioritise the _order_ of the request dict rather than the `Session` dict. The cost is that we do substantially extra computation in order to achieve this relatively minor effect.

I am open to making this change, but it does rather feel like using a sledgehammer to crack a nut. @kennethreitz @sigmavirus24?
",Lukasa,sigmavirus24
3096,2016-06-07 22:51:46,"@Lukasa I think this is something we need to document instead of work around. I'm not sure ""fixing"" that particular behaviour wouldn't introduce some other subtle bug.
",sigmavirus24,Lukasa
3096,2016-06-08 06:09:54,"I don't disagree with @sigmavirus24. While I do feel like this pattern should ""just work"" as requested, I feel like it's extremely uncommon for someone to want/need this, and we should not bend over backwards to accomplish this. 
",kennethreitz,sigmavirus24
3095,2016-06-21 17:08:36,"@Lukasa How did you do the quick check with openssl?  I've having the same problem, and suspect I may have the same cause.
",Singletoned,Lukasa
3093,2016-04-15 21:38:22,"@Nuruddinjr please answer @Lukasa soon. Otherwise, I'm going to close this issue as it does not appear to be a bug.
",sigmavirus24,Lukasa
3091,2016-04-13 18:20:09,"This looks great to me. @bodgit could you add some tests to ensure this doesn't regress?
",sigmavirus24,bodgit
3091,2016-04-13 20:39:37,"To be clear, my code review is +1 on this too. @sigmavirus24 has the best testing approach here, I think we just need to mock it out.
",Lukasa,sigmavirus24
3091,2016-04-15 12:50:12,"Thanks @bodgit! :tada: 
",sigmavirus24,bodgit
3090,2016-04-13 14:37:35,"Great spot, this is definitely a bug!

The bug is in `HTTPAdapter.close`: this currently clears the basic `PoolManager`, but doesn't clear any instantiated `ProxyManager` objects. That means that they inadvertently get preserved, which makes using them for this use-case untenable.

This bug is easily fixed, though: clearing out `HTTPAdapter.proxy_manager` is going to be the way to go.

In the meantime @bodgit, to work around this problem you can not just close the Session but actively mount new `HTTPAdapters`:



This will hopefully all become needless in a future version of requests which will include TLS information in the connection pooling, but we're not there yet.
",Lukasa,bodgit
3090,2016-04-15 13:36:43,"Closed this as #3091 has been merged. Thanks @Lukasa for the workaround and the explanation of the bug.
",bodgit,Lukasa
3089,2016-04-14 01:19:28,"@Lukasa 
But i can't use `cookielib.CookieJar.add_cookie_header` to add a raw cookie to requests
can you give me a example?
thanks!
",liuyang007,Lukasa
3089,2016-04-14 08:54:00,"@Lukasa 
Yes, I copy a cookie from browser want to attach it to a request.
How to do this with `cookielib.CookieJar.add_cookie_header`?
",liuyang007,Lukasa
3089,2016-04-15 01:12:44,"@Lukasa 
Thanks.
 but I think use `Cookie.SimpleCookie` is too complex,use this pull request to very easy to achive.
",liuyang007,Lukasa
3089,2016-04-15 02:40:35,"@sigmavirus24 
Ok.thank you
",liuyang007,sigmavirus24
3089,2016-05-21 17:07:27,"if found a simple way to  achieve my target.



it work well.
Thanks all of you.
@kennethreitz 
@Lukasa 
@sigmavirus24 
",liuyang007,Lukasa
3089,2016-05-21 17:07:27,"if found a simple way to  achieve my target.



it work well.
Thanks all of you.
@kennethreitz 
@Lukasa 
@sigmavirus24 
",liuyang007,sigmavirus24
3085,2016-04-11 07:13:31,"There is no builtin method to make requests do this, nor will there be: servers that don't understand the format requests uses are old an non-standards-compliant. However, as @TetraEtc points out, you can use the [PreparedRequest flow](http://docs.python-requests.org/en/master/user/advanced/#prepared-requests) to mutate the body as you wish to, which would allow you to change the multipart-encoded body to whatever form you like.

You can also manually set the filename yourself by using longer tuples in the [files parameter](http://docs.python-requests.org/en/master/api/#requests.request): in particular, if you use a _bytestring_ in the filename portion then requests will leave it alone.
",Lukasa,TetraEtc
3085,2016-04-11 09:51:22,"@TetraEtc thanks, I have succeed using  urllib2 to send the data which encoded manually, although it is a boring work. I will also try the PreparedRequest method.

@Lukasa  do you mean use  something like `""测试中文视频.mp4"".encode('utf-8')` in the filename portion?
 when I try this, because of `result.encode('ascii')` (at requests/packages/urllib3/fields line38) fail, the post content is also  changed to something like this `filename*='%E6%B5%8B%E8%AF%95%E4%B8%AD%E6%96%87%E...`, which is not I want

Thank you.
",imnisen,TetraEtc
3085,2016-04-11 09:51:22,"@TetraEtc thanks, I have succeed using  urllib2 to send the data which encoded manually, although it is a boring work. I will also try the PreparedRequest method.

@Lukasa  do you mean use  something like `""测试中文视频.mp4"".encode('utf-8')` in the filename portion?
 when I try this, because of `result.encode('ascii')` (at requests/packages/urllib3/fields line38) fail, the post content is also  changed to something like this `filename*='%E6%B5%8B%E8%AF%95%E4%B8%AD%E6%96%87%E...`, which is not I want

Thank you.
",imnisen,Lukasa
3082,2016-04-11 07:21:23,"Ok @kennethreitz, you can hit the big green merge button whenever you're ready.
",Lukasa,kennethreitz
3077,2016-04-06 17:31:11,"Thanks for the response, @Lukasa.

Fair point regarding equivalency and the server misbehaving. However, one doesn't always have control over the misbehaving server, so sometimes you have to have a way to send what needs to be sent, even if it's technically wrong. :-)

Thanks for the info on how to get around this with a PreparedRequest!

Out of curiosity, I understand a stance of not percent-encoding things that don't typically need it, but why percent-**de**code things (specifically the period in this case)? Does it benefit other parts of the codebase by normalizing encoded and decoded periods when performing operations?

Also, regarding Requests' stance on URL forms and encodings, is there documentation regarding what gets encoded/decoded where? For example, I noticed passing query string params as a dict results in encoding, but passing as bytes doesn't.

I'm mainly asking because I'm writing a tool that walks through a series of requests (to and from servers I don't have any control over ;-) ), and part of the challenge is to ensure certain pieces of data pass through the process in their intended form. If I pull a URL out of a Location header, for example, do I need to do any manually encoding on certain characters before I pass it to Requests (like plus signs)?

Thank you for your time on this! I'm a bit of a Python noob (maybe I've graduated to noob+ at this point ;-) ), and I'm even more of a Requests noob, so I appreciate you taking the time to field my feedback.
",correcthorsebatterystaple-,Lukasa
3076,2016-04-05 14:05:58,"@Lukasa The result is the same for `resp.text` and `resp.content`
",hachterberg,Lukasa
3075,2016-04-05 18:11:36,"Hi @Lukasa, thanks for the quick response!

Is the urllib3 issue specific to https? Because it seems to work with mixed case protocol when it's http, and only has issues with https.

Regards,
-Justin
",correcthorsebatterystaple-,Lukasa
3075,2016-04-05 21:15:03,"@Lukasa Gotcha. Luckily, this is relatively easy to workaround in my case. Though, I wonder what would happen if Requests were configured to auto-follow redirects (while using proxies) and it encountered an HTTPS URL in the Location header (which is what would happen in my case, except I'm manually following each redirect, so I can check for the case and adjust it).

Anyway, thanks again for your help. :-)
",correcthorsebatterystaple-,Lukasa
3075,2016-04-05 21:41:04,"@Lukasa Is there a way to workaround the issue in a custom HTTPAdapter subclass? I tried fixing the scheme in the add_headers method, but it looks like that doesn't catch the issue early enough in the flow (I haven't dug through the code enough yet to know exactly what's happening when).
",correcthorsebatterystaple-,Lukasa
3075,2016-04-05 22:02:38,"@Lukasa Nvm, I overrode the get_connection method in my own HTTPAdapter child class to make any https schemes lower case before calling the parent class's get_connection method. :-)
",correcthorsebatterystaple-,Lukasa
3072,2016-03-31 19:17:34,"@Lukasa I feel dumb! Turns out my `username` and `password` were incorrect, and thus I was not authenticated, and thus no cookie. My bad. Thanks for the super quick reply though, Lukasa!
",jackyliang,Lukasa
3070,2016-03-29 20:44:27,"@sigmavirus24 I don't know the argument pro/con sessions having a timeout, but I'm happy to defer to other's judgement and experience on that, keeping these waters clean, like you say. 

I'm just happy that I haven't gotten a swift education about timeouts.
",mlissner,sigmavirus24
3070,2016-05-13 10:28:10,"@kuraga No. Per @sigmavirus24, and in many many previous discussions:

> I think I could actually still implement it using hooks - create a method which uses the prepared request workflow, and in the hook, just call it again. What would be your suggested solution?
",Lukasa,sigmavirus24
3070,2016-05-13 10:39:02,"@Lukasa ok, but which discussion did you cite? Was it private? Which previous discussions?
",kuraga,Lukasa
3070,2016-05-13 16:44:49,"@sigmavirus24 I added a caveat to [my initial comment, above](https://github.com/kennethreitz/requests/issues/3070#issue-144324837). Hopefully that should get this discussion focused.
",mlissner,sigmavirus24
3070,2016-11-13 22:04:40,"> It's extremely unclear to me what ""wrong"" means here. They aren't wrong: they work, as designed.

I think the point that's generally acknowledged by this bug is that the design _was_ wrong. There's a general consensus here that adding a default timeout or requiring it as an argument is a good idea. The  docs could address that, and that seems like a simple step to take until this issue is resolved in the next major version.

What happens in practice is that people grab the examples from the docs, don't read the timeout section carefully (or don't understand its implications -- I didn't for years until I got bit), and then wind up with programs that can hang forever. I completely agree with @chris-martin that until this issue is fixed, all examples in the docs should provide the timeout argument. Otherwise, we're providing examples that can (and probably will) break your programs.
",mlissner,chris-martin
3070,2016-11-14 17:50:19,"That's fair enough, @Lukasa. What about making the Timeout section more explicit then? Right now it says:

> You can tell Requests to stop waiting for a response after a given number of seconds with the timeout parameter.

And then goes on with a long, somewhat complicated warning. Could the first part say:

> You can tell Requests to stop waiting for a response after a given number of seconds with the timeout parameter. Nearly all production code should use this parameter in nearly all requests. Failure to do so can cause your program to hang indefinitely:

Something like that? 
",mlissner,Lukasa
3069,2016-03-29 11:31:53,"\o/ This looks great, thanks @achermes! :sparkles: :cake: :sparkles:
",Lukasa,achermes
3068,2016-03-28 15:32:16,"Since this is strictly not something we can fix in requests (or urllib3) I think we should close this. Thoughts @Lukasa ?
",sigmavirus24,Lukasa
3066,2016-05-06 16:47:54,"@Lukasa has anyone started work on this? I have some free time today, I was thinking I'd take a look at it. If I come up with something I can put together a PR.
",davidsoncasey,Lukasa
3066,2016-05-06 17:40:53,"@Lukasa great, I'm working now on writing a test to replicate the issue. Once I've got that, I may ask you to take a look to verify that I'm correctly capturing the issue, and then I'll proceed with a fix.
",davidsoncasey,Lukasa
3066,2016-05-06 21:54:19,"@Lukasa I just made PR #3181, with the solution to not call `prepare_content_length` if the `Transfer-Encoding` header is present. I went back and forth on whether to do the check there or to short-circuit out of `prepare_content_length` and I'm still not sure which I like better, so if you take a look and think it would be better to do within `prepare_content_length` then by all means.

I was thinking it may be more clear to refactor `prepare_body` slightly to not set the `Content-Length`  header at all in `prepare_body` (line 442 of models.py), and to call `prepare_content_length` regardless of if the data is a stream. I think then it could be a bit more explicit that the two headers should never coexist in the same request. Let me know if you have thoughts.
",davidsoncasey,Lukasa
3066,2016-11-03 20:52:54,"I had a brief chat about this issue with @Lukasa this morning, and wanted to update the status. Due to some recent improvements to `super_len` and `prepare_content_length`, it seems we've inadvertently solved @julian-r's original issue. You'll find the code below (a minor variant of the Stackoverflow question) now works.



~~Note I use the word _works_ loosely. Due to an oversight I made, Requests is currently setting all streamable bodies with a length of 0 to 'Transfer-Encoding: chunked'. This doesn't cause any severely negative behaviour but is likely better if we used 'Content-Length' instead. We can either integrate that into the work in #3338, or I'll open a separate issue/PR for it.~~

---

@davidsoncasey, you've done a lot of great work in #3184 and #3338 that would be beneficial for Requests. Would you be willing to reframe a few parts of it around these recent changes?
1. I think it would be helpful to move your three non-exception tests from #3338 into the master branch. This would show that the functionality is currently working and will prevent us from regressing it moving forward.
2. We can then integrate your simplification of `prepare_body` and `prepare_content_length`, along with your exceptions, with the current changes on master. That would be an excellent addition in 3.0.0.

How does that sound?
",nateprewitt,Lukasa
3065,2016-03-24 19:40:59,"@sigmavirus24 Any luck with this?
",james-hoegerl,sigmavirus24
3065,2016-03-25 14:16:58,"Yah was just about to do that. Thanks for your help @sigmavirus24 @Lukasa. Sorry this ended up being so dumb. Heading to lambda forums. 
",james-hoegerl,Lukasa
3065,2016-03-25 14:16:58,"Yah was just about to do that. Thanks for your help @sigmavirus24 @Lukasa. Sorry this ended up being so dumb. Heading to lambda forums. 
",james-hoegerl,sigmavirus24
3063,2016-03-23 12:36:20,"@asieira This is an idea we've been interested in for a long time: see shazow/urllib3#607.

Unfortunately, any support we're likely to have will be of minimal use in practice unless deliberate effort is taken to harden your application. We'd need a cache in non-ephemeral storage (or the exit of your app cleans the in-memory cache), and PKP headers are extremely uncommon.

Regardless, the issue to track is the one linked above.
",Lukasa,asieira
3061,2016-03-20 16:33:30,"@Lukasa It contains the service-specified header X-ProxyMesh-IP which could be used to control request-rate. I could manage a proxies-count to do deeper requests-handling.

I have searched for a long time, found nothing could help me to figure it out. Could you offer me some advices on getting tunnel CONNECT responses headers. It is so significant to my application.
",jkryanchou,Lukasa
3061,2016-03-20 16:42:31,"@Lukasa https://bugs.python.org/issue24964 Here is a patch to python httplib which is same to mine. However it was python 3 patch not python 2.7. On the other hand, I think the rotating proxy service design was not easy handful. Oops...  I have no idea how could I works with the proxy in a convenient way. 
",jkryanchou,Lukasa
3061,2016-03-21 12:32:14,"@Lukasa Awesome. Thanks for your suggestion. I'm so sorry for making mistake the purpose of ssl.wrap_socket() method. I will try to do some monkey-patch hacking based on your reply. 
",jkryanchou,Lukasa
3060,2016-03-18 10:01:21,"So, I'm totally happy with this code change: I think it's a reasonable refactoring that helps improve some clarity, _as well_ as allow @benweatherman to perform the override you want.

I'm also delighted to see some extra tests added for this. I think this change is wonderful! Pinging @kennethreitz or @sigmavirus24 for an extra round of review before merging.
",Lukasa,benweatherman
3057,2016-03-17 15:19:57,"Thanks @kevinburke! :sparkles: :cake: :sparkles:
",Lukasa,kevinburke
3056,2016-03-16 19:13:39,"@kennethreitz I actually was going to rename complexjson to json but there are json variable references all over the models.py file and decided it was probably best not to refactor it. If you still think it's a good idea I'm game to change it.
",digitaldavenyc,kennethreitz
3053,2016-03-16 15:16:23,"@Lukasa That's a good point. There is actually only file that is importing json, [models.py](https://github.com/kennethreitz/requests/blob/master/requests/models.py#L34) So the diff would be about the same. Updated the PR
",digitaldavenyc,Lukasa
3052,2016-03-15 22:06:22,"@kennethreitz Exactly. You just mentioned that not many people use Python 2.6 anymore. Is it still a nice to have feature that's worth including?
",digitaldavenyc,kennethreitz
3052,2016-03-15 22:25:58,"I agree. A few options here:
1. Keeping things as they are (always preferred)
2. Removing simplejson logic completely (I think this would be fine)
3. Limiting simplejson logic to 2.6 only (unideal, but would limit potential side-effects)

I like **1** the best, with a ""if it ain't broke, don't fix it!"" mentality, very loosely held. I know **2** is what @sigmavirus24 and @Lukasa prefer, and if it's worth the (minimal) effort, I'm not against it if they're for it. 
",kennethreitz,Lukasa
3052,2016-03-15 22:25:58,"I agree. A few options here:
1. Keeping things as they are (always preferred)
2. Removing simplejson logic completely (I think this would be fine)
3. Limiting simplejson logic to 2.6 only (unideal, but would limit potential side-effects)

I like **1** the best, with a ""if it ain't broke, don't fix it!"" mentality, very loosely held. I know **2** is what @sigmavirus24 and @Lukasa prefer, and if it's worth the (minimal) effort, I'm not against it if they're for it. 
",kennethreitz,sigmavirus24
3050,2016-03-13 12:49:24,"@alexanderad The `ProxyError` you see in that traceback is actually a urllib3 `ProxyError` class. If we wanted to try to abstract the `ProxyError` out we could in principle do that, and it looks like the same change that we had to work around for the timeout error we also have to workaround for the `ProxyError`. 

A pull request to fix this should be fairly simple: do you want to tackle it @alexanderad?
",Lukasa,alexanderad
3050,2016-03-14 07:48:47,"@Lukasa I think I can take a look and provide a patch, thanks for the confirmation. I see some `ProxyError` handling on lines https://github.com/kennethreitz/requests/blob/master/requests/adapters.py#L442-L443, need to take closer look what _that_ `ProxyError` refers to in it's original purpose and why we don't see it in this case (perhaps it comes from system-wide configured proxy?)
",alexanderad,Lukasa
3050,2016-03-14 08:29:54,"@alexanderad So you can work out why we don't see it in this case by looking at the exception itself, which is a nested collection of ever-lower-level exceptions, which you can see more clearly here



So the specific problem is that this _kind_ of `ProxyError` now causes a `MaxRetryError`, where previously it did not. So we just need some error handling code in the `MaxRetryError` branch to look for a `ProxyError` in the `reason`, and reraise appropriately.

Given that `ProxyError` is a subclass of `ConnectionError`, this change should be non-breaking.
",Lukasa,alexanderad
3049,2016-03-13 11:23:35,"This also seems reasonable to me, but @Stranger6667 is sat right next to me. ;) @sigmavirus24/@kennethreitz, mind doing an extra review for me? This is +1 from me.
",Lukasa,Stranger6667
3049,2016-03-13 11:37:35,"Hello folks seems somehow I got placed on this email alias By mistake.
Anyway I can be removed.

Thank you
-Ryan
On Sun, Mar 13, 2016 at 7:24 AM Cory Benfield notifications@github.com
wrote:

> This also seems reasonable to me, but @Stranger6667
> https://github.com/Stranger6667 is sat right next to me. ;)
> @sigmavirus24/@kennethreitz https://github.com/kennethreitz, mind doing
> an extra review for me? This is +1 from me.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/pull/3049#issuecomment-195938314
> .
",ryandebruyn,Stranger6667
3049,2016-03-15 13:46:15,"Hello, @sigmavirus24 @Lukasa !
I've updated this PR :)
",Stranger6667,Lukasa
3049,2016-03-15 13:46:15,"Hello, @sigmavirus24 @Lukasa !
I've updated this PR :)
",Stranger6667,sigmavirus24
3049,2016-04-06 19:04:11,"@Stranger6667 I don't like having tons of pull requests open, so I'm merging this. If you wouldn't mind, it would be appreciated if you opened another PR that addresses the notes @Lukasa left on your test. 
",kennethreitz,Stranger6667
3049,2016-04-06 19:04:11,"@Stranger6667 I don't like having tons of pull requests open, so I'm merging this. If you wouldn't mind, it would be appreciated if you opened another PR that addresses the notes @Lukasa left on your test. 
",kennethreitz,Lukasa
3048,2016-03-13 11:13:15,"@sigmavirus24 @kennethreitz Can one of you two do a separate code review of this? @Stranger6667 is here with me at PyCon SK writing some of these tests. I'm happy with these, but it'd be good if one of you two gave the ok/not-ok.
",Lukasa,Stranger6667
3048,2016-03-15 13:51:17,"Hello @Lukasa @sigmavirus24 !
I've updated this PR :)
",Stranger6667,Lukasa
3048,2016-03-15 14:10:28,"Awesome, we're very close now @Stranger6667. One small note. =)
",Lukasa,Stranger6667
3045,2016-03-10 22:04:48,"@Lukasa both Chrome and Safari according to their ""Conditions"" section.
",sigmavirus24,Lukasa
3043,2016-03-09 15:47:57,"@sigmavirus24 : In line 174 in `adapters.py` `if url.lower().startswith('https') and verify`  is there and then `if not verify` is there. Since `and` condition is there, `if not verify` won't be `True`. The other thing is more pythonic way of if/else i.e in line 200 
",tusharmakkar08,sigmavirus24
3043,2016-03-09 18:28:08,"@Lukasa : I have done couple of more changes and few of them are [anti-patterns](https://www.quantifiedcode.com/app/issue_class/62b0f5b7b69e4a2498568b32bfa30991) as suggested by http://docs.quantifiedcode.com/python-code-patterns/ . This isn't random refactoring, we are removing anti-patterns from code @sigmavirus24. 

Thanks. 
",tusharmakkar08,Lukasa
3043,2016-03-09 18:28:08,"@Lukasa : I have done couple of more changes and few of them are [anti-patterns](https://www.quantifiedcode.com/app/issue_class/62b0f5b7b69e4a2498568b32bfa30991) as suggested by http://docs.quantifiedcode.com/python-code-patterns/ . This isn't random refactoring, we are removing anti-patterns from code @sigmavirus24. 

Thanks. 
",tusharmakkar08,sigmavirus24
3043,2016-03-16 07:37:33,"@sigmavirus24 @Lukasa @kennethreitz: Since requests is being mentioned over [here](http://docs.python-guide.org/en/latest/writing/reading/) among the best python codes, I believe these kind of antipatterns shouldn't exist over here. 
",tusharmakkar08,kennethreitz
3043,2016-03-16 07:37:33,"@sigmavirus24 @Lukasa @kennethreitz: Since requests is being mentioned over [here](http://docs.python-guide.org/en/latest/writing/reading/) among the best python codes, I believe these kind of antipatterns shouldn't exist over here. 
",tusharmakkar08,Lukasa
3043,2016-03-16 07:37:33,"@sigmavirus24 @Lukasa @kennethreitz: Since requests is being mentioned over [here](http://docs.python-guide.org/en/latest/writing/reading/) among the best python codes, I believe these kind of antipatterns shouldn't exist over here. 
",tusharmakkar08,sigmavirus24
3038,2016-03-08 10:03:18,"@kennethreitz Nope, that got broken a _long_ time ago for headers. Back in 2012 it looks like.
",Lukasa,kennethreitz
3038,2016-03-08 10:04:39,"@kennethreitz We never had a test that enforces that invariant, which means that commit 366e8e849877aea44ce96abebd4f26f5bcce12fb didn't spot that we broke it in the rewrite for v2.0.
",Lukasa,kennethreitz
3038,2016-03-08 10:10:56,"@kennethreitz We could always replace the `CaseInsensitiveDict` used in this place with the [`HTTPHeaderMap`](https://github.com/Lukasa/hyper/blob/development/hyper/common/headers.py) from hyper.
",Lukasa,kennethreitz
3038,2016-04-14 04:24:46,"@Lukasa if @piotrjurkiewicz's patch has no unintended side-effects, I'm +1 for incorporating this. 
",kennethreitz,piotrjurkiewicz
3038,2016-04-14 04:24:46,"@Lukasa if @piotrjurkiewicz's patch has no unintended side-effects, I'm +1 for incorporating this. 
",kennethreitz,Lukasa
3038,2016-04-14 07:34:38,"I'm fine with it as well, would you like to raise a PR @piotrjurkiewicz?
",Lukasa,piotrjurkiewicz
3036,2016-03-11 00:31:23,"@Lukasa can you add release notes for this too?
",sigmavirus24,Lukasa
3036,2016-03-11 09:58:09,"@sigmavirus24 Done. =)
",Lukasa,sigmavirus24
3036,2016-03-11 16:15:13,"@kennethreitz Each time we change something from guessing a content length to just saying ""chunked is fine for this"", I feel like we're striking a victory for the way the web _should_ have been, rather than the status quo. 
",Lukasa,kennethreitz
3036,2016-03-11 16:43:11,"@Lukasa Thanks for the fix! 

Btw, I think it would useful if there was a warning in the docs about that resulting TCP inefficiency of simply passing a file object you mention above. The current example is simply `data=f`.
",jakubroztocil,Lukasa
3034,2016-03-09 21:31:41,"@erydo expressed on the original issue that they were going hands off. That implies to me that they're done working on this if we're not going to accept their changes. If I'm wrong, I'll be happy to reopen this if they wish to follow our recommendations for the appropriate fix. Otherwise, this will sit open for a while and I'd rather that not happen.
",sigmavirus24,erydo
3032,2016-03-05 17:08:57,"@Lukasa great, thanks for taking a look! I'll have a chance later today to make those edits.
",davidsoncasey,Lukasa
3032,2016-03-06 04:40:20,"@Lukasa I added a couple fixes, take a look and let me know if there's anything else!
",davidsoncasey,Lukasa
3029,2016-03-03 22:30:53,"@sigmavirus24 I understand. I figured it might be worth pointing out. 

> It takes a great deal of freedom away from the user which we have intentionally placed into their hands.

As long as this is an intentional choice.

> This code does not just change this for the multipart/form-data case.

You are right. I missed that, i will at-least make it specific to the files case, even if we end up not merging this.

The silent nature of the problem can cause a lot of headaches. I don't think a majority of the users using `requests` are going for the over-ridden behavior or API Fuzzers.
",vaibhavkaul,sigmavirus24
3029,2016-03-04 16:25:18,"@sigmavirus24 I made the change so it doesn't affect other things besides files.

> Sometimes overriding requests default behaviour is correct for user

Yes, which which is why `sometimes` should not dictate default behavior.

> Will make the correct body with an incorrect header. 

I dont think thats true since for the body to be parseable by any standard multi-part library (cgi etc) it would need to have the correct header. Just having the data in the body does not make it correct. I would argue the body is wrong in this case for a multipart request.

My motive right now is not to get this merged in at all. Happy to close the PR. But we should be clear about what the issue is here. Specifically I would like to address things like:

> That means that one of the cases above prevents the user from doing exactly what they intend to do

and 

> then changing the behaviour to differently silently subvert expectations isn't a great direction

The specific part of code I am referring to deals [exclusively with Multipart Files](https://github.com/kennethreitz/requests/blob/master/requests/models.py#L445-L447). The whole [idea of adding a `file=` param](http://docs.python-requests.org/en/master/user/quickstart/#post-a-multipart-encoded-file) is to encode the request as a multi-part request. `Most` developers using that code path clearly want the request to be posted as multipart.

Multipart requests must [have a boundary specified in the content-type](https://www.w3.org/Protocols/rfc1341/7_2_Multipart.html), if this is missing, the whole purpose for passing a `files=` param is defeated. This is no longer a multipart request. This is not a `correct body with the wrong header`. Clearly the `requests` framework [does not allow a user to pass their own boundary](https://github.com/kennethreitz/requests/blob/master/requests/models.py#L156) (notice the missing boundary kwarg) and use that. It `always` relies on a auto generated boundary string. We should either give people the ability to be in control of the `content-type` (by letting them specify a boundary string) or do the right think in the library.

> In general the requests header dictionary lets people do stupid things. If the user wants to change content-length, they can. If they want to change transfer-encoding, they can

I agree. Devs do stupid things. But then why make [this](https://github.com/kennethreitz/requests/blob/master/requests/models.py#L434-L438) raise an exception. There should at least be a warning somewhere about the request not being encoded as a multi part request if the headers being passed include a `content-type`, even if that content type is `multipart/form-data`.

I completely agree that we should not make this change if you feel this breaks the existing API, so maybe this should be considered for a future release or maybe not. I'll leave it up to @kennethreitz.
",vaibhavkaul,sigmavirus24
3029,2016-03-04 17:16:49,"> FWIW, do you guys think a better solution would be to allow the user to pass a boundary string and use that instead?

You already can with the toolbelt's [`MultipartEncoder`](https://toolbelt.readthedocs.org/en/latest/uploading-data.html#streaming-multipart-data-encoder) and since 99% of our users don't need that in requests itself, it makes perfect sense to have something that lives elsewhere to do that.

> But then why make [this](https://github.com/kennethreitz/requests/blob/46184236dc177fb68c7863445609149d0ac243ea/requests/models.py#L434-L438) raise an exception.

Because one is exceptional. We could issue a warning when overriding the content-type header when a user specifies both `data=` and `files=` but I think that will only make a lot of people very angry with us.

> The specific part of code I am referring to deals exclusively with Multipart Files.

Right you're special casing **silent** behaviour for one very narrow case of something that should either be the case for everything or not. APIs cease being for humans when you have to say to yourself ""Wait, will requests overwrite this header for me if I'm doing this or is it only in this other case?"" every time they go back to do something. That is not a human API, that is an awful API design.

> > Sometimes overriding requests default behaviour is correct for user
> 
> Yes, which which is why sometimes should not dictate default behavior.

The default behaviour of requests is not wrong. The default behaviour is to trust the user that they know what they're doing.

> Most developers using that code path clearly want the request to be posted as multipart.

And they will get a body that is correct for a `multipart/form-data` request. If they're intentionally changing the header and the server they're talking to does not understand the header, they'll figure that out. People using requests are generally rather intelligent. Most have read the documentation and understand what they need to do when creating a valid `multipart/form-data` request. It is often the people who do not read the documentation that run into this, go and read the documentation, and fix it themselves.

> I completely agree that we should not make this change if you feel this breaks the existing API.

It's not a matter of _feeling_ that this is backwards incompatible. It is backwards incompatible.

And let me just strongly reinforce a point @Lukasa and I seem to be unable to convey appropriately:

This is taking documented and expected behaviour that you consider to be _silently_ broken and replacing it with behaviour that will be undocumented, unexpected, and will _silently_ break users' code. Your justification is to fix _silent_ behaviour but you're replacing it with inconsistent _silent_ behaviour.
",sigmavirus24,Lukasa
3029,2016-03-04 17:46:33,"@sigmavirus24 

> It's not a matter of feeling that this is backwards incompatible. It is backwards incompatible.

Yes, that was a bad choice of words. My Bad.

> People using requests are generally rather intelligent. Most have read the documentation and understand what they need to do when creating a valid multipart/form-data request. 

Thats great!

> It is often the people who do not read the documentation that run into this, go and read the documentation, and fix it themselves.

Can you point to to where in the documentation I would have seen the `content-type` behavior. I was able to find it because I looked at the code.
",vaibhavkaul,sigmavirus24
3029,2016-03-04 19:56:09,"FWIW, this becomes a much bigger problem when you are trying to Proxy requests and want to maintain most of the original headers. 

Having a list of headers that would potentially cause issues would be great for documentation.

@Lukasa big :+1: 
",vaibhavkaul,Lukasa
3029,2016-03-04 19:57:34,"@Lukasa likely shouldn't provide _but are certainly able to, if needed_. This is a feature, not a bug. 
",kennethreitz,Lukasa
3027,2016-03-02 21:55:53,"Thanks for the tip @Lukasa. I'm a newbie when it comes to Python web serving. Been struggling getting Apache WSGI module + Flask on OS X el capitan all day. First it was SIP (system integrity protection) and now it's getting Apache/OS X python to recognize my anaconda python modules. This whole process is extremely unpleasant. Do you happen to know what ""proper WSGI server"" folks tend to use?
",rajrsingh,Lukasa
3025,2016-02-27 05:49:53,"1. If you look at my script (the one @kennethreitz  posted), that's what I did. 
   proxies = {'https': 'https:131.96.228.236:9064',
          'https': 'https:54.85.61.208:80',
          'https': 'https:54.209.36.19:80',}
2. The proxies I tried to use are HTTPS
3. If you look at the cookies for http://epiccodes.com, you'll see your IP in the cookies, I was expecting to see the proxy's IP in the cookie when I use proxies=proxies in the get request, instead of my actual IP.
4. My bad, I didn't know. Will do in the future if I have any questions.
",Tosible,kennethreitz
3025,2016-02-27 05:52:27,"> If you look at my script (the one @kennethreitz posted), that's what I did. 

It isn't. Please look closely at what you wrote and what I wrote. Further, which proxy do you expect to be used? Creating a dictionary with a repeated key is undefined behaviour (as to which value is kept) in Python.
",sigmavirus24,kennethreitz
3023,2016-02-18 19:19:15,"I'm with @kennethreitz this isn't necessary. Because of the line @kennethreitz is citing, users can already just do



Both work perfectly fine. Beyond that, we don't need a gitignore line for PyCharm/idea projects because that should already be in your [global gitignore](https://help.github.com/articles/ignoring-files/#create-a-global-gitignore).
",sigmavirus24,kennethreitz
3023,2016-02-18 19:50:45,"@kennethreitz Thanks, I was wondering about that line. Two questions:
1. That import makes them available as `requests.RequestException`. It's a subtle difference, but [this doc page](http://docs.python-requests.org/en/master/api/#exceptions) instructs users to include them as `requests.exceptions.RequestException`. Which one is the preferred or supported method?
   - If we want to include both, we could specify both in `__init__.py`.
   - If `requests.RequestException` is preferred (which I like), perhaps we should update the docs.
2. It looks like `ConnectTimeout` is missing from the list. Should it be added?
",jtpereyda,kennethreitz
3023,2016-02-18 19:55:03,"@sigmavirus24 Thanks for the global gitignore tip! I hadn't heard about that.
",jtpereyda,sigmavirus24
3023,2016-02-21 01:27:46,"@kennethreitz Thanks!
",jtpereyda,kennethreitz
3014,2016-02-14 20:46:01,"@Lukasa hmmm, that works.
",kennethreitz,Lukasa
3014,2016-02-16 08:35:51,"@sigmavirus24 @Lukasa last call for comments. Will merge/release tomorrow. 
",kennethreitz,Lukasa
3014,2016-02-16 08:35:51,"@sigmavirus24 @Lukasa last call for comments. Will merge/release tomorrow. 
",kennethreitz,sigmavirus24
3014,2016-02-16 14:00:13,"Yeah. So, I agree with @Lukasa whole-heartedly. I don't think we should be vendoring libraries just for colorizing this output. This feels like a vast step outside the domain of this library - HTTP. That kind of behaviour belongs elsewhere. We also don't even remotely attempt to reflect whether or not the user is attempting to use proxies, which I think will be far more confusing for those users when they expect to see that output here.

To be clear:
- I think we're violating the law of constraints of limiting this library to HTTP
- I think we're doing our users a grave disservice by having this functionality here in such a naive implementation
- I think we're adding more (vendored) dependencies than we should ever rightfully need
",sigmavirus24,Lukasa
3014,2016-02-16 17:00:13,"@kennethreitz the proxy information is tricky. We have a hacky way of incorporating it into the information in the toolbelt, but I don't think this implementation would have the same ability to determine if a proxy is in use.
",sigmavirus24,kennethreitz
3014,2016-02-16 23:13:52,"@TetraEtc thanks for your thoughts!

I think the colors are enjoyable and make the information easier to parse. Why do you not care for them? Just preference?
",kennethreitz,TetraEtc
3014,2016-02-16 23:41:51,"@TetraEtc for that, there is `pretty(colors=False)`. Also, colors are automatically disabled if the code is not running in an interactive console. 
",kennethreitz,TetraEtc
3014,2016-02-25 21:59:07,"I don't actually think including the pretty formatting matters in any way, shape, or form, to be completely honest. Meaning, I see absolutely no reason not to. I think it's a nice feature, and I would be very pleasantly surprised to find that level of polish in my favorite HTTP library. 

That being said, I'm taking extra time to be considerate of @Lukasa and @sigmavirus24's objections (regardless of final result). 
",kennethreitz,Lukasa
3014,2016-02-25 21:59:07,"I don't actually think including the pretty formatting matters in any way, shape, or form, to be completely honest. Meaning, I see absolutely no reason not to. I think it's a nice feature, and I would be very pleasantly surprised to find that level of polish in my favorite HTTP library. 

That being said, I'm taking extra time to be considerate of @Lukasa and @sigmavirus24's objections (regardless of final result). 
",kennethreitz,sigmavirus24
3014,2016-02-25 22:44:26,"> I would be very pleasantly surprised to find that level of polish in my favorite HTTP library.

And I'd be very confused why an HTTP library is doing anything with colorized output. `¯\_(ツ)_/¯` I think we have different definitions of the word ""polish"" w/r/t what a library should do.

> I'm taking extra time to be considerate of @Lukasa and @sigmavirus24's objections (regardless of final result). 

Sounds like you're going to merge this regardless of our objections? If so, I'm not sure why you would take extra time.
",sigmavirus24,Lukasa
3014,2016-02-25 22:44:26,"> I would be very pleasantly surprised to find that level of polish in my favorite HTTP library.

And I'd be very confused why an HTTP library is doing anything with colorized output. `¯\_(ツ)_/¯` I think we have different definitions of the word ""polish"" w/r/t what a library should do.

> I'm taking extra time to be considerate of @Lukasa and @sigmavirus24's objections (regardless of final result). 

Sounds like you're going to merge this regardless of our objections? If so, I'm not sure why you would take extra time.
",sigmavirus24,sigmavirus24
3014,2016-02-25 23:38:24,"@sigmavirus24 if I was sure, I would have merged it a week ago :)
",kennethreitz,sigmavirus24
3014,2016-02-26 14:26:28,"So, while we're taking feedback, here's my 2¢:

The philosophy of this project since about v1.0, at least as I understand it, has been to aggressively resist scope creep. We have rejected innumerably more feature requests than we've accepted, saying ""no"" to almost all feature requests that failed to meet one of three criteria:
1. It's extremely difficult or impossible to implement the feature from outside the library.
2. It's a convenience feature that would be used by a substantial majority of our users.
3. There is some subtlety in ""correctness"" of the feature that many users would miss, and that we have an opportunity to get right for them.

As far as I can see this feature fits into none of those categories.

To the first point, it is not difficult to implement this feature from outside Requests: it is exactly as hard there as it is here. The objects being used here are public and are regularly exposed to users: there's nothing that users would ordinarily be unable to find or struggle to reach.

To the second point, this feature would not be used by a majority of our users. The colorizing feature in particular suffers here because it only works when printing directly to a terminal and having a user observe those responses. That is a vanishingly small use-case of requests compared to automated use, and while it's important that those users have a good experience, I don't see value in implementing something solely for them unless it meets criteria 1 or 3. The dumping feature, even ignoring colorizing, is also something that is unlikely to be used by a substantial majority of our users, particularly because it is thoroughly ill-suited to logfiles. This is for two reasons: first, the dump contains newlines which make parsing logfiles extremely unpleasant; and second, the dump is really quite verbose and contains a lot of extraneous information that will serve only to bloat log files.

To the third point, the only subtlety in correctness here is not dealt with by this code, it is _encouraged_ by this code, and that is the fact that the representation dumped here is not the representation as received from or sent to the network. It is _related_ to those representations, but in no way conforms to them. Users who do not grasp this subtlety are, in my opinion, likely to be confused by the representation used here. The example used in the original post is great for this:



In this example the `Host` header is missing entirely, and the request URI is not what is actually sent on the network (that would be `/ip`, unless a proxy is in use in which case it _is_ what would be sent on the network, unless we're making a HTTPS requests and then there's a whole separate web request that we aren't showing at all here). I should note also that the REQUESTS/2.9.1 block is not in the same place as the HTTP/1.1 block would be in a request, though I don't know if that's intentional or not. The `Response` suffers even more from this because the response headers have been thoroughly transformed by the time they get to requests and frequently look almost nothing like they did when they were received from the network.

All of that goes to say that I don't believe we'd accept this feature if it came from outside the project. I think it represents a maintenance burden for the future, because once we implement this we'll need to support it, and I don't believe that it pays for that burden in utility. And I haven't even begun to mention that to implement this feature we are adding two implicit dependencies that will now require updates in order to fix bugs that affect this rendering and that will create substantial work downstream of us to unbundle.

Generally speaking we have been fairly aggressive about getting features _out_ of this library that are not required. I think this represents a step in the other direction, and I don't believe that we gain enough by doing it to justify it.

Of course, @kennethreitz, this is still your project and so my opinion is only that. =) If you merge this, I'll help maintain it.
",Lukasa,kennethreitz
3014,2016-02-26 17:39:24,"@Lukasa failurel! The above post stated:

> No further :+1:s or :-1:s needed at this point, but random anecdotes are more than welcome!

My cat won't leave me alone today.
",kennethreitz,Lukasa
3013,2016-02-14 16:50:31,"@kennethreitz How ""near exact"" is ""near exact""?

Note, for example, that the `PreparedRequest` does not have a `Host:` header: this is because httplib attaches it for us. It is also missing some `Accept-Encoding` headers that urllib3 adds for us. The reality is that Requests does not have access to enough information to render this out unless we want to guess at what the rest of the stack will do, and if we do that then we'll start getting bugs raised when the output of `render` does not match the actual transmitted bytes.
",Lukasa,kennethreitz
3013,2016-02-14 17:08:19,"@kennethreitz I have no objection to rendering a representation of the preparedrequest object, but I suspect it's _extremely_ unwise to render in a structure that suggests completeness.

In particular, the structure given here (which renders out to a 'valid' HTTP/1.1 request/response) doesn't work in the following cases:
1. If using HTTP/2, when the request/response are structured entirely differently and made of binary frames.
2. If we send a file or send using chunked transfer-encoding, where we cannot render the body before we send it.
3. If we _receive_ data using chunked transfer-encoding, where we cannot accurately represent it because urllib3 has transparently removed the chunking.
4. If the request is sent via a proxy there's an extra CONNECT request we're simply not printing.

There's nothing wrong with wanting to have a printable representation of all the data on the `PreparedRequest`, but we should be very wary before we format that like a HTTP/1.1 request.
",Lukasa,kennethreitz
3011,2016-02-13 08:49:23,"That version of OpenSSL is _very_ old, which may be why you're having problems but @kennethreitz is not. Unfortunately, our PyOpenSSL-based workaround does not currently work on Python 3, though we're working on a fix for that. 

An alternative approach would be to install a newer OpenSSL from Homebrew and then get your Python 3 from Homebrew as well. That would likely resolve your problem. 
",Lukasa,kennethreitz
3009,2016-03-06 19:57:56,"Extra datapoint, I've been bitten by this too while interacting with the API of a Spanish-language server. The correct fixes here are, I believe:
1. Properly decode the HTTP Reason Phrase field as described by @denis-ryzhkov in [#1181](https://github.com/kennethreitz/requests/pull/1181#issuecomment-13423623) for header values.
   - According to [RFC2616](https://tools.ietf.org/html/rfc2616#section-6.1.1), Reason Phrase is `TEXT` which defaults to `latin1` and supports other characters under RFC2047.
2. Store the `.reason` attribute as unicode if it isn't already.
3. In `.raise_for_status` use `%r` as @Lukasa suggests (since exception messages are supposed to be bytestrings).
",erydo,Lukasa
3009,2016-03-06 20:04:57,"@erydo Changing the decoding of the reason phrase is beyond the scope of requests: that is handled entirely by httplib, and we can't change it. Generally speaking that works in a way that is acceptable: it misbehaves only in a few situations on Python 2.
",Lukasa,erydo
3009,2016-03-06 20:23:25,"@Lukasa I would expect that RFC obsolescence to apply more to developers of new servers rather than consumers of them.

Apache Tomcat, for example, sends down ~~unicode~~ `latin1` reason phrases for non-en locales, and that's not a rarely-encountered server.
",erydo,Lukasa
3009,2016-03-07 04:25:48,"> It conforms correctly to RFC2616

Please stop trying to justify the wrong behaviour by saying it conforms to an obsoleted specification. You're not convincing us of anything as we've been working towards conforming with the new set of HTTP/1.1 specifications as best we can while dealing with `httplib`.

I also don't understand why you're referring to sections in the RFC when the ABNF clearly states (as @Lukasa has already shown) that the reason phrase should be `HTAB`, `SP`, `VCHAR`, and `obs-text`.

> I believe RFC2047's recommendation ... is a bug in the specification.

Then please file errata against the RFC to address that. Please also note that 2047 is not the latest RFC on the topic.

---

Regardless, I have to agree with @Lukasa that the reason string should be treated as opaque bytes. I don't understand why you think your decision to run an internally patched would sway us, since you can decode the raw reason bytes before using the reason string.
",sigmavirus24,Lukasa
3007,2016-02-12 20:54:55,"@Lukasa is there a way to query presense of this API in runtime? Could you give me some links to read more about it?
",Kentzo,Lukasa
3007,2017-01-13 05:27:57,@Lukasa I think the original issue is orthogonal to #2966 and this issue should be re-opened because of the discovery made by @dsully.,Kentzo,Lukasa
3006,2016-02-12 04:37:22,"@sigmavirus24 It's just some bug in my own code without anything to do with networking.
I'm not sure whether this is the real cause or just coincidence.

But one thing is for sure, after some point, I'm completely unable to make request to https://www.telegram.org, which I can do right after install request.

Just FYI: #2906
",caizixian,sigmavirus24
3006,2016-02-12 05:22:29,"@sigmavirus24 So this there anything I can do to help you?
",caizixian,sigmavirus24
3006,2016-02-12 09:34:43,"@Lukasa I think since requests is shipped with its own urllib3, I could not import urllib3 alone. And the following result confirms it.


",caizixian,Lukasa
3006,2016-02-12 10:19:14,"@Lukasa 


",caizixian,Lukasa
3006,2016-02-20 12:54:52,"@Lukasa Any progress with urllib3?
",caizixian,Lukasa
3006,2016-02-21 00:26:22,"@Lukasa @shazow urllib3 always has a home at http://ci.kennethreitz.org, if desired!
",kennethreitz,Lukasa
3006,2016-12-02 18:37:47,"@Lukasa Thanks for your quick response. The output as you asked is as follows:

Also I am adding the response from OpenSSL 1.0.1f 6 Jan 2014
It appears to be working on this version.



Also I tried with earlier version of openssl and it seems to be working:

",ceprateek,Lukasa
3006,2017-01-09 20:06:25,@Lukasa Thanks for your support. I contacted the admin of the site to report what you say and try to find a better solution than using the workaround you suggest. ,rubendibattista,Lukasa
3005,2016-02-11 18:34:07,"Hey @scop I was wondering if you could provide detail about why this is desirable and why we should change the existing behaviour as well as a description of when/where monotonic will be available and where/when it won't.
",sigmavirus24,scop
3004,2016-03-02 13:43:49,"@Lukasa thanks. I wasn't sure if `-1` was the error code exactly. Is this error being caused from server end or from client machine? Any help or link that you can share which explains what this error is? Thanks
",grushler,Lukasa
3003,2016-02-10 18:35:44,"@Lukasa It looks like this could be accomplished using utils.dump by adding a `print_body=True` flag and wrapping the following lines in a conditional statement:

`line 78: bytearr.extend(prefix + _coerce_to_bytes(request.body))`

`line 106: bytearr.extend(response.content)`

This would allow a user to print Request & Response while not flooding their screen with data returned in the response. If it's preferred to keep this out of Requests, I can move the code over to the utils.dump code.
",gitpraetorianlabs,Lukasa
3002,2016-02-10 09:59:51,"So @shazow: the problem we have here is that the connection_from_url would not pass the port explicitly. That's avoided with the pool manager (which in `connection_from_host` defaults the port appropriately) and it's avoided in requests because it uses pool manager's `connection_from_url` which _also_ defaults the port.

I think this suggests a good fix.
",Lukasa,shazow
3002,2016-04-12 09:09:25,"Ok, thank you @Lukasa !
",fsat,Lukasa
3002,2016-04-13 00:10:07,"Thanks @kennethreitz!
",fsat,kennethreitz
2998,2016-02-10 13:52:30,"@sigmavirus24 That's the workaround I intend to use, although it's a little troublesome since I'm using a library which is itself using `requests`, and it doesn't expose `trust_env`, so I'm having to patch it in. It's also troublesome because it disables proxy configuration via the environment.

Thanks though, I didn't see the other issue!
",jcmcken,sigmavirus24
2996,2016-02-05 13:19:26,"Pinging @kennethreitz again for review on test code.

Thanks for this @Stranger6667!
",Lukasa,Stranger6667
2996,2016-02-05 20:34:51,"@Stranger6667 more tests for `utils` are more than welcome!
",kennethreitz,Stranger6667
2996,2016-02-05 20:58:24,"Thank you, @Lukasa @kennethreitz ! :)
",Stranger6667,kennethreitz
2996,2016-02-05 20:58:24,"Thank you, @Lukasa @kennethreitz ! :)
",Stranger6667,Lukasa
2993,2016-02-03 18:16:12,"@Lukasa Hey thanks. Help me with this: Is this ""https://github.com/shazow/urllib3"" the oficial urllib3 repo?
Thanks again!!
",angelcaido19,Lukasa
2992,2016-02-04 12:13:25,"@Lukasa @schlamar I agree I'd need to make a custom cache function. Do you think it is appropriate to make it a standalone package and vendor it under `packages`?
",lazywei,schlamar
2992,2016-02-04 12:13:25,"@Lukasa @schlamar I agree I'd need to make a custom cache function. Do you think it is appropriate to make it a standalone package and vendor it under `packages`?
",lazywei,Lukasa
2992,2016-04-16 09:00:30,"@kennethreitz @Lukasa Sorry for taking so long. I implemented a naive TTLCache for caching the `proxy_bypass` function call. Please have a look at it and let me know your thoughts.
If there is no big problem with the implementation, I'll add some documentation and testing if necessary later.

Thanks.
",lazywei,kennethreitz
2992,2016-04-16 09:00:30,"@kennethreitz @Lukasa Sorry for taking so long. I implemented a naive TTLCache for caching the `proxy_bypass` function call. Please have a look at it and let me know your thoughts.
If there is no big problem with the implementation, I'll add some documentation and testing if necessary later.

Thanks.
",lazywei,Lukasa
2991,2016-02-02 20:41:44,"- The rewrite of `test_requests.py` looks great to me. I originally wanted our test suite to be fully compatible with unittest's test runner (and it originally was), but over time this slowly became less and less the case. 
- Jython is a very rarely used Python implementation. If someone fixes a Jython-specific bug in a way that doesn't complicate the Requests codebase, I'm happy for us to accept the patch. But, our level of support should (and does) stop there. 
- I don't think having `tox.ini` is very useful, as we won't be using it in any official capacity. Use of `tox.ini` requires that every specified Python installation be available in order to be used effectively, which is quite difficult to do properly on most systems. This project shipped `tox.ini` long ago, but it was removed for a number of reasons. Tox is a wonderful local testing tool, and anyone is free to use it with Requests — that doesn't mean we need to provide the file, though. It adds (yet) another file to the root of the repository, which I would like to keep as clean as possible.
- I have little opinion regarding coverage, except to echo @Lukasa's `coverage.py` statement. If we can get rid of the `.coveragerc` file, that would be great. Again, I'd like to keep those files to a minimum.
",kennethreitz,Lukasa
2991,2016-02-02 20:52:32,"Regarding Travis: we used to utilize Travis, but I quickly grew frustrated with it reporting false negatives for our test suite on a regular basis. I was also frustrated by them randomly removing a few older versions of Python one day, as well. I was also frustrated by all the email notifications it sent me (many regarding forks of kennethreitz/requests). I'm also not a fan of its UI. :)

That was a long time ago, and I'm sure it's improved quite a bit since then. 

Our current CI integration with my Jenkins server seems to be suiting our needs perfectly fine. I personally quite enjoy having a private Jenkins server. However, I wouldn't be against considering a switch back to Travis if there was an obvious benefit to myself, @Lukasa, or @sigmavirus24. I'm not aware of any. Are there any?
",kennethreitz,Lukasa
2990,2016-02-01 21:00:14,"Yeah, this would be a 3.0.0 change. 

@sigmavirus24 I was just trying to use it directly while debugging an infinitely redirecting URL, and I had to look at the source code to figure out how to do so. I just really need to spend some time figuring out why I wrote it that way, there must have been a reason.
",kennethreitz,sigmavirus24
2988,2016-02-03 14:59:28,"@Lukasa Oh, thanks for your advice! Using timestamp is brilliant!
I have made the first attempt and opened the PR!

Thanks!
",lazywei,Lukasa
2988,2016-03-16 09:12:56,"@TetraEtc The pull-request is still open, I'm afraid.
",Lukasa,TetraEtc
2988,2016-06-08 17:17:33,"@Lukasa : Thanks! That´s exactly what I thought of. I tried to implement your suggestion into [robotframework-requests](https://github.com/bulkan/robotframework-requests) (see [PR 117](https://github.com/bulkan/robotframework-requests/pull/117))  but was not successful - still have a delay of approx. 5 seconds. But I am just starting out with Python and programming at all, so I am very sure that I missed something :(
",Tset-Noitamotua,Lukasa
2988,2016-06-09 11:27:13,"@Lukasa : I knew I missed somethink :). Thanks again!  I fixed it now but still see a delay :( (more details [here](https://github.com/bulkan/robotframework-requests/issues/116#issuecomment-224866699))
",Tset-Noitamotua,Lukasa
2988,2016-06-10 06:30:29,"Hi @Lukasa, 

We are using the [requests.Session](https://github.com/bulkan/robotframework-requests/blob/master/src/RequestsLibrary/RequestsKeywords.py#L78)

[Get request](https://github.com/bulkan/robotframework-requests/blob/master/src/RequestsLibrary/RequestsKeywords.py#L840)
",bulkan,Lukasa
2988,2016-06-23 11:24:57,"Hi @Lukasa, @bulkan 
how can I help so that we make progress on this issue. Please give me some noob friendly instructions how to verify whether the used session is the expected one :-)
",Tset-Noitamotua,Lukasa
2988,2016-07-04 11:43:31,"@Lukasa  Thank you! Did as you said and it came out value of `trust_env` was string represantation  of `False` ... thus it was `'False'` (If you interessted [here](https://github.com/bulkan/robotframework-requests/issues/116#issuecomment-230270721) are more details).
",Tset-Noitamotua,Lukasa
2988,2017-02-21 19:27:58,@Lukasa Is this issue still open? Would be willing to take a shot at it using the `@functools.lru_cache` and time-stamping approach.,davidfontenot,Lukasa
2988,2017-02-22 04:22:25,@schlamar would you be able to review?,davidfontenot,schlamar
2986,2016-01-30 19:14:16,"Thanks @untitaker!
",Lukasa,untitaker
2982,2016-01-27 22:46:47,"@TetraEtc that's overly general. While curl and browsers detect the `.` as the sole element of the path, they don't do anything fancy for `https://github.com/foo.` which yields a 404.

Further Requests is an HTTP client, not an oracle or a browser. We can't know what URLs to meaningfully trim/munge/etc. unless a specification tells us to do so. Even then I'd air on the side of not doing this behind the scenes for the user.
",sigmavirus24,TetraEtc
2982,2016-01-27 23:54:49,"@Lol4t0 as @t-8ch points out, relative-references are not what requests accepts (we only accept absolute references which is what you passed us).

> It have to resolve at least to perform redirects

What?
",sigmavirus24,t-8ch
2982,2016-01-27 23:57:43,"Server can return you a relative URL as redirect target with 302 for
example. How are you going to handle that?
L
28 янв. 2016 г. 2:55 пользователь ""Ian Cordasco"" notifications@github.com
написал:

> @Lol4t0 https://github.com/Lol4t0 as @t-8ch https://github.com/t-8ch
> points out, relative-references are not what requests accepts (we only
> accept absolute references which is what you passed us).
> 
> It have to resolve at least to perform redirects
> 
> What?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/2982#issuecomment-175915919
> .
",Lol4t0,t-8ch
2982,2016-01-28 09:27:11,"This is a remarkably tricky issue, but in general I'm sympathetic to the idea that requests should normalise the URL as much as possible when provided by the user or when received in the Location header.

I appreciate that generally speaking we try not to manipulate the URLs provided by users _too_ extensively, but we do still play with them: we try to urlencode partially encoded URLs and generally just try to make sense of what the remote party has sent us. For that reason, I also think we should take a similar attitude to partial paths.

However, there are some reasons to be tentative here. The first is that this amounts to a major change, which would mean it has to go in to 3.0.0. The second is that if we're determined to start handling URLs ""properly"", we should aim to handle them _really_ properly. I believe @glyph is working on a URL handling library, but generally speaking I think we should be aiming to bring in something that lets us work with URLs more effectively so that we can do something that amounts to appropriate behaviour.

The upshot means that, if @sigmavirus24 agrees with me, we'll need to put this on the backburner until we get a library that can let us work with URLs more effectively.
",Lukasa,sigmavirus24
2982,2016-01-28 10:29:28,"> Thanks @glyph. I'll try to play around with this at some point and see if it's useful for requests.

Thanks for considering this code to solve this problem, @Lukasa.  I'm optimistic you'll like it; we've been screwing up URLs real bad for a real long time in Twisted, and the design on this one really clicked together nicely.  The one part that might seem a little odd to Python programmers at first is the totally functional interface, but being able to treat URLs as immutable like the strings they came from actually ends up being really handy.
",glyph,Lukasa
2982,2016-01-28 10:40:45,"@Lukasa `URL.normalizePath() → URL with no ""."" or "".."" segments` work as a signature? Any other options which might be required? I could probably have that landed on trunk by Friday.
",glyph,Lukasa
2982,2016-01-28 10:43:08,"I think strictly we want to follow section 5 of RFC 3986. Note also that @sigmavirus24 has a library with exactly that name that may also end up a good fit here, so you may not want to leap immediately into writing that code: we need to work out exactly what we need here!
",Lukasa,sigmavirus24
2982,2016-01-28 23:24:26,"@sigmavirus24 - Yeah. It would be nice to decide on one or the other. However, they both have a pretty clear internal idea of what a URI is already, which would make it difficult to merge them together much.
",glyph,sigmavirus24
2976,2016-01-22 04:49:26,"@sigmavirus24 

Ok, I'll update.
",shoma,sigmavirus24
2972,2016-01-20 09:04:14,"@sigmavirus24 We shouldn't leak sockets here: the redirect logic consumes all the data from the connection, which should return it back to the connection pool, and that happens before we raise `TooManyRedirects`.
",Lukasa,sigmavirus24
2972,2016-01-20 18:59:33,"> Got a few small notes, but this looks really good! :cake:

Yay!  Thanks a lot for the fast response! @sigmavirus24 @Lukasa 
",munro,Lukasa
2972,2016-01-20 18:59:33,"> Got a few small notes, but this looks really good! :cake:

Yay!  Thanks a lot for the fast response! @sigmavirus24 @Lukasa 
",munro,sigmavirus24
2972,2016-01-21 22:22:24,"LGTM. @Lukasa thoughts?
",sigmavirus24,Lukasa
2969,2016-01-14 15:06:22,"@ralphbean At some point, sure. We weren't generally planning on racing to new urllib3 versions, especially as really the only change in that urllib3 release is the SOCKS proxy support.

Is there a particular rush to get urllib3 1.14 into Fedora?
",Lukasa,ralphbean
2966,2016-01-13 18:52:47,"@alex would something like



Be a sufficiently good API?
",sigmavirus24,alex
2966,2016-01-19 23:55:18,"Echoing the Chrome policy that @dstufft referenced above, I think that maybe the right thing to do here is (for now) to trust the Windows and OS X trust stores (inasmuch as we can) since Microsoft and Apple have demonstrated a baseline level of trustworthiness in their management of certificates.

It seems like neither Debian nor Red Hat is similarly responsible, at least, at this time.

@dstufft asks: what is different about trusting the OS's cert stores than trusting the OS to properly configure OpenSSL? In the Microsoft and Apple case, they knew better than to ship OpenSSL, because it's not actually a proper transport security solution: it's part of a construction kit for building your own transport security.  They built their own, more complete TLS implementations, which have interfaces that let you do things like select trust roots in a sensible way.  In a perfect world, IMHO, we'd be using `cryptography.tls`, which would actually back-end to SChannel or Secure Transport on those platforms, and bundle its own OpenSSL only for linux.  Given that we're a long way away from that world (just in terms of the amount of work it would take), bundling OpenSSL for the protocol implementation on all platforms but trusting the platform trust store on those platforms which seem trustworthy, which hopefully _eventually_ will be all of them.
",glyph,dstufft
2966,2016-01-22 01:13:44,"Debian and CentOS aren't broken. On both systems the configuration works fine with SSL_CTX_set_default_verify_paths(). Only one of both CAfile and CApath must work for SSL_CTX_set_default_verify_paths(). The patch tried to outsmart the system and work around the default API. Also requests can't handle CAfile and CApath at the same time. The combination caused the bug.

I understand @dstufft motivation for the heuristic. OpenSSL doesn't report if SSL_CTX_set_default_verify_paths() has loaded any certs or has found a CApath with valid certs. For CAfile SSLContext.get_ca_certs() helps but for CApath you are lost.

certifi isn't an option. At best it's a matter of last resort for pip. 
",tiran,dstufft
2966,2016-01-28 14:45:29,"@untitaker Yes, we can, and so I'm not worried about Linux. Linux is the easy case here. It's everything else that is tricky.
",Lukasa,untitaker
2966,2016-01-28 16:20:27,"@Lukasa Thanks for your explanation. SSL and certificates are even more of a minefield than I already thought.
My ""simple"" problem is that _requests_ rejects quite a few valid certificates, which are accepted by Python-urllib2, Firefox, IE and Chrome. Installing _certifi_ doesn't resolve this issue.
",shypike,Lukasa
2966,2016-01-28 16:37:06,"@Lukasa Maybe. Using 2.7.11 on Windows and forced to use eGenix.com's pyopenssl binaries (because those are the only ones that don't crash on me). They are at OpenSSL level 1.0.1q
Even so, I think further discussion of my specific problem doesn't contribute much to the larger issue being discussed here. Thanks for your time.
",shypike,Lukasa
2966,2016-01-29 14:47:29,"On Twitter, @Lukasa responded to my suggestion that requests might punt on this problem by using libcurl as a transport, presumably via pycurl. @Lukasa's concern is that depending on libcurl and pycurl would make requests more difficult to install. Using libcurl is probably off the table anyway; @glyph was right to point out the inherent security risk in such a complex C library. Since @Lukasa pointed out that pycurl is only distributed as source and as a .exe installer for Windows, and not as a wheel for Windows and OS X, I think we can eliminate it from discussion.

I like @Lukasa's idea of using the native APIs on Windows and OS X to do the certificate verification, and configuring OpenSSL to use those APIs through a callback. Does pyOpenSSL support this yet? Of course, to use the appropriate APIs on Windows and OS X, it would be necessary to use ctypes, cffi, or a C extension module. Given that the latter is problematic with PyPy, we can probably eliminate that option. pyOpenSSL already uses cffi (indirectly, via cryptography), so that's probably the best option. So would we create bindings for the certificate verification APIs of Windows and OS X as separate Python packages, and add those packages to requests' extras_require list?
",mwcampbell,Lukasa
2966,2016-01-29 22:28:13,"I also agree with @kennethreitz.  This is a laudable goal for any library, but given that `requests` is the transport for how you _get_ other libraries, it's particularly important here.  However, ""no compilation"" does not need to mean ""no C code"" any more, with the advent of the https://github.com/manylinux organization, and the possibility of allowing of linux wheels on PyPI.

Compilation on OS X requires installation of the Xcode command line tools, which requires interactive user acceptance of an EULA, and the error messages that users receive when these tools are _not_ present are totally inscrutable.  Part of the problem here rests with `pip`, of course; it should determine if a package needs compilation and just straight up tell you `""run xcode-select --install""` and not barf out a traceback about a missing executable.
",glyph,kennethreitz
2966,2016-01-30 00:16:35,"@smiley - If you have extracted this information from Chrome, perhaps you could comment on [my Stack Overflow question](https://stackoverflow.com/questions/34732586/is-there-an-api-to-pre-retrieve-the-list-of-trusted-root-certificates-on-windows), for posterity?
",glyph,smiley
2966,2016-01-30 08:38:17,"@smiley With OS X I plan to hook into the urllib3.contrib.pyopenssl module and interfere with `ssl_wrap_socket`. My PoC just waited until after the handshake to do the verification, so you can do that too and then provide us with the PoC code. In the proper code it would be better to hook into the verify callback so that we can fail the handshake, but that is a bit trickier.
",Lukasa,smiley
2966,2016-04-25 13:08:02,"Finally got around to doing the Windows implementation, and it looks like the configurable callback in PyOpenSSL's [`Context.set_verify(mode, callback)`](https://pyopenssl.readthedocs.org/en/latest/api/ssl.html#OpenSSL.SSL.Context.set_verify) will be a good place to implement this. (as @Lukasa suggested) The only downside is that this will force Windows users to install `ndg-httpsclient`, but a simple `pip install ndg-httpsclient` worked perfectly, so that might not be so bad.

However, I noticed the PR in urllib3 and certitude. Should I implement it there somehow, or directly in Requests?

**EDIT:** I just noticed the native part of Certitude. Guess there's already an implementation. Is this still needed or just waiting on `urllib3` to merge?
",smiley,Lukasa
2966,2016-04-25 15:06:46,"@smiley It's waiting on a few other things. The native part of certitude needs a setup for wheel builders, which I haven't gotten around to providing yet.
",Lukasa,smiley
2966,2016-04-25 15:50:23,"@Lukasa It's on my list! (But you should remind me on occasion)
",reaperhulk,Lukasa
2966,2016-04-25 18:31:09,"@piotrjurkiewicz It won't require compilation on install, we'll distribute wheels. =)

And generally speaking ctypes doesn't behave well on PyPy (it's very slow), and PyPy is a first-class supported platform for requests. =)
",Lukasa,piotrjurkiewicz
2966,2016-04-27 02:21:39,"@dstufft if that's true, perhaps we can rig it up similar to the current auto-use of PyOpenSSL, if available. 

Taken further, could be a package like `requests-systemcerts`, included in `requests[security]`.
",kennethreitz,dstufft
2966,2016-05-20 19:48:23,"@dstufft how does this sound to you?
",Lukasa,dstufft
2966,2016-05-20 20:01:26,"@sigmavirus24 Eh, for that I'd defer to [Ten Immutable Laws Of Security (Version 2.0)](https://technet.microsoft.com/en-us/library/hh278941.aspx), particularly:
- Law #1: If a bad guy can persuade you to run his program on your computer, it's not solely your computer anymore.
- Law #2: If a bad guy can alter the operating system on your computer, it's not your computer anymore.
- Law #3: If a bad guy has unrestricted physical access to your computer, it's not your computer anymore.

Once you install someone's software it's not solely your computer anymore.
",dstufft,sigmavirus24
2966,2016-05-20 20:05:50,"@tiran, I don't think you're rude, but I don't see how that matters. The current implementation already assumes OpenSSL on Linux or BSD by virtue of return the path to a PEM bundle. While I concede that I don't have complete solution, I think it is an _improvement_.
",tmehlinger,tiran
2966,2016-05-20 20:18:48,"I think you're responding to the comment I pulled because I second-guessed myself. :)

Regardless--I agree with @dstufft and I'd argue that it's the consumer's job to ensure they're installing sane packages. I don't think we're any less safe without this change; someone could just as easily upload their own fork of requests (many of which already exist) with a malicious PEM bundle and unsuspecting users would be compromised.

I don't deny that it's an attack vector. I also argue that it's no worse than the current situation, which includes a multitude of system administrators like me forced to forego certificate validation just to make everything will work.
",tmehlinger,dstufft
2966,2016-05-20 20:24:02,"@tiran I like that
",tmehlinger,tiran
2966,2016-05-20 20:57:01,"@coderanger @dstufft meh, you are right. It's good enough for the simple cases.

For proper SChannel support we would have to provide a SSLContext-like object that wraps SChannel.
",tiran,dstufft
2966,2016-05-20 21:10:46,"@dstufft yeah, I've been planning to propose such an ABC for some time. I'll talk about it at the language summit

@coderanger There is another option. OpenSSL has an internal API to look up certs by subject or issuer. For example a builtin X509_LOOKUP loads certs for the CA directory and CA file. I believe that it is possible to write a Windows X509_LOOKUP for crypt32.dll CAPI to fetch certs from the system store, convert it to an OpenSSL X509 object and inject it into the X590_STORE of the current SSL_CTX. Additional trust flags can be put in the AUX section of the cert. I haven't figured out if CertFindCertificateInStore() triggers a download of missing trust anchors.
",tiran,dstufft
2966,2016-12-06 17:33:17,@tiran I'm under the impression it will. It's a far more reliable way to handle certificates on Windows than what Python standard library ssl uses.,nanonyme,tiran
2966,2016-12-26 01:24:57,"Talking to @dstufft  about something else, it occurred to me that PEP 493 describes a  `/etc/python/cert-verification.cfg` file that can be used to opt-in to stdlib cert validation as the default on older Python 2.7 releases. Since a large part of the Linux problem here is the difficulty of reliably detecting where the system cert stores are located (even when using a custom `SSLContext`, perhaps it would make sense to define an addition to that file that lets the distro tell Python applications definitively where to find them?",ncoghlan,dstufft
2966,2016-12-28 08:48:50,"@Lukasa Now the question would be *how* to define such an extension (since that file is currently a relatively ad hoc solution to the PEP 476 backport problem).

@tiran has talked about extending verified-by-default support to protocols other than HTTPS in the past, so perhaps it would make sense to pursue as a stdlib config format definition for Python 3.7+ first, and then folks could look at making it available earlier for the benefit of third party libraries?",ncoghlan,tiran
2966,2016-12-28 08:48:50,"@Lukasa Now the question would be *how* to define such an extension (since that file is currently a relatively ad hoc solution to the PEP 476 backport problem).

@tiran has talked about extending verified-by-default support to protocols other than HTTPS in the past, so perhaps it would make sense to pursue as a stdlib config format definition for Python 3.7+ first, and then folks could look at making it available earlier for the benefit of third party libraries?",ncoghlan,Lukasa
2963,2016-01-10 10:50:54,"@sigmavirus24 is right. There is an issue open tracking this problem. We have zero code change required to use that fix, all we have to do is update our dependency (which we'll do in the next 2.X release just as part of the release process). The correct issue to track is the one on urllib3.
",Lukasa,sigmavirus24
2961,2016-01-05 17:23:19,"\o/ Thanks @whit537! :sparkles: :cake: :sparkles:
",Lukasa,whit537
2961,2016-01-05 17:26:55,"Huzzah, thanks! :-)

@Lukasa++
",whit537,Lukasa
2961,2016-01-05 17:37:52,"@Lukasa please add release notes for this. I think it may be a good idea to have some shim for this as people may be catching `InvalidSchema` or `MissingSchema` exceptions and they won't be able to catch those anymore in 3.0
",sigmavirus24,Lukasa
2961,2016-01-06 18:22:07,"@whit537 yeah, it isn't _necessary_ for 3.0 though and there's no way to tell people to move to those new messages unless we forcibly break things for them though.

What we could do is add `InvalidScheme` and `MissingScheme` as sub-classes of their now defunct relatives so that people can start (with 2.10.0) using those exceptions so they're future proofed for 3.0.0. I'm not sure if @Lukasa would agree that there is value in that though.
",sigmavirus24,Lukasa
2961,2016-01-06 18:22:07,"@whit537 yeah, it isn't _necessary_ for 3.0 though and there's no way to tell people to move to those new messages unless we forcibly break things for them though.

What we could do is add `InvalidScheme` and `MissingScheme` as sub-classes of their now defunct relatives so that people can start (with 2.10.0) using those exceptions so they're future proofed for 3.0.0. I'm not sure if @Lukasa would agree that there is value in that though.
",sigmavirus24,whit537
2960,2016-01-05 17:04:04,"Thanks @whit537. Unfortunately, this is a breaking change so we can't accept it until 3.0.0: can you adjust this PR to point to the proposed/3.0.0 branch instead please?
",Lukasa,whit537
2958,2016-01-04 16:55:46,"> Does equality not fall back on identity? Is it not possible to simply confirm that it was called with a specific auth handler?

@Lukasa you're correct. That said it's not exactly simple. Consider a poorly written API wrapper where someone does something like:



Obviously if ^ were better written it'd be easier to test the way you describe, but we know nothing about @Malizor's case (or anyone else's) and we always get push back for encouraging better design/testing practices.

> the python 2.7 documentation states ...

Ah, I forgot that the always ancient Python 2 has silly things like this.
",sigmavirus24,Malizor
2958,2016-01-04 16:55:46,"> Does equality not fall back on identity? Is it not possible to simply confirm that it was called with a specific auth handler?

@Lukasa you're correct. That said it's not exactly simple. Consider a poorly written API wrapper where someone does something like:



Obviously if ^ were better written it'd be easier to test the way you describe, but we know nothing about @Malizor's case (or anyone else's) and we always get push back for encouraging better design/testing practices.

> the python 2.7 documentation states ...

Ah, I forgot that the always ancient Python 2 has silly things like this.
",sigmavirus24,Lukasa
2958,2016-01-04 17:13:00,"Well my specific use case is similar to your example @sigmavirus24. It is a wrapper except it actually does some things before (build params depending on args…) and after (error handling, result formatting).

@Lukasa : HTTPBasicAuth and HTTPProxyAuth just add a constant header on each request, so equality should not cause any problem here.

HTTPDigestAuth is indeed more complicated with it's hook system. Perhaps I should only compare the username and password?
",Malizor,Lukasa
2958,2016-01-04 17:13:00,"Well my specific use case is similar to your example @sigmavirus24. It is a wrapper except it actually does some things before (build params depending on args…) and after (error handling, result formatting).

@Lukasa : HTTPBasicAuth and HTTPProxyAuth just add a constant header on each request, so equality should not cause any problem here.

HTTPDigestAuth is indeed more complicated with it's hook system. Perhaps I should only compare the username and password?
",Malizor,sigmavirus24
2958,2016-01-04 17:20:23,"@Malizor you could just do



And then


",sigmavirus24,Malizor
2958,2016-01-30 03:08:24,"@Malizor this seems like a fine usecase to me. I'm going to merge this in, then simplify the digest check to simply verify the parameters: username, password. 

In other news, I hadn't looked at that digest code in a long time! I had a blast writing that long ago. So many memories. 
",kennethreitz,Malizor
2955,2016-05-27 15:50:49,"@Lukasa You are absolutely right, server config was broken. I fixed it and now it works fine - thanks!
",msztolcman,Lukasa
2953,2015-12-30 23:21:28,"@Lukasa what do you think about having a `socks` extra for requests that installs `PySocks` and whatever else?
",sigmavirus24,Lukasa
2953,2016-04-05 10:04:39,"@Lukasa is this was merged or not? Btw, how to configure the socks proxy?
",loretoparisi,Lukasa
2953,2016-04-05 10:23:01,"@Lukasa so it should work like


",loretoparisi,Lukasa
2953,2016-04-05 12:05:05,"@Lukasa uhm, so supposed to use on google appengine, I have to wait the `devappserver2` python sdk update...
",loretoparisi,Lukasa
2953,2016-04-06 19:41:56,"@kennethreitz I'm away from my laptop, so if you're in a merging mood you can land the urllib3 update and then merge this.

Please note that it makes everyone's life easier if you merge in urllib3 directly from the brand new tag (1.15). Just a suggestion!
",Lukasa,kennethreitz
2950,2015-12-29 12:20:30,"@Lukasa I do not understand the usefulness of the wrapper (except for progression).

My transfer is not streamed, (except if I control the sending of blocks in the proxy object, we agree) but the server does not receive header that tells it that the transfer is chunked. Should I force the header before send the request? Without this header, it will be complicated for components like uwsgi to understand the request and to avoid content buffering (although in my case for now).

It is not stated anywhere in the documentation it is not possible to stream with post method the contents of a file (using a generator or not) and I still do not understand why Flask (without nginx and uwsgi) can not read the stream
",yoyoprs,Lukasa
2950,2015-12-29 13:22:25,"@Lukasa sorry for my broken English, on the flask side issue I mention ""chunked"" and not on this one, my mistake.
What interests me is to successfully write directly content of chunked transfer on Flask side and so now I know it is not ""requests"" the problem (i was redirected here ...)

There is a good chance that Flask adds this header, but it does not explain why without uwsgi interface (with it works), it is impossible to read the stream (Flask would require the Content-Length to start streaming ? Maybe ...)

I will make several tests (without/with uwsgi interface) and try this: http://uwsgi-docs.readthedocs.org/en/latest/Transformations.html#streaming-vs-buffering

Thank you for your patience really.
",yoyoprs,Lukasa
2950,2015-12-29 13:33:17,"@sigmavirus24 yes you can :)
",yoyoprs,sigmavirus24
2950,2015-12-30 12:15:31,"@Lukasa Just a question off issue ... with this method:



Do you have a bug when the file size is 0 byte ?
",yoyoprs,Lukasa
2950,2015-12-31 13:42:57,"@Lukasa I read that it was not possible (without generator) to define the size of sent blocks during upload (due to a httplib limitation). Have I understood correctly or is there a way via a wrapper to do this?
",yoyoprs,Lukasa
2949,2016-09-14 21:51:51,"@Lukasa when you say ""the easiest way to work around the problem is to set a session-level auth handler,"" is that something that works today?  Based on what I'm seeing in the code, the answer is no but your wording makes me wonder if I'm missing something.  You're talking about setting the Session auth attribute, right?
",reteptilian,Lukasa
2947,2015-12-28 18:47:14,"So @sigmavirus24, are you thinking that we should attempt to release 3.0.0 sometime in January including a fix for this issue?
",Lukasa,sigmavirus24
2947,2016-09-06 00:07:58,"Thanks for fixing this @nateprewitt!
",sigmavirus24,nateprewitt
2946,2015-12-26 01:25:41,"@Lukasa I would prefer not to but if you under the API var put your email address you should hit the same issue I am having as long as you are not in any of the breach lists
",L1ghtn1ng,Lukasa
2942,2015-12-21 09:49:25,"Thank you @Lukasa . My mistake, I thought requests has to read it all into memory before send to network.In fact requests just ""stream"" it, which is cool.Thank you!
",jchluo,Lukasa
2939,2015-12-19 16:39:33,"Yes, @L2501. Please continue to act aggressively towards us for not having encountered such a poorly configured server. Please don't take any action to correct that server.

---

@Lukasa There was a proposed version of the Header Dict that ignored multiple headers for anything other Set-Cookie headers. We could fix this faster in requests by using `.getlist` on the original Header Dict from urllib3 and throwing an exception if it returns more than one value.
",sigmavirus24,Lukasa
2939,2015-12-19 16:42:49,"@L2501 That doesn't prevent CRLF injection attacks, it just keeps functioning in the face of them. My proposed solution above would actually avoid _some portion_ CRLF injection attacks by throwing an exception when an invalid header block is received. Of course, we cannot _generally_ prevent CRLF injection attacks and also re-use TCP connections, so any client that _does_ re-use TCP connections is vulnerable to a well-crafted CRLF injection attack. Generally speaking, defending against CRLF injection attacks is the responsibility of the server operator, not the client.

@sigmavirus24 The better long-term fix might be to promote urllib3's `HeaderDict` to use in requests. The risk with doing it your way is that we assume that the `raw` object exists and is a `urllib3` one. I suppose we could have a conditional path that _attempts_ to use urllib3's but falls back to using ours.

We could in fact do that, and then plan in 3.0.0 to promote urllib3's `HeaderDict` to use in requests which would allow us to remove the conditional code path in 3.0.0.
",Lukasa,sigmavirus24
2939,2015-12-19 16:46:02,"@sigmavirus24 No, I'm thinking of test libraries or anything else that returns a `Response` object from a `HTTPAdapter` subclass that is not actually backed by a urllib3 response object.
",Lukasa,sigmavirus24
2939,2015-12-19 16:49:55,"@Lukasa I'm not as concerned about them. Any test library that hasn't been half-written and thrown up on PyPI will handle this just fine (requests-mock, betamax, vcrpy, all do just fine).
",sigmavirus24,Lukasa
2939,2015-12-19 16:51:45,"@sigmavirus24 Be that as it may, in general our code assumes that [`.raw` may not come from urllib3](https://github.com/kennethreitz/requests/blob/fc8fa1aa265bb14d59c68eb68a179bce17953967/requests/models.py#L657-L667), and we should endeavour to maintain that if possible.
",Lukasa,sigmavirus24
2939,2015-12-19 17:02:18,"@sigmavirus24 The common situation is where the response object is `StringIO` or something similar. I think going out of our way to avoid breaking that is a good idea. If we were going straight to 3.0.0 I'd be fine with breaking it, but ideally we'd deal with this sooner.
",Lukasa,sigmavirus24
2939,2016-01-04 02:51:55,"@sigmavirus24

> There was a proposed version of the Header Dict that ignored multiple headers for anything other Set-Cookie headers. We could fix this faster in requests by using `.getlist` on the original Header Dict from urllib3 and throwing an exception if it returns more than one value.

This would break the list-like headers though? The ones defined as `#(values)`, which RFC 7230 [permits](https://tools.ietf.org/html/rfc7230#section-3.2.2) to split into multiple fields.
",vfaronov,sigmavirus24
2939,2016-01-27 14:40:45,"That would be a 3.0 change but I'm amenable to that. @Lukasa, what about you?
",sigmavirus24,Lukasa
2939,2016-01-31 22:20:12,"@kennethreitz Yup agreed
",foxx,kennethreitz
2937,2015-12-18 14:18:56,"@Lukasa are you thinking about making an exception to [our release document](https://github.com/kennethreitz/requests/blob/master/docs/community/release-process.rst#hotfix-releases) in this case?
",sigmavirus24,Lukasa
2937,2015-12-18 14:19:59,"@sigmavirus24 Yeah, good question, but I think I am, given that the urllib3 release would itself be a patch release. Any concerns there? @ralphbean @eriol?
",Lukasa,sigmavirus24
2937,2015-12-18 14:39:36,"@Lukasa Could do. Could try to sneak in SOCKS support too, if you wanna.

Also I noticed some of your changelist includes an update note to the urllib3 version, some don't. Dunno if that's intentional.
",shazow,Lukasa
2937,2015-12-18 14:40:46,"@shazow I'd rather leave SOCKS out for now: makes 1.13.1 very clearly a simple bugfix release.

Yeah, the missing changelist notes are in error, we should try to fix it.
",Lukasa,shazow
2937,2015-12-18 15:03:58,"@Lukasa yeah, if it's just a couple small bug-fixes, that's fine by me. I think we've been bitten in the past by urllib3 having a lot more than just bug fixes when we pulled it in.
",sigmavirus24,Lukasa
2937,2015-12-18 18:04:23,"@ralphbean Frankly I don't recommend releasing 2.9.0, it had enough nasty bugs that going straight to 2.9.1 is going to be the better approach.
",Lukasa,ralphbean
2937,2015-12-19 08:39:50,"\o/ Thanks @shazow!
",Lukasa,shazow
2937,2015-12-21 14:43:20,"@Lukasa is this still a WIP? I believe you wanted to cut this today
",sigmavirus24,Lukasa
2936,2015-12-19 17:34:31,"Woot! :+1: 

Thanks @Lukasa (also YAY OUR CI IS WORKING AGAIN)
",sigmavirus24,Lukasa
2935,2016-02-26 12:51:31,"@Lukasa, but even if `Queue.Empty` is monkeyparched, how the message ends up being `KeyError: (1, True)` ?? And _if_ the method `LifoQueue.get()` or `LifoQueue` was monkeyparched, the traceback would come with the monkeyparched version of it, wouldn't it? This is weird!
",Kronuz,Lukasa
2935,2016-02-26 13:33:03,"@Lukasa, Sentry provides the full raw log as well, it's the one I pasted above... But I'll try to log more things ...the bad news is it's not always happening either :/
",Kronuz,Lukasa
2935,2016-02-26 14:22:33,"@sigmavirus24, I'm still in 2.8.1

The search across all my files with those regexps didn't yield anything even remotely interesting...
",Kronuz,sigmavirus24
2934,2015-12-17 16:32:11,"Thank you @Lukasa .  I guess something else went wrong for me originally (when I thought I did use basic auth), and I did misread wget output (which first ""senses"" which authentication mechanism to use).  Sorry about the noise
",yarikoptic,Lukasa
2927,2015-12-15 15:22:00,"@sigmavirus24 I intentionally did not do that, so that we have a single commit entitled `v2.9.0`, rather than a merge commit that hits it. =D
",Lukasa,sigmavirus24
2926,2015-12-11 19:51:10,"Hey @bsamek! Thanks for the PR.

I don't think we want to change the default parameters to the `request` method. We want to be able to discern between something that's a session setting and a request setting which we cannot do if we change that signature. We use [`merge_setting`](https://github.com/bsamek/requests/blob/master/requests/sessions.py#L49..L53) to make that determination and I don't think we want to break this.

Please revert that as the docstring changes _are_ desirable.
",sigmavirus24,bsamek
2926,2015-12-11 22:14:12,"Thanks again @bsamek !
",sigmavirus24,bsamek
2926,2015-12-12 00:43:29,"Ohhhhhhhhh I see. Thanks @sigmavirus24 !
",bsamek,sigmavirus24
2926,2015-12-12 01:05:42,"Anytime @bsamek :) Thanks for the PR!
",sigmavirus24,bsamek
2925,2015-12-11 20:49:31,"@sigmavirus24 No, no manually acquired lock. The bash script I pasted is all.
It easy to position the stuck by adding print statements around that line:
https://github.com/kennethreitz/requests/blob/2d91365cba9ddca25ddd28f5c0ecd9497b8a2e24/requests/utils.py#L95

It seems that import lock is automatically acquired by CPython interpreter during import.

Call stack of CPython 2.7 __import__ should look like this (not verified):

builtin___import__
https://github.com/python/cpython/blob/61ee2ece38e609a444575dda0302907fa7a2752f/Python/bltinmodule.c#L36

PyImport_ImportModuleLevel
https://github.com/python/cpython/blob/61ee2ece38e609a444575dda0302907fa7a2752f/Python/import.c#L2287

_PyImport_AcquireLock
https://github.com/python/cpython/blob/61ee2ece38e609a444575dda0302907fa7a2752f/Python/import.c#L292
(reenterable lock)
",guojh,sigmavirus24
2925,2015-12-11 21:25:15,"@Lukasa It seems to have been resolved in CPython 3.3. (per-module import lock instead of global import lock)
https://github.com/python/cpython/blob/d5adb7f65d30afd00921e6c22e9e2b8c323c058d/Doc/whatsnew/3.3.rst#a-finer-grained-import-lock
",guojh,Lukasa
2925,2015-12-11 23:34:08,"`get_netrc_auth` probably isn't the only such function though and there may be other places where a delayed import is necessary.

That said, @Lukasa, @guojh's point is that using `str.encode` and `str.decode` will cause this too because they also use delayed imports apparently (the last test module actually also works with a valid encoding that does exist too).

That said, the other work around is to defer the work in the module such that it doesn't happen at import time.
",sigmavirus24,Lukasa
2923,2015-12-09 16:33:02,"Thanks @bsamek! :sparkles: :cake: :sparkles:
",Lukasa,bsamek
2920,2015-12-07 23:08:33,"@sigmavirus24 The main reason I was avoiding posting the whole code was because it's long and dirty, but here you go:


",JKulakofsky,sigmavirus24
2920,2015-12-07 23:55:31,"@ueg1990 you're misunderstanding the problem.
",sigmavirus24,ueg1990
2915,2015-12-04 12:10:19,"@Lukasa: Thanks for spotting this problem! I've added back the pre-existing ad trackers in `/_templates/layout.html` (Click on ""Files Changed"" link at the top). [Reference](https://stackoverflow.com/questions/5585250/how-can-i-add-a-custom-footer-to-sphinx-documentation-restructuredtext)
",ArcTanSusan,Lukasa
2915,2015-12-08 21:36:23,"@kennethreitz so do you want to fix these concerns up first or merge and then fix these up?
",sigmavirus24,kennethreitz
2915,2015-12-11 21:41:38,"@kennethreitz thoughts?
",sigmavirus24,kennethreitz
2915,2015-12-27 19:07:11,"@onceuponatimeforever I think what @kennethreitz wants is for the column beneath it to be aligned with the `R` in `Requests` under the turtle logo. (At least that's roughly what it looks like where it was from the before picture)
",sigmavirus24,kennethreitz
2913,2016-04-20 10:05:57,"@Lukasa How to disable Retrying warning messages?



I am getting thousands of Warning messages in my testcase, due to this effect the log file size was increasing drastically. Is there any possibility to remove/disable warning message when NewConnectionError ?
",vkosuri,Lukasa
2913,2016-04-20 10:26:39,"Thank you @Lukasa I am using [Robotframework-Requests](https://github.com/bulkan/robotframework-requests). I have added fallowing lines code to [Requests.py](https://github.com/bulkan/robotframework-requests/blob/master/src/RequestsLibrary/RequestsKeywords.py)

Still it is not working? Am i missed anything? Could you please suggest me?


",vkosuri,Lukasa
2913,2016-04-20 11:53:28,"Thanks @Lukasa It's working


",vkosuri,Lukasa
2912,2015-12-02 12:01:20,"@Lukasa I see. Yes, I'm concerned about the _standard library_ URL parser, but also curious what is a common practice in other parsers. So, thanks for the clarifications so far.

From your earlier response:

> This API design is super fragile, and I guarantee that it'll be prone to breaking in mysterious ways.

That is exactly what worries me, so I'm trying to understand what are the _super fragile_ traits exactly and what are the ways the API may fails.
",mloskot,Lukasa
2912,2015-12-02 13:56:36,"@Lukasa I think we strip it but I can imagine a way to avoid that.
",sigmavirus24,Lukasa
2912,2015-12-02 14:03:13,"@Lukasa Thanks very much. I sympathise with your critique.
",mloskot,Lukasa
2912,2015-12-10 16:45:59,"@Lukasa Yes. That example surprised me and I thought I will share it as an example of bad design..

@sigmavirus24 

> constantly posting information here will not get this fixed any faster. 

You are misreading my intention - I'm not trying to press anything at all.
I'm simply sharing some further details which I believe are relevant to this issue.
I'm sorry if I caused any annoyance.
",mloskot,Lukasa
2912,2015-12-10 16:45:59,"@Lukasa Yes. That example surprised me and I thought I will share it as an example of bad design..

@sigmavirus24 

> constantly posting information here will not get this fixed any faster. 

You are misreading my intention - I'm not trying to press anything at all.
I'm simply sharing some further details which I believe are relevant to this issue.
I'm sorry if I caused any annoyance.
",mloskot,sigmavirus24
2912,2016-04-08 07:55:54,"@Lukasa I'm simply doing this:



Thanks for the quick reply!
",internaught,Lukasa
2912,2016-04-08 07:59:50,"@Lukasa thanks for the help, working great now. Simple mistake, thanks!
",internaught,Lukasa
2910,2015-12-01 20:35:41,"@sigmavirus24 - something like that, preferably with some information about the additional packages that are necessary, or under what (known) conditions you'll need to do more work. For a library that is explicitly ""HTTP for humans"", and has ""Browser-style SSL Verification"" high up on its list of features, this seems fairly important. The choice to rely on Python/OS for some of its functionality is a choice made by this library, and the user shouldn't be expected to know all the details.
",spookylukey,sigmavirus24
2909,2015-12-01 11:28:04,"Thanks @jwilk! :sparkles: :cake: :sparkles:
",Lukasa,jwilk
2907,2015-11-30 14:01:33,"@Lukasa - `null` perhaps?


",rohithpr,Lukasa
2907,2015-11-30 14:15:41,"I see.

So, the use of `null` in this fashion is specific to your server: it is not _generally_ true about the URL query string. Requests tries to restrict itself to the subset of behaviour that is universally understood in the query string. Implementation-specific variances have to handle that themselves: otherwise, requests will grow a million different feature flags that subtly change its behaviour in all sorts of weird ways. This kind of combinatorial expansion of the API and the possible configurations of requests would make it a nightmare to use, maintain, and debug.

@sigmavirus24 has provided you a code snippet that should do what you're looking for (with some small edits). I hope you find that helpful!
",Lukasa,sigmavirus24
2907,2015-11-30 14:19:37,"@Lukasa @sigmavirus24 - Thanks! :)
",rohithpr,Lukasa
2907,2015-11-30 14:19:37,"@Lukasa @sigmavirus24 - Thanks! :)
",rohithpr,sigmavirus24
2906,2015-11-28 13:56:26,"@Lukasa So maybe I need to install a newer version of OpenSSL using homebrew and use it to replace the old OpenSSL shipped with OS.
",caizixian,Lukasa
2906,2015-11-29 08:09:26,"@Lukasa I had following errors when installing requests[security], both related to cffi.
1. `c/_cffi_backend.c:14:10: fatal error: 'ffi.h' file not found`
   
   so I `brew install pkg-config libffi`, which solved this problem.
2. After that, I tried to install requests[security] again, and I got `Failed building wheel for cffi` . 
   
   Despite this error, pip said that all things were installed. So I checked `ssl.OPENSSL_VERSION` again, still the same.

FYI, my Xcode is 7.1.1
",caizixian,Lukasa
2903,2016-02-26 16:28:02,"All the +1s. You rock @Lukasa thanks! 
",standaloneSA,Lukasa
2903,2016-04-06 19:18:40,"@Lukasa why is this being delayed until 3.x? It doesn't appear to be break any backwards compatibility to me. 
",kennethreitz,Lukasa
2903,2016-04-07 17:22:11,"@Lukasa ah, think that'll be during the sprints? If so, I might try to stay for them then (was planning on not attending sprints this year).
",kennethreitz,Lukasa
2899,2016-06-10 14:50:18,"@luv Currently the expectation is that a future urllib3 release will allow the passing of custom SSLContext objects to urllib3. That will allow construction of adapters that let you use the system certificate store on Linux. For OS X and Windows we have the beginnings of some code to do it, but it's a lot of very subtle work that needs a lot of bugs ironed out.
",Lukasa,luv
2899,2016-06-26 09:13:02,"@Lukasa `/etc/ssl/cert.pem` is now by default linked by the `ca_root_nss` port.
",michael-o,Lukasa
2899,2016-11-18 19:54:07,"As @luv suggested, that will work.
",Lukasa,luv
2899,2016-11-20 22:19:44,"@luv Thanks, I can confirm that if I do `export REQUESTS_CA_BUNDLE=$SSL_CERT_FILE` i would be able to use requests the same way as other libraries. I dont really know why requests needs another environment variable instead of re-using the same one but I guess there were some reasons for that.

While I did this I observed something that looks like a bug: `requests.certs.where()` returns the same location even when you define `REQUESTS_CA_BUNDLE`, even if the library is using the ones from `REQUESTS_CA_BUNDLE`. Should I open a bug for this or is not really a bug?
",ssbarnea,luv
2898,2015-11-25 07:03:30,"Thanks @lndbrg!
",Lukasa,lndbrg
2897,2015-11-26 09:01:10,"Yup, that seems right. They don't consistently hang, but they do sometimes hang. Are you able to run the tests locally @BraulioVM?
",Lukasa,BraulioVM
2897,2015-11-26 09:01:25,"PS @sigmavirus24 I don't seem to be able to stop the builds: can you do it?
",Lukasa,sigmavirus24
2897,2015-11-29 15:35:37,"@Lukasa I ran the tests locally on python2.7 and know why they hang. I have run the tests for the last version both on 2.7 and 3.4, and they are fine. Is there any easy way to replicate the build environment of the CI server? 

I am sorry for breaking it, I never thought there wasn't going to be a timeout or something like that.
",BraulioVM,Lukasa
2897,2015-12-07 21:30:16,"@BraulioVM So, that test doesn't fail for me, it passes, but more importantly it does run!

Having played with this quickly on my local machine, this seems generally good: the test runs, the structure of the function mostly seems to be good, so I'm happy to have this as the basis of a set of more specific tests. I would like us to add some more helpers, but we don't need to do that right now: we can definitely tackle it later.

Well done @BraulioVM! Did you have more work you wanted to do before you proposed this for merging?
",Lukasa,BraulioVM
2897,2015-12-08 14:44:11,"Thank you @Lukasa! I am happy this is finally going to be useful. 

I don't mind adding those helpers you talk about, but as I am not totally sure about how this server will be used I don't know what helpers should be implemented. I guess some logic for parsing the chunked content must be implemented in order to fully test the library, however, I don't think that logic should be built into TestServer.

There's no more work I was planning on doing. I was hoping you would tell some other things you wanted this PR to have (such as those helpers).
",BraulioVM,Lukasa
2897,2015-12-26 11:51:26,"Not right now @BraulioVM, we're pretty quiet over the holiday period so we probably won't find time to focus on reviewing this until the new year.
",Lukasa,BraulioVM
2897,2016-03-01 08:29:25,"I believe this is generally going in the right direction, but now that @kennethreitz is more active he may want to see if this is something he wants. @sigmavirus24 and I are definitely :+1: on this for improving our testing, but it's up to Kenneth.
",Lukasa,sigmavirus24
2897,2016-03-02 15:51:29,"Ok @BraulioVM I've left some notes in the diff. =)

Unfortunately, while you've been away we moved the tests to a `test` subdirectory, so you'll need to move this change onto that directory structure. Sorry!
",Lukasa,BraulioVM
2897,2016-03-04 09:54:27,"Ok, we're making some good progress here @BraulioVM! We'll have a few more rounds of code review while we get this stuff settled, and right now our Jenkins runs are hanging (I'll investigate why).

Did you lose `test_chunked_upload` entirely? It doesn't seem to be in the diff at all: commit f17ef753d2c1f4db0d7f5aec51261da1db20d611 just removed it, without moving it _to_ anywhere.
",Lukasa,BraulioVM
2897,2016-03-04 20:06:34,"@BraulioVM I can't stress this enough: you don't need to apologise for code review. Code review is a healthy part of the development process, and it doesn't matter how good you are: you _will_ get code review, and it's a good thing. You should see some of my patches on Twisted, which have had more than 100 code-review comments on them. ;)
",Lukasa,BraulioVM
2897,2016-03-14 18:28:28,"@BraulioVM I think I'm happy with this as-is at this point, but I'm also a bit too close to it now to be objective. I'd like to ask @sigmavirus24 to do a review as well now, if he has time: just to make sure the code is as good as it can be. Otherwise, I think this is very close to being ready to merge.
",Lukasa,BraulioVM
2897,2016-03-14 18:28:28,"@BraulioVM I think I'm happy with this as-is at this point, but I'm also a bit too close to it now to be objective. I'd like to ask @sigmavirus24 to do a review as well now, if he has time: just to make sure the code is as good as it can be. Otherwise, I think this is very close to being ready to merge.
",Lukasa,sigmavirus24
2897,2016-04-08 10:58:22,"@sigmavirus24, what are the odds that you'll have time in the next 7 days to do a review?
",Lukasa,sigmavirus24
2896,2015-11-23 20:09:35,"@BraulioVM This is a good first pass! I have a draft of this fix sitting around on my laptop, so when I get home I'll show you how I approached it and let you take the ideas you like best. 
",Lukasa,BraulioVM
2896,2015-11-23 20:20:09,"@Lukasa Thanks!

I have just seen #2866, which won't allow me to implement the tests I want, so I might as well think of a solution for that issue.
",BraulioVM,Lukasa
2896,2015-11-24 12:46:04,"@BraulioVM That's actually intentional: sending a chunked zero-length stream is fine, there won't be a problem there. That way, the code is clearer. =)

So, this change looks good to me, but I'll let @sigmavirus24 review it.
",Lukasa,BraulioVM
2896,2015-12-02 08:09:20,"@BraulioVM Nope, just waiting for @sigmavirus24 to get enough time to swing by and take a look at it. =)
",Lukasa,BraulioVM
2896,2015-12-02 14:35:36,"Woops! Sorry. I lost track of this with recent goings on IRL. This looks great to me! Thanks @BraulioVM 
",sigmavirus24,BraulioVM
2890,2015-11-19 07:58:14,"@Bekt The problem with the header dict is that `Set-Cookie` _cannot_ be simply joined by commas in the general case, because its values _contain_ commas. This is some truly terrible design, but we're stuck with it now. This is why you need to use @sigmavirus24's approach.
",Lukasa,sigmavirus24
2879,2015-11-12 18:40:42,"@Lukasa The capture was not possible due to IT restrictions on Wireshark. Anyway I have tried curl and it works. the site has 'SSL connection using TLS_RSA_WITH_RC4_128_SHA'   and I tried the suggestion [on Stack Overflow](http://stackoverflow.com/questions/32650984/why-does-python-requests-ignore-the-verify-parameter) but still get the same error
",datarup,Lukasa
2877,2015-11-12 13:22:01,"@kiddick You're entirely right: that import of the `json` module is from an old version of the documentation. Would you like to open a pull request to remove it?
",Lukasa,kiddick
2877,2015-11-12 13:34:26,"@Lukasa Thanks for reply. Yep I'll open pull request!
",kiddick,Lukasa
2877,2015-11-12 14:11:11,"Thanks for fixing this @kiddick 
",sigmavirus24,kiddick
2876,2015-11-12 22:18:09,"I agree that the information should be available, but currently it is very difficult to access. Presumably it is accessible through the exception object itself, but how to access it is undocumented and seems to differ from exception to exception. The only thing I reliably have access to is the exception cast to a string, and there if I want to use it I have to parse the string. Preferably the components of that string would be available through attributes or methods of the exception.

In my experience, casting the exception to a string generally results is a reasonably easy to read summary of the error in a form that could be passed to the end user, and the components of that message are also available so that a tailored response is possible. For example, an IOError contains filename, strerror, errno, etc. to make it possible to roll your own message or craft a response.

The problem I am having with requests at the moment is that I cannot produce an error message suitable for end users. Casting the exception to a string produces something too complex, and I cannot find the things I would need to use to create an informative message. I am reduced to an error message that simply parrots the name of the exception:



My fear is that not only is the information I need not documented, but getting it is complicated and highly exception specific. This seems to be what @Lukasa is suggesting. Then, to produce good responses to exceptions I will have to understand all of those exceptions in detail and craft code that handles all the cases that could occur. And of course, most users would have to do the same thing, meaning that this code will be written over and over. Perhaps someday someone will create a requests-like package for interpreting the exceptions produced by requests. I guess I am suggesting that it would be good to build that into requests itself, or it it already exists, then give some examples of how to use it in the documentation.
",KenKundert,Lukasa
2872,2015-11-10 23:04:03,"@sigmavirus24: Are you opposed to setting the `Content-Length` to `s.len - s.tell()`?

To be clear, my expectation was that the post request would have an empty body precisely for the reason that you state. The surprise was that the request hung, ie. the `Content-Length` is set to the full size, rather than what's remaining.
",braincore,sigmavirus24
2872,2015-11-11 01:26:03,"As @braincore stated, it's more the fact that the request will hang due to the mismatch between the `Content-Length` and the actual content that is being sent. The misuse of the `StringIO` API was the reason this was discovered, and not problem to be solved.

> It will be more burdensome for developers to get around your forced seek(0) than for developers to add a seek(0) themselves when needed.

I couldn't agree more, especially due to the rather simple fact that the forced seek would make explicit partial file uploads more difficult than they need to be as mentioned by @sigmavirus24.
",jperras,sigmavirus24
2871,2015-11-10 15:28:17,"@Lukasa let's just remove them. This is a hassle and we've had .... 4 bugs about this? It's worthless and it doesn't provide real value.
",sigmavirus24,Lukasa
2871,2015-11-10 15:54:03,":heart: @sigmavirus24 

Thanks for the report @edmorley!
",Lukasa,sigmavirus24
2870,2015-11-10 15:32:36,"@untitaker I can understand that, but I don't believe we can really discuss doing that unless we get the whole team together, which realistically means PyCon 2016.
",Lukasa,untitaker
2870,2015-11-10 15:58:40,"@untitaker He is, correct. However, of the three maintainers he's the one _opposed_ to unbundling.

Ian and I are both unanimous that we'd like to unbundle. However, we have a duty of care over this library, and that includes not changing things that Kenneth cares a great deal about without his explicit say-so.

I don't want to have a detailed discussion with Kenneth about this remotely, because this issue is close to all our hearts and we should discuss it in an environment that makes it easy for us to treat each other like humans, with respect and kindness. Those two qualities are often lost online, and that would be a tragedy: our working relationships are more important than that.
",Lukasa,untitaker
2870,2015-11-11 09:11:32,"@Lukasa I'm sad to confirm but yes, the problem actually exists in Debian testing (with the new import machinery).
I tried the same setup of https://github.com/kennethreitz/requests/issues/2867 using only system packages (so I took python-etcd from sid) and I can confirm it.
I looked at python-etcd code, but the problem was already explained  by @fasaxc and is about exceptions.

On Debian this is what I got:



I'm real sorry I did not noticed when we discussed https://github.com/kennethreitz/requests/pull/2567.

I'm not neither an import logic expert, but your proposal seems the only way to fix this, without unvendoring. Maybe we can ask to @brettcannon if he can take a look at this: I can volunteer to recap all the story so far.

Also, I just want to say thanks to you, @sigmavirus24 and @kennethreitz because although you can just mark this as wontfix, we started working together talking without forget that we are all human beings. 
So, thank you.
",eriol,sigmavirus24
2870,2015-11-11 09:11:32,"@Lukasa I'm sad to confirm but yes, the problem actually exists in Debian testing (with the new import machinery).
I tried the same setup of https://github.com/kennethreitz/requests/issues/2867 using only system packages (so I took python-etcd from sid) and I can confirm it.
I looked at python-etcd code, but the problem was already explained  by @fasaxc and is about exceptions.

On Debian this is what I got:



I'm real sorry I did not noticed when we discussed https://github.com/kennethreitz/requests/pull/2567.

I'm not neither an import logic expert, but your proposal seems the only way to fix this, without unvendoring. Maybe we can ask to @brettcannon if he can take a look at this: I can volunteer to recap all the story so far.

Also, I just want to say thanks to you, @sigmavirus24 and @kennethreitz because although you can just mark this as wontfix, we started working together talking without forget that we are all human beings. 
So, thank you.
",eriol,Lukasa
2870,2015-11-11 09:32:34,"@eriol Thanks for investigating. =) And thanks for working with us on this. Again, I can't reiterate enough, it's really important to us to get to a place where everyone's getting a good experience. We're not there yet, but we'll keep trying.

I'd be delighted to see if someone who knows the import machinery really well can propose a better solution to this problem. If you're willing to do the legwork on ramping up @brettcannon that would be even better @eriol.

Otherwise, we should aim to take that boring manual step or see if we can avoid the problem in requests in some other way.
",Lukasa,eriol
2870,2015-11-16 23:29:16,"Basically @fasaxc is right in his analysis of the problem. The tricky bit is that this is all influenced by how you actually import something. You have to realize each of the following statements use slightly different logic to get you what you want:



If you use the last approach then that will definitely trip you up in this instance because import will do an explicit import for `urllib3.exceptions` _and then_ look for the `HTTPError` attribute. This means that since you are not patching `urllib3.exceptions` in `sys.modules` you end up having `from requests.packages.urllib3 import HTTPError` needing to import `requests.packages.urllib3` directly and since that isn't in `sys.modules` it leads to a fresh import. Now you have two separate modules with two separate classes and thus they won't succeed in a `issubclass()` check in an `except` clause.

Now to solve this, @Lukasa was right and you can just patch every module in `urllib3`. Another option is a custom importer that handles just this case, but I don't know if you want such a magical solution. A third option is to insert an object into `sys.modules` who uses `__getattribute__` to direct to the proper module and handle all magical tweaks to `sys.modules`. And lastly, the simplest solution is either let Debian handle their own desire to monkeypatch vendored code or stop vendoring stuff entirely.
",brettcannon,Lukasa
2870,2015-11-17 14:14:16,"So I'm enumerating the options and asking questions to make sure I understand correctly.

> you can just patch every module in `urllib3`

As in, ensuring that `sys.modules` has both `urllib3.submodule` and `requests.packages.urllib3.submodule`? I think this is the least magic way and given that @Lukasa and I are both core developers of urllib3, we will catch any new submodules that need to be added to requests' patching logic. I'm most strongly in favor of this one.

> Another option is a custom importer that handles just this case, but I don't know if you want such a magical solution.

We had that option and it broke many a thing. I'm not in favor of going back down that route.

> insert an object into `sys.modules` who uses `__getattribute__` to direct to the proper module and handle all magical tweaks to `sys.modules`. 

Hm. This sounds like something in the vein of option 2 but a little less magic-y. It still feels like some abuse of the module system and like it might cause us problems. If we come up with something like this, I would appreciate it if @fasaxc and @eriol would commit to testing this with python-etcd and making sure things still aren't broken before shipping a release with it.

> And lastly, the simplest solution is either let Debian handle their own desire to monkeypatch vendored code or stop vendoring stuff entirely.

Breaking this into two sub-suggestions:
- ""let Debian handle their own desire to monkey patch vendored code"" No. I don't want @eriol to have to figure that out. Vendoring is a position of this project. As much as I dislike downstreams unvendoring things, it isn't the individual maintainer's fault or decision and I don't want to push this work onto their backs.
- ""stop vendoring stuff entirely"" Many people constantly badger (or even, at times, yell) this at us (I know you're not Brett) but it simply just won't happen like that (if it happens at all).

---

My 2¢:

We can take a short term solution of the first option for a `2.8.2` release and get that updated patch to @eriol, @warsaw, @ralphbean, etc. so distros can fix their packages while taking a look at the feasibility and reliability of option 3. I still prefer the first option to the third, but the third may have benefits I'm not seeing due to the bad taste left in my mouth by the `VendorAlias` (a.k.a., option 2).
",sigmavirus24,eriol
2870,2015-11-17 14:14:16,"So I'm enumerating the options and asking questions to make sure I understand correctly.

> you can just patch every module in `urllib3`

As in, ensuring that `sys.modules` has both `urllib3.submodule` and `requests.packages.urllib3.submodule`? I think this is the least magic way and given that @Lukasa and I are both core developers of urllib3, we will catch any new submodules that need to be added to requests' patching logic. I'm most strongly in favor of this one.

> Another option is a custom importer that handles just this case, but I don't know if you want such a magical solution.

We had that option and it broke many a thing. I'm not in favor of going back down that route.

> insert an object into `sys.modules` who uses `__getattribute__` to direct to the proper module and handle all magical tweaks to `sys.modules`. 

Hm. This sounds like something in the vein of option 2 but a little less magic-y. It still feels like some abuse of the module system and like it might cause us problems. If we come up with something like this, I would appreciate it if @fasaxc and @eriol would commit to testing this with python-etcd and making sure things still aren't broken before shipping a release with it.

> And lastly, the simplest solution is either let Debian handle their own desire to monkeypatch vendored code or stop vendoring stuff entirely.

Breaking this into two sub-suggestions:
- ""let Debian handle their own desire to monkey patch vendored code"" No. I don't want @eriol to have to figure that out. Vendoring is a position of this project. As much as I dislike downstreams unvendoring things, it isn't the individual maintainer's fault or decision and I don't want to push this work onto their backs.
- ""stop vendoring stuff entirely"" Many people constantly badger (or even, at times, yell) this at us (I know you're not Brett) but it simply just won't happen like that (if it happens at all).

---

My 2¢:

We can take a short term solution of the first option for a `2.8.2` release and get that updated patch to @eriol, @warsaw, @ralphbean, etc. so distros can fix their packages while taking a look at the feasibility and reliability of option 3. I still prefer the first option to the third, but the third may have benefits I'm not seeing due to the bad taste left in my mouth by the `VendorAlias` (a.k.a., option 2).
",sigmavirus24,Lukasa
2870,2015-11-17 14:51:54,"@brettcannon many thanks!

I'm also in favour of the less magic solution.
I will be happy to test before the release python-etcd and all the dependants of requests that can be affected by this: using codesearch.debian.net I noticed that also cinder and proliantutils import urllib3's exceptions using `requests.packages.urllib3`, so I will check also them.

@sigmavirus24 I really appreciate your words, thanks!
",eriol,sigmavirus24
2870,2015-11-17 15:36:29,"On Nov 17, 2015, at 06:14 AM, Ian Cordasco wrote:

> As in, ensuring that `sys.modules` has both `urllib3.submodule` and
> `requests.packages.urllib3.submodule`? I think this is the least magic way
> and given that @Lukasa and I are both core developers of urllib3, we will
> catch any new submodules that need to be added to requests' patching
> logic. I'm most strongly in favor of this one.

It's mildly disconcerting that a library would fiddle with another library's
sys.modules namespace, but I agree that this is probably the least magical
(and thus most likely to work) way.  Maybe we can convince @brettcannon to
build a nicer foolproof <wink> module alias system for 3.6. :)

> We can take a short term solution of the first option for a `2.8.2` release
> and get that updated patch to @eriol, @warsaw, @ralphbean, etc. so distros
> can fix their packages while taking a look at the feasibility and reliability
> of option 3. I still prefer the first option to the third, but the third may
> have benefits I'm not seeing due to the bad taste left in my mouth by the
> `VendorAlias` (a.k.a., option 2).

Yep, I suspect that #1 may be the best approach, and that #3 would be
difficult to debug if things go south.  It may be that no solution is perfect,
given Python's current import semantics and implementation, in which case
doing the best you can with the least magic (i.e. most discoverable and
debuggable) would suck less.

Thanks!
",warsaw,eriol
2870,2015-11-17 15:36:29,"On Nov 17, 2015, at 06:14 AM, Ian Cordasco wrote:

> As in, ensuring that `sys.modules` has both `urllib3.submodule` and
> `requests.packages.urllib3.submodule`? I think this is the least magic way
> and given that @Lukasa and I are both core developers of urllib3, we will
> catch any new submodules that need to be added to requests' patching
> logic. I'm most strongly in favor of this one.

It's mildly disconcerting that a library would fiddle with another library's
sys.modules namespace, but I agree that this is probably the least magical
(and thus most likely to work) way.  Maybe we can convince @brettcannon to
build a nicer foolproof <wink> module alias system for 3.6. :)

> We can take a short term solution of the first option for a `2.8.2` release
> and get that updated patch to @eriol, @warsaw, @ralphbean, etc. so distros
> can fix their packages while taking a look at the feasibility and reliability
> of option 3. I still prefer the first option to the third, but the third may
> have benefits I'm not seeing due to the bad taste left in my mouth by the
> `VendorAlias` (a.k.a., option 2).

Yep, I suspect that #1 may be the best approach, and that #3 would be
difficult to debug if things go south.  It may be that no solution is perfect,
given Python's current import semantics and implementation, in which case
doing the best you can with the least magic (i.e. most discoverable and
debuggable) would suck less.

Thanks!
",warsaw,Lukasa
2869,2015-11-10 12:51:02,"How about now @Lukasa ? I rewrote the third `if` statement as well, for consistency.
",miroli,Lukasa
2868,2015-11-17 00:57:35,"@Lukasa Yes that's all correct, it's just that the SAML case is the first time i've run across cookies being used at all in OpenStack. I need to look into a way of managing cookies seperate to what the requests.Session is doing. For almost all cases what i need now is just drop all cookies.
",jamielennox,Lukasa
2867,2015-11-09 02:49:43,"@sigmavirus24 No problem - the pip versions should be good though?
",sjmh,sigmavirus24
2867,2015-11-09 03:05:09,"@sigmavirus24 Alrighty - thanks, just confirmed those versions work! Do you know if there's any way to get around the issue with those RPM versions?  If not, is there a minimum version that this issue would be fixed in ( Looks like newest from pip is 2.8.1 ) or is this a general thing for all downstream redistributors in any version?

Thanks for the help.
",sjmh,sigmavirus24
2865,2015-11-07 18:02:11,"@Lukasa that won't work. We expect `data` to be a dictionary (or list of tuples with length two) and `files` to be the same. That said, without a description of what the endpoint you're talking to is actually expecting, we can't provide more help than we already have.

This is probably a problem better suited for [StackOverflow](https://stackoverflow.com/questions/tagged/python-requests).
",sigmavirus24,Lukasa
2864,2015-11-07 21:41:13,"Thanks @aartur 
",sigmavirus24,aartur
2863,2016-03-22 21:24:54,"Yeah, essentially that's the plan. Last I checked, I think @shazow and I were unclear on exactly the direction that change should take.
",Lukasa,shazow
2862,2015-11-06 15:51:06,"This issue can now be followed as [PyPy issue #2183](https://bitbucket.org/pypy/pypy/issues/2183/pypy-400-ssl-module-appears-to-leak-memory). Thanks for the report @aartur!
",Lukasa,aartur
2861,2015-11-07 14:30:23,"@sigmavirus24 This should be good to merge.
",Lukasa,sigmavirus24
2861,2015-11-07 17:50:48,"@Lukasa I'm :+1: but I think we should work on fixing our test suite. If Kenneth comes along and decimates it again, then we at least had test coverage for some period of time there.
",sigmavirus24,Lukasa
2859,2015-11-05 15:37:35,"Thanks @Lukasa!
",sigmavirus24,Lukasa
2852,2015-10-29 22:19:17,"So @Lukasa, how does that little change affect us here? How will requests act differently with the wrapped error?
",steveoh,Lukasa
2852,2015-10-30 01:03:56,"@Lukasa Any idea why the socket is throwing the error in the first place?
",steveoh,Lukasa
2852,2015-11-02 19:07:40,"@Lukasa could it be that there's a load balancer terminating TLS and that's timing out the connection?
",sigmavirus24,Lukasa
2850,2015-10-26 23:42:29,"@Lukasa I am sorry that I don't understand requests well enough. Could you show me how to do it? Thanks.
",pcsysen,Lukasa
2850,2015-10-27 00:48:34,"@Lukasa Are you talking about the normal result? When it is working it is like this



I don't know if it tells anything. I was thinking maybe you want to see the header when the error happens, which I don't know and I failed to learn it from the example you posted.
",pcsysen,Lukasa
2850,2015-10-28 22:50:44,"@sigmavirus24 I don't quite understand what you mean by ""always bytes"". The response should be all ASCII characters.
",pcsysen,sigmavirus24
2844,2015-12-16 14:48:23,"@untitaker please open a new issue.
",sigmavirus24,untitaker
2840,2015-10-21 11:54:19,"@Lukasa I build distribution packages for openSUSE. Since the tests will fail in Build Service due to ""no network"" policy during build, I test these packages in a local installation at least. 

cd requests-2.8.1
./test_requests.py

BTW, I considered to add a feature request to have an --local test option, e.g. using vcrpy.
",frispete,Lukasa
2840,2015-10-21 11:57:42,"@Lukasa Does that allow offline tests? Scratch that, I read the purpose: sounds great.
",frispete,Lukasa
2840,2015-10-24 15:42:29,"@frispete we have a similar policy in Debian and I follow the same workflow, testing requests locally.

I'm testing calling py.test (and py.test-3: on Debian we had to version the name, I know it's ugly...) directly and all the tests are fine (with one xfailed):



@Lukasa it's a very good news the plan about switching to pytest-httpbin! I would like to help, is there a ticket about this?
",eriol,Lukasa
2840,2015-10-24 15:47:34,"@eriol Yup, see #2184. We're very close now, just got problems with one threaded test.  
",Lukasa,eriol
2840,2015-10-28 12:22:03,"@Lukasa Confirming this issue too (requests 2.8.1). The invocation method is setup.py test, which runs py.test directly. I was a bit confused too when I saw the mark.xfail, but it's definitely making the tests fail:



pytest version is 2.7.1.
",koobs,Lukasa
2840,2015-10-28 12:50:25,"@Lukasa It's on my list of ports to update, will report back post-update :)
",koobs,Lukasa
2840,2015-10-28 14:04:26,"@Lukasa Same failure with pytest 2.8.2 (latest py as well)

Edit: Fails with setuptools test command. Direct py.test passes (that test), but fails on another :)


",koobs,Lukasa
2840,2015-10-29 02:59:53,"@Lukasa I would concur given the tests/evidence so far. Perhaps @hpk42 could shed some light on it
",koobs,Lukasa
2837,2015-10-20 08:24:32,"@ueg1990 This is occurring because you're using this on an old version of Python which does not have a fully-featured `ssl` module. You can fix it either by upgrading to 2.7.9 or later, or by installing `pyopenssl`, `ndg-httpsclient`, and `pyasn1`.
",Lukasa,ueg1990
2836,2015-10-20 16:19:58,"@Lukasa the rearrangement is something we had discussed for 3.0.0 because it's fundamentally backwards incompatible. I'm in favor of it though because I think the order of precendence should be (in order of least to most important):
- Environment
- Session settings
- Per-call settings

(For those features to which this applies)
",sigmavirus24,Lukasa
2835,2015-10-21 17:57:50,"Ok, I am trying to be practical here. I don't want to wait on the response coming back from the server but i do want to obligate the system to send the request. I could use UDP sockets but that requires a new server and a bunch more overhead. I completely understand TCP/IP and HTTP aren't really built for this but i think i can get 90%+ of what i need just setting a small value on the read-timeout. I did read in a couple places that setting a timeout of zero doesn't ""obligate"" the system to send the message (might only make it to buffers without getting transmitted) depending on the local implementation so i see why my problem might have been occurring. 

@sigmavirus24 thank you for the note on the headers. Emulating a ""best effort"" protocol on top of TCP/IP and HTTP with a upper bound on total wait time (limitations acknowledged) seems to work better with low read-timeout settings than enabling streaming responses. Eventually I think we will enable a UDP server.
",trcarden,sigmavirus24
2831,2015-10-16 12:21:43,"Thanks for this @shagunsodhani! :sparkles: :cake: :sparkles:
",Lukasa,shagunsodhani
2829,2015-10-15 14:37:15,"Thanks for this @dblia! :cake: :sparkles: :cake:
",Lukasa,dblia
2829,2015-10-15 14:38:23,"Thanks for the immediate response @Lukasa and @sigmavirus24!
",dblia,Lukasa
2829,2015-10-15 14:38:23,"Thanks for the immediate response @Lukasa and @sigmavirus24!
",dblia,sigmavirus24
2828,2015-10-15 13:44:50,"It is not an error in the Json handler. The json handler does not accept a _string_, it accepts the Python object directly. The difference between what you provided and what @sigmavirus24 gave is this:


",Lukasa,sigmavirus24
2827,2015-10-15 13:01:10,"Thanks for this @daniel-mueller!
",Lukasa,daniel-mueller
2826,2015-10-16 11:36:37,"@Lukasa I have opened a pull request for this.
",shagunsodhani,Lukasa
2826,2015-10-19 09:48:10,"Thanks @shagunsodhani. I got busy and didn't notice this.
",TorKlingberg,shagunsodhani
2825,2015-10-14 13:29:40,"@Lukasa do we want to add that warning in `super_len` and then make it a hard exception in 3.0.0?
",sigmavirus24,Lukasa
2825,2015-10-14 13:35:01,"@sigmavirus24 Yeah, probably.
",Lukasa,sigmavirus24
2825,2015-10-14 15:58:40,"&hearts; thanks brosephs @sigmavirus24 @Lukasa 
",chrismattmann,Lukasa
2825,2015-10-14 15:58:40,"&hearts; thanks brosephs @sigmavirus24 @Lukasa 
",chrismattmann,sigmavirus24
2825,2015-10-24 04:41:24,"@Lukasa do you want me to send that separately or do you want to do that as part of this PR?
",sigmavirus24,Lukasa
2825,2015-10-24 14:37:28,"Excellent work @Lukasa . Thanks!
",sigmavirus24,Lukasa
2819,2015-10-11 14:32:28,"@asieira Thanks for this! This is a duplicate of #2813, and should be resolved in 2.8.1.
",Lukasa,asieira
2818,2015-10-14 19:04:58,"@Lukasa I understand, but I don't have time to crack open Tweepy at the moment. The POST payload did contain unicode, I can tell you that.
",fitnr,Lukasa
2818,2015-10-20 00:58:51,"@Lukasa It appears that example works with requests 2.7.0 and a default encoding of 'utf-8'. The issue we are encountering is that upgrading requests from 2.7.0 now breaks all requests that could have utf-8 data being sent in the data field, without prior warning. Would there be a way to avoid this breaking change?
",vinodc,Lukasa
2818,2015-10-20 07:52:19,"@vinodc To be clear, the error you're encountering is a known bug in urllib3 that has been fixed (see shazow/urllib3#719). However, that fix will not be pulled into the released version of requests until urllib3 ships a new release and then requests ships 2.9.0. Until such time, you could apply the diff from the linked patch to your installation or, alternatively, simply encode the data yourself.

> It appears that example works with requests 2.7.0 and a default encoding of 'utf-8'.

The reason I encourage you to encode the data yourself is that the example only works _by sheer good luck_. There are many ways this could fail, and it could fail in other environments. Relying on Python 2.7's implicit encoding/decoding is extremely dangerous, and I strongly encourage you to resolve this problem.
",Lukasa,vinodc
2818,2015-10-20 08:02:04,"@Lukasa Got it. Thanks for the detailed response.
",vinodc,Lukasa
2817,2016-09-08 17:17:58,"@Lukasa Do you think that on https://github.com/JohnVillalovos/requests/blob/a3c5b0325f74517707ddda91de38d55071c9c76a/requests/utils.py#L613

It should maybe just be:


",JohnVillalovos,Lukasa
2817,2016-09-08 17:31:39,"@Lukasa Arg, I just removed it!  heh.

Let me know if I should put it back, or if you think it will work.
",JohnVillalovos,Lukasa
2816,2015-10-09 18:50:26,"@warsaw thanks to make this happen! :smile: 
I started reading debian-python@l.d.o after my reply here!
@Lukasa if you can point me to the OpenStack issue I will include it into the description of the patch Debian will use.
Thanks!
",eriol,Lukasa
2816,2015-10-10 08:18:20,"@eriol OpenStack issue is [here](https://review.openstack.org/#/c/213310/0).
",Lukasa,eriol
2816,2015-10-11 15:20:26,"Thank you @ralphbean and @eriol for working with us on this. :)
",sigmavirus24,ralphbean
2816,2015-10-11 15:20:26,"Thank you @ralphbean and @eriol for working with us on this. :)
",sigmavirus24,eriol
2816,2015-10-23 14:10:41,"@ralphbean Theories?
",Lukasa,ralphbean
2816,2015-11-05 10:00:58,"Alright, that's Debian and Fedora handled. Thanks so much @ralphbean @warsaw @eriol, we :heart: you greatly.
",Lukasa,ralphbean
2816,2015-11-05 10:00:58,"Alright, that's Debian and Fedora handled. Thanks so much @ralphbean @warsaw @eriol, we :heart: you greatly.
",Lukasa,eriol
2816,2015-11-05 17:20:27,"Thanks @eriol and I'm always happy to help sponsor.  One thing we need to keep in mind is that as we update requests' dependencies, we have to make sure to also update requests setup.py.  I don't think we have any mechanisms in Debian to ensure this but we can probably rely on Ubuntu's -proposed migration to keep track of mismatches: http://people.canonical.com/~ubuntu-archive/proposed-migration/update_excuses.html

The other thing we may want to think about is DEP-8 tests to make these verifications.  Then at least we'll see problems on qa.debian.org.  A README.Debian is a good idea too, but they are easy to miss :(
",warsaw,eriol
2815,2015-10-09 14:41:38,"@Lukasa yes, you are right,this is may be a issue of python 3.5,I will close this issue:)
",gengjiawen,Lukasa
2811,2015-10-08 12:50:55,"@Lukasa if the OpenStack gate is indicative of anything, it's between 1.10.4 and 1.11
",sigmavirus24,Lukasa
2811,2015-10-08 12:52:51,"@Lukasa : yes looks like new NewConnectionError raised here: https://github.com/kennethreitz/requests/blob/master/requests/packages/urllib3/connection.py#L142
",aarefiev22,Lukasa
2809,2015-10-08 16:37:21,"@Lukasa Thanks, I'll test this now.
",hombremuchacho,Lukasa
2808,2015-10-07 21:37:48,"@Lukasa @sigmavirus24 Thanks for clarifying how this works. I'll open an issue. We use the requests library as part of our project and are missing a certain certificate -- this certificate is present in certifi; however, it hasn't been added to requests. https://github.com/certifi/python-certifi/blob/master/certifi/cacert.pem#L5026
",hombremuchacho,Lukasa
2808,2015-10-07 21:37:48,"@Lukasa @sigmavirus24 Thanks for clarifying how this works. I'll open an issue. We use the requests library as part of our project and are missing a certain certificate -- this certificate is present in certifi; however, it hasn't been added to requests. https://github.com/certifi/python-certifi/blob/master/certifi/cacert.pem#L5026
",hombremuchacho,sigmavirus24
2807,2015-10-10 14:59:30,"Alright, thanks for the clarification.

Then how should I use merge_environment_setting in order to merge only HTTP proxies var ?

Is this correct ?



@sigmavirus24 : I agree, the existing behavior is fine, It just lacks some precision in the documentation though.
",mxjeff,sigmavirus24
2803,2015-10-05 14:32:31,"@asieira No, I think I recommend adding it to your local `.git/info/exclude` file. Text-editor specific config really shouldn't go into the project repository, the world just gets really complex really fast that way.
",Lukasa,asieira
2803,2015-10-05 14:40:22,"LGTM. @sigmavirus24?
",Lukasa,sigmavirus24
2803,2015-10-05 14:54:31,"@Lukasa will you handle the release notes for this or should I do it later tonight?
",sigmavirus24,Lukasa
2803,2015-10-05 14:56:21,"@sigmavirus24 I'll do it.

@asieira Thanks so much for this change! :sparkles: :cake: :sparkles:
",Lukasa,sigmavirus24
2803,2015-10-05 14:56:21,"@sigmavirus24 I'll do it.

@asieira Thanks so much for this change! :sparkles: :cake: :sparkles:
",Lukasa,asieira
2800,2015-10-03 16:32:38,"@Lukasa would you like to revert these changes, or should I?
",sigmavirus24,Lukasa
2800,2015-10-03 17:34:05,"Thanks so much @hosamaly!
",Lukasa,hosamaly
2800,2015-10-03 17:49:21,"You're welcome @Lukasa . Thanks for accepting it.
",hosamaly,Lukasa
2797,2015-10-02 09:08:22,"Thanks @sumitbinnani! :sparkles: :cake: :sparkles:
",Lukasa,sumitbinnani
2795,2015-10-02 07:13:59,"Sorry @sumitbinnani, but there's already an open fix for this in #2763 that I think we're likely to merge. Thanks so much for your contribution though! :sparkles: :cake: :sparkles:
",Lukasa,sumitbinnani
2793,2015-10-03 20:30:12,"@Lukasa You are most certainly correct.  God only knows when I did it.
",MonsieurCactus,Lukasa
2791,2015-09-29 03:12:48,"@sigmavirus24 
Thanks for take care the case.
More details are:

## Environment

1)  A basic ElementaryOS without any configurations, which means a total clean installation from DVD or ISO. (Actually I used 3 different Ubuntu)
- ElementaryOS Freya, based on Ubuntu 14.04 TLS
- ElementaryOS Luna, based on Ubuntu 12.02 TLS
- Docker official python 2.7.10 image which is based on Debian.

2)  A basic CentOS 6.5 without any configuration.
- Python version: 2.6.6
- >>> import ssl
- >>> import requests
- >>> import urllib3

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ImportError: No module named urllib3

## Problem

I wrote a script to require Nuance ASR service, see below code. When I run this script on ElementaryOS or Debian, it get a 500 error from Nuance ASR. And Nuance ASR returns correct 200 http response with recognized text content when I run the script on CentOS.

<pre ><code>
import os
import requests
class Nuance_api():
        """"""Nuance ASR Service API""""""
        def __init__(self, appid, appkey, n_id):
                self.appid = appid
                self.appkey = appkey
                self.id = n_id
        def recogenize(self, language, voicefilename):
                nuance_url = 'https://dictation.nuancemobility.net/NMDPAsrCmdServlet/dictation'
                n_params = {'appId':self.appid, 'appKey':self.appkey, 'id':self.id}
                voice_file = {'file': open(voicefilename, 'rb')}
                n_headers = {'Accept-Language':language, 'Content-Type':'audio/amr;codec=amr;bit=4.75;rate=8000', 'Accept':'text/plain', 'Accept-Topic':'Dictation'}
                r = requests.post(url=nuance_url, params=n_params, headers=n_headers, files=voice_file, verify=False)
                if r.status_code == 200 :
                        return r.text
</code></pre>

## Additional problem

When I use `openssl s_client -connect dictation.nuancemobility.net:443` on Ubuntu evn (whatever which one), it gives an error: 

<pre><code>
SSL handshake has read 4032 bytes and written 609 bytes
---
New, TLSv1/SSLv3, Cipher is RC4-SHA
Server public key is 2048 bit
Secure Renegotiation IS supported
Compression: NONE
Expansion: NONE
SSL-Session:
    Protocol  : TLSv1.2
    Cipher    : RC4-SHA
    Session-ID: 9986FC667995B0BEF3325136811A0D8C4E14901627854D8DC042F9920F341903
    Session-ID-ctx: 
    Master-Key: 15199FB75D47C6B7C1CA86D12010C63C34E7908EBC4902EB5AC91E39A6C965121F9906719857CFDCBC319BEA5824C356
    Key-Arg   : None
    PSK identity: None
    PSK identity hint: None
    SRP username: None
    Start Time: 1443494743
    Timeout   : 300 (sec)
    Verify return code: 20 (unable to get local issuer certificate)
---
</code></pre>

When I use `openssl s_client -connect dictation.nuancemobility.net:443` on CentOS env or `openssl s_client -connect dictation.nuancemobility.net:443 -CApath /etc/ssl/certs` on Ubuntu or Debian, it gives correct response:

<pre><code>
---
New, TLSv1/SSLv3, Cipher is RC4-SHA
Server public key is 2048 bit
Secure Renegotiation IS supported
Compression: NONE
Expansion: NONE
SSL-Session:
    Protocol  : TLSv1.2
    Cipher    : RC4-SHA
    Session-ID: 9986FC667994C5F4F3325136811A0C8C0F5A769166086DE7C042F9920F341B05
    Session-ID-ctx: 
    Master-Key: 0B893DBEF29821430934D7AB665AF2E24E6322E7990EB77415A6715D6196F73E503DC0395DDDFF209D491A144371F5D5
    Key-Arg   : None
    Krb5 Principal: None
    PSK identity: None
    PSK identity hint: None
    Start Time: 1443495250
    Timeout   : 300 (sec)
    Verify return code: 0 (ok)
---
</code></pre>

## Reproduce the fault

for debug purpose, I would suggest the easiest way that to create Docker env and run official python 2.7.10 image, then test Request in it. The instructions for these:

<pre><code>
# Docker installation
$ sudo sh -c ""echo deb https:/get.docker.io/ubuntu docker main > /etc/apt/sources.list.d/docker.list""
$ curl -s https://get.docker.io/gpg | sudo apt-key add -
$ sudo apt-get update
$ sudo apt-get install lxc-docker
# run docker image
$ sudo docker run -i -t python:2.7.10 /bin/bash
# then the current bash is in python 2.7.10 image, and user is root.
# pip install requests
# python MY_SCRIPT_WITH_NECESSARY_PARAMETERS'_VALUE or just to connect dictation.nuancemobility.net:443
</code></pre>
",adrianzhang,sigmavirus24
2791,2015-09-29 09:41:13,"@sigmavirus24 When using Requests, it gets below response.

<pre><code>
/usr/local/lib/python2.7/dist-packages/requests-2.6.0-py2.7.egg/requests/packages/urllib3/connectionpool.py:769: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html
  InsecureRequestWarning)
<html>
<head>
<meta http-equiv=""Content-Type"" content=""text/html; charset=UTF-8""/>
<title>Error 500 x-nuance-sessionid71887ede-d26d-448d-b154-256eecd3dda0
Received QueryRetry: 1 AUDIO_INFO</title>
</head>
<body><h2>HTTP ERROR 500</h2>
<p>Problem accessing /NMDPAsrCmdServlet/dictation. Reason:
<pre>    x-nuance-sessionid71887ede-d26d-448d-b154-256eecd3dda0
Received QueryRetry: 1 AUDIO_INFO</pre></p>
</body>
</html>
</code></pre>

There are tons discussions of the openssl behavior on Debian/Ubuntu. It is quite clear that every ssl client software on Debian/Ubuntu MUST define CA bundle with some parameters by themselves. The sample is how I use `openssl s_client`. To Requests, I tried `export REQUEST_CA_BUNDLE=/etc/ssl/certs/ca-certificates.crt` but still got 500. 

If Nuance ASR service is hard for troubleshooting. My question can be replaced by ""How to make Requests working to access https://www.google.com via TLS v1.2 on Debian/Ubuntu?""

BTW, in order to identify where the problem is, I also tried to revise my code below to use TLS v1, but also got 500. Can you also let me know the usage of Adapter is correct or not please? 

<pre><code>
import os
import requests
from requests_toolbelt.adapters.ssl import SSLAdapter
import ssl
s = requests.Session()
s.mount('https://dictation.nuancemobility.net/', SSLAdapter(ssl.PROTOCOL_TLSv1))
class Nuance_api():
        """"""Nuance ASR Service API""""""
        def __init__(self, appid, appkey, n_id):
                self.appid = appid
                self.appkey = appkey
                self.id = n_id
        def recogenize(self, language, voicefile):
                nuance_url = 'https://dictation.nuancemobility.net/NMDPAsrCmdServlet/dictation'
                n_params = {'appId':self.appid, 'appKey':self.appkey, 'id':self.id}
                voice_file = {'file': open(voicefile, 'rb')}
                n_headers = {'Accept-Language':language, 'Content-Type':'audio/amr;codec=amr;bit=4.75;rate=8000', 'Accept':'text/plain', 'Accept-Topic':'Dictation'}
                r = requests.post(url=nuance_url, params=n_params, headers=n_headers, files=voice_file, verify=False)
                if r.status_code == 200 :
                        return r.text
</code></pre>
",adrianzhang,sigmavirus24
2791,2015-09-30 16:09:22,"@Lukasa 
First of all, I am really appreciate of your support. I did not post the full url because there are credentials in it. Can I send  to you and sigmavirus24 via email? 
",adrianzhang,Lukasa
2791,2015-09-30 16:28:17,"@Lukasa Thank you so much, email sent to cory@lukasa.co.uk. Any more details you want to know, feel free and let me know please.
",adrianzhang,Lukasa
2785,2015-09-30 17:36:36,"For the record I agree completely with @Lukasa, his proposal matches perfectly my original suggestion of using `python-requests/<version> (http://http://www.python-requests.org/)` as the default UA.
",asieira,Lukasa
2785,2015-10-05 01:25:25,"@asieira that is not @Lukasa's proposal. Please don't misrepresent people's comments.
",sigmavirus24,Lukasa
2785,2015-10-05 01:25:25,"@asieira that is not @Lukasa's proposal. Please don't misrepresent people's comments.
",sigmavirus24,asieira
2785,2015-10-05 12:11:35,"If I understand @Lukasa 's comments correctly, he agrees that a) removing the kernel version is a good idea, and that b) the Python version is not a vital information to include in the UA, so it could be removed provided this doesn't break pip (which was subsequently confirmed by @dstufft). 

So that would leave us with a default UA with no kernel or Python version, and only contains the requests version. Which is precisely what I had originally proposed.

I'm sorry if I misunderstood any of that. I don't want to start a flame war here or anything, but now I'm genuinely curious as to what part of @Lukasa 's comment you believe I misinterpreted or misrepresented.
",asieira,Lukasa
2785,2015-10-05 12:11:35,"If I understand @Lukasa 's comments correctly, he agrees that a) removing the kernel version is a good idea, and that b) the Python version is not a vital information to include in the UA, so it could be removed provided this doesn't break pip (which was subsequently confirmed by @dstufft). 

So that would leave us with a default UA with no kernel or Python version, and only contains the requests version. Which is precisely what I had originally proposed.

I'm sorry if I misunderstood any of that. I don't want to start a flame war here or anything, but now I'm genuinely curious as to what part of @Lukasa 's comment you believe I misinterpreted or misrepresented.
",asieira,dstufft
2785,2015-10-05 13:17:18,"Damnit @Lukasa you always post a comment ~30s before I can.

@asieira I didn't mean to put you on the offensive. I simply didn't want someone coming along and implementing the wrong thing based on your comment. Clarity and correctness is important.

Frankly, I'm -0 on removing this information but I wouldn't block a PR removing it.
",sigmavirus24,Lukasa
2785,2015-10-05 13:17:18,"Damnit @Lukasa you always post a comment ~30s before I can.

@asieira I didn't mean to put you on the offensive. I simply didn't want someone coming along and implementing the wrong thing based on your comment. Clarity and correctness is important.

Frankly, I'm -0 on removing this information but I wouldn't block a PR removing it.
",sigmavirus24,asieira
2785,2015-10-05 13:23:59,"@Lukasa and @sigmavirus24 thank you for clarifying that. The important part for me is removing the sensitive information. Not at all married to the specific format or having the requests URL there at all, sorry I didn't make that clearer before. You are absolutely right that clarity and correctness are important.

Do we all agree with `requests/<version>` as the UA string, then?

I can submit a PR for this, if you wish, as soon as we agree on the exact format.
",asieira,Lukasa
2785,2015-10-05 13:23:59,"@Lukasa and @sigmavirus24 thank you for clarifying that. The important part for me is removing the sensitive information. Not at all married to the specific format or having the requests URL there at all, sorry I didn't make that clearer before. You are absolutely right that clarity and correctness are important.

Do we all agree with `requests/<version>` as the UA string, then?

I can submit a PR for this, if you wish, as soon as we agree on the exact format.
",asieira,sigmavirus24
2782,2015-09-22 03:38:43,"@sigmavirus24 Thanks for your reply.

When I use requests to post json data, the json data will be escaped by default. 
My code is here:



The api server can't parse data correctly, `if include \\uxxxx will create fail!` (The api server is a third party)

Because of requests's default json dumps, all Chinese character will be escaped `\\uxxx`, so I want to change default json.dumps() by it's `ensure_ascii` options.
",tao12345666333,sigmavirus24
2782,2015-09-27 13:10:56,"Hi， @sigmavirus24  I have modified the code before, does not change the original behavior, but to provide a keyword argument. Using this because escape does not an action of `prepare_body()` function.
Therefore, don't use explicit arguments.
",tao12345666333,sigmavirus24
2782,2016-01-30 07:02:10,"@kennethreitz Thanks! I will try again. :-)
",tao12345666333,kennethreitz
2780,2015-09-20 12:51:17,"@sigmavirus24 As per my further debugging, the issue is not related to requests. I am closing this issue.
Thanks @sigmavirus24 
",muhammad-ammar,sigmavirus24
2779,2015-12-04 08:25:03,"@kennethreitz: Is this what you have in mind? https://github.com/kennethreitz/requests/pull/2915 How do I confirm that ad tracking codes are still enabled and nothing broke?
",ArcTanSusan,kennethreitz
2778,2015-09-18 16:11:07,"So I took a quick look



So using something like what @Lukasa has suggested should work. Note: the adapter that @Lukasa references in his blog post is also available in the [requests-toolbelt](https://toolbelt.readthedocs.org/en/latest/adapters.html#ssladapter).
",sigmavirus24,Lukasa
2778,2015-09-18 22:41:17,"@Lukasa @sigmavirus24 Thanks a bunch for the advice! I've used the adapters, and:
- Python code works on Ubuntu machines but fails with `SSLV3_ALERT_HANDSHAKE_FAILURE` on OSX
- `openssl s_client -connect service.isracard.co.il:443 -no_ssl2 -no_ssl3 -no_tls1 -no_tls1_1` works on the OSX machine, with `OpenSSL 1.0.2d 9 Jul 2015`

It's probably out of the  `requests` scope, but any idea how to fix this on OSX? The obvious solution seems to be upgrading openSSL version, but it's already upgraded to a pretty new version. Is Python using the same OpenSSL binaries as the command line tools, or is there a way to upgrade the libraries to a working version?
",adamatan,Lukasa
2778,2015-09-18 22:41:17,"@Lukasa @sigmavirus24 Thanks a bunch for the advice! I've used the adapters, and:
- Python code works on Ubuntu machines but fails with `SSLV3_ALERT_HANDSHAKE_FAILURE` on OSX
- `openssl s_client -connect service.isracard.co.il:443 -no_ssl2 -no_ssl3 -no_tls1 -no_tls1_1` works on the OSX machine, with `OpenSSL 1.0.2d 9 Jul 2015`

It's probably out of the  `requests` scope, but any idea how to fix this on OSX? The obvious solution seems to be upgrading openSSL version, but it's already upgraded to a pretty new version. Is Python using the same OpenSSL binaries as the command line tools, or is there a way to upgrade the libraries to a working version?
",adamatan,sigmavirus24
2775,2015-09-13 12:26:03,"Thanks @ueg1990! :cake:
",Lukasa,ueg1990
2773,2015-09-12 18:35:48,"@jwilk This is certainly a thing we could do. However, it's a bizarrely application specific fix that will not actually help in a lot of cases because applications that use requests will need to opt-in to that functionality. This means they need to know enough to do that, which is not likely.

Really from a security perspective we should switch to disable netrc auth by default (a change that would need to wait until a 3.0.0 release because it's backwards incompatible, though potentially something worth doing).

In the short term, you will get more security either by not using `~/.netrc` files at all (thereby removing the source of the vulnerability altogether) or by constructing AppArmor profiles that limit access to the file to those applications you have pre-authorized to use it.

@sigmavirus24 For the longer term, I'm open to swapping our default here, which is arguably somewhat insecure, though I also just think people shouldn't be writing their passwords down anywhere at all, at least not in plaintext.
",Lukasa,jwilk
2773,2015-09-12 19:05:56,"@Lukasa isn't there already an open issue for turning auto-loading of netrc off by default for 3.0.0? I thought we were in agreement on this already. I've never once thought this was a good idea but we haven't had opportunity to break this behaviour previously.

I'd also be okay moving `get_netrc_auth` (or whatever the function is actually called) into the `auth` module and documenting it publicly.
",sigmavirus24,Lukasa
2772,2015-09-11 19:22:07,"Thanks @jimbrowne! :cake: :sparkles: :cake:
",Lukasa,jimbrowne
2771,2015-11-05 22:30:25,"@Lukasa it's getting late by you, want me to tackle this tonight?
",sigmavirus24,Lukasa
2769,2015-09-10 21:14:05,"Thanks @jwilk! :cake: :sparkles: :cake:
",Lukasa,jwilk
2768,2015-09-10 22:01:59,"Actually `__class__` and `self.__class__` are a whole different level of magic that will thoroughly break this for any subclass. Sorry @ueg1990 this is not something I will allow in requests.
",sigmavirus24,ueg1990
2767,2015-09-10 21:12:57,"Thanks @ueg1990! :cake: :sparkles: :cake:
",Lukasa,ueg1990
2765,2015-09-10 11:29:42,"Thanks @ueg1990! I think you're right: feel free to provide a pull request. =)
",Lukasa,ueg1990
2765,2015-09-10 13:36:31,"@ueg1990 It doesn't exist in Python 3. =) Given that this is a one-time operation, we don't care too much about the one-off memory allocation for the temporary list on Python 2.
",Lukasa,ueg1990
2763,2015-10-02 07:30:53,"@sumitbinnani I'd happily accept a pull request that adds only that documentation change.
",Lukasa,sumitbinnani
2763,2015-10-02 13:05:21,"This looks okay to me. How about you @Lukasa? We should probably not have this pending after having merged the documentation for next week.
",sigmavirus24,Lukasa
2758,2015-09-05 21:01:37,"Fantastic @mhils! Thanks! :cake: :sparkles: :cake:
",Lukasa,mhils
2757,2015-09-05 20:59:40,"Thanks @keyan! From the issue:

> On a prepared request, the only thing I think can hurt us is the case when the body attribute is a file object. Is there a specific hook that we're concerned about not being pickleable?

So the concerns seem to be body file objects and hooks.
",Lukasa,keyan
2757,2015-09-20 02:28:18,"@keyan do you have intentions to update this with the tests we were looking to add?
",sigmavirus24,keyan
2757,2015-09-24 01:46:51,"Hey @sigmavirus24, I could use some advice on how to add a file object in the test environment. I just added a commit with a failing test containing a hook. Not sure how to address the actual issue though.
",keyan,sigmavirus24
2757,2015-09-30 12:55:44,"@keyan It is often easiest to just create a file handle to the file under test: `open(__file__)`.
",Lukasa,keyan
2754,2015-09-18 22:16:12,"@kennethreitz You're doing an admirable job. ;)
",Lukasa,kennethreitz
2754,2015-10-01 09:29:25,"Hurrah! All tests green. Over to you @sigmavirus24.
",Lukasa,sigmavirus24
2751,2015-08-29 12:21:12,"Thanks @carlosvargas 
",sigmavirus24,carlosvargas
2749,2015-08-31 11:12:00,"@sigmavirus24 sorry.

urlencode in python 2 encode unicode to ?

this is a problem
",pynixwang,sigmavirus24
2742,2015-08-25 01:45:00,"Thanks @everett-toews ! :sparkles: :cake: :sparkles: 
",sigmavirus24,everett-toews
2741,2015-08-25 14:39:13,"@Lukasa - that's what I intended, and what I'm sure the code does.  Fall back to the scheme lookup if the scheme+hostname lookup fails.  Would you rather I make it 5 lines of code to make that clearer?
",jasongrout,Lukasa
2741,2015-08-25 14:40:09,"@sigmavirus24 - this is my first contribution to requests.  How does testing work in this package?
",jasongrout,sigmavirus24
2741,2015-08-25 18:29:17,"@sigmavirus24, I didn't see any tests in test_requests.py for the current proxies argument to request methods.  I saw a few places where it is set from the urllib getproxies function, presumably to do some actual network calls.  I also saw two tests for the no_proxy environment variable.  Is there a test for the proxies argument to a request that I missed?  I'm happy to write one, just wanted to make sure I didn't miss it (and I was looking for a model for how to write a test that is consistent with your framework).
",jasongrout,sigmavirus24
2741,2015-08-27 18:29:35,"@Lukasa, I've done that now.  I'm not very clear on how this plays with the environment proxies, especially in rebuild_proxies() in the sessions code (https://github.com/kennethreitz/requests/blob/master/requests/sessions.py#L228).  Any help from someone that already knows the call flow and how the proxies are changed at each step is appreciated.  I think I don't need to do anything, but I'm not absolutely sure.
",jasongrout,Lukasa
2741,2015-09-04 19:41:56,"@Lukasa - a friendly ping about reviewing this.  Thanks again for helping this along!
",jasongrout,Lukasa
2741,2015-09-05 04:01:38,"Ok, I'm officially happy with this. @sigmavirus24, can I get you to take a quick look?
",Lukasa,sigmavirus24
2741,2015-09-06 02:17:59,"Looks good to me. Thanks for your hard work and diligence @jasongrout 
",sigmavirus24,jasongrout
2741,2015-09-06 02:26:29,"Hurrah! @sigmavirus24 do we want to merge this before 2.8.0 or hold off until the release after?
",Lukasa,sigmavirus24
2741,2015-09-06 02:45:56,"May as well pull it into 2.8.0. Seems like @jasongrout needs this sooner rather than later.
",sigmavirus24,jasongrout
2741,2015-09-14 16:10:40,"@Lukasa - just curious, is there a planned release date for 2.8.0?
",jasongrout,Lukasa
2740,2015-08-24 12:43:00,"Thanks @qingyunha! :cake:
",Lukasa,qingyunha
2737,2015-08-21 08:08:17,"Yup, this looks right to me. Thanks @mjpieters!
",Lukasa,mjpieters
2730,2015-08-15 20:02:04,":sparkles: :cake: :sparkles: Thanks @bmispelon!
",sigmavirus24,bmispelon
2725,2015-08-15 02:31:40,"Hi @Lukasa - what's weird though is that this works fine on *Nix and Mac. Tika Python is a Python library  that uses (at its lowest level) requests to talk to the [Tika JAX RS Server](http://wiki.apache.org/tika/TikaJAXRS). It posts to the /rmeta endpoint. On Linux, calls like `parser.from_file()` work fine - on Windows, they block and block and then finally timeout. I'm not sure how the answer above has to do with that behavior but perhaps I didn't explain it well enough. Any ideas?
",chrismattmann,Lukasa
2725,2015-08-15 03:25:12,"@sigmavirus24 thanks for the insight. I'll take a look at https://github.com/sigmavirus24/requests-toolbelt/pull/84
",chrismattmann,sigmavirus24
2725,2015-08-15 03:26:05,"One thing too @sigmavirus24 that is kind of odd though - this same behavior with e.g., data=open(filename, 'r') works fine on Linux, e.g., with Tika-server running on Linux and Tika Python running on Linux. It only seems to fail on Windows.
",chrismattmann,sigmavirus24
2725,2015-08-17 05:47:45,"thanks @Lukasa and @sigmavirus24 . The odd thing is this is an extremely small file (default win.ini) and the behavior is that i make the post - then it waits for like 2-3 minutes, then times out. Just odd.
",chrismattmann,Lukasa
2725,2015-08-17 05:47:45,"thanks @Lukasa and @sigmavirus24 . The odd thing is this is an extremely small file (default win.ini) and the behavior is that i make the post - then it waits for like 2-3 minutes, then times out. Just odd.
",chrismattmann,sigmavirus24
2725,2015-08-18 01:13:54,"thanks @Lukasa  and @sigmavirus24 . Here is the file I was using (C:\Windows\win.ini)



I tried with the 'b' flag as well, and there was no difference. 

As for:

> 1. Tika Server running on a non-Windows operating system, with Tika-Python running on Windows
> 2. Tika Server running on Windows, with Tika-Python running on a non-Windows OS.

Got it. I will try both and report back. Thanks for helping.
",chrismattmann,Lukasa
2725,2015-08-18 01:13:54,"thanks @Lukasa  and @sigmavirus24 . Here is the file I was using (C:\Windows\win.ini)



I tried with the 'b' flag as well, and there was no difference. 

As for:

> 1. Tika Server running on a non-Windows operating system, with Tika-Python running on Windows
> 2. Tika Server running on Windows, with Tika-Python running on a non-Windows OS.

Got it. I will try both and report back. Thanks for helping.
",chrismattmann,sigmavirus24
2725,2015-08-31 19:55:43,"any comments here @sigmavirus24 @Lukasa ?
",chrismattmann,Lukasa
2725,2015-08-31 19:55:43,"any comments here @sigmavirus24 @Lukasa ?
",chrismattmann,sigmavirus24
2725,2015-09-03 16:13:37,"hi @Lukasa nope I haven't used it. I did try and find some programs that would do this after I found out Wireshark won't capture localhost on Windows (or it will if you hack some networking routes, etc., but I tried and none of the approaches worked). Any advice here would be appreciated.
",chrismattmann,Lukasa
2725,2015-09-04 19:37:29,"hey @Lukasa yep I tried to use RawCap unfortunately the frickin' page download redirects to an non existent URL and I can't download the tool :( I also couldn't find it on the internet, sadly.
",chrismattmann,Lukasa
2725,2015-09-05 07:04:51,"Hi @Lukasa well see that's the thing - this error only manifests on Windows. There are not any issues from Linux to Linux; from Linux to Windows; but there are errors Windows to Windows.
",chrismattmann,Lukasa
2725,2015-09-07 02:05:52,"have either of you @sigmavirus24 and @Lukasa been able to replicate https://github.com/kennethreitz/requests/issues/2725#issuecomment-132040020 and https://github.com/kennethreitz/requests/issues/2725#issuecomment-132092182 on your own machines? That would help I think and I did do the work there hoping you guys would try to replicate.
",chrismattmann,Lukasa
2725,2015-09-07 02:05:52,"have either of you @sigmavirus24 and @Lukasa been able to replicate https://github.com/kennethreitz/requests/issues/2725#issuecomment-132040020 and https://github.com/kennethreitz/requests/issues/2725#issuecomment-132092182 on your own machines? That would help I think and I did do the work there hoping you guys would try to replicate.
",chrismattmann,sigmavirus24
2725,2015-09-18 05:18:39,"hey guys @Lukasa @sigmavirus24 either of you get a chance to try this or replicate?
",chrismattmann,Lukasa
2725,2015-09-18 05:18:39,"hey guys @Lukasa @sigmavirus24 either of you get a chance to try this or replicate?
",chrismattmann,sigmavirus24
2725,2015-10-13 03:23:44,"guys any progress @Lukasa @sigmavirus24 ?
",chrismattmann,Lukasa
2725,2015-10-13 03:23:44,"guys any progress @Lukasa @sigmavirus24 ?
",chrismattmann,sigmavirus24
2725,2015-10-14 12:18:48,"Interestingly, none of our documentation has a single example of `open` used _without_ binary mode. I'll just provide a docs update that warns about this, then.

@sigmavirus24 files opened in text mode is a thing we can detect in code pretty easily: is it worth us lobbing out a warning about it?
",Lukasa,sigmavirus24
2725,2015-10-14 16:00:14,"thanks @Lukasa and @sigmavirus24 #brosephs &hearts; 
",chrismattmann,Lukasa
2725,2015-10-14 16:00:14,"thanks @Lukasa and @sigmavirus24 #brosephs &hearts; 
",chrismattmann,sigmavirus24
2722,2015-08-19 16:17:46,"Sadly @jasongrout the project has a low communication bandwidth at the moment: I'm moving flat and so am without fixed-line internet, and the other two maintainers are taking well-needed rests. This hasn't been lost, I promise!
",Lukasa,jasongrout
2717,2015-08-12 15:39:11,"@Lukasa that's inaccurate. The traceback comes from an SSL wrapped socket. This has nothing to do with httplib from what I can see. `OverflowErrors` are raised when a value is larger than the underlying C integer size allowed. This can be seen if you call `len(something_larger_than_four_gb)` on a 32 bit system.
",sigmavirus24,Lukasa
2717,2017-02-15 22:24:25,"@Lukasa 's method does not work - even with httplib off the signing still happens for the transport itself.  In my case I have a 2GB+ POST request (not a file, just POST data).  This is for an elasticsearch bulk update.  The endpoint only has HTTPS so I'm working through other solutions now.

",adamn,Lukasa
2716,2015-08-28 17:36:50,"@lukasgraf RFC 6265 addresses the issue of empty cookie values in both the `Set-Cookie` grammar in [section 4.1.1](http://tools.ietf.org/html/rfc6265#section-4.1.1) and its interpretation by the client in [section 5.2](http://tools.ietf.org/html/rfc6265#section-5.2) wherein nil values are permitted. Therefore a user agent should return an empty value, subject to constraints imposed when the cookie was set (lifetime, path, etc.), in future requests.
As you remark, an equals symbol is required in the relevant headers, which `cookielib` fails to include.

As a historical note, the grammar in [section 3.2.2 of RFC 2965](https://tools.ietf.org/html/rfc2965#section-3.2) (and [RFC 2901](https://tools.ietf.org/html/rfc2109#section-4.2.2) before it) forbade nil values by specifying cookie values to be tokens (see [section 2.2 of RFC 2616](https://tools.ietf.org/html/rfc2616#section-2.2)) or quoted strings, though the latter could be empty. The original [Netscape proposal](http://curl.haxx.se/rfc/cookie_spec.html) was silent on the matter.
",MikeWinter,lukasgraf
2715,2015-08-08 20:06:01,"+100 to what @Lukasa said. If you want normalized responses, you can use a mapping of status codes to whatever you'd like them to represent.
",sigmavirus24,Lukasa
2714,2015-08-09 15:05:23,"Yeah @sigmavirus24, I'd like to do that too.
",Lukasa,sigmavirus24
2714,2015-08-11 02:51:55,"@Lukasa so this breaks some of our tests around cookies. Particularly because HTTPbin doesn't set the `domain` attribute. So we at least know this works as intended. I'll see how difficult it would be to fix this in httpbin
",sigmavirus24,Lukasa
2714,2015-08-15 15:34:46,"So this isn't breaking tests the way I thought it was. When I looked at which tests are failing, I noticed what was happening. The tests in question all do roughly the following:



In other words, we're expecting a `Cookie` header to be sent that looks like `Cookie: foo=bar` to that URL. This fails because those cookies are naively added to a Cookie Jar and that cookie has no domain associated with it which causes it to not match the request host.

This makes me ask some questions (since I rarely use cookies like this):
- Do users actually use cookies like this?
- Can we safely assume that cookies like this are always meant for the host, e.g., unless the cookie is parsed to have a domain, we forcibly set it to our request host for that request?

Further this also affects users who do something like:



This, however, is far more nebulous. There is no good way to know what domains to send that cookie for. Previously we did something that was arguably really really awful (send it for all domains, I suspect). I think this begs, then, for a helper to create a cookie to be used here. Thoughts @Lukasa?
",sigmavirus24,Lukasa
2714,2015-08-15 19:45:22,"Okay, so @Lukasa and I discussed this in [IRC](https://botbot.me/freenode/python-requests/2015-08-15/?msg=47298372&page=1). The result was the following:
1. Continue with this as it is
2. Update `create_cookie` helper to **require** a domain attribute
3. Disallow string-ish types (str, unicode, bytes, etc.) when assigning to a CookieJar (e.g., the case where you update a session's cookies by doing `s.cookies['foo'] = 'bar'`). This will mean only accepting Cookie objects as the value. (We may also want to validate that said cookie has a domain attribute, or at least issue a warning if it does not.)
4. When passing `cookies=` to a request method, **assume** that if a domain isn't present, it is explicitly for the request domain.

What does this mean for the backport to the `requests-toolbelt` and 2.x:
1. The toolbelt will start to carry the re-implementation of the CookieJar from this request when we move this functionality there.
2. The toolbelt will also carry the implementation of `create_cookie` that **requires** a domain
3. requests 2.x will start issuing a `DeprecationWarning` when doing `s.cookies['foo'] = 'bar'` to warn people of the change coming in 3.0
4. requests 2.x will carry this policy function that's already written so people can start using it and seeing how their code may break but only if they opt in by creating a Policy and a new Cookie Jar with that policy.
",sigmavirus24,Lukasa
2714,2016-03-13 02:59:59,"@Lukasa @sigmavirus24 what is the status of this? I was working a bit in the `RequestsCookieJar` code for #3028, and I'd be willing to work on this a bit if it's still something you think should move forward.
",davidsoncasey,Lukasa
2714,2016-03-13 02:59:59,"@Lukasa @sigmavirus24 what is the status of this? I was working a bit in the `RequestsCookieJar` code for #3028, and I'd be willing to work on this a bit if it's still something you think should move forward.
",davidsoncasey,sigmavirus24
2713,2015-08-07 23:52:35,"@sigmavirus24 got it, I'll remove all the ones except the one for the cookies section. Should I stick with the `api-` prefix or drop that as well?
",lukasgraf,sigmavirus24
2713,2015-08-07 23:57:40,"@sigmavirus24 updated
",lukasgraf,sigmavirus24
2713,2015-08-08 01:18:18,"Thanks @lukasgraf! :sparkles: :cake: :sparkles: 
",sigmavirus24,lukasgraf
2711,2015-08-31 07:27:47,"Thanks for your reply @Lukasa, I understand where you're coming from.  Really appreciate you considering my idea :smile: 
",fgimian,Lukasa
2709,2015-08-05 12:57:15,"@sigmavirus24  I understand that.
but why works perfeclty with pycurl?

import pycurl
c = pycurl.Curl()
c.setopt(c.URL,  'https://server.com/server.jsp?login')
postfields = 'USUARIO=username&PASSWORD=contrase%F1a15'
c.setopt(c.VERBOSE, True)
c.setopt(c.POSTFIELDS, postfields)
c.perform()
c.close()

I think maybe is some bug with the encoding.

Because if I try the same with requests, this dont work.
",Wu4m4n,sigmavirus24
2709,2015-08-05 13:36:27,"Thanks for the reply, I hope this could help to another people with the same Issue.
Thanks @sigmavirus24 
",Wu4m4n,sigmavirus24
2706,2015-08-05 05:41:20,"@sigmavirus24 @Lukasa This bug was found when we generate signature based on the order of parameters (signature would be a hash of the url, for example), while on the server side, the signature is validated using the same process based on the query string passed to the server. In this case, the order of the parameters were changed by requests library underneath.

On the other hand, the approach #1921 took is not recommended either, because the order of keys even in normal dictionaries depends on when the insertion and deletion happens, so it is better not to reconstruct the dictionary, do a `del` instead. See the example below:


",ak1r4,Lukasa
2706,2015-08-05 05:41:20,"@sigmavirus24 @Lukasa This bug was found when we generate signature based on the order of parameters (signature would be a hash of the url, for example), while on the server side, the signature is validated using the same process based on the query string passed to the server. In this case, the order of the parameters were changed by requests library underneath.

On the other hand, the approach #1921 took is not recommended either, because the order of keys even in normal dictionaries depends on when the insertion and deletion happens, so it is better not to reconstruct the dictionary, do a `del` instead. See the example below:


",ak1r4,sigmavirus24
2706,2015-08-06 21:19:26,"@sigmavirus24 @Lukasa Any thoughts on this PR?
",ak1r4,Lukasa
2706,2015-08-06 21:19:26,"@sigmavirus24 @Lukasa Any thoughts on this PR?
",ak1r4,sigmavirus24
2706,2015-08-06 22:31:46,"I'm :+1:: @sigmavirus24?
",Lukasa,sigmavirus24
2706,2015-08-06 23:02:55,"Alright, let's try to land this in 2.8.0. @sigmavirus24 I'll go ahead and merge it to the 2.8.0 branch and update the changelog appropriately.
",Lukasa,sigmavirus24
2703,2015-08-04 20:07:36,"@sigmavirus24 

While I don't disagree with you it may be a good idea to update the documentation to state that the quote_plus (or equivalent) is required if passwords are going to contain those characters. This may be the simplest way to resolve this issue. That way future users don't get confused.
",kharmalord,sigmavirus24
2699,2015-07-30 21:24:43,":clap: @Lukasa 
",sigmavirus24,Lukasa
2699,2015-07-30 21:28:40,"@sigmavirus24 Sounds good to me.

Separately, urllib3 may want a fix for this, because it is _also_ affected. /cc @shazow
",Lukasa,sigmavirus24
2699,2015-07-30 21:31:16,"@sigmavirus24 Actually, now that I think about it that's overly aggressive. Generally, things that are `float`-y or `int`-y should be totally acceptable. I think we just explicitly want to forbid `True`/`False`, because they happen to be `int`-y but really shouldn't be.
",Lukasa,sigmavirus24
2699,2015-07-30 21:33:06,"@shazow Not according to the docstring for the `Timeout` class, which is where this happens:



Now, admittedly, the docstring does not allow `True`/`False`, but those can be integer or float if evaluated in that context, so you may want to police them. I can kinda see that `timeout=False` putting the socket in non-blocking mode might make sense in some kind of weird world, but I cannot see how `timeout=True` should put the socket into one-second-timeout mode.
",Lukasa,shazow
2699,2015-07-30 21:37:24,"@Lukasa, this is working now that I'm policing the timeout on my side. For my end, this is fixed. Feel free to leave this open if you want to track the core issue, or close & make a separate one.

Thanks!
",offbyone,Lukasa
2699,2015-07-30 22:51:55,"@Lukasa I was thinking along the lines of



If people have something that should behave like an int/float/tuple they can cast it to that before giving it to us IMO. That said, if we want to eventually allow `Timeout` objects then that might be problematic, but we've never had a desire to allow that so I don't see that being a problem.
",sigmavirus24,Lukasa
2699,2015-08-31 07:00:46,"@sigmavirus24 Ah, crap, that doesn't work. Observe the majesty of Python:



I wonder if we should explicitly disallow anything that compares numerically equal to zero, because setting the socket to non-blocking mode is a wacky and bizarre thing to want to do. Alternatively, we could coerce anything that compares numerically equal to zero to `None` instead, which makes the concrete assertion that when you said `timeout=0` or `timeout=False` what you meant was ""don't time out"".

@shazow @kevinburke It's my assertion that we shouldn't let anyone set `0` as their timeout for either connection or read timeouts, because it doesn't mean what they think it means. We should either coerce to `None` or reject outright. Do either of you disagree?
",Lukasa,shazow
2699,2015-08-31 07:00:46,"@sigmavirus24 Ah, crap, that doesn't work. Observe the majesty of Python:



I wonder if we should explicitly disallow anything that compares numerically equal to zero, because setting the socket to non-blocking mode is a wacky and bizarre thing to want to do. Alternatively, we could coerce anything that compares numerically equal to zero to `None` instead, which makes the concrete assertion that when you said `timeout=0` or `timeout=False` what you meant was ""don't time out"".

@shazow @kevinburke It's my assertion that we shouldn't let anyone set `0` as their timeout for either connection or read timeouts, because it doesn't mean what they think it means. We should either coerce to `None` or reject outright. Do either of you disagree?
",Lukasa,sigmavirus24
2699,2015-09-01 01:32:51,"@Lukasa we could also just explicitly check for boolean before hand so we know we've handled that case. Yes, it is ugly, but we're trying to hide complexity as it is (which is what @offbyone is explaining by thinking aloud through this problem) so we can probably continue to do that.
",sigmavirus24,Lukasa
2699,2015-09-03 03:32:55,"@kevinburke There is one test (`test.with_dummyserver.test_connectionpool.TestConnectionPool.test_total_timeout`) that appears to expect a read timeout of 0 to work (specifically, to raise EAGAIN). This makes me think that maybe you wanted timeout of 0 to work for read. Is that right, or were you just asserting the actual behaviour of setting the timeout in that way?
",Lukasa,kevinburke
2699,2015-11-05 13:26:37,"Ping again @kevinburke.
",Lukasa,kevinburke
2699,2016-07-14 20:05:42,"@Lukasa @sigmavirus24 Is this something we still want patched? [Here](https://github.com/nateprewitt/requests/commit/6b7dd6ae4265cc308e6a7cdbc17fe4812e6caa67) is a quick pass at it, but I'm not sure if this should be PR'd against master or 3.0. There's also a minor side issue with urllib3 that it raises a `ValueError` for values < 0 stating that timeout can be set to 0 or greater.
",nateprewitt,Lukasa
2699,2016-07-14 20:05:42,"@Lukasa @sigmavirus24 Is this something we still want patched? [Here](https://github.com/nateprewitt/requests/commit/6b7dd6ae4265cc308e6a7cdbc17fe4812e6caa67) is a quick pass at it, but I'm not sure if this should be PR'd against master or 3.0. There's also a minor side issue with urllib3 that it raises a `ValueError` for values < 0 stating that timeout can be set to 0 or greater.
",nateprewitt,sigmavirus24
2693,2015-07-31 08:09:24,"@Lukasa I wanna ask though, how is `requests` sending the POST requests? My friend has a C code that sends with me to the same table and I just noticed that in one situation, he was sending while my code suddenly stopped sending for around a minute. I am guessing that my code got stuck in one POST request for that minute before it went through. We are both connected to the same network. 

In this case, is there a way to make this more efficient? If I put a timeout, it will just create connections during that minute and then all of them will be received at the same time!
",itsHaddad,Lukasa
2690,2015-07-24 18:21:45,"> You appear to be the first person to have checked this and found it to be an issue.

Oh, really? I thought I'm just too lazy to find an old and closed issue.

I just happen to have ""transfer.fsckObjects = true"" in my ~/.gitignore...

@sigmavirus24, yep, this looks like a very popular repo. Forking just for one commit would be a solution, but... Ok, I'll just disable the checks in .git/config after the checkout.
",anatolyborodin,sigmavirus24
2688,2015-07-26 01:40:19,"Oops, @tiran already confirmed that OpenSSL only accepts 8-bit paths, so there's no need to experiment to determine that.

The issues addressed by win_unicode_console are still a potentially confounding factor in some of the experiments above.

At this point, I'd say the suggestion of force encoding the cert path to UTF-8 (or UTF-8-BOM?) to try to trigger OpenSSL's encoding detection may be worth trying, but it will likely only work in this case if that actually prompts OpenSSL to switch to using the UTF-16-LE Windows APIs.
",ncoghlan,tiran
2688,2015-07-28 02:49:41,"I made an mistake. In python2, `os.getcwdu()` and unicode string should be used instead of normal version, so we can still get the path.

@alanhamlett 
It's hard to test this case, for old python which doesn't has `ssl.SSLContext`(like python 2.7), wrapper in `urllib3/util/ssl_.py` is used instead, which doesn't crash with `load_verify_locations`

---

BTW: Is `os.path.supports_unicode_filenames` should be considered for the case?

Sorry for my poor English, and I'm not an python programmer.
",yyjdelete,alanhamlett
2688,2015-08-24 05:37:31,"@alanhamlett
`os.path.supports_unicode_filenames` is `True` in both  py2 and py3
",yyjdelete,alanhamlett
2684,2015-07-22 05:59:26,"@sigmavirus24 the link you posted is interesting, but I don't see that it addresses:

> For instance, we can no longer return the response to the caller then inspect status_code.

The linked issue had an insightful explanation by @jvanasco of why this feature is needed, then no response... The issue was closed.

If `requests` means to live up to **http for humans**, then... (quoting the first post):

In keeping with simple is beautiful, it would be terrific if we could do:
`requests.get(url, max_response_size=1024*1024)`
",boolbag,sigmavirus24
2684,2015-07-22 20:53:49,"@Lukasa 

Thank you very much for your extraordinarily detailed and cogent reply. 

I fully get your point.

I'm more than happy to impose checks on `requests` from outside. I just thought that what I needed was not possible because I had misunderstood how to do it.

> For instance, we can no longer return the response to the caller then inspect status_code.

What I probably should have said there is that you can't return the response and inspect the text. I now see that I had misunderstood how to use `stream=True`. My understanding now is that `response.text` gets consumed while we stream, so we have to rebuild it separately, but the other attributes of the response are still available. So instead of returning the response, we can return a tuple with the response and the rebuilt text. Is that right?

Your idea of posting a recipe is a good one, as the few lines currently in the docs are not enough for a hobbyist like me.

The following seems to work. It follows a [recipe by Martijn Pieters](http://stackoverflow.com/q/23514256/), except that in Python 3 it only worked for me once I set `content` to a byte string.

Is this how you suggest it should be done?

To work with unicode inside instead of bytes, do I just `content.decode(r.encoding)` at the end? By the way for that page, it doesn't work. I thought `r.encoding` was the same as `chardet`, but I get something different and end up having to `content.decode(chardet.detect(content)['encoding'])` Maybe that detection discrepancy is specific to streams?



Thank you again for taking the time to explain the interface design.
",boolbag,Lukasa
2680,2015-07-19 09:56:59,"Yeah, this is _mostly_ true. I'm going to merge this as-is @peplin, but I'd really love it if you'd follow up with another pull request that adds a paragraph about _why_ you might want to add the `[]` (namely, because many servers expect that notation).

Thanks so much for the contribution! :cake: :sparkles: :cake:
",Lukasa,peplin
2678,2015-07-22 19:30:12,"@kennethreitz #2567 is already running on Debian (and maybe ubuntu?) against Requests 2.7.0 and has yet to produce any bug reports there. It's been running there for _at least_ a month. Also iirc, Cramer signed off on it and pip has been using something similar to #2567 for a while now.
",sigmavirus24,kennethreitz
2678,2015-09-09 04:12:35,"@sigmavirus24 IIRC Ubuntu either pulls from Debian testing or unstable during a new OS release cycle, so if #2567 was put in to one of those branches it will be in Ubuntu by now. Also you would be able to check the patches that are in the source for packaging as well. just my 2 cents
",L1ghtn1ng,sigmavirus24
2676,2015-07-17 11:31:10,"Thanks for the review @Lukasa and @shazow :) I've updated the PR now. 
https://github.com/kennethreitz/requests/blob/master/test_requests.py#L1275 fails for me on Python 3 because the keys are not in order, but looking at the implementation the keys are not expected to be in order? 
",sunu,shazow
2676,2015-07-17 11:31:10,"Thanks for the review @Lukasa and @shazow :) I've updated the PR now. 
https://github.com/kennethreitz/requests/blob/master/test_requests.py#L1275 fails for me on Python 3 because the keys are not in order, but looking at the implementation the keys are not expected to be in order? 
",sunu,Lukasa
2676,2015-07-17 12:13:37,"@Lukasa I've updated the PR.
",sunu,Lukasa
2675,2015-07-17 09:44:35,"@Lukasa Great! I'll submit a PR. Thanks for the quick response :)
",sunu,Lukasa
2675,2015-07-18 15:24:21,"Hmm .. I guess this makes sense. I'll handle this in the application level then. 
Closing the issue. Thanks for your time and input @Lukasa and @sigmavirus24 :)
",sunu,Lukasa
2675,2015-07-18 15:24:21,"Hmm .. I guess this makes sense. I'll handle this in the application level then. 
Closing the issue. Thanks for your time and input @Lukasa and @sigmavirus24 :)
",sunu,sigmavirus24
2672,2015-07-16 06:19:17,"Thanks @petedmarsh, I missed this when I changed the example from content-type to user-agent!
",Lukasa,petedmarsh
2666,2015-07-14 06:11:56,"@Lukasa I believe I've addressed your concern for that :)
",gvangool,Lukasa
2656,2015-06-29 03:24:18,"@sigmavirus24 thanks for the feedback.  Added a new commit to change the logic as suggested.
",dpursehouse,sigmavirus24
2656,2015-06-29 04:02:25,"Cool. LGTM. I'll let @Lukasa weigh in.

Thanks @dpursehouse 
",sigmavirus24,dpursehouse
2655,2015-09-01 08:32:21,"Ok, I've fixed the broken tests. I agree with you though @sigmavirus24, I think this is scary enough to want to go into 3.0.0, so I'm closing this PR and opening a new one with the correct target.
",Lukasa,sigmavirus24
2653,2015-06-27 19:08:29,"Hello, thanks for your answer.

@Lukasa 
This is both from response.text and from response.content:



@sigmavirus24 



Maybe it is also a problem that the Apache server sends the header in ISO-8859-1? But this would likely be a problem of the shared hoster setup, I guess?



I am sorry that I have obfuscated my original domain. Can I somehow privately send it to you? I would not really like to have it here on Github for all eternity, but of course I have no problem sending it directly to you so that you can check it out.
",TheHellstorm,Lukasa
2653,2015-06-27 19:08:29,"Hello, thanks for your answer.

@Lukasa 
This is both from response.text and from response.content:



@sigmavirus24 



Maybe it is also a problem that the Apache server sends the header in ISO-8859-1? But this would likely be a problem of the shared hoster setup, I guess?



I am sorry that I have obfuscated my original domain. Can I somehow privately send it to you? I would not really like to have it here on Github for all eternity, but of course I have no problem sending it directly to you so that you can check it out.
",TheHellstorm,sigmavirus24
2652,2015-06-25 12:17:16,"@t-8ch is right, this _is_ documented, just not very well. As to your point of `verify` vs `cert`, that's covered in the same section of the prose documentation that @t-8ch linked to.
",Lukasa,t-8ch
2649,2015-06-23 10:13:42,"@Lukasa I would like to provide traceback but how can I do that? I'm getting lots of exceptions because it's uses 200 threads. Generally like these but not in particular order.. And after it stucks and does nothing.

---

('Connection aborted.', gaierror(-2, 'Name or service not known'))
ERROR OCCURED:
karawanghosting.net
('Connection aborted.', gaierror(-2, 'Name or service not known'))
ERROR OCCURED:
bjjzk.net
HTTPConnectionPool(host='bjjzk.net', port=80): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<requests.packages.urllib3.connection.HTTPConnection object at 0x7f97dc0c1ad0>, 'Connection to bjjzk.net timed out. (connect timeout=3)'))

---
",sezginriggs,Lukasa
2649,2015-10-01 20:31:05,"@Lukasa, this snippet doesn't have timeout, but my call does. Edited the snippet anyway.
",eltermann,Lukasa
2649,2015-10-01 20:39:33,"@Lukasa, what do you recommend to print an useful stack? And where to place it?
",eltermann,Lukasa
2649,2016-08-27 15:39:48,"@Lukasa 

Right, I realized that, I am not a Python expert,  but I wonder what's the best practice of  sharing data (let's say a global task queue) between those process ?
",metrue,Lukasa
2647,2015-06-22 07:20:29,":joy: Thanks  @Lukasa 
",neosab,Lukasa
2646,2015-06-22 06:12:32,"Thanks @sigmavirus24  
",neosab,sigmavirus24
2645,2015-06-22 18:45:10,"@sigmavirus24 that sounds great to me. 

`if response.status_code in requests.codes.success`
",kennethreitz,sigmavirus24
2645,2016-05-16 04:02:01,"The solution proposed is to add a success attribute listing all the status codes that are considered ok (200, 201, 202, ..., 226). Then, we compare the status code of the response with the ones listed in success.

What happens is that the response for a 301 redirect, for example, is a status code 200. The same happens for other status codes of the 3xx family. @sigmavirus24 came with the idea that one of the reasons for that change is to narrow the definition of Response.ok. If that's the case, wouldn't it be better to rethink the outcome of those 3xx cases?
",AlexPHorta,sigmavirus24
2645,2016-05-16 07:10:21,"I think @sigmavirus24's point is just that 3XX status codes should probably not be considered ""ok"": they aren't. But requests _always_ follows redirects, so the only way to _end up_ at a 3XX status code is if the 3XX code is not an unambiguous redirect: i.e., it does not have a `Location` header.

The reason your testing showed no following for 304 and friends is because 304 is not a defined HTTP status code, so httpbin was not generating a Location header for it. That means requests ends up with the 304 code and cannot follow it any further. @sigmavirus24 rightly proposes that that response should not be ""ok"": it is almost certainly not.
",Lukasa,sigmavirus24
2645,2016-05-16 13:58:46,"I see, if a 301 redirect ends up on a successful retrieval of some
resource, then the final status is a 200 code.

Alright, my attempt is probably right. Tonight I'll make the PR, hope
everything is ok!

Thanks!
Am 16.05.2016 04:10 schrieb ""Cory Benfield"" notifications@github.com:

> I think @sigmavirus24 https://github.com/sigmavirus24's point is just
> that 3XX status codes should probably not be considered ""ok"": they aren't.
> But requests _always_ follows redirects, so the only way to _end up_ at a
> 3XX status code is if the 3XX code is not an unambiguous redirect: i.e., it
> does not have a Location header.
> 
> The reason your testing showed no following for 304 and friends is because
> 304 is not a defined HTTP status code, so httpbin was not generating a
> Location header for it. That means requests ends up with the 304 code and
> cannot follow it any further. @sigmavirus24
> https://github.com/sigmavirus24 right proposes that that response
> should not be ""ok"": it is almost certainly not.
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/2645#issuecomment-219363390
",AlexPHorta,sigmavirus24
2645,2016-06-15 08:12:30,"@Lukasa when designed, 'ok' meant 'not an error'. 300 is not an error. I still think should be the case. 

e.g. '.ok' isnt OMG ALL IS PERFECT, it is NOTHING APPEARS IMMEDIATELY BROKEN
",kennethreitz,Lukasa
2642,2015-06-15 15:41:28,"`BadStatusLine` is extremely unclear. It can occur in a number of situations, and makes no guarantees about whether the request made it through. It just fundamentally means httplib failed to parse the header line.

In _this_ instance it's unlikely that the server ""sent back the empty string"" because that's not a thing you can meaningfully do in TCP (I guess you could try emitting a zero length packet? not sure if that's valid TCP). What this means is that the connection got closed, likely by the shutdown of the server, without any data left in the send buffer. That will have caused a zero-length read.

A good example of why we _shouldn't_ blindly retry in this case is that plenty of servers will reject requests they don't like by just closing the connection, which will cause exactly this error. This kind of thing tells you nothing about whether the request succeeded. This means @kevinburke's advice is right (only retry idempotent requests), but not quite for the right reason.

</pedantry>
",Lukasa,kevinburke
2642,2015-06-18 12:27:49,"@Lukasa  thank you very much! I was not aware of the ""zero means any port"" method.

For the unix-sockets way I found this:

https://pypi.python.org/pypi/requests-unixsocket/
",guettli,Lukasa
2641,2015-06-15 09:21:23,"@Lukasa 

I agree with you, but the global shared socket connection, which is common in programming languages such as Java, is also practical. and perhaps adapters.py is more appropriate for doing this.
",duanhongyi,Lukasa
2641,2015-06-15 15:41:06,"I agree with @Lukasa. This will also likely negatively affect things like grequests which build on the api module's design. I also don't think we want this in `requests/adapters.py`.
",sigmavirus24,Lukasa
2636,2015-06-13 09:49:13,"@Lukasa I did always like future-you better than past-you.
",shazow,Lukasa
2634,2015-06-10 14:13:34,"I have to agree with @Lukasa. This is not on the roadmap for future versions of requests at this point.
",sigmavirus24,Lukasa
2630,2015-06-05 05:35:02,"@sigmavirus24

TLDR: Best practice python packaging.

Longer version:

The 'test' command is the canonical method for running tests in Python land (ala make check for autotools projects) independent to how or what tests are run, or the tools that get used.

It also makes for a great downstream (users and os packaging) experience so we/they can QA, orthogonally to how an upstream might do 'development' testing (read: tox, travis, whatever)

For me (and FreeBSD), I have the following in the port for requests, so that I can QA updates and commit changes quicker with confidence.


",koobs,sigmavirus24
2630,2015-06-05 13:24:34,"@Lukasa that command class is definitely what I had in mind. Without using py.test to _run_ the tests, only a fraction of them will be run since we have test functions that aren't attached to any test class that `unittest` could discover.

Also, regarding unverified TLS, I hope that setuptools has improved it but I'd be skeptical. It'd have to load certificates from somewhere and I doubt that it is also willing to vendor them. (The problem may be mostly alleviated on 2.7.9+ or 3.x, but that still leaves tons of people on systems without those versions in a lurch.) That said, I like being friendly to our downstream redistributors, so I'm happy to accept work to make `python setup.py test` work, but I think we need to actively discourage contributors from using it given the security concerns.
",sigmavirus24,Lukasa
2630,2016-01-30 05:34:25,"@kennethreitz Where is the full test suite and how is it run?
",koobs,kennethreitz
2625,2015-06-15 03:28:33,"@sigmavirus24 Pardon my poor English, but how could  `elapsed` be widely used and you weren't keen to expose it?

My rationale is like this, since `elapsed` is widely used, and it involves a system call to get start request time, so it'd better to directly expose the start request time as a property directly, so other developers can make more use of this measure without wrapping yet another get current time system calls. As you can see people use `requests` often involve with time based constraints and there would be a ton duplicated code.
",est,sigmavirus24
2622,2015-06-01 21:38:44,"@t-8ch has diagnosed this correctly. Sadly there isn't much we can do about this on our side beyond what has been described here.
",Lukasa,t-8ch
2622,2015-06-01 22:12:39,"@mgdelmonte That's extremely disingenuous, and I think you know that.

Chrome, Firefox, and Internet Explorer are browsers whose development is backed by the paid, full-time efforts of a team of developers. They own the entirety of their software stack, do not have backwards compatibility concerns with other libraries, and are beholden only to themselves for release schedules.

Requests is a tool whose development is funded by zero people. Three core developers devote more than 40 cumulative hours a week to this project, _for free_, with ancillary and extremely important contributions from people like @t-8ch which total probably another 10 hours, also _for free_. We do not control our entire software stack: many of the tools we build on top are are in the python standard library, like `httplib`, `cookielib`, `ssl`, and more. The fact that this project exists _at all_ is entirely down to the goodwill of a series of developers whose names you will never know and who received exactly no compensation for their work other than, if they are extremely lucky, the thanks of someone else.

As @sigmavirus24 points out, the bug is in httplib, which we rely on to do our HTTP parsing. Even then, I wouldn't call it a bug: there is a very clear specification for HTTP headers which is being violated by the server in question. At some point we need to stop tolerating the mistakes of others and say that something is simply not HTTP. This is particularly true of [Merrick Bank](http://www.merrickbank.com), a presumably financial institution that actually _did_ pay someone to build their website.

If you would like to get this fixed, please file a bug against httplib. More than that, please submit a patch. The last time we had one of these bugs was in #1804. For that bug I actually did provide a fix for `httplib`, which was merged more than one year after it was proposed. For that reason, I do not have particularly high hopes for this being fixed any time soon.

@mgdelmonte Please consider the way you act towards purely volunteer run projects. We are doing the best we can, but we need to pick our battles. If you'd like to pay one of us some money to address this bug, I'm sure we can come to some arrangement. Otherwise, we will pursue this in terms of the priorities of the requests project, and in those cases we promise nothing when the bug is upstream of us.
",Lukasa,t-8ch
2622,2015-06-01 22:12:39,"@mgdelmonte That's extremely disingenuous, and I think you know that.

Chrome, Firefox, and Internet Explorer are browsers whose development is backed by the paid, full-time efforts of a team of developers. They own the entirety of their software stack, do not have backwards compatibility concerns with other libraries, and are beholden only to themselves for release schedules.

Requests is a tool whose development is funded by zero people. Three core developers devote more than 40 cumulative hours a week to this project, _for free_, with ancillary and extremely important contributions from people like @t-8ch which total probably another 10 hours, also _for free_. We do not control our entire software stack: many of the tools we build on top are are in the python standard library, like `httplib`, `cookielib`, `ssl`, and more. The fact that this project exists _at all_ is entirely down to the goodwill of a series of developers whose names you will never know and who received exactly no compensation for their work other than, if they are extremely lucky, the thanks of someone else.

As @sigmavirus24 points out, the bug is in httplib, which we rely on to do our HTTP parsing. Even then, I wouldn't call it a bug: there is a very clear specification for HTTP headers which is being violated by the server in question. At some point we need to stop tolerating the mistakes of others and say that something is simply not HTTP. This is particularly true of [Merrick Bank](http://www.merrickbank.com), a presumably financial institution that actually _did_ pay someone to build their website.

If you would like to get this fixed, please file a bug against httplib. More than that, please submit a patch. The last time we had one of these bugs was in #1804. For that bug I actually did provide a fix for `httplib`, which was merged more than one year after it was proposed. For that reason, I do not have particularly high hopes for this being fixed any time soon.

@mgdelmonte Please consider the way you act towards purely volunteer run projects. We are doing the best we can, but we need to pick our battles. If you'd like to pay one of us some money to address this bug, I'm sure we can come to some arrangement. Otherwise, we will pursue this in terms of the priorities of the requests project, and in those cases we promise nothing when the bug is upstream of us.
",Lukasa,sigmavirus24
2622,2015-06-01 22:39:02,"@Lukasa my sincere apologies, and I do appreciate the value of the requests lib.  My point was more to mock Internet Explorer (which frankly surprises me if it handles anything less than perfectly clean data) than to deride your efforts.

I'll file a bug report with httplib, which appears to be the source.  It's hard to foresee someone breaking spec like Merrick has, but nevertheless it seems like an easy accommodation to make with no penalties elsewhere, to ignore a broken HTTP header line.
",mgdelmonte,Lukasa
2622,2015-06-02 15:12:50,"@Lukasa no problem, we're all pulling in the same direction (usually).  I submitted an issue for httplib here

http://bugs.python.org/issue24363

This is my first time submitting an issue through that system so I was a little mystified and wasn't sure how to submit a patch, so just recommended my fix.  I'm amazed there aren't more problems with readheaders() as it seems to completely ignore the requirement of a blank line to terminate the header.  Yes, you shouldn't have malformed header lines, either; but certainly it's easier to skip them and accommodate the mistakes of others than to terminate parsing altogether, especially when the specification has a clear and (mostly) unambiguous terminator.

Anyway, hopefully this clears it up for good.  Famous last words, right?
",mgdelmonte,Lukasa
2621,2015-06-02 08:11:47,"Oooof. Thanks @tiran. For the moment I'll leave the stdlib in your hands, I think we should update the algorithm we've back ported to add this support. I'll take a look at this today and see if I can extend it.
",Lukasa,tiran
2621,2015-06-02 12:59:37,"@tiran the use case is only for private CAs. Can't imagine a public CA would sign an IP, yes.
",itamarst,tiran
2621,2015-06-02 13:43:33,"@Lukasa do you think cURL handles this? Should we give Daniel a ping?
",sigmavirus24,Lukasa
2618,2015-06-02 01:08:05,"@t-8ch Thanks, your solution also works.
",gengjiawen,t-8ch
2617,2015-05-30 14:38:19,"@Lukasa thoughts on the last commit that I pushed?
",sigmavirus24,Lukasa
2617,2015-06-02 19:21:07,"@Lukasa updated with that approach.
",sigmavirus24,Lukasa
2615,2015-05-26 14:12:46,"Thanks @awiddersheim! :sparkles: :cake: :sparkles: 
",sigmavirus24,awiddersheim
2614,2015-05-26 19:56:18,"@colindickson Thanks for this! Rather than merge two identically named commits and a merge commit I rebased them down into fd31453aa25f80f04e1ce36de9abff0460bd136a, and merged it in 8b5e457b756b2ab4c02473f7a42c2e0201ecc7e9.

Thanks again! :cake: :sparkles:
",Lukasa,colindickson
2613,2015-05-26 13:10:02,"@bboe it's in your best interest to copy and paste `to_native_string` out of requests though. It's an undocumented function that's effectively meant to be internal to requests. If we move it around or change something in it, it could cause compatibility problems for you and there's no guarantee of backwards compatibility for that function as it isn't a defined member of the API.

That said, @Lukasa and I agree that it's highly unlikely to break, change, or disappear. So, while I'd prefer you to copy and paste it out, there's nothing I can do to enforce that. ;)
",sigmavirus24,bboe
2613,2015-05-26 13:10:02,"@bboe it's in your best interest to copy and paste `to_native_string` out of requests though. It's an undocumented function that's effectively meant to be internal to requests. If we move it around or change something in it, it could cause compatibility problems for you and there's no guarantee of backwards compatibility for that function as it isn't a defined member of the API.

That said, @Lukasa and I agree that it's highly unlikely to break, change, or disappear. So, while I'd prefer you to copy and paste it out, there's nothing I can do to enforce that. ;)
",sigmavirus24,Lukasa
2612,2015-05-25 14:56:45,"Hey @Lukasa !

I use this command. (http://justniffer.sourceforge.net/)
sudo justniffer -i eth0 -r > log_socket.txt

Here is the log related with request:
http://paste2.org/mhZCj2t7

and here is the log related with socket.

http://paste2.org/8ztZbD7K

what I see is, socket make two request. and requests just make one.

the important value for me is:
Set-Cookie: LOGIN_USERNAME_COOKIE=<ID_USERNAME>
Set-Cookie: WWV_CUSTOM-F_1279122985171442_102=8DA9C805E89B5CAE; path=/

because with that value I create another requests to get the content in the system.

I hope this help!

And thanks Lukasa!
",Wu4m4n,Lukasa
2609,2015-05-22 16:33:31,"@Lukasa  It is very difficult to using `objgraph` in my code.
Because the memory leak is slow and `objgraph` can't track every `requests` instance,
 it will create a lot of pictures, Which picture Useful?

Thank You for your help.
If I find the code of memory leaks, I will contact you again.
",030io,Lukasa
2609,2015-05-25 11:46:16,"@sigmavirus24 
I use `http.client` instead of `requests`. And the memory leak is gone.
",030io,sigmavirus24
2608,2015-05-21 08:16:19,"\o/ Thanks @radarhere! :cake: :sparkles:
",Lukasa,radarhere
2604,2015-05-17 20:48:29,"@Lukasa it's probably best to use print() for instructions like that in case the end user is on a distro which has changed the sym link for python to point to python 3 instead of 2.

As a side note, I generally recompile python after installing a new version of OpenSSL, so I was a little surprised to see my workstation here reporting the version as 1.0.2 instead of 1.0.2a and it turns out that the cryptography module requires a force reinstall to detect that change in addition to whatever is done to the python installation (which normally includes byte compiling installed modules), it would be quite easy for anyone to overlook that even after upgrading OpenSSL.
",Hasimir,Lukasa
2602,2015-05-15 09:42:55,"LGTM, but I'm going to wait for @sigmavirus24 on this one.
",Lukasa,sigmavirus24
2602,2015-05-15 15:14:56,"Hi @ly0 

I pulled down your PR and squashed those three commits with the same exact message. The fix is on master as of https://github.com/kennethreitz/requests/commit/ab1f493c8b6f82cbf80f8554b5fbbd02f2b2a363

Thanks!

:sparkles: :cake: :sparkles: 
",sigmavirus24,ly0
2598,2015-05-14 19:59:00,"ping @sigmavirus24 
",Lukasa,sigmavirus24
2595,2015-05-11 21:26:40,"@Lukasa The problem happens in requests 2.7.0 as well -- all released versions after 2.6.0. The problem is in urllib3 versions before https://github.com/shazow/urllib3/commit/22a9713fab2ed831204711906a974c3beba3319e, so would be fixed by merging in a more recent urllib3.
",agfor,Lukasa
2595,2015-05-11 23:02:02,"@sigmavirus24 @Lukasa  It looks like there are actually two different problems, and that commit only fixed one. With requests 2.6.2 / urllib3 before https://github.com/shazow/urllib3/commit/22a9713fab2ed831204711906a974c3beba3319e it blows up inside requests:



but after that commit / in 2.7.0, we blow up later inside the Braintree library. This appears to be because requests has tried to convert gzipped data to Unicode as if it had already been un-gzipped, and so replaced most of it with the unicode replacement character:



example unicode code points for the response body:



Versions 2.6.0 and below work.
",agfor,Lukasa
2595,2015-05-11 23:02:02,"@sigmavirus24 @Lukasa  It looks like there are actually two different problems, and that commit only fixed one. With requests 2.6.2 / urllib3 before https://github.com/shazow/urllib3/commit/22a9713fab2ed831204711906a974c3beba3319e it blows up inside requests:



but after that commit / in 2.7.0, we blow up later inside the Braintree library. This appears to be because requests has tried to convert gzipped data to Unicode as if it had already been un-gzipped, and so replaced most of it with the unicode replacement character:



example unicode code points for the response body:



Versions 2.6.0 and below work.
",agfor,sigmavirus24
2595,2015-05-12 15:20:53,"@sigmavirus24 @Lukasa I haven't been able to reproduce off of Google App Engine, so here is a minimal GAE repo you can test with the development environment that exhibits the problem: https://github.com/agfor/requests-2.7-appengine-fail

The core of it is:



It will dump the still-gzipped response body, and the un-gzipped response body, to the logs, then blow up on the `response.text.encode('ascii')` line, since `text` is producing gibberish full of unicode replacement characters from trying to encode gzipped data as unicode.
",agfor,Lukasa
2595,2015-05-12 15:20:53,"@sigmavirus24 @Lukasa I haven't been able to reproduce off of Google App Engine, so here is a minimal GAE repo you can test with the development environment that exhibits the problem: https://github.com/agfor/requests-2.7-appengine-fail

The core of it is:



It will dump the still-gzipped response body, and the un-gzipped response body, to the logs, then blow up on the `response.text.encode('ascii')` line, since `text` is producing gibberish full of unicode replacement characters from trying to encode gzipped data as unicode.
",agfor,sigmavirus24
2595,2015-05-12 17:06:39,"@shazow Just to close the loop, I was able to bisect the issue back to https://github.com/shazow/urllib3/commit/f21c2a2b73e4256ba2787f8470dbee6872987d2d specifically.
",agfor,shazow
2595,2015-05-12 17:25:25,"@agfor Ah good find, thanks.

Freakin' chunked encoding, breaking errything.

Does this mean GAE still works for non-chunked/non-streaming?

@sigmavirus24 Lovely idea, any interest in reaching out to the GAE team and asking if they'll fund this? :P
",shazow,sigmavirus24
2594,2015-05-13 06:56:30,"Thanks for your response.
This may be a solution.  Close this issue. I will come back to you if I got some useful error message with your method.

PS: It looks like our proxy problem. The code ran smoothly last night. Thanks, @Lukasa 
",fivejjs,Lukasa
2590,2016-02-02 12:44:54,"Thanks @Lukasa !
",borbamartin,Lukasa
2586,2015-05-03 14:55:58,"@sigmavirus24 Any objections to me merging this PR (manually without a merge commit, because we don't need it) and pushing the release?
",Lukasa,sigmavirus24
2585,2015-05-02 11:33:30,"@sigmavirus24 I'm beginning to wonder if we should have a 3.0.0 feature branch, and should start landing these things we keep deferring to 3.0.0.
",Lukasa,sigmavirus24
2585,2015-05-02 14:49:12,"@Lukasa I've been thinking of starting just such a branch in my fork. I wasn't sure if we would want to keep it here or elsewhere. While having people test it would be nice, having bug reports here about it might become confusing to some.
",sigmavirus24,Lukasa
2583,2015-05-01 14:35:07,"@Lukasa Thanks for the quick reply. I can confirm the error is gone in 2.6.1.
",htgoebel,Lukasa
2582,2015-05-01 05:59:45,"Thanks for this @mhavard999, it looks great! :cake:
",Lukasa,mhavard999
2580,2015-04-29 21:20:00,"@sigmavirus24 Thanks for the quick response with examples. I'll try these and report back...
",Microserf,sigmavirus24
2576,2015-04-29 06:10:51,"@sigmavirus24 Can users not provide a stricter policy by replacing the cookiejar on the `Session` object?
",Lukasa,sigmavirus24
2575,2015-04-28 15:26:27,"@t-8ch This is an odd dupe of shazow/urllib3#601 then, but I suppose it's possible. Definitely seems like the relevant smoking gun for now, so I'm closing to centralise there.
",Lukasa,t-8ch
2575,2015-04-28 15:44:09,"@Lukasa ofc you are right :-)



It gets another redirect with a content-length and then seems to wait for the body



Yields no redirect for me and also hangs.

cc @bagder
",t-8ch,Lukasa
2575,2015-04-28 16:00:34,"@t-8ch I'm sure @bagder will tell you you're not the first person to make this mistake, and you won't be the last. ;)

Yeah, that's the bug.
",Lukasa,t-8ch
2571,2015-04-25 10:33:38,"@Lukasa - there is no redirection in my php code. this is what the myapp index.php file looks like 


",bmutinda,Lukasa
2571,2015-04-25 10:49:50,"@Lukasa realized the same thing too. Just seen a note from this link http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html that 



The redirection might be in the server configuration if the fallback is not done in the requests lib..
",bmutinda,Lukasa
2571,2015-04-25 13:19:03,"@Lukasa do you have an alternative fix for this that would force the post request to be treated as a post no matter what?? Its an app that I am building that would require developers to put their urls so that I can ping their url with post params only. 
",bmutinda,Lukasa
2567,2015-04-24 13:12:42,"@untitaker why not just add it while I'm testing it?
",sigmavirus24,untitaker
2567,2015-05-03 23:15:47,"@untitaker thanks for working on this. Just for reference, here is what it's used in Debian at the moment:
https://anonscm.debian.org/viewvc/python-modules/packages/requests/trunk/debian/patches/04_make-requests.packages.urllib3-same-as-urllib3.patch?revision=32576&view=markup

Yes, I'm exporting only urllib3 since importing chardet from requests.packages seems not used, at least no one complained about this.

I choosed to not cherry pick  #2375 due the problems emerged: I don't want Debian and Ubuntu users to have a system version of requests not in the best shape: it will give only more problems.
",eriol,untitaker
2567,2015-05-04 07:31:21,"Seems good @eriol. Would you adopt a solution offered by requests though, to avoid Distro-specific breakage?

BTW @sigmavirus24 this is ready for review.
",untitaker,eriol
2567,2015-05-04 07:31:21,"Seems good @eriol. Would you adopt a solution offered by requests though, to avoid Distro-specific breakage?

BTW @sigmavirus24 this is ready for review.
",untitaker,sigmavirus24
2567,2015-05-04 18:26:05,"@untitaker of course I will like to drop a custom patch! :smile:
I had to write more to explain better my opinion, sorry for this. My opinion is that less divergence from upstream is the better, and it's for this reason that I forward all modification non Distro-specific to upstream.
Furthermore the Debian patch is not perfect, see [Debian Bug #771349](https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=771349).
So to recap I will be extremely happy to drop 04_make-requests.packages.urllib3-same-as-urllib3.patch in favor of an upstream solution.
Thanks!
",eriol,untitaker
2567,2015-05-09 19:55:46,"This works well for me. Objections to merging @Lukasa?
",sigmavirus24,Lukasa
2567,2015-05-10 12:31:44,"@sigmavirus24 I'm super reluctant to rush this out, given the way stuff previously went. Can we try to tap people who've had problems/objections in the past and get them to confirm that this works better for them?
",Lukasa,sigmavirus24
2567,2015-05-10 15:43:41,"@untitaker did you mean @mitsuhiko? I would hope a direct ping would suffice.

---

@Lukasa I would argue this is probably, on the whole, better than our previous meta_path hackery for several reasons:
1. It provides the same functionality
2. It's significantly simpler (simple is better than complex)
3. It allows for all of this to work in the case where requests is vendored without its vendored dependencies (see @untitaker's use of `__name__` when adding the alias to `sys.modules`)
4. It doesn't mess with other meta_path plugins (e.g., PyInstaller, the hack that @dcramer and @mitsuhiko are using, etc.)
5. It falls back in the correct order
6. It's far easier to explain to someone
7. When urllib3 is not vendored, the following works as we'd like it to:
   
   

I have yet to test this with PyInstaller, but the root of the problem there was the fact that our meta_path plugin was in the wrong place relative to the multiple plugins that PyInstaller uses. So with that removed, this should just work. I'm also confident that if @mitsuhiko and @dcramer test this with their code that hacks the meta_path, then they'll not see any problems.
",sigmavirus24,Lukasa
2567,2015-05-10 15:43:41,"@untitaker did you mean @mitsuhiko? I would hope a direct ping would suffice.

---

@Lukasa I would argue this is probably, on the whole, better than our previous meta_path hackery for several reasons:
1. It provides the same functionality
2. It's significantly simpler (simple is better than complex)
3. It allows for all of this to work in the case where requests is vendored without its vendored dependencies (see @untitaker's use of `__name__` when adding the alias to `sys.modules`)
4. It doesn't mess with other meta_path plugins (e.g., PyInstaller, the hack that @dcramer and @mitsuhiko are using, etc.)
5. It falls back in the correct order
6. It's far easier to explain to someone
7. When urllib3 is not vendored, the following works as we'd like it to:
   
   

I have yet to test this with PyInstaller, but the root of the problem there was the fact that our meta_path plugin was in the wrong place relative to the multiple plugins that PyInstaller uses. So with that removed, this should just work. I'm also confident that if @mitsuhiko and @dcramer test this with their code that hacks the meta_path, then they'll not see any problems.
",sigmavirus24,untitaker
2567,2015-05-10 16:04:52,"@sigmavirus24

> @untitaker did you mean @mitsuhiko? I would hope a direct ping would suffice.

Unfortunately I don't think so. I haven't heard back from him either, perhaps
@dcramer can ping him about that?

---

@eriol BTW this should also remove the need to rewrite every import statement
inside requests itself.
",untitaker,eriol
2567,2015-05-10 16:04:52,"@sigmavirus24

> @untitaker did you mean @mitsuhiko? I would hope a direct ping would suffice.

Unfortunately I don't think so. I haven't heard back from him either, perhaps
@dcramer can ping him about that?

---

@eriol BTW this should also remove the need to rewrite every import statement
inside requests itself.
",untitaker,sigmavirus24
2567,2015-05-10 16:04:52,"@sigmavirus24

> @untitaker did you mean @mitsuhiko? I would hope a direct ping would suffice.

Unfortunately I don't think so. I haven't heard back from him either, perhaps
@dcramer can ping him about that?

---

@eriol BTW this should also remove the need to rewrite every import statement
inside requests itself.
",untitaker,untitaker
2567,2015-05-10 17:34:27,"@sigmavirus24 I'm not disputing better at all. =) What I'd like to do is to take all reasonable precautions to reduce the risk of deploying this fix. For example, can @eriol and @ralphbean confirm that their package building functions correctly with this patch?

Basically, rushing helps nobody, and I'd like to try to begin a run of stable requests releases if at all possible. The last run of four-or-five broken releases in a row is bad, and we need to not get in the habit of doing that.
",Lukasa,eriol
2567,2015-05-10 17:34:27,"@sigmavirus24 I'm not disputing better at all. =) What I'd like to do is to take all reasonable precautions to reduce the risk of deploying this fix. For example, can @eriol and @ralphbean confirm that their package building functions correctly with this patch?

Basically, rushing helps nobody, and I'd like to try to begin a run of stable requests releases if at all possible. The last run of four-or-five broken releases in a row is bad, and we need to not get in the habit of doing that.
",Lukasa,sigmavirus24
2567,2015-05-10 18:21:15,"@Lukasa Note that I'm not arguing for this to be merged with a release to follow immediately after. We could even defer this to 3.0 if you want to not rush it (which I don't think we're doing frankly). The current state of affairs only makes end users' lives more difficult at this point.
",sigmavirus24,Lukasa
2567,2015-05-12 11:11:10,"@Lukasa if is it possible I would like to have a bit more of time: right now I'm checking reverse dependencies of requests in Debian to not break stuff, see: https://lists.debian.org/debian-python/2015/05/msg00021.html (@sigmavirus24 thanks for details, I did not have time to reply on the list, but your mail was appreciated :smile:).
I should finish during this week, so should be able to cherry pick and test this immediately after.
",eriol,Lukasa
2567,2015-05-12 11:11:10,"@Lukasa if is it possible I would like to have a bit more of time: right now I'm checking reverse dependencies of requests in Debian to not break stuff, see: https://lists.debian.org/debian-python/2015/05/msg00021.html (@sigmavirus24 thanks for details, I did not have time to reply on the list, but your mail was appreciated :smile:).
I should finish during this week, so should be able to cherry pick and test this immediately after.
",eriol,sigmavirus24
2567,2015-05-12 13:10:31,"@eriol always happy to help. We don't have a release planned so we don't have a timeline for when this would be merged. Take your time. Don't stress out. We'll be here and so will the patch. =D
",sigmavirus24,eriol
2567,2015-05-19 09:05:07,"@eriol  Ths!  I yet don't know. with command_line? 
",wangshaochen,eriol
2567,2015-05-27 12:50:35,"@Lukasa @sigmavirus24 any updates on this?
",untitaker,Lukasa
2567,2015-05-27 12:50:35,"@Lukasa @sigmavirus24 any updates on this?
",untitaker,sigmavirus24
2567,2015-05-27 13:02:45,"We're still hoping to have @eriol and/or @ralphbean take a swing at this code and confirm it's working for them.
",Lukasa,eriol
2567,2015-06-09 14:50:32,"Sorry for the delay! I have tested it and all seem fine! Many thanks @untitaker!
",eriol,untitaker
2567,2015-06-10 20:14:50,"I've done a fair bit of testing of this patch for Debian and I think it does exactly what we want it to do.  @eriol I've been able to remove the 02_user-system-chardet-and-urllib3.patch and 04_make-requests.packages.urllib3-same-as-urllib3.patch quilt patches and replace it with one that essentially does the PR submitted here.  AFAICT, it all looks pretty good.  I have a diff against the package that makes it all work, and I'd be inclined to upload a 2.7.0-3 with this patch, even if upstream requests puts it off until 2.8.  I'll open a Debian bug with additional details.

+1 for this PR from me.
",warsaw,eriol
2567,2015-06-30 16:00:56,"@untitaker to be honest, there are some differences. @dstufft mentioned that his patch includes something that handles `from pip._vendor import requests` as well as `import pip._vendor.requests` but I'm not sure we need that because I think ours just works as it's written. Perhaps @dstufft could explain more about what he found to be necessary in his patch that didn't work with this (assuming he tried this one in pip)
",sigmavirus24,untitaker
2567,2015-09-01 07:16:47,"I don't think we _can_ tell people not to import from urllib3: there are too many documents etc. around the web that indicate that we expected people to do that. It'll be really painful trying to discourage that behaviour in future, especially as we can't necessarily very easily deprecate/remove it.

I'm mostly interested to see if @kennethreitz is actually open to considering unvendoring. If he's not, this is the best solution we have so far and we should roll with it.
",Lukasa,kennethreitz
2567,2015-09-03 13:06:54,"@dstufft : +1
",warsaw,dstufft
2567,2015-09-16 20:24:22,"@untitaker yes, it's included in Debian (and Ubuntu) since 2.7.0-3. It was uploaded on 11 Jun 2015. I did not receive complains related to it yet.
",eriol,untitaker
2560,2015-04-23 06:20:45,"@kennethreitz In that case, what do you think of [this doc](https://hyper.readthedocs.org/en/latest/contributing.html).
",Lukasa,kennethreitz
2560,2015-04-23 16:08:20,"@Lukasa wonderful!
",kennethreitz,Lukasa
2558,2015-04-22 19:37:06,"@Lukasa I did try the referenced sha (which was master an hour or so ago) and it still exhibited the same issue. It's possible this is an issue in Logan, but the code there isn't very hacky (and I'm mostly certain its correct).
",dcramer,Lukasa
2556,2015-04-22 16:37:42,"Ah okay. I think we need to update our release process then to pull from the release branch unless we specifically need to pull from master. @Lukasa I think you have write access to my fork, would you mind updating our vendored copy of requests to use urllib3 from the ""release"" branch? If not, I'll do it over lunch or after work today.
",sigmavirus24,Lukasa
2556,2015-04-22 17:42:43,"Sadly, I'm not a collab on your repo @sigmavirus24. =( I opened a pull request instead: sigmavirus24/requests#3.
",Lukasa,sigmavirus24
2556,2015-04-22 21:33:09,"Alright @sigmavirus24, this all LGTM except that the changelog probably needs an update to say that we aren't fixing the machinery, we're just removing it wholesale.
",Lukasa,sigmavirus24
2556,2015-04-22 21:53:02,"> Alright @sigmavirus24, this all LGTM except that the changelog probably needs an update to say that we aren't fixing the machinery, we're just removing it wholesale.

:heavy_check_mark: 
",sigmavirus24,sigmavirus24
2556,2015-04-22 22:00:36,"Make it happen, @sigmavirus24. :shipit: 
",Lukasa,sigmavirus24
2556,2015-04-22 22:09:19,"Many thanks for the update, I will include it as a patch, and I will be very happy to help on making it robust, thread-safe... I will also put a note on README.Debian explaining why in Debian VendorAlias patch is used, but I don't know how many users look at /usr/share/doc: I will try to monitor all issues here but please ping me on a issue related to Debian. @sigmavirus24 many thanks for your offer, it's highly appreciated! :smile:

I will update requests for Debian during the weekend. Hopefully Debian Jessie will be released on saturday, so I will be able to backport 2.6.1 for Jessie as soon as it will migrate to Debian Testing.
",eriol,sigmavirus24
2554,2015-06-05 15:59:50,"Apologies for the slow response on this. After trying several different approaches, it seems the most flexible and cleanest approach is subclassing.

Here's an example that meets our particular needs of prepending a URL;



As for documentation, I'm thinking perhaps a ""recipes"" page of patterns and user contributed subclasses for achieving common goals, which aren't appropriate for inclusion into the core.

Thoughts @kennethreitz / @sigmavirus24 ?
",foxx,sigmavirus24
2554,2015-07-28 20:23:51,"@jaraco double checking the toolbelt, no it does not seem as if @foxx ever did.
",sigmavirus24,jaraco
2550,2015-04-17 12:44:36,"@sigmavirus24 send another pr with that :)
",kennethreitz,sigmavirus24
2547,2015-04-11 03:02:46,"Thanks @sh1buy! :sparkles: :cake: :sparkles:
",sigmavirus24,sh1buy
2543,2015-07-06 16:37:21,"@edsu what version of Python is installed and what version of requests is in use?
",sigmavirus24,edsu
2543,2015-07-07 13:08:47,"I agree with @Lukasa, this is something requests is expecting urllib3 to catch, wrap, and raise for us to do the same.
",sigmavirus24,Lukasa
2537,2015-04-09 17:15:41,"@kevinburke thanks for your response too. I'm actually already using timeout in my code, but I think that also controls how long the request can take. We have some requests that take quite a while like 25 seconds (not great form) based on some app requirements we have.

I'll see how hard it might be to add a connection_timeout option and maybe put in a request or pull request for that. 

:update looking at the code I immediately see something about connection timeout in a tuple of some sort.) So hopefully that works.

Thanks!!!!
",twiggy,kevinburke
2532,2015-04-06 23:02:22,"I have no remarks about `prepare_cookies` specifically, however...

Generally speaking, I found the adapter API for sessions and cookie handling difficult to work with, as you can tell by some of the odd things I'm doing in my own code. Using the private-sounding `_cookies` attribute feels icky, and having to pass the cookie from the first response so I can artificially merge it into the headers of the second response (rather than a simple `self.cookies.set(...)` or the like) is irritating.

There seem to be some disconnects between the adapter API and the concept of a Requests session (or even just an HTTP session). It's very possible such a connection was never intended in the first place, but it is frustrating nonetheless. If there is a way of refactoring my code to make it more idiomatic I would greatly appreciate any suggestions and advice.

@smiley I might be interested in merging this functionality into cloudflare-scrape. I assume this is for a competing CDN like Incapsula. Feel free to email me.
",Anorov,smiley
2528,2015-04-06 00:19:14,"@hartwork as @Lukasa has already kindly explained, PyPI is the source of truth. Not GitHub. You might notice that there are _several_ releases missing notes on GitHub.
",sigmavirus24,Lukasa
2527,2015-04-05 18:16:28,"Hi @sigmavirus24,
Thanks for quick response.  This situation is a little tricky that the base CookieJar class does not supply the 'copy' method but the RequestsCookieJar does. However, in the method 'prepare_cookies' of PreparedRequests, you permit all the instances of the subclass of CookieJar preserved as its original form(which may not contain 'copy' method). As I suppose, one uniformed way is to remove the copy method of  RequestsCookieJar and use the python copy function when copy is needed(This may need changes of other places). The other is more conservative, that is, restrict the self._cookies of PreparedRequest to be only the instance of RequestsCookieJar or else check whether having copy method before calling this method of a CookieJar instance.


",zhaoguixu,sigmavirus24
2526,2015-04-04 06:53:49,"Hi @Lukasa, I actually started by writing an auth handler. The main issue I ran into was that it only works for non-SSL requests. SSL requests through a proxy are made through a CONNECT tunnel, and the CONNECT request must be authenticated. The auth handler appears to be unable to hook into the proxy tunnel creation step, and therefore SSL requests will always fail.

Are there other ways to add this authentication without modifying the core? Maybe some more hooks around the proxy tunnel creation?
",justintime32,Lukasa
2525,2015-04-03 15:57:47,"\o/ This is beautiful @benjaminran! Thanks so much! :cake: :sparkles:
",Lukasa,benjaminran
2524,2015-04-02 19:30:59,"@Lukasa Thanks for fast reply!
",mktums,Lukasa
2523,2015-04-03 12:22:18,"@tardyp : You are correct. This needs further improvement, just as you point out there are other attributes currently shared accross threads that will lead to auth failures in certain conditions.

I'll extend the per-thread approach of num_401_calls to the remaining state-tracking attributes and update the PR for review.

Regarding the drawback you refer to I think it comes as a reasonable compromise:
- This approach won't hurt the currently working single-thread case.
- It guarantees correct authentication in a multi-threaded case (which requests claims to support).
- Simple and easy to understand code change.
",exvito,tardyp
2523,2015-04-03 13:15:09,"@exvito : @vincentxb and I are on the same team, and have story on our sprint to resolve this issue. I was working on it this morning, and am at the point of creating a unit test.
",tardyp,exvito
2523,2015-04-03 13:29:35,"Updated with commit e8d9bc5. Highlights:
- All state now in thread local storage.
- Factored out state initialization to `init_per_thread_state()` which must be called from `__init__()` for the regular, single-threaded case and, eventually, from `__call__()` in the cases where threads that did not create the auth handler invoke it.
- Maybe the `self.tl` name can be better: I opted for short, project leaders may prefer `self.thread_local` or some variation.
- Maybe the test for thread local state initialization in `__call__()` can take a different approach.

Here's a contribution. Thanks for your feedback.
PS: @tardyp does that mean you'll work from this PR or you'll take a different approach and suggest I drop this PR?
",exvito,tardyp
2523,2015-04-03 13:45:26,"@exvito, we worked on the same approach at the same time. 

I opted for the name thd for the local variable, and did the same ini_per_thread call. I think it is not necessery in **init** however, only in _call_

 :+1:  

I have a unit test on a separate commit:

https://github.com/tardyp/requests/commit/e15ed2c46823183b73250503fc44146f979eb891

Please cherry-pick it!
",tardyp,exvito
2523,2015-04-21 12:52:50,"@kennethreitz The unit test for it is, but the product code doesn't. It's just to make it safe when it's called from multiple threads.
",Lukasa,kennethreitz
2519,2016-09-14 15:29:49,"nice! thanks for the update, @Lukasa 
",traut,Lukasa
2518,2015-03-27 00:15:58,">  I think using the dollar and thus requiring the ""version"" line ends with a newline is a bad idea : it might end with a whitespace or a comment, it's valid.

No one asked for you to add anything other than the `^`.

Thanks for fixing this up. Any further feedback @Lukasa?
",sigmavirus24,Lukasa
2518,2015-03-27 12:49:49,"I misspoke twice then! Sorry for the confusion @deronnax. Thanks for the contribution! :cake: 
",sigmavirus24,deronnax
2516,2015-03-24 14:12:32,"@sigmavirus24 Which people would rather use simplejson? For what reason?
",ionelmc,sigmavirus24
2516,2015-03-24 14:56:57,"@ionelmc it was an oft-requested feature. simplejson supposedly has better performance. As for ""which people"" my job is not to catalogue all users of requests relying on _X_ feature. The people using this are likely easy to find in the issue history of requests. Searching ""simplejson"" should probably get you 98% of the way there.
",sigmavirus24,ionelmc
2516,2015-03-24 17:13:42,"@sigmavirus24 Seems your simplejson tests were incomplete, see https://github.com/simplejson/simplejson/issues/114 for details.

Victor Stinner [seemed to be able to reproduce the issue](https://gist.github.com/ionelmc/811bbf8ceed66ca83a2a). But alas, cpython devs aren't interested in fixing simplejson. Neither am I.
",ionelmc,sigmavirus24
2516,2015-04-02 18:48:48,"@kennethreitz they are in fact slightly different, the builtin json receiving serveral bugfixes simplejson didn't.
",ionelmc,kennethreitz
2513,2015-03-23 15:13:56,"@Montycarlo please see #2514. This should have been opened at [urllib3](/shazow/urllib3) and the change needs to be much better since this will introduce a breaking change to urllib3.
",sigmavirus24,Montycarlo
2513,2015-03-23 15:14:07,"Sorry @Montycarlo! I should have caught that earlier. =(
",Lukasa,Montycarlo
2511,2015-03-21 16:48:54,"Actually @Lukasa that's not exactly correct. See https://github.com/kennethreitz/requests/pull/2209 for more details.
",sigmavirus24,Lukasa
2511,2015-03-21 17:10:48,"To be clear, it seems to me that `elapsed` serves almost no purpose at all if it does not work as @Lukasa describes - measuring the total time taken is trivial to do in the application, but measuring the ""time to first byte"" is impossible if Requests does not provide that information itself.
",jribbens,Lukasa
2511,2015-03-22 13:00:57,"This sort of confusion is why I thought it important that the expected behaviour be documented ;-)
TL;DR for the below: it looks like the behaviour changed between versions of Requests and @sigmavirus24 used to be correct but now @Lukasa is correct...

I've set up two URLs. `redirect.php`:



`delay.php`:



With Requests 2.2.1, this is what happens:



With Requests 2.6.0, this happens:



So with `stream=True` the behaviour is consistent between versions, and matches what @Lukasa describes. With `stream=False`, in 2.2.1 `elapsed` changes from ""time to first byte"" to ""time to last byte"", but in Requests 2.6.0 there is no difference.

Can I humbly suggest that this change be regarded as a ""bug fix"" rather than a ""bug to fix""? The new behaviour is more logical, consistent and useful, and to change it would be to remove a feature (i.e. the ability to get TTFB with `stream=False`).
",jribbens,Lukasa
2511,2015-03-22 13:00:57,"This sort of confusion is why I thought it important that the expected behaviour be documented ;-)
TL;DR for the below: it looks like the behaviour changed between versions of Requests and @sigmavirus24 used to be correct but now @Lukasa is correct...

I've set up two URLs. `redirect.php`:



`delay.php`:



With Requests 2.2.1, this is what happens:



With Requests 2.6.0, this happens:



So with `stream=True` the behaviour is consistent between versions, and matches what @Lukasa describes. With `stream=False`, in 2.2.1 `elapsed` changes from ""time to first byte"" to ""time to last byte"", but in Requests 2.6.0 there is no difference.

Can I humbly suggest that this change be regarded as a ""bug fix"" rather than a ""bug to fix""? The new behaviour is more logical, consistent and useful, and to change it would be to remove a feature (i.e. the ability to get TTFB with `stream=False`).
",jribbens,sigmavirus24
2511,2015-03-22 20:19:30,"@sigmavirus24 you misread my comment. I said that time to _last_ byte may not be available, not that `elapsed` (time to _first_ byte) may not be available. And yes, that `sum` expression is exactly what I am using in my application to determine TTFB :-)
",jribbens,sigmavirus24
2511,2015-03-22 20:23:45,"Final question on this - @Lukasa's documentation change specifies that the DNS-lookup time and socket connect() time are excluded from `elapsed`, which seems peculiar if true. Is it true?
",jribbens,Lukasa
2504,2015-03-24 15:16:37,"> The one thing I'm thinking about is whether we want to allow for forward compatibility by actually storing these kwargs in a dictionary for resolve_redirects

Could you expand on that @Lukasa? I'm not quite certain what you mean.
",sigmavirus24,Lukasa
2504,2015-04-16 03:21:59,"I'm okay with this if you are @Lukasa.
",sigmavirus24,Lukasa
2499,2015-03-16 15:45:03,"@nanonyme Currently we can't achieve this: we just don't have the primitives. At the very least we would need shazow/urllib3#507 to get merged before this is even possible.

Additionally, the stdlib SSL module only allows this for some versions: specifically, those with SSLContext objects. That limits it to 2.7.9 and later, which is problematic.

Finally, of course, this doesn't solve the Windows problem since Windows users can't point OpenSSL at a file _period_, they need another solution.

Altogether, I think this is just too niche a request to satisfy at the moment. It seems like it would make life easier for a tiny minority of users, but that minority would be just as well served by a merge of sshazow/urllib3#507 and then playing with SSLContext objects.

Thanks for the suggestion, though! Please keep them coming. =)
",Lukasa,nanonyme
2497,2015-03-16 16:00:55,"Some lead time on the changes in 3bd8afbff would have been nice, yes.  @alex's recommendations look quite good to me as a 'maximal effort'.  If even portions of those suggestions could be adopted, it would be an improvement.  Thanks for the effort on this!
",ralphbean,alex
2497,2015-03-16 16:09:54,"I agree with @ralphbean and I too would like to thank your effort on this!
",eriol,ralphbean
2497,2015-03-16 20:12:13,"I'm entirely happy to adopt almost all of @alex's suggestions. I'll take a look at writing up a procedure at some point shortly and pass it around for consideration.
",Lukasa,alex
2497,2015-03-16 20:15:10,"FWIW, if you need those in a more prose version, you're welcome to any text
from http://cryptography.readthedocs.org/en/latest/security/

On Mon, Mar 16, 2015 at 4:12 PM, Cory Benfield notifications@github.com
wrote:

> I'm entirely happy to adopt almost all of @alex https://github.com/alex's
> suggestions. I'll take a look at writing up a procedure at some point
> shortly and pass it around for consideration.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/2497#issuecomment-81910438
> .

## 

""I disapprove of what you say, but I will defend to the death your right to
say it."" -- Evelyn Beatrice Hall (summarizing Voltaire)
""The people's good is the highest law."" -- Cicero
GPG Key fingerprint: 125F 5C67 DFE9 4084
",alex,alex
2497,2015-03-18 17:49:03,"@ralphbean Ah, yes, that was something I intended but clearly failed to make explicit in the document. I'll extend it to include it.
",Lukasa,ralphbean
2493,2015-03-15 18:44:00,"@yasoob Feel free to open a pull request adding yourself to AUTHORS.rst!
",Lukasa,yasoob
2490,2015-03-14 12:41:39,"@sigmavirus24 FYI, if you're planning to take this, I think the fix will actually go into urllib3 (or maybe even httplib, if it turns out httplib can't handle it).
",Lukasa,sigmavirus24
2490,2015-03-21 16:46:13,"@Lukasa you're correct.
",sigmavirus24,Lukasa
2487,2015-03-12 13:21:10,":cake: :star: :cookie: Thanks for this @ulope! Want to add yourself to AUTHORS.rst while we're here?
",Lukasa,ulope
2487,2015-03-14 11:19:03,"Beautiful! :cookie: :sparkles: :cake:

Thanks so much @ulope!
",Lukasa,ulope
2481,2015-03-10 14:39:39,"@Lukasa passing credentials to the proxy url does work indeed! I'd never figure it out on my own, thank you so much for the investigation!

PS: I linked to this ticket on the selected ServerFault answer in case someone stumbles on it too.
",gsakkis,Lukasa
2480,2015-03-09 14:19:57,"@rubentorresbonet if you choose to modify httplib like that, that's your decision. It's not up to our discretion nor is it our place to recommend it.

I think modifying a standard library is a poor way of handling this, but if that's what you choose to do, there's nothing for anyone to do about it.

Also @Lukasa didn't tell you to do _everything_ with sockets, only portions dealing with ICY.
",sigmavirus24,Lukasa
2479,2015-03-08 18:06:43,"@sigmavirus24 Thanks for prompt reply! Lemme write a script to reproduce this issue.. Gimme a day or so..

Thanks
",manojgudi,sigmavirus24
2478,2015-03-07 09:50:52,"@plaes This is a great PR. One small note however, we don't use `unittest`, we use `pytest`. This is why your test suite failed: `unittest.expectedFailure` doesn't exist in Python 2.6. Instead, use [`pytest.mark.xfail`](http://pytest.org/latest/skipping.html#mark-a-test-function-as-expected-to-fail).
",Lukasa,plaes
2478,2015-03-07 10:03:41,"Hurrah! Thanks so much for this @plaes!
",Lukasa,plaes
2478,2015-03-07 14:01:11,"Thanks @plaes!
",sigmavirus24,plaes
2476,2015-03-06 18:17:51,"So I took a quick look at the implementation of [`Response.iter_lines`](https://github.com/kennethreitz/requests/blob/master/requests/models.py#L693)

We read a chunk from the response, and then find the line within that chunk and return it. We store the remainder locally in the function and not on a the response object itself. I think an update to the docs like @Lukasa described is warranted. 
",sigmavirus24,Lukasa
2474,2015-03-05 12:27:29,"Thanks @dieterv, this is a known issue, see shazow/urllib3#561. We aim to push out a fix for this very shortly.
",Lukasa,dieterv
2468,2015-03-03 20:01:38,"Thanks @scholer! :cake: 
",sigmavirus24,scholer
2468,2015-03-03 20:37:13,"Thank you for accepting my pull request, @sigmavirus24  :)
",scholer,sigmavirus24
2467,2015-03-03 16:12:47,"@sigmavirus24 is entirely right. Testing against http2bin.org (which correctly returns a content-length) does not reveal this bug.

Focusing further on this behaviour, the problem is very, very deep. So far I've tracked it as low as line 371 of socket.py, in the `SocketIO.readinto` method, which looks like this:



This method calls `socket.recv_into` for non-SSL methods, and `SSLSocket.recv_into` for SSL. This is where the problem seems to lie. That recv_into method seems to block for quite a long time in the SSL case, presumably waiting on an underlying socket timeout.
",Lukasa,sigmavirus24
2467,2015-03-03 16:36:32,"So at this point, it would seen this is a stdlib/language-level bug, if I'm following your research correctly @Lukasa. In that case, we should continue this on bugs.python.org. With that in mind, @zsalzbank you absolutely will not be better served by waiting for requests to cut a release because there's nothing the library can do (I don't think) that will allow us to work around this for you given the level it seems to be at. If you'd like to wait longer, you can but I think you'll see better results in general by writing RFC compliant servers.
",sigmavirus24,Lukasa
2465,2015-03-29 14:39:26,"@sigmavirus24 Seems that we're still having some trouble here with our new vendoring logic. Care to take a look?
",Lukasa,sigmavirus24
2462,2015-03-06 07:14:24,"@sigmavirus24 What's the odds this is related to our import machinery?
",Lukasa,sigmavirus24
2462,2015-03-11 14:58:58,"@sigmavirus24 You've got more experience in this area than me. Thoughts?
",Lukasa,sigmavirus24
2460,2015-02-24 22:53:16,"To be clear, @Lukasa is talking about 



I'm rather surprised though that that would even be used. I thought that absolute imports on Python 3 would be preferring the stdlib's copy module.
",sigmavirus24,Lukasa
2460,2015-02-25 08:32:40,"@Lukasa Yes, this was a second python script, that used request module. Importing request library worked well in first file.
",theasder,Lukasa
2460,2015-02-25 18:47:17,"@sigmavirus24 hooray, it works, thank you!
",theasder,sigmavirus24
2455,2015-02-24 07:07:11,"@sigmavirus24 I would rather blame the new ciphersuite or cert bundle (not tested)
",t-8ch,sigmavirus24
2455,2015-02-24 07:21:11,"@Lukasa I can now reproduce it, the URLs are right, there seems to be an redirect
",t-8ch,Lukasa
2455,2015-02-24 07:29:37,"@Lukasa Are you running from a git checkout? It works for me there to, but not from a wheel, will try a sdist
",t-8ch,Lukasa
2455,2015-02-24 07:42:18,"@t-8ch What makes you say that?
",Lukasa,t-8ch
2455,2015-02-24 09:55:55,"@t-8ch To verify: does your bundle succeed with `s_client`?
",Lukasa,t-8ch
2455,2015-02-24 15:39:05,"Ok, @dstufft just realised his 2.7.9 success is a red herring, so the summary is this:
- The bundle works fine when used from the command line with openssl s_client
- It does not work when used from within Python
",Lukasa,dstufft
2455,2015-03-01 12:31:07,"@Lukasa 




",t-8ch,Lukasa
2455,2015-03-01 15:07:22,"I followed the same steps as @t-8ch (sans the checkout of 2.5.2) and got the same problem on OSX 10.10 with 1.0.1l
",sigmavirus24,t-8ch
2455,2015-03-01 16:56:51,"@dstufft There's some extra stuff here. In particular, we need a way to identify which certificates were removed because they're 1024 bit certs and which were removed for other reasons. Mozilla doesn't really publish that information, which makes it trickier than I'd like to do this in an automated way.

As for dropping the 1024-bit roots, I'm extremely nervous about doing that. We'll fall into a trap of ""this works in my browser but not with requests"" and I don't think ""It's OpenSSL's fault"" is a sufficiently good answer.
",Lukasa,dstufft
2455,2015-03-01 17:09:11,"@Lukasa yea, it could catch certificates removed for other reasons, though maybe the timing would mean that it's unlikey? Did all the 1024 bit roots get removed in one commit maybe?
",dstufft,Lukasa
2455,2015-03-01 18:31:12,"@dstufft Is there even a way to inspect the certificate chain? `getpeercert()` only gives you the leaf certificate.
",t-8ch,dstufft
2455,2015-03-01 19:48:19,"@sigmavirus24 has a better relationship with the distros than I do.
",Lukasa,sigmavirus24
2455,2015-04-06 14:02:00,"@Lukasa actually, I like that far better. I think we'll need to really work hard to publicize the decision properly. Should we plan the strong bundle for something like 2.8/3.0 so we have at least 2.7 as a release to use so we can really publicize the notion that 2.8/3.0 will break stuff unless they use certifi to work around it?
",sigmavirus24,Lukasa
2455,2015-04-06 20:20:41,"@sigmavirus24 Yeah, I think that would be a good idea. 

@asmeurer We can probably write a tool from the CLI, but ideally I'd add warnings into requests and I don't think we can easily do that.
",Lukasa,sigmavirus24
2455,2015-04-23 18:01:11,"Alright, fuck that. Here's my new plan.

The following are the Mozilla issues that track the removal of 1024 bit certs that I could find:
- [X] [856718](https://bugzilla.mozilla.org/show_bug.cgi?id=856718)
- [x] [881553](https://bugzilla.mozilla.org/show_bug.cgi?id=881553)
- [x] [936105](https://bugzilla.mozilla.org/show_bug.cgi?id=936105)
- [x] [936304](https://bugzilla.mozilla.org/show_bug.cgi?id=936304)
- [x] [986005](https://bugzilla.mozilla.org/show_bug.cgi?id=986005)
- [x] [986014](https://bugzilla.mozilla.org/show_bug.cgi?id=986014)
- [x] [986019](https://bugzilla.mozilla.org/show_bug.cgi?id=986019)
- [x] [1155279](https://bugzilla.mozilla.org/show_bug.cgi?id=1155279)

I plan to identify every certificate that was removed by these purges and check whether any of them are in the current requests cert bundle. If they are, I plan to restore them to the new certifi bundle.

Does anyone think this plan is bad? @reaperhulk @dstufft @alex @sigmavirus24
",Lukasa,sigmavirus24
2455,2015-04-23 18:01:11,"Alright, fuck that. Here's my new plan.

The following are the Mozilla issues that track the removal of 1024 bit certs that I could find:
- [X] [856718](https://bugzilla.mozilla.org/show_bug.cgi?id=856718)
- [x] [881553](https://bugzilla.mozilla.org/show_bug.cgi?id=881553)
- [x] [936105](https://bugzilla.mozilla.org/show_bug.cgi?id=936105)
- [x] [936304](https://bugzilla.mozilla.org/show_bug.cgi?id=936304)
- [x] [986005](https://bugzilla.mozilla.org/show_bug.cgi?id=986005)
- [x] [986014](https://bugzilla.mozilla.org/show_bug.cgi?id=986014)
- [x] [986019](https://bugzilla.mozilla.org/show_bug.cgi?id=986019)
- [x] [1155279](https://bugzilla.mozilla.org/show_bug.cgi?id=1155279)

I plan to identify every certificate that was removed by these purges and check whether any of them are in the current requests cert bundle. If they are, I plan to restore them to the new certifi bundle.

Does anyone think this plan is bad? @reaperhulk @dstufft @alex @sigmavirus24
",Lukasa,alex
2455,2015-04-23 18:01:11,"Alright, fuck that. Here's my new plan.

The following are the Mozilla issues that track the removal of 1024 bit certs that I could find:
- [X] [856718](https://bugzilla.mozilla.org/show_bug.cgi?id=856718)
- [x] [881553](https://bugzilla.mozilla.org/show_bug.cgi?id=881553)
- [x] [936105](https://bugzilla.mozilla.org/show_bug.cgi?id=936105)
- [x] [936304](https://bugzilla.mozilla.org/show_bug.cgi?id=936304)
- [x] [986005](https://bugzilla.mozilla.org/show_bug.cgi?id=986005)
- [x] [986014](https://bugzilla.mozilla.org/show_bug.cgi?id=986014)
- [x] [986019](https://bugzilla.mozilla.org/show_bug.cgi?id=986019)
- [x] [1155279](https://bugzilla.mozilla.org/show_bug.cgi?id=1155279)

I plan to identify every certificate that was removed by these purges and check whether any of them are in the current requests cert bundle. If they are, I plan to restore them to the new certifi bundle.

Does anyone think this plan is bad? @reaperhulk @dstufft @alex @sigmavirus24
",Lukasa,dstufft
2455,2015-04-23 18:42:28,"(Oh, /cc @t-8ch as well).
",Lukasa,t-8ch
2454,2015-02-22 12:44:31,"@Lukasa Instead of trusting the intermediate, would it not be easier then to use a single self-signed cert and thrust it directly or just pin the fingerprint?
",t-8ch,Lukasa
2450,2015-02-20 02:02:07,"Thanks @iKevinY !
",sigmavirus24,iKevinY
2449,2015-03-15 10:31:39,"@Lukasa Since it's in urllib3 now, how we can take advantage of it?
",TomasTomecek,Lukasa
2446,2015-10-14 20:04:25,"@Lukasa:
I'm wondering if it's possible to use Requests to decode a gzip-encoded file (e.g. not-for-transport), or would that not be in the spirit of the library (file provenance and all)?

I've got a large number of gzipped text file to download then parse (that would be amazing to just stream), and if it was the server doing the encoding, it seems like Requests could handle it no problem, but if the file's already in that form, would it work?
",riordan,Lukasa
2446,2015-10-16 17:40:03,"@sigmavirus24 @Lukasa This is _awesome_ thank you. That's exactly what I wound up doing.
",riordan,Lukasa
2446,2015-10-16 17:40:03,"@sigmavirus24 @Lukasa This is _awesome_ thank you. That's exactly what I wound up doing.
",riordan,sigmavirus24
2444,2015-02-11 19:07:03,"@sigmavirus24 sounds good- I updated to HEAD, but feel free to drop this PR in favor of another one if this is in flight elsewhere.
",Yasumoto,sigmavirus24
2444,2015-02-24 15:27:40,"Thanks @sigmavirus24 !
",Yasumoto,sigmavirus24
2441,2015-02-08 16:05:46,"@sigmavirus24 Well, I wonder if we could take both files and json. It's a common case to upload files with json meta data, so maybe it makes sense to have a nice api for this.
",andreif,sigmavirus24
2441,2015-02-08 16:50:09,"@Lukasa Alright. Should I change to `ValueError` in this case?
",andreif,Lukasa
2441,2015-02-10 14:17:43,"@sigmavirus24 I took existing raise as an example. Should I change all of them to `ValueError` or should I keep the original as it is?


",andreif,sigmavirus24
2434,2015-02-02 19:57:56,"@shazow Maybe not?  Looking at the code in `urllib3.response`, `self._fp` is set from `body`, which could be a string type instead of a file pointer.

...oh wait, that's a lie, it couldn't be a string type in that case, but it only checks for a `read` method so it might not have `fileno()`, I guess.
",larsks,shazow
2431,2016-04-06 19:25:35,"This PR hasn't seen any love in a long time. @ianepperson, would mind performing a rebase to make this once-again mergable? 

@sigmavirus24 @Lukasa +1? -1? I want to get this merged or closed out. 
",kennethreitz,sigmavirus24
2429,2015-01-28 14:59:10,"Added you @Lukasa and added a bit more detail about vulnerability reporting.
",sigmavirus24,Lukasa
2429,2015-03-04 13:51:18,"@Lukasa ping ;)
",sigmavirus24,Lukasa
2429,2015-03-04 14:03:30,"@sigmavirus24 Is it worth adding GPG key fingerprints for you and I?
",Lukasa,sigmavirus24
2427,2015-01-26 05:54:36,"I haven't read this yet, and I'm certain it really is this simple, but I want to re read the RFC about this regardless to make sure this applies to all cases. I suppose we've never had trouble with this because people were using URIs that had paths when authenticating. It doesn't hurt to be safe though. =)

Thanks for your patience @luozhaoyu 
",sigmavirus24,luozhaoyu
2427,2015-01-26 07:00:17,"@sigmavirus24 Sure! It is rare because I could only reproduce it when use a proxy and encounter a redirect...
",luozhaoyu,sigmavirus24
2427,2015-04-06 03:32:08,"I'm comfortable with this. Sorry for the delay @luozhaoyu. I lost track of this.

Thoughts @Lukasa?
",sigmavirus24,luozhaoyu
2427,2015-04-06 15:19:21,"Thanks @luozhaoyu 
",sigmavirus24,luozhaoyu
2427,2015-04-06 15:27:07,"@sigmavirus24 You're welcome!
",luozhaoyu,sigmavirus24
2424,2015-01-25 18:11:17,"> I think the reality is that if this module enters the standard library the current core team will move on from it. I certainly have little interest following it into the quagmire that is core dev. The most likely to steward requests in the stdlib is @sigmavirus24, and he's just one man. That loss of direction will inevitably lead to an erosion of the library's interface over time, and I think that would be a tragic thing.

I would wander into the stdlib to try to help, but given the fact that exactly one of I don't know how many previous patchsets I've submitted has been accepted and one other _reviewed_ makes me wary of wanting to bother with that process. I know the core devs are entirely swamped by more important things. I also know someone else has decided randomly that they want to maintain httplib/http but they're clearly not suited for the job (yet) and I don't have the patience to work on httplib when patches that both @Lukasa and I sit around, unreviewed, and not cared about (when they fix pressing issues with the library).

I'd probably end up just forking requests to continue using it.

> requests is absolutely unsuitable for stdlib inclusion for the many reasons stated before me. The urllib3 dependency alone is a complete showstopper; we don’t want it to got to die in the stdlib.

It's always been a contention of @kennethreitz (and therefore, the project as a whole) that urllib3 is an implementation detail. Many of requests' biggest features are handled entirely by urllib3, but it doesn't mean they couldn't be reimplemented with care into truly dependency-less library.

Regarding the chardet dependency: it's been nothing but a headache to us (and to me specifically). It used to have separate codebases for py2 and py3 until I got it into a single codebase library (which has only in the last several months been merged back into chardet proper). The library is slow and a huge memory hog (which angers many people to the point of yelling at us here on the issue tracker). It's not entirely accurate and Mozilla's universalchardet that it is modeled after has all but been abandoned by Mozilla. So removing chardet would probably be a net positive anyway.

Regarding whether we should do this or not, I'm frankly unconcerned. Whatever would be in the stdlib would end up being requests in API only. The Python 3 adoption rate is slow enough that I don't think people will be meaningfully affected by this for the next N years (where N is the globally unknown number of years for 3.5 to be used in production by corporations).

And like I said, I'd probably end up just forking requests or using urllib3 directly at that point.
",sigmavirus24,kennethreitz
2424,2015-01-25 18:11:17,"> I think the reality is that if this module enters the standard library the current core team will move on from it. I certainly have little interest following it into the quagmire that is core dev. The most likely to steward requests in the stdlib is @sigmavirus24, and he's just one man. That loss of direction will inevitably lead to an erosion of the library's interface over time, and I think that would be a tragic thing.

I would wander into the stdlib to try to help, but given the fact that exactly one of I don't know how many previous patchsets I've submitted has been accepted and one other _reviewed_ makes me wary of wanting to bother with that process. I know the core devs are entirely swamped by more important things. I also know someone else has decided randomly that they want to maintain httplib/http but they're clearly not suited for the job (yet) and I don't have the patience to work on httplib when patches that both @Lukasa and I sit around, unreviewed, and not cared about (when they fix pressing issues with the library).

I'd probably end up just forking requests to continue using it.

> requests is absolutely unsuitable for stdlib inclusion for the many reasons stated before me. The urllib3 dependency alone is a complete showstopper; we don’t want it to got to die in the stdlib.

It's always been a contention of @kennethreitz (and therefore, the project as a whole) that urllib3 is an implementation detail. Many of requests' biggest features are handled entirely by urllib3, but it doesn't mean they couldn't be reimplemented with care into truly dependency-less library.

Regarding the chardet dependency: it's been nothing but a headache to us (and to me specifically). It used to have separate codebases for py2 and py3 until I got it into a single codebase library (which has only in the last several months been merged back into chardet proper). The library is slow and a huge memory hog (which angers many people to the point of yelling at us here on the issue tracker). It's not entirely accurate and Mozilla's universalchardet that it is modeled after has all but been abandoned by Mozilla. So removing chardet would probably be a net positive anyway.

Regarding whether we should do this or not, I'm frankly unconcerned. Whatever would be in the stdlib would end up being requests in API only. The Python 3 adoption rate is slow enough that I don't think people will be meaningfully affected by this for the next N years (where N is the globally unknown number of years for 3.5 to be used in production by corporations).

And like I said, I'd probably end up just forking requests or using urllib3 directly at that point.
",sigmavirus24,Lukasa
2424,2015-01-25 18:14:13,"I discussed this at length with Guido the other day — chardet would have to be included first. I think that urllib3 and requests could be included into the http package together. 

However, I'm very inclined to agree with @hynek and @dstufft. Perhaps requests is fine just the way it is :)
",kennethreitz,dstufft
2424,2015-01-25 21:51:04,"I too have spoken with Guido about tossing urllib3 into the stdlib some years ago with the conclusion that it's not a great idea, but I'm fairly neutral about it at this point.

urllib3's API has been mostly-stable and pretty much completely backwards compatible for several years now. Its' pace is possibly even slower than that of the stdlib today, with the vast majority of changes being minor fixes or security improvements (with occasional backwards-compatible feature additions like granular timeouts/retries). If somebody really wanted to try and get urllib3 into the standard library, I don't think it's a terrible idea—it's just not the _best_ idea.

(I'm not speaking for requests, as it moves at a very different pace with different goals than urllib3.)

The best idea, in my opinion, would be for the PSF to hire (or maybe Kickstart or something) 1-3 developers to build out a brand new http library on top of asyncio with HTTP/2 support with heavy inspiration from urllib3, requests, and hyper. I'd be happy to see as much code taken verbatim as possible but laid out in a consistent, modular, and reusable manner. Ideally target Python 4 or something, and get rid of all the urllibs and httplibs. I expect this would be 6-9mo of hard work, but possibly more.

The very worst part about urllib3, which I'd love to see replaced if somebody attempts to rewrite it per @sigmavirus24's suggestion, is that it depends on httplib. urllib3's functionality is substantially limited with lots of code spent working around shortcomings of httplib. Though if HTTP/2 support would be taken seriously in this goal, then the scope of re-implementing HTTP/1.1 would be a very comforting fraction of the work required.

Many PyCons ago, a bunch of us met up and whiteboarded a layout of a brand new http library that refactors all the pieces into the ideal arrangement we could imagine at the time. I'd be happy to dig up these notes if anyone is going to attempt this.
",shazow,sigmavirus24
2424,2015-01-25 23:12:20,"+1 @shazow

Again, if anyone finds themselves with the time and inclination to take on that fairly large project, I've sketched out a putative API design that might make a good starting point.
",Lukasa,shazow
2424,2015-01-26 02:00:17,"@dstufft this is in projects that generally don't, where everyone can't be bothered to figure out how to use urllib. (people aren't adding it as a dep because of ssl/etc generally, but out of laziness)
",dcramer,dstufft
2424,2015-01-26 02:01:13,"@dstufft also multi-version deps basically make it hard to use things in libraries. You probably want to use requests in your project and if we require it then there's a potential for a world of hurt if API changes happen in versions.
",dcramer,dstufft
2424,2015-01-26 02:38:44,"@sigmavirus24 I disagree. requests has had its API change in the past. APIs change, thats why we have versioning, thats why dependencies are complex. This is a perfect case for that discussion because requests is a dependency in a lot of projects.

If you move into the stdlib the API must be stable.
",dcramer,sigmavirus24
2424,2015-01-26 03:02:46,"@sigmavirus24 you're now purely turning this into an API debate. I was just pointing out the reason I dont include it is because it can change, and everyone uses it, and everyone uses different versions. It's great that you guys never change your API but I have no desire or time to follow it or assume thats true.
",dcramer,sigmavirus24
2424,2015-01-26 04:25:28,"> @dcramer 
> Not sure what you're all bitching about.

Very appropriate language for this debate. When legitimate counters are made to your position, you use a slur meant to demean women. Par for the course though. Moving on,

---

@ncoghlan 

Re point 1: I think the documentation would be drastically simplified with requests (requests-alike) in the stdlib. One of the first things I do when learning a new language is figure out how do HTTP. Having that featured is something the guide would benefit from regardless.

Re point 2: There's a difference between the API and the library being de facto vs. de jure. The API could easily be provided by the standard library. I think your concern about support would be more aimed towards requests (the code) being included.

Re points 3 & 4: I'm not sure that's something to be discussed here. Maybe python-ideas would be better.

> As far as the idea of PSF managed development work goes, I'd be heavily against that, as we don't have the management infrastructure in place to handle that kind of thing.

That's interesting. I didn't think it was a probability but it'd be great to have something better than http(lib). 

> With the standard library, we generally don't have that concern - while redistributors occasionally break things, in a commercial context, vendors breaking stuff that works upstream gives you quite a lot of leverage to get the offending vendor to fix things.

I'm not sure what leverage you're talking about. I've seen ensurepip, venv, and other things broken by Debian and other redistributors in CPython. That's tangential to this discussion though.

> Oh, one other key question to answer before volunteering to actually maintain stuff in the standard library: are you prepared to accept the responsibility of shipping software that helps power half the world's stock exchanges, is one of the most popular languages for corporate infrastructure, one of the most popular languages for scientific programming (including being used for trajectory planning on inter-planetary space missions), one of the most popular languages for web development, and one of the key languages being employed in new computer literacy initiatives in educational institutions?

As already mentioned, most of the people currently involved wouldn't continue with the project. I'd probably be the only person but given my track record with upstream CPython development, it wouldn't be productive leaving that burden to the existing (and other future) core developers.

> That post-chasm portion of the technology adoption curve are the ones we reach when we say ""yes, this approach is now sufficiently mature that we can push it, or something based on it, into the standard library to help make it a truly universal assumption"".

The reality is that those people will never catch up though, no? People are still running software on Python 2.4 and 2.5. F5's load balancers still only support Python 2.5. 2.7 will be in use probably until the end of my natural life (which I hope will be quite long). Are these really the people this decision will affect most strongly? Those same people you describe may never make the leap to Python 3. And currently, it still is a _leap_. Maybe by the time they've decided to consider it, Python 3.8 or 3.9 or 4.2 will be out and will be much less of a hassle for them.
",sigmavirus24,ncoghlan
2424,2015-01-26 16:33:15,"@sigmavirus24 all is well :)
",kennethreitz,sigmavirus24
2423,2015-01-24 14:56:38,"Yea what @sigmavirus24 said. Some systems unbundle requests from pip.
",dstufft,sigmavirus24
2423,2015-01-24 14:59:55,"@dstufft is this maybe related to debian preventing pip from removing packages installed by the system package manager?
",sigmavirus24,dstufft
2422,2015-03-23 01:10:40,"@sigmavirus24 

I have gone a bit further in research and this is what I have found.

Basically, it's this:



Environment details:
- Python==2.7.3
- requests==2.5.1

I managed to reproduce the problem exactly. In my case, the endpoint is a multi-lingual Django site. Requests to URL without the language prefix (like ""http://example.com/foo/endpoint/"") do get redirected to the same URL with the language prefix (like ""http://example.com/en/foo/endpoint/"").

If `some_url` is ""http://example.com/foo/endpoint/"" I get an error:

('Connection aborted.', error(32, 'Broken pipe'))

If `some_url` is ""http://example.com/en/foo/endpoint/"" it all goes fine. Likely, `requests` doesn't handle large post requests to endpoints which do redirect them to some other endpoints.

I have just tried it with requests==2.6.0 and I do get the same problem.
",barseghyanartur,sigmavirus24
2422,2015-03-23 14:39:35,"@Lukasa or if we change the method (i.e., in a case like 7231 Section 6.6.4) without stripping the body/content-length. (or strip the body and not the content-length).
",sigmavirus24,Lukasa
2422,2015-03-23 15:03:32,"@sigmavirus24 

Hey, thanks for getting back on this. I'll post it here quite soon after I get home. :) That's gonna be in about 5 hours.
",barseghyanartur,sigmavirus24
2422,2015-03-24 22:13:19,"@sigmavirus24 

I don't get any response back, since it fails hard.



And this is the code:


",barseghyanartur,sigmavirus24
2422,2015-03-25 16:58:14,"@sigmavirus24 

Ah, sorry, I didn't realise you were asking for that. It was 301, as far as I remember.
",barseghyanartur,sigmavirus24
2422,2015-04-07 23:31:47,"@jazzfan thanks for doing that. I'll look tonight and I suspect that @Lukasa will look when he wakes up
",sigmavirus24,Lukasa
2422,2015-04-08 00:24:36,"@sigmavirus24 You forget that I am in NYC at the minute, and so am (relatively) awake!

As I've said elsewhere, EPIPE almost always comes when we attempt to send on a connection that has been remote-closed.

I think it would be most interesting if you could run a patch in requests' vendored copy of urllib3 that would throw the socket object up with the `ConnectionError`, and then read from it. The exact patch to apply here is a bit unclear and overlaps with some other stuff we've been thinking about when it comes to EPIPE.

Alternatively, a non-SSL repro would work!
",Lukasa,sigmavirus24
2422,2015-04-08 00:39:46,"@Lukasa, does this mean that the SSL tcpdump file is essentially useless?  Also, I'm afraid I'm too much of a newbie to be able to figure this patch you're describing, especially if it's unclear to you.  

I guess I'll rig up some kind of little test HTTP server on my localhost and see if I can reproduce the error using that.
",JazzFan,Lukasa
2422,2015-04-08 06:50:31,"@Lukasa 

Well, one thing is clear - it happens only when large amounts of data (files) and transferred.
",barseghyanartur,Lukasa
2422,2015-04-08 12:24:10,"@Lukasa 

Are you interested to know what happens if 20 Mb plain text (textarea) is posted?
",barseghyanartur,Lukasa
2422,2015-04-09 16:08:11,"Oops, I forgot to include @Lukasa in my previous comments.
",JazzFan,Lukasa
2422,2015-04-11 02:09:18,"@Lukasa, please don't let me take you away from PyCon!

I think you are right about needing to drain the POST data.  When I change my test server's `do_POST` method from this:



to this, by adding code to read all of the input stream for this post request:



then the error goes away; `requests` does not terminate in a stack trace.  

So based on my evidence, when `requests` abnormally terminates, it is due to a problem in the server implementation, not in `requests` itself.  Maybe other people have evidence that it's a problem in `requests`, but on the face of it, I don't.   Sorry to have troubled you!
",JazzFan,Lukasa
2422,2015-07-30 15:57:04,"@Lukasa I am seeing a similar issue, but the cause is from nginx immediately returning a `413 Request Entity Too Large`. RFC2616 says this about 413:

> The server _MAY close the connection_ to prevent the client from continuing the request. 

The server closing the connection is the behaviour I am seeing, and requests (actually, I think the issue is in Python's `httplib`) is not handling that case.

I have a server on EC2 that can be used for testing against. nginx is configured with a `max_client_body_size` of `1k`, and the Python server behind nginx is the example posted above in this issue (with the modification to consume the whole body). The catch is that nginx doesn't proxy the request to the Python server due to the `413` error. You can POST to `http://52.18.181.108/`.
In my case, when uploading a file larger than roughly 75MB I see this issue.

Using wireshark, I see the below behaviour, where nginx responds upfront with a 413, but Python httplib doesn't handle the response until the very end of the request (happily uploading the whole file while the server has already rejected the request). If the file is large enough, nginx will terminate the connection before httplib/urllib3 handles the response, resulting in either a `Broken Pipe`, or a `Connection reset by peer`.



cc @cournape
",sjagoe,Lukasa
2422,2016-06-29 13:56:19,"Thanks @Lukasa ! May i ask if there is workaround ? I didn't run into broken pipe issue by using httplib2 by issuing the same request with the same payload.
",chenziliang,Lukasa
2417,2015-01-20 18:08:44,"@Lukasa : I am following a well known practice that is detailed in Idiomatic Python author:
http://www.jeffknupp.com/blog/2013/11/15/supercharge-your-python-developers/
Also detailed in Google coding standards.
FWIW:  pylint has caught many errors in my code and others in the past.
While we may differ in our views, not placing value judgements.  Just a data point.
Like I said in my prior post, my intent is to weight differing approaches.  Request at the top of that list.

Was hoping that this kind ask would be more open to possible changes... looks like closed for discussion already.

If you guys are open, I'd be open to making changes once I had more time.
But then that would be un-welcomed based on your statement too.
my 2 cents
",DavidHwu,Lukasa
2417,2015-01-23 13:53:41,"@Lukasa 
Are you open to annotating source with comments disabling pylint warnings? Given that one [can](http://docs.pylint.org/faq.html#do-i-have-to-remember-all-these-numbers) use symbolic names of warnings/errors such annotation would serve as documentation at the same time which should be useful.
",piotr-dobrogost,Lukasa
2416,2015-01-20 14:56:05,"Apologies @sigmavirus24, definitely short on info:
- Redirect is append-slash like (`dev.local/api/test` -> `dev.local/api/test/`)
- Both served over HTTP
- Cookie is set in the headers dict passed to `requests.get`, no jar/session

Let me know if you need anything else.
",Fizzadar,sigmavirus24
2413,2015-01-19 13:35:11,"@Lukasa This is what I get for working on this way too late last night.

@arthurdarcet thanks. Done.
",sigmavirus24,Lukasa
2413,2015-01-21 02:47:17,"@Lukasa fwiw, I edited the commit message on my commit and retitled the PR to be more accurate descriptions of what's happening. The contents of the PR didn't change though.
",sigmavirus24,Lukasa
2411,2015-01-18 19:40:09,"@Lukasa Admittedly, that is an issue. There is no way for you to know. I guess in the past is just encoded to `ascii`?

In which case, this issue is still two things: 
- v2.5.1 broke compatibility with v2.5.0.
- I have just verified that under Python 3, if the filename _is_ encoded beforehand, then the original issue I reported under Python 2 rears its head. That is, requests does not send the filename and my server sees the filename simply as `'file'`, instead of the original.

EDIT: To clarify, under Python 3, a Unicode filename is _required_ (any encoded filename, e.g. `ascii`, `utf-8`, etc, is not sent to the server) and under Python 2, and encoded filename is _required_.
",sjagoe,Lukasa
2411,2015-04-11 14:50:44,"Yeah, suspect you're right @sigmavirus24. Note that this is a fairly widely-deployed standard, and the HTTPBis is working on bringing it up to date in the draft for [RFC 5987bis](https://tools.ietf.org/html/draft-reschke-rfc5987bis-07), so servers and frameworks that don't support it really should.
",Lukasa,sigmavirus24
2411,2015-04-11 15:12:01,"@Lukasa it seems that 5987bis is expired =( (Expires Jan 3, 2015)
",sigmavirus24,Lukasa
2411,2015-04-11 20:25:40,"@Lukasa this is what I get:


",abbeycode,Lukasa
2411,2015-04-11 21:56:52,"@Lukasa thanks for all the help, but I'm getting this:

> TypeError: Type str doesn't support the buffer API

on this line:



in `urllib3/fields.py` (line 34). This seems like a Python 3 issue, perhaps?
",abbeycode,Lukasa
2411,2015-04-11 22:02:14,"Oh wait, you mean from using @Lukasa's example using encode? Yeah. I think you're better bet is to annoy Slack until it works. I'll have to look into your existing problem first though.
",sigmavirus24,Lukasa
2411,2015-04-12 14:55:01,"I sent Slack an email describing what's happening, so hopefully they'll respond and fix the issues. Thanks @Lukasa and @sigmavirus24 for all the help!
",abbeycode,Lukasa
2411,2015-04-12 14:55:01,"I sent Slack an email describing what's happening, so hopefully they'll respond and fix the issues. Thanks @Lukasa and @sigmavirus24 for all the help!
",abbeycode,sigmavirus24
2409,2015-06-25 16:25:57,"@dpursehouse What's in the response history for each response?
",Lukasa,dpursehouse
2409,2015-06-26 02:02:37,"@Lukasa the history seems to be empty for both responses


",dpursehouse,Lukasa
2409,2015-06-26 08:03:53,"@dpursehouse That strongly suggests one of two things:
1. The connection is setting a cookie that it then does not expect to receive back. Unlikely.
2. The remote server doesn't like connection reuse. _Way_ more likely. Particularly if it's behind HAProxy, which can get all kinds of sad here.

Can you try mounting a HTTPAdapter with a poolsize of 1?


",Lukasa,dpursehouse
2409,2015-06-29 12:16:32,"@dpursehouse Yeah, that can work, though hopefully you'll be able to put it back with the above code and get the benefits of connection pooling.
",Lukasa,dpursehouse
2409,2015-06-30 12:25:53,"@Lukasa maybe we should add that to the toolbelt :)
",kennethreitz,Lukasa
2409,2015-06-30 13:02:51,"@kennethreitz Issue created. =) I'm leaving it on the vine as something that'd be a great ""first issue"" for someone. @sigmavirus24 may even have someone in mind he could pair with for it.
",Lukasa,kennethreitz
2409,2015-06-30 13:02:51,"@kennethreitz Issue created. =) I'm leaving it on the vine as something that'd be a great ""first issue"" for someone. @sigmavirus24 may even have someone in mind he could pair with for it.
",Lukasa,sigmavirus24
2408,2015-01-14 17:16:00,"@Lukasa You are right, I misunderstood. Thanks again.
",berndschultze,Lukasa
2406,2015-01-13 13:05:57,"@Lukasa Sure, I hope we can get to the bottom of this.

For now I'm running the gist on an AWS instance with ubuntu-trusty-14.04-amd64-server-20140927 (ami-f0b11187) via SSH. This means I cannot disable the network adapter (can't do `sudo ifconfig en0 down`), so I thought to use iptables to block outgoing http traffic (`sudo iptables -A OUTPUT -p tcp --dport 80 -j REJECT`) which is not exactly the same.. 

By doing this I cannot reproduce the issue, `requests` behaves properly -- it raises ConnectionError: ('Connection aborted.', error(111, 'Connection refused')) when traffic is rejected by the firewall, and recovers to running normally when the rule is deleted (`sudo iptables -D OUTPUT -p tcp --dport 80 -j REJECT`).

Python 2.7.6 (default, Mar 22 2014, 22:59:56) [GCC 4.8.2] on linux2

Please let me know if I'm off-track here..
I will try doing the same iptables trick on my Mac tonight and see if there's any difference.
",ducu,Lukasa
2406,2015-01-13 14:57:24,"@sigmavirus24 I suspected it's because of forking.

As I mentioned previously, having a single process with a simple loop to perform `count_words_at_url` doesn't cause this issue.. as `requests` recovers after the network gets back up.
",ducu,sigmavirus24
2406,2015-01-13 15:03:06,"@sigmavirus24 But even using the `rq`..
It also recovers if `requests.get` is **not** called before forking -- comment that [line](https://gist.github.com/ducu/ee8c0b1028775df6c72e#file-_test_rq_requests-py-L23) and there's no issue.
",ducu,sigmavirus24
2406,2015-01-15 09:50:52,"Ok guys, I managed to test this on Ubuntu via VirtualBox and no issue there..

The exception being raised when disabling the network is

> ConnectionError: ('Connection aborted.', gaierror(-2, 'Name or service not known'))

different than what I get on the Mac

> ConnectionError: ('Connection aborted.', gaierror(8, 'nodename nor servname provided, or not known'))

The script goes on properly after the network gets back up, so `requests` is getting responses. I guess I can close this ticket, may be what @Lukasa is suggesting, something to do with the OS X DNS service.

Thanks a lot for your support, cheers
",ducu,Lukasa
2406,2016-01-01 22:21:23,"@Lukasa I'm currently chasing rq forked child processes that die on my without notice on a larger POST request. No exception, nothing to be found. I've tracked it down to the actual POST. 
",erdillon,Lukasa
2406,2016-07-21 17:16:12,"just reposting an issue I've been having related to this:

Hey guys, @selwin @ducu  @Lukasa @erdillon 

I've got an issue related to this (i think). But it's actually really easy to replicate. When making a request from a worker fails, the worker just dies, but no errors are reported nor is the job sent to the failed queue. 

my example:



When I queue `make_request` the process just quits when making the request. Whereas a traceback is thrown when I execute the script directs. Took me a while to track this down, is there a fix or something I may be doing wrong?
",dflatow,Lukasa
2406,2016-07-21 18:17:44,"thanks @jjwon0. @Lukasa Yeah agree, will see what the RQ folks say.
",dflatow,Lukasa
2402,2015-01-09 00:38:39,"Thanks for the quick response @sigmavirus24. That makes a lot of sense. I was thinking that a verbose flag would avoid the need for the try/except structure, but it is simple enough to a make a little helper function that processes the request and does that. Thanks again! Please go ahead and close this out.
",mrname,sigmavirus24
2400,2015-01-06 03:30:52,"@sigmavirus24 Thanks for the suggestion. Will look it up.
Does using grequests allow connection pooling? I briefly looked at it right now and it does not look to be as feature rich as the requests library.
We make a ton of HTTP requests and connection pooling is important for us.
",sinank,sigmavirus24
2399,2015-01-06 01:09:05,"Thank you for helping with this!

Trying to clarify a bit what's happening..
As @sigmavirus24 mentioned, I'm using [`rq`](https://github.com/nvie/rq/) and [`summary`](https://github.com/svven/summary/) to process a bunch of URLs. I start several simple (forking) [workers](http://python-rq.org/docs/workers/) so they are doing one URL per job in totally separate processes on the same machine. It all goes fine for a while and suddenly, for no apparent reason, all workers hang at the same time at this line:

https://github.com/svven/summary/blob/master/summary/__init__.py#L193

By hanging I mean suspending the running job and leaving it in the _started_ state. There's no exception whatsoever and it doesn't get passed that line so the job is not finished properly. Then it gets to the next job and does the same thing going on in this ""strike"" mode until the main worker process is killed. All running workers behave this way. Newly started workers perform properly though.

It's a very peculiar issue and I haven't been able to reproduce it. It just happens after a while - few hours or even days, probably depending on the number of URLs that had been processed.

Running out of available sockets may be the cause of the problem, I think it's a good assumption but I didn't verify it yet. What puzzles me is the fact that I get no exception whatsoever, the child (forked) process doing the job simply dies at that line.
",ducu,sigmavirus24
2399,2015-01-06 11:01:24,"I wonder if calling `select` and checking if the socket is marked readable is a good idea. If it is we can pre-emptively close it, because either there's still data on it (and so it hasn't been exhausted) or it was closed remotely (and so closing it here is a no-op). @sigmavirus24?
",Lukasa,sigmavirus24
2399,2015-01-06 15:49:24,"@sigmavirus24: Ok I understand there's no easy way to access the socket..
But in my opinion, reading the whole content of the response when `stream=True` defies the purpose of having the streaming feature, isn't it?

Anyway if `requests` follows the redirects there's no problem with `summary`. It actually follows http_equiv_refresh ""redirects"" as well: https://github.com/svven/summary/blob/master/summary/__init__.py#L209

Getting back to the actual issue..
I'm trying to reproduce the problem like this: https://gist.github.com/ducu/a684c1e96afdaf2c5657

Do you think that [`psutil.Process.connections`](https://pythonhosted.org/psutil/#psutil.Process.connections) would show leaking sockets? Didn't try it properly yet but seen it running on Windows and Ubuntu, interesting how they behave differently, but no signs of leaking sockets for ~300 URLs.
I'll try it out on OS X tonight where I experienced the ""worker strike"" issue, and I'll feed it more URLs.
Am I on the right track here?
",ducu,sigmavirus24
2393,2015-01-18 18:13:47,"Added two tests and gave the strings slightly better names. Not sure we need to spell out every last RFC referenced in the comments though since they're all pretty well known. @Lukasa I'd appreciate some review.
",sigmavirus24,Lukasa
2393,2015-01-19 13:36:21,"@Lukasa forgot to mention this was updated to address your feedback.
",sigmavirus24,Lukasa
2389,2014-12-23 16:51:12,"@Lukasa I'm just waiting for @Jaypipes to verify this fixes their issues.
",sigmavirus24,Lukasa
2389,2014-12-23 17:39:02,"@sigmavirus24 I have now verified that this fixes the issue I was seeing. Thanks very much! :)
",jaypipes,sigmavirus24
2389,2014-12-23 17:40:32,"Cool. @Lukasa I'm going to cut 2.5.1 with this because it's a rather serious bug. (I have to wonder if it could be considered a CVE because someone could perform a denial of service on a client if it knows the clients behaviour and when to expire the nonce.)
",sigmavirus24,Lukasa
2388,2015-06-05 06:09:05,"@sigmavirus24 Resubmitted this against proposed/3.0.0 branch in https://github.com/kennethreitz/requests/pull/2631
",neosab,sigmavirus24
2386,2015-01-27 12:57:27,"Ah, thanks for the reminder @schlamar, I had forgotten that we regressed/removed this.
",Lukasa,schlamar
2385,2014-12-21 15:41:52,"@sigmavirus24 You are right. That was because of `print(r.text)` call. Thank you!
",burdiyan,sigmavirus24
2381,2014-12-15 15:22:58,"I would make a joke about how omitting random letters is cool, but it wouldn't be funy. 

Thanks @namlede 
",sigmavirus24,namlede
2380,2014-12-14 18:13:28,"No problems, @Lukasa !

Can I help you somehow? I'm assuming a feature freeze is applied to fix bugs. 
",syndbg,Lukasa
2379,2014-12-12 16:28:05,"@arthurdarcet have you reported this as a bug to CherryPy?
",sigmavirus24,arthurdarcet
2379,2014-12-13 00:39:37,"@arthurdarcet I respectfully disagree that this isn't a bug on their side.
",sigmavirus24,arthurdarcet
2379,2014-12-13 19:49:01,"I agree @arthurdarcet. I just did not want the cherrypy issue to go unreported.
",sigmavirus24,arthurdarcet
2375,2014-12-09 13:53:23,"@Lukasa because it's faster if we fix it and provide a little extra flexibility at ~~no~~ little extra cost.
",sigmavirus24,Lukasa
2375,2014-12-09 14:27:43,"Hello,
I can understand how @lukasa is annoyed by unvendoring made downstream, but I can also understand why this is done. For example, in Debian, Python SSLv3 support was removed and I only had to patch (I forwared my patch upstream of course[¹]) urllib3 to fix all packages depending on urllib3.

So, yes, this is a downstream problem, but as in past (even when this issue arosed on Debian Bug Tracker) I will never add something without asking first if you, upstream developers, are ok with a downstream change. 
 As I said a lot of time before being the Debian maintainer of requests I'm one of its users: I want requests to be in the best shape in Debian. :)

I agree with @dstufft and @sigmavirus24 but, if you don't agree, I can also replace the currently used patch in Debian with this so, at least, Debian, Ubuntu and pip will use the same code.
IMHO cooperating we will arrive to the best solution for all.

[¹] http://anonscm.debian.org/viewvc/python-modules/packages/python-urllib3/tags/1.9.1-3/debian/patches/06_do-not-make-SSLv3-mandatory.patch?view=markup Yes, next time I will use a PR, fortunately @dstufft forwarded it properly! :)
",eriol,sigmavirus24
2375,2014-12-09 14:27:43,"Hello,
I can understand how @lukasa is annoyed by unvendoring made downstream, but I can also understand why this is done. For example, in Debian, Python SSLv3 support was removed and I only had to patch (I forwared my patch upstream of course[¹]) urllib3 to fix all packages depending on urllib3.

So, yes, this is a downstream problem, but as in past (even when this issue arosed on Debian Bug Tracker) I will never add something without asking first if you, upstream developers, are ok with a downstream change. 
 As I said a lot of time before being the Debian maintainer of requests I'm one of its users: I want requests to be in the best shape in Debian. :)

I agree with @dstufft and @sigmavirus24 but, if you don't agree, I can also replace the currently used patch in Debian with this so, at least, Debian, Ubuntu and pip will use the same code.
IMHO cooperating we will arrive to the best solution for all.

[¹] http://anonscm.debian.org/viewvc/python-modules/packages/python-urllib3/tags/1.9.1-3/debian/patches/06_do-not-make-SSLv3-mandatory.patch?view=markup Yes, next time I will use a PR, fortunately @dstufft forwarded it properly! :)
",eriol,dstufft
2375,2014-12-09 14:40:29,"@eriolv question: Since the symlink is in place (it may just be Fedora that has this in place), is there any chance of the imports that import from `.packages` inside of requests could not be rewritten? 

If not, can the imports not be rewritten after we ship this patch? (After we've updated it to give proper attribution to @dstufft and pypa/pip) The crux of this issue is that sys.modules is incorrectly populated and needs to work a certain way for users to not run into surprises like this.

[/Edit - I submitted my comment too soon]
And I appreciate your collaboration @eriolv. That's why I pinged you immediately. I wanted to make you aware of this from the start and get your feedback as well as @Lukasa's and @ralphbean's
",sigmavirus24,Lukasa
2375,2014-12-09 14:40:29,"@eriolv question: Since the symlink is in place (it may just be Fedora that has this in place), is there any chance of the imports that import from `.packages` inside of requests could not be rewritten? 

If not, can the imports not be rewritten after we ship this patch? (After we've updated it to give proper attribution to @dstufft and pypa/pip) The crux of this issue is that sys.modules is incorrectly populated and needs to work a certain way for users to not run into surprises like this.

[/Edit - I submitted my comment too soon]
And I appreciate your collaboration @eriolv. That's why I pinged you immediately. I wanted to make you aware of this from the start and get your feedback as well as @Lukasa's and @ralphbean's
",sigmavirus24,dstufft
2375,2014-12-09 16:40:17,"My snarkiness was mostly the result of me waking up late and being late for work and having not had coffee, apologies all.

In reality I'm +0 on this. I don't like that we have to do it, and the unvendoring zealots have hardened my opinion towards the idea of doing them any favours on any issue whatsoever. (I don't include you in that group @eriolv, you have not displayed any zealotry that I'm aware of :wink: )

However, I acknowledge the Catch-22 of the fact that _we_ will get blamed for the zealots decision to unbundle us breaking their code. For that reason I have no intention of blocking this patch: punishing users is unacceptable.

However, I'd like _someone_ to test the change, as neither @sigmavirus24 nor @dstufft appear to have. Ideally I'd like some form of automated testing for it as well: having that would raise me to +0.5.
",Lukasa,sigmavirus24
2375,2014-12-09 16:40:17,"My snarkiness was mostly the result of me waking up late and being late for work and having not had coffee, apologies all.

In reality I'm +0 on this. I don't like that we have to do it, and the unvendoring zealots have hardened my opinion towards the idea of doing them any favours on any issue whatsoever. (I don't include you in that group @eriolv, you have not displayed any zealotry that I'm aware of :wink: )

However, I acknowledge the Catch-22 of the fact that _we_ will get blamed for the zealots decision to unbundle us breaking their code. For that reason I have no intention of blocking this patch: punishing users is unacceptable.

However, I'd like _someone_ to test the change, as neither @sigmavirus24 nor @dstufft appear to have. Ideally I'd like some form of automated testing for it as well: having that would raise me to +0.5.
",Lukasa,dstufft
2375,2014-12-09 16:58:46,"@dstufft Indeed, and that's what's important. I want to make sure that if we're doing this we do it right the first time.

Note that it obviously won't do that on vendored systems. =)
",Lukasa,dstufft
2375,2014-12-10 15:27:05,"Thanks @ralphbean. Will Fedora 21 be able to generate a new build with this change at least?

Also, to everyone commenting on the fact that this file still references pip, I pulled this in and committed it to start a discussion. This will not be merged as is.
",sigmavirus24,ralphbean
2375,2014-12-24 20:06:33,"@Lukasa thoughts?
",sigmavirus24,Lukasa
2375,2014-12-24 20:07:19,"I'm with @kennethreitz. I don't like it, but I think we have to do it.
",Lukasa,kennethreitz
2375,2014-12-31 14:57:27,"@kennethreitz @Lukasa either of you have time to give this another once over? This will vastly improve user experience when using downstream distributed modules. Believe it, or not, it will also reduce the number of changes downstream re-distributors need to make to requests itself as well. Finally, it will make requests far more stable for our users on those systems who do not have the ability to ""Just use pip"". I still prefer to use pip myself (because I like to have a better control over what I use) but that isn't a luxury everyone has who wants to (and needs to) use requests. We shouldn't begrudge them for having a different set of constraints than we typically have.
",sigmavirus24,kennethreitz
2375,2014-12-31 14:57:27,"@kennethreitz @Lukasa either of you have time to give this another once over? This will vastly improve user experience when using downstream distributed modules. Believe it, or not, it will also reduce the number of changes downstream re-distributors need to make to requests itself as well. Finally, it will make requests far more stable for our users on those systems who do not have the ability to ""Just use pip"". I still prefer to use pip myself (because I like to have a better control over what I use) but that isn't a luxury everyone has who wants to (and needs to) use requests. We shouldn't begrudge them for having a different set of constraints than we typically have.
",sigmavirus24,Lukasa
2375,2014-12-31 15:28:27,"The code looks good to me, as it did originally, though I'm hardly an expert on Python's import machinery.

I still resent the requirement to add this code. I accept the benefits and acknowledge the need for it, I just refuse to be happy about it. ;) Basically, I feel the same way about this as I do about cleaning the house.

I'm +1 on merging, I'm sure I'll get over the pain. Want @kennethreitz to sign off though.
",Lukasa,kennethreitz
2375,2015-01-09 19:41:44,"@kennethreitz you have approximately a zero chance of convincing them to do that. I spent months on it for pip and basically gave up. As far as I can tell your options are either provide the tooling for unbundling to happen in a way that you control, or they are going to do it however each individual distro feels like doing it with varying levels of support for thinks like requests-toolbelt needing to import from urllib3.

Ubuntu 14.04 for instance ships with a completely broken ensurepip and a broken by default venv module, and they knew it was broken at the time, because ensurepip bundles pip and they didn't have a a solution for handling the bundling in a way that they considered acceptable. From what I can tell a lot of distros are perfectly happen shipping broken things as long as it's not broken for their particular use cases. This sucks but I don't think it's going to change.

Personally in pip I decided that grabbing my nose to deal with the stink and getting at least consistent de-bundling was worth the somewhat gross code when the alternative was that end users would suffer.
",dstufft,kennethreitz
2375,2015-01-09 19:44:28,"Cool. While we wait for you to do that @kennethreitz our users can continue to suffer. 
",sigmavirus24,kennethreitz
2375,2015-01-09 20:33:46,"This is officially @dstufft's decision. :)
",kennethreitz,dstufft
2375,2015-01-09 20:38:45,"yay delegation

@dstufft gets a cookie :cookie:
",kennethreitz,dstufft
2375,2015-01-09 20:47:49,"@kennethreitz et al, many thanks for this! :)
",eriol,kennethreitz
2374,2014-12-08 23:46:14,"Thanks @krvc 
",sigmavirus24,krvc
2373,2014-12-08 18:40:42,"reStructuredText on GitHub is so weird. Thanks @frewsxcv 
",sigmavirus24,frewsxcv
2371,2014-12-05 17:58:06,"@alex Interestingly, neither `env PYTHONUNBUFFERED=` or `-u` has the same effect on Python 2. Results from my machine incoming.
",Lukasa,alex
2371,2014-12-05 18:06:31,"FWIW, I just merged adding `buffering=True` from @kevinburke, do your runs
include that?

On Fri Dec 05 2014 at 12:04:40 PM Cory Benfield notifications@github.com
wrote:

> Alright, the below data comes from a machine that is doing nothing else
> but running these tests. The last test was run with the Python -u flag
> set, and as you can see that flag has no effect.
> 
> Python 2.7.6
> go version go1.2.1 linux/amd64
> BENCH SOCKET:
>    8GiB 0:00:16 [ 500MiB/s] [================================>] 100%
> BENCH HTTPLIB:
>    8GiB 0:01:32 [88.6MiB/s] [================================>] 100%
> BENCH URLLIB3:
>    8GiB 0:01:20 [ 101MiB/s] [================================>] 100%
> BENCH REQUESTS
>    8GiB 0:01:21 [ 100MiB/s] [================================>] 100%
> BENCH GO HTTP
>    8GiB 0:00:21 [ 385MiB/s] [================================>] 100%
> 
> Python 2.7.6
> go version go1.2.1 linux/amd64
> BENCH SOCKET:
>    8GiB 0:00:16 [ 503MiB/s] [================================>] 100%
> BENCH HTTPLIB:
>    8GiB 0:01:33 [87.8MiB/s] [================================>] 100%
> BENCH URLLIB3:
>    8GiB 0:01:20 [ 101MiB/s] [================================>] 100%
> BENCH REQUESTS
>    8GiB 0:01:22 [99.3MiB/s] [================================>] 100%
> BENCH GO HTTP
>    8GiB 0:00:20 [ 391MiB/s] [================================>] 100%
> 
> Python 2.7.6
> go version go1.2.1 linux/amd64
> BENCH SOCKET:
>    8GiB 0:00:16 [ 506MiB/s] [================================>] 100%
> BENCH HTTPLIB:
>    8GiB 0:01:31 [89.1MiB/s] [================================>] 100%
> BENCH URLLIB3:
>    8GiB 0:01:20 [ 101MiB/s] [================================>] 100%
> BENCH REQUESTS
>    8GiB 0:01:20 [ 101MiB/s] [================================>] 100%
> BENCH GO HTTP
>    8GiB 0:00:21 [ 389MiB/s] [================================>] 100%
> 
> These numbers are extremely stable, and show the following features:
> 1. Raw socket reads are fast (duh).
> 2. Go is about 80% the speed of a raw socket read.
> 3. urllib3 is about 20% the speed of a raw socket read.
> 4. requests is slightly slower than urllib3, which makes sense as we
>    add a couple of stack frames for the data to pass through.
> 5. httplib is slower than requests/urllib3. That's just impossible,
>    and I suspect that we must be configuring httplib or the sockets library in
>    a way that httplib is not.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/2371#issuecomment-65829335
> .
",alex,kevinburke
2371,2014-12-08 17:04:52,"Cake for @alex for being super helpful :cake:
",kennethreitz,alex
2371,2014-12-08 17:10:48,"@nelhage did some stracing of the various examples (in the transfer
encoding: chunked case) https://gist.github.com/nelhage/dd6490fbc5cfb815f762
are the results. It looks like there's a bug in httplib which results in it
not always reading a full chunk off the socket.

On Mon Dec 08 2014 at 9:05:14 AM Kenneth Reitz notifications@github.com
wrote:

> Cake for @alex https://github.com/alex for being super helpful [image:
> :cake:]
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/2371#issuecomment-66147998
> .
",alex,alex
2371,2014-12-08 17:13:42,"So what we have here is a bug in a standard library that no one is really maintaining? (@Lukasa has at least 2 patch sets that  have been open for >1 year.) Maybe I'll raise a stink on a list somewhere tonight
",sigmavirus24,Lukasa
2371,2014-12-08 17:15:06,"Someone (I might get to it, unclear) probably needs to drill down with pdb
or something and figure out what exact code is generating those 20-byte
reads so we can put together a good bug report.

On Mon Dec 08 2014 at 9:14:09 AM Ian Cordasco notifications@github.com
wrote:

> So what we have here is a bug in a standard library that no one is really
> maintaining? (@Lukasa https://github.com/Lukasa has at least 2 patch
> sets that have been open for >1 year.) Maybe I'll raise a stink on a list
> somewhere tonight
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/2371#issuecomment-66149522
> .
",alex,Lukasa
2371,2014-12-22 18:55:06,"Thanks @Lukasa. I'm dealing with a performance issue where download speed on a chunked response using urllib3/requests is much slower than with curl and other libraries, and trying to understand if this is the culprit.
",kislyuk,Lukasa
2371,2015-03-07 19:29:41,"Fascinating, thanks for sharing! :)

Seems like a great goal for @Lukasa's Hyper to address, too.
",shazow,Lukasa
2371,2015-03-07 20:56:00,"@alex - I toyed around a little with the urllib3 vs requests non-chunked performance issue you mentioned.  I think I see a similar 20% drop in requests.

In requests I speculatively tried replacing the call to self.raw.stream with the inlined implementation of stream() (from urllib3).  It seemed to bring the throughput a lot closer between requests and urllib3, at least on my machine:



Maybe you could try the same on your machine to see if it makes a difference for you too.

(Note yes I know the call to is_fp_closed is encapsulation busting, it isn't meant as a serious patch just a data point)
",gardenia,alex
2371,2015-03-07 22:29:53,"@shazow It's my hope that the `BufferedSocket` that hyper uses should address a lot of that inefficiency, by essentially preventing small reads. I wonder if `httplib` on Py3 has this problem, because it uses `io.BufferedReader` extensively, which should provide roughly the same kind of benefit as the `BufferedSocket`.

Certainly, however, when `hyper` grows enough HTTP/1.1 functionality to be useful we should try to benchmark it alongside these other implementations and make efforts to make `hyper` as fast as possible.
",Lukasa,shazow
2369,2014-12-04 02:57:41,"@kevinburke Wrong repo. ;) Merge to urllib3 first, then we'll take the fix.
",Lukasa,kevinburke
2368,2014-12-04 05:21:59,"So that's also my first instinct @Lukasa but I don't think `getpeercert` would remove a wildcard `subjectAltName` entry randomly. Still, I'd like to know from @EthanBlackburn or @buttscicles what happens on Python 3 (preferably 3.3 or 3.4).
",sigmavirus24,Lukasa
2368,2014-12-04 14:40:17,"I beat @Lukasa to commenting on an issue! Holy smokes!
",sigmavirus24,Lukasa
2366,2014-12-07 18:58:22,"Yeah. I agree with @Lukasa. By specifying a custom header like this, you're basically asserting that you know what you want to happen and if something unexpected happens then you have to remedy it, even if that means having to manually handle redirects
",sigmavirus24,Lukasa
2366,2014-12-18 08:17:38,"@Lukasa ok, i got it.  you could try with curl -H""Host: xxx.yyy.com"" url.  (curl/7.19.7)
the behavior of curl is also use new-host header, if a 302 return, not user set one.
",hakulat,Lukasa
2364,2014-12-02 16:00:39,"@kevinburke it usually takes about a minute to fail.

I've attached a few screenshots from wireshark. Let me know if a screenshot of something else might be more helpful.

![initial](https://cloud.githubusercontent.com/assets/410872/5265689/49539ce0-7a12-11e4-9fea-fde29204fe43.png)

![exchange](https://cloud.githubusercontent.com/assets/410872/5265696/5426e35c-7a12-11e4-85ee-e26cfc64c9db.png)

Thanks guys
",greedo,kevinburke
2364,2014-12-02 16:34:36,"Are there any machine or proxies sittin in between the server and the
client? does the client machine have a default socket timeout configured?

On Tuesday, December 2, 2014, Joe Cabrera notifications@github.com wrote:

> @kevinburke https://github.com/kevinburke it usually takes about a
> minute to fail.
> 
> I've attached a few screenshots from wireshark. Let me know if a
> screenshot of something else might be more helpful.
> 
> [image: initial]
> https://cloud.githubusercontent.com/assets/410872/5265689/49539ce0-7a12-11e4-9fea-fde29204fe43.png
> 
> [image: exchange]
> https://cloud.githubusercontent.com/assets/410872/5265696/5426e35c-7a12-11e4-85ee-e26cfc64c9db.png
> 
> Thanks guys
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/2364#issuecomment-65253593
> .

## 

## 

Kevin Burke
phone: 925.271.7005 | twentymilliseconds.com
",kevinburke,kevinburke
2364,2014-12-02 16:51:18,"@kevinburke there is a NAT sitting between the machine and the server, but no proxy.

I am using a `(1,30)` timeout and my understanding is `requests` overrides whatever the client timeout defaults are.
",greedo,kevinburke
2364,2014-12-02 17:01:07,"That's correct. Curious why it's taking a minute to time out if you set the
timeout to 30 seconds.

On Tuesday, December 2, 2014, Joe Cabrera notifications@github.com wrote:

> @kevinburke https://github.com/kevinburke there is a NAT sitting
> between the machine and the server, but no proxy.
> 
> I am using a (1,30) timeout and my understanding is requests overrides
> whatever the client timeout defaults are.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/2364#issuecomment-65262796
> .

## 

## 

Kevin Burke
phone: 925.271.7005 | twentymilliseconds.com
",kevinburke,kevinburke
2364,2014-12-02 19:34:44,"@kevinburke it could be closer to 30 seconds, however I also make more than 1 request
",greedo,kevinburke
2364,2016-05-24 10:32:26,"Thanks @Lukasa. **`data={'cmd': 'date +%Y%m%d'}`** fails to receive command results, ex:


",nixawk,Lukasa
2364,2016-05-24 10:39:47,"@Lukasa  I've tested this against **`D-LINK DIR-600`** routers

## requests



## curl   -- OK


",nixawk,Lukasa
2364,2017-03-12 12:02:12,"@sigmavirus24 The reason `RemoteDisconnected` exception was added, is so that HTTP clients can safely retry (see [issue 3566](https://bugs.python.org/issue3566)).
Don't you think the right behavior here is adding an automatic retry?",shoham-stratoscale,sigmavirus24
2363,2014-12-01 22:30:08,"@alex those tests that were using https didn't seem to have any necessity to rely on https. They were likely written by me and I just write `'https://httpbin.org'` everytime I want to talk to httpbin by force of habit.
",sigmavirus24,alex
2362,2014-12-01 19:56:28,"@kevinburke hmm, i suppose so . 
",kennethreitz,kevinburke
2362,2014-12-01 20:05:35,"@kevinburke if someone tells me what command to type in, i'll do it lol
",kennethreitz,kevinburke
2362,2014-12-03 02:25:06,"@sigmavirus24 now has access to dnsimple. @Lukasa, can you tell me your dnsimple email address? 
",kennethreitz,sigmavirus24
2362,2014-12-03 07:12:31,"@kennethreitz Sorry, used a different one: lukasaoz@gmail.com.

Always best to have 8 million email addresses.
",Lukasa,kennethreitz
2362,2014-12-08 17:10:08,"Yeah. I'm curious why that's no longer building PRs or anything. If you want, I can set up ci.sigmavir.us to do CI here for now, but I'll need access to settings (or will need to sync up with @Lukasa about adding it)
",sigmavirus24,Lukasa
2359,2014-12-02 14:02:19,"@sigmavirus24, yes, workarounds are well understood. I was just concerned that the default behavior leaves the user exposed to this issue (with no warnings in the docs - that I could find at least).
",marcocova,sigmavirus24
2356,2014-12-25 16:22:45,"So, looking at this again, I did tried the following:



I assume this is something along the lines of what @suhaasprasad is seeing. I'm going to see if following @Lukasa's idea will work for this.
",sigmavirus24,Lukasa
2355,2014-11-25 01:00:14,"Very nice @msabramo 
",nuxlli,msabramo
2355,2014-11-25 02:26:18,"@sigmavirus24: Thanks for the detailed explanation!
",msabramo,sigmavirus24
2352,2014-11-23 13:51:20,"@Lukasa All good points. From the perspective of a new user who isn't aware of the architecture of requests, they just want a simple mechanism that will allow easy debugging without having to use wireshark filters, passive burp proxy etc. For example, when using `curl` you can use the `-v` option to dump all request data, something like that for requests would be more than sufficient. 

I'm wondering if it would be more appropriate to raise a feature request in `urllib3` for a simple session method, such as `verbose(True/False)` which then implements the necessary recipe to dump traffic data in a similar format as `curl -v`. If accepted, then we could implement a method on `requests.Session` that called the urllib3 verbose method.

Does that sound like a good way forward?
",foxx,Lukasa
2352,2014-11-23 21:11:08,"@sigmavirus24 Normally I don't bother commenting on rudeness, but I felt compelled to on this occasion because of my prior background with requests/urllib3. I really feel like your response was overly aggressive and borderline rude. I've never had this issue with any of the other core maintainers in the past, and I spent time specifically replying because they have always been polite and reasonable, even when rejecting my code or requests. This interaction has left me feeling like I no longer want to bother contributing towards either of those projects, your reaction/approach was completely unnecessary and similar in tone with the Django core developers poor bed side manner.

Sometimes things get rejected, and that's fine, but there's no need to be rude about it.
",foxx,sigmavirus24
2352,2014-11-24 00:02:27,"@Lukasa Thank you for the follow up, it's appreciated. Based on comments in those other issues and a quick inspection of the source, additional debugging support would _probably_ have to rely on monkey patching. Given that's not a great approach, I'd agree that a rewrite of `httplib` is sounding like the cleaner option. 

Out of curiosity, were you planning on keeping the `httplib` rewrite backwards compatible, or would it be `httplib2`?
",foxx,Lukasa
2352,2014-11-24 02:38:25,"@sigmavirus24 I appreciate the follow up, thank you, and no hard feelings :)
",foxx,sigmavirus24
2345,2014-11-21 18:55:26,"@mattrobenolt Not your fault, our CI was busted so you had no way to know. =)
",Lukasa,mattrobenolt
2344,2014-11-16 15:43:28,"@Lukasa it's a test failure on master apparently. Besides that, I'm -1 on this. A `LocationParseError` happens long before a connection is even attempted. This should be an `InvalidURL` error
",sigmavirus24,Lukasa
2344,2014-11-17 05:39:00,"@sigmavirus24 If I've understood you correctly, I have changed the ConnectionError to an InvalidURL error.
",ContinuousFunction,sigmavirus24
2344,2014-11-18 03:54:43,"@ContinuousFunction could you amend your commit messages? Two commits in a row with the same message is incredibly unhelpful when looking at the log.
",sigmavirus24,ContinuousFunction
2344,2014-11-18 04:11:25,"@sigmavirus24 sorry about that. I've changed the message.
",ContinuousFunction,sigmavirus24
2344,2014-12-15 17:41:30,"@ContinuousFunction can you rebase this, uncomment the test that you commented, and ensure the tests pass. Then force-push to this same branch?
",sigmavirus24,ContinuousFunction
2344,2014-12-15 19:05:32,"@sigmavirus24 I believe I have made the appropriate changes.
",ContinuousFunction,sigmavirus24
2344,2014-12-17 04:14:20,"Thanks @ContinuousFunction I'll run these tests now and make sure they pass before merging.
",sigmavirus24,ContinuousFunction
2344,2014-12-17 04:50:48,"There was a test failure on Python 3 because `urllib3.exceptions.LocationParseError` doesn't have a `message` attribute. That's fixed in bd3cf95e34aa49c8d764c899672048df107e0d70.

Thanks @ContinuousFunction !
",sigmavirus24,ContinuousFunction
2344,2014-12-17 16:50:52,"@sigmavirus24 My pleasure!
",ContinuousFunction,sigmavirus24
2338,2014-11-17 04:43:52,"@Lukasa can this be closed?
",sigmavirus24,Lukasa
2336,2014-12-03 15:03:39,"We haven't shipped the fix for this yet. Ping @shazow and @sigmavirus24

On Wednesday, December 3, 2014, Zhang Yibin notifications@github.com
wrote:

> I also encoutered a deadlock problem caused by this similar timeout issue.
> 
> What I did is sending a GET request to some site (like http://jsonip.com)
> using an HTTP proxy to get proxy's public ip address, the code is like:
> 
> s = requests.Session()
> s.proxies = {'http': 'http://xx.xx.xx.xx:xxx', 'https': 'http://xx.xx.xx.xx:xxx'}
> r = s.get('http://jsonip.com', timeout=5)
> r.raise_for_status()
> ...
> 
> After deadlock, the stack trace of the problmatic thread is (other threads
> are waiting for its completion):
> 
> File: ""C:\Python27\lib\threading.py"", line 783, in __bootstrap
>   self.__bootstrap_inner()
> File: ""C:\Python27\lib\threading.py"", line 810, in __bootstrap_inner
>   self.run()
> File: ""C:\Python27\lib\threading.py"", line 763, in run
>   self.__target(_self.__args, *_self.__kwargs)
> File: ""C:\Python27\lib\site-packages\concurrent\futures\thread.py"", line 73, in _worker
>   work_item.run()
> File: ""C:\Python27\lib\site-packages\concurrent\futures\thread.py"", line 61, in run
>   result = self.fn(_self.args, *_self.kwargs)
> File: ""E:\zbb\projects\crawler\new\zbb-crawler\zbb_proxymanager\app\proxy_filter.py"", line 45, in test
>   ip = self.public_ip.get(host)
> File: ""E:\zbb\projects\crawler\new\zbb-crawler\zbb_proxymanager\utils.py"", line 91, in get
>   ip = func(session, timeout)
> File: ""E:\zbb\projects\crawler\new\zbb-crawler\zbb_proxymanager\get_ip.py"", line 43, in get_ip_4
>   r = session.get('http://jsonip.com', timeout=timeout)
> File: ""C:\Python27\lib\site-packages\requests\sessions.py"", line 473, in get
>   return self.request('GET', url, *_kwargs)
> File: ""C:\Python27\lib\site-packages\requests\sessions.py"", line 461, in request
>   resp = self.send(prep, *_send_kwargs)
> File: ""C:\Python27\lib\site-packages\requests\sessions.py"", line 599, in send
>   history = [resp for resp in gen] if allow_redirects else []
> File: ""C:\Python27\lib\site-packages\requests\sessions.py"", line 192, in resolve_redirects
>   allow_redirects=False,
> File: ""C:\Python27\lib\site-packages\requests\sessions.py"", line 573, in send
>   r = adapter.send(request, **kwargs)
> File: ""C:\Python27\lib\site-packages\requests\adapters.py"", line 370, in send
>   timeout=timeout
> File: ""C:\Python27\lib\site-packages\requests\packages\urllib3\connectionpool.py"", line 513, in urlopen
>   conn = self._get_conn(timeout=pool_timeout)
> File: ""C:\Python27\lib\site-packages\requests\packages\urllib3\connectionpool.py"", line 231, in _get_conn
>   return conn or self._new_conn()
> File: ""C:\Python27\lib\site-packages\requests\packages\urllib3\connectionpool.py"", line 717, in _new_conn
>   return self._prepare_conn(conn)
> File: ""C:\Python27\lib\site-packages\requests\packages\urllib3\connectionpool.py"", line 690, in _prepare_conn
>   conn.connect()
> File: ""C:\Python27\lib\site-packages\requests\packages\urllib3\connection.py"", line 217, in connect
>   self._tunnel()
> File: ""C:\Python27\lib\httplib.py"", line 752, in _tunnel
>   (version, code, message) = response._read_status()
> File: ""C:\Python27\lib\httplib.py"", line 365, in _read_status
>   line = self.fp.readline(_MAXLINE + 1)
> File: ""C:\Python27\lib\socket.py"", line 476, in readline
>   data = self._sock.recv(self._rbufsize)
> 
> I think it's the same bug discussed in this issue, right? The version of
> Requests is 2.5.0.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/2336#issuecomment-65412519
> .

## 

## 

Kevin Burke
phone: 925.271.7005 | twentymilliseconds.com
",kevinburke,sigmavirus24
2336,2015-01-07 11:41:32,"@kevinburke could you confirm that your changes to urllib3 are in the requests master branch?
",glaslos,kevinburke
2336,2016-07-23 13:31:19,"@Lukasa my bad 



use a fake https proxie to access real https://baidu.com,it will just hang there.

i do this in ipython with no context.
",Petelin,Lukasa
2336,2016-07-24 20:35:09,"@Lukasa many thanks for pointing me to this!
@Petelin If I understand correctly you were using a backported system package of requests, since on Trusty I see requests 2.2.1 as system package. Is this right?
I'm asking because I see an instant timeout too using requests (from the system package) 2.10.0 on Debian testing. I don't have at the moment an Ubuntu trusty VM, but I will be happy to indvestigate if you can give me more details. Thanks! 
",eriol,Lukasa
2334,2015-04-02 10:59:42,"Update, correcting myself:
- Went reading RFC2617.
- Figured out that my understanding about Digest auth was wrong: Digest auth does not authenticate connections (contrarily to NTLM), it does authenticate requests.
- Given this, please disregard my considerations above concerning HTTPAdapters, Pools, etc.
- Digged deeper into this particular case:
  - The culprit seems to be the fact that `HTTPDigestAuth.handle_401` only produces the `Authorization` header if it has been called less than 2 times (see ...`and num_401_calls < 2:` around line 172 in auth.py)
  - With two near-simultaneous threads, the first one entering `handle_401` will correctly produce the necessary auth headers, the second one will not.
- My hack test:
  - Commented out the `and num_401_calls < 2` in auth.py.
  - Ran a variation of the test above successfully with 2, 20 and 200 threads (with the original code, I would only get 50% of the requests authenticated).
- Possible fix:
  - The underlying `and num_401_calls < 2` seems to stem from #547 which, rightfully so, tries to prevent infinite auth failure loops. This means throwing the 401 call count away would lead to regressions.
  - A primitive approach would count 401 calls on a per-thread basis: not sure if feasible, not sure if elegant.

Here are my 2c.

PS: @Lukasa, given this, your comment about the (agreed, really nasty) possibility of pulling the raw `httplib` connection object would not apply here, but maybe feasible (at least for quick-hack tests) in https://github.com/requests/requests-ntlm. However, I'm not sure how to grab that object: any pointer? (lest's move this discussion away from this issue, shall we?) :)

More PS: For completeness, I dump my test code (easier to raise up thread count)


",exvito,Lukasa
2334,2015-04-03 09:24:57,"Greetings all. Thanks for taking care of this @exvito. My only concern is about nonce and nonce_count values in a multi-threaded context. More generally, how do you deal with two threads being simultaneously in the handle_401() method?
",vincentxb,exvito
2333,2014-11-12 15:53:45,":sparkles: :cake: :sparkles: Thanks @akitada !!!
",sigmavirus24,akitada
2327,2014-11-07 18:26:17,"This is an interesting proposal, that I think suffers from being for an extremely specific use-case that is relatively uncommon. Let's take it in two parts.

First, `max_connectiontime`. `time` in this context is underspecified. What do we mean? CPU time? On a heavily loaded CPU you may not get scheduled and so this can turn into an extremely long wall-clock duration. Wall-clock time? This is more likely, but it still allows for us to be resource starved out, and to achieve it correctly requires that we register for signals from the OS, which is an extremely unpleasant thing for a library to do. For us to pick which of these the user meant is extremely presumptuous, especially when, as @kevinburke says, it's not that hard for the user to do themselves.

The max file-size is an interesting idea, but fundamentally starts adding some weirdness to the API. Firstly, what does `requests.send(url, stream=False, max_filesize=80)` mean? It should never be possible to write a mutually contradictory line of code in this manner. I'd be _more_ open to having this be a property on a `Response` object, realistically I think I'm pretty happy with the situation as is. I'd be interested to hear from the other maintainers though.
",Lukasa,kevinburke
2327,2014-11-07 19:49:19,"""Why does there need to be a maximum filesize enforced by requests. Why aren't you using streaming and detecting this yourself?""

I recently implemented streaming and chunking.  It wasn't difficult for me to implement.

However:
- This appears to be a question/need that often occurs on StackOverflow and in this issue tracker
- Because there are no limits to filesize in the requests library, it is easy for a user of the library to request a large file or endless stream of data and not realize this is happening. This can lead to issues with memory and disk usage. 
- Dealing with a chunked read is a bit of an advanced topic as it deals with manipulating the `raw` file-like object.  While it is something that is old and familiar for people with a few years of Python experience, it's not something that is easy for junior developers.  Some sort `max_filesize` argument or a `safe_read` function could encapsulate this logic and make it accessible to all.

""You've described a ""content stash"" that has no basis in anything this library does and did not define it.""
- The response object has a `.content` property/method (https://github.com/kennethreitz/requests/blob/master/requests/models.py#L714-L736)
- Said property iterates through the `.iter_content()` filelike object (https://github.com/kennethreitz/requests/blob/master/requests/models.py#L639-L683) 
- The read content is then stored in an internal stash named `._content` (https://github.com/kennethreitz/requests/blob/master/requests/models.py#L539-L540)

I think I was wrong about placement, and it would better function as a method in Response as @Lukasa noted.  What I'm essentially suggesting, is extending the library to have a safeguard around the call to `.iter_content`.  The data would be read in chunks, and stored in the object's `._content` stash.  If the entirety of the data is read, everything functions as normal.  If the filesize is exceeded, an exception is raised.  It would make a defensive programming concept relatively accessible.

This is in line with stdlib functions like `file.read` and `httlib.read` (and consquently urllib) that all support a form of ranged reads without having to loop over chunks.  It is possible to do a ranged read on the `raw` property of the response object, but it's a looped chunk read and it appears to possibly have the potential for unintentionally changing some internal flags on the response object and create issues with future calls to `.content`.

I think features/recipes like this could work in requests-toolbelt if there were an ability/hook to register consumed content into the response object so that the API doesn't change for end users.  For example, if I were to read everything by chunks into a variable, I could them call ""Response._register_content(content)"" which would set  `Response._content`, and `Response._content_consumed`.  
",jvanasco,Lukasa
2326,2014-11-07 17:00:47,"@Lukasa it may be CPython being overly cautious but it will be a problem on PyPy. Stuff like [this](https://bugs.launchpad.net/dateutil/+bug/1376343) can cause an application to crash because there are too many file descriptors open. Others can explain why this is important to PyPy better than I can, but I'm very confident this is a worth while change.

My only qualm is worrying about users who may be accessing `r.raw._pool`. Granted they shouldn't be, but I'm worried about an influx of users who will complain that we broke X.
",sigmavirus24,Lukasa
2313,2015-07-10 14:44:35,"@Lukasa  OK, I didn't test so much, my statement maybe wrong.
I just found that the behavior of requests wasn't same as browser(for example chrome), what  I thought is that the method chrome using is workable.

@sigmavirus24 I will try to make clear and submit issue to those server if needed.
",zhangchunlin,Lukasa
2313,2015-07-10 14:44:35,"@Lukasa  OK, I didn't test so much, my statement maybe wrong.
I just found that the behavior of requests wasn't same as browser(for example chrome), what  I thought is that the method chrome using is workable.

@sigmavirus24 I will try to make clear and submit issue to those server if needed.
",zhangchunlin,sigmavirus24
2312,2016-01-27 11:53:27,"@Lukasa Thanks a lot. It works perfectly with unencrypted key. Probably worth mentioning this in the docs.
",skvsn,Lukasa
2309,2014-10-28 22:06:09,"So, @kennethreitz could you give me a bit more direction regarding what you want me to do with this? Should I default disable the warning everywhere and then provide a function for users to enable them? 
",sigmavirus24,kennethreitz
2307,2014-10-26 22:51:43,"This was extremely subtle and something none of us caught. I'm sorry for the inconvenience @itaiin.

Thanks for figuring this out @kevinburke. I agree with your reasoning about not retrying a POST request for those exact reasons.
",sigmavirus24,kevinburke
2307,2014-10-28 09:51:40,"@kevinburke thanks for all the effort!
",itaiin,kevinburke
2304,2014-10-25 12:10:27,"@sigmavirus24 I think this is different than SNI.
@Fuzion24 wants, that requests accepts certificates for `*.example.com` when requesting `a.b.c.example.com`. The standard defines, that `*` is _not_ recursive and only matches one level.

The problem with the example is, that with SNI we get a certificate which is valid for `*.clients.google.com` which is correct according to the standard.
Without SNI get one which is only valid under the requested validation rules.

Personally I am -1 on this.
",t-8ch,sigmavirus24
2304,2014-10-26 01:28:54,"Yeah, @t-8ch, we're in agreement here. =D
",sigmavirus24,t-8ch
2301,2014-10-24 14:29:53,"@Lukasa I just ran the tests and it's arising from `test_prepare_unicode_url` because it's creating a `PreparedRequest` and then calling `prepare` without passing any hooks. This is a poorly written test, not something that should be fixed here.
",sigmavirus24,Lukasa
2299,2014-10-24 14:52:45,"@Lukasa I'd really like to see something like `urllib3`s LRUCache used here as well.
",sigmavirus24,Lukasa
2299,2014-10-24 14:54:05,"Also thank you so very much for this @mattrobenolt 
",sigmavirus24,mattrobenolt
2299,2014-10-24 14:57:39,"I'll switch this up in a bit. I wasn't aware there was an LRUCache as a part of urllib3 already.

@sigmavirus24 do you have an opinion on the size? Is 10k alright? That number is completely arbitrary. 
",mattrobenolt,sigmavirus24
2299,2014-10-27 00:34:55,"@mattrobenolt why not cap it a bit lower? Or is 10k already ""low""?
",sigmavirus24,mattrobenolt
2299,2014-10-27 16:23:57,"@sigmavirus24 sorry for the delay. Updated to use the `RecentlyUsedContainer` instead and I dropped the cache size down to 1000. We could also make this configurable on the `Session`.
",mattrobenolt,sigmavirus24
2299,2014-10-27 16:32:22,"> We could also make this configurable on the Session

I'd rather we didn't.

@Lukasa this looks good to me. How do you like it?
",sigmavirus24,Lukasa
2299,2014-11-07 04:44:40,"LGTM. @Lukasa ?
",sigmavirus24,Lukasa
2299,2014-11-07 08:08:15,"@kevinburke it's an LRU (least recently used) algorithm. So that means that the least used objects get evicted first. Active/hot objects stay in. Similar to how memcached works. So there is no error. Objects are just evicted based on LRU.
",mattrobenolt,kevinburke
2299,2014-11-08 04:22:27,"Thank you @mattrobenolt !
",sigmavirus24,mattrobenolt
2297,2014-10-22 20:36:20,"> Would it be useful to add a keywork argument specifying if the url is encode, unencoded, or current behavior of unknown?

No. In spite of RFC 3986 there wouldn't be a good way for requests to always encode a URL. As it is now, you should be encoding this yourself as @Lukasa already pointed out. We will not be adding extra keyword arguments and we will not guess this for you.
",sigmavirus24,Lukasa
2291,2014-10-21 04:22:40,"@sigmavirus24 I am asking about a pyhton-requests functionality. Can you please give me a simple yes or no answer if this is possible?
",HulaHoopWhonix,sigmavirus24
2291,2014-10-21 12:23:51,"@Lukasa Ok I see what you mean. Any input for this question on stackoverflow would be appreciated. I posted it here:

https://stackoverflow.com/questions/26479039/python-requests-direct-pem-pinning-with-self-signed-cert
",HulaHoopWhonix,Lukasa
2290,2014-10-19 09:03:32,"@syedsuhail Quick question, why'd you change the width?
",Lukasa,syedsuhail
2289,2014-10-19 07:56:35,"Thanks for this @syedsuhail! :cake:
",Lukasa,syedsuhail
2289,2014-10-19 09:03:59,"@syedsuhail I'm glad you chose to contribute to requests!
",Lukasa,syedsuhail
2283,2014-10-15 07:09:11,"@t-8ch I haven't read up on POODLE yet: are cookies enough?
",Lukasa,t-8ch
2282,2014-10-14 15:26:26,"Thanks @morganthrapp! :cake: 
",sigmavirus24,morganthrapp
2276,2014-10-13 02:15:48,"@mikecool1000 why did you close this?
",sigmavirus24,mikecool1000
2276,2014-10-13 18:00:57,"I just didn't want try to submit work I am unsure of

Sent from my iPhone

> On Oct 12, 2014, at 7:16 PM, Ian Cordasco notifications@github.com wrote:
> 
> @mikecool1000 why did you close this?
> 
> —
> Reply to this email directly or view it on GitHub.
",mikecool1000,mikecool1000
2275,2014-10-12 18:18:55,"@Lukasa I was under the same impression frankly
",sigmavirus24,Lukasa
2264,2014-10-05 01:27:30,"@sigmavirus24 Thanks for giving me pointers for troubleshooting. The file I was originally using was haphazardly called 'urllib.py'. Once I ran Requests from a different directory it worked fine.
",evanemolo,sigmavirus24
2264,2014-10-05 01:51:44,"@kevinburke it's doubtful that this will be updated in 2.7 and in 3.x it's a non-issue. I tried adding `from __future__ import absolute_import` and that doesn't fix it either. There's not much we can do here.
",sigmavirus24,kevinburke
2263,2014-10-03 18:50:43,"This looks great @daftshady, thanks! :cake: I'll let @sigmavirus24 review, but I'm :+1:.
",Lukasa,daftshady
2263,2014-10-05 07:07:09,"@sigmavirus24 I agree with you. It's fairly reasonable to use a set here. I added another commit.
",daftshady,sigmavirus24
2263,2014-10-05 16:52:28,"Looks great now! Thanks @daftshady!
",sigmavirus24,daftshady
2262,2014-10-03 13:50:01,"This should be fixed in chardet now (I just remembered why this seemed familiar, it's because of https://github.com/chardet/chardet/issues/30). That said, this really is a chardet issue, as we've already said and as @Lukasa said, we do the right thing when you tell us you want JSON from the body. Other than that, we have to assume it falls under the domain of HTTP and that explicitly requires a `charset` parameter in `Content-Type`. We have no intention of every doing anything more than using that or trying to guess the encoding with chardet. We already do not introspect the body automatically (although we provide that as a convenience function) for meta tags or anything else that would hint the character encoding and we provide you with the ability to tell us what encoding to use.

There's enough degrees of freedom to do what you want to do. Further, if you're only ever using a session to talk to your API, you could (if you desired) do something like:



If you're not using a session, you can do:


",sigmavirus24,Lukasa
2262,2014-10-03 14:10:38,"Good to know it's fixed in chardet, thanks @sigmavirus24. Yeah, I have ways of detecting the encoding in my client code. I'll close this now.

Thanks for your time, guys.
",noseworthy,sigmavirus24
2261,2014-10-02 19:43:29,"Here I was writing a nice response and @Lukasa said it faster and better... again. ;)

Thanks for the suggestion @dequis and taking the time to file the bug. That said, you said one thing that @Lukasa didn't address and which bothered me. 

We should take the following conversation off this bug because it's entirely tangential. _Why are you copying recipes from ActiveState to do `multipart/form-data` file uploads?_ Requests provides that for you and the requests-toolbelt ensures you don't need to think about it in the case where your file is too big to be loaded into memory all at once. Feel free to email me and reply to this. 
",sigmavirus24,Lukasa
2260,2014-10-02 12:49:03,"@Hasimir is correct. Please consult the documentation before raising bugs. =)
",Lukasa,Hasimir
2258,2014-10-05 16:49:29,"@willingc thank you so much for your hard work on this. :cake: (Those emoji from @kennethreitz are for you ;))
",sigmavirus24,kennethreitz
2258,2014-10-05 17:50:15,"Thank you @sigmavirus24 for your support and mentoring :guitar: :sunrise: :evergreen_tree: , @lukasa for the review :cat: :cat2: :smile_cat:  , and @kennethreitz for the community atmosphere :camera: :v: :guitar:
",willingc,kennethreitz
2258,2014-10-05 17:50:15,"Thank you @sigmavirus24 for your support and mentoring :guitar: :sunrise: :evergreen_tree: , @lukasa for the review :cat: :cat2: :smile_cat:  , and @kennethreitz for the community atmosphere :camera: :v: :guitar:
",willingc,sigmavirus24
2258,2014-10-05 18:42:33,"@willingc thank YOU, and you're welcome :)
",kennethreitz,willingc
2257,2014-09-30 16:00:54,"@gagoman someone had been working on this for a while and stalled. I've opened a PR on their behalf. Did you read the entirety of the issue?
",sigmavirus24,gagoman
2257,2014-09-30 16:08:53,"@sigmavirus24 reopen if it is still required.
",gagoman,sigmavirus24
2257,2014-09-30 16:12:16,"@gagoman the discussion said the work would be slow but that two people were owning the work. The proper etiquette would have been to ask if they were still working in it. Thanks for your contribution, but we'll be continuing forward with their work.
",sigmavirus24,gagoman
2257,2014-09-30 16:17:19,"@sigmavirus24 got it.
",gagoman,sigmavirus24
2253,2014-09-27 19:58:41,"Hey @yossigo thanks for the pull request!

While you're here, could you fix something that I think is a bug? On [this line](https://github.com/yossigo/requests/commit/c28da22e9c42e22b303bb07da434ce65e10c0cb2#diff-9f3a95293a5d26032b1c588167760362R193) could you change it to



We still want to check to prevent users from being caught in an endless challenge state with a server.

Right now, I'm trying to decide if I think `handle_302` should check the response code before resetting the attribute because it will be called regardless of whether a 302 was received or not.

Also, @yossigo what do you think about setting `num_401_calls` to 1 in `handle_302` instead of deleting the attribute? It would be much simpler. Also, there's no need to return `r` from `handle_302`.
",sigmavirus24,yossigo
2253,2014-10-25 14:10:06,"I think this is ready to merge. Sorry I didn't see your update @yossigo 

Thanks for this!
",sigmavirus24,yossigo
2249,2014-09-26 16:12:45,"@t-8ch that's roughly `rfc3986`'s API, except that I don't think I call it `replace` but maybe I do.... I'll double check later.
",sigmavirus24,t-8ch
2249,2014-09-26 16:31:19,"@sigmavirus24 Please open the specific issue with urllib3's url parser, no need to be backhanded. :)
",shazow,sigmavirus24
2249,2014-09-26 16:35:36,"Sorry @shazow, it wasn't meant to be back handed. I'll pull together the list of things the object is missing and make an issue with it tonight.
",sigmavirus24,shazow
2249,2014-10-05 16:48:51,"@kennethreitz :+1: 
",sigmavirus24,kennethreitz
2247,2014-09-25 16:26:42,"The full request URL is only sent when sent to a proxy. I presume you have your `$HTTP_PROXY` environment variable set to Fiddler. We've detected that and used our proxy-specific handling. This is why @sigmavirus24 couldn't reproduce your behaviour. 
",Lukasa,sigmavirus24
2244,2014-09-23 14:52:33,"@RuudBurger both @Lukasa and I have our emails available on our GitHub profiles. If you'd rather use PGP, you can find my PGP key (and associated email address) by searching https://pgp.mit.edu/ for my real name.
",sigmavirus24,Lukasa
2244,2014-09-24 08:39:10,"In my opinion an exception needs to be raised and `RedirectLoopError` is always going to be a `TooManyRedirects` at the end, if you want to subclass it is fine but maybe unnecessary.

I like @sigmavirus24 set() solution, maybe you want to pack it a bit more:


",fcosantos,sigmavirus24
2244,2014-09-24 15:25:20,"@Lukasa I can think of a couple ways to make it influence the max_redirects count.
1. Move the logic into `Session#resolve_redirects`. This unfortunately would mean actually using the network for the first (possibly permanent) redirect.
2. Keep count along with the set of how many times we go through that loop and add a new optional parameter to `Session#resolve_redirects` along the lines of `redirects_already_followed=0`. We can then initialize `i` in `Session#resolve_redirects` with that and that will affect the max number of redirects possible (including using the cache).

Option 2 seems most practical, but I just loathe adding more arguments (that could confuse a user) to a public API like this.

Also, I think there's value in using a subclass of `TooManyRedirects`. I can see an instance where this might cause confusion because of a case like @RuudBurger has. In a browser, it might very well work just fine, but because of oddities in the usage of requests the loop is caused by the redirect cache. I think providing users a way to disambiguate where the error is actually coming from is very useful (and a better experience).
",sigmavirus24,Lukasa
2244,2014-09-24 16:15:38,"@Lukasa good point. I'm not sure we can actually reconstruct the history accurately unless we also cache responses in the redirect cache. In other words, we'd have a cache something like:



And a big problem with that would be timestamps in headers and such (e.g., cookies). All of this which makes me wonder exactly how good an idea it is to keep the redirect cache around.
",sigmavirus24,Lukasa
2241,2014-09-22 19:12:25,"@tijko so could you update this PR to add the described exception to `requests/exceptions.py` and then use it here where you're raising the `RuntimeError`? Thanks in advance
",sigmavirus24,tijko
2241,2014-10-05 16:47:25,"@sigmavirus24 looks like @tijko updated this as requested
",kennethreitz,sigmavirus24
2241,2014-10-05 16:47:25,"@sigmavirus24 looks like @tijko updated this as requested
",kennethreitz,tijko
2241,2014-10-05 16:48:31,"@kennethreitz looks good to me
",sigmavirus24,kennethreitz
2239,2014-10-20 22:02:48,"@sigmavirus24 is right. We are still arguing about whether or not that was ok.
",Lukasa,sigmavirus24
2238,2014-09-20 23:37:55,"I agree with @Lukasa that this is the behaviour we want. We absolutely want `unicode` urls because on Python 3, we can handle IRIs. We would need to adopt something that implements RFC 3987 to support it on Python 2, but once we did, we would be able to support them there too. For example, http://☃.net should be supported. (Your browser, and Python 3 should properly ""encode"" that to http://xn--n3h.net/.)
",sigmavirus24,Lukasa
2238,2014-09-30 18:49:08,":heart: @buttscicles 

@Lukasa this looks okay to me. Thoughts?
",sigmavirus24,Lukasa
2236,2014-09-19 12:52:30,"As @sigmavirus24 says, lots and lots of projects hook into requests and we very definitely don't want to include them in requests. =)
",Lukasa,sigmavirus24
2235,2014-09-21 11:58:16,"UTF8 is the most reasonable default.
Besides, python3 string defaults to unicode, many data read from db/http is auto convert to unicode(default utf8).
so, why not accept utf8 as default unicode encoding?
The python standard library ALREADY AUTO convert unicode to utf8 when write to socket.
(that why the chunk size is wrong, but the chunk body is ok)

@sigmavirus24 this bug only appears when using generator as data (chunked encoding)
post data=unicode works because 

> The python standard library ALREADY AUTO convert unicode to utf8 when write to socket.
",ilovenwd,sigmavirus24
2230,2014-09-18 06:09:32,"Sorry @kevinburke, it's not clear to me what the bug is here.
",Lukasa,kevinburke
2230,2014-09-18 06:45:43,"A ConnectionError implies that the server never received/began acting on
data. This way all ConnectionErrors are safe to retry.

imagine you send a POST request to a server and then wait for a response
for 30 seconds, at which point the server closes the connection without
sending data, headers, anything. Requests will raise a ConnectionError. Is
that appropriate?

On Wednesday, September 17, 2014, Cory Benfield notifications@github.com
wrote:

> Sorry @kevinburke https://github.com/kevinburke, it's not clear to me
> what the bug is here.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/2230#issuecomment-55998776
> .

## 

## 

Kevin Burke
phone: 925.271.7005 | twentymilliseconds.com
",kevinburke,kevinburke
2229,2015-04-07 11:33:11,"@benjaminran Nope, it's a corner case in how requests handles proxies.

If you pass a proxy whose proxy url scheme is 'https://', we don't use CONNECT to tunnel through it, because what's the point? Instead, we connect directly to it over TLS and then treat it like a plaintext proxy, which explains the behaviour you're seeing.
",Lukasa,benjaminran
2228,2014-09-16 15:38:11,"@Lukasa I'm surprised that we don't use the default socket timeout. I'm more surprised that the ""default"" timeout is not automatically applied to a socket. That said, I don't think there's a good (or easy way) to handle adding this at this juncture. We could consider it for 3.0 but if we truly have no timeout set on sockets by default (which I'm prepared to believe), then we can't change it easily and it _should_ be documented for now.

The thing we should determine is whether we want that behaviour at all. The sad thing is that more often than not, not being able to put a timeout on `socket.getaddrinfo` has affected me more than a connection not timing out appropriately. 
",sigmavirus24,Lukasa
2228,2014-09-16 16:44:24,"I disagree that it is sufficient as timeouts  are only mentioned at the
very bottom of the page.  Users often base their solutions off of examples
and don't read all the documentation, only what they need to get a result.
 While examples should be brief, they should not lead a user to writing
code that has a pretty strong potential to lock up.

The default socket timeout is used by urllib2 when a timeout is not
specified. This further compounds the above problem for people moving from
that library.  To have no timeout specified I can see either None or 0
being passed as the timeout value.  Explicit is better than implicit.

On Tue, Sep 16, 2014 at 9:26 AM, Cory Benfield notifications@github.com
wrote:

> Thanks for the suggestion! However, I disagree.
> 
> In many cases, users find it valuable to add a timeout to their calls.
> This is entirely reasonable. However, the quickstart code is not an example
> of 'general use', it's intended to be illustrative and educational. We
> should not obscure the pedagogical point in the pursuit of some form of
> pure correctness.
> 
> There is a timeout section
> http://docs.python-requests.org/en/latest/user/quickstart/#timeouts in
> the quickstart docs, which we believe to be sufficient. Do you disagree?
> 
> As for mentioning the default socket timeout, I hadn't spotted that. That
> may actually be a bug. @sigmavirus24 https://github.com/sigmavirus24,
> what do you think: should we use the default socket timeout when no value
> is provided? If so, how do users specify they want no timeout at all? Am I
> overthinking the whole thing?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/2228#issuecomment-55760567
> .
",mountainpaul,sigmavirus24
2228,2014-09-16 17:25:50,"> @Lukasa I'm surprised that we don't use the default socket timeout. I'm more surprised that the ""default"" timeout is not automatically applied to a socket.

Ordinarily it would be: this is a consequence of the way our library interacts with urllib3. We build a Timeout object with `None, None` as its values and send that down to urllib3. This looks for a sentinel object: if it finds it it'll set the timeout to the default. We aren't sending that sentinel object, we're sending `None, None`, which sets the socket timeout to `None`, which means it never times out. A bit sad really.

> Users often base their solutions off of examples and don't read all the documentation, only what they need to get a result.

Yes, they do. However, this is not a justification for all examples being bulletproof. Our examples don't catch all exceptions, they don't list all possible failure cases, and they don't explain some of the ins-and-outs of weirder parts of the library. I'd argue they shouldn't. That overwhelms beginners by giving them far too many things to focus on at once, which prevents them from taking away the key lesson.

Our position is that documentation should not be optimised for copy-and-paste coding sessions. You can see this by the fact that our quickstart code examples usually aren't code blocks, but representations of an interactive session. They are intended as a guide, not an example of how to build a production-ready requests application.

The correct way to handle this is not to litter our code examples with irrelevant noise to avoid a footgun in production code, but to remove the footgun entirely. Users should not be subject to the TCP timeout (often very long) in the _default_ case, they should be subject to something vaguely reasonable (in this case the default socket timeout).

Note also that the timeouts are not a panacea. The timeouts apply to each connection event, but if `getaddrinfo` returns many many IP addresses it can take a _very_ long time to time them all out. This is not a _hang_, but it's frequently very close to one (and indeed we've seen it reported that way in the past).

> Explicit is better than implicit.

This is a point that is extremely unclear with the Python community. Let me show you something that is unarguably explicit:



_Nothing_ happens automatically here, everything is stated explicitly. Realm, the URL to apply to, everything. Here's what we do:



There is so much that is implicit here. The auth type is implicit (Basic), the realm it applies to is not specified (because _no-one cares_, the realm is basically irrelevant on Basic auth), the URL it applies to is inferred by the library (anything on `http://www.example.com/`). But here's the thing: people love our way, and hate urllib2's way.

The Zen of Python is great, but every line of it needs to be followed by asterisks for qualification. It is not the dogma of Python. You cannot wave it at a solution you do not like to force compliance. And for every clause in it that supports your point, I can find plenty that don't:

> Beautiful is better than ugly.
> Simple is better than complex.
> Readability counts.

We have timeouts, and we mention them in their own section in the quickstart guide. Users who care will find it. And if they don't find it, they clearly don't care about errors at all because the ""Errors and Exceptions"" section is _below_ the timeouts section!

Our documentation cannot accommodate all cases, so we optimise. The quickstart guide is just that: a quickstart guide. We cannot prevent users doing something else with it, but that shouldn't change what _we_ do with it.

**tl;dr**: I think the documentation isn't the problem, the default behaviour is the problem.
",Lukasa,Lukasa
2228,2014-09-16 18:17:47,"@Lukasa hit every point on the head. Moving forward we can repurpose this issue to discuss setting the default socket timeout _or_ we can close this and open a new issue to discuss that.
",sigmavirus24,Lukasa
2228,2014-09-16 20:32:29,"On Tue, Sep 16, 2014 at 11:26 AM, Cory Benfield notifications@github.com
wrote:

> @Lukasa https://github.com/Lukasa I'm surprised that we don't use the
> default socket timeout. I'm more surprised that the ""default"" timeout is
> not automatically applied to a socket.
> 
> Ordinarily it would be: this is a consequence of the way our library
> interacts with urllib3. We build a Timeout object with None, None as its
> values and send that down to urllib3. This looks for a sentinel object: if
> it finds it it'll set the timeout to the default. We aren't sending that
> sentinel object, we're sending None, None, which sets the socket timeout
> to None, which means it never times out. A bit sad really.
> 
> Users often base their solutions off of examples and don't read all the
> documentation, only what they need to get a result.
> 
> Yes, they do. However, this is not a justification for all examples being
> bulletproof. Our examples don't catch all exceptions, they don't list all
> possible failure cases, and they don't explain some of the ins-and-outs of
> weirder parts of the library. I'd argue they shouldn't. That overwhelms
> beginners by giving them far too many things to focus on at once, which
> prevents them from taking away the key lesson.
> 
> Our position is that documentation should not be optimised for
> copy-and-paste coding sessions. You can see this by the fact that our
> quickstart code examples usually aren't code blocks, but representations of
> an interactive session. They are intended as a guide, not an example of how
> to build a production-ready requests application.
> 
> The correct way to handle this is not to litter our code examples with
> irrelevant noise to avoid a footgun in production code, but to remove the
> footgun entirely. Users should not be subject to the TCP timeout (often
> very long) in the _default_ case, they should be subject to something
> vaguely reasonable (in this case the default socket timeout).
> 
> I completely agree with this, my suggestion was only for if you decided to
> leave the ""footgun"" in place.   I would not consider adding the one
> parameter, littering the code with irrelevant noise, but this maybe a
> matter of taste as to where to draw the line and can appreciate your point
> of view.
> 
> Note also that the timeouts are not a panacea. The timeouts apply to each
> connection event, but if getaddrinfo returns many many IP addresses it
> can take a _very_ long time to time them all out. This is not a _hang_,
> but it's frequently very close to one (and indeed we've seen it reported
> that way in the past).
> 
> Thanks for bringing that to my attention, and that is an interesting point.
> 
>  Explicit is better than implicit.
> 
> This is a point that is extremely unclear with the Python community. Let
> me show you something that is unarguably explicit:
> 
> Good point. Although I still believe it is better to be explicit when the
> behavior differs from the expected (using the default socket timeout).
> 
> import urllib2auth_handler = urllib2.HTTPBasicAuthHandler()auth_handler.add_password(realm='PDQ Application',
>                           uri='https://mahler:8092/site-updates.py',
>                           user='klem',
>                           passwd='kadidd!ehopper')opener = urllib2.build_opener(auth_handler)urllib2.install_opener(opener)urllib2.urlopen('http://www.example.com/login.html')
> 
> _Nothing_ happens automatically here, everything is stated explicitly.
> Realm, the URL to apply to, everything. Here's what we do:
> 
> import requestsr = requests.get('http://www.example.com/login.html', auth=('klem', 'kadidd!ehopper')
> 
> There is so much that is implicit here. The auth type is implicit (Basic),
> the realm it applies to is not specified (because _no-one cares_, the
> realm is basically irrelevant on Basic auth), the URL it applies to is
> inferred by the library (anything on http://www.example.com/). But here's
> the thing: people love our way, and hate urllib2's way.
> 
> The Zen of Python is great, but every line of it needs to be followed by
> asterisks for qualification. It is not the dogma of Python. You cannot wave
> it at a solution you do not like to force compliance. And for every clause
> in it that supports your point, I can find plenty that don't:
> 
> Beautiful is better than ugly.
> Simple is better than complex.
> Readability counts.
> 
> We have timeouts, and we mention them in their own section in the
> quickstart guide. Users who care will find it. And if they don't find it,
> they clearly don't care about errors at all because the ""Errors and
> Exceptions"" section is _below_ the timeouts section!
> 
> Our documentation cannot accommodate all cases, so we optimise. The
> quickstart guide is just that: a quickstart guide. We cannot prevent users
> doing something else with it, but that shouldn't change what _we_ do with
> it.
> 
> _tl;dr_: I think the documentation isn't the problem, the default
> behaviour is the problem.
> 
> AGREED! I just didn't know if you would be amenable to the change your
> proposing and I whole heartily agree with.

Thank you one and all for your attention to this matter.  It is greatly
appreciated

> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/2228#issuecomment-55779615
> .
",mountainpaul,Lukasa
2228,2014-09-16 20:35:33,"Excellent, I think we're all on the same page! Now we just have to work out whether we can make this change before 3.0 or whether it's likely to break too many people's code. @sigmavirus24, you suggested we'd have to wait until 3.0: what makes you say that?
",Lukasa,sigmavirus24
2228,2016-08-14 15:11:00,"Hi,

sorry if I'm missing the obvious here, I'm rather new to Python (and to requests as well, of course) and I was thinking of picking this up. I noticed that @Lukasa did some changes in July 2015 after the last comment (https://github.com/kennethreitz/requests/issues/commit/e9a1e35f8989691afe99407d9b65a67684aa8bac). After looking at the code I'm not even sure if I'm getting the point here. Is this one still open? What is the desired behavior?
",Brausepaul,Lukasa
2228,2016-08-16 20:14:07,"@Brausepaul I would be delighted to merge a PR that adds a note in the documentation to clarify this.

Hopefully in the longer term we'll get rid of httplib and get to a place where we can set timeouts in a substantially more sensible manner than we currently do.
",Lukasa,Brausepaul
2227,2014-09-16 13:10:21,"@Lukasa I'm -0 about the reference in the quickstart guide to the advanced section. How do you feel about it?
",sigmavirus24,Lukasa
2225,2014-09-15 18:29:45,"@sigmavirus24, thanks
I send this 



at answer I get



how I may look send or not data at request? 

Then I print header



I look this
http://c2n.me/iUQk1B
",temi4,sigmavirus24
2224,2014-09-12 15:47:27,"@blueyed thanks but we do not follow `pep8` in requests. 
",sigmavirus24,blueyed
2224,2014-09-12 16:09:19,"@sigmavirus24 
It's not about pep8, but comes from frosted (and maybe other type (not style) checkers).
",blueyed,sigmavirus24
2223,2014-09-12 16:09:46,"Look good to me. I'll just make sure Jenkins is happy with it and them I'll merge it. Thanks @blueyed 
",sigmavirus24,blueyed
2222,2014-09-12 02:46:45,"@blueyed I elaborated on this change in the issue you opened on shazow/urllib3. This allows for third-party developers to create adapters using the `file` protocol and has the side-effect of allowing `data` URLs as well even though requests will not handle them.
",sigmavirus24,blueyed
2222,2014-09-12 03:05:02,"I see.

So it's probably expected and sane to get a `InvalidSchema: No connection adapters were found for 'localhost:5432'` exception instead of `MissingSchema: Invalid URL 'localhost:5432': No schema supplied. Perhaps you meant http://localhost?` then.

I've thought about adding a regex that would let it fall through, but it doesn't change much in the end.

The expected outcome should get added as test however.

@jvantuyl 

> the error you're getting (although as an AssertionError, which is probably bad form). Perhaps you could replace that assertion with a different error (say UnrecognizedScheme), check for it, then implement the fallback behavior?

I do not understand this. It appears to be a proper exception already (`InvalidSchema`).
Where do you mean to add the check and fallback?

I came to this issue through pip and the handling of its proxy setting initially (IIRC, and unrelated).
",blueyed,jvantuyl
2221,2014-09-12 06:19:58,"My objection to this functional change is simpler than @sigmavirus24's. URLs are not strings, and treating them like strings is how mistakes get made. I think the correct approach would be to try to use `urllib3`'s URL utilities for this.
",Lukasa,sigmavirus24
2221,2014-09-12 13:39:06,"Thanks for your feedback.

@Lukasa 
I've initially used urllib3's `Url` (https://github.com/blueyed/requests/commit/1bcc46e50661a68e08723571ac42d8069d2b4063 - was amended), but then noticed that I would have to handle `auth` explicitly, in the same way it gets done in PrepareRequest (IIRC), and ""simplified"" it.

If urllib3 had a method to ""urlunparse"" (proposed in https://github.com/shazow/urllib3/pull/394), this could be used here. But then it seems that urllib3 would also fail for `//localhost:80`, because my simplification was inspired by its scheme detection: https://github.com/shazow/urllib3/blob/master/urllib3/util/url.py#L111-L113:



For now, I propose to only change the documentation to state that the default scheme is ""http"".
What about the documentation at https://github.com/kennethreitz/requests/blame/master/docs/api.rst#L238-249 ?

As for the approach to simplify it, it could get expanded to also check for `//` at the beginning of url.

Given that the function is meant to only prepend a scheme, it should be enough to detect if a scheme exists already, and that does not require to parse all of the url in the end.

Side note: about the validity/usefulness of `//localhost`: would/could this be interpreted to be `https://localhost` for `https_proxy` and `http://localhost` for `http_proxy`? (relative to the protocol that is being accessed).
([RFC](http://tools.ietf.org/html/rfc3986#page-26)).
",blueyed,Lukasa
2221,2014-09-12 14:38:01,"@Lukasa if you build a good parser for RFC 3986 then it will. [rfc3986](/sigmavirus24/rfc3986) for example parses the examples here with ease:



The only hiccup is that RFC 3986 does not define every URI to have the structure `scheme://authority{/path}{?query}{#fragment}`. A valid URI, for example is: `mailto:user@domain.com:port` and in this case `mailto` is the scheme, so the following happens:



Sadly this isn't a bug and attempting to special case it will lead only to trouble in all likelihood.
",sigmavirus24,Lukasa
2217,2014-09-10 18:37:56,"@Lukasa this is a duplicate of https://github.com/kennethreitz/requests/issues/2117
",sigmavirus24,Lukasa
2216,2014-09-10 16:27:28,"@kevinburke always read the source ;)
",sigmavirus24,kevinburke
2216,2014-09-10 20:02:47,"@kevinburke how does ""If you need to provide granular detail about what to retry and how many times to retry it, import urllib3's `Retry` class and pass that instead."" sound?
",sigmavirus24,kevinburke
2216,2014-09-10 20:16:08,"Good! Id maybe pass a link to the object or the urllib3 docs too

On Wednesday, September 10, 2014, Ian Cordasco notifications@github.com
wrote:

> @kevinburke https://github.com/kevinburke how does ""If you need to
> provide granular detail about what to retry and how many times to retry it,
> import urllib3's Retry class and pass that instead."" sound?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/pull/2216#issuecomment-55174200
> .

## 

## 

Kevin Burke
phone: 925.271.7005 | twentymilliseconds.com
",kevinburke,kevinburke
2216,2014-09-11 17:35:01,"@kevinburke I would guess that `MaxRetryError` inherits from `ProtocolError` somewhere. While I have no strict opinions about the exception hierarchy in urllib3, this could simply mean that we need to re-order our except clauses in requests.
",sigmavirus24,kevinburke
2216,2014-10-25 03:24:02,"@kennethreitz I removed the sentinel object and added a quick test. @kevinburke also pointed out that it makes sense for this to be able to raise a new exception since people will not run into this if they're not using the new logic.
",sigmavirus24,kennethreitz
2216,2014-10-25 03:24:02,"@kennethreitz I removed the sentinel object and added a quick test. @kevinburke also pointed out that it makes sense for this to be able to raise a new exception since people will not run into this if they're not using the new logic.
",sigmavirus24,kevinburke
2216,2014-10-25 04:47:59,"LGTM!

(_pretends to know how to add a cake emoji_)

On Friday, October 24, 2014, Ian Cordasco notifications@github.com wrote:

> @kennethreitz https://github.com/kennethreitz I removed the sentinel
> object and added a quick test. @kevinburke https://github.com/kevinburke
> also pointed out that it makes sense for this to be able to raise a new
> exception since people will not run into this if they're not using the new
> logic.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/pull/2216#issuecomment-60470082
> .

## 

## 

Kevin Burke
phone: 925.271.7005 | twentymilliseconds.com
",kevinburke,kennethreitz
2216,2014-10-25 04:47:59,"LGTM!

(_pretends to know how to add a cake emoji_)

On Friday, October 24, 2014, Ian Cordasco notifications@github.com wrote:

> @kennethreitz https://github.com/kennethreitz I removed the sentinel
> object and added a quick test. @kevinburke https://github.com/kevinburke
> also pointed out that it makes sense for this to be able to raise a new
> exception since people will not run into this if they're not using the new
> logic.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/pull/2216#issuecomment-60470082
> .

## 

## 

Kevin Burke
phone: 925.271.7005 | twentymilliseconds.com
",kevinburke,kevinburke
2216,2014-11-01 14:38:14,"Ping @kennethreitz ;)
",sigmavirus24,kennethreitz
2216,2014-11-12 19:58:21,"@kennethreitz I've rebased this.
",sigmavirus24,kennethreitz
2216,2014-12-25 14:52:02,"@sigmavirus24 Sorry, I have searched some places on documents, but found nothing, can you provide me an URL?
",dawncold,sigmavirus24
2216,2015-02-06 00:45:42,"@sigmavirus24 no need to be smug :)
",kennethreitz,sigmavirus24
2215,2014-09-10 13:07:14,"Ok @sigmavirus24 , I didn't know your policy about that but that sounds consistent.

> That said, I'm +0 on the proposal of not returning anything that is non-positive from `super_len`.

I totally agree with you. This is definitely not acceptable, `super_len` has specific behavior depending the object it gets. So testing the non-positive value should be considered _only_ when using st_size of the stat of the fileno.

> Sending a chunked upload of zero length is a bit weird, but not the end of the world, and it works fine.

Maybe that's the best solution after all...

...or not guessing the size of a file descriptor no matter what (and let the user deal with it)? This super power may looks handy but at the end this is not an accurate behavior.
",cecton,sigmavirus24
2215,2014-09-11 10:00:52,"No there is no way it would enter in memory... Actually they are database dumps and it seems, some of them are huge like 13GB.

But you're right, it doesn't look complicated to implement. I will think of it if I want to implement it for the server. On werkzeug I can use `request.input_stream` to access the raw body, so it's definitely possible, but I prefer to keep the code clean from very specific handling like this.

But thank you very much for your help @Lukasa here and on urllib3. (^_^)/
",cecton,Lukasa
2214,2014-09-09 22:24:09,"At this point it's fairly obvious that @Lukasa and I are -1 on this feature. @kennethreitz @shazow any opinions?
",sigmavirus24,Lukasa
2214,2014-09-09 22:25:29,"To some extent, I agree that the warnings are important, but I think that there are multiple factors that might need to be considered.

From a developer perspective, know that I know about this, I can just turn this off if I feel inclined. I'm newer to packages, so it when I read the docs, in the warning, that solution didn't actually work. I like the idea that @Lukasa presented about making something that 's specific to `requests`. 

From a user perspective, I installed `pyvmomi` with `pip`today, which uses `requests` in its internals. It's really a non-transparent error that gets emitted back to the user in a case where `requests` is a silent supporting library.
",invisiblethreat,Lukasa
2214,2014-10-09 07:25:30,"Just wanted to say I agree with @kankri and that @kennethreitz's remark

> verify=False is a feature, albeit not a best-practice one. That's none of our business. 

sums it up well.
",piotr-dobrogost,kennethreitz
2214,2015-01-29 12:17:23,"@Lukasa 

> Or simply do this:
> 
> 

Except that I find no mention of this function in the requests manual.

While it is far from everybody who knows about it, I would argue that the `warnings` module _is_ the standard tool a Python programmer should look to when he wants to disable warnings. It is part of the standard library and is well documented.

I suggest putting a reference to `warnings` into the `requests` documentation — or to the convenience `disable_warnings` function if you like, as long as there is a corresponding `enable_warnings` function (it [seems that there isn't such a function](https://urllib3.readthedocs.org/en/latest/security.html#insecurerequestwarning)).
",mgeisler,Lukasa
2214,2016-02-06 07:25:18,"Hi @Lukasa,

I put the breakpoint after the if. In the end I stopped using debian testing as I came across too many issues, and this may very well be one of them. I would just ignore my comment, I am not sure what happened but it is likely not something that will affect a lot of people.

Thanks!
Pedr
",droope,Lukasa
2209,2014-09-07 15:27:38,"Final comment of the morning (as I have other things I have to do), I'm not sure there is anything preventing us from moving that consumption back to `HTTPAdapter#send`. I'm genuinely interested in @schlamar's reason for moving it though if they can remember it.

Also, @heyman thank you for changing the test style. We may not end up merging this right away, but your cooperation and bug report are much appreciated.
",sigmavirus24,heyman
2209,2014-09-07 15:42:40,"@sigmavirus24 Thanks for your thorough response!

Just a quick note. I tried to move the consumption of r.content back into HTTPAdapter.send. As expected it fixes the test I've supplied, but it also makes the `test_redirect_with_wrong_gzipped_header` test (https://github.com/kennethreitz/requests/blob/359659cf4b9dbeeef1ed832501dc1f99b0f0beac/test_requests.py#L978) fail with a ContentDecodingError.
",heyman,sigmavirus24
2209,2014-09-07 19:00:18,"The redirect issue is exactly why we moved it. We were encountering problems on redirect where we'd barf on invalid response bodies, even though we didn't really need them to proceed. We judged that to be a bad idea, so we moved the decode to ensure that it would happen in a guarded block for redirects, but still fail hard for non-redirected requests.

As @sigmavirus24 has observed, this bug is an accidental consequence of that change. Thanks for the fix, I'll review it shortly.
",Lukasa,sigmavirus24
2209,2014-09-07 19:01:27,"Ah, I misunderstood: this PR is the test, but not the fix. @heyman were you planning to provide the fix as well? Were you looking for guidance in how to do that?
",Lukasa,heyman
2209,2014-09-08 02:00:19,"I've been thinking about this for most of today and I'm not certain that there's a _good_ way to solve this. I had a similar idea to yours @heyman but one of the catches with that is the use case where the user decides to handle redirects themselves, e.g.,



In that case, `r`'s content will not be consumed. Prior to this, it was guaranteed to be consumed (unless the user specifies `stream=True`). In this case, we're breaking that guarantee for the user.
",sigmavirus24,heyman
2209,2014-09-09 10:01:15,"@heyman Yes. That's considered to be OK: with allow_redirects set to false you really need to handle all the redirect logic yourself. We simply don't know what you want to do.
",Lukasa,heyman
2209,2015-04-02 18:44:21,"@sigmavirus24 and I talked about doing a big issue and pull request cleanup at PyCon.

> On 2 Apr 2015, at 19:38, Kenneth Reitz notifications@github.com wrote:
> 
> Okay, we currently have 10 pull requests open for this project, many of which are labelled ""do not merge"".
> 
> Let's get this cleaned up.
> 
> —
> Reply to this email directly or view it on GitHub.
",Lukasa,sigmavirus24
2209,2015-04-05 16:03:15,"I think our documentation now very clearly illustrates that the elapsed time is always the time to the first byte of the body. Which means that this can be closed as we no longer expect this behaviour. @Lukasa or @kennethreitz should re-open this if they disagree.
",sigmavirus24,kennethreitz
2209,2015-04-05 16:03:15,"I think our documentation now very clearly illustrates that the elapsed time is always the time to the first byte of the body. Which means that this can be closed as we no longer expect this behaviour. @Lukasa or @kennethreitz should re-open this if they disagree.
",sigmavirus24,Lukasa
2208,2014-09-07 19:09:16,"@mlissner it's above all of our paygrades ;)
",sigmavirus24,mlissner
2208,2014-09-07 19:21:08,"@mlissner Trust me, this is one of those situations where you'd really rather not know. Get out, get out now. This way lies madness. Dive not into the murky world of HTTP, for monsters lurk in the depths.
",Lukasa,mlissner
2207,2014-09-06 16:49:07,"Don't be too happy @Lukasa. =P
",sigmavirus24,Lukasa
2206,2014-09-05 21:23:43,"Thanks for the thorough reply, @sigmavirus24.

> There is a good reason not to do it on the module scope. That function is so rarely used by 98% of requests users that triggering the compile when they do import requests will hurt their performance for no real benefit.

A typical solution is to compile them lazily (on first use) then, no? That way when you don't need to use the function you don't have to pay for it, and when you do, you're not punished for it on every call.

That said, out of curiosity I measured the performance impact of adding



to an otherwise empty module, and the difference was indistinguishable from noise. Did I measure wrong?

> That said, how would you determine which was the first encoding in the body? You could do something like...

(I was even thinking something simpler, something like:



)

> But that seems kind of arbitrary. On the one hand, returning all of them is also arbitrary but if we had written it the way you want, someone else would have wanted us to write it the way it is currently written.

Hm. Having a function that returned the first match rather than all matches for all three patterns makes the use case of getting the charset declared in the head of a large html document much more efficient. That seems like a primary use case for this function, in light of `get_unicode_from_response`'s docstring (though see #2205). So it's not arbitrary. At least, an additional `get_first_encoding_from_content` function could be offered to facilitate this use case.

I did a github code search for `get_encodings_from_content` to see how people are using this and so far most are using it like [this](https://github.com/ackwell/ninjabot/blob/68163212380a1cd82fda4e0c6e0d1b09a7c5a64e/plugins/web/linkinfo.py#L49..L51):



I'm not sure if you've gotten any requests for it to be the way it's currently written, but the way I'm proposing seems more efficient for the use cases I'm finding.

Thanks for the pointer to #2086. I'll keep an eye on that and also look forward to others' feedback on this.
",jab,sigmavirus24
2206,2014-09-05 23:42:21,"> A typical solution is to compile them lazily (on first use) then, no? That way when you don't need to use the function you don't have to pay for it, and when you do, you're not punished for it on every call.

Yeah, I'm not adverse to a pattern like this:



> That said, out of curiosity I measured the performance impact [...] Did I measure wrong?

You didn't show us how you measured it.

> At least, an additional get_first_encoding_from_content function could be offered to facilitate this use case.

I'm not sure it's necessary though. I understand there are a lot of people using this just like you describe but I also found plenty of people using the entire list. 

> I'm not sure if you've gotten any requests for it to be the way it's currently written, but the way I'm proposing seems more efficient for the use cases I'm finding.

I wouldn't be surprised if it were. That said, I'd like to reiterate @Lukasa's assertion that requests is fundamentally a HTTP library. I have not so quietly been an advocate of not having this in `requests.utils` at all. Further a function as you describe is something I don't feel comfortable including. If we include the first of the three that match, then someone will want to reorder the list of regular expressions we use. That will only lead to pain for @Lukasa and myself. The whole soup-y mess that consistently results in the `get_encodings_from_content` function is enough to make me want to push to remove it yet again.

Few people can agree on how it should behave, and some of the decisions have been quite arbitrary.
",sigmavirus24,Lukasa
2206,2014-09-06 08:52:50,"My position here is basically a less negative version of @sigmavirus24's. I simple _don't care_ about these functions. They used to be used by requests pre-1.0 and aren't now, but we left them in because some people found them and used them. They aren't documented and the maintainers don't advertise them, but ordinarily they don't represent a maintenance cost to us.

However, fundamentally, all of these functions are _trivial_ utilities. My recommendation is that, rather than change behaviour or add yet another function I'll studiously ignore, people should just copy the function out of `utils.py` and change it to do what they need it to do. It should take all of 10 seconds. =)
",Lukasa,sigmavirus24
2206,2014-09-11 16:57:28,"I think we've come to the conclusion that we will not be fixing this then. @Lukasa are you okay with my closing this?
",sigmavirus24,Lukasa
2205,2014-12-07 18:56:03,"Good catch @ibnIrshad 
",sigmavirus24,ibnIrshad
2204,2014-09-04 19:15:35,"@Lukasa `retries=False` does not raise `MaxRetryError`s, not sure if the same is true for `retries=0`.
",sigmavirus24,Lukasa
2204,2014-09-04 19:21:53,"@Lukasa I think this is fixed in #2193 since this is very related to #2192.
",sigmavirus24,Lukasa
2201,2014-09-04 16:50:16,"@Lukasa still doesn't work:


",stantonk,Lukasa
2201,2014-09-04 16:51:59,"@t-8ch system time is correct.

Oddly, requests made with the oauth2 lib works when I use certify:


",stantonk,t-8ch
2201,2014-09-05 17:57:43,"@Lukasa i did try it with cerifi above, it still fails


",stantonk,Lukasa
2201,2014-09-05 18:02:44,"@t-8ch NICE! That exposed the issue. The problem is that we were statically routing in /etc/hosts through an Nginx proxy with a self-signed cert :) Closing as this is not a requests issue.

Thanks for all your help guys!
",stantonk,t-8ch
2199,2014-09-03 19:00:03,"@sigmavirus24 Why do we think it might?
",Lukasa,sigmavirus24
2195,2014-08-30 08:02:08,"I agree that it won't hurt us to have, but I agree with @t-8ch that we should avoid the slightly prejudicial name. Maybe just call it `pyopenssl`?
",Lukasa,t-8ch
2195,2014-09-05 15:15:30,"@t-8ch false
",kennethreitz,t-8ch
2195,2014-09-05 15:44:02,"@t-8ch I don't understand your concerns in this statement:

> In requests < 2.4.0 The syntax error will even bubble up and just crash everything.

The extra does not ship with anything `<= 2.4.0`. How does this affect those people?
",sigmavirus24,t-8ch
2194,2014-08-29 21:40:12,"@kevinburke I'll take care of that as I pull this into my PR. Thanks
",sigmavirus24,kevinburke
2193,2014-08-29 21:25:46,"@kevinburke are you okay with me pulling #2194 in here to keep things simple?
",sigmavirus24,kevinburke
2193,2014-08-29 21:29:20,"Yep

## 

Kevin Burke
phone: 925.271.7005 | twentymilliseconds.com

On Fri, Aug 29, 2014 at 2:26 PM, Ian Cordasco notifications@github.com
wrote:

> @kevinburke https://github.com/kevinburke are you okay with me pulling
> #2194 https://github.com/kennethreitz/requests/pull/2194 in here to
> keep things simple?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/pull/2193#issuecomment-53931737
> .
",kevinburke,kevinburke
2193,2014-09-07 16:50:37,"@sigmavirus24 should we cut it?
",kennethreitz,sigmavirus24
2193,2014-09-07 16:56:33,"@kennethreitz I think we should. This also forced OpenStack to pin to 2.3.0 in some (if not all) projects.
",sigmavirus24,kennethreitz
2193,2014-09-07 16:57:40,"This also is preventing requests upgrade in pip

On Sunday, September 7, 2014, Ian Cordasco notifications@github.com wrote:

> @kennethreitz https://github.com/kennethreitz I think we should. This
> also forced OpenStack to pin to 2.3.0 in some (if not all) projects.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/pull/2193#issuecomment-54752715
> .

## 

## 

Kevin Burke
phone: 925.271.7005 | twentymilliseconds.com
",kevinburke,kennethreitz
2192,2014-08-29 19:37:25,"Looking at it right now @Lukasa 
",sigmavirus24,Lukasa
2184,2014-09-14 22:55:49,"@kennethreitz yeah, pytest makes this a bit awkward, so I just released a new version of pytest-httpbin to make this easier.  Check out this section of the readme: https://github.com/kevin1024/pytest-httpbin#using-pytest-httpbin-with-unittest-style-test-cases

Here is the example usage:


",kevin1024,kennethreitz
2184,2015-11-10 15:40:02,"To be clear, @kennethreitz, the current test suite has been ported to use pytest-httpbin. =D At PyCon 2016 we'll probably want to have a discussion about the longer-term approach to testing in requests, but for now this is good enough. =)
",Lukasa,kennethreitz
2181,2014-08-26 18:58:37,"@sigmavirus24 Nope, that's a graph of network throughput (IIRC). We should be able to get this at least as fast, even if it means working out what the hell httplib is doing.
",Lukasa,sigmavirus24
2181,2014-08-26 19:16:27,"@sigmavirus24  we should document this wealth of information you have in your brain :)
",kennethreitz,sigmavirus24
2181,2014-08-26 19:31:01,"@sigmavirus24 The graph is from the networking tab of Windows Task Manger. It is indeed showing network throughput. 

Here is the code for `requests` using `MultipartEncoder`. 



And here is the code that is just using `requests`



And lastly, the command for `cURL`



Here is the output from `cURL`



It does not seem to make a difference whether `MultipartEncoder` is in use. Both uploads using `requests` take ~16.5 seconds. 
",Tech356,sigmavirus24
2176,2014-08-29 14:28:02,"@sigmavirus24 that can be 2.4.1 :)
",kennethreitz,sigmavirus24
2176,2014-08-29 14:28:13,"@kevinburke want to add some docs? :)
",kennethreitz,kevinburke
2176,2014-08-29 17:59:30,"@kennethreitz I added this section on timeouts:
http://docs.python-requests.org/en/latest/user/advanced/#timeouts

Just noticed I missed some method signatures, will add the docs for those
now.

## 

Kevin Burke
phone: 925.271.7005 | twentymilliseconds.com

On Fri, Aug 29, 2014 at 7:28 AM, Kenneth Reitz notifications@github.com
wrote:

> @kevinburke https://github.com/kevinburke want to add some docs? :)
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/pull/2176#issuecomment-53882868
> .
",kevinburke,kennethreitz
2176,2014-08-29 17:59:30,"@kennethreitz I added this section on timeouts:
http://docs.python-requests.org/en/latest/user/advanced/#timeouts

Just noticed I missed some method signatures, will add the docs for those
now.

## 

Kevin Burke
phone: 925.271.7005 | twentymilliseconds.com

On Fri, Aug 29, 2014 at 7:28 AM, Kenneth Reitz notifications@github.com
wrote:

> @kevinburke https://github.com/kevinburke want to add some docs? :)
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/pull/2176#issuecomment-53882868
> .
",kevinburke,kevinburke
2169,2014-08-27 05:36:26,"@kevinburke Interesting! I didn't have wireshark working at the time so I didn't check it. You're right though, we just aren't emitting it at all. Not quite so bad. =)

I'm with @sigmavirus24 on this: we should definitely set it by default, but we should hold off until at least 2.4.0 and possible until 3.0.
",Lukasa,kevinburke
2169,2014-08-27 05:36:26,"@kevinburke Interesting! I didn't have wireshark working at the time so I didn't check it. You're right though, we just aren't emitting it at all. Not quite so bad. =)

I'm with @sigmavirus24 on this: we should definitely set it by default, but we should hold off until at least 2.4.0 and possible until 3.0.
",Lukasa,sigmavirus24
2169,2014-08-27 14:54:57,"@tychotatitscheff I believe that's closer to what we want to do, yeah. We'll see, it depends a bit on what @kennethreitz wants.
",Lukasa,kennethreitz
2169,2014-08-28 17:02:47,"@sigmavirus24 what were the results of your investigation? I intend to cut 2.4.0 shortly, which will add the proper header in the defaults. 
",kennethreitz,sigmavirus24
2169,2014-08-28 17:06:28,"They were inconclusive. From what I can tell I don't think there will be problems, but I can't be 100% certain. The alternative to @Lukasa's suggestion of using `'close'` is also to set the value to `None`.
",sigmavirus24,Lukasa
2168,2014-08-19 07:37:37,"@Lukasa I will try to come up with a better way to test.
",ContinuousFunction,Lukasa
2168,2014-08-21 17:10:46,"@Lukasa I believe I have made the appropriate changes.
",ContinuousFunction,Lukasa
2165,2014-08-12 17:44:54,"OK, got familiar with tcpdump .  It's so much easier than Wireshark for analysis.  Issue is not with requests.  In case (b) and (d) from original post, server was returning HTTP response with content length of 132 (as expected).  In case (b), the JSON response was included to fulfill the length of 132.  In case (d), the JSON response was not included so I guess client is waiting for the JSON response that never comes and then times out.  Weird thing is, once I started debug printing out the request content on the server-side flask app, I started getting the JSON response for L>3487 and things are working perfectly now.  But anyway, thanks @Lukasa for your help.
",ekw,Lukasa
2164,2016-11-02 11:42:10,"@Lukasa understand that... talking to a proprietary server. Just thinking what can be done to help investigate this issue further.
",kagupta28,Lukasa
2163,2014-08-12 15:41:18,"@Lukasa that was why I asked question 5. I just didn't want to jump the gun on the association.
",sigmavirus24,Lukasa
2163,2014-08-27 16:08:54,"@sigmavirus24 
Sorry for late answer.
- Random https proxies from public lists
- Traceback


- Python 2.7.6
- tested on requests 2.2.1, requests 2.3.0
- Looks like it's fixed in master.
",m-novikov,sigmavirus24
2159,2014-08-02 12:37:04,"Hi, thanks for raising this issue!

This is a known bug in the Python standard library, [issue 17849](http://bugs.python.org/issue17849). I have submitted a patch for this bug upstream, but it hasn't been merged at this time (despite being submitted in January). The best way to get this properly fixed is to try to agitate to get that patch merged.

We _may_ be able to fix this by doing #1869. @sigmavirus24, do you think it's worth it?
",Lukasa,sigmavirus24
2159,2015-01-18 20:21:18,"@Lukasa didn't we merge a PR that fixes this?
",sigmavirus24,Lukasa
2158,2016-04-06 14:22:48,"@Lukasa what are your thoughts/feelings on a `Response.origin`? Seems like it would be a nice addition to me. 
",kennethreitz,Lukasa
2155,2014-08-01 13:48:51,"@sigmavirus24 Thanks, that's definitely an elegant solution to the problem I outlined above!

I would recommend adding that to requests' documentation, e.g. in the FAQ: http://docs.python-requests.org/en/latest/community/faq/#encoded-data  
Currently, the statement ""Requests automatically decompresses gzip-encoded responses"" is not correct for the `stream=True` case and can lead to surprises.

As for my problem, as you've read on the [urllib3 issue](https://github.com/shazow/urllib3/issues/437), the urllib3 implementation of the gzip decompression has its own little quirks I have to work around in my code, but that is no longer a problem for requests.
",hheimbuerger,sigmavirus24
2155,2014-08-01 14:05:43,"@sigmavirus24 I believe it should be documented, as the current documentation is incorrect.

But if you disagree with that, yes, close away!
",hheimbuerger,sigmavirus24
2153,2014-07-30 03:42:48,":heart: Thanks @jschneier !

LGTM! :+1: 
",sigmavirus24,jschneier
2152,2014-07-27 16:52:19,"@Lukasa Because it is very convenient, can save lots of code (let's just say a project uses request. This project uses about 100 times a request. So, it _could_ save 200 lines of code, and prevent typos because I would have to write the exception 100 times (in case I don't use a method for raising an exception), and it unifies exceptions/errors (another API may use requests and raise a custom error in case of an invalid status code. I like it better if it throws the same exception as requests does). 

@sigmavirus24  Sorry, I didn't know this method exists. You should mention that  an HTTPError can be raised by raise_for_status in Error and Exceptions section in the docs. An HTTPError currently claims only to be raised ""In the rare event of an invalid HTTP response"".  
You could also consider implementing some of my ideas into my raise_for_status so that it can allow other status codes, too. 
",Matt3o12,Lukasa
2152,2014-07-27 16:52:19,"@Lukasa Because it is very convenient, can save lots of code (let's just say a project uses request. This project uses about 100 times a request. So, it _could_ save 200 lines of code, and prevent typos because I would have to write the exception 100 times (in case I don't use a method for raising an exception), and it unifies exceptions/errors (another API may use requests and raise a custom error in case of an invalid status code. I like it better if it throws the same exception as requests does). 

@sigmavirus24  Sorry, I didn't know this method exists. You should mention that  an HTTPError can be raised by raise_for_status in Error and Exceptions section in the docs. An HTTPError currently claims only to be raised ""In the rare event of an invalid HTTP response"".  
You could also consider implementing some of my ideas into my raise_for_status so that it can allow other status codes, too. 
",Matt3o12,sigmavirus24
2148,2014-07-24 21:31:13,"@sigmavirus24 @alex Fixed. Not sure what I was thinking, there.
",romanlevin,sigmavirus24
2145,2014-07-22 17:10:57,"@sigmavirus24 I've updated the wording - comments?
",TkTech,sigmavirus24
2145,2014-07-22 20:20:36,"@TkTech I completely agree! A pull requests that added that would be accepted :)
",kennethreitz,TkTech
2143,2014-09-12 15:58:06,"@Lukasa yeah. I'm pretty sure we can't really do anything about this sadly. Closing until that changes.
",sigmavirus24,Lukasa
2136,2014-07-16 12:14:49,"That's embarrassing. Thanks @dpursehouse !
",sigmavirus24,dpursehouse
2134,2014-07-15 17:13:06,"Thanks @nonZero !
",sigmavirus24,nonZero
2133,2014-07-15 17:12:54,"Thanks @esparta !
",sigmavirus24,esparta
2129,2014-07-15 02:42:42,"@sigmavirus24 fixed, thanks.
",jamielennox,sigmavirus24
2129,2014-07-22 20:23:04,"There are some nice changes in here. Looking forward to hearing what @sigmavirus24 and @Lukasa have to say :)
",kennethreitz,sigmavirus24
2129,2014-07-23 00:39:10,"I agree with @Lukasa and want to stress that it's a very subtle way in which it affects the API. Most people who use `Session#send` are not expecting environment based data to affect their request. If they are using prepared requests to avoid this altogether, we're making this basically impossible for them now. We might need to wait for Requests 3.0 to merge this.
",sigmavirus24,Lukasa
2129,2014-07-23 04:54:46,"@Lukasa Yea, i understand that. I was thinking by doing it this way we didn't change the existing behaviour for anyone and people who wanted to use the environment variables could opt-in. I don't know what the timeframe is but that's preferable to me than waiting for 3.0. 

Do you want me to revert back to the older commit? (wish github had a better way of managing this). 
",jamielennox,Lukasa
2129,2014-08-12 19:06:55,"@kennethreitz it seems @jamielennox has abandoned this. I'll pull their commit(s) into a branch of my own and update it with our decision tonight.
",sigmavirus24,kennethreitz
2129,2014-08-12 21:26:59,"Sorry everyone, i had let this slip off my list.

This is at least an example of extracting that information to a function, there is a question there as to whether the trust_env check should go inside the apply_environ() function or not, i've gone back and forth. 

I'm not sure if this is what you were looking for - it doesn't feel as 'clean' as the last patches for some reason. 

@sigmavirus24 don't be worried about stepping me through the review process, i'd prefer to get this feature in than worry about who contributed it. If you have something in mind that satisfies the case then merge it and close this out. 
",jamielennox,sigmavirus24
2129,2014-08-22 13:05:32,"@sigmavirus24 want to own this?
",kennethreitz,sigmavirus24
2128,2014-07-14 14:39:17,"Thanks @tshepang !
",sigmavirus24,tshepang
2124,2014-09-04 12:08:24,"That's a risk. If we take the policy that our new dependence on certifi means we no longer need to update the build-in bundle we've exposed our users to risk.

I have no particular objection, so I'll let @kennethreitz make the call: should the hard dependency on certifi be removed?
",Lukasa,kennethreitz
2118,2014-07-02 16:39:48,"> Further, we're ignoring whether or not @kennethreitz would even want to add this keyword argument.

By **no means** did I intend to ignore @kennethreitz’s wishes! Alas. Opening this issue was, I had thought, precisely the means by which Mr. Reitz’s wishes could become known. Would there have been a more appropriate forum for asking my question?

> In this I agree with you.

Thank you, @Lukasa! Sorry if I worded the issue awkwardly and it required multiple read-throughs.

>  I think requests should continue its proud tradition of solving the 80% use-case … I'd never accept adding a `ciphers` kwarg to the main API … This would allow us to write something like the `SSLAdapter`

So the keyword arguments to `get()` and the other functions are not “kitchen sink” collections of everything that _could_ be specified, but a smaller collection of settings, and users are intended to step back and create adapters for more difficult cases? Then you are correct that what I probably need is an adapter that accepts an `SSLContext` for building connections!

The `urllib3` library accepts an `ssl_version` keyword? That, I fear, is a first step towards insanity, and a course which can be stopped by turning to `SSLContext`. Because the next logical step after `ssl_version` (which is really setting what OpenSSL calls the “protocol” version, from what I can see?) is adding a `ciphers` keyword, and then a `dh_params` keyword, and then `ecdh_curve`, and so forth.

In fact, what I really probably want is an adapter that does not even know that `SSLContext` exists, but simply accepts that I have gotten ready an object with a `wrap_socket()` method that it can use when it is ready to negotiate an encrypted connection. That way, as long as an SSL library that I want to use in the future also offers a `wrap_socket()` method (think of PyOpenSSL, or that new `cryptography` project), then it would automatically work if dropped in to the transport object.
",brandon-rhodes,Lukasa
2118,2014-07-02 16:49:28,"> Would there have been a more appropriate forum for asking my question?

No. I just like to couch my positive replies in ""But Kenneth might not want this, so don't get your hopes up"". It might take him a while to get around to reading or replying though.

> So the keyword arguments to get() and the other functions are not “kitchen sink” collections of everything that could be specified, but a smaller collection of settings, and users are intended to step back and create adapters for more difficult cases?

That is correct. There are currently a few good examples of this (like the SSLAdapter @Lukasa linked above).

> Then you are correct that what I probably need is an adapter that accepts an SSLContext for building connections!

Good news! @Lukasa has a blog post about building adapters for requests and we can both help you with building one of these. Further, I'd be interested in it if only to add it to the [`toolbelt`](https://github.com/sigmavirus24/requests-toolbelt) if only to help it be discovered more easily.

> In fact, what I really probably want is an adapter that does not even know that SSLContext exists, but simply accepts that I have gotten ready an object with a wrap_socket() method that it can use when it is ready to negotiate an encrypted connection.

You _could_ do this, but then you would probably doing a lot of `urllib3`'s work since it doesn't (if I remember correctly) provide a way for you to pass in a socket to use (and I don't think it should start accepting sockets to use either).

It might be more helpful to acquaint yourself further with the adapters that currently exist in requests, to understand how they work. They're not terribly difficult to understand either.
",sigmavirus24,Lukasa
2118,2014-07-20 11:44:19,"@EnTeQuAk I continue to be in favour of my option 2. =)
",Lukasa,EnTeQuAk
2118,2014-10-16 20:44:40,"I second @alex's suggestion. Constructing it once and not having to use those 4 lines each time I need a slight variation is much nicer. Immutability will also be pleasant so the options cannot be changed under anyone's feet.
",sigmavirus24,alex
2118,2014-10-16 21:25:20,"@shazow it's lived almost entirely here so let's just finish it here?
",sigmavirus24,shazow
2118,2015-12-03 21:47:14,"@Lukasa , is the plan for `requests` to handle both Python `ssl`'s [`SSLContext`](https://docs.python.org/3/library/ssl.html#ssl.SSLContext) and PyOpenSSL's [`Context`](http://pyopenssl.sourceforge.net/pyOpenSSL.html/openssl-context.html) objects equivalently?

Despite some similarities, these classes don't have compatible APIs and aren't related.
",aadamowski,Lukasa
2118,2017-01-13 15:56:19,@Lukasa Are there docs on doing so? I'm not finding anything.,dsully,Lukasa
2118,2017-01-13 16:03:23,"@Lukasa Thanks, I'll start playing around with that. FWIW, the reason I need this is adding our internal CA to the trust store in addition to locking down the cipher suite.",dsully,Lukasa
2117,2014-07-03 14:17:16,"@homm What is your proposal for requests? The best option I've seen so far is to send both `filename` and `filename*` in that order. If that happens, what should we put in `filename`?
",Lukasa,homm
2117,2014-07-03 14:19:44,"@homm you're inherently wrong. Rack _claims_ to support RFC 2231, this is clearly a failure to properly do so given how clear 2231 (and 5987) are in their grammar. You claim earlier versions of requests do not cause the problem, the solution to your problem then is to clearly use old versions since you seem to think broken behaviour is correct.
",sigmavirus24,homm
2117,2014-07-03 14:21:31,"@sigmavirus24 I am open to sending both `filename` and `filename*`, this should be supported, but I don't know how we'd populate `filename`.
",Lukasa,sigmavirus24
2117,2014-07-03 14:24:40,"@Lukasa
1) whatever is enough for file recognition for not compliant server and does not brake others (because ""specification suggests that a parameter using the extended syntax takes precedence"").
2) it can be value without any non-ascii chars and ""unknown"" as fallback.
3) should be a way to send encoded `filename` if I exactly know how server handles encoded values.
",homm,Lukasa
2117,2014-07-03 14:26:38,"@sigmavirus24 I can't use old requests version because I need SNI on python 2 :(
",homm,sigmavirus24
2117,2014-07-03 14:28:05,"@Lukasa ostensibly through the toolbelt since that's the only place where users have complete control over the fields but even so, this would probably need to be worked on in urllib3 because I'm fairly certain it doesn't currently provide a way to do this.

Further RFC 2231 is the authority on this since 5987 (as I've re-read it) is for HTTP Headers (not MIME/multipart headers) and 2231 does not allow for multiple parameters (on quick glance over it)

---

@homm 

>  2) it can be value without any non-ascii chars and ""unknown"" as fallback.

This is not an accurate representation in the slightest. At best it will introduce utter confusion.

>  3) should be a way to send encoded filename if I exactly know how server handles encoded values.

You can never know exactly how any server handles these values unless you've written it from scratch yourself.
",sigmavirus24,homm
2117,2014-07-03 14:28:05,"@Lukasa ostensibly through the toolbelt since that's the only place where users have complete control over the fields but even so, this would probably need to be worked on in urllib3 because I'm fairly certain it doesn't currently provide a way to do this.

Further RFC 2231 is the authority on this since 5987 (as I've re-read it) is for HTTP Headers (not MIME/multipart headers) and 2231 does not allow for multiple parameters (on quick glance over it)

---

@homm 

>  2) it can be value without any non-ascii chars and ""unknown"" as fallback.

This is not an accurate representation in the slightest. At best it will introduce utter confusion.

>  3) should be a way to send encoded filename if I exactly know how server handles encoded values.

You can never know exactly how any server handles these values unless you've written it from scratch yourself.
",sigmavirus24,Lukasa
2117,2014-07-03 14:33:11,"Let's be clear on our thinking here @homm. What we need is something that satisfies criteria 1) and 2) in an unambiguous way while providing a pleasant API. I contend that no such solution exists, because criteria 1) and 2) are ambiguous and resolving that ambiguity dirties up the API.

Requests does not need to be able to generate every valid HTTP request under the sun, only a subset of them. We've decided how we're approaching this, and we're backed up by the specs. We've also provided you with a workaround. That workaround reveals a bug in urllib3 that we're going to pursue fixing.

Unless you can design a pleasant, non-ambiguous API, we're not going to be too worried about this.
",Lukasa,homm
2115,2014-07-03 12:19:40,"@kennethreitz Disagree: verify may be set to a string (path to the cert bundle) and we shouldn't overwrite it.
",Lukasa,kennethreitz
2113,2014-07-01 18:58:05,"@sigmavirus24 Both without unicode headers against HTTPS and with unicode headers against HTTP.  In my particular case the data I am sending is always ASCII, it just happens to be a unicode object because i got it from a different library that always returns unicode objects.
",joeshaw,sigmavirus24
2113,2014-07-01 19:03:30,"@joeshaw the fix is to call `string.encode` on each string that library gives you. You also seem to know the encoding (something requests should never guess at), so you can get the string from that library and then simply call `s.encode('utf-8')` on it.

As @Lukasa the only totally safe header encoding is ASCII. The next safest would be Latin-1 (ISO-8859-1). Other than that, there's not much we can do for you.
",sigmavirus24,Lukasa
2111,2014-06-26 18:41:01,"@sigmavirus24 I just took you up on your suggestion http://www.saulshanabrook.com/requests-timeout-and-odd-traceback/
",saulshanabrook,sigmavirus24
2108,2014-06-23 09:05:41,"@Lukasa: anyway it's urllib3 related issue
",vitek,Lukasa
2106,2014-06-23 12:33:33,"@xiongxoy can you apply the commit in the PR that @Lukasa referenced (#1860) and see if that fixes the test for you?
",sigmavirus24,Lukasa
2105,2014-08-28 17:11:00,"@sigmavirus24 you're too abrasive :)
",kennethreitz,sigmavirus24
2104,2014-06-21 05:50:07,"I'm generally in agreement with @sigmavirus24. More generally, if you want redirects on (or off), you should explicitly turn them on (or off). That's the only way to write code that is never going to change under your feet.
",Lukasa,sigmavirus24
2095,2014-06-12 13:14:46,"@sigmavirus24 @dstufft Both good points that I hadn't considered. I'm moving back down to -0, waiting to hear from some of the other big guns. =)
",Lukasa,sigmavirus24
2095,2014-06-12 13:14:46,"@sigmavirus24 @dstufft Both good points that I hadn't considered. I'm moving back down to -0, waiting to hear from some of the other big guns. =)
",Lukasa,dstufft
2095,2014-06-12 18:09:16,"Interesting @dolph .
@Lukasa , I caught the ""SHOULD"" as well, but @dolph  did state it is only recommendation.

It seems this comes down to whether requests is just a http library or a http client.
Perhaps a http library shouldn't do caching but a http client should?
So, the question of the day: is a requests.Session object a client?... or is it just some percentage of a client?
",ericfrederich,Lukasa
2095,2014-06-12 18:11:39,"@Lukasa My apologies for not being aware of prior discussions. Is there a documented guideline on where requests draws the line between the actions delegated to users of the library and requests' own responsibilities?
",dolph,Lukasa
2094,2014-06-12 13:09:49,"Thanks for all the effort you put in @moliware ! Even though we didn't accept the work, we appreciate your effort to make requests better :)
",sigmavirus24,moliware
2092,2014-06-11 13:12:35,"I second @Lukasa's far more explicit check for `None`.
",sigmavirus24,Lukasa
2086,2015-11-24 01:13:02,"@Lukasa  @kennethreitz 
Hey guys, for the time being, I don't think we have a obvious solution yet, but can we at least make this `ISO-8859-1` optional? 



This looks way too brutal. Some parameters like `Session(auto_encoding=False)` would be nice. What do you guys think?
",est,kennethreitz
2086,2015-11-24 01:13:02,"@Lukasa  @kennethreitz 
Hey guys, for the time being, I don't think we have a obvious solution yet, but can we at least make this `ISO-8859-1` optional? 



This looks way too brutal. Some parameters like `Session(auto_encoding=False)` would be nice. What do you guys think?
",est,Lukasa
2086,2015-12-15 17:07:37,"@Lukasa But you can't determine whether the initial `response.encoding` came from the `Content-Type` header or whether it's the `ISO-8859-1` fallback, which means if you want to avoid the fallback you have to start parsing the `Content-Type` header yourself, and that's quite a lot of extra complexity all of a sudden.
",gsnedders,Lukasa
2086,2015-12-15 17:32:28,"@Lukasa uh, I thought there was whitespace (or rather what 2616 had as implicit *LWS) allowed around the equals, apparently not.

The grammar appears to be:



So I guess the only issue here is something like `text/html; foo=""charset=bar""`.

FWIW, html5lib's API changes are going to make the correct behaviour with requests require something like:


",gsnedders,Lukasa
2086,2016-03-29 09:12:23,"@Lukasa My point is that you're not necessarily just looking for a `<meta>` tag, you could also be wanting to see if the opening bytes are 00 3C 00 3F (which obviously you can do as they'll just be U+0000 U+003C U+0000 U+003F), and then start decoding as UTF-16 until you find the inline encoding declaration. There's a lot more complexity (esp. in the XML case) than just looking for one ASCII string. ISO-8859-1 isn't that bad _insofar_ as it causes no data loss, but it doesn't really make searching any easier.
",gsnedders,Lukasa
2083,2014-06-03 14:25:33,"@Lukasa Makes sense, thankyou.
",JakeAustwick,Lukasa
2072,2014-05-28 13:17:35,"@Lukasa fixed in 5ab79e2
",sigmavirus24,Lukasa
2068,2014-05-26 13:45:00,"I agree entirely with @Lukasa. You may want to investigate how (if at all) other libraries do this @taku-pl like [httplib2](https://github.com/jcgregorio/httplib2) or [httpplus](https://code.google.com/p/httpplus/) and use them instead if possible.
",sigmavirus24,Lukasa
2066,2014-05-26 15:27:08,"Thanks. @Lukasa, can `response.request` be added to the [Response documentation](http://docs.python-requests.org/en/latest/api/#requests.Response)?
",migurski,Lukasa
2065,2014-05-27 15:24:40,"@sigmavirus24 noted.
",kennethreitz,sigmavirus24
2064,2014-05-26 13:37:14,"I could be wrong, but I think I remember @dstufft saying that pip support for 3.2 wasn't a high priority. That aside, I'm not quite sure why we support using simplejson. If @mgeisler hadn't linked to #710 I would have guessed it was to support Python 2.5 (which we no longer do).
",sigmavirus24,mgeisler
2063,2014-05-28 01:36:56,"> Oh d'oh. That would be me not copying the url correctly. Try the updated comment.

No worries. I wasn't sure if I was missing something =P

> .netrc seems potentially important

It can be. That's why I mentioned it as a downside :)

>  I just opened this issue because it was confusing and I don't think I should have to do it. 

I agree. I don't think you should have to either. It seems, however, that this is a duplicate of #2018 after all. @Lukasa should we close this in deference to #2018?
",sigmavirus24,Lukasa
2062,2014-05-24 21:10:04,"@sigmavirus24 Sorry, I should have mentioned that I already circumvented the problem by writing an authentication handler that does not modify the request. My post was meant as a suggestion to improve requests, as I suppose other users might run into the same problem.

> Hey @joe42 There are several headers we don't expect users to set on their own because they really shouldn't be.

I see. I do set several headers directly. Can you point me to a resource explaining which headers should not be set and why? Setting the headers directly seemed to be the way to go after reading the documentation's ""Custom Headers"" section.

Thx,
Joe
",joe42,sigmavirus24
2062,2014-05-25 10:21:57,"@Lukasa : I can understand why you choose to design it this way. But I think that the result of the following request with default credentials in .netrc  is not obvious at all. The point I do still not get is that I cannot imagine a case where a user would actually set a specific header (like in your example) and not expect it to be used by requests. Why should he bother doing this? So it might be good to either make it obvious (by documentation/warning/error) or to respect the users explicit settings.


",joe42,Lukasa
2062,2014-05-26 15:44:26,"I had a related problem in #2066.

My issue arose using Heroku’s API, which has two confusing factors relevant to this issue. The toolbelt [quietly creates a .netrc file](https://devcenter.heroku.com/articles/heroku-command#logging-in) (mine was a year old; I didn't know it existed) and Heroku asks users to [create a custom Authorization header](https://devcenter.heroku.com/articles/oauth#web-application-authorization):

> For example, given the access token _01234567-89ab-cdef-0123-456789abcdef_, request headers should be set to _Authorization: Bearer 01234567-89ab-cdef-0123-456789abcdef_.

I am now using `Session.trust_env` to prevent this issue thanks to @Lukasa, but I found the silent overwrite of the `Authorization` header to be a nasty surprise. I’m in agreement with @joe42 on this: specifically providing a header ought to overrule environmental factors. Alternatively, could the `.netrc` behavior be documented [under API](http://docs.python-requests.org/en/latest/api/#requests.request), [custom authentication](http://docs.python-requests.org/en/latest/user/advanced/#custom-authentication), and [quickstart](http://docs.python-requests.org/en/latest/user/quickstart/#custom-headers)? My searches for “auth” in the requests documentation should have mentioned this.
",migurski,Lukasa
2062,2014-05-26 16:20:04,"@Lukasa: Thanks for your detailed answer. 

> headers are the lowest source of truth, and we feel quite happy to replace many kinds of headers that the user sets. Some examples:
> 
>    We will replace Authorization headers when an alternative auth source is found.
>     We will remove Authorization headers when you get redirected off-host.
>     We will replace Proxy-Authorization headers when you provide proxy credentials in the URL.
>     We will replace Content-Length headers when we can determine the length of the content.
> 
> We do this for very good reason: a headers-based API is utterly terrible. Affecting the library behaviour based on arbitrary key-value pairs set in the headers dictionary is asking for all kinds of terrible bugs

It would be great if you could add this sentence to the ""Custom Headers"" section, after it states:

> If you’d like to add HTTP headers to a request, simply pass in a dict to the headers parameter.
",joe42,Lukasa
2062,2014-05-26 17:08:22,"@sigmavirus24 Do you feel like this would be a good addition to the docs?
",Lukasa,sigmavirus24
2062,2015-04-03 13:54:01,"@benjaminran Agreed, that looks like extremely useful documentation to me. If you open a pull request that adds it, I'll happily merge it. =)
",Lukasa,benjaminran
2062,2015-04-03 14:45:49,"@benjaminran one thing: documentation shouldn't be overly verbose but they also shouldn't be too concise. The difficulty in writing documentation is finding the right balance such that everyone can understand what's going on without reading a novel. ;)
",sigmavirus24,benjaminran
2062,2015-12-21 02:45:22,"I was about to suggest that we close this issue, since the documentation given by @benjaminran is now merged into the master docs (in Quickstart->Custom Headers). However I have one more question: what happens when the `auth=` parameter is used, AND the `.netrc` environment config is set? It is not clear from the docs which one takes precedence. And as @migurski pointed out, we have a definite case where a hosting provider (Heroku) quietly created a `.netrc` file, so it would be important to know this. 

I suspect that `auth=` should override `.netrc`, but I wanted to confirm before creating a PR.
",ibnIrshad,benjaminran
2062,2015-12-21 14:44:49,"@ibnIrshad please do not create a PR. That would be a breaking change that we will not accept until 3.0.0
",sigmavirus24,ibnIrshad
2061,2014-05-25 18:34:08,"So I want to be sure I'm understanding everyone's concerns here properly:

@asmeurer when you say you'd like requests to prompt for proxy auth, do you mean you'd like us to literally use `raw_input` (or `input` in the case of Python 3)? I'm pretty sure that's not what you want and that's not something we'll ever support. Further, I'm not quite certain how we would properly implement a callback system for this particular case since the only other system like it in requests relies on having a response which we don't have in this case.

That said, we've had a troubled history (which @Lukasa knows much better than I) dealing with HTTPS Proxy Authentication. If there were a better way of handling them, we would have already implemented it (I'd like to think).

This discussion should remain in this issue. Your other problem @asmeurer (in which mutating a Session's proxies does not affect subsequent requests should be a separate issue). I'm trying to think of a good way to handle that case since I think I've located the problem above.
",sigmavirus24,Lukasa
2061,2014-05-27 17:26:05,"Not sure if this discussion is resolved or not, but please ping me again if I still have an action item/question. I will be home tomorrow for more in-depth reading. :)

@Lukasa If this is still an open question, handling special-handling 407 in urllib3 sounds sensible if we can do it in a low-impact way.
",shazow,Lukasa
2057,2014-05-22 18:28:11,"The purpose of `dict_class` is stated in the docstring:

> If a setting is a dictionary, they will be merged together using `dict_class`.

This means that `dict_class` is intended to control the merging logic. An example here is that of `MultiDict`, which will treat merging settings fundamentally very differently to the way a `dict` or `OrderedDict` would.

_With that said_, I have no particular objection to having `merge_setting` return the `dict_class`. Clearly @sigmavirus24 doesn't agree, but I'll try to have a chat with him and see why we're on different pages. I suspect he's seeing something I'm not. =)
",Lukasa,sigmavirus24
2053,2014-05-20 16:56:58,"@Lukasa as you might see in its source code, it applies gevent.monkeys.patch_all, which is known to break various things and is a generally dangerous thing to do.  (worse, it does that on import-)
",HoverHell,Lukasa
2053,2014-05-20 17:36:04,"@sigmavirus24 I think that's mostly out of the issue. Regardless, it threw an “this call would block forever” exception (on a code which otherwise works normally) which is enough to be suspicious; additionally (but not critically), it is known to have some problems with celery, raven, possibly uwsgi, and various other things. Not quite something I would want to deal with in an actively used django application.
",HoverHell,sigmavirus24
2053,2014-05-25 06:33:54,"@Anorov 

> Your geventhttpclient adapter would likely be the overall safest bet in that regard
> Certainly; my question is: can it be done in a less hack-ish / more future-proof way, perhaps by integrating it more with the requests library e.g. making few minor changes in the methods arhitecture of the requests connection / connectionpool modules?

@sigmavirus24 

> I agree. I don't think I've ever seen that problem before when using gevent correctly.

I wasn't even using gevent at that point – I simply installed grequests, and another imported library (which wasn't being used either) imported grequests; in all the other regards it is simply a django application.

(though, thinking of it, I could guess that it is the late importing that made it problematic; still, I expect other problems regardless)
",HoverHell,sigmavirus24
2053,2014-05-25 18:40:53,"@sigmavirus24 I'm intent on using it (or an alternative, really) without _potentially_ problematic monkeypatching (sometimes even small chances of breaking something are too much); that's why I need more support from the requests.
",HoverHell,sigmavirus24
2053,2014-05-26 05:19:41,"@sigmavirus24, @Anorov  as you can see by the link in the first message, I've already made a no-monkeypatching adapter through subclassing. However, it is variably hack-ish and incomplete and unreliable.

My question, thus, is: what changes can be made to requests that would make it better? The very least would be making `pool_classes_by_scheme` an attribute of the PoolManager; and, likely, adding more attributes like ConnectionCls.

Basically, my problem is that the whole default _HTTPAdapter_ of the requests is nearly a monolithic untweakable thing if you don't count in monkeypatching.
",HoverHell,sigmavirus24
2050,2014-05-18 01:42:48,"This is great work @Hasimir! I really appreciate your effort in putting this together.

Personally, I think this is better suited to a blog post and not the documentation. I'll leave this open, though, until @Lukasa wakes up. :)
",sigmavirus24,Hasimir
2050,2014-05-18 08:03:15,"This is brilliant, @Hasimir! Unfortunately, I don't think it belongs in the documentation. =)

Requests documentation doesn't really contain 'recipes', or pre-canned instructions for how to do specific things. This is for two reasons: firstly, they add greatly to the (already not inconsiderable) size of the docs; and secondly, they then become something that needs to be actively maintained. This is harder for recipes like this one because they depend on external projects, meaning that if those projects change our documentation becomes out of date with no real way for us to spot it.

I agree that a blog post is a good idea. You'll also find that a good blog post providing a tutorial on using Requests in a certain way has a tendency to move up quite high in the Google listings. As two examples from my own blog, the specific search [""force ssl version python requests""](https://www.google.co.uk/search#q=force+ssl+version+python+requests) has my blog post as the top entry, and even the fairly general [""proxies python requests""](https://www.google.co.uk/search#q=proxies+python+requests) has my blog post on the front page. This is really where the information you've provided should belong. =)
",Lukasa,Hasimir
2050,2014-05-18 11:48:12,"@Hasimir Do send me and @sigmavirus24 tweets when you write it and we'll make sure we RT and talk about your blog post lots. =D
",Lukasa,Hasimir
2050,2014-05-18 11:48:12,"@Hasimir Do send me and @sigmavirus24 tweets when you write it and we'll make sure we RT and talk about your blog post lots. =D
",Lukasa,sigmavirus24
2049,2014-05-17 22:44:58,"LGTM. @codedstructure you should checkout [the toolbelt](https://gitlab.com/sigmavirus24/toolbelt). We just moved the `SourceAddressAdapter` there so people can use it without having to maintain their own version.
",sigmavirus24,codedstructure
2049,2014-05-18 08:07:02,"This looks great @codedstructure, but I want to bikeshed here for a moment. I'm wondering whether the right API is actually for `proxy_manager_for` to take an arbitrary keyword argument dict that it passes to `proxy_from_url`. This saves several lines from the `SourceAddressAdapter` because it can now do:



Thoughts?
",Lukasa,codedstructure
2049,2014-05-18 14:30:35,"@Lukasa that sounds like a good idea. The revised PR would look like this then?


",sigmavirus24,Lukasa
2049,2014-05-18 19:18:10,"Thanks both, I'm 100% in favour of anything which makes adapters easier to write. The PR code now looks like @sigmavirus24' code above.

Although while we're at it / (& for consistency), should `init_poolmanager` also have a similar kwargs parameter `pool_kwargs`?
",codedstructure,sigmavirus24
2049,2014-05-18 19:30:07,"Consistency is the hobgoblin of the foolish mind. That aside, I'm in favor of consistency. I just like keeping PRs narrowly focused to one topic. If you're going to make that change, _I_ would prefer it in a separate PR, but @Lukasa should feel free to disagree. 
",sigmavirus24,Lukasa
2049,2014-06-09 14:59:09,"@kennethreitz I think we're good to go at this stage. =)
",Lukasa,kennethreitz
2048,2014-05-17 21:17:37,"And yes I'm :+1:. Go for it @codedstructure 
",sigmavirus24,codedstructure
2046,2014-05-16 21:05:47,"Thanks @alex 
",sigmavirus24,alex
2044,2014-05-19 07:31:43,"@sigmavirus24  You forgot to add the authentification. Without it always worked for me but with authentification it didnt.

Anyway I tried to test it out on an other environment and it worked without problems, even on production! So this bug only appears on my work PC which is frustrating but oh well...

Thank you for your help,
",p-clem,sigmavirus24
2042,2014-06-12 12:48:01,"@untitaker Way ahead of you, see #2086. =)
",Lukasa,untitaker
2038,2014-05-12 18:59:41,"@Lukasa @sigmavirus24 alright, let's get a release ready. :)
",kennethreitz,Lukasa
2036,2014-05-08 09:19:54,"@Lukasa Yes, I want requests not to explicitly consult a proxy. I use ISA Client for proxy-transparency.

I do not set the environment variables `HTTP_PROXY` or `HTTPS_PROXY`. Other programs work transparently with ISA Server proxy via ISA Client.

Thank you for info about `NO_PROXY=1`. It works! My troubles are ended for ""python networking"". :)
",espdev,Lukasa
2028,2014-04-30 14:40:54,"@sigmavirus24 @Lukasa I'm using 2.2.1 from pypi...

I saw in #2026 that it was ""fixed"" by installing 2.2.1 from pypi, but mine IS a normal, vanilla version...
",poolski,Lukasa
2028,2014-04-30 14:40:54,"@sigmavirus24 @Lukasa I'm using 2.2.1 from pypi...

I saw in #2026 that it was ""fixed"" by installing 2.2.1 from pypi, but mine IS a normal, vanilla version...
",poolski,sigmavirus24
2025,2014-04-29 09:05:37,"Tagging this as contributor friendly because it's perfect for @sigmavirus24 to pair with a new developer on.
",Lukasa,sigmavirus24
2023,2014-04-29 00:22:43,"@kennethreitz this will help a great deal when debugging issues where people are running into errors with HTTPS connections.
",sigmavirus24,kennethreitz
2023,2014-05-02 19:20:16,"@kennethreitz I don't think we can do that. We want to know if the user installed the dependencies to make the top level import/execution work.
",sigmavirus24,kennethreitz
2023,2014-05-13 01:55:11,"I agree with @t-8ch that this belongs here, even if it's hidden somewhere
",sigmavirus24,t-8ch
2022,2014-04-26 20:55:00,"@t-8ch Thanks for taking a look at this, I'm a bit confused. OpenSSL makes my life really hard =(
",Lukasa,t-8ch
2022,2014-04-26 20:57:44,"@t-8ch I haven't installed PyOpenSSL if that's what you're asking?

I would have assumed (perhaps incorrectly) that `pip install requests` should give me everything I need to successfully call `requests.get('...')` on an HTTPS page.  Which, of course, it works for the most part, just not for this site for some reason. 
",jaddison,t-8ch
2022,2014-04-26 20:59:25,"@jaddison There are two different codepaths behind the scenes. You shouldn't have to care about those, but it helps to know when debugging.

However I can now reproduce this on ubuntu. But only o Py2. On Py3 everything is fine.
I suspect @Lukasa is right and the server fails when the client is not using SNI.
",t-8ch,Lukasa
2022,2014-04-26 21:02:05,"@Lukasa was right. Compare:


",t-8ch,Lukasa
2022,2016-05-31 23:24:30,"@jvanasco what are you using to install those packages? I assume pip. Why are you installing urllib3 and requests separately?
",sigmavirus24,jvanasco
2022,2016-06-01 01:27:10,"Well I needed urllib3 in the virtualenv... but I installed it to try and get the requirements installed by pip and easy_install.  (I used both)

I have a web indexer and a few urls broke.  I wrote a quick script to try the broken ones, and kept reinstalling/delete+installing the packages in the urllib3 instructions on ssl issues until they worked.

On May 31, 2016, at 7:25 PM, Ian Cordasco notifications@github.com wrote:

@jvanasco what are you using to install those packages? I assume pip. Why are you installing urllib3 and requests separately?

—
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub, or mute the thread.￼
",jvanasco,jvanasco
2022,2016-06-15 16:43:06,"@sigmavirus24 Apologies. I meant to provide more information and then got side tracked (since I had no time for this). I'm using Ubuntu 14.04, python 2.7.6 and the latest requests version on pip. This happens when I try to access as API Gateway endpoint (they might be quite restrictive).

I tried removing the virtualenv and regenerating it but unfortunately that didn't solve it.

Let me know what else you need. I switched to nodejs for the time but would be happy to help with a resolution.
",lukas-gitl,sigmavirus24
2022,2016-06-25 11:49:21,"@Lukasa Thanks for your reply. I reconfirmed that I did install them:

`$ pip install pyopenssl ndg-httpsclient pyasn1
Requirement already satisfied (use --upgrade to upgrade): pyopenssl in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python
Requirement already satisfied (use --upgrade to upgrade): ndg-httpsclient in /Library/Python/2.7/site-packages
Requirement already satisfied (use --upgrade to upgrade): pyasn1 in /Library/Python/2.7/site-packages`

but code still down.

Anyway, I figured out that everything goes well in Python3, and I am glad to be able to code in python3.
Thank you very much.
",jschwinger23,Lukasa
2020,2017-02-12 18:49:40,"@Lukasa  http/2 may not call it chunked transfers, but it does have DATA frames as mentioned here: [rfc7540](http://httpwg.org/specs/rfc7540.html#HttpSequence). The 'pad length' concept is analogous to chunk size.",electronicsguy,Lukasa
2020,2017-02-13 06:17:37,"@Lukasa   oh I see. Thanks for clearing that up about lad length. But I don't understand why DATA frames are not analogous to chunking. It is in-fact mentioned in this mailing list that it is the case, at least logically: [http/2-chunking](https://lists.w3.org/Archives/Public/ietf-http-wg/2014JulSep/1676.html). Even in the case of http/1.1, chunked transfer encoding is not visible to the users since it is at the transport layer. So when you said above that chunked encoding is an artefact of http/1.1 and not present in http/2, I'm not able to understand how that is.",electronicsguy,Lukasa
2018,2014-05-04 17:28:09,"It's been noted that this issue is poorly named. I'm not sure what to rename it or even if it should be now that it has been created. @Lukasa, feel free to rename it if and as you see fit.
",ouroborus,Lukasa
2018,2014-08-13 03:10:02,"@Lukasa wasn't this already fixed?
",sigmavirus24,Lukasa
2018,2014-08-13 06:28:04,"@sigmavirus24 Not that I can see. =)
",Lukasa,sigmavirus24
2013,2014-04-23 06:24:30,"Yeah, I'm with @sigmavirus24 I'm afraid. This is really nice work, but it's just not the way the requests project tends to work. Sorry! :cake:
",Lukasa,sigmavirus24
2010,2014-04-18 19:20:57,"@kennethreitz Yeah, I'm more interested in the description of responsibilities. =)
",Lukasa,kennethreitz
2008,2014-04-18 07:34:20,"@Lukasa Thanks for your reply. But, what I want is to send http requests with specifying source address. I've no idea how to use the  Transport Adapter abstraction to finish this. Maybe you can give an example how it does. 
",bofortitude,Lukasa
2008,2014-04-18 08:08:57,"@Lukasa Great! That's what I want. After replacing with the new version urllib3, I did it! I hope there will be new version requests which embedded with new urllib3 as soon as possible. 
It's so kind of you to show me this! Thanks you again. 
",bofortitude,Lukasa
2008,2015-09-14 11:25:16,"@Lukasa 

thanks for the above dode
your code above have a small issue.



the source_address should be a pair (host, port).

You pass a string and this will cause an exception when the sock.bind method is called.

The correct way is to pass a tuple like:



cheers
",gosom,Lukasa
2008,2015-09-14 15:57:42,"@Lukasa 
I think the version of the code should not work with the current requrests version.

check here:https://github.com/kennethreitz/requests/blob/master/requests/packages/urllib3/util/connection.py#L77

The socket.bind method it is called and you get an exception if you pass a string.

Ok, this is not a bug but would be nice to be documented that source_address 
should be a (host, post) tuple.
cheers
",gosom,Lukasa
2008,2015-09-14 19:18:46,"@Lukasa 
yes you are right. I was not even aware of the request-toolbelt :+1:  

Thanks for the reply
",gosom,Lukasa
2006,2014-04-16 20:03:11,"This is an interesting idea @untitaker! It does feel like it'd be useful.

I wonder if we should trial it in [the toolbelt](https://github.com/sigmavirus24/requests-toolbelt) first, see if it's useful. @sigmavirus24?
",Lukasa,untitaker
2006,2014-04-16 20:12:30,"@untitaker That's not entirely true. It's what we do for Basic auth, but it's not what we do for Digest. And the fact that we do it pre-emptively for Basic is very deliberate: GitHub's API requires it. =)
",Lukasa,untitaker
2006,2014-04-28 19:55:10,"@sigmavirus24
I had a similar usage to `AuthBase` in mind:



Where GuessAuth is something like this:



I don't think it would be nice to blend into `AuthHandler`, as the auth type
to use can vary within a domain.
",untitaker,sigmavirus24
2003,2016-12-16 18:50:48,"@nateprewitt I am in agreement, but I would be unsurprised if Cory changed his mind. Since he's on holiday, why don't you hold off on working on this so you don't waste your time.",sigmavirus24,nateprewitt
2003,2016-12-16 19:01:45,"Yep, will do. Just floating it out there for when you've both got a free moment. Thanks @sigmavirus24 :)",nateprewitt,sigmavirus24
2002,2014-04-14 21:23:05,"@sigmavirus24 heh, appears this issue has just bitten you too. 

> You did get something if resp is not None.

isn't true given the current implementation of `__nonzero__` (which kicks to `ok`), which is the point I was trying to get across.
",jamesob,sigmavirus24
2002,2014-04-14 21:28:54,"Yeah I figured this would break backward compatibility... somebody somewhere is surely depending on the current behavior.  I just really dislike it :)

Thanks @sigmavirus24 
",slinkp,sigmavirus24
2002,2014-05-23 01:58:29,"@sigmavirus24 Sorry my comment came across as negative. I only meant to suggest a way that that common pattern can work with requests' current design. As KR suggested, the intention was to be educational/constructive and not insulting.
",fletom,sigmavirus24
1997,2014-04-09 20:50:42,"At the end of that section that @Lukasa linked to, there's a bit about the [requests-toolbelt](https://gitlab.com/sigmavirus24/toolbelt). Using that library would allow you to specify your own boundary without having to craft the header yourself.
",sigmavirus24,Lukasa
1997,2014-04-09 23:30:13,"@Lukasa Yes, I appreciate how simple and clear the document said about posting files. However, from the same page of document it also specified [custom headers](http://docs.python-requests.org/en/latest/user/quickstart/#custom-headers). And some samples from the document also have custom 'Content-Type' header. So it would seem totally OK to do that.
@sigmavirus24 Thanks for point that out. But I've encountered this issue when I'm just playing around with fairly small sized files. So at first glance, I'd think that I don't need streaming and skip that part.

The reason I report this is that when I'm trying requests, there's no warning about custom boundaries in document or stderr ouput. If there were, it'd save me a lot of time trying to figure out why the sent request isn't working.
So I think at least there should be a note like ""Custom boundaries are not supported. Please let requests generate that specific header for you"".
",jcfrank,Lukasa
1997,2014-04-09 23:30:13,"@Lukasa Yes, I appreciate how simple and clear the document said about posting files. However, from the same page of document it also specified [custom headers](http://docs.python-requests.org/en/latest/user/quickstart/#custom-headers). And some samples from the document also have custom 'Content-Type' header. So it would seem totally OK to do that.
@sigmavirus24 Thanks for point that out. But I've encountered this issue when I'm just playing around with fairly small sized files. So at first glance, I'd think that I don't need streaming and skip that part.

The reason I report this is that when I'm trying requests, there's no warning about custom boundaries in document or stderr ouput. If there were, it'd save me a lot of time trying to figure out why the sent request isn't working.
So I think at least there should be a note like ""Custom boundaries are not supported. Please let requests generate that specific header for you"".
",jcfrank,sigmavirus24
1995,2014-04-26 21:16:29,"@dstufft can we do something like `requests[+PyOpenSSL]` or `requests[+betterssl]`? By which I mean: is the `+` allowed by distutils/setuptools?
",sigmavirus24,dstufft
1995,2014-04-26 21:17:31,"No, a `+` won't parse correctly on the `pip install` side.

On Sat, Apr 26, 2014 at 2:16 PM, Ian Cordasco notifications@github.comwrote:

> @dstufft https://github.com/dstufft can we do something like
> requests[+PyOpenSSL] or requests[+betterssl]? By which I mean: is the +allowed by distutils/setuptools?
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/issues/1995#issuecomment-41481134
> .

## 

""I disapprove of what you say, but I will defend to the death your right to
say it."" -- Evelyn Beatrice Hall (summarizing Voltaire)
""The people's good is the highest law."" -- Cicero
GPG Key fingerprint: 125F 5C67 DFE9 4084
",alex,dstufft
1994,2014-04-05 14:45:03,"@ionelmc Thanks for this! Leave the '$' signs in (we like it stylistically) and I'll happily merge it. =)
",Lukasa,ionelmc
1994,2014-04-05 14:51:00,"@ionelmc regardless of whether you think it is poor style, it is convention in all technical documentation and is a clear signal (to most) that you should be using the text following it on a command line.
",sigmavirus24,ionelmc
1987,2014-04-01 13:43:46,"FWIW I don't mind the code in pip, we have to subclass session anyways for some other stuff and the code to add timeout support isn't very large :) I opened primarily at @Lukasa's prompting :)
",dstufft,Lukasa
1985,2016-08-18 00:02:31,"So I'm going to start working on adding this support to [rfc3986](/sigmavirus24/rfc3986). In reading the specification, there's this:

>   In a URI, a literal IPv6 address is always embedded between ""["" and
>   ""]"".  This document specifies how a <zone_id> can be appended to the
>   address.  According to URI syntax [RFC3986], ""%"" is always treated as
>   an escape character in a URI, so, according to the established URI
>   syntax [RFC3986] any occurrences of literal ""%"" symbols in a URI MUST
>   be percent-encoded and represented in the form ""%25"".  Thus, the
>   scoped address fe80::a%en1 would appear in a URI as
>   http://[fe80::a%25en1].

So the right thing to do is encode the `%` sign it seems.

@Lukasa should we reopen this to track the work that we need to do in urllib3?
",sigmavirus24,Lukasa
1985,2016-10-04 22:42:51,"Hi @Lukasa

Wondering if you could post the related issue number for this on urllib3 so I could track it?  I tried looking for it, but didn't see it (I did see something about proxy handling...).  Thanks in advance

Also, for others that might be wondering - there is a workaround: if your system only has 1 interface with IPv6 addressing, then you do not need to supply the %<interface>, and things work just fine.  Unfortunately, all of my testing involves many networks with different vlans and I need IPv6 support on them all.
",briggr1,Lukasa
1985,2016-10-12 08:14:37,"@briggr1 I don't believe any tracking issue for this was actually opened on urllib3. I'd need @sigmavirus24 to outline exactly what work he believes is needed though: in my initial testing urllib3 seems to be handling this appropriately.
",Lukasa,sigmavirus24
1981,2014-03-31 14:29:29,"@Lukasa that is not true :)
",kennethreitz,Lukasa
1980,2014-05-04 14:01:41,"@alekstorm also there's no rush. If needed, @Lukasa can improve the PR on a branch of our own (using all of your work as a base). And congratulations on the new job! I hope it treats you well :)
",sigmavirus24,Lukasa
1977,2014-03-27 18:10:41,"@t-8ch @athoik 

wildcard certs support only one level wildcard-ish. 

But: The SSL certificate on hostname `ia600301.us.archive.org` is issued on the name `*.us.archive.org`. 

The error-message in requests shows another domain name .. 
",syphar,t-8ch
1977,2014-03-27 18:44:22,"Thanks for the explanation @Lukasa, lets hope that PEP 466 will accepted in Python 2.7. Until then i am afraid that setting `verify=False` is the only way.
",athoik,Lukasa
1976,2014-03-26 13:38:02,"In related news, @kennethreitz is there an email address that the Jenkins server emails when tests fail? If not, could you add @Lukasa and/or me to the notifications? Assuming Jenkins runs on pushes to master (i.e., when you merge a PR) I would have seen this and fixed it sooner.
",sigmavirus24,Lukasa
1976,2014-03-26 15:34:33,"@sigmavirus24 I can set that up if you'd like, but it's pretty damn annoying :)
",kennethreitz,sigmavirus24
1976,2014-03-26 16:23:59,"@kennethreitz It's annoying, but it's the kind of annoying that @sigmavirus24 and I are for. =D
",Lukasa,kennethreitz
1976,2014-03-26 16:23:59,"@kennethreitz It's annoying, but it's the kind of annoying that @sigmavirus24 and I are for. =D
",Lukasa,sigmavirus24
1976,2014-03-26 16:31:24,"What @Lukasa said. Also I like to know when I've broken stuff. (Because this was sort of my fault)
",sigmavirus24,Lukasa
1973,2014-03-24 20:51:13,"@Lukasa try that:



do an 



you should get 60

then ""release"" the connections to the pool:



do the same 



There are still 60 sockets in CLOSE_WAIT state.

I think that something is not as expected.

Can you verify that it is ok?
Thanks
",gosom,Lukasa
1973,2014-03-25 11:57:53,"@Lukasa 

you are right it works correct this way.
But when you explicitly close the connections there should be only MAX_CONNECTION_POOL_SIZE open correct?
",gosom,Lukasa
1972,2014-03-23 16:49:03,":+1: :cake: Thanks. Assigning @kennethreitz since @Lukasa and I agree this is ready to merge.
",sigmavirus24,Lukasa
1972,2014-04-04 17:04:24,"@avidas Kenneth is insanely busy (and probably even more so than usual as PyCon is next week), and so he does these things in batches. He'll get to it. =)
",Lukasa,avidas
1970,2016-03-09 21:28:47,"@Lukasa clearly _it_ is happening. :wink: 
",sigmavirus24,Lukasa
1968,2014-03-19 22:43:18,"@Lukasa I didn't trigger this but I prefer it this way. Certain things have been fixed/broken in 2.3.0. That behaviour is not available/visible in 2.2.1 and so the documented behaviour is accurately documented. :)
",sigmavirus24,Lukasa
1968,2014-03-20 00:46:38,"Thanks for the info @lukasa.

Do we all agree the current info on the
http://docs.python-requests.org/en/latest/ page is misleading, as it
(a) calls 2.3.0 ""released"", and (b) calls it ""latest"" (in the url)? From
that I'd conclude 2.3.0 was the latest release.

On Wednesday, March 19, 2014, Ian Cordasco
<notifications@github.com<javascript:_e(%7B%7D,'cvml','notifications@github.com');>>
wrote:

> @Lukasa https://github.com/Lukasa I didn't trigger this but I prefer it
> this way. Certain things have been fixed/broken in 2.3.0. That behaviour is
> not available/visible in 2.2.1 and so the documented behaviour is
> accurately documented. :)
> 
> ## 
> 
> Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/issues/1968#issuecomment-38116629
> .
",skivvies,Lukasa
1968,2014-03-20 00:55:32,"If you change the current latest version to something like ""2.3.0-dev"" (and
then update the docs, ideally substituting ""release"" with ""version"" for
docs on not-yet-released versions) it'd be clearer, and more semantic too.

On Wednesday, March 19, 2014, _pants@getlantern.org wrote:

> Thanks for the info @lukasa.
> 
> Do we all agree the current info on the
> http://docs.python-requests.org/en/latest/ page is misleading, as it
> (a) calls 2.3.0 ""released"", and (b) calls it ""latest"" (in the url)? From
> that I'd conclude 2.3.0 was the latest release.
> 
> On Wednesday, March 19, 2014, Ian Cordasco notifications@github.com
> wrote:
> 
> > @Lukasa https://github.com/Lukasa I didn't trigger this but I prefer
> > it this way. Certain things have been fixed/broken in 2.3.0. That behaviour
> > is not available/visible in 2.2.1 and so the documented behaviour is
> > accurately documented. :)
> > 
> > ## 
> > 
> > Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/issues/1968#issuecomment-38116629
> > .
",skivvies,Lukasa
1966,2014-03-18 23:38:14,"I can reproduce this but I have the same question as @Lukasa 
",sigmavirus24,Lukasa
1965,2014-03-24 15:40:29,"Exactly what @Lukasa said — the Mixin is all about readability :)
",kennethreitz,Lukasa
1964,2014-03-16 14:34:09,"@sigmavirus24 : Thank you so much; will keep these things in mind.
",jayanthkoushik,sigmavirus24
1962,2014-03-24 15:44:41,"@sigmavirus24 you seem to really be in a rush lately.
",kennethreitz,sigmavirus24
1961,2014-03-15 16:54:50,"Good catch @Lukasa. There's a [related issue](https://github.com/shazow/urllib3/issues/356) already open on urllib3.

I'm closing this since we pull in urllib3 before releasing each time anyway.
",sigmavirus24,Lukasa
1959,2014-03-14 13:28:38,"@Feng23 thanks in advance. Also thank you for being patient and understanding with @Lukasa and me. It's our job to make sure your code is excellent enough to be merged without Kenneth objecting. Thanks for this contribution. It's excellent. :cake: 
",sigmavirus24,Feng23
1959,2014-03-14 13:28:38,"@Feng23 thanks in advance. Also thank you for being patient and understanding with @Lukasa and me. It's our job to make sure your code is excellent enough to be merged without Kenneth objecting. Thanks for this contribution. It's excellent. :cake: 
",sigmavirus24,Lukasa
1959,2014-03-24 15:46:50,"@Feng23 Can you do a rebase for me? I'll merge once you do :)
",kennethreitz,Feng23
1959,2014-03-26 16:38:59,"@Feng23 one more rebase and your build should pass. :)
",sigmavirus24,Feng23
1956,2014-10-10 21:01:10,"Hey @mikecool1000 welcome to the project. So you can't simply change the function signature like that, here's why.

Right now, someone could be doing this:



If we then change the signature, even one extra positional argument will fail, e.g., try the following:



So you'd want to add `**kwargs` after the existing positional arguments (that have defaults). Maybe this will set you on the right course without giving you too much room to shoot yourself in the foot or giving you the all the answers. :)
",sigmavirus24,mikecool1000
1956,2014-10-10 21:56:51,"Thanks that's very helpful

Sent from my iPhone

> On Oct 10, 2014, at 2:01 PM, Ian Cordasco notifications@github.com wrote:
> 
> Hey @mikecool1000 welcome to the project. So you can't simply change the function signature like that, here's why.
> 
> Right now, someone could be doing this:
> 
> s = requests.Session()
> 
> # ...
> 
> s.resolve_redirects(resp, req, False, 1.0, True, None, {})
> If we then change the signature, even one extra positional argument will fail, e.g., try the following:
> 
> def foo(a, b, c=3):
>     return a, b, c
> 
> foo(1, 2)  # should return (1, 2, 3)
> foo(1, 2, 4)  # should return (1, 2, 4)
> 
> def foo(a, b, **kwargs):
>     c = kwargs.pop('c', 3)
>     return a, b, c
> 
> foo(1, 2)  # should return (1, 2, 3)
> foo(1, 2, 4)  # should raise a TypeError about foo only accepting 2 arguments and receiving 3
> So you'd want to add **kwargs after the existing positional arguments (that have defaults). Maybe this will set you on the right course without giving you too much room to shoot yourself in the foot or giving you the all the answers. :)
> 
> —
> Reply to this email directly or view it on GitHub.
",mikecool1000,mikecool1000
1956,2014-10-10 22:13:29,"@mikecool1000 if you need other help, please hop onto #python-requests on Freenode this weekend. If I'm around, I'll be happy to chat with you about this.
",sigmavirus24,mikecool1000
1956,2014-10-13 02:14:29,"@mikecool1000 how familiar are you with generators? You should try this out at the interpreter:



When you run `g = s.resolve_redirects(r, r.request)` note that it will return immediately. It doesn't talk to the network until you start to consume the generator. There's no unnecessary computation at all :)
",sigmavirus24,mikecool1000
1956,2014-10-13 18:07:30,"No one called you a bad coder @mikecool1000 and you don't need to apologize for not having had prior experience with generators
",sigmavirus24,mikecool1000
1956,2014-10-13 18:58:02,"Thanks for helping me learn then :)

Sent from my iPhone

> On Oct 13, 2014, at 11:07 AM, Ian Cordasco notifications@github.com wrote:
> 
> No one called you a bad coder @mikecool1000 and you don't need to apologize for not having had prior experience with generators
> 
> —
> Reply to this email directly or view it on GitHub.
",mikecool1000,mikecool1000
1955,2014-03-15 17:55:05,"@zackw check out #1963 if you have some time.
",sigmavirus24,zackw
1954,2014-03-14 13:37:07,"I'm with @Lukasa on this one and I'll work on an example. That said, we allow the user to shoot themselves in the foot plenty. There's absolutely no reason to stop them from doing so here. It's our job to provide an elegant API to the user, not to make sure they do everything right.

You're also proposing that we should favor the prepared request on the Response object to the one passed in. Perhaps someone is relying on this specific behaviour for a really good reason. If we change this the way you're suggesting, that person's code will break in an awful and entirely unexpected way. No matter how well you document changes, people will ignore that documentation and expect the same behaviour unless the API is totally changed, i.e., the request parameter is totally removed.
",sigmavirus24,Lukasa
1953,2014-03-14 14:22:43,"Taking @sigmavirus24's concern on board, is there an elegant way we can do this _outside_ of the library, e.g. in the toolbelt?
",Lukasa,sigmavirus24
1953,2014-03-15 17:21:34,"> Taking @sigmavirus24's concern on board

It isn't a very strong concern. It's more of a pattern I've seen develop as of late. People watch the repo for a tiny change and use that change to get their foot in the door for a larger one that is widely unnecessary. It's a tiny concern that's ever present now.

Likewise, I think the toolbelt could easily accomodate this. That said, I'm not convinced it should be either in or outside of the core (i.e., I don't actually know where it belongs).

I've also been thinking along the same lines @zackw, but more geared towards making an eventual refactor a lot easier. I like having a compliment to `prepare_request` sibling. How does `prepare_redirected_request` sound?
",sigmavirus24,zackw
1953,2014-03-15 17:21:34,"> Taking @sigmavirus24's concern on board

It isn't a very strong concern. It's more of a pattern I've seen develop as of late. People watch the repo for a tiny change and use that change to get their foot in the door for a larger one that is widely unnecessary. It's a tiny concern that's ever present now.

Likewise, I think the toolbelt could easily accomodate this. That said, I'm not convinced it should be either in or outside of the core (i.e., I don't actually know where it belongs).

I've also been thinking along the same lines @zackw, but more geared towards making an eventual refactor a lot easier. I like having a compliment to `prepare_request` sibling. How does `prepare_redirected_request` sound?
",sigmavirus24,sigmavirus24
1953,2014-03-16 13:45:56,"I really want to get some work done on the toolbelt and betamax today. If @zackw has the time to throw together an example of `prepare_redirected_request` that'd be great. Otherwise, I'll likely work on it later this week.
",sigmavirus24,zackw
1953,2014-03-16 14:28:47,"@sigmavirus24 Not a problem - it's a simple matter of moving code around.  See #1965.

(I am going to be offline for most of the rest of the day, though.)
",zackw,sigmavirus24
1952,2014-03-13 16:59:41,"@zackw can you please give more detailed steps to reproduce (code example or test case). 
",schlamar,zackw
1951,2014-03-23 14:45:45,"Assigning to @kennethreitz since this looks good to me. :shipit: 
",sigmavirus24,kennethreitz
1949,2014-03-12 18:36:54,"Can I get review from @sigmavirus24 and @ionrock?
",Lukasa,ionrock
1949,2014-03-12 22:37:22,"This seems to fix things in CacheControl. Thank you @Lukasa! 

My reasoning for suggesting the `HTTPResponse` was because then someone doesn't have to check whether or not a response was cached or created outside a normal request. For example, if you used `resp.raw.seek(0)` you'd get an error as `None` obviously doesn't have a `seek` method. 

With that said, `seek` doesn't work anyway! No blood, no foul. 

Thanks for fixing this so quickly. I'll add some docs in CacheControl to help communicate that a cached response's `raw` attribute will be `None`. 
",ionrock,Lukasa
1945,2014-03-11 18:52:29,"Given @kennethreitz's -1 I'm going to close this.
",sigmavirus24,kennethreitz
1945,2014-03-12 20:56:11,"@sigmavirus24 I can close pull requests.
",kennethreitz,sigmavirus24
1945,2014-10-10 14:03:32,"@kennethreitz Here is a use case regarding `assert_hostname`. I want to run a HTTPS web server with a self signed certificate and run some tests with requests against it. As this web server can run on multiple nodes it has no static host/IP. 

If I set the requests CA_BUNDLE to my certificate it fails on `ssl_match_hostname`, because there is no hostname defined in the cert (_requests.exceptions.SSLError: no appropriate commonName or subjectAltName fields were found_ ).

Right now, the only way of making this configuration to work is a) completely disable certificate verification or b) patch urllib3/requests to pass `assert_hostname=False`. Both options are not really nice.

Can you elaborate on your proposal to use a class? I guess I can help with a PR if you are more specific about what you expect.
",schlamar,kennethreitz
1945,2014-10-10 15:16:45,"@schlamar in this case, if I were you, I would do the following:
1. Use @t-8ch's [StackOverflow answer](http://stackoverflow.com/a/22794281/1953283) to create a new Adapter
2. Register the adapter on your session like so:


",sigmavirus24,schlamar
1944,2014-03-10 14:41:12,"@maxcountryman Does this fix your issue (https://github.com/shazow/urllib3/issues/206)?
",schlamar,maxcountryman
1944,2014-03-10 14:48:09,"@schlamar it looks like it should. I can't test it at the moment. A little later on I'll see if I have time to give it a proper run with Photobucket.
",maxcountryman,schlamar
1944,2014-03-13 15:56:30,"In addition to #1939, I believe this also fixes #1952 (which I just filed - sorry about that).

@schlamar Could you please add some test cases for these bugs as well?  A test case for #1952 can be dug out of the (now-scrapped) pull request #1919.

@sigmavirus24 Regarding ""`except RuntimeError: pass # already decoded`"", I sympathize with your ""should this exception have been thrown at all?"" reaction, but it makes sense for `Response.content` to throw an exception when all the content has already been consumed; it happens that in this case we don't care since we are only accessing `.content` for its side effects.  (Would it make you more comfortable if a more specific exception were thrown?)
",zackw,schlamar
1944,2014-03-13 15:56:30,"In addition to #1939, I believe this also fixes #1952 (which I just filed - sorry about that).

@schlamar Could you please add some test cases for these bugs as well?  A test case for #1952 can be dug out of the (now-scrapped) pull request #1919.

@sigmavirus24 Regarding ""`except RuntimeError: pass # already decoded`"", I sympathize with your ""should this exception have been thrown at all?"" reaction, but it makes sense for `Response.content` to throw an exception when all the content has already been consumed; it happens that in this case we don't care since we are only accessing `.content` for its side effects.  (Would it make you more comfortable if a more specific exception were thrown?)
",zackw,sigmavirus24
1944,2014-03-13 16:39:43,"> Could you please add some test cases for these bugs as well?

Very interesting. I guess this should be possible (while a test case for the original issue is actually hard, we would need a misbehaving web service for that).

> should this exception have been thrown at all

I don't think this was his point. I guess you mixed two things together. My interpretation of @sigmavirus24 comments is:
1. Instead of a RuntimeError there should be a more explicit exception if the content is already consumed
2. Don't handle the RuntimeError separately in resolve_redirects, just do a `r.raw.read(...)` in all exception cases.
",schlamar,sigmavirus24
1944,2014-03-14 13:43:41,"> My interpretation of @sigmavirus24 comments _[snip]_

Your interpretation is correct. I've discussed this with several (far more experienced and knowledgeable Python developers, including core PyPy and CPython developers). `RuntimeError` exceptions should never be raised by any library ever. Yes they're there but that does not mean you should use them.

This also works with your overall goal @zackw: If the error were instead a child of a `RequestException` we could name it well and have:



Work in every case. In the redirect case, before this PR, it wouldn't. I know that there are several ideas of ""good"" Python code that I disagree with others on (including Kenneth), so I don't push those issues normally.

I haven't found a good way to preserve backwards compatibility though for those catching the RuntimeError and allowing for a RequestException. I don't think creating a new one that inherits from both is a good idea in the slightest. I'd love if either of you had the solution. In fact, I'd probably send you :cake: :)
",sigmavirus24,zackw
1944,2014-03-14 13:43:41,"> My interpretation of @sigmavirus24 comments _[snip]_

Your interpretation is correct. I've discussed this with several (far more experienced and knowledgeable Python developers, including core PyPy and CPython developers). `RuntimeError` exceptions should never be raised by any library ever. Yes they're there but that does not mean you should use them.

This also works with your overall goal @zackw: If the error were instead a child of a `RequestException` we could name it well and have:



Work in every case. In the redirect case, before this PR, it wouldn't. I know that there are several ideas of ""good"" Python code that I disagree with others on (including Kenneth), so I don't push those issues normally.

I haven't found a good way to preserve backwards compatibility though for those catching the RuntimeError and allowing for a RequestException. I don't think creating a new one that inherits from both is a good idea in the slightest. I'd love if either of you had the solution. In fact, I'd probably send you :cake: :)
",sigmavirus24,sigmavirus24
1944,2014-03-15 18:01:41,"@schlamar it took me several months to get a feature merged and deployed on HTTPBin in order to test a bug fix. I'm not certain we should bother waiting that long. Do you have sufficient confidence that you could fake out a gzipped response from a redirect? You could take a similar approach to my test in #1963. Unfortunately, when I planned that test code, I was planning for the simplest case (mine). If there's a good way to adapt it to this PR, that would be awesome.
",sigmavirus24,schlamar
1944,2014-03-15 20:35:19,"@sigmavirus24 @zackw Added a test. What do you think?
",schlamar,zackw
1944,2014-03-15 20:35:19,"@sigmavirus24 @zackw Added a test. What do you think?
",schlamar,sigmavirus24
1944,2014-03-16 17:47:13,"@sigmavirus24 Should I change 



to


",schlamar,sigmavirus24
1944,2014-03-18 11:10:26,"> I prefer to do something as opposed to using pass in an except block. 

@sigmavirus24 I guess you'll like the new [`contextlib.suppress`](http://docs.python.org/3.4/library/contextlib.html#contextlib.suppress) in Python 3.4. =)
",schlamar,sigmavirus24
1944,2014-03-24 17:00:25,"@kennethreitz If `.transfer_encoding` is something you're interested in I'll take a crack at it.
",Lukasa,kennethreitz
1944,2014-03-31 14:31:38,"@Lukasa by all means! 
",kennethreitz,Lukasa
1944,2014-03-31 14:32:36,"@schlamar or specifying a transfer encoding other than what the server provided. It'll just give is a more flexible architecture around this, in general.
",kennethreitz,schlamar
1944,2014-05-12 19:22:05,"@schlamar if you perform a rebase this will be merged and released today!
",kennethreitz,schlamar
1942,2014-03-07 13:28:36,"Mm, I agree with @sigmavirus24 here. Something weird should be going on with YCM to cause this. Here's my reasoning:
1. `socket` must be imported because it's unconditionally imported at the top of `adapters.py`: any failure to import would cause an `ImportError`.
2. `socket` must be imported because if the import hadn't run at all we wouldn't get an `AttributeError`, we'd get a `NameError` on `socket`.

These two facts suggest that `requests.adapters.socket` has been monkeypatched to `None`. We never do this in Requests, so I'd look at YCM.
",Lukasa,sigmavirus24
1942,2014-03-07 20:13:32,"@sigmavirus24 It happens for me only when I write-quit or quit out of vim and only from time to time. I am fine with it silently swallowing it. The reason I brought it up in the first place with @Valloric was that I thought it was unintended behaviour. :)
",ashemedai,sigmavirus24
1941,2014-03-06 16:48:47,"@Lukasa sounds like a worthwhile addition.
",sigmavirus24,Lukasa
1940,2014-03-08 02:32:36,"> The actual point of raising the exception is here: https://github.com/kennethreitz/requests/blob/master/requests/adapters.py#L391

@schlamar we cannot catch that in `Session#resolve_redirects`. To catch it in the adapter would be a serious change in behaviour. I wonder why we consume all of the content in the adapter though. If there's a decoding error, we don't even attempt to return a Response to the user. That may be fodder for an entirely different issue though. If @Lukasa is satisfied with this as an immediate solution to what was reported, I am too. But I want to investigate that other piece in the adapter as well.
",sigmavirus24,schlamar
1940,2014-03-08 14:53:49,"Reviewing the issue, you're once again correct @schlamar. That aside @mechanical-snail is not wrong that this could in fact also be a problem. Their issue won't be fixed by this but it may fix the case where streaming is used.
",sigmavirus24,schlamar
1939,2014-03-05 06:44:15,"We decode the data because your assertion that it won't be read is false. You may read the response body from any redirect because we save it. Each redirect builds a _full_ response object that can be used exactly like any other. This is a very good thing, and won't be changed. =)

The fix, as @sigmavirus24 has suggested, is simply to catch this error.
",Lukasa,sigmavirus24
1939,2014-03-07 14:14:10,"@schlamar That's very interesting.

However, this isn't a dupe, it's just related. The key is that we shouldn't really care even if we hit a legitimate decoding error when following redirects: we just want to do our best and then move on.
",Lukasa,schlamar
1939,2014-03-07 14:23:03,"@Lukasa hit the nail on the head :)
",sigmavirus24,Lukasa
1939,2014-03-07 14:29:03,"@schlamar So the issues you linked cause the exception, but they aren't the problem being referred to. The key problem in _this_ issue is that if we hit an error decoding the response body of a redirect, we'll stop following redirects. That _shouldn't_ happen: we understood enough of the message to follow the redirect, so there's no reason to stop following them. =)

Fixing the bugs you linked fixes the specific case in question, but not the general one.
",Lukasa,schlamar
1938,2014-03-05 06:48:26,"There are two forms of decoding here: decoding from compressed data to bytes, and then decoding from bytes to unicode. The first is done in urllib3, the second in Requests. Because the first is done in urllib3, Requests never sees the gzipped bytes. This means the only way to achieve what you want _in requests_ is to reimplement the convenience methods that urllib3 already has for decompressing data. That just seems silly, so I recommend following @sigmavirus24's advice and getting this into urllib3.
",Lukasa,sigmavirus24
1935,2014-03-01 01:10:52,"I think I've fixed the styling issues, now @Lukasa - thanks! :) Let me know if I've missed anything. 
",ceaess,Lukasa
1935,2014-03-03 20:25:19,"@vlevit: Good catch, that section of the docs has been changed.

@kennethreitz: For the moment I've made sure it'll be in the changelog. If you want something more drastic I'll whip up a section of the documentation that explains timeouts in more detail. =)
",Lukasa,kennethreitz
1935,2014-03-12 18:28:01,"@Lukasa Regarding docs again, I think the code example after the timeout changes explanation is no more relevant:-)
",vlevit,Lukasa
1930,2014-02-26 14:17:08,"I agree with @Lukasa's comment. Further, `Session#send` should not return either an iterator or a response. It should only return one ever. The 98% use case desires a Response and that's all we should ever return. I will not budge on this.

I haven't reviewed the code at all, but I will probably leave PR review when I do. As this is described right now, I'm :-1: on the entire change with the caveat that I haven't reviewed much beyond your description @zackw 
",sigmavirus24,zackw
1930,2014-02-26 14:17:08,"I agree with @Lukasa's comment. Further, `Session#send` should not return either an iterator or a response. It should only return one ever. The 98% use case desires a Response and that's all we should ever return. I will not budge on this.

I haven't reviewed the code at all, but I will probably leave PR review when I do. As this is described right now, I'm :-1: on the entire change with the caveat that I haven't reviewed much beyond your description @zackw 
",sigmavirus24,Lukasa
1930,2014-02-27 02:44:10,"I agree with all of @Lukasa's feedback. Frankly @zackw you seem to want to throw everything out and rewrite it from scratch. While that isn't always a bad thing, you seem to be taking advantage of our desire to help you out. I for one will not accept that. We have tried to help usher you through the process of making your work as close to perfect as possible and all I have seen in this thread is you fighting us. I understand why you're fighting but I see no genuine attempts at compromise. At best you're not making me anymore sympathetic to your goal.
",sigmavirus24,zackw
1930,2014-02-27 02:44:10,"I agree with all of @Lukasa's feedback. Frankly @zackw you seem to want to throw everything out and rewrite it from scratch. While that isn't always a bad thing, you seem to be taking advantage of our desire to help you out. I for one will not accept that. We have tried to help usher you through the process of making your work as close to perfect as possible and all I have seen in this thread is you fighting us. I understand why you're fighting but I see no genuine attempts at compromise. At best you're not making me anymore sympathetic to your goal.
",sigmavirus24,Lukasa
1930,2014-02-28 12:40:42,"Also, for what it is worth, having a huge change like this in one commit is not helpful to either @Lukasa or me. This [Guide to Git Commits](https://wiki.openstack.org/wiki/GitCommitMessages) for OpenStack should explain why it's harder for us to review one huge commit as opposed to a series of smaller ones which tell a story.
",sigmavirus24,Lukasa
1930,2014-03-12 20:41:36,"@kennethreitz I'm @zackw, not @zachw :-)

Could probably sit down and talk sometime tomorrow.  I'm in US/Eastern and can do either Skype or Google (but Skype is preferred).  I will also find time to file bugs for all the things I was trying to fix, as requested earlier.
",zackw,kennethreitz
1930,2014-03-12 20:41:36,"@kennethreitz I'm @zackw, not @zachw :-)

Could probably sit down and talk sometime tomorrow.  I'm in US/Eastern and can do either Skype or Google (but Skype is preferred).  I will also find time to file bugs for all the things I was trying to fix, as requested earlier.
",zackw,zackw
1929,2014-02-25 22:42:16,"Thanks for this @alex! Assuming #1919 gets cleaned up this should be fixed by that pull request. =)
",Lukasa,alex
1929,2014-08-21 17:43:19,"@alex fixed!
",kennethreitz,alex
1928,2014-02-22 20:49:58,"@Lukasa,

Thank you for the monkey patch. I have it running on a test stream right now. I understand using this patch is on my own recognizance. That you disavow all knowledge of its existence and will deny that you were ever involved. ;-)

There are three main reasons I've moved to using your general purpose stack. First, I'll be able to use this knowledge elsewhere. (The database I use, Couchbase/CouchDB, uses HTTP for control messages.) Second, Requests API is almost as efficient as the Twitter library I am used to. This is a great accomplishment. Third, Requests is more performant. Requests is able to turn gzip on during the stream. This reduces my bandwidth use by a factor of 4 and, apparently, Twitter gives me more tweets. Yay!

Keep up the good work. I'll watch this repo. I'll try to be aware when you've come to a decision. That said, do not hesitate to ping me. As long running timeouts are hard to test, I'll be happy to start testing the fix when you're team is ready.

Anon,
Andrew
",adonoho,Lukasa
1928,2014-02-24 14:30:19,"@Lukasa,

I've had the patch running for two days now and it appears to have properly detected hangups from Twitter. My app then safely restarts the connection.

I'm looking forward to your full featured solution. Rest assured though that it is needed and will be used.

Anon,
Andrew
",adonoho,Lukasa
1928,2014-02-24 19:20:34,"@Lukasa 

I was testing this out with it and it seemed to work:



I'd call get within the try/except after a with time_limit(10):

That seemed to work.

Same concept?

Thanks.
",reticulatingspline,Lukasa
1925,2014-02-18 21:12:42,"@t-8ch Correct. =)

@ntucker Thanks for the suggestion! Unfortunately, this is not a direction we're prepared to go. As I said [in the linked issue](https://github.com/kennethreitz/requests/issues/1595#issuecomment-30993198):

> I see no reason for Requests to favour ujson over any other third-party JSON decoder. We do nothing very complicated with JSON decoding, so replacing the decoder we use either via monkeypatching or via doing the decoding yourself is totally safe. With that in mind, there's no good reason to move away from the standard library in Requests proper.
",Lukasa,t-8ch
1924,2014-03-13 06:45:42,"@Lukasa I assume this stalled until 2.3 is in sight? Any rough ETA?
",schlamar,Lukasa
1924,2014-03-14 13:44:56,"@schlamar I've been doing this for PRs that need to be merged quickly so they don't get lost. I'll assign it and then comment along the lines ""LGTM!"" so that Kenneth gets an email (I'm not sure he has emails turned on for every issue/PR). This is just my way of being certain that he receives a notification. :)
",sigmavirus24,schlamar
1920,2014-02-14 20:31:06,"@sigmavirus24 Sorry, I had the context for this issue already. =)

Basically, we allow you to temporarily unset a header like this:



But if you try to permanently unset a header on a `Session` in an analogous way, you get surprising behaviour:



The question is, should we allow the example above to work, or should we just continue to use the `del` behaviour?
",Lukasa,sigmavirus24
1920,2014-02-14 22:17:09,"@Lukasa I think this is actually a regression in how we used to behave but such are the consequences when less tests are preferred to more tests. =P
",sigmavirus24,Lukasa
1918,2014-02-13 21:07:28,"I've discovered the flow for getting @kennethreitz to merge things faster. /Social Coding Hacking/
",sigmavirus24,kennethreitz
1916,2014-03-06 16:17:44,"> I don't want to be too coupled to urllib3.

@kennethreitz You know that urllib3 is doing the decompression, right?
",schlamar,kennethreitz
1916,2014-03-08 15:37:11,"@kennethreitz updated
",schlamar,kennethreitz
1916,2014-03-10 13:01:54,"@schlamar yes — I wrote the code, believe it or not :)

Just because urllib3 gives us a place to bind to doesn't mean that we should. As a matter of fact, it's often a good place to question ourselves in every way and learn a lot about ourselves.
",kennethreitz,schlamar
1916,2014-03-12 11:15:06,"@kennethreitz updated > just remove compress from accepted encoding.
",schlamar,kennethreitz
1915,2015-04-28 19:43:34,"@Lukasa : wondering if the issue is related , however i am trying different option as you stated earlier , but none seems to work for me , Please suggest if you have any pointer:

when I am making this api call to fetch the details from iCloud , it thows error 

> > > api.contacts.all()

Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py:768: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html
  InsecureRequestWarning)
/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py:768: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html
  InsecureRequestWarning)
Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py"", line 372, in _make_request
    httplib_response = conn.getresponse(buffering=True)
TypeError: getresponse() got an unexpected keyword argument 'buffering'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py"", line 544, in urlopen
    body=body, headers=headers)
  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py"", line 374, in _make_request
    httplib_response = conn.getresponse()
  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/http/client.py"", line 1162, in getresponse
    raise ResponseNotReady(self.__state)
http.client.ResponseNotReady: Request-sent

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/requests/adapters.py"", line 370, in send
    timeout=timeout
  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py"", line 597, in urlopen
    _stacktrace=sys.exc_info()[2])
  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/requests/packages/urllib3/util/retry.py"", line 245, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/requests/packages/urllib3/packages/six.py"", line 309, in reraise
    raise value.with_traceback(tb)
  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py"", line 544, in urlopen
    body=body, headers=headers)
  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py"", line 374, in _make_request
    httplib_response = conn.getresponse()
  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/http/client.py"", line 1162, in getresponse
    raise ResponseNotReady(self.__state)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection aborted.', ResponseNotReady('Request-sent',))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/gaurav/pyicloud-0.6.2/pyicloud/services/contacts.py"", line 55, in all
    self.refresh_client()
  File ""/Users/gaurav/pyicloud-0.6.2/pyicloud/services/contacts.py"", line 44, in refresh_client
    self.session1.post(self._contacts_changeset_url, params=params_refresh)
  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/requests/sessions.py"", line 508, in post
    return self.request('POST', url, data=data, json=json, *_kwargs)
  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/requests/sessions.py"", line 465, in request
    resp = self.send(prep, *_send_kwargs)
  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/requests/sessions.py"", line 573, in send
    r = adapter.send(request, **kwargs)
  File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/requests/adapters.py"", line 415, in send
    raise ConnectionError(err, request=request)
requests.exceptions.ConnectionError: ('Connection aborted.', ResponseNotReady('Request-sent',))

The code which is making this call is 

host = self._service_root.split('//')[1].split(':')[0]
        self.session.headers.update({'host': host})
        params_contacts = dict(self.params)
        params_contacts.update({
            'clientVersion': '2.1',
            'locale': 'en_US',
            'order': 'last,first',
        })
        req = self.session.get(
            self._contacts_refresh_url,
            params=params_contacts
        )
        self.response = req.json()
        params_refresh = dict(self.params)
        params_refresh.update({
            'prefToken': req.json()[""prefToken""],
            'syncToken': req.json()[""syncToken""],
        })
        self.session.post(self._contacts_changeset_url, params=params_refresh)
        req = self.session.get(
            self._contacts_refresh_url,
            params=params_contacts
        )
        self.response = req.json()

Appreciate your support . I am currently using Python 3.4.3 ..
",Gaurav-Ambasta,Lukasa
1915,2015-05-04 16:44:03,"@Lukasa YES! After upgrading, the problems seems to be solved!!! Thank you a lot!  
",1a1a11a,Lukasa
1915,2016-01-12 16:16:09,"@Lukasa Alright, understand.
",laike9m,Lukasa
1914,2014-02-13 19:26:54,"@sigmavirus24 -- Thanks for the clarification. We were not relying on your documentation here; we were just leaning on past expectations of convention, which was clearly mistaken.

Thanks the for explanation, and you're right about coercing something into the format you expect it to be in, rather than just trying to marshal it and crossing your fingers.

Thanks the the replies!

--Robin
",hobbeswalsh,sigmavirus24
1913,2014-02-11 21:49:37,"@sigmavirus24 re `next(self.resolve_redirects())`, the most significant reason for doing it differently is that `resolve_redirects` doesn't include the very first response in its iterable (and I didn't feel safe changing that).  That means code that needs to look at each response as it comes in is most naturally structured like this:



rather than like this:



which duplicates part of the processing and has some finicky logic after the loop to get hold of the final response.
",zackw,sigmavirus24
1913,2014-02-12 02:17:28,"@zackw Also, as you reply to PR feedback (since there will be so much of it here) can you reply to each comment your fixing with ""Fixed in <sha>"". I won't mind the emails sent and it will help us keep track of what was fixed and when. It also provides more context when performing final reviews.
",sigmavirus24,zackw
1913,2014-02-12 08:49:35,"I've glanced through the code review comments stuff so far, and wanted to talk more generally about how this relates to our API freeze.

@zackw has said in one of his comments that he believes these changes would be welcome extensions to the API for anyone who has to manually handle redirects. That's probably true. However, our API freeze policy does not say that we are freezing the API ""except when it'll be useful for people if it was extended"". By default, the Requests answer to API changes will always be ""no"". The reason this issue is still open and being discussed is because we think there is potentially enough value here that we want to look at it in depth. Please don't assume that we hate your PR, @zackw, we're just starting from a very conservative position.

Next, there is a question about how much affordance we should give to people who circumvent Requests' redirection policy. The general Requests policy on this sort of thing (see also: `PreparedRequest` objects) is that if you don't like the way Requests does it you should do it yourself. We have made concessions here in the past, but not many and always under substantial duress.

Again, I'm going to hold off more dramatic code review until @kennethreitz gives an idea of whether he's likely to want this change at all.
",Lukasa,zackw
1913,2014-02-12 16:04:29,"@zackw can you explain the `Response.is_redirect` reasoning to me? I can understand the thought process, as redirects are a very first-class citizen in the HTTP world, but I'd love to hear your feelings on it :)
",kennethreitz,zackw
1913,2014-02-12 16:17:42,"I gotta get other work done this afternoon, but, would it be useful for me to split this pull request into two? One strictly for bug fixes, and another for anything that changes the API even a little.

@kennethreitz given that you are open to the possibility of some amount of API changes, I'd like to observe that one API change I didn't make, but rather wanted to, was to have `Session.resolve_redirects` return an iterable that _does_ include the very first response.  Such a generator (under another name, for compatibility's sake) would be a workable, perhaps even preferable, alternative to the `resolve_one_redirect` API I did add.
",zackw,kennethreitz
1913,2014-02-12 16:22:21,"@zackw make one for `Response.is_redirect` first. I'll merge it right away :)
",kennethreitz,zackw
1913,2014-02-12 16:23:11,"@zackw perhaps we could give resolve redirects an argument to include the first response? 
",kennethreitz,zackw
1913,2014-02-12 16:34:56,"@zackw let's break each change into a new PR as we discuss them and keep this one open for discussion. Then, we can bite one thing off at a time. 

So, first thing first — open a new PR for `Request.is_redirect` :)
",kennethreitz,zackw
1913,2014-02-12 17:32:22,"+1 on using smaller PRs.

Also @zackw I'm sorry that none of us ever answered you: we use `py.test` to run the tests.
",sigmavirus24,zackw
1913,2014-02-12 21:14:58,"Alright, made some review comments inline. They are a supplement to @sigmavirus24's. =)
",Lukasa,sigmavirus24
1912,2014-02-11 20:08:25,"@zackw actually this is the right direction. On any response that did have history previously we were returning tuples and on any without we were returning an empty list. The fact of the matter is that history on a response should be immutable. For that to be the case, it should be a tuple. I also don't quite understand your arguments. If you're manually processing redirections or using a hook you will now get a tuple after this change, as you should have in the first place. That does raise a good point that hook authors will expect a list though (if they're writing hooks dealing with manual processing of redirecitons).

@Lukasa that makes this (sort of) a backwards incompatible change.
",sigmavirus24,zackw
1912,2014-02-11 20:35:12,"@sigmavirus24 

> The fact of the matter is that history on a response should be immutable. For that to be the case, it should be a tuple.

It is _conceptually_ immutable, but I do not see why the library needs to bother enforcing that. I think it's rather more important for the type of the property to be the same in all contexts.  And since the response-modification hook is allowed to modify history, it needs to be a list then.  Ergo it should always be a list.

Please see pull request #1913 -- I have put a good deal of thought into how this stuff needs to work, based on actual application experience.

> If you're manually processing redirections or using a hook you will now get a tuple after this change, as you should have in the first place.

On reflection, I think that accurately describes the behavior after your patch for manual redirection processing, but _not_ for the response-modification hook, which fires before the conversion to a tuple.
",zackw,sigmavirus24
1912,2014-02-12 16:12:37,"I agree with @zackw on this. Our current tuple-ness is a bit pedantic and we have to dance about it internally for no reason. Let's not do that.
",kennethreitz,zackw
1912,2014-02-13 12:53:36,"The tuple-ness on the contrary is meaningful and it would be no extra work for @zackw to adapt to this in his PR. Since `redirect_once` does the work for him now, he won't need to worry about munging the history (nor should anyone else). And we should keep in mind that people already expect a tuple _when there is history to be had_. Regardless, I agree it isn't worth arguing about.
",sigmavirus24,zackw
1910,2014-02-11 20:27:24,"Yes, I have: `py27-asn1-0.1.4_1,1`, `py27-ndg_httpsclient-0.3.2`, `py-openssl 0.13` but following the FreeBSD ports tree:
- `py27-asn1` is required by `py27-ndg_httpsclient`
- `py27-ndg_httpsclient` is required by `py-urllib3` (mentioned by @t-8ch) and requires to run: `py-openssl` and `py-asn1`
  Of course, I can de-install any of them, but I like to keep full functionality of requests (SSL, cert. etc.). What I should to do - ""to be or not to be, this is the question..."" ;)
",e-manuel,t-8ch
1910,2014-02-12 08:32:10,"I ranted about this to my housemate last night, by the by. I don't understand why PyOpenSSL busy-waits on this socket instead of calling select like a good person. @t-8ch are you making changes in the urllib3 version, or are you going to submit your patch upstream?
",Lukasa,t-8ch
1910,2014-02-13 01:49:54,"@e-manuel all of the changes are in PyOpenSSL. Since you're using the packages from your distro, you'll need to bother the maintainer of that package when it gets released.

@Lukasa your question is especially relevant since FreeBSD seems to strip out the vendored packages so it should be sent upstream if possible.
",sigmavirus24,Lukasa
1910,2014-02-13 13:05:41,"@t-8ch thank You very much - Your patch works great, now when GET starts it uses below 2% of CPU and after few seconds less :)
",e-manuel,t-8ch
1910,2014-02-13 13:21:38,"Hurrah! Let's leave this open until we get an idea of what's happening upstream.

@t-8ch saves the day again! I should buy him a boat.
",Lukasa,t-8ch
1910,2014-02-19 09:06:20,"@t-8ch You are great - now timeout works fine in every case - and may CPUs can ""sleep"" calmly ;)
",e-manuel,t-8ch
1907,2014-02-08 10:45:30,"Hi @Lukasa ,
Today if a user import `requests` from global python, and if this package has been patched (like it is intended and encouraged) the user will get the platform CA and not the bundle CA.
This pull request aim to give to the user to the same behaviour when he uses `requests` from its own virtualenv, not from global python. In this regard, I think, this PR is pretty consistent.

Now if we want to make progress. Do you suggest to go in a way that is more explicit for the user ?



or you completely reject the idea ?

thx
",ticosax,Lukasa
1907,2014-03-26 09:45:46,"_Use by default SSL CA certificate bundle from the platform_ is what OP proposes in this issue and that's something I totally agree with. It feels so natural to me that I find it difficult someone could oppose this, yet alone argue that not doing this is better experience. Yet this is what I see here and that's why I decided to share my opinion with you.
@Lukasa states

> I don't think we should change which certificates we use based on whether or not you install an optional dependency. Our current behaviour WRT pyopenssl is to enable additional features without changing our current behaviour. That's what you should be aiming for here.

This argument is misguided here as pretty much everyone agrees that when you're dealing with security you should **by default** be as secure as you are able to be, given the environment you operate in. This means using system CA certificates by default (and fallback to bundled ones if it's not possible or very hard to do) and not bundled ones. I think one might argue that security begs for policy of graceful 
degradation.

Also @Lukasa states

> Surely you'd want to ensure that all your users have the exact same certs on all platforms, to avoid annoying platform-specific bugs?

Surely, if the only goal is to _avoid annoying platform-specific bugs_ then yes. But if the goal is to make requests secure by default (and this is in line with _HTTP library for humans_ motto) then surely you would want to ensure that all your users have _the best certs available on their systems_.

Additional bonus is that you save packagers from having to patch this (mis)feature.

I'm curious what @dstuff and @t-8ch think.
",piotr-dobrogost,Lukasa
1907,2014-03-26 13:12:38,"@sigmavirus24, That's the point of this PR, pyopenssl takes care to provide the default platform CA.

https://github.com/shazow/urllib3/commit/5c25a73dfb48e4260c44e19e3a50fb5d46832c52
http://pyopenssl.sourceforge.net/pyOpenSSL.html/openssl-context.html#l2h-132
",ticosax,sigmavirus24
1907,2014-03-26 13:17:43,"@Lukasa summed up everything I was about to say in a much nicer way. Count this as a +1 for what he just said.
",sigmavirus24,Lukasa
1907,2014-03-26 14:46:37,"@sigmavirus24 

> These people then sent PRs to extend the list of possible locations to include the specific locations for the different versions of that distribution they happened to use.

Then all you have to do per @ticosax's remark – _(...) pyopenssl takes care to provide the default platform CA._ – is to direct these people to pyopenssl and close issue right away :)

The theme of this project is that you care about what most people do and you try to make their lives easier. Now, I bet most people use only a handful of systems so there's no problem for pyopenssl project to take care of those. When someone uses unpopular system he should not expect that every piece of software on earth would handle his system. In this case telling him he has to point requests to his CA certs is perfectly fine.

@Lukasa 

> The section of the documentation you linked to quite literally says 'This method may not work properly on OS X'. Such an admission is tantamount to saying 'This method only sometimes works'. 

OS X is a popular OS (unfortunately as I don't like Apple at all) so if it really does not work for it then it's unacceptable. However I don't believe OS X actively hides its CA certs :)
",piotr-dobrogost,Lukasa
1907,2014-03-26 14:46:37,"@sigmavirus24 

> These people then sent PRs to extend the list of possible locations to include the specific locations for the different versions of that distribution they happened to use.

Then all you have to do per @ticosax's remark – _(...) pyopenssl takes care to provide the default platform CA._ – is to direct these people to pyopenssl and close issue right away :)

The theme of this project is that you care about what most people do and you try to make their lives easier. Now, I bet most people use only a handful of systems so there's no problem for pyopenssl project to take care of those. When someone uses unpopular system he should not expect that every piece of software on earth would handle his system. In this case telling him he has to point requests to his CA certs is perfectly fine.

@Lukasa 

> The section of the documentation you linked to quite literally says 'This method may not work properly on OS X'. Such an admission is tantamount to saying 'This method only sometimes works'. 

OS X is a popular OS (unfortunately as I don't like Apple at all) so if it really does not work for it then it's unacceptable. However I don't believe OS X actively hides its CA certs :)
",piotr-dobrogost,sigmavirus24
1906,2014-02-08 15:00:52,"@ssbarnea can you at least share the URL so that we can attempt to reproduce it? So far I think we have almost sufficient information, we just need the URL or characteristics of the server to reproduce it:
- Multiprocessing using _10 threads_
- Function that instantiates a Session and makes more than one request (ostensibly to the same server) with it.
",sigmavirus24,ssbarnea
1906,2015-02-12 19:54:54,"@maxcountryman can you provide any of the details we've been asking for?
",sigmavirus24,maxcountryman
1906,2015-02-12 20:01:20,"@sigmavirus24 well, I'm not using Heroku. What other details did you want?
",maxcountryman,sigmavirus24
1906,2015-02-12 20:14:33,"@maxcountryman 
- Your version of python and openssl
- You operatingsystem and version
- A minimal breaking code snippet
- If possible a public URL which triggers the bug (together with the mentioned code snippet)
",t-8ch,maxcountryman
1906,2015-03-17 08:51:34,"Hi @Lukasa @sigmavirus24 @t-8ch ,
Is your article (https://lukasa.co.uk/2013/01/Choosing_SSL_Version_In_Requests/) compatible with Python 3? Is it right that the current issue will be solved by updating python to version 2.7.9?
",ulandj,t-8ch
1906,2015-03-17 08:51:34,"Hi @Lukasa @sigmavirus24 @t-8ch ,
Is your article (https://lukasa.co.uk/2013/01/Choosing_SSL_Version_In_Requests/) compatible with Python 3? Is it right that the current issue will be solved by updating python to version 2.7.9?
",ulandj,Lukasa
1906,2015-03-17 08:51:34,"Hi @Lukasa @sigmavirus24 @t-8ch ,
Is your article (https://lukasa.co.uk/2013/01/Choosing_SSL_Version_In_Requests/) compatible with Python 3? Is it right that the current issue will be solved by updating python to version 2.7.9?
",ulandj,sigmavirus24
1906,2015-03-17 09:11:45,"@Lukasa i.e. after upgrading to Python 2.7.9 I don't need use your adapter in the article, right? And 



will work with any count of multiprocesses?
",ulandj,Lukasa
1906,2015-03-18 05:10:31,"@Lukasa our customer says that they have the following error: 



Does it say about something?

Using:
python (2.7.9)
requests (2.3.0)
",ulandj,Lukasa
1906,2015-03-18 06:03:31,"@sigmavirus24 
you think that if I move this line of code (from constructor of class) - https://github.com/iron-io/iron_core_python/blob/master/iron_core.py#L162  here - https://github.com/iron-io/iron_core_python/blob/master/iron_core.py#L188, then it will work without errors?
",ulandj,sigmavirus24
1906,2015-03-19 04:11:55,"@maxcountryman thanks,
We offered our customer to install iron_core from [ulan-multiprocessing-sslerror](https://github.com/iron-io/iron_core_python/compare/ulan-multiprocessing-sslerror) branche. We will observe it. Hope the move ""requests"" will help to resolve the ssl error in multiprocessing.
",ulandj,maxcountryman
1906,2016-03-25 00:45:43,"@Lukasa simple repro description:
1. Make session in master process
2. Make some requests to an SSL-enabled site
3. Start 2 multiprocessing processes
4. In the first, make some more requests to the same SSL-enabled site (so connection pooling kicks in)
5. In the second, wait a little bit, then make a request to the same SSL-enabled site

I can't post my script that's hitting this, but I'll see if I can make a simple script that reproduces this with httpbin.

Why this happens: The two children processes share the SSL state initially, and are using the same socket for communication, but once one of them makes a request, the state becomes desynchronized, so when the other tries to use it, the SSL decryption fails.

Some possible fixes:
- Throw out SSL connections from the connection pool upon unpickling the session and close their associated socket objects
  - This is fine because closing the socket object only closes that process's file descriptor; the underlying connection will not be closed until all file descriptors pointing to it are closed
  - Do it on unpickling instead of preventing the connections from making it into the pickle so that you can clean up the file descriptors in the child processes
- Throw out the whole connection pool on unpickling
  - Probably the best idea, because things are probably going to break
- Warn/explode on unpickling a session that has SSL/any connections in its connection pool
",dwfreed,Lukasa
1906,2016-03-25 09:36:36,"@Lukasa I'm MITMing the connection using a transparent proxy.  Only 1 connection is ever made.
",dwfreed,Lukasa
1905,2015-11-24 19:29:53,"@Lukasa think that should go into the toolbelt?
",sigmavirus24,Lukasa
1900,2014-02-03 14:09:54,"Thanks @mjpieters! :cake: 
",sigmavirus24,mjpieters
1900,2014-02-11 16:40:36,"@Lukasa for the record, i only got notified when someone directly mentions me ;)
",kennethreitz,Lukasa
1898,2014-02-02 16:49:39,"The exact line that @Lukasa is referring to is [here](https://github.com/kennethreitz/requests/blob/master/requests/sessions.py#L533). Just out-dent that line and you'll be fine. Please revert the change you've made here before making this change. Also please continue working on this PR and do not open a new one @Zopieux 

Please leave a comment when you've updated the PR.
",sigmavirus24,Lukasa
1896,2014-01-31 14:16:54,"Beyond the security implications that @Lukasa has already outlined, I am 100% opposed to the hard coding of an import statement of a module that is not maintained by one of us. This provides a rich opportunity for someone to make a package on PyPI that subtly hides this and installs a package named `requests_extension`. Let's say I added that to a package I already distribute, then I could, by someone installing an otherwise innocuous package, totally take control of certificate discovery on their installation without their knowledge.

You claim requests is insecure already but this would introduce the largest of backdoors into requests as it exists now.

Thank you for your contribution but we can not accept it in good conscience.

Furthermore, I don't feel there is further need for discussion on this issue.
",sigmavirus24,Lukasa
1896,2014-01-31 14:44:11,"thank you @Lukasa I understand the use case you pointed now,
and agreed it was a bad idea to do it that way.

What about adding built-in operating system flavor instead ?
Most package maintainer of requests for major linux distributions just hardcode the path to the main cert file like `/etc/ssl/certs/ca-certificates.crt` for debian based distributions for instance.

Then `requests` could provide out of the box some of those locations, if we can guess reliably the type of distribution requests is running on.

How does it sounds ?
",ticosax,Lukasa
1892,2014-01-30 20:38:17,"@kennethreitz we absolutely cannot continue reusing authorizations on redirects to sites that are not the same host. With that in mind we almost certainly need to issue a CVE. I'll happily work on that though.

We've been leaking credentials and we need to at least address that. Whether we repopulate the auth after stopping the leak or not is more of a feature decision. I'm sure one of the security experts, like @dstufft would back up @Lukasa and I on that.
",sigmavirus24,kennethreitz
1892,2014-01-30 20:38:17,"@kennethreitz we absolutely cannot continue reusing authorizations on redirects to sites that are not the same host. With that in mind we almost certainly need to issue a CVE. I'll happily work on that though.

We've been leaking credentials and we need to at least address that. Whether we repopulate the auth after stopping the leak or not is more of a feature decision. I'm sure one of the security experts, like @dstufft would back up @Lukasa and I on that.
",sigmavirus24,Lukasa
1892,2014-01-31 07:19:31,"@kennethreitz , call me the outsider but @sigmavirus24 got a point. As the new guy 'round these parts. You have 1.1million downloads. Even if half that still use, we are talking about a **major** security failure in the code base. :no_good: 
",Stephn-R,kennethreitz
1892,2014-01-31 07:19:31,"@kennethreitz , call me the outsider but @sigmavirus24 got a point. As the new guy 'round these parts. You have 1.1million downloads. Even if half that still use, we are talking about a **major** security failure in the code base. :no_good: 
",Stephn-R,sigmavirus24
1892,2014-01-31 07:44:24,"@Lukasa So in that sense, are you suggesting that the session itself may be invalidated and then a restart is required?
",Stephn-R,Lukasa
1892,2014-01-31 16:59:22,"@sigmavirus24 this was an explicit design decision, and it has been stated as such many times before. I've considered implementing a patch much like many times before, and this was a minor concern in the back of my mind when I did ""the big refactor"". As I said, I need to think about it.

Further comments about are neither helpful nor welcome. :)
",kennethreitz,sigmavirus24
1891,2014-02-01 15:28:26,"@kennethreitz I have no strong opinion on this either way frankly. #1890 seemed like a reasonable feature request though. The decision is all yours.
",sigmavirus24,kennethreitz
1891,2014-02-11 17:14:05,"@kennethreitz you assume correctly. And this was a one time idea anyway. It seemed reasonable enough to allow it.
",sigmavirus24,kennethreitz
1885,2014-01-27 19:46:16,"> EDIT: To be clear, we should conditionally delete the Authorization header, only if we're being redirected to a new host.

@Lukasa I agree!
",eriol,Lukasa
1885,2014-01-31 12:38:35,"@sigmavirus24 unfortunately I don't have experience with CVEs, but it's well described here:
http://people.redhat.com/kseifrie/CVE-OpenSource-Request-HOWTO.html

Sending a request to oss-security@lists.openwall.com should be enough,
but maybe security folks at Red Hat made a request for a CVE since they are following the issue too: https://bugzilla.redhat.com/show_bug.cgi?id=1046626

As you can see Endi Sukma Dewata also asked me if there are tests for the Proxy-Authorization case.
I'm going to reply on Red Hat tracker asking if they already sent a request for a CVE.
",eriol,sigmavirus24
1885,2014-09-12 20:03:14,"Good catch @blueyed. This was fixed in [v2.3.0](https://github.com/kennethreitz/requests/releases/tag/v2.3.0).
",sigmavirus24,blueyed
1884,2014-04-02 03:55:28,"@sigmavirus24 As of a few minutes ago, 100% of the unit tests for requests master now pass when run on this branch of Jython, which supports SSL and other goodies: https://bitbucket.org/jimbaker/jython-socket-reboot

Please note that the above branch still needs to be cleaned up (at the very least, remove copious print debugging!), but that's all pretty obvious in the FIXMEs and prints.

I will ping again when we have this merged against Jython trunk, in prep for beta 3 - it would be great for us to have Jython as part of your testing.
",jimbaker,sigmavirus24
1884,2014-09-04 19:18:43,"@kennethreitz sorry?
",sigmavirus24,kennethreitz
1882,2014-08-15 07:41:36,"@Lukasa: if this is not a bug, then what exactly is it? It seems to be that its complaining that a connection was left open after the unit tests ended, but how can that be if I'm wrapping the requests.post() call with contextlib.closing() like the documentation mentions here? http://docs.python-requests.org/en/latest/user/advanced/#body-content-workflow am I doing something incorrect?
",mgrandi,Lukasa
1882,2015-09-03 03:36:27,"@siebenschlaefer That's correct, which is why we recommend you use explicit `Session` objects. =)
",Lukasa,siebenschlaefer
1879,2014-01-25 20:58:44,"> What are docker-py doing to route over the unix domain socket? At the very 
> least they'll have to be mounting a Transport adapter.

I was thinking the same thing, but the Transport adapter doesn't handle 
params. That can only be handled by a pepared request object. I'm not sure 
they could pass on the params they need to tack on with the adapter in this 
case. Unless, you had another idea how to go about that @Lukasa, I think that 
may be a dead end. I agree though that the parameter handling should only be 
for HTTP(S) URLs since those are the only ones we are really positioned or 
likely to support.

docker-py could use URI templates to bypass having to use the `params` 
parameter to requests, but that would likely introduce a new dependency which 
they might not want.
",sigmavirus24,Lukasa
1879,2014-01-26 00:23:51,"So, the issue is that as @sigmavirus24 mentioned, Requests handles parsing the URL about two layers higher than the Transport Adapter (in the PreparedRequest object). Certainly docker-py _can_ work around this by playing about with that object (either monkeypatching it or using the [explicit PreparedRequest flow](http://docs.python-requests.org/en/latest/user/advanced/#prepared-requests)), but the real meat of this issue is whether they should have to.

I remain as I was before, at -0.5. It feels like we'd be doing the wrong thing. I am, however, open to being convinced on this issue.
",Lukasa,sigmavirus24
1879,2014-01-26 00:34:28,"I took a look to see how simple that would be for them to implement but they're code seems quite full of misdirection. That said, the way they're currently doing everything, I find it hard to believe they'll be up for using the explicit flow @Lukasa linked to.
",sigmavirus24,Lukasa
1879,2014-01-27 00:05:23,"> There should be a nice hook to provide custom logic to deal with params.

I think it's fairly obvious that @Lukasa and I disagree with this sentiment. I'm not sure we can say it enough times but requests is an HTTP library. While docker-py is using it to perform HTTP over a unix socket, not every client using it on a Unix socket will be doing HTTP necessarily. There is no reasoning beyond some extraordinary uses for us to enable a hook or any other simpler means when there are already documented ways around this.

Frankly, it is far from impossible to accomplish but just because we make it possible does not mean we should encourage it by making it simple.

I'm glad you've found one way around it and are working on a pull request to docker-py.

Cheers!
",sigmavirus24,Lukasa
1879,2014-01-27 00:59:28,"Thanks for your help and quick replies.  I should have clarified from the
beginning that I was talking about HTTP over a custom transport.  It never
even occurred to me that requests could be used for non-HTTP.

On Sun, Jan 26, 2014 at 5:05 PM, Ian Cordasco notifications@github.comwrote:

> There should be a nice hook to provide custom logic to deal with params.
> 
> I think it's fairly obvious that @Lukasa https://github.com/Lukasa and
> I disagree with this sentiment. I'm not sure we can say it enough times but
> requests is an HTTP library. While docker-py is using it to perform HTTP
> over a unix socket, not every client using it on a Unix socket will be
> doing HTTP necessarily. There is no reasoning beyond some extraordinary
> uses for us to enable a hook or any other simpler means when there are
> already documented ways around this.
> 
> Frankly, it is far from impossible to accomplish but just because we make
> it possible does not mean we should encourage it by making it simple.
> 
> I'm glad you've found one way around it and are working on a pull request
> to docker-py.
> 
> Cheers!
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/issues/1879#issuecomment-33335146
> .
",ibuildthecloud,Lukasa
1879,2014-01-27 02:41:11,"@ibuildthecloud we love to help when possible. 

> It never even occurred to me that requests could be used for non-HTTP.

As @Lukasa mentioned, he wrote an FTP adapter. User's can implement whatever backend they want with Transport Adapters. They do not have to use urllib3. They can also process a Prepared Request however they like. We cannot facilitate all of their needs even when they're performing HTTP over a different transport.
",sigmavirus24,Lukasa
1877,2014-01-24 06:52:54,"@sigmavirus24 is exactly right.

The `Response.json()` method has caused us a lot of trouble. In almost every situation we reject features that involve manipulating content on the grounds that requests is an HTTP library, not an ""anything-else"" library, and every time we do that `Response.json()` gets pointed out. The fact is, we only implemented that because it's an overwhelmingly common use-case, which parsing multipart data is not.

That said, I encourage you to to whack it in the toolbelt. That would be an excellent place for it.

Thanks for raising this issue!
",Lukasa,sigmavirus24
1875,2014-01-22 14:29:31,":heart: our release manager @Lukasa 
",sigmavirus24,Lukasa
1871,2015-12-14 15:23:37,"@Lukasa If the issue is limited to the stdlib cookie jar, then using a requests.Session across multiple threads should be fine for APIs (without cookies).

Is this correct?
",pior,Lukasa
1871,2015-12-15 08:15:27,"@pepijndevos That should not happen. The connection pool is thread-safe, and when a connection is removed from the pool it is owned entirely by the object that withdrew it. As a result, the connection should not be dropped before use. It is _possible_ that there is a TCP FIN packet in flight when the connection is being handled before use, and as a result the connection is torn down: in that situation, a simple retry is a good idea (and something that should be being done anyway).
",Lukasa,pepijndevos
1869,2014-01-31 07:32:36,"@jschneier I'm not so worried about that particular issue: we've never hit it in Requests. I'm just wondering whether there's a good technical reason we shouldn't just limit ourselves to well-formed HTTP/1.1.
",Lukasa,jschneier
1868,2014-01-18 22:16:33,"@creese is it possible that you could share the URL with us as well so we can see if we can reproduce the behaviour? It would also be very helpful if you could answer @Lukasa's questions. Thanks for helping us help you!
",sigmavirus24,Lukasa
1868,2014-01-18 22:31:26,"@sigmavirus24 There was an issue with the form data in the initial request. It's resolved.
",creese,sigmavirus24
1863,2014-01-13 21:31:36,"Thanks for raising this @kevinburke!

That area is very carefully designed. We aren't reraising exceptions, we're wrapping them. This means we don't want the urllib3 exception to be raised, we want to raise the `requests.exceptions.SSLError`. In principle we could attach the traceback from the previous exception to this one, but that's unfortunately somewhat misleading. I don't really see any particular problem with this traceback, if I'm honest.

Does anyone disagree?
",Lukasa,kevinburke
1863,2014-01-13 22:38:47,"I'm in complete agreement with @Lukasa. FYI @kevinburke there has been a recent push to catch the last few exceptions that we didn't realize were spuriously being raised by urllib3 and bleeding through.

+1 to close this.
",sigmavirus24,kevinburke
1863,2014-01-13 22:38:47,"I'm in complete agreement with @Lukasa. FYI @kevinburke there has been a recent push to catch the last few exceptions that we didn't realize were spuriously being raised by urllib3 and bleeding through.

+1 to close this.
",sigmavirus24,Lukasa
1862,2014-01-13 13:12:53,"thanks @Lukasa sorry I didn't notice your weekend work..!
",sybeck2k,Lukasa
1862,2014-01-13 13:18:16,"@sybeck2k It's really not a problem at all. =) Ego is a dangerous thing in open source so I try to check mine; and besides, you did the very generous thing of merging my changes with yours, so I think you handled this very well. Thankyou. =)
",Lukasa,sybeck2k
1861,2014-01-13 09:00:08,"@Lukasa 

> My only worry is that technically this changes what our multipart requests look like on the wire

I don't think it changed anything. A proper guess of file content type shouldn't break anything (I hope).

ping @sigmavirus24 for a code review.
",lepture,Lukasa
1861,2014-01-13 10:46:17,"@Lukasa Yes, you are definitely right. But it has a chance to do such thing:



I think we are here to discuss which is the best default way to handle file uploading.
",lepture,Lukasa
1861,2014-01-13 12:47:59,"@lepture are you suggesting that in order to achieve the old behaviour (that probably most of our users rely on) they'll have to pass a tuple of `(filename, data, None)`? To me that is entirely backwards incompatible behaviour and would indicate this change would need to wait until 3.0.0.

I share @Lukasa's concern that while this is technically valid and should not break anything that it might. I'm trying to imagine a file type for which `mimetools` could return the wrong `Content-Type` header. All I can imagine is something entirely proprietary that isn't necessary publicly available and that a corporate client is posting to a service they own. It's entirely plausible for them to not use a `Content-Type` header in that case because their server will know how to handle it. In that case, since they control both ends, I can't imagine this would necessarily break anything but it would certainly cause them a headache.

That aside, my larger sticking point is that this is a backwards incompatible change. To achieve the same behaviour as before, users have to pass extra parameters that they didn't before. That's not good.
",sigmavirus24,Lukasa
1861,2014-01-13 14:44:05,"@sigmavirus24 I am not so sure now. I think it would be better for a smart guessing of content type. Because when I first use this lib for posting files, I thought it should handle smarter.

I had a look at the code. And I found that urllib3 handles smart, but requests not. I thought maybe requests just missed it. I didn't expect that you intended to handle it this way.

However, if you find this patch is meaningless, just ignore it.
",lepture,sigmavirus24
1861,2014-01-14 03:30:55,"@lepture like I said, it _should not_ break anything and I'm sure it will improve some user's experience with requests. The issue is that it is not backwards compatible because to keep the same behaviour a user has to pass extra parameters. The key part is that I **never** said this pull request was ""meaningless"". I think @Lukasa and I both agree that it is an improvement, but it is an improvement we cannot make until we start considering a 3.0.0 release which seems to me to be very far off.
",sigmavirus24,Lukasa
1861,2014-01-14 22:10:10,"@kennethreitz As accurate as Python's stdlib mimetype guessing. http://docs.python.org/2/library/mimetypes.html#mimetypes.guess_type
",shazow,kennethreitz
1861,2014-01-14 22:13:23,"@Lukasa Time for another Python core patch? :P
",shazow,Lukasa
1861,2014-01-15 09:46:13,"@Lukasa That's weird. I never knew it would be a disaster on Windows.
",lepture,Lukasa
1861,2014-01-16 03:10:37,"I'm with @Lukasa until we can drop 2.6, let's leave this feature out. A point of inquiry though - hasn't 2.6 seen the last of it's bug/security releases? In other words, I think 2.6 has reached its end of life. We could start planning 3.0 to abandon 2.6 and introduce this feature.
",sigmavirus24,Lukasa
1861,2014-01-16 07:20:13,"@sigmavirus24 Is python 2.7 the default python on every linux distribution now? If so, I think 2.6 has reached its end of life.
",lepture,sigmavirus24
1860,2014-01-12 20:34:24,"That's a good point @Lukasa. I'm guessing @gazpachoking might have a good idea. I'm just too tired to think this hard right now =P
",sigmavirus24,Lukasa
1860,2014-01-23 23:28:02,"Sorry I never got around to that. @Lukasa how about you?
",sigmavirus24,Lukasa
1860,2014-01-30 12:45:39,"@Lukasa how should we make sure this doesn't mess anything up?
",sigmavirus24,Lukasa
1860,2015-12-16 15:32:50,"Ok, I'm reopening this because I've finally validated that the stdlib does this. That suggests that it's the right thing to do. I'll have to do the merge manually, but I'd like to have this.

@sigmavirus24, one question: could we meaningfully add this to a 2.9.1, or should it wait for 2.10.0?
",Lukasa,sigmavirus24
1859,2014-01-12 20:07:38,"Why the hell is the relevant function [here](http://docs.python.org/2/library/calendar.html#calendar.timegm)?



@sigmavirus24 Can you confirm that works OK for you too?
",Lukasa,sigmavirus24
1858,2014-01-12 14:26:45,"And the other place it's used is here: https://github.com/kennethreitz/requests/blob/ac4e05874a1a983ca126185a0e4d4e74915f792e/requests/models.py#L456 and notice that there's an optional param to that method that is never used if it is ever passed. We should either use it or remove it (not necessarily in this PR though).

Finally, given that the call to `HTTPAdapter#proxy_headers` is wrapped inside an `if proxy:` block, the param it sends to `get_auth_from_url` should never be `None` or `''`. And `prepare_auth` on the `PreparedRequest` is called after `prepare_url` which should blow up if `url` is not a valid `url`. I think we're safe making `get_auth_from_url` a bit less paranoid. As with all of my reviews though, this is totally up to the discretion of @kennethreitz and @Lukasa 
",sigmavirus24,Lukasa
1858,2014-01-12 14:43:30,"Thanks for that @sigmavirus24, that's a really helpful set of information! I'll update this PR to reflect it.
",Lukasa,sigmavirus24
1858,2014-01-13 12:42:17,"@Lukasa @sybeck2k found a problem with this test: https://github.com/kennethreitz/requests/pull/1862/files#diff-56c2d754173a4a158ce8f445834c8fe8R705 can you fix it here too?
",sigmavirus24,Lukasa
1858,2014-01-13 13:08:44,"@sigmavirus24 @Lukasa sorry I didn't notice the issue was tracked also here. I've tried to merge all the changes of Lukasa into my pull request - the main difference is about the test `test_get_auth_from_url_percent_chars` that on my opinion was flawed as the input url was not url-encoded.
",sybeck2k,Lukasa
1858,2014-01-13 13:08:44,"@sigmavirus24 @Lukasa sorry I didn't notice the issue was tracked also here. I've tried to merge all the changes of Lukasa into my pull request - the main difference is about the test `test_get_auth_from_url_percent_chars` that on my opinion was flawed as the input url was not url-encoded.
",sybeck2k,sigmavirus24
1857,2014-01-10 18:54:02,"Fine then. So we just need to work out how to do it. It's clear that @t-8ch has some plans to investigate nginx, so I'm open to doing that for now.
",Lukasa,t-8ch
1857,2014-01-10 18:57:43,"+1 on disabling TLS compression.

On Fri, Jan 10, 2014 at 10:54 AM, Cory Benfield notifications@github.comwrote:

> Fine then. So we just need to work out how to do it. It's clear that
> @t-8ch https://github.com/t-8ch has some plans to investigate nginx, so
> I'm open to doing that for now.
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/issues/1857#issuecomment-32054215
> .

## 

""I disapprove of what you say, but I will defend to the death your right to
say it."" -- Evelyn Beatrice Hall (summarizing Voltaire)
""The people's good is the highest law."" -- Cicero
GPG Key fingerprint: 125F 5C67 DFE9 4084
",alex,t-8ch
1857,2014-01-10 22:39:13,"@jmhodges This is the plan.
@alex I guess your are running it with openssl > 1.0.0. If yes, the last point of my initial list applies.
",t-8ch,alex
1856,2014-01-11 09:43:26,"@Lukasa I think the unquoted `#` is not valid at this position:

[RFC 3986](https://tools.ietf.org/html/rfc3986)



While `#` is in the `gen-delims` group. Encoding it as `%23` makes both parse functions work for me.
",t-8ch,Lukasa
1856,2014-01-11 18:16:40,"@Lukasa Could make urllib3's parser more forgiving if that's what you really want, or just encode things properly. :)
",shazow,Lukasa
1856,2014-01-11 18:23:14,"@shazow something something if you change your parsers to accept bad URLs you're going to have a bad time something something. =P
",sigmavirus24,shazow
1856,2014-01-12 09:40:58,"Nah @shazow, you're fine. We'll always have encoded the URL at the point we call `get_auth_from_url`, so we should never have a hash that doesn't represent the fragment. All is well. =)
",Lukasa,shazow
1855,2014-01-10 13:40:16,"@Lukasa I agree wholeheartedly with everything you said. Your reservations (regarding data loss), however, make me wonder if we shouldn't be attaching the response object to the exceptions we throw. That's a different discussion which should take place on a different issue but I thought I might raise that idea before I forget it.

On the topic of this issue, I can understand that people may want this and that there are corner cases where the Content Length is not exactly a match for the response body (and in those cases it is okay). I can especially see the benefit where users use requests to perform tests on their servers and web apps. With that in mind, would something similar to `raise_for_status` be a reasonable compromise? We won't break backwards compatibility for existing users who do not realize their depending on our decision to not be strict and we give these extraordinary users (that I'm probably completely imagining) a way to reasonably achieve their goal.

One other alternative is to provide this functionality via the toolbelt. This way users have confidence in the implementation and they just need to import one extra thing.
",sigmavirus24,Lukasa
1853,2014-01-09 17:50:18,"@t-8ch Brilliant, if urllib3 disables it altogether that'll make me very happy, as we won't have to worry about the API. =) Give me a shout on that issue if you think I can help.
",Lukasa,t-8ch
1853,2014-01-09 19:25:47,"@t-8ch sorry my answer was a bit short, we could indeed disable SSL comprssion in swift (and openstack in general) but that's not how people usally runs it they would have usually things like (since this is how we advise to run it) pound/nginx doing that for us. 
",chmouel,t-8ch
1851,2014-01-08 20:40:36,"So is the general idea that I'd subclass `HTTPAdapter`, and do something approximately like this?



(And in response to @sigmavirus24 , it's a bit more complicated than just elapsed time, since the profiler is also profiling memory usage and things from `resource.getrusage()`.)
",eklitzke,sigmavirus24
1850,2014-01-07 23:14:19,"Thanks @t-8ch 
",sigmavirus24,t-8ch
1848,2014-02-17 18:33:03,"The primary use-case here is to avoid Kenneth getting spammed with bug reports in his inbox that basically go: ""This website doesn't work for me in Requests"". I'm open to this being a more general service like @shshank suggested, but right now our interest is in getting that crud off Kenneth's plate and onto mine and @sigmavirus24's. =)
",Lukasa,sigmavirus24
1847,2014-01-07 20:03:09,"So, @Lukasa what's causing this? 
",kennethreitz,Lukasa
1847,2014-01-07 20:07:23,"@alex In principle I agree, but currently Requests just uses whatever the Python version has as the default in the `ssl` module.

@kennethreitz We _are_ autonegotiating (I'm pretty sure), but the remote server refuses the initial version we try.
",Lukasa,alex
1847,2014-01-07 20:07:23,"@alex In principle I agree, but currently Requests just uses whatever the Python version has as the default in the `ssl` module.

@kennethreitz We _are_ autonegotiating (I'm pretty sure), but the remote server refuses the initial version we try.
",Lukasa,kennethreitz
1847,2014-01-07 20:55:30,"We're really in between a rock and a hard place. Is our current solution of pointing people to [here](https://lukasa.co.uk/2013/01/Choosing_SSL_Version_In_Requests/) really unacceptable?

@kennethreitz I think it's harder than that, I just realised that my Python 3 is using a more recent OpenSSL:




",Lukasa,kennethreitz
1847,2015-07-17 22:49:29,"Hi @Lukasa ,

I am trying to use python requests to send a requests to a gunicorn server that accepts tls_v1 protocol which means that the client will use tls_v1 only. I create an adapter like:
class TLSAdapter(HTTPAdapter):
  def init_poolmanager(self, connections, maxsize, block=False):
    self.poolmanager = PoolManager(num_pools=connections,
                                   maxsize=maxsize,
                                   block=block,
                                   ssl_version=ssl.PROTOCOL_TLSv1)

And use it like:
requests_session = requests.Session()
requests_session.mount('https://', TLSAdapter())

Now when I try to send a request to my server like:
resp = requests_session.put(request.full_uri, data=request.data, headers=request_headers, verify=verify, cert=(cert, key))

Then I get an error like:
requests.exceptions.ConnectionError: ('Connection aborted.', BadStatusLine('',))

Could you please let me know if this issue has been seen before.
I am running server on my mac and sending the request from the same mac.

I am using python 2.6.9
",pankit,Lukasa
1846,2014-01-07 16:38:56,"@Lukasa I took a first pass at a patch in 0b41cec
",acdha,Lukasa
1844,2014-01-07 09:41:28,"Fix is in #1845. Feel free to take a look, @dstufft.
",Lukasa,dstufft
1844,2014-01-08 19:08:09,"Ok @dstufft, I think all the things you needed are merged (though I can't find the comment where you listed them).
",Lukasa,dstufft
1844,2014-01-09 13:08:04,"@Lukasa Looks good to me. I had the reporter of the bugs test it out and he was able to successfully install stuff without error. So once @kennethreitz cuts a new release (hopefully soon!) pip 1.5.1 can vendor that release and close out those bugs.
",dstufft,Lukasa
1842,2014-01-07 13:18:18,"@dstufft I'd greatly appreciate that. :)
",sigmavirus24,dstufft
1840,2014-01-05 13:21:42,"@sigmavirus24 thanks for editing, didn't know how to do it !

I have tested with 2.1.0 (arch version) and installed it again just to be sure

On my machine:


",dvasseur,sigmavirus24
1839,2014-03-21 14:25:38,"@sigmavirus24 and I have had the same experience, which is also the prevailing impression in #salesforce on freenode.  Oracle, Salesforce, and any other big companies have more concern over their slipping ship dates than their adherence to rfc and test coverage. This is why, after 20 years of development behind me, I touch data as little as possible. :)
",jsullivanlive,sigmavirus24
1837,2014-01-03 07:30:43,"Looks like @daftshady nailed this one down
",sigmavirus24,daftshady
1835,2014-01-02 17:18:57,"@sigmavirus24 I understand your point, also fine with disabling proxy by setting `trust_env`. Just feeling odd first time I tried to use requests.get(..., proxies=None/{}) to bypass env proxies.
Anyway, it's OK with us, thank you all for reply!
",V-E-O,sigmavirus24
1833,2013-12-28 12:28:19,"@Lukasa, it worked, thanks for your help.
",xjsender,Lukasa
1829,2013-12-26 02:18:41,"@sigmavirus24 Ok, maybe my title confused you, now it is ok.
",douglarek,sigmavirus24
1825,2013-12-23 01:32:08,"This was fixed in version 2.1.0 of requests. 

I, like @Lukasa, see this work properly when tried interactively:


",sigmavirus24,Lukasa
1824,2013-12-20 15:18:27,"@ssbarnea That's interesting, you seem to have contradictory views about these two issues. In the first case, we're continuing in the face of errors, which you deem to be a bug. In the second case, Python aborts in the face of errors, which you deem to be a bug.

Furthermore, if you fixed the second issue (Python does not fail loading netrc if parsing one line fails), Requests will never silently bypass errors in parsing `.netrc` because it will never find out about those errors!

I'm +0 on the idea of no longer catching the `Netrc` exception. We could do it. Some people will find unexpected failures because they've never noticed that they even had anything in `.netrc`, but they'll be able to fix that in time. However, we certainly can't do it until at least 2.2, because we cannot start throwing exceptions where previously we worked just fine.
",Lukasa,ssbarnea
1822,2013-12-20 05:42:41,"@sigmavirus24 , 

I used requests in sublime plugin, if the soap_body in below statement didn't contains any Chinese characters, there will be no exception.

`response = requests.post(self.apex_url, soap_body, verify=False, headers=headers)`
",xjsender,sigmavirus24
1819,2013-12-20 02:40:07,"@ogrishman, @Lukasa explained on #1820 that this is a bug tracker and not a question and answer forum. Further questions you ask here will be closed without answer.
",sigmavirus24,Lukasa
1816,2013-12-19 09:09:23,"LGTM! Thanks so much @daftshady! :cake:
",Lukasa,daftshady
1813,2013-12-18 22:29:27,"@shazow - interesting... is there currently a way to manually use a `RequestField` while using `request.post(...)`? If not, that would surely be something worth my contributing when I get a moment.

CC @Lukasa 
",orokusaki,shazow
1813,2013-12-18 22:29:27,"@shazow - interesting... is there currently a way to manually use a `RequestField` while using `request.post(...)`? If not, that would surely be something worth my contributing when I get a moment.

CC @Lukasa 
",orokusaki,Lukasa
1813,2013-12-18 22:37:48,"@shazow No docs changes on our side. =) There's no plan to officially document the crazy things you can do with multipart encoding because they make us sad. There's plans afoot to move some multipart stuff outside the main library, which will likely become the preferred way of doing anything non-trivial with multipart encoding.
",Lukasa,shazow
1813,2013-12-18 22:48:50,"@shazow - sorry to pester, but I'm not sure I follow. I normally use `post` like this:



Where would manually creating a `RequestField` fit into that equation, simply replacing the list of `files` with a list of `RequestField` instances? If so, it would seem that the header would still be overwritten per https://github.com/kennethreitz/requests/blob/d88cd02fd86c6e74ef8a2d1928db78b8976ce00f/requests/models.py#L141 - also would result in a likely error due to the lines preceding that line, right?

I might simply be confused as to what you mean though.

CC @Lukasa 
",orokusaki,shazow
1813,2013-12-18 22:48:50,"@shazow - sorry to pester, but I'm not sure I follow. I normally use `post` like this:



Where would manually creating a `RequestField` fit into that equation, simply replacing the list of `files` with a list of `RequestField` instances? If so, it would seem that the header would still be overwritten per https://github.com/kennethreitz/requests/blob/d88cd02fd86c6e74ef8a2d1928db78b8976ce00f/requests/models.py#L141 - also would result in a likely error due to the lines preceding that line, right?

I might simply be confused as to what you mean though.

CC @Lukasa 
",orokusaki,Lukasa
1813,2013-12-18 22:56:48,"Ah, so it sounds like maybe a `requests==3.0.0` type of feature, if it were to be added. Thanks for checking on that for me @shazow.

@Lukasa @kennethreitz If more things were delegated to urllib3, it would simplify the requests codebase even more. If either of you have suggestions pertaining a change like this, I'd be more than happy to take a stab at it.
",orokusaki,shazow
1813,2013-12-18 22:56:48,"Ah, so it sounds like maybe a `requests==3.0.0` type of feature, if it were to be added. Thanks for checking on that for me @shazow.

@Lukasa @kennethreitz If more things were delegated to urllib3, it would simplify the requests codebase even more. If either of you have suggestions pertaining a change like this, I'd be more than happy to take a stab at it.
",orokusaki,Lukasa
1812,2013-12-18 16:03:36,"@kennethreitz Any write-ups/blogs you can point to on the reasons?
",semarj,kennethreitz
1812,2014-03-20 11:46:51,"@westurner while I appreciate your concern, your efforts are for naught. Allow me to relay to you what I recently learned, security is not the foremost concern of requests (not @Lukasa's or my opinion). Its API is the first concern. Trying to support your point by making valid points about security will do nothing. Since little is likely to change, I'm unsubscribing from this issue and would encourage others who value their time to do the same.
",sigmavirus24,Lukasa
1812,2014-09-17 19:40:25,"@kennethreitz totally agree with the decisions made here. If anyone has ever tried to ship Python software to be standalone you'll know why it's done like this. 
",whalesalad,kennethreitz
1811,2013-12-18 15:46:43,"@sigmavirus24 The big problem is that distros remove the bundled version anyways, making conflicts when you want to handle exceptions / use urllib3 directly, since requests is using a different version than everything else.
",sontek,sigmavirus24
1808,2013-12-18 07:44:53,"Looks good too! Thanks for fixing it so quickly @sigmavirus24 
",teleyinex,sigmavirus24
1804,2013-12-16 13:12:58,"Thanks @Lukasa - quick and detailed follow up. I'll keep an eye on the httplib bug. 
",ChristopherHackett,Lukasa
1804,2013-12-16 14:02:33,"@Lukasa how can I get some beer to you? Seriously, that was incredibly fast and awesome.
",sigmavirus24,Lukasa
1803,2013-12-16 14:10:21,"Once again @Lukasa is 100% correct. This isn't an issue or a bug in requests, just in your expectations of how it behaves.
",sigmavirus24,Lukasa
1803,2013-12-19 08:24:10,"So, I chatted briefly via email with @sigmavirus24 about this. Actually changing the library so that timeouts apply to streaming data is a trivial change with a negative diff. The problem is that it's a backwards incompatible change.

So I'm in favour of fixing this, but not right now. Maybe as part of 2.2. I'm going to schedule this issue for that release unless the BDFL or @sigmavirus24 complains.
",Lukasa,sigmavirus24
1803,2017-02-08 00:56:17,"@Lukasa afaic, the test that was meant to test that timeout works when stream=True, is not correct, as it is not setting stream=True and the default value is stream=False. See: https://github.com/kennethreitz/requests/blob/master/tests/test_requests.py#L1943",julienaubert,Lukasa
1802,2013-12-16 08:51:58,"Thanks for the pull request @kevinburke!

Unfortunately, I'm -1 on this feature. My problems with this feature are as follows:
1. It's misleading. You say in your original description that ""the simplest debugging information I want is the actual HTTP content that was sent over the wire"". This is almost certainly true, and is exactly what any user of this feature would expect it to give. Unfortunately, that's not what this feature provides. Instead, it provides Requests' view of the response, which is not the same thing. Mostly this is stuff that's probably trivial: all the header keys are lower case, their order isn't preserved, etc. However, the fact of the matter is that we're not showing the actual response, we're reconstructing something that matches our view of the response. This means that there's nothing in the textual output you've provided that isn't in the `Response` object already. Any unusual stuff in the HTTP response won't be shown.
   
   This gets even worse if you consider extending this to serialize HTTP requests (which is almost certainly the next step here, being infinitely more useful in debugging weird failures), since Requests cannot possibly reconstruct the HTTP request: we just don't have all the information.
2. It extends the API. We're fighting very hard to avoid extending the API if at all possible: as a result, any extension to the API automatically starts in negative territory, and must provide a very strong justification. =)

Sorry about this: thanks for doing the work, but I don't think we'll merge it. My advice to people who need this debugging functionality is either to get familiar with Wireshark or to consider using a service like [Runscope](https://www.runscope.com/).
",Lukasa,kevinburke
1802,2013-12-16 14:08:23,"I agree with 100% of what @Lukasa said. In fact, even curl would give you far more information to debug with. _Furthermore_, there are headers that we compute that we don't even set on the `PreparedRequest` object so adding this method to that would be useless too quite frankly. Unfortunately your best bet is something like `mitmproxy` or `wireshark` (locally) or a service like `Runscope` or `httpbin`.

With that said, the both of us are in agreement and I'm certain @kennethreitz will agree with both of us, so I'm going to close this.
",sigmavirus24,Lukasa
1801,2013-12-16 08:56:58,"In principle I'm +0.5 on this. It's a nice extension. However, as memory serves, @kennethreitz considered this approach and ended up not taking it. It might be that now that someone has written the code he'll be happy with it, but equally, it might be that he doesn't like the approach.

@sigmavirus24 Yeah, `TimeoutSauce` is used for the urllib3 `Timeout` object, because we have our own `Timeout` object (an exception).
",Lukasa,sigmavirus24
1801,2013-12-16 14:05:32,"@Lukasa As I understood it @kennethreitz was more concerned with the addition (and requirement) of the `Timeout` class from urllib3. And thanks for clearing up the naming, I still think there has to be a better name for it. (I'm shaving a yak, I know)
",sigmavirus24,Lukasa
1801,2013-12-16 14:49:50,"@sigmavirus24 That was my understanding from IRC as well.
",kevinburke,sigmavirus24
1801,2013-12-16 16:08:14,"@kevinburke I discussed it with you on IRC so the likelihood is that you came to that conclusion through me.
",sigmavirus24,kevinburke
1801,2014-01-06 21:04:58,"Hmm. I'd rather not do a `Timeout` class, I'd prefer the optional tuple I think. But hold off until @kennethreitz gets another chance to look at this.
",Lukasa,kennethreitz
1801,2014-04-05 14:40:08,"That's weird, it works fine on Python 2.7. Seems like a Python 3 bug, because I can reproduce your problem in 3.4. @kevinburke, are you aware of any timeout bugs in urllib3?
",Lukasa,kevinburke
1801,2014-05-10 16:05:57,"@kevinburke That is kind of what I do now, I was just wondering if this would make sense as a default approach for _requests_ as well. I personally don't have a need to specify individual timeouts, but that assumption may be too naïve.
",p2,kevinburke
1801,2014-05-11 08:05:32,"Ok, a lot of conversation happened here, let me deal with it in turn.

> Anyway, I just checked Firefox and it is trying exactly one IPv6 and exactly one IPv4 address. I believe, that multiple DNS records are mostly used for load balancing, not fault-tolerance, so attempting only the first address by default makes most sense.

You checked Firefox against actual google.com then, not on an incorrect port. [Browsers will also fall back to the other addresses if the first doesn't respond](http://webmasters.stackexchange.com/a/12704). This makes sense. Having multiple A records means ""this host is available at these addresses"". If I can't contact that host at one of those addresses, it's nonsensical to say ""welp, clearly the host is down"" when I know several other addresses I might be able to contact them at.

This feature of 'multiple addresses' is widely used for both balancing load _and_ fault tolerance. In fact, if you _really_ want to balance load then DNS SRV is the mechanism to use, not A/AAAA, as it provides better control over how the load is spread.

> Is there need to be able to specify both timeouts independently? When I specify the timeout, I'm thinking of it as ""I don't want this line of code to run longer than x seconds"", I don't care which part of the connection takes how long.

The short answer is 'yes', because of the `stream` keyword argument. If you've set `stream=True` and use `iter_content()` or `iter_lines()`, it's useful to be able to set a timeout for how long those calls can block.

> It seems to me this would be a true ""human"" interpretation and could be implemented without having to rely on urllib3 by an internal timer that kills the request if it hasn't returned within the timeout.

As @kevinburke points out, this isn't as easy as it seems. More importantly, it also leaves us exposed to implementation details. 'Until the request returns' is not a well-defined notion. What does it mean for the request to return? Do I have to download the whole body? Just the headers? Just the status line? Whatever we choose is going to be utterly arbitrary.

> Also, as a human when I'm telling a line of code “go, try to connect to Google with this timeout” I'm not thinking about multiple DNS A-records. I'm thinking of Google as a single entity. 

Agreed.

> 1. Do not attempt multiple IPs. If some library code does this, I consider this code broken. If some OS does this, I consider the OS poor.

Woah, now you go off the rails. If you are thinking of Google as a single entity, then you would expect us to connect to it if it's up. If one time in seven we fail to connect, even though you always connect fine in your browser, you're going to assume requests is bugged as hell.

If a host is up, we must be able to connect you to it.

---

The ideal fix, from my position, would be to take over the logic used in `socket.create_connection()`. This allows us to have fine-grained control over timeouts. Unfortunately, it also complicates the `timeout` logic further, as you'd now have per-host connection attempt timeouts, total connection attempt timeout, and read timeout. That's beginning to become hugely complicated and to expose people to the complexities of TCP and IP in a way that I'm not delighted about.
",Lukasa,kevinburke
1801,2014-08-22 13:06:36,"@kevinburke can you rebase?
",kennethreitz,kevinburke
1799,2013-12-14 20:49:28,"You're right @sigmavirus24, that's not what I want. I didn't know warnings worked like that, thanks for teaching me something new!

How about doing `log.warning()` instead of `warnings.warn()`? Here's how it would look on the REPL:


",martinblech,sigmavirus24
1798,2013-12-14 18:44:04,"Everything @Lukasa said has been correct and as far as I'm concerned this is not a valid issue. Your concerns are duly noted and appreciated. Don't hesitate to open further issues if you think you've found other bugs.
",sigmavirus24,Lukasa
1798,2013-12-15 17:54:30,"@Lukasa @sigmavirus24 the method `prepare_url` from  `PreparedRequest` [already checks](https://github.com/kennethreitz/requests/blob/master/requests/models.py#L375) if the given url contains previous params. I don't understand why `PreparedRequest` shouldn't have a params property as they are serializable into a HTTP req when the method `prepare` is called.
",juanriaza,Lukasa
1798,2013-12-15 17:54:30,"@Lukasa @sigmavirus24 the method `prepare_url` from  `PreparedRequest` [already checks](https://github.com/kennethreitz/requests/blob/master/requests/models.py#L375) if the given url contains previous params. I don't understand why `PreparedRequest` shouldn't have a params property as they are serializable into a HTTP req when the method `prepare` is called.
",juanriaza,sigmavirus24
1797,2013-12-14 16:05:48,"As @daftshady points out, please check the issues tracker before opening a new issue.
",Lukasa,daftshady
1797,2013-12-14 16:55:16,"Thanks for opening this @joepie91 and thank you @daftshady for taking the time to find the already opened issue.
",sigmavirus24,daftshady
1796,2013-12-14 19:14:34,"@Lukasa I understand and I love Requests' simplicity. Still, I think it's not safe for Requests to rely so heavily on charade as default and hide it completely from the user.

Here's some examples of what happens to common German, Spanish, French and Portuguese words after being ""charadized"":


",martinblech,Lukasa
1795,2013-12-13 23:56:02,"@Lukasa Absolutely agree: I wouldn't expect Requests to parse the XML declaration to read the encoding, that completely out of scope for a HTTP library.

I wonder, though, what made Requests autodetect a Thai encoding for an XML document with some Spanish content. It's latin alphabet after all.

If the autodetection feature is not reliable enough, maybe the documentation should discourage the users from using `response.text` at all and use `response.content` instead. Or made an opt-in feature instead of default?
",martinblech,Lukasa
1795,2013-12-15 15:09:24,"@Lukasa Got it. It wasn't obvious to me but it's [right there](http://requests.readthedocs.org/en/latest/api/?highlight=text#requests.Response.text) in the docs: ""If Response.encoding is None, encoding will be guessed using charade."" My bad.

My point is, I'm not sure if falling back to `charade`/`chardet` solves more pain than it creates. It didn't for me, but maybe I just hit an unusual number of edge cases that most users of Requests won't. It's hard to estimate how well my experience extrapolates to the average Requests user.

I would love to see a shorter version of this part of your answer in the docs for `Response.text`, just in case:

> […] users who are in a position to know better should take a more active role in choosing text encodings. For instance, on HTML/XML pages, people should look for an encoding directive in the content.
",martinblech,Lukasa
1794,2013-12-13 12:32:44,"My fix initializes those attributes ahead of time in the event that they ever get added to the `__attrs__` list. It seems the most fool-proof way at the moment and far less complex than first checking if it has an attribute. (I don't quite like `hasattr` or how frequently we use it.)

The only issue with both of our pull requests is that the original issue could still come back (sort of) if the for-loop ever initializes `proxy_manager` to `None`. I'm going to pull @erikcw's commit into my branch and work from there.
",sigmavirus24,erikcw
1794,2013-12-13 20:01:56,"In retrospect, I tend to agree with @sigmavirus24 approach of initializing the attributes instead of using `hasattr` in this case.  Since it's before the loop, there are no concerns about clobbering pickled values with an empty dictionary -- while future-proofing the ability to move it into `__attrs__`.

My first approach to solving this was to simply add `proxy_manager` to `__attrs__` -- but that won't work because of the lambdas in `proxy_manager`.  My recommendation would be to initialize the attributes before the loop as suggested by @sigmavirus24 , and to incorporate my code comments to save future contributors from stumbling down the `__attrs__` approach.
",erikcw,sigmavirus24
1794,2013-12-14 04:29:58,"I cherry-picked your commits into my PR. Thanks for your work here @erikcw ! Keep the PRs coming!
",sigmavirus24,erikcw
1790,2013-12-12 06:57:48,"Hey, thanks for thi @kracekumar!

I don't think this is a good idea, however. The key reason is that auto-converting outgoing data is upsettingly magic. As much as possible, what you put in one parameter to the request function (e.g. `headers`) should not affect what happens to something that came in on another parameter (e.g. `data`, or `auth`).

It's also not quite the same as what we do on `Response`s. For a `Response`, we don't automatically convert to JSON unless explicitly asked to by the user: that is, they have to actually call `Response.json()`. That fits the Zen of Python (""Explicit is better than implicit""). For that reason as well, I'd rather that we stick to the current behaviour.

Let's leave this open to see if @sigmavirus24 agrees.
",Lukasa,kracekumar
1790,2013-12-12 12:32:24,"This has been proposed countless times before and every time it has been shot down. We analyze your request to make sure that it appears as correct as we can possibly ensure without restricting what you can actually send.

As @Lukasa said, headers have currently no effect on any other parameter we use and they shouldn't have any effect here. `data`+`files` has an effect because it's the simplest API to making a `multipart/form-data` request especially since sending a file is safest when using that multipart type. Beyond the fact that this is a bad design for an API (it is entirely unintuitive), we would then have people insisting that because we accepted this then we should also accept other `Content-Type` headers and parse the other parameters based upon that. For example, someone might insist that we accept a header like `multipart/form-data; boundary=xyz` and dictionaries for `data` and `files` and then parse out the boundary and create a multipart/form-data` request. That is insanity.

Someone else might want us to turn his dictionaries into XML for him... 

You can see exactly where this rabbit hole goes, just like https://github.com/kennethreitz/requests/pull/1779 led to https://github.com/kennethreitz/requests/pull/1785

Finally, you seem to already understand it, but let me stress:



Will raise an exception as well it should. In both your example, and this one though, calling `r.content` **or** `r.text` will get you the body of the response.

I'm :-1: \* 10 on this (regardless of how convenient it would make developing things like [github3.py](https://github.com/sigmavirus24/github3.py)).
",sigmavirus24,Lukasa
1790,2013-12-12 13:18:18,"Alright then, let's close this. Thanks for the suggestion @kracekumar and please keep them coming! We always say 'no' to more things than we say 'yes' to, but we can't say 'yes' to things people never ask us to do. You're doing good work! Keep it up.

:cake:
",Lukasa,kracekumar
1789,2013-12-11 16:30:35,"Yeah, @t-8ch seems to have the right analysis. In CPython, it takes 29milliseconds per call to `requests.get()`, of which 22ms is `getaddrinfo()`. In PyPy, it takes 47 ms per call to `requests.get()`, of which 35ms is `getaddrinfo()` That accounts for 13 ms of the 18ms difference. I imagine the remaining portion of that time difference is probably because the profiler doesn't play well with PyPy (I seem to recall that being a problem, though @alex will surely be able to correct me if it isn't).
",Lukasa,t-8ch
1787,2013-12-18 17:23:54,"@ThiefMaster , Hi, can you still reproduce this using the requests package compiled from the sources on here?
I burrowed down into the urllib3 and it seems i can't reproduce this issue.
Can you provide me with the server code or give me some hints of what it is or description something to help me diagnose the problem better?
Thanks.
",adaschevici,ThiefMaster
1787,2013-12-19 02:59:20,"@adaschevici I've been experiencing this bug sporadically too. It's been occurring under similar circumstances to @ThiefMaster. I'm just running a script which makes a bunch of GET requests on 3rd party servers. About 1 of every 100 requests which time out throw the bad exception.

I've been attempting to reproduce it consistently, but haven't had any luck.

edit: This seems to be the exact same issue as: kennethreitz/requests/issues/1236. I'm also using 2.1.0
",adamsc,ThiefMaster
1787,2013-12-19 05:57:52,"@ThiefMaster , Thanks. I will try and reproduce this. Regarding the description provided it seems that it will occur on server overload.
When i tried to overload it in ubuntu what i got was a pipe error. I've been trying it on windows but unfortunately the tests are failing before this one.
We might be looking into a platform dependent test?
Or maybe a patch test to provide the coverage?

CrossRef: shazow/urllib3#297
",adaschevici,ThiefMaster
1787,2013-12-19 08:48:51,"@shazow Pfft, as a socket-level test that's easy =D. I'll take a swing at it at somepoint today if someone doesn't beat me to it.
",Lukasa,shazow
1787,2013-12-19 08:57:35,"@Lukasa , where do you mean to catch the exception
In httpresponse.read(), yes?
I have been working with it and trying to get the exception thrown from there but i've had no successful attempts so far.

Anyway...i will retrace my steps and see where i went wrong maybe i will come up with the proper test.
",adaschevici,Lukasa
1787,2014-03-20 18:21:58,"@Lukasa Sure, here is the full exception (starting at requests code):

  File ""C:\dev\virtualenvs\py27mft\lib\site-packages\requests\sessions.py"", line 395, in get
    return self.request('GET', url, *_kwargs)
  File ""C:\dev\virtualenvs\py27mft\lib\site-packages\requests\sessions.py"", line 383, in request
    resp = self.send(prep, *_send_kwargs)
  File ""C:\dev\virtualenvs\py27mft\lib\site-packages\requests\sessions.py"", line 486, in send
    r = adapter.send(request, **kwargs)
  File ""C:\dev\virtualenvs\py27mft\lib\site-packages\requests\adapters.py"", line 394, in send
    r.content
  File ""C:\dev\virtualenvs\py27mft\lib\site-packages\requests\models.py"", line 679, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File ""C:\dev\virtualenvs\py27mft\lib\site-packages\requests\models.py"", line 616, in generate
    decode_content=True):
  File ""C:\dev\virtualenvs\py27mft\lib\site-packages\requests\packages\urllib3\response.py"", line 236, in stream
    data = self.read(amt=amt, decode_content=decode_content)
  File ""C:\dev\virtualenvs\py27mft\lib\site-packages\requests\packages\urllib3\response.py"", line 183, in read
    data = self._fp.read(amt)
  File ""C:\Python27\Lib\httplib.py"", line 567, in read
    s = self.fp.read(amt)
  File ""C:\Python27\Lib\socket.py"", line 380, in read
    data = self._sock.recv(left)
socket.timeout: timed out

When I merged the fixed response.py (https://raw.githubusercontent.com/adaschevici/urllib3/296-exception-not-properly-wrapped/urllib3/response.py), the exception now changed to (the retry did not occur as I hoped):

  File ""C:\dev\virtualenvs\py27mft\lib\site-packages\requests\sessions.py"", line 395, in get
    return self.request('GET', url, *_kwargs)
  File ""C:\dev\virtualenvs\py27mft\lib\site-packages\requests\sessions.py"", line 383, in request
    resp = self.send(prep, *_send_kwargs)
  File ""C:\dev\virtualenvs\py27mft\lib\site-packages\requests\sessions.py"", line 486, in send
    r = adapter.send(request, **kwargs)
  File ""C:\dev\virtualenvs\py27mft\lib\site-packages\requests\adapters.py"", line 394, in send
    r.content
  File ""C:\dev\virtualenvs\py27mft\lib\site-packages\requests\models.py"", line 679, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File ""C:\dev\virtualenvs\py27mft\lib\site-packages\requests\models.py"", line 616, in generate
    decode_content=True):
  File ""C:\dev\virtualenvs\py27mft\lib\site-packages\requests\packages\urllib3\response.py"", line 245, in stream
    data = self.read(amt=amt, decode_content=decode_content)
  File ""C:\dev\virtualenvs\py27mft\lib\site-packages\requests\packages\urllib3\response.py"", line 223, in read
    ""Remote connection closed. Read timed out."")
urllib3.exceptions.ReadTimeoutError: <requests.packages.urllib3.response.HTTPResponse object at 0x000000000326BDD8>: Remote connection closed. Read timed out.

By the way: this issue is easily reproduced on this particular embedded device--occurs within seconds.  I took a packet trace and I can see a difference between no-issue and issue--but not sure if it is the client (requests) or the server (my embedded device) that is misbehaving.

Thanks!
Darhsan
",darshahlu,Lukasa
1786,2015-12-28 15:41:13,"I am also interested by this feature. When using an internal CA, the CRL as a file is the easiest path. Far easier than using an OCSP endpoint and more reliable than embedding an HTTP URL into the certificate (no way that the CRL can be unavailable due to a temporary outage).

@Lukasa I don't understand when you say this can be done outside requests. There is no way to preprocess the certfile with the CRL. The certfile is usually a root certificate and the CRL contains the serial numbers of revoked certificates. There is no way to apply a CRL to a root certificate.

With curl, you can specify it with `--crlfile`.
",vincentbernat,Lukasa
1786,2015-12-28 15:58:06,"@vincentbernat Yeah, that's a fair point.

We're aiming to add support for providing an external SSLContext object, which should make this possible, but until that's done I don't think we can do much else.
",Lukasa,vincentbernat
1781,2013-12-06 17:02:16,"@kracekumar I know. =) The only reason I did this was because Kenneth got eager to push out a release last night, and I wanted to make sure we didn't slow him down. =) Your PR was a good one, thanks for providing it!
",Lukasa,kracekumar
1781,2013-12-06 17:13:05,"@kracekumar it's no big deal at all :)
",kennethreitz,kracekumar
1781,2013-12-06 17:16:26,"@kennethreitz @Lukasa That is great. 
",kracekumar,kennethreitz
1781,2013-12-06 17:16:26,"@kennethreitz @Lukasa That is great. 
",kracekumar,Lukasa
1780,2013-12-12 07:39:16,"@Lukasa I personally make great use of Requests' file uploading options (very helpful when debugging and auditing web apps), and would like to see it remain in core if possible. It's features like this (being able to specify a filename, a string or file object, and a content type in any multipart upload) that make Requests far more versatile than urllib2.

A potential API change could be to allow each file to be either a tuple as it is now, or a dict, where the dict can specify any number of parameters as in: `{""name"": ""filename.xls"", ""content"": open(""filename.xls""), ""content_type"": ""application/vnd.ms-excel""}`. This is more verbose, but removes the need to remember where to position each element in the tuple (though unfortunately might cause the need to remember the filename is ""name"", etc..).
",Anorov,Lukasa
1779,2013-12-05 10:07:46,"@Lukasa I am wondering why you didn't use `0`. 
",kracekumar,Lukasa
1779,2013-12-05 11:32:27,"I'm surprised that you haven't seen +0 before @kracekumar. We use it excessively to vote on topics and it is used in lots of other projects.

+1 := strongly in favor
+0 := neutral but leaning positively toward the change
-0 := neutral but not exactly a fan of the change
-1 := very much against the change.

I'll respond to the PR itself later today
",sigmavirus24,kracekumar
1779,2013-12-05 11:46:31,"@sigmavirus24 I have seen `+0` in mailing lists and other places. Sure. I am glad to fix the bugs pointed out by @Lukasa if needed. Waiting for the comments to improve. 
",kracekumar,Lukasa
1779,2013-12-05 11:46:31,"@sigmavirus24 I have seen `+0` in mailing lists and other places. Sure. I am glad to fix the bugs pointed out by @Lukasa if needed. Waiting for the comments to improve. 
",kracekumar,sigmavirus24
1779,2013-12-05 18:29:19,"I'm not really sure. It's a small change but we would then have to expose it to the user and that's what I'm not exactly a fan of. Certainly they can find it on their own now but that's not the same as exposing it to them which would be encouraging its use. How commonly do requests users set their own User-Agent string and want it to include all of that information? Frankly I'm not convinced it is all that frequently. That said if @kennethreitz wants this I'm perfectly okay with it granted that it handles strings correctly (as @Lukasa already mentioned).
",sigmavirus24,Lukasa
1777,2013-12-12 16:28:01,"@erikcw Any updates on this issue? I'm really curious about how that happens in normal use case.
",daftshady,erikcw
1777,2013-12-13 00:36:15,"@erikcw There already is related PR :)
https://github.com/kennethreitz/requests/pull/1793
",daftshady,erikcw
1775,2013-12-04 03:27:50,"@sigmavirus24 you're more than welcome to work in the main repo in side-branches :)
",kennethreitz,sigmavirus24
1774,2013-12-03 14:08:30,"This issue has been raised many times in the past (please see #1737, #1604, #1589, #1588, #1546. There are others, but this list should be sufficient). The issue @sigmavirus24 is looking for is #1604.

RFC 2616 is very clear here: if no encoding is declared in the Content-Type header, the encoding for text/html is assumed to be ISO-8859-1. If you know better, you are encouraged to either decode `Response.content` yourself or to set `Response.encoding` to the relevant encoding.
",Lukasa,sigmavirus24
1774,2013-12-04 01:40:21,"As usual @Lukasa is 100% correct.
",sigmavirus24,Lukasa
1774,2013-12-04 02:14:36,"@Lukasa thanks for your explanation! I think not every user knows the detail defined in RFC2616, so should you add some comment on `Response.text`?
",weiqiyiji,Lukasa
1772,2013-12-04 01:51:08,"With the exception that the diff is a bit noisy due to @mdbecker alphabetizing portions (imports and one functions params), this is good to merge in my opinion.

For those who don't care to read :+1: :shipit: 
",sigmavirus24,mdbecker
1772,2013-12-06 02:18:50,"Thanks @kennethreitz. Looks like there is a bug here (either in my test or in the code.) The issue is that `time.mktime` returns a time in localtime which I wasn't accounting for in my test. There are 2 ways to solve this problem:
1. Modify `morsel_to_cookie` to subtract `time.timezone` so that the resulting value represents UTC time. This will result in the value of `expires` being the same on all systems regardless of timezone.
2. Modify `TestMorselToCookieExpires.test_expires_valid_str` to subtract `time.timezone` so that the test always passes.

The former solution seems correct to me, and I can't find anything in the code or RFCs to contradict this. Unfortunately this change would definitely impact users of this module so I'd like to hear others opinions on this.

Thanks!
",mdbecker,kennethreitz
1772,2013-12-06 03:22:05,"@mdbecker the PR currently doesn't merge cleanly, could you rebase off of master?
",sigmavirus24,mdbecker
1772,2013-12-06 15:09:55,"@Lukasa re-pushed
",mdbecker,Lukasa
1772,2013-12-06 16:18:20,"Awesome, this is great work! Thanks @mdbecker! :cake:
",Lukasa,mdbecker
1770,2013-12-03 03:43:49,"I second everything @Lukasa said. If you can, please rebase out everything except f9a48e0
",sigmavirus24,Lukasa
1770,2013-12-04 18:58:27,"@Lukasa don't apologize. Those were good catches that I totally missed because I skimmed over them.

I'm :+1: when @Lukasa's comments are addressed.
",sigmavirus24,Lukasa
1770,2013-12-05 22:29:41,"@Lukasa what feedback do you have? Is it still standing?
",kennethreitz,Lukasa
1765,2013-11-29 16:38:30,"@bicycle1885 Sorry, I didn't spot this was a Pull Request! Do you want to add the test I wrote in #1766 to this PR, and I'll close the other one?
",Lukasa,bicycle1885
1765,2013-11-29 16:45:36,"@Lukasa if you rebase your branch off of his, you can give him credit for the fix, while adding the test yourself. You kill two birds with one stone. You both get credit for the work you did.
",sigmavirus24,Lukasa
1765,2013-11-29 16:51:28,":+1: Thanks for taking care of that @Lukasa 
",sigmavirus24,Lukasa
1765,2013-11-29 17:06:05,"Thanks for your attention @sigmavirus24.
And I can save the cost to write a test code thanks to @Lukasa's job!
",bicycle1885,Lukasa
1765,2013-11-29 17:06:05,"Thanks for your attention @sigmavirus24.
And I can save the cost to write a test code thanks to @Lukasa's job!
",bicycle1885,sigmavirus24
1764,2013-11-29 16:59:48,"Looking over the PRs listed above now:
- #1713 :+1: merge it
- #1657 :+1: merge it
- #1729 :-1: do not merge it (I don't fee it is ready yet. @Lukasa and I left feedback that hasn't been addressed)
- #1628 :-1: do not merge it (neither @kennethreitz nor I are very fond of this)
- #1766 :+1: merge it
- #1743 +0 I don't see anything awful about allowing for separate timeouts, it is just the API under question. I've proposed a different way of handling the same feature. I'm not sure this _has_ to be in 2.1 though 
",sigmavirus24,Lukasa
1764,2013-12-05 22:34:05,"@sigmavirus24 d'oh, sorry guys. I got ahead of myself :) Hoping to do this release tonight. 
",kennethreitz,sigmavirus24
1755,2013-11-27 02:29:41,"@akitada please don't close your own issues. We'll close them when we are ready to.
",sigmavirus24,akitada
1755,2013-11-28 02:02:48,"No need to apologize @akitada 
",sigmavirus24,akitada
1744,2013-11-18 15:46:30,"@auworkshare If you're using cookies you got from a successful login, you should really just be using a `Session` to persist your state for you. I'm assuming you're just taking the cookies from the `Response` object, which are in a `CookieJar`. This means that @sigmavirus24 is correct, this is a duplicate of #1711.
",Lukasa,sigmavirus24
1743,2013-11-20 09:03:16,"@kevinburke i'll try to chat with you on IRC about this this week. :)
",kennethreitz,kevinburke
1739,2013-11-15 18:51:45,"i also love you, @pengfei-xue :cake:
",kennethreitz,pengfei-xue
1738,2013-11-15 14:24:14,"@Lukasa i created another pull reuqest, please close this out thanks.
",pengfei-xue,Lukasa
1737,2013-11-14 13:00:00,"@Lukasa you were tricked into saying this:

> Stripping all the functionality from Response.text, as you suggest in your last point, seems silly to me. If we're going that far, we should remove Response.text altogether.

This is clearly the agenda of this issue as you can tell by:

> If you're going to keep the .text property

@itsadok clearly wants the `.text` property to disappear because issues have been filed regarding it in the past.

Let me address one other thing that @Lukasa didn't before I add my opinion.

> Additionally, the documentation should contain a warning not to use it for arbitrary web pages, and perhaps a code sample showing the proper way to do it.
1. charade works fairly well for well established codecs. There are new ones that subsume old ones which it doesn't support yet. Why? There aren't publicly available statistics for those encodings and that's what charade relies on. If you disagree with how something is being detected, why not file a bug report on charade?
2. That code sample is **not** the proper way to do it. Using regular expressions on HTML is insanity and is **never** the correct answer.

With that addressed, let me address one more theme of this issue: Because _some negligible percentage_ of all issues have been filed about _x_, _x_ should be (changed|removed).

One thing to note is that all the issues with numbers lower than 1000 were filed before requests 1.0 which is when the API was finalized. If there were legitimate bugs in this attribute prior to that, I would be far from surprised. Also some of those issues are instead about the choice that chardet/charade made. Those are not bugs in requests or `.text` but instead in the underlying support.

Finally, after the release of 1.0 we had a lot of issues about the change from `json` being a property on a Response object to becoming a method. We didn't remove it or change it back for a good reason. It was a deliberate design decision.

The `.text` attribute is quite crucial to this library, especially to the `json` method, and it will likely never be removed. Can it be improved? Almost certainly. You provided a couple of good suggestions, but the overall tone this issue is meant to convince the reader that it should be removed and that will not happen. Without a reasonable guess at the encoding of the text, we cannot provide the user with the `json` method which also will not go away. Simply, the user is not the sole consumer of `.text`.
",sigmavirus24,Lukasa
1737,2013-11-14 13:20:14,"Sorry about the closing and reopening, that was a mis-click.

@sigmavirus24 I'm sorry if it seems like I have an agenda. I'm honestly just trying to help. I read through the discussions in **all** of the issues I posted. They all really seem to revolve around the same basic confusion, with people expecting Response.text to be an all-encompassing solution where in reality it is not. 

Let me just clarify some misunderstandings in what I wrote.

> > Additionally, the documentation should contain a warning not to use it for arbitrary web pages, and perhaps a code sample showing the proper way to do it.
> > charade works fairly well for well established codecs. There are new ones that subsume old ones which it doesn't support yet. Why? There aren't publicly available statistics for those encodings and that's what charade relies on. If you disagree with how something is being detected, why not file a bug report on charade?

I merely meant that it should be noted that `Response.text` should not be used willy-nilly on arbitrary web page, precisely because it avoids using charade in many places where it can be used.

> That code sample is not the proper way to do it. Using regular expressions on HTML is insanity and is never the correct answer.

The `get_encodings_from_content` function is fully copied from `requests.utils`, with the added comment by me that it doesn't really work in Python 3. In any case the point was that this needs to be clarified, not my specific solution.

> The .text attribute is quite crucial to this library, especially to the json method, and it will likely never be removed. Can it be improved? Almost certainly. You provided a couple of good suggestions, but the overall tone this issue is meant to convince the reader that it should be removed and that will not happen. Without a reasonable guess at the encoding of the text, we cannot provide the user with the json method which also will not go away. Simply, the user is not the sole consumer of .text.

This is a valid point, and perhaps serves to explain the weird nature of .text. Perhaps all that is needed is a note in the documentation.
",itsadok,sigmavirus24
1737,2013-11-14 14:39:45,"> I merely meant that it should be noted that Response.text should not be used willy-nilly on arbitrary web page, precisely because it avoids using charade in many places where it can be used.

Just because something _can_ be used somewhere does not mean it should be. `charade` is slow as a result of its accuracy and painstaking meticulousness. Using it when it _can_ be used as opposed to when it _must_ be used makes the performance difference in the user's eyes.

> The get_encodings_from_content function is fully copied from requests.utils

But we never use it. It is cruft and _should_ be removed. It is the wrong way to do things.

> That means that the cases where automatic charset detection is actually used are pretty rare, and unpredictable for the user. So why keep the charade?

You're assuming everything behaves the same way and that RFCs are followed by servers. They're not. Charade is occasionally used. We keep it because it is essentially part of the API. Response's couldn't have an `apparent_encoding` attribute if we discarded charade. We can break the API if we ever release 3.0 but until then, the API is frozen except for backwards compatible changes. Also, that's an excellent pun.

> It seems like I hit a nerve, which was really not my intention. I'm going to close the issue

I'm going to re-open it. You made valid points as @Lukasa pointed out. It just needs to be clear that this is not any agreement about removing the property.
",sigmavirus24,Lukasa
1737,2013-11-14 14:44:20,"Heh, @sigmavirus24 both have the same reactions. Neither of us thinks this issue should be closed: I reopened it just before he could!

Neither of us is angry, or unhappy about having this feedback. We're both delighted. You just can't tell because of the limitations of textual communication! =D

The reality is that text encoding is hard, everyone is doing the wrong thing from time to time, and we cannot possibly please anyone. For that reason, we do the best we can, and then we expose the `Response.encoding` property for people who don't like the way we do it.

You've identified good issues with `Response.text`, and I plan to fix them (unless someone else does so first). They're not all backward compatible, so we'll need to sit on them for a bit, but they're good.
",Lukasa,sigmavirus24
1736,2013-11-14 14:43:53,"@Lukasa there was a reason I started https://github.com/sigmavirus24/requests-data-schemes. It was a bad name but the intent was to provide a better way of doing complicated multipart uploads (specifically when no files should be sent, which is an **extremely** common use case).
",sigmavirus24,Lukasa
1735,2013-11-15 12:52:32,"Why were you afraid @kennethreitz ?
",sigmavirus24,kennethreitz
1733,2013-11-10 01:59:14,"If you feel you want to add more tests @ionrock then go ahead but I'm happy with this as it is. :+1:
",sigmavirus24,ionrock
1729,2013-12-04 01:48:49,"@Lukasa if I take over the rest of this PR (to address our feedback) would we be comfortable getting it into 2.1? I feel like this addresses a real concern and I think you were in agreement with @gazpachoking at the end of #1728. Correct me if I'm wrong. 

For reference, I'll pull his commits onto a branch of mine, and just fix up the one minor thing and send a new PR and then close this one. Sound good?
",sigmavirus24,gazpachoking
1729,2013-12-04 01:48:49,"@Lukasa if I take over the rest of this PR (to address our feedback) would we be comfortable getting it into 2.1? I feel like this addresses a real concern and I think you were in agreement with @gazpachoking at the end of #1728. Correct me if I'm wrong. 

For reference, I'll pull his commits onto a branch of mine, and just fix up the one minor thing and send a new PR and then close this one. Sound good?
",sigmavirus24,Lukasa
1729,2013-12-04 12:47:15,"@Lukasa done in #1776 
",sigmavirus24,Lukasa
1728,2013-11-07 14:03:55,"@gazpachoking I agree that for that specific example the correct behaviour is clear. But what about this one?



I continue to believe that from where we stand it seems obvious that the answer should be 'it shouldn't be persisted', but I'm convinced that people will get this wrong. I just want to make sure we're all aware of the various subtleties here before we make a call. =)
",Lukasa,gazpachoking
1727,2013-11-10 16:02:04,"@Lukasa frankly, I don't understand the problem well. Proxies have always been your thing and I never attempted to get enough background on them.

So what Betamax does is replace the adapters that the Session object is using and then use a regular HTTPAdapter to make the actual request if necessary. The reason I'm not certain, is because of [`get_connection`](https://github.com/kennethreitz/requests/blob/master/requests/adapters.py#L187..L211] on the `HTTPAdapter`.

Betamax looks at the Prepared Request that comes through and then matches that against previously recorded (if any) requests to find an appropriate response. Because of how requests handles redirects, even redirects are recorded properly. If the logic you're looking for happens outside of the HTTPAdapter (any of the layers above it), Betamax can probably help so long as the info is on the Prepared Request. If it happens underneath the covers of the HTTPAdapter, then the answer is still ""maybe"" because the request may have the data you're looking for, but I'm not certain.
",sigmavirus24,Lukasa
1727,2014-09-12 13:28:08,"Ah, correct, so it is. Thanks @blueyed!
",Lukasa,blueyed
1726,2013-11-05 03:57:25,"@t-8ch how were you not already in the AUTHORS.rst file? This is an unacceptable oversight and I'm glad you remedied it.
",sigmavirus24,t-8ch
1726,2013-11-05 22:01:57,"@sigmavirus24 To be honest, there are only a handful of contributions codewise (directly to requests not updating urllib3) by me. 
It seems you also took your time when adding yourself to AUTHORS.rst. Happens to the best. :smile: 
",t-8ch,sigmavirus24
1724,2013-11-04 12:55:06,"@daftshady please use pytest's capabilities to add a test that should only ever fail on Python 2.x please.
",sigmavirus24,daftshady
1724,2013-11-04 13:36:15,"@sigmavirus24 I moved logic and added test case. Please review this. Thanks.
",daftshady,sigmavirus24
1724,2013-11-05 03:56:43,":cake: Thanks @daftshady 
",sigmavirus24,daftshady
1723,2013-11-04 15:38:38,"Thanks for the quick response!  I just found some relevant history from 6 months ago:

https://github.com/kennethreitz/requests/issues/1252#issuecomment-17875143

At that time, @Lukasa you didn't consider this worth fixing.  Has anything changed?

In my projects I'm still using Python 2.7 but I would like to migrate to Python 3 someday, so I always use unicode unless I actually need bytes.  This did not seem to be one of those cases; since the `method` parameter can be passed as lowercase and `requests` will convert it to the proper format, I assumed if it needed converting from unicode that `requests` would handle that as well.
",hwkns,Lukasa
1717,2013-10-31 18:33:34,"@sigmavirus24 How's that?
",jvantuyl,sigmavirus24
1717,2013-11-15 09:22:08,"@jvantuyl This isn't Kenneth sitting on the fence, this is Kenneth being super busy. =) Requests takes some time to merge pull requests because Kenneth has the final say, and he's got about a million responsibilities. You'll just have to be patient I'm afraid.

(Also, it would help if you rebased to make it easier for Kenneth to merge if he decides to.)
",Lukasa,jvantuyl
1717,2013-11-15 09:32:42,"@Lukasa you make me sound way more impressive than I am. If I ever need a PR person, remind me to hire you :P
",kennethreitz,Lukasa
1717,2013-11-15 09:33:02,"@jvantuyl :sparkles: :beers: :sparkles: 
",kennethreitz,jvantuyl
1717,2013-11-15 09:38:10,"Yay!  @kennethreitz Look me up next time you're in San Francisco.  Beer, on me.
",jvantuyl,kennethreitz
1714,2013-10-30 11:45:21,"I support absolutely everything @Lukasa has said here.
",sigmavirus24,Lukasa
1713,2013-11-05 05:51:10,"@kennethreitz It's easy to do so. But in my opinion, if `CookieJar` updated by another `CookieJar` happens in `cookiejar_from_dict`, it may harm originality(converting `dict` to `CookieJar`) of `cookiejar_from_dict`. (If that happens, method name may should be changed too)
Do you have any good idea about moving into `cookiejar_from_dict`?
What about moving whole logic into another new method like `merge_cookies`?
",daftshady,kennethreitz
1713,2013-11-05 08:25:12,"@Lukasa Is it good if i update that method not to create new instance?
",daftshady,Lukasa
1713,2013-11-05 08:28:54,"@daftshady Yes, I think so. As you've pointed out, the basic `CookieJar` doesn't have a `.update()` method. But I think that's fine, we can either let the `AttributeError` bubble up or rethrow something nicer, e.g. `requests.exceptions.CookieJarError`.
",Lukasa,daftshady
1713,2013-11-05 14:51:16,"@Lukasa I pushed updated commit. 'merge_cookies' method does not create new `CookieJar` anymore.
",daftshady,Lukasa
1713,2013-11-05 16:27:28,"@Lukasa Thanks for reviewing. 
I think build has been failed because 'httpbin' does not be responded in one test :(
",daftshady,Lukasa
1713,2013-11-06 09:50:45,"@Lukasa I tried to unnecessary re-throw because you pointed out that :). Your `right code` is exactly same as my first version of code. I will attach fixed code. thanks.
",daftshady,Lukasa
1713,2013-11-06 19:18:19,"@daftshady can you do a rebase? I'd love to get this merged in today if @Lukasa or @sigmavirus24 +1
",kennethreitz,Lukasa
1713,2013-11-06 19:18:19,"@daftshady can you do a rebase? I'd love to get this merged in today if @Lukasa or @sigmavirus24 +1
",kennethreitz,sigmavirus24
1713,2013-11-06 19:18:19,"@daftshady can you do a rebase? I'd love to get this merged in today if @Lukasa or @sigmavirus24 +1
",kennethreitz,daftshady
1713,2013-11-06 19:36:25,"I'm definitely +1, though I'd like it if @sigmavirus24 could give his input too. =)
",Lukasa,sigmavirus24
1713,2013-11-07 00:02:22,"I'll look at this when I get to a computer but I trust @gazpachoking 's judgment. Also, Chase can you pinpoint which commit started that? I'll have to see if I remember the context for it.
",sigmavirus24,gazpachoking
1713,2013-11-07 03:12:34,"@kennethreitz Sure, I rebased it.
",daftshady,kennethreitz
1713,2013-11-25 19:41:53,"@daftshady can you rebase? It's been a while :)
",kennethreitz,daftshady
1713,2013-11-26 06:41:58,"@kennethreitz yes it's been a while. I rebased it again :)
",daftshady,kennethreitz
1711,2013-10-30 01:56:48,"@sigmavirus24 https://github.com/kennethreitz/requests/pull/1713 works for me.
",abn,sigmavirus24
1710,2013-10-29 14:15:42,"@jvantuyl I actually disagree with that part. Requests is fundamentally a HTTP library. It's nice that Transport Adapters make it possible to use whole different protocols (with `file://` and `ftp://` being the only ones I've seen done), but it's not actually what Requests is intended to do. When we don't need to make it harder we shouldn't, but neither should we make Requests protocol-agnostic.
",Lukasa,jvantuyl
1710,2013-10-29 16:33:47,"@Lukasa Hmmmmm, I'm not suggesting it needs to be entirely protocol agnostic; merely that, if we're going to purport to have pluggable protocols at all, we should compartmentalize them as much as reasonable.

@sigmavirus24 Please describe the smell.  Am I looking for cheese or rotting meat?  Seriously though, the file RFC specifies that empty hosts equate to localhost.  file:///yada/yada/yada is perhaps the most common form of file URL that there is.  Refusing to accept that as possible is refusing the file protocol in its entirety.

How do you propose to allow this work?  Back in Pull Request #1109, I was told that this was ""exactly the type of thing that I was hoping people would make with Connection Adapters"" and that ""just doesn't belong in Requests itself, but another module"" and ""it would be awesome"".  Are we making this impossible now?  That would not be awesome.

Then again, one man's cheese is another man's mold?  Indulge me for a minute.  Sample my cheese and see if you like the smell after tasting the flavor.  As it turns out, this eliminates something phenomenally useful.  Let me explain what I actually _do_ with this code, as it all comes down to two very useful things.

Firstly, I like to test my code.  Tested code is great.  Testing code with requests is hard, because it's network-active.  Essentially, you have to mock it out; which, in turn, makes your tests inaccurate.  When requests changes, my mocks don't, and my tests don't reflect the changes.

To get good coverage, this forced me to set up a mock web-API and run it on a small web server.  It made my code have environmental dependencies for testing, and building the test environment was a pain.  As I developed this mock server, I realized that 95% of the handlers were just returning a file.

I thought to myself, ""Why not just use file:/// into the test directory and work with the files? That makes it self-contained, and only requires a different URL for testing, which I already do!""  That's when I implemented the file:/// URL because, well, it was easy.

Secondly, I love documentation.  The more I started developing my code this way, I started to collect a battery of files.  These files were checked in with my code, contained JSON of the actual requests, and turned out to entirely document the expectations I had for the API I was interacting with.  I could even look in the git history to see when I started using a feature, etc.  It made my code much more accessible to other developers because it was so much ""look here, see what it's doing"".

So, here are two very real benefits in very real code that are made possible by this rather modest change.  I feel that they heavily outweigh any small smell that they may cause (of course, I'm biased).  This is a five-line change limiting HTTP-specific behavior to HTTP-specific protocols in a system that is architected to support additional protocols through plugins in order to support one of the other most widely-used RFC-specified protocols on the planet.  Isn't it a smell if a minor, overzealous sanity check prevents this from being attainable?

If that's too much, can I just remove the checkl?  Or can I move it into the HTTP adapter so it happens later?  I don't think either of those make it more maintainable, thought they at least make things possible.
",jvantuyl,Lukasa
1710,2013-10-29 16:33:47,"@Lukasa Hmmmmm, I'm not suggesting it needs to be entirely protocol agnostic; merely that, if we're going to purport to have pluggable protocols at all, we should compartmentalize them as much as reasonable.

@sigmavirus24 Please describe the smell.  Am I looking for cheese or rotting meat?  Seriously though, the file RFC specifies that empty hosts equate to localhost.  file:///yada/yada/yada is perhaps the most common form of file URL that there is.  Refusing to accept that as possible is refusing the file protocol in its entirety.

How do you propose to allow this work?  Back in Pull Request #1109, I was told that this was ""exactly the type of thing that I was hoping people would make with Connection Adapters"" and that ""just doesn't belong in Requests itself, but another module"" and ""it would be awesome"".  Are we making this impossible now?  That would not be awesome.

Then again, one man's cheese is another man's mold?  Indulge me for a minute.  Sample my cheese and see if you like the smell after tasting the flavor.  As it turns out, this eliminates something phenomenally useful.  Let me explain what I actually _do_ with this code, as it all comes down to two very useful things.

Firstly, I like to test my code.  Tested code is great.  Testing code with requests is hard, because it's network-active.  Essentially, you have to mock it out; which, in turn, makes your tests inaccurate.  When requests changes, my mocks don't, and my tests don't reflect the changes.

To get good coverage, this forced me to set up a mock web-API and run it on a small web server.  It made my code have environmental dependencies for testing, and building the test environment was a pain.  As I developed this mock server, I realized that 95% of the handlers were just returning a file.

I thought to myself, ""Why not just use file:/// into the test directory and work with the files? That makes it self-contained, and only requires a different URL for testing, which I already do!""  That's when I implemented the file:/// URL because, well, it was easy.

Secondly, I love documentation.  The more I started developing my code this way, I started to collect a battery of files.  These files were checked in with my code, contained JSON of the actual requests, and turned out to entirely document the expectations I had for the API I was interacting with.  I could even look in the git history to see when I started using a feature, etc.  It made my code much more accessible to other developers because it was so much ""look here, see what it's doing"".

So, here are two very real benefits in very real code that are made possible by this rather modest change.  I feel that they heavily outweigh any small smell that they may cause (of course, I'm biased).  This is a five-line change limiting HTTP-specific behavior to HTTP-specific protocols in a system that is architected to support additional protocols through plugins in order to support one of the other most widely-used RFC-specified protocols on the planet.  Isn't it a smell if a minor, overzealous sanity check prevents this from being attainable?

If that's too much, can I just remove the checkl?  Or can I move it into the HTTP adapter so it happens later?  I don't think either of those make it more maintainable, thought they at least make things possible.
",jvantuyl,sigmavirus24
1710,2013-10-29 16:40:44,"@jvantuyl In principle, sure. In practice, Requests only knows about HTTP. Everything about the Transport Adapter is totally about HTTP. If we were moving protocol-specific features to the Transport Adapter, we'd just have `RequestsAdapter`. =D

Don't get me wrong. I'm in favour of supporting `file://` URLs, which is why I'm in favour of this change. I'm just trying to make it clear that I'll otherwise draw a fairly strong line in the sand at ""this far but no further"". There is a limit to what Requests should be able to do in non-HTTP protocols.
",Lukasa,jvantuyl
1710,2013-10-30 02:54:52,"I guess what I really mean, is that your goal is achievable without these changes which may be minor but which are likely to be removed in future refactors of the code-base. As @Lukasa explained, we're really primarily concerned with HTTP and while other adapters don't run into this issue, there are probably other ways around this. One thing is that the implementation details of the Adapter are completely opaque to requests and should always be. Certainly `file://` is common among browsers and even curl, but why not do something more clever?

For example, you could just as easily have your adapter receive requests on: `http://localhost:64000/` which I'm pretty sure is safe from conflict. Really, it could match on anything the user wants. If they're wrapping an API (like I am), they can add an adapter for the API they're wrapping, e.g.:



This would allow the user to not need to specify `file://` when making their ""mocked"" requests.

My last comment was not meant to sell you Betamax.

Frankly here are the reasons I'm :-1:
- The code is likely to get lost in some future shuffle. This will break your adapter then and we'll have to rehash this issue.
- There are other ways that don't require this change
- There are other libraries that seem to satisfy your needs and other people's needs
  - Betamax
  - requestions
  - vcrpy
  - and the list goes on
- Historically we as trio have been wont to encourage people to use the `file://` scheme and to support it
",sigmavirus24,Lukasa
1710,2013-10-30 17:19:43,"@kennethreitz That's basically what this change does, makes the PreparedRequest a little less paranoid, so I can subsume this type of handling inside the adapter.  Without this change any PreparedRequest bombs out when it tries to handle the non-native URL.
",jvantuyl,kennethreitz
1710,2013-10-30 17:20:53,"@kennethreitz Creating?  Well, sort of.  When you create a Request, it automatically creates a PreparedResponse and prepares it.  I _could_ create one manually, not run any of the sanity checks, and then use it for this.  Though, that makes something that's otherwise fairly transparent into something that's significantly more complicated.
",jvantuyl,kennethreitz
1709,2013-10-28 17:27:48,"@samuelhug is the the only thing that doesn't work on GAE?
",kennethreitz,samuelhug
1709,2013-10-28 17:36:03,"@kennethreitz I haven't run extensive tests. However, I came across this issue while testing out coto/gae-boilerplate and this fix was the only modification required.
",samuelhug,kennethreitz
1705,2013-10-26 17:20:37,"@t-8ch Thanks for the quick reply. Perhaps there was a typo in your request? Please try : requests.get(""https://register123.blib.us"", verify=False) - you seem to be trying blip.us? register123.blib.us should resolve to  216.230.230.76.
",pikumar,t-8ch
1705,2013-10-26 21:42:23,"@t-8ch Indeed wget/curl/requests all hang on this new box. Looking into it. 
",pikumar,t-8ch
1705,2013-10-27 02:19:10,"@t-8ch This is what is happening on wireshark: http://cloudshark.org/captures/7e86057afd34 (Its a debian Virtualbox VM under Win7 = Box 1). openssl version 1.0.1e. With openssl version 1.0.0 on another box (Box 2), things look fine (A non-existent https+sni domain access gives --> curl: (35) Unknown SSL protocol error in connection to register123.blib.us:443) . For Box 1, this is what a correct url + successful connection looks like : http://cloudshark.org/captures/687634100c81. (Server = https+SNI).

On Box 1, requests/wget/curl all hang with : https://non-existant-domain.blib.us - but this fixes the hang for wget:
wget ----secure-protocol=SSLv3 https://non-existant-domain.blib.us

It might not be a firewall issue? It probably is an openssl/tls issue? It might be my server not handling TLS 1.2 properly - even in that case, the clients should not hang I would think. (Remember, for existing domains, everything seems to be fine (both box 1/2), its only the non-existent domains that hang things)
",pikumar,t-8ch
1705,2013-10-27 08:38:28,"@pikumar I don't see SNI in your requests at all. There should be a `Extension: server_name` in your `Client hello`.
My Debian does this.
The trace from your hanging request looks like mine from the working one.
Where did you take the trace? Could it be the `RST` packet gets dropped after that?
(Like you traced on the Windows System and the Debian box itself does some firewalling)

To ask again: You are not using gevent or anything like that?

You can also force requests to use `SSLv3`.
Like always theres a blog post by @Lukasa for that: http://lukasa.co.uk/2013/01/Choosing_SSL_Version_In_Requests/
",t-8ch,Lukasa
1705,2013-10-27 17:27:32,"@t-8ch http://cloudshark.org/captures/1f95da7bf5a7 - this is a capture from inside the debian box. And you are right: I do not see the RST in there. What baffles me, is that my iptables is completely open. Everything else works as it should, except non-existent domains. 

Your answers:

SNI : Indeed register.blib.us is not SNIed, but any existing name is: for example https://rrc.imp.blib.us should be.

I am not using gevent, unless requests automatically picks it up when ndg-.., pyasn1,... is installed. I did install the dependencies in urllib3 that helps with SNI. 
",pikumar,t-8ch
1705,2013-10-27 18:22:36,"@pikumar requests doesn't automatically pick anything up. @t-8ch was only asking because you hadn't answered earlier and wanted to make sure this wasn't one of the few issues we're starting to see with requests + gevent.
",sigmavirus24,t-8ch
1704,2013-10-26 06:57:31,"Once again @sigmavirus24 is right, that's almost certainly the problem.
",Lukasa,sigmavirus24
1704,2013-10-30 21:28:20,"Hi all, I apologize for keeping everyone waiting, and appreciate all the quick and thoughtful responses.

Indeed, @sigmavirus24 was correct--our service was performing a redirect, and requests handles those differently in v0.13.3 and v2.0.1. Making the requests with allow_redirects=False yielded a 301: moved permanently, so that was eye-opening. It turns out that in these tests we were using an old version of our api, so the redirect is, as I understand, the solution we implemented for the sake of backwards compatibility with older versions of the apis. The older version of requests will allow a redirect when it receives a 301 as long as a) allow_redirects is True (which it is by default--in both versions) and b) the method was not a POST [v 0.13.3, requests/modules.py, line 260]. In the new version, the same is true with the allow_redirects boolean but the method _must_ be either a GET or a HEAD--and of course, delete is neither of those [v 2.0.2, requests/sessions.py, line 116]. Thus, exactly as @sigmavirus24 thought, DELETE gets changed to GET.

I actually finally managed to sit down with the last guy that worked on apis and he postulated that I should be able to avoid a redirect by explicitly specifying the most recent version of our apis in the tests, so that's the next step for me. But, if I may ask, why the change? Is the method change to GET a safety measure that we just so happen to not need because we've successfully setup a redirect due to the changing apis? Had we not implemented that, would the older version of requests not be functioning properly? On the same note, I'm curious about the comments above each line:

v 0.13.3: # Do what the browsers do if strict_mode is off...
v 2.0.1: # Do what browsers do, despite standards...

Thanks!
",zhuffman,sigmavirus24
1704,2013-10-30 21:37:44,"The comments indicate the exact rationale. The standard specifies that a 301 (and a 302) in response to something that isn't a GET (or a HEAD) should be followed by the same method, but only after confirming with the user. However, web browsers have basically never done that, instead following a POST with a GET if they receive a redirect.

What happened in 2.X to explain the comments is that we gave up on `strict_mode`. I'm not 100% on the changing DELETE to GET for 301s though. I quite want to see what @sigmavirus24 thinks.
",Lukasa,sigmavirus24
1704,2013-11-17 14:27:04,"@Lukasa I guess it is backwards incompatible but this behaviour is extraordinarily wrong. I doubt there is anyone actually relying on this behaviour so we should be safe fixing it.
",sigmavirus24,Lukasa
1704,2013-11-24 14:54:14,"@Lukasa just to be clear, when you say ""301 was rewritten to GET"", do you mean all verbs that receive a 301 are rewritten to a GET? That's the only explanation that makes sense to me given how IE behaves. Thanks for digging in and getting the right answer here.
",sigmavirus24,Lukasa
1703,2013-10-25 13:38:46,"@canibanoglu Heh, this is not a great beginner problem. This issue is that, when HTTPS proxies were added to urllib3 they added a `proxy_headers` parameter which adds headers to all messages sent via a proxy. This is clearly the right place for `Proxy Authorization` to go. However, it turns out urllib3 only applies the HTTP versions when you use `ProxyManager.urlopen()`, which Requests doesn't do. Hence the work I'm doing in urllib3 to try to make some kind of consistent interface. =)
",Lukasa,canibanoglu
1703,2013-10-25 15:09:23,"@canibanoglu The question is not actually technically difficult, but it's about how urllib3 should be structured and how its interfaces should be used. You're easily technically good enough to do it: the only advantage I have is that I'm familiar with urllib3. =) I invite you to take a look at the issues I linked to above, and consider how you'd solve them. If you come up with a solution you like, open a pull request. I'm always happy for you to do the work instead of me!
",Lukasa,canibanoglu
1698,2013-10-24 09:51:57,"Thanks @Lukasa that makes a lot of sense. I'll fix up the problem in my code, get it deployed and then try to make a patch to the docs covering what you've got above. 

I think Session.send() not applying things like the session auth is pretty counter intuitive. While I get why you'd want a nice thin wrapper there it needs to be clearer that its not actually going to use much of the session.
",drsm79,Lukasa
1696,2013-10-23 01:48:00,":+1: This is a no brainer

Good work @canibanoglu 
",sigmavirus24,canibanoglu
1694,2013-10-21 08:11:16,"@sigmavirus24 that one doesn't work when there's a referrer.
",kennethreitz,sigmavirus24
1693,2013-10-20 17:26:08,"I might go through and update all of these to use `#format` instead in the library. It is the ""future"" after all. Any objections @Lukasa @kennethreitz ?
",sigmavirus24,Lukasa
1693,2013-10-20 17:26:49,"And thanks @kevinburke. LGTM!
",sigmavirus24,kevinburke
1693,2013-10-21 10:10:39,"@sigmavirus24 Go for it. =)
",Lukasa,sigmavirus24
1691,2014-03-19 13:20:45,"@Lukasa do you think an IPv6 Transport Adapter would be a good addition to the [toolbelt](/sigmavirus24/requests-toolbelt)?
",sigmavirus24,Lukasa
1690,2013-10-19 05:25:58,"Thanks for the quick response @sigmavirus24! I think this could be very useful in bandwidth constrained environments but admittedly I'm unfamiliar with requests inner workings so I completely understand if it's unfeasible to implement. Let me know if this does become a possibility in the future roadmap. Happy to contribute any way I can.
",shrredd,sigmavirus24
1690,2013-10-19 07:05:58,"The answer is it's not in the roadmap. There are discussions afoot about removing urllib3's dependence on httplib, and if that ever happens it's a discussion we can reopen. However, as @sigmavirus24 quite rightly suggests, even if urllib3 could do it we wouldn't expose it. There's just not a nice way to do it.
",Lukasa,sigmavirus24
1687,2013-10-20 16:25:27,"@kevinburke I'm quite baffled by this. How do we not already have a custom theme? Certainly we share it with Flask and the other Flask related projects that use it (and anyone else who decides to use it contrary the license) but this is not a standard or built-in theme.

Also, why move off of Sphinx? We would give up the automatic updates provided by rtfd.org.
",sigmavirus24,kevinburke
1686,2013-10-30 08:13:42,"@Lukasa I misinterepreted and yes it was hanging.

The cause is as smoser says  it's a squid bug when you have a concurrent request for one url with the first reader stalled neither closing nor reading.

 This was caused because

cq = requests.get(sys.argv[1], stream=True)
cq.raw.read(1024)
cq.close()

doesn't actually close the socket for cq until you do 'del cq'. Thats a bug IMO.
",rbtcollins,Lukasa
1686,2013-10-30 11:47:04,"This is absolutely not a bug in requests. As @Lukasa rightly states this is a very intentional design decision.
",sigmavirus24,Lukasa
1685,2014-11-02 18:56:42,"@sigmavirus24 Wow.  Great work!   It looks like you may have pinpointed the hot spot in the code.
As for tracing the which object is responsible for the memory leak, you might get some extra hints by using objgraph like so:



Lemme know if I can help in any way.

-Matt
",mhjohnson,sigmavirus24
1685,2014-11-04 00:14:35,"@sigmavirus24 You're crushing it!  :)

Hmm... Python only releases memory to be reused by itself again, and the system doesn't get the memory back until the process terminates.  So, I would think the flatline you are seeing at 13.3MiB is probably indication there is not a memory leak present with urllib2, unlike with urllib3.

It would be nice to confirm that the problem can be isolated to urllib3.  Can you share the scripts you're using to test with urllib2?  
",mhjohnson,sigmavirus24
1685,2014-11-04 05:27:56,"@sigmavirus24 
Yeah, I got a little lost with that last graphic.  Probably because I don't know the code base very well, nor am I very seasoned on debugging memory leaks.  

Do you know which object this is that I am pointing at with the red arrow in this screenshot of your graphic? 
http://cl.ly/image/3l3g410p3r1C
",mhjohnson,sigmavirus24
1685,2014-11-04 05:43:48,"I was able to get the code to show the same slowly increasing memory usage
on python3 by replacing urllib3/requests with urllib.request.urlopen.

Modified code here: https://gist.github.com/kevinburke/f99053641fab0e2259f0

## 

Kevin Burke
phone: 925.271.7005 | twentymilliseconds.com

On Mon, Nov 3, 2014 at 9:28 PM, Matthew Johnson notifications@github.com
wrote:

> @sigmavirus24 https://github.com/sigmavirus24
> Yeah, I got a little lost with that last graphic. Probably because I don't
> know the code base very well, nor am I very seasoned on debugging memory
> leaks.
> 
> Do you know which object this is that I am pointing at with the red arrow
> in this screenshot of your graphic?
> http://cl.ly/image/3l3g410p3r1C
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/1685#issuecomment-61595362
> .
",kevinburke,sigmavirus24
1685,2014-11-04 06:11:23,"As far as I can tell making requests to a website that returns a
Connection: close header (for example https://api.twilio.com/2010-04-01.json)
does not increase the memory usage by a significant amount. The caveat is
there are multiple different factors and I'm just assuming it's a socket
related issue.

## 

Kevin Burke
phone: 925.271.7005 | twentymilliseconds.com

On Mon, Nov 3, 2014 at 9:43 PM, Kevin Burke kev@inburke.com wrote:

> I was able to get the code to show the same slowly increasing memory usage
> on python3 by replacing urllib3/requests with urllib.request.urlopen.
> 
> Modified code here:
> https://gist.github.com/kevinburke/f99053641fab0e2259f0
> 
> ## 
> 
> Kevin Burke
> phone: 925.271.7005 | twentymilliseconds.com
> 
> On Mon, Nov 3, 2014 at 9:28 PM, Matthew Johnson notifications@github.com
> wrote:
> 
> > @sigmavirus24 https://github.com/sigmavirus24
> > Yeah, I got a little lost with that last graphic. Probably because I
> > don't know the code base very well, nor am I very seasoned on debugging
> > memory leaks.
> > 
> > Do you know which object this is that I am pointing at with the red arrow
> > in this screenshot of your graphic?
> > http://cl.ly/image/3l3g410p3r1C
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > https://github.com/kennethreitz/requests/issues/1685#issuecomment-61595362
> > .
",kevinburke,sigmavirus24
1685,2014-11-07 10:28:47,"After spending some time on this together with @mhjohnson, I can confirm @kevinburke theory related to the way GC treats the sockets on PyPy.

The 3c0b94047c1ccfca4ac4f2fe32afef0ae314094e commit is an interesting one. Specifically the line https://github.com/kennethreitz/requests/blob/master/requests/models.py#L736

Calling `self.raw.release_conn()` before returning content reduced significantly the used memory on PyPy, though there's still room for improvements.

Also, I think it would be better if we document the `.close()` calls that relate to the session and response classes, as also mentioned by @sigmavirus24. Requests users should be aware of those methods, because in most of the cases the methods are not called implicitly.

I also have a question and a suggestion related to the QA of this project. May I ask the maintainers why we don't use a CI to ensure the integrity of our tests? Having a CI would also allow us writing benchmark test cases where we can profile and keep a track of any performance/memory regressions.

A good example of such an approach can be found in the pq project:
https://github.com/malthe/pq/blob/master/pq/tests.py#L287

Thanks to everyone who jumped on this and decided to help!
We will keep investigating other theories causing this.
",stas,kevinburke
1685,2014-11-07 10:28:47,"After spending some time on this together with @mhjohnson, I can confirm @kevinburke theory related to the way GC treats the sockets on PyPy.

The 3c0b94047c1ccfca4ac4f2fe32afef0ae314094e commit is an interesting one. Specifically the line https://github.com/kennethreitz/requests/blob/master/requests/models.py#L736

Calling `self.raw.release_conn()` before returning content reduced significantly the used memory on PyPy, though there's still room for improvements.

Also, I think it would be better if we document the `.close()` calls that relate to the session and response classes, as also mentioned by @sigmavirus24. Requests users should be aware of those methods, because in most of the cases the methods are not called implicitly.

I also have a question and a suggestion related to the QA of this project. May I ask the maintainers why we don't use a CI to ensure the integrity of our tests? Having a CI would also allow us writing benchmark test cases where we can profile and keep a track of any performance/memory regressions.

A good example of such an approach can be found in the pq project:
https://github.com/malthe/pq/blob/master/pq/tests.py#L287

Thanks to everyone who jumped on this and decided to help!
We will keep investigating other theories causing this.
",stas,sigmavirus24
1685,2014-11-08 23:58:15,"@alex, 

I believe that @stas had used regular http (non-SSL/TLS) connection for this benchmark.  Just in case, I also used @stas's benchmark script and preformed it on my Mac (OSX 10.9.5 2.5 GHz i5 8 GB 1600 MHz DDR3) with a regular http connection.

If it helps, here are my results to compare (using your instructions):
https://gist.github.com/mhjohnson/a13f6403c8c3a3d49b8d

Let me know what you think.

Thanks,

-Matt
",mhjohnson,alex
1685,2016-01-22 14:50:09,"@sigmavirus24 Thanks Ian, It was some miss use of the Session across threads as you've mention. Thanks for the explanation and sorry for updating the wrong issue.
",barroca,sigmavirus24
1684,2013-10-18 07:47:17,"Unfortunately, users setting the `Content-Type` header explicitly seems very common. Equally common is users specifying `Host`, `Content-Length` and `User-Agent`, all of them unnecessarily.

With that said, I think I'm with @kennethreitz on this one. While I can't think of a reason a user would legitimately want to do this, they _might_ (I guess misbehaving servers could be a reason). If they do, we shouldn't be getting in their way.
",Lukasa,kennethreitz
1682,2013-10-17 14:30:49,"So if you were to set it to `None` as @Lukasa suggests, we would guess that it was encoded as `ISO-8859-2` instead of what Chrome detects it as.

I tried this on OSX w/ Python 2.7.5. It appears you're using 3.3 though and that may be entirely relevant too. I'll test with that too.

With Python 3.3, using `r.text` I too get the exception and the same guessed encoding. Setting `r.encoding` to `None` does not cause the exception to be raised though.

To be fair, I'm not sure the exception is entirely incorrect on Python's part. It seems like an extraordinary exception that they were not even planning on finding. That said, I'm sure they'll either update the documentation or they'll fix it to return either of those two errors (more likely LookupError).

I'm also bugging someone who can get your bug report out of the spam filter.
",sigmavirus24,Lukasa
1672,2013-10-15 15:21:30,"@Lukasa , thank you very much for your help.
I've create a test account so if you need, you can test the script.
I've made a few commentaries in the code, if you need anything, please tell me.

If you run over and over again, like ten times, it should has at least one fail attempt.

Here's it:
https://gist.github.com/BernardoLima/e4f7f8018e44f62393a5
",BernardoLima,Lukasa
1672,2013-10-16 10:40:21,"@sigmavirus24 , I didn't knew it was automatic, I thought when you used HTTP requests you should specify every detail, thanks for the tip.

@Lukasa, yes, unfortunately I guess that it might be the only option, I'm going to test it in Chrome, maybe it happens there too.

Thanks for the help guys.

> > Edit:
> > Guys, it's defitnely a server problem, it happens also in Chorme, shame on you bad server.
",BernardoLima,Lukasa
1672,2013-10-16 10:40:21,"@sigmavirus24 , I didn't knew it was automatic, I thought when you used HTTP requests you should specify every detail, thanks for the tip.

@Lukasa, yes, unfortunately I guess that it might be the only option, I'm going to test it in Chrome, maybe it happens there too.

Thanks for the help guys.

> > Edit:
> > Guys, it's defitnely a server problem, it happens also in Chorme, shame on you bad server.
",BernardoLima,sigmavirus24
1671,2013-10-21 11:40:51,"@Lukasa I think so. My concern is that we keep special-casing areas of the library around this and I really really hate that.
",sigmavirus24,Lukasa
1671,2014-02-17 14:18:05,"@Lukasa do we still want to do this?

I can't think of a better name for the attribute frankly, but a better implementation might be:



That should be slightly faster than checking if the header is in the dictionary and then retrieving it. 

I don't like `asserted_host` at all.

I continue to be :-1: on changes like this though. Further, do we even do any of the certificate name checking or is that entirely in urllib3?
",sigmavirus24,Lukasa
1669,2013-10-13 15:07:17,"@Lukasa this definitely looks correct. If there's a better way to do this in urllib3, then I'll hold off on providing testing advice, especially since the best way I can think to test this particular bit is through a urllib3 inspired manner.
",sigmavirus24,Lukasa
1669,2013-10-14 00:07:16,"@kennethreitz the only thing failing in the tests are the regression tests I added to prevent cookies from being lost on authentication challenge responses. If you run the tests against a local HTTPBin instance (based off of master) then you'll see green. Regardless, I think this wouldn't hurt if we merged it early as a sort of bug fix. I would, however, like to see some of the functionality move up into urllib3 though.
",sigmavirus24,kennethreitz
1669,2013-10-14 00:08:51,"@sigmavirus24 :cake:
",kennethreitz,sigmavirus24
1666,2013-10-12 20:04:42,"We have a new, better plan, thanks to @sigmavirus24.
",Lukasa,sigmavirus24
1665,2013-10-12 20:04:11,"@sigmavirus24 Yeah, that's a much better way. Let's do that instead.
",Lukasa,sigmavirus24
1665,2013-10-12 20:11:38,":heart: @Lukasa 
",sigmavirus24,Lukasa
1663,2013-11-11 12:55:02,"@Lukasa, I'm having similar issue right now:



and this started to happen after 2.0.1 upgrade.
",dmakhno,Lukasa
1663,2013-11-11 15:46:00,"I think it seems to be fault of my commit as @sigmavirus24 said.
I already attached pull request for that issue in #1713.
That will be fixed in 2.0.2 by merging my pull request or some better way to fix it.
Sorry for the inconvenience.
",daftshady,sigmavirus24
1662,2013-10-10 22:47:16,"Hi, @voberoi!

It is a [Dream Come True](http://www.youtube.com/watch?v=HH5lEIhjOHY).
",mattspitz,voberoi
1662,2013-10-10 22:47:54,"@mattspitz you're just filled with those youtube videos, aren't you? :P
",kennethreitz,mattspitz
1662,2013-10-10 22:49:53,"@mattspitz writes the best test cases.
",voberoi,mattspitz
1659,2013-10-24 11:13:13,"@dstufft requests explicitly prefers not to rely on the system certs due to massive compatibility headaches the variance in distros often cause. I believe we have a utility function to enable this functionality built-in though. Will update shortly.
",kennethreitz,dstufft
1659,2013-10-24 11:20:11,"@dstufft one thing to clarify here: historically, we only provide security releases for the latest release. We now have rigid API stability, so this shouldn't effect Pip at all. But, it is something to be aware of. 
",kennethreitz,dstufft
1659,2013-10-24 11:28:30,"@dstufft are you aware of any sort of alert system for cert blacklisting? Is it possible to automate this? 

I may make a simple service for this.
",kennethreitz,dstufft
1659,2013-10-24 11:34:32,"@dstufft interesting. What ""transformations"" need to be done against Mozilla's raw cert file?

http://mxr.mozilla.org/mozilla/source/security/nss/lib/ckfw/builtins/certdata.txt

I believe the workflow I used was taking the certs provided by [curl's caextract](http://curl.haxx.se/docs/caextract.html) and manually removing blacklisted certs.
",kennethreitz,dstufft
1659,2013-10-24 11:35:52,"Go is perfect if it functions properly. @dstufft if you're up for it, could you vet it?
",kennethreitz,dstufft
1659,2013-10-24 11:41:05,"You know who does know go, right? @Lukasa that's who. ;)
",sigmavirus24,Lukasa
1659,2013-10-24 12:07:18,"Tentative new cert is available here:

http://ci.kennethreitz.org/job/ca-bundle/lastSuccessfulBuild/artifact/certs.pem

+1 from @dstufft, @agl, or @tiran would be appreciated :)

Once I receive a +1, a new release will be cut.
",kennethreitz,dstufft
1659,2013-10-24 12:36:28,"@tiran Really? I don't know my PEM well enough, but it looks fine to me based on that Gist...
",Lukasa,tiran
1659,2013-10-24 13:11:28,"@tiran noticed the CVS_ID was missing, I guessed my way to https://gist.github.com/dstufft/7137007
",dstufft,tiran
1659,2013-10-24 13:15:06,"@dstufft Exactly what I did.https://github.com/Lukasa/extract-nss-root-certs/commit/cdead23c389fde8cf7c0d9648b4b52b891fa8d93
",Lukasa,dstufft
1659,2013-10-24 13:19:19,"Agreed. =) I think that @tiran's solution is the best for Pull-Request action, because it's a lot smarter than mine. =) Gotta fix up the slightly inconsistent indentation in it though (L162).
",Lukasa,tiran
1659,2013-10-24 13:21:37,"@kennethreitz If you're interested, I can try to wrap this code up into a nice binary that does all of the work itself: run one command and you're done. Sound good?
",Lukasa,kennethreitz
1659,2013-10-24 13:36:49,"@Lukasa i think this will work fine, but i won't stop you :)
",kennethreitz,Lukasa
1657,2013-10-11 01:49:53,"@Lukasa any further review on the code? 

In reality, if we change the future behaviour it's a really simple way of actually merging the two sets of hooks. I had that in my first stab at this [here](https://github.com/sigmavirus24/requests/commit/5bf396d5d12bf9debc667a509c84f640560da517#diff-28e67177469c0d36b068d68d9f6043bfR84).
",sigmavirus24,Lukasa
1657,2013-11-27 14:41:57,"@Lukasa @kennethreitz can we get this merged soon? I just realized I'll probably need this for github3.py so that I don't have to do [this](https://github.com/sigmavirus24/github3.py/commit/9a1b4d892a31bd12ee477f39caada76670cfc062#diff-8a0b7651f0a27fb4490e3f2d87206eedR67) ;)
",sigmavirus24,Lukasa
1655,2013-10-07 18:18:54,"@sigmavirus24 :+1:
",mgax,sigmavirus24
1654,2013-10-05 14:38:33,"I think a lot of negativity in this discussion comes from language 
barriers. Even though i agree with @sigmavirus24 about the incorrect 
description, the resulting diff looks reasonable, and i'd like to see 
these changes in requests, regardless of how they're ""described"" or 
""labelled"".
",untitaker,sigmavirus24
1654,2013-10-05 14:49:11,"@untitaker I was not objecting to the changes themselves. I have far more to maintain than just requests and I do far more offline than anyone really knows or cares to know about. @kennethreitz has an ostensibly similar schedule (which is why he has minions) and having exact language to review in an email makes our lives far easier. Stressing that point with @riyadparvez will only make future pull requests to this and other projects on his behalf better and perchance allow them to be merged in a quicker fashion.
",sigmavirus24,riyadparvez
1654,2013-10-05 14:49:11,"@untitaker I was not objecting to the changes themselves. I have far more to maintain than just requests and I do far more offline than anyone really knows or cares to know about. @kennethreitz has an ostensibly similar schedule (which is why he has minions) and having exact language to review in an email makes our lives far easier. Stressing that point with @riyadparvez will only make future pull requests to this and other projects on his behalf better and perchance allow them to be merged in a quicker fashion.
",sigmavirus24,untitaker
1654,2013-10-05 16:55:25,"LGTM.

@riyadparvez if you feel that strongly about `BaseException` then feel free to submit a PR to shazow/urllib3
",sigmavirus24,riyadparvez
1654,2013-10-05 17:02:18,"Cool, this is great. Thanks @riyadparvez! :cake:
",Lukasa,riyadparvez
1652,2013-10-04 13:52:06,"@sigmavirus24 How would you handle the use case in the issue then (you want to use generators, but avoid chunked transfer encoding)? The Content-Length is valuable information, why should it be impossible to transmit? There is no way ""requests"" can know how much data the generator outputs. I suggest that ""requests"" throws an Exception if the generator generates less/more bytes than the specified Content-Length says. Of course, the data will already be sent by then, but at least the error will probably get discovered before production, and people will be aware of the fact that it needs to generate the right amount of bytes.
",ysangkok,sigmavirus24
1652,2015-08-24 21:18:03,"@sigmavirus24: I do not need help, I have a trivial patch that makes requests use the Content-Length again. After our code base works with the current requests for a while we will probably just do what you suggested:

> Don't just use the plain requests API, instead prepare a request yourself (which is now infinitely easier) and remove the header

I just liked the approach that the caller can set the headers without having requests overwrite/delete them better.
",Bluehorn,sigmavirus24
1650,2013-10-04 06:15:00,"@GrahamDumpleton is the best type of contributor.
",kennethreitz,GrahamDumpleton
1650,2013-10-04 14:12:42,"@GrahamDumpleton is the best kind of everything
",sigmavirus24,GrahamDumpleton
1648,2013-10-04 20:16:23,"@sigmavirus24 You're not using Requests 2.0.0 on your Python 3.3 installation.
",ysangkok,sigmavirus24
1648,2013-10-05 05:59:56,"You're probably right @sigmavirus24, we should probably strip the Content-Length header. 
",Lukasa,sigmavirus24
1648,2013-10-05 15:01:27,"Hm, I'm not entirely convinced of my own argument anymore. I went looking for the old discussions surrounding users setting the `Content-Length` header themselves and I must be remembering old IRC conversations.

I'm still of the opinion that users should really not be setting those headers themselves and that we should remove them, however, I think this issue points out a far more important issue, which is the vastly different behaviour of requests on two different versions of Python.

On 2.7 (as I demonstrated) setting the `Content-Length` header does nothing to change how requests uploads the data. On 3.3, however, @ysangkok is correct that setting it sends everything as soon as it can (it still uses the generator but does not send it in an actually chunked manor). 

One easy way to fix this is to remove the header when using a generator (or always to provide a consistent behaviour), the flaw with this is that this is backwards incompatible behaviour.

The other easy way is to break the consistency of the API by not always using chunked transfer encoding with a generator. @Lukasa this definitely needs some deeper thought since I can't find the old conversations where users were admonished for setting that header themselves.

To be honest though, I would never expect that setting a header would change the behaviour of using a generator though.

This certainly is an extremely sticky situation
",sigmavirus24,Lukasa
1648,2013-11-15 12:55:21,"@bryanhelmig does the server you're uploading to require you to send the Content-Length header?
",sigmavirus24,bryanhelmig
1648,2013-11-15 16:15:58,"@bryanhelmig did you see the comments in the linked pull request?

Anyway, I don't understand why Content-Transfer-Encoding isn't just a flag. No need to delete any headers (or do any other kind of hand-holding), it was never a question of the Content-Length being right or wrong, the actual issue is that the presence of Content-Length semi-disables Content-Transfer-Encoding, which makes no sense at all. But just making Requests ignore Content-Length is not solving the real problem, which is, that Requests uses Content-Transfer-Encoding when it feels like it (sounds like that is supposed to be when reading from a generator), even though many web servers don't even support it.

Ignoring Content-Length will confuse people who supply it. If you (@sigmavirus24) insist on not transmitting it, why not just throw an exception? As you said, this functionality is probably not used widely.

In the pull request, you said ""The use case in the issue is just an example of the behaviour. There is no justification there for why you're doing what you're doing."". I disagree, I think the original code in this issue is perfectly normal behaviour, and in fact I think that streaming of POST data is a huge use case, and that it's ridiculous if one is forced to use Content-Transfer-Encoding or resort to lower-level libraries when streaming/using generators.

So to summarize: Content-Transfer-Encoding should be a flag, illegal parameter combinations should provoke exceptions, and user-supplied flags should be sent when possible. And of course, it shouldn't be possible to semi-disable Content-Transfer-Encoding.
",ysangkok,sigmavirus24
1648,2013-11-15 16:15:58,"@bryanhelmig did you see the comments in the linked pull request?

Anyway, I don't understand why Content-Transfer-Encoding isn't just a flag. No need to delete any headers (or do any other kind of hand-holding), it was never a question of the Content-Length being right or wrong, the actual issue is that the presence of Content-Length semi-disables Content-Transfer-Encoding, which makes no sense at all. But just making Requests ignore Content-Length is not solving the real problem, which is, that Requests uses Content-Transfer-Encoding when it feels like it (sounds like that is supposed to be when reading from a generator), even though many web servers don't even support it.

Ignoring Content-Length will confuse people who supply it. If you (@sigmavirus24) insist on not transmitting it, why not just throw an exception? As you said, this functionality is probably not used widely.

In the pull request, you said ""The use case in the issue is just an example of the behaviour. There is no justification there for why you're doing what you're doing."". I disagree, I think the original code in this issue is perfectly normal behaviour, and in fact I think that streaming of POST data is a huge use case, and that it's ridiculous if one is forced to use Content-Transfer-Encoding or resort to lower-level libraries when streaming/using generators.

So to summarize: Content-Transfer-Encoding should be a flag, illegal parameter combinations should provoke exceptions, and user-supplied flags should be sent when possible. And of course, it shouldn't be possible to semi-disable Content-Transfer-Encoding.
",ysangkok,bryanhelmig
1648,2013-11-15 17:06:24,"Stop stop stop.

Everyone take a breather.

@ysangkok You can do streaming uploads without generators just fine. Provide Requests a file-like object in the data parameter and that will work. Yes, it's not as simple as using a generator, but that's ok because it's still not very hard.

In the meantime, Requests should not suggest that it is chunking data when it does not. We are all agreed on that. The question is what we should do _in your specific case_: namely, providing a generator and a `Content-Length`. You and @sigmavirus24 legitimately disagree on this issue, _which is fine_. However, can we all please acknowledge that both camps have rational reasons to expect their position?

@ysangkok You've said that ""Ignoring Content-Length will confuse people who supply it."" @sigmavirus24 contends that [ignoring the very clear documentation](http://docs.python-requests.org/en/latest/user/advanced/#chunk-encoded-requests) when provided with a generator will confuse people who do _that_. You are both right.

(As a side note, the fact that many servers don't understand `Transfer-Encoding` is just a wild assertion based on no evidence that I've seen. Until any evidence is provided, I'm choosing to ignore it.)

One way or another we're going to have to pick what we do here. It's possible that the correct decision is to throw an exception when both a generator and `Content-Length` are provided. That's viable. It doesn't even make @bryanhelmig's case worse, because he should just be passing `response.raw` straight through rather than wrapping it in a decorator ([for your benefit, Bryan](http://docs.python-requests.org/en/latest/user/advanced/#streaming-uploads)).

I'm naturally inclined to sit on the fence here and throw a `YoureACrazyPerson` exception, but I can see why both of you believe what you believe. In particular, making decisions based on user-supplied headers is lame and confusing, and we should try not to do that. The following, however, are hard lines:
1. Controlling `Transfer-Encoding` will not be a flag. Not now, not ever. Requests does not do tiny special case flags like this.
2. We cannot do nothing.
3. Requests is _not_ obliged to support all use cases. I will happily throw either use case under a bus if it makes the API better.
",Lukasa,sigmavirus24
1648,2013-11-15 17:06:24,"Stop stop stop.

Everyone take a breather.

@ysangkok You can do streaming uploads without generators just fine. Provide Requests a file-like object in the data parameter and that will work. Yes, it's not as simple as using a generator, but that's ok because it's still not very hard.

In the meantime, Requests should not suggest that it is chunking data when it does not. We are all agreed on that. The question is what we should do _in your specific case_: namely, providing a generator and a `Content-Length`. You and @sigmavirus24 legitimately disagree on this issue, _which is fine_. However, can we all please acknowledge that both camps have rational reasons to expect their position?

@ysangkok You've said that ""Ignoring Content-Length will confuse people who supply it."" @sigmavirus24 contends that [ignoring the very clear documentation](http://docs.python-requests.org/en/latest/user/advanced/#chunk-encoded-requests) when provided with a generator will confuse people who do _that_. You are both right.

(As a side note, the fact that many servers don't understand `Transfer-Encoding` is just a wild assertion based on no evidence that I've seen. Until any evidence is provided, I'm choosing to ignore it.)

One way or another we're going to have to pick what we do here. It's possible that the correct decision is to throw an exception when both a generator and `Content-Length` are provided. That's viable. It doesn't even make @bryanhelmig's case worse, because he should just be passing `response.raw` straight through rather than wrapping it in a decorator ([for your benefit, Bryan](http://docs.python-requests.org/en/latest/user/advanced/#streaming-uploads)).

I'm naturally inclined to sit on the fence here and throw a `YoureACrazyPerson` exception, but I can see why both of you believe what you believe. In particular, making decisions based on user-supplied headers is lame and confusing, and we should try not to do that. The following, however, are hard lines:
1. Controlling `Transfer-Encoding` will not be a flag. Not now, not ever. Requests does not do tiny special case flags like this.
2. We cannot do nothing.
3. Requests is _not_ obliged to support all use cases. I will happily throw either use case under a bus if it makes the API better.
",Lukasa,bryanhelmig
1648,2013-11-15 19:41:53,"> does the server you're uploading to require you to send the Content-Length header?

@sigmavirus24 it does. :-( 411 for attempts without a Content-Length. Quite annoying IMO.

> did you see the comments in the linked pull request?

@ysangkok I did.

> he should just be passing response.raw straight through rather than wrapping it in a decorator

@Lukasa That was my original edit actually, but I'm not sure what that nets us in this case besides a chance to link to the docs. Unless I am mistaken, a file object isn't guaranteed to give you a length, generators are just guaranteed to _not_ give a length. I noticed in testing that small files didn't get chucked, a generator forced it.

Thanks for all the thorough replies everyone. Really though, it is fine if users in exotic situations have to swap to a different lib for one out of a 100 requests. Life is much easier for the 99 other cases.
",bryanhelmig,Lukasa
1648,2013-11-15 19:41:53,"> does the server you're uploading to require you to send the Content-Length header?

@sigmavirus24 it does. :-( 411 for attempts without a Content-Length. Quite annoying IMO.

> did you see the comments in the linked pull request?

@ysangkok I did.

> he should just be passing response.raw straight through rather than wrapping it in a decorator

@Lukasa That was my original edit actually, but I'm not sure what that nets us in this case besides a chance to link to the docs. Unless I am mistaken, a file object isn't guaranteed to give you a length, generators are just guaranteed to _not_ give a length. I noticed in testing that small files didn't get chucked, a generator forced it.

Thanks for all the thorough replies everyone. Really though, it is fine if users in exotic situations have to swap to a different lib for one out of a 100 requests. Life is much easier for the 99 other cases.
",bryanhelmig,sigmavirus24
1648,2013-11-17 19:30:06,"A few things:

As far as I'm concerned, this has been (and will continue to be, until we come to a decision) undefined behaviour

> As a side note, the fact that many servers don't understand Transfer-Encoding is just a wild assertion based on no evidence that I've seen. Until any evidence is provided, I'm choosing to ignore it.

There's a difference between servers not understanding a chunked Transfer-Encoding and servers not wanting to respect it. I suspect the latter is the case in @bryanhelmig's case. Alternatively, the application could have been written by someone who doesn't understand or know about Transfer-Encoding and so requires a Content-Length.

> It's possible that the correct decision is to throw an exception when both a generator and Content-Length are provided. That's viable.

Exceptions might be too extreme in this case. We don't generally raise exceptions for anything other than invalid URLs. The way we process the data and files parameters can raise exceptions but we don't special case anything there. That said, we've been talking about how poor an idea it is when users specify their own Content-Length and Host headers (among others which I'm probably forgetting). Since these aren't technically invalid practices, but rather practices we advise against, I suggest that we instead trigger a warning and then do the right thing in certain well documented situations.
- If we receive a Host header we raise a warning but do not delete it.
- If we receive a object whose size we can determine and the Content-Length header is provided, we should raise a warning that providing it in such a case they shouldn't do so. I'm not certain if we should override their setting though.
- If we receive a generator and a Content-Length header, we should raise a warning and remove the header.

Using warnings is a more gentle way to inform the user what they're doing is not advisable. It also covers the fact that so many of our users do not seem to bother to read the documentation and so the library becomes self-documenting of sorts. This also gives us the ability to change the behaviour in the future. And even better, if users want to disable the warnings, they can because python provides a way of silencing warnings.
",sigmavirus24,bryanhelmig
1648,2013-11-17 20:14:55,"> There's a difference between servers not understanding a chunked Transfer-Encoding and servers not wanting to respect it. I suspect the latter is the case in @bryanhelmig's case. Alternatively, the application could have been written by someone who doesn't understand or know about Transfer-Encoding and so requires a Content-Length.

I actually think this is very likely the case in most of my examples. We (@zapier) attempt to upload files to over a dozen different APIs and the few of them that require Content-Length (seem to) timeout with chunked Transfer-Encoding.

> As a side note, the fact that many servers don't understand Transfer-Encoding is just a wild assertion based on no evidence that I've seen. Until any evidence is provided, I'm choosing to ignore it.

I could put together a test suite of how various services respond to Content-Length/Transfer-Encoding, but I feel like even if it is incorrectly implemented APIs, that shouldn't really advise the design of python-requests. Easier still, I could just name names based on my experience fighting this for the last week, but again, if they are API/server bugs, what is the use of such information to python-requests?

> I suggest that we instead trigger a warning and then do the right thing in certain well documented situations.

Agree on default behavior, but sometimes reality trumps the ""right thing"" (especially when you have no control over a potentially broken server). It might be nice to document a technique to override (even if it advocates for the user to do a lot of work, like writing a custom Adapter).
",bryanhelmig,bryanhelmig
1648,2013-11-18 08:42:57,"Bleh. This makes me sad.

Ok, I think @sigmavirus24's plan is the best one here, at least for now.
",Lukasa,sigmavirus24
1648,2014-02-18 09:18:41,"@bryanhelmig Mm, yes. I think the correct solution there is from now on to use the [requests toolbelt](https://github.com/sigmavirus24/requests-toolbelt)'s streaming multipart encoder.

Right now though, it's unclear to me why there is such strong resistance to using file-like objects here.
",Lukasa,bryanhelmig
1648,2014-02-19 08:50:32,"Anyway, this discussion has gone on long enough. When provided with a lengthless iterator and a user-supplied Content-Length header, we have these options:
1. Raise an exception.
2. Blow away the Content-Length header
3. Don't chunk.

Any of these three options brings us into compliance with the RFC. It is clear that everyone raising this issue prefers (3). @sigmavirus24?
",Lukasa,sigmavirus24
1648,2014-02-20 19:12:36,"I'm inclined to agree with you @sigmavirus24. I'll see if I can get a few minutes with Kenneth at some point soon to talk this over with him.
",Lukasa,sigmavirus24
1648,2014-05-12 08:41:43,"I'm in the same boat as @bryanhelmig and @netheosgithub, I have a generator where I know in advance what size the combined chunks will have, and have a server that does not support chunked uploads (a WSGI app, WSGI according to my research does not support chunked encoding at all). The data from the generator is too big to fit into RAM, so combining the chunks before and passing it to requests is out of the question.
Has there been any new development regarding this issue?
",jbaiter,bryanhelmig
1648,2014-05-12 13:51:02,"Thanks @sigmavirus24 :+1:, I did precisely that, i was just wondering if by now there was a more elegant way to go about it
",jbaiter,sigmavirus24
1648,2014-05-12 13:54:33,"Perhaps there's a good way of providing this via the toolbelt, eh @Lukasa ?
",sigmavirus24,Lukasa
1648,2014-05-12 18:06:44,"@sigmavirus24 Absolutely. =)
",Lukasa,sigmavirus24
1648,2014-05-13 02:33:40,"@Lukasa any feedback on https://gitlab.com/sigmavirus24/toolbelt/merge_requests/2
",sigmavirus24,Lukasa
1648,2016-03-25 10:18:39,"@sigmavirus24 ""We're not doing anything wrong by sending it regardless of the encoding, the server is doing the wrong thing by not ignoring it."" is actually now wrong:

RFC 7230: ""A sender MUST NOT send a Content-Length header field in any message that contains a Transfer-Encoding header field.""
",ztane,sigmavirus24
1648,2016-03-26 19:16:34,"@sigmavirus24 that was caused by ""transfer-Encoding: chunked, content-length set => body not chunked."" And RFC 7230 **forbids setting Content-Length with Transfer-Encoding**.
",ztane,sigmavirus24
1648,2016-05-06 22:15:03,"@Lukasa @sigmavirus24 fair enough -- thanks for the prompt reply. I'll continue to look to fix the boto issue in that project.
",timuralp,Lukasa
1648,2016-05-06 22:15:03,"@Lukasa @sigmavirus24 fair enough -- thanks for the prompt reply. I'll continue to look to fix the boto issue in that project.
",timuralp,sigmavirus24
1647,2013-10-05 05:47:17,"@t-8ch It's very possible that it's not allowed unencoded, I didn't check, I just checked if urlparse would split it.
",Lukasa,t-8ch
1647,2013-10-05 07:40:45,"Scumbag urlparse, ruining other libraries from the grave. :P 

@t-8ch Worth removing what I just added? Seems harmless. (/me imagines urlparse authors saying this a decade ago.) 
",shazow,t-8ch
1647,2013-10-05 08:53:52,"@shazow Doesn't seem to hurt.

Using `str.rsplit` would be easier on the eyes though :smiley:


",t-8ch,shazow
1647,2013-10-05 09:00:36,"@t-8ch Ugh I literally spent 5 minutes yesterday looking for that method, and gave up. Thank you. :) My venture into golangland has ruined my pyfu. Fixed in https://github.com/shazow/urllib3/commit/9552344989bfd8c06214692e612aac9a9fc83abc
",shazow,t-8ch
1640,2013-10-01 08:11:00,"This in principle looks great @abarnert, thanks so much!

We wouldn't need to wait until 3.0, because this change isn't backwards incompatible: adding the ability to pass a 4-tuple can be done in a minor release (e.g. 2.1.0) according to [semver](http://semver.org/).

The real question is whether we think this API extension is the right way to handle it. As you pointed out in #1640, we already poorly document this corner of the API so that definitely has to be fixed.

@kennethreitz: Are you happy with this extension to the API, or would you like to reconsider the multipart file API entirely?
",Lukasa,abarnert
1640,2013-10-02 18:28:26,"@Lukasa Here's an example of what I'm trying to do: http://paste2.org/kgNztbBv

Basically, each part of the multipart has a different `Content-Type`, separate and distinct from the one in the message header.
",mrichman,Lukasa
1640,2013-10-04 08:49:59,"@abarnert Yes it is. =)
",Lukasa,abarnert
1640,2013-10-07 23:21:15,"I'm +1, specifically because this changes makes the code much easier to understand.

Thanks, @abarnert.
",kennethreitz,abarnert
1638,2013-10-01 08:50:30,"@Lukasa  Yeah i wasn't suggesting monkeypatch cookielib.  That was just for a quick work around in case anybody needed it.  My comment around about what requests might consider was as you said adjusting what get_full_url returns in MockRequests.  (comment clarified above)
",dknecht,Lukasa
1638,2013-10-01 14:56:56,"@Lukasa Also wanted to mention that the use case by @mtourne was meant as easy example.  The main use case is being able to test sites where we need to change the IP from what the domain would normally resolve to.  

Maybe the right way to solve this is how curl solved it recently?

Previously in curl you would do `curl -H'Host: fake.net' http://127.0.0.1` and now you can do `curl -v --resolve fake.net:80:127.0.0.1 http://fake.net``


",dknecht,Lukasa
1638,2013-10-01 15:02:32,"@Lukasa  Cool.  Just trying to throw out other options that are cleaner.  We broke most of our test suite with when we upgraded to get some of the other new features and trying to avoid downgrading.  
",dknecht,Lukasa
1638,2013-10-05 14:55:33,"@Lukasa agreed. 
",sigmavirus24,Lukasa
1638,2013-10-09 15:22:53,"@sigmavirus24 Go ahead and open the PR with that commit. =)
",Lukasa,sigmavirus24
1638,2013-10-09 15:54:42,"@sigmavirus24  Thank you.  I tested and it works.  
",dknecht,sigmavirus24
1638,2013-10-22 16:39:15,"@Lukasa Do you think we can get this merged in?  =)
",dknecht,Lukasa
1635,2013-09-28 16:10:39,"@Lukasa I pushed updated version, Thanks!
",daftshady,Lukasa
1635,2013-09-28 17:16:26,"LGTM to me too. :+1: @daftshady 

Please add yourself to the [AUTHORS.rst](https://github.com/kennethreitz/requests/blob/master/AUTHORS.rst#patches-and-suggestions) file?
",sigmavirus24,daftshady
1634,2013-09-28 15:24:29,"@Lukasa `WWW-Authenticate` you meant...
",thikonom,Lukasa
1630,2013-09-27 11:36:55,"@Lukasa I will try on it.
I think implementing deepcopy of models.PreparedRequest and call req.deepcopy() instead of req.copy() will fix super extra bonus bug easily. Is there any expectable drawback of using deepcopy?
",daftshady,Lukasa
1630,2013-09-27 11:44:00,"@daftshady It's unnecessary. =) `PreparedRequest.copy()` should just make sure it calls `req.headers.copy()` to get the new head dict. =)
",Lukasa,daftshady
1630,2013-09-27 12:08:51,"@Lukasa It' s strange.. I think even if PreparedRequest.copy creates new object, as you know,  assignment like this (p.headers = self.headers) only make shallow copy of CaseInsensitiveDict. so the change in new `p` created by copy() can affect origin so that extra bonus bug happens.
Anyway, i will try another way to fix it if you think deepcopy is not suitable =)
",daftshady,Lukasa
1630,2013-09-27 12:59:03,"@Lukasa I understand. I misunderstood that you are saying 'p.headers = self.headers' is just enough.
As you said, `CaseInsensitiveDict.copy()` works correctly. So just fixing it to 'p.headers = self.headers.copy() is enough, not deepcopy.

In your example, if

> > > b = {'first' : [1,2,3]}
> > > d = b.copy()
> > > b is d
> > > False
> > > b['first'].append(4)
> > > b
> > > {'first' : [1,2,3,4]}
> > > d
> > > {'first' : [1,2,3,4]}
> > > Changes that affect B can affect D in this case.

But, as you said, `CaseInsensitiveDict.copy()` is just enough to fix extra bonus bug.
Sorry for my carelessness =)
",daftshady,Lukasa
1629,2013-10-05 17:22:34,"@kennethreitz Did you forget about this? Can't find a relevant commit ;)
",schlamar,kennethreitz
1628,2013-09-26 12:51:53,"@Lukasa Right, It's quite tricky. 

I guess a solution could be to remember that a custom `Host` has been supplied and use it only as an alias of the one included in the request URL for any subsequent requests to the same host within a session. Let me try to demonstrate:


",jakubroztocil,Lukasa
1628,2013-10-10 09:55:38,"Any feedback on update, @Lukasa?
",Scorpil,Lukasa
1628,2013-11-27 02:28:20,"@kennethreitz I have avoided being too negative about this, but I agree with you. I don't see anything wrong with the current behaviour. We're inconveniencing users who are doing something we strongly recommend against, that doesn't seem like a bad thing frankly.
",sigmavirus24,kennethreitz
1628,2013-12-06 11:29:10,"Thanks @kennethreitz 
No problem, if you think it's not appropriate change - then it's not. No hard feelings :)
",Scorpil,kennethreitz
1625,2013-09-25 14:59:54,"@Lukasa Sure. It's just a request. :)
",f,Lukasa
1625,2013-09-25 15:58:22,"Thanks @kennethreitz :) It's up to you.
",f,kennethreitz
1624,2013-09-28 17:43:55,"No worries @monocasual 

Also, @Lukasa sometimes you have to ask the obvioius question
",sigmavirus24,Lukasa
1622,2013-09-25 08:01:34,"@GrahamDumpleton Could you please run these tests against plain urllib3, too (see https://gist.github.com/schlamar/5080598#file-test_proxy-py for example code)?
",schlamar,GrahamDumpleton
1622,2013-09-25 10:54:38,"@GrahamDumpleton I'm pretty sure the only issue is your setup. Please check first how curl behaves (I expect exactly like requests 2.x).

> they have broken something that used to work

That's wrong. Proxy support was broken before, it is fixed with 2.x. 

`http_port_with_https_scheme` is now exactly how it should work. There is just a TCP connection to the proxy (doesn't matter if port 80 or port 443). All HTTP data is encrypted with the cert of the target. There is no security issue here (you can check with curl/wireshark).

`https_port_with_https_scheme` should fail at cert validation because squid is configured as MITM proxy. Adding your custom cert to the request call should work.
",schlamar,GrahamDumpleton
1622,2013-09-26 09:39:44,"> Then why did you ever bother making the scheme mandatory in the proxies map when you ignore it? You could have still accepted host:port aline and just always made it use http anyway.

I have asked that myself. Because curl and other HTTP clients do support `http_proxy=host:port` without scheme. @Lukasa ?

> Anyway, what it comes down to is the documentation needs to state then that the scheme is totally irrelevant and that http:// is the only one that makes sense, because https:// is a lie.

[subjective opinion] The documentation shows only examples with http:// so this kind of implies that. I think you cannot expect CONNECT to work via SSL while it is not explicitly stated. 
",schlamar,Lukasa
1622,2013-10-11 11:08:20,"@schlamar Sorry I let this slip past me. The answer there is 'maybe'. @t-8ch was one of the people strongly pushing for explicit proxy schemes: do you have an opinion?
",Lukasa,schlamar
1622,2013-10-12 11:22:09,"@Lukasa 
Like @schlamar I think the 1.x behaviour was wrong.
The proposal for explicit schemes in lieu of a default `http`-scheme was only about the second zen of python: `Explicit is better than implicit.`

If people are more comfortable with a default scheme I'm totally fine with this.
",t-8ch,schlamar
1622,2013-10-12 11:22:09,"@Lukasa 
Like @schlamar I think the 1.x behaviour was wrong.
The proposal for explicit schemes in lieu of a default `http`-scheme was only about the second zen of python: `Explicit is better than implicit.`

If people are more comfortable with a default scheme I'm totally fine with this.
",t-8ch,Lukasa
1622,2013-10-12 13:43:25,"> If people are more comfortable with a default scheme I'm totally fine with this.

I would think telling people to explicitly set the scheme is good but having a default for it is just as good. If this helps @GrahamDumpleton in anyway I'm especially :+1: for it.
",sigmavirus24,GrahamDumpleton
1622,2013-10-12 20:03:04,"@schlamar this is not introducing a new feature per-se. Are you instead considering semantic versioning? Technically this would be backwards compatible since we would still be allowing/encouraging users to use a scheme, so it is also permissible by semantic versioning.
",sigmavirus24,schlamar
1621,2013-09-25 01:43:50,"@davidfischer I think the link would be fantastic, actually. Want to add it?
",kennethreitz,davidfischer
1621,2013-09-25 02:01:21,"@kennethreitz linked!
",davidfischer,kennethreitz
1621,2013-09-25 07:43:16,"This is awesome @davidfischer, thanks! =D :cake:
",Lukasa,davidfischer
1618,2013-09-24 11:30:10,"Yeah I was of a similar opinion @smokey42 but I agree with the decision in the issue linkes by @Lukasa. It allows for the most degrees of freedom.
",sigmavirus24,Lukasa
1616,2013-09-21 14:53:07,"@Lukasa my searches were bad! Thanks for the quick response.
",abn,Lukasa
1615,2013-09-21 13:42:08,"@Lukasa they are thoroughly distinct values in this context. I would be okay with seeing `timeout=0` and immediately raising a `Timeout` exception before doing anything. Since we don't return a response (or request) this won't affect the API.

While I don't think we're doing anything wrong by passing this along and catching (and raising) the right exception, I think @Damgaard is justified in his confusion.

Regardless, I'm not entirely convinced this justifies a change. Perhaps the documentation could be improved, but if we do change this it will be a breaking API change and will have to be put off until 2.0. There are probably plenty of people properly handling this and changing the exception raised would break their code.
",sigmavirus24,Damgaard
1615,2013-09-21 13:42:08,"@Lukasa they are thoroughly distinct values in this context. I would be okay with seeing `timeout=0` and immediately raising a `Timeout` exception before doing anything. Since we don't return a response (or request) this won't affect the API.

While I don't think we're doing anything wrong by passing this along and catching (and raising) the right exception, I think @Damgaard is justified in his confusion.

Regardless, I'm not entirely convinced this justifies a change. Perhaps the documentation could be improved, but if we do change this it will be a breaking API change and will have to be put off until 2.0. There are probably plenty of people properly handling this and changing the exception raised would break their code.
",sigmavirus24,Lukasa
1606,2013-09-17 11:14:20,"And @t-8ch's suggestion is exactly why I'm kind of -1 on this. I would love extra conveniences for accessing some urllib3's extra features but I don't want us caught in parameter hell.

Would it make sense to consolidate a bunch of these options into a dictionary? 
",sigmavirus24,t-8ch
1604,2013-09-15 18:21:21,"What @Lukasa said + the fact that if the encoding retrieved from the headers is non-existent we rely on [charade](https://github.com/sigmavirus24/charade) to guess at the encoding. With so few characters, charade will not return anything definitive because it uses statistical data to **guess** at what the right encoding is.

Frankly, the year makes no difference and does not change specification either.

If you know what encoding you're expecting you can also do the decoding yourself like so:



There is nothing wrong with requests as far as I'm concerned and this is not a bug in charade either. Since @Lukasa seems to agree with me, I'm closing this.
",sigmavirus24,Lukasa
1604,2013-09-15 18:26:16,"@lavr (/cc @sigmavirus24), even easier than that, you can simply provide the encoding yourself.



Then, proceed normally.
",kennethreitz,sigmavirus24
1604,2013-09-15 18:27:29,"@kennethreitz that's disappointing. Why are we making that easy for people? =P
",sigmavirus24,kennethreitz
1604,2013-09-15 18:58:19,"@sigmavirus24
please note, that utils.get_encoding_from_headers always returns 'ISO-8859-1', and charade has no chance to be called. 
so bug is: we expect that charade is used to guess encoding, but it is not. 
",lavr,sigmavirus24
1604,2013-09-16 19:38:00,"@Lukasa 
Well, I can use this hack.
And everybody in Eastern Europe and Asia can use it.

But what if we fix it in requests ? ;)
What if requests can honestly set `enconding=None` on response without charset ?
",lavr,Lukasa
1602,2013-09-14 13:21:08,"@Lukasa yeah that makes perfect sense actually (about `auth-int`). It [`auth-int`] looks (at a glance) like it is a variation on the `auth` algorithm. I would be perfectly willing to punt on this but at some point it looks like it was planned to be added: https://github.com/sigmavirus24/requests/commit/22e31b4b737c2a3b61b3ab4fccd534b2eee65a87#L0R126

Regardless #1601 will be fixed by this PR and since we haven't seen many complaints I would guess that `auth-int` isn't as widely used.
",sigmavirus24,Lukasa
1599,2013-09-13 15:21:29,"@Lukasa  you don't have to explain to me such obvious things, really ;-)

The problem is that it happens very rare, when it happened it was Connection Reset probably. Also it happened from daemonized thread (daemon=True). The real problem is that it happened once a milion reqs or so. I wonder if it's not a GC ""problem"" in python or something.
",pigmej,Lukasa
1599,2015-06-26 08:00:23,"@dalanmiller Can you follow @sigmavirus24's advice from earlier in the thread, please?
",Lukasa,dalanmiller
1599,2015-06-26 08:00:23,"@dalanmiller Can you follow @sigmavirus24's advice from earlier in the thread, please?
",Lukasa,sigmavirus24
1595,2013-09-12 21:24:07,"Fair :)

Thank you @Lukasa !
",woozyking,Lukasa
1595,2013-12-20 07:11:50,"@Lukasa - would you consider using ujson by default if it is installed?

Something like:


",ziadsawalha,Lukasa
1592,2013-09-12 07:58:43,"@homm Thanks for this! Unfortunately, I don't agree with most of these documentation changes. My notes, in the order the change appears in the diff:
1. `stream` (or more accurately `prefetch`) was always required for `Response.iter_content`, but not always required for `Response.raw`. The portion of the documentation you're changing is about the changes introduced in the move to 1.x, and so correctly reflects the change in behaviour. This should not be changed.
2. I'm happy with this one. =)
3. Whitespace change, always fine.
4. As (3).
5. We should not be hiding `Response.raw` from people. This portion of the documentation is clear about what `Response.raw` is, so I see no reason to believe that it's dangerous. =)

If you strongly disagaree with any of those points, please let me know. Otherwise, if you remove changes 1 and 5 I'll happily merge this. =)
",Lukasa,homm
1592,2013-09-12 09:23:56,"Beautiful, thanks so much @homm! Would you like to add yourself to the AUTHORS file as well?
",Lukasa,homm
1592,2013-09-12 09:33:05,"@Lukasa I accidentally use unicode — instead of - in authors list. Sorry. Fix it please.
",homm,Lukasa
1586,2014-01-23 13:06:02,"@yydonny why wouldn't you use streaming on a file like that anyway? Or are you just trying to argue a point? Passing `stream=True` will not decompress the file for you unless you use `iter_content` (or `iter_lines`). @Lukasa is 100% correct that you should be using the response's `raw` method. It is documented and it was the prior conclusion of this issue. For 98% of our users, the decompression is exactly what they want. For the 2% use case we have built in considerations but those users need to be considerate enough to read the documentation first.

Arguing that we should abandon what we're doing because there are servers that do the wrong thing on the web is like saying certain countries should start devaluing its currency because other nations do the same thing. That approach to economic ""development"" is as harmful to the citizens as deactivating automatic gzip decompression would be to the 98% of our users.
",sigmavirus24,Lukasa
1584,2013-09-10 09:01:36,"Hi @Lukasa and @robi-wan I'm delighted to see this question and already find it answered by you. I happen to just have hit the same issue with missing streaming functionality for multipart file uploads.

I suggest to you to look at the _poster_ module - I have used this instead for realizing the upload in a script that otherwise uses requests. I have implemented the whole upload request with this other module and urllib2, however it might as well be possible to use it to prepare the file-like `data` argument for requests.

The support for this kind of streaming uploads was a prime reason for going with requests, therefore I was disappointed when encountering the `NotImplementedError` that is thrown by `PreparedRequest.prepare_body` when the `files` as well as the `data` argument is provided. This could be made clearer in the documentation.
",avallen,Lukasa
1584,2013-09-10 10:36:53,"Hi @Lukasa thanks for your quick response. I read the Microsoft Knowledge Base Article and tried the suggested solution without success.

@avallen In the meantime I found _poster_ and used it for solving this problem:



This works... but I would like to use _requests_ for task like this.
",robi-wan,Lukasa
1584,2013-10-15 01:35:56,"@sigmavirus24 sorry, you're right, I will email you, if you don't mind.
Thank you very much.
",BernardoLima,sigmavirus24
1583,2013-09-09 11:36:20,"@Lukasa is this documented anywhere? 
",sigmavirus24,Lukasa
1583,2013-09-09 12:09:19,"@sigmavirus24 Not as a single flow, no. It's not clear to me that it should be part of the formal documentation though: perhaps another blog post?
",Lukasa,sigmavirus24
1582,2016-03-10 21:16:40,"Hello gents.  I apologize to wake the sleeping giant.   However Ive been trying to track down a viable workaround to request https: with NTLM proxy authentication and have come up short.  @Lukasa et al I  appreciate all the work to date.   As of what is currently available it doesn't look like this is supported in urllib3, requests-ntlm or request.   What does one do?   Any suggestions?
",ryandebruyn,Lukasa
1582,2016-03-11 04:08:57,"@Lukasa. From what I can tell  requests-ntlm will let you do NTLM auth with a proxy when using HTTP, but not with HTTPS.   Am I missing something?  Thanks again for your time.
",ryandebruyn,Lukasa
1581,2013-09-08 18:45:33,"@t-8ch is so :metal:
",sigmavirus24,t-8ch
1580,2013-09-10 10:06:21,"@sigmavirus24 thanks for the pointer to ws4py looks interesting.  With my limited understanding of both the codebases it sounds like while I can start with request to upgrade a connection to a Websocket I would have to do heart surgery to get hold of the actual TCP connection which would not be the right thing to do is your concern am I right?
",vivekhub,sigmavirus24
1579,2013-09-08 15:04:57,"This is excellent @yang, thank you so much! :cake:
",Lukasa,yang
1578,2013-09-05 06:44:21,"Well deserved @jparise, thanks so much for your work! :grin:
",Lukasa,jparise
1573,2013-09-03 14:46:38,"@sigmavirus24 
The tuple is for `(certificate, key)`. Currently there is no support for encrypted keyfiles.
The [stdlib](file:///usr/share/doc/python/html/library/ssl.html#ssl.SSLContext.load_cert_chain) only got support for those in version 3.3.
",t-8ch,sigmavirus24
1573,2013-09-03 15:05:15,"Heh, @t-8ch, you accidentally linked to a file on your local FS. ;) [Correct link](http://docs.python.org/3.3/library/ssl.html#ssl.SSLContext.load_cert_chain).
",Lukasa,t-8ch
1573,2013-09-04 02:43:57,"Quite right @t-8ch. This is why I should never answer issues from the bus. :/
",sigmavirus24,t-8ch
1573,2014-07-30 15:06:41,"@maxnoel I'm pretty sure this is in OpenSSL's hands but if you can answer @Lukasa's question (the last comment on this issue) it would be very helpful in giving a definite answer regarding if there was anything we can do to help.
",sigmavirus24,Lukasa
1573,2016-02-24 07:36:00,"@botondus I think I found a simpler way to achieve this with request library. I am documenting this for other people who are facing the issue.

I assume that you have a .p12 certificate and a passphrase for the key.

### Generate certificate and private key.



Well, we are not done yet and we need to generate the key that doesn't require the PEM password every time it needs to talk to the server.

### Generate key without passphrase.



Now, you will have `certificate.pem` and `plainkey.pem`, both of the files required to talk to the API using requests.

Here is an example request using these cert and keys.



Hope this helps:

cc @kennethreitz @Lukasa @sigmavirus24 
",vinitkumar,Lukasa
1573,2016-02-24 07:36:00,"@botondus I think I found a simpler way to achieve this with request library. I am documenting this for other people who are facing the issue.

I assume that you have a .p12 certificate and a passphrase for the key.

### Generate certificate and private key.



Well, we are not done yet and we need to generate the key that doesn't require the PEM password every time it needs to talk to the server.

### Generate key without passphrase.



Now, you will have `certificate.pem` and `plainkey.pem`, both of the files required to talk to the API using requests.

Here is an example request using these cert and keys.



Hope this helps:

cc @kennethreitz @Lukasa @sigmavirus24 
",vinitkumar,sigmavirus24
1573,2016-08-24 14:24:15,"@ahnolds: Does this also work for for PKCS#12 files, or is this PEM only?

@Lukasa: Is the PKCS#12 case really supposed to be handled here, or should I open a separate issue for that?
",vog,Lukasa
1573,2016-08-24 14:49:21,"@Lukasa: I was thinking more of providing a good high-level API in requests. For example, simply providing the `client_cert.p12` filename and the password through the `cert=...` keyword parameter.
",vog,Lukasa
1573,2016-08-24 16:40:53,"@Lukasa I'm not sure about the internals of `requests`, so maybe I underestimate what is already there, but I think one of the following things needs to be done:
- Either we have a way to provide a PKCS#12 filename directly to the lower layers (urllib3, etc.). And maybe the password, too. (Because I know nobody who wants a URL library to ask the admin interactively to enter their PKCS#12 password on a tool that runs server-side.)
- If that is impossible, we would need to convert PKCS#12 (+password) to PEM, then providing these to the lower levels. This is done with a few calls directly to the `OpenSSL` binding. However, the result is the PEM certificate as a string, and I haven't yet found a way to provide the (unencrypted) PEM as string to the lower layers (except maybe by using OpenSSL / python ""ssl"" ""buffer"" wrapper, e.g. `wrap_bio`, but this is only available in latest Python 3 versions, not Python 2).
- So if that is impossible, too, we would not only need to convert PKCS#12 to PEM, but also have to create a temporary file containing the (unencrypted) PEM data.

Note that the last point is what I'm essentially doing at the moment, but I don't like this at all. Why can't I provide a simple string to OpenSSL containing the certificate? Moreover, why can't I simply pass the PKCS#12 filename and password to the lower layers?
",vog,Lukasa
1573,2016-08-24 16:55:43,"@Lukasa Thanks for taking this issue seriously. Sorry if this sounds too technical, but essentially it is just this:

You want to access a service through a client certificate. Almost everywhere you get this as a file and a password (where the file is PKCS#12 encoded). In most APIs, such as the Java standard library, you simply give it the filename and password, and are done with it.

However, in Python this is complicated as hell.

That's why almost nobody does it. Instead, they convert their file and password to a PEM file by hand, through OpenSSL, and use that file. This is administrative overhead for every user of such an application. Because they can't simply name the (PKCS#12) file and password.

I think the `requests` library should make it at least as simple as in Java.

`requests` already does a great job of simplifying stupid complex APIs, and the PKCS#12 use case is just another example of a stupid complex API.
",vog,Lukasa
1570,2013-09-02 18:23:09,"@sigmavirus24 That was not a `SSLError` but a `socket.error` which doesn't really indicate a SSL problem.
",t-8ch,sigmavirus24
1570,2013-09-02 18:25:45,"@t-8ch maybe I wasn't clear but I never said that issue was caused by this, just that discussion about negotiating a different version could be found there. =)
",sigmavirus24,t-8ch
1567,2013-09-02 15:17:32,"I can reproduce this on Arch x64 with both python2 and 3.
If I force the SSL version to be `TLSv1` like described on [@Lukasa s blog](http://lukasa.co.uk/2013/01/Choosing_SSL_Version_In_Requests/) it works for me.
",t-8ch,Lukasa
1567,2013-09-02 16:43:58,"@t-8ch so is it then safe to say this is not an issue with requests?

Should we turn this into a feature request for falling back on older versions of TLS? I would really rather we didn't given how broken some of them are. If we automatically fall back on them we'll avoid bug reports like this but we will be giving our users a somewhat false sense of security, no? I for one would rather deal with these bug reports than compromise our users' security.

On a related note, could you provide your method of detecting what the server will accept/respond to? I would rather not keep bugging you @t-8ch. I don't want to keep nagging you to figure out our TLS/SSL issues.
",sigmavirus24,t-8ch
1567,2013-09-02 17:10:42,"Yes, I don't think this is an issue with requests.

Falling back to older versions of TLS shouldn't be much of an issue securitywise (if we stay in TLS land).
On the other hand broken servers respond in all imaginable ways to handshakes.
If they behave correctly they could simply negotiate a lower version of TLS or
abort the handshake correctly which will result in an SSLError with a more or
less understandable error message.
If they are broken it's down to pure guessing what an error means.
(Servers reset the connection, send no certificate, time out and whatnot)
I really doubt we want to do this :smile:

@sigmavirus24
The best way is probably the utility `gnutls-cli` which is part of `gnutls`.
One can use it like this:



This means: Use your normal settings, but only use TLSv1.2 as TLS version, then
connect to the given host (port 443 is the default).
(Full docs are here: http://www.gnutls.org/manual/html_node/Priority-Strings.html, `openssl s_client` has similary functionality)
",t-8ch,sigmavirus24
1567,2013-09-02 17:32:09,"@t-8ch how complex is the re-negotiation process? I think the only good idea is to raise an exception when the server is doing something unexpected and re-negotiate only when it explicitly asks us to. If you don't have time to describe the process of negotiation, feel free to link the RFC.

In the meantime, I'm going to open an issue/feature request for this and close this issue. Thanks for opening @jameh
",sigmavirus24,t-8ch
1567,2013-09-02 17:34:31,"Thanks a lot @t-8ch and everyone. Yes I was on Arch Linux x64. Subclassing the HTTPAdapter class as shown on [your blog](http://lukasa.co.uk/2013/01/Choosing_SSL_Version_In_Requests/) and indicating to use `ssl.PROTOCOL_TLSv1` got me my 200 response. :)
",jameh,t-8ch
1567,2013-09-02 17:47:20,"@sigmavirus24 I got it a bit wrong. There isn't an explicit negotiation of the TLS version. Instead the client sends a handshake with the highest version it supports the server then can respond with the version it wishes to use.

[RFC 5246 Appendix E](https://tools.ietf.org/html/rfc5246#appendix-E):



So a server timing out is simply broken.
",t-8ch,sigmavirus24
1567,2013-09-02 17:58:08,"@Lukasa but we could support an older version of TLS.
",sigmavirus24,Lukasa
1567,2013-09-02 17:59:11,"We do. =D

As @t-8ch points out, there is an explicit negotiation for TLS version, and we support them all. The problem here was that the server utterly failed to perform that negotiation.
",Lukasa,t-8ch
1565,2013-09-03 11:46:17,"@aesptux That's an excellent question. =)

An absolute import here will search `sys.path` for a module called `requests`. That works brilliant in 99.9% of cases, which is where people have done `pip install requests`. It'll find the correct module (namely the one we're running from), and then correct find its submodules and grab the right data. All great. The time it doesn't work is when you either don't have `requests` in any of `sys.path`, or you have something else called `requests` ahead of the actual module.

The specific bug report that triggered this fix is the first one. @dstufft was building a python project that included Requests as part of its source code under a different name, to avoid clashing with a user install of Requests. He was therefore getting requests using something like `import vend_requests` or `import vend_requests as requests`. That works great for almost everything, because they were all using relative imports, so they didn't care about whether the project was actually called `requests`. This line, however, does care.

To double up the pain, the line is deliberately inside a `try...except` block that catches `ImportError`. That makes this bug particularly tough to find, as we swallow the exception that would have told you about it.
",Lukasa,dstufft
1565,2013-09-03 12:12:13,"@dstufft @Lukasa Thanks for the clarification! :+1: 
",aesptux,Lukasa
1565,2013-09-03 12:12:13,"@dstufft @Lukasa Thanks for the clarification! :+1: 
",aesptux,dstufft
1561,2013-08-30 21:17:45,"It's been doing that for over a year so I would guess that this is perfectly fine otherwise we would have run into issues much sooner. In other words @Lukasa, I agree with you.
",sigmavirus24,Lukasa
1561,2013-08-31 05:49:27,"@kennethreitz Looks like urllib3 changed its behaviour. I'll see if we can fix it up upstream.
",Lukasa,kennethreitz
1561,2013-08-31 14:46:38,"@Lukasa, if you look at the blame on that file though, the change was a year ago. Something quite possibly may have changed but it isn't on that line of that file. It is probably somewhere else and in a change far more recent.
",sigmavirus24,Lukasa
1560,2013-08-30 12:03:29,"Thanks for raising this issue @dstufft! It's not at all clear to me why that import wouldn't work when requests is vendored, though. Can you elaborate on what the problem is?
",Lukasa,dstufft
1560,2013-08-30 12:05:14,"Yea what @kennethreitz said.
",dstufft,kennethreitz
1558,2013-08-28 11:34:51,"@ssbarnea To the best of my knowledge we have no plans to release any further 1.2.X point releases. The next planned release is 2.0.0. I'm potentially open to you making a patch against master, but I'd want to check with @kennethreitz first.
",Lukasa,ssbarnea
1558,2013-08-28 12:09:55,"Go for it



Kenneth Reitz

On Wed, Aug 28, 2013 at 7:34 AM, Cory Benfield notifications@github.com
wrote:

> ## @ssbarnea To the best of my knowledge we have no plans to release any further 1.2.X point releases. The next planned release is 2.0.0. I'm potentially open to you making a patch against master, but I'd want to check with @kennethreitz first.
> 
> Reply to this email directly or view it on GitHub:
> https://github.com/kennethreitz/requests/issues/1558#issuecomment-23407592
",kennethreitz,ssbarnea
1558,2014-02-07 14:09:25,"@Lukasa Using cloudpickle from piclouds cloud library(https://pypi.python.org/pypi/cloud) seems like a good option to me. It supports a lot more types than pythons pickle module, including functions (http://docs.picloud.com/client_pitfall.html#nonsupported-objects lists the exceptions). It would require adding a dependency on cloud, but we could just copy the source, the same way as urllib3 and chardet since the serialization code is not likely to change.

If there are no objections, I can implement this and send in a pull request.
",ankitson,Lukasa
1554,2013-08-26 22:21:20,"Wow, thanks @Lukasa ... I tried your code but it seems that I don't understand it ;)



will join all line-breaks? The desired XML paylout itself also contains ""\r\n""



so I helped me with: 



works fine for me :) 
",wiesson,Lukasa
1554,2014-01-19 14:28:31,"@Lukasa beat me to this, but I was going to include the caveat that questions should be posted to [StackOverflow](http://stackoverflow.com/questions/tagged/python-requests) in the future.
",sigmavirus24,Lukasa
1554,2014-01-19 14:51:55,"@sigmavirus24 sorry for that, I will post my problems on stackoverflow in the future. :) @Lukasa, thanks for the quick answer!
",wiesson,Lukasa
1554,2014-01-19 14:51:55,"@sigmavirus24 sorry for that, I will post my problems on stackoverflow in the future. :) @Lukasa, thanks for the quick answer!
",wiesson,sigmavirus24
1554,2014-01-19 16:49:19,"@Lukasa, here is my approach: Any comments or suggestions for improvements? No clue how to catch a disconnect :)


",wiesson,Lukasa
1547,2013-09-09 11:35:11,"I like the way you think @kennethreitz. :P

It sounds more to me, however, that 64 bit cygwin is broken (maybe).
",sigmavirus24,kennethreitz
1547,2013-09-24 08:09:29,"@sigmavirus24 I have Windows 8. =) Strongly considering moving to Linux on that machine though. Anyway, off-topic.

It's worth noting that this is absolutely not reproducible on 32-bit Cygwin on Windows 7, which strongly points to a problem with Cygwin.
",Lukasa,sigmavirus24
1547,2013-09-24 17:26:24,"@sigmavirus24 - here's the dump.  Basically running python3 or python3.2 both yield stack dumps for python 3.2.  And no, this isn't my personal box that I'm running Windows 8 on.  :)

https://gist.github.com/mikewn/6688148
",mikewn,sigmavirus24
1547,2013-10-03 13:39:28,"Since there's a solution here, I'm actually tempted to close this. How do you feel @Lukasa ?
",sigmavirus24,Lukasa
1547,2013-10-04 20:17:15,"@Lukasa ?
",sigmavirus24,Lukasa
1547,2013-10-05 16:57:36,"@Lukasa sometimes people don't even look at open ones =P. Either way you're correct. I wonder if we might want a special tag for this kind of issue (where there's a problem we cannot resolve but has some kind of fix available.
",sigmavirus24,Lukasa
1547,2013-10-07 18:18:23,"@sigmavirus24 Nope. It just quits with no traceback.
",joshfriend,sigmavirus24
1547,2013-10-07 19:21:41,"Sorry @sigmavirus24, I promise to stop after this post. @ahmadia: you can email me at the address listed on my profile.
",joshfriend,sigmavirus24
1542,2013-09-24 03:27:53,"Thanks for helping me confirming the problem, @sigmavirus24!
",kqdtran,sigmavirus24
1542,2014-05-20 19:51:32,"@sigmavirus24 yup, curl fails as well. Have a ticket open to Heroku--will update if I find out anything helpful. 
",cvolawless,sigmavirus24
1537,2013-08-21 18:29:06,"@Lukasa will this interfere with the undocumented streaming upload functionality?
",kennethreitz,Lukasa
1537,2013-08-21 18:55:36,"@kennethreitz I'm pretty sure it is documented. I'd tell you, but the docs are 503ing.
",Lukasa,kennethreitz
1537,2013-08-21 18:56:33,"@kennethreitz Yup: https://github.com/kennethreitz/requests/blob/master/docs/user/advanced.rst#streaming-uploads
",Lukasa,kennethreitz
1533,2013-08-23 05:08:18,"@jaraco If you desperately need a workaround you can change the header key on L398 of models.py to be `b'Content-Type'`. That should work. I'm not expecting a huge wait for the 2.0 release, but that's really up to Kenneth.
",Lukasa,jaraco
1524,2013-08-12 12:24:47,"Mm, I'm broadly with @sigmavirus24. Seeing as you'll need to monkeypatch _anyway_ to get any of your hooks to be called, it's not a huge chore to monkeypatch this method to do the right thing. If we were going to take this, we'd have to decide to take a whole 'custom hooks' package, and that just seems like massive unnecessary complexity.

Thanks for the pull request though, and please do keep contributing!
",Lukasa,sigmavirus24
1523,2013-08-12 11:42:45,"I'm with @Lukasa. This is convenient but non-obvious frankly. Most people will expect this to return the entire header not just one portion of what was returned. Even if this had a chance of making it I would want to rename it but there unfortunately isn't a better name.
",sigmavirus24,Lukasa
1522,2013-08-30 22:29:54,"@t-8ch Here are three machines I tried it on: (2 Windows7 boxes and 1 Ubuntu)

# 

c:\Python27\Scripts>..\python.exe
Python 2.7.3 (default, Apr 10 2012, 23:31:26) [MSC v.1500 32 bit (Intel)] on win
32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.

> > > import requests
> > > requests.get(""https://nonexistant-1111111111.blib.us"")
> > > <Hangs>

Ctrl+C does not work anymore.

# 

Machine 2 (Windows 7 again):

c:\Python27>python
Python 2.7.3 (default, Apr 10 2012, 23:31:26) [MSC v.1500 32 bit (Intel)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.

> > > import requests
> > > requests.get(""https://nonexistant-1111111111.blib.us"")
> > > Traceback (most recent call last):
> > >   File ""<stdin>"", line 1, in <module>
> > >   File ""c:\Python27\lib\site-packages\requests\api.py"", line 55, in get
> > >     return request('get', url, *_kwargs)
> > >   File ""c:\Python27\lib\site-packages\requests\api.py"", line 44, in request
> > >     return session.request(method=method, url=url, *_kwargs)
> > >   File ""c:\Python27\lib\site-packages\requests\sessions.py"", line 354, in request
> > >     resp = self.send(prep, *_send_kwargs)
> > >   File ""c:\Python27\lib\site-packages\requests\sessions.py"", line 460, in send
> > >     r = adapter.send(request, *_kwargs)
> > >   File ""c:\Python27\lib\site-packages\requests\adapters.py"", line 246, in send
> > >     raise ConnectionError(e)
> > > requests.exceptions.ConnectionError: HTTPSConnectionPool(host='nonexistant-1111111111.blib.us', port=443): Max retries exceeded with url: / (Caused by <class 'socket.error'>: [Errno 10054] An existing connection was forcibly closed by the remote host)

# 

Machine 3 (ubuntu box):

> > > requests.get(""https://nonexistant-1111111111.blib.us"")
> > > Traceback (most recent call last):
> > >   File ""<stdin>"", line 1, in <module>
> > >   File ""/usr/local/lib/python2.7/dist-packages/requests/api.py"", line 55, in get
> > >     return request('get', url, *_kwargs)
> > >   File ""/usr/local/lib/python2.7/dist-packages/requests/api.py"", line 44, in request
> > >     return session.request(method=method, url=url, *_kwargs)
> > >   File ""/usr/local/lib/python2.7/dist-packages/requests/sessions.py"", line 354, in request
> > >     resp = self.send(prep, *_send_kwargs)
> > >   File ""/usr/local/lib/python2.7/dist-packages/requests/sessions.py"", line 460, in send
> > >     r = adapter.send(request, *_kwargs)
> > >   File ""/usr/local/lib/python2.7/dist-packages/requests/adapters.py"", line 246, in send
> > >     raise ConnectionError(e)
> > > requests.exceptions.ConnectionError: HTTPSConnectionPool(host='nonexistant-1111111111.blib.us', port
> > > =443): Max retries exceeded with url: / (Caused by <class 'socket.error'>: [Errno 104] Connection re
> > > set by peer)

========================================== Back to Machine 1

On Machine 1 where things hang, if I change the timeout:

> > > requests.get(""https://nonexistant-1111111111.blib.us"", timeout=5.0)
> > > Traceback (most recent call last):
> > >   File ""<stdin>"", line 1, in <module>
> > >   File ""c:\Python27\lib\site-packages\requests-1.2.3-py2.7.egg\requests\api.py"",
> > >  line 55, in get
> > >     return request('get', url, *_kwargs)
> > >   File ""c:\Python27\lib\site-packages\requests-1.2.3-py2.7.egg\requests\api.py"",
> > >  line 44, in request
> > >     return session.request(method=method, url=url, *_kwargs)
> > >   File ""c:\Python27\lib\site-packages\requests-1.2.3-py2.7.egg\requests\sessions
> > > .py"", line 335, in request
> > >     resp = self.send(prep, *_send_kwargs)
> > >   File ""c:\Python27\lib\site-packages\requests-1.2.3-py2.7.egg\requests\sessions
> > > .py"", line 438, in send
> > >     r = adapter.send(request, *_kwargs)
> > >   File ""c:\Python27\lib\site-packages\requests-1.2.3-py2.7.egg\requests\adapters
> > > .py"", line 331, in send
> > >     raise SSLError(e)
> > > requests.exceptions.SSLError: _ssl.c:489: The handshake operation timed out

I am guessing that when one does not provide a timeout, this is the place where the code hangs? (And ctrl+c is disabled ? )
",pikumar,t-8ch
1522,2013-08-31 06:01:32,"Mm, this looks like it isn't a Requests issue. I do hope you work out what's going on, though, @pikumar!

Thanks again @t-8ch!
",Lukasa,t-8ch
1515,2013-08-03 18:15:31,"Looks like these test failures are transient, I think this is good. Thanks so much for your work @schlamar, as well as all of the urllib3 people who worked on this! =)
",Lukasa,schlamar
1507,2013-08-01 01:12:45,"Here's my hang-up. It consists of two parts:
1. > 90% of our users won't ever need this as such, our existing strategy is to use Request objects as organizational throw away objects, as such we never present them to the user directly as part of a Response. What we do allow is for a user to replicate what we do in order to send a PreparedRequest. So as the API is concerned, the most import objects are the Session and Response object. Next most important are PreparedRequest objects because those are directly exposed via the Response object and finally the Request object. I fail to see how we can not just explicitly document for the user that the Request object will be mutated. We're not using a purely functional language so there's no reasonable expectation that the Request will not be mutated.
2. The second part, contingent on that first, is that when you prepare a request, you're not going to get it back and so it may be mutated. If you're using this advanced API then you should have read the docs where we can explicitly document that those objects will be mutated. Does this hamper your use case? Yes. Does it meet the needs of what is likely > 90% of our users? More emphatically, yes.

I haven't skimmed your PR because it seems (from the conversation that I've read via email) that it's very much in flux. That said, if you can sell @Lukasa and me on why we need Ruby-ish methods here, we can probably sell @kennethreitz and frankly you haven't sold me.
",sigmavirus24,Lukasa
1507,2013-08-01 01:18:10,"@kennethreitz That's what this diff does.
",erydo,kennethreitz
1507,2013-08-01 06:10:58,"@erydo Despite what it looks like above, I really did want this change. =) Just wanted the best possible form of it. I think, looking at the diff that got merged, this is probably either that or extremely close to it. Thanks so much for your work! (And for putting up with my constant badgering. :grin:)
",Lukasa,erydo
1507,2013-08-01 06:24:10,"@Lukasa I actually very much appreciate the level of feedback on this :) And I'm glad it ended up with a better solution than my original suggestion. :+1: Thanks!
",erydo,Lukasa
1503,2013-07-30 07:27:44,"@skastel I think the answer here is yes. Here's why:

If the user passes the `files` parameter, Requests will _always_ do a `multipart/form-data` upload. Providing a simple plain text string as well doesn't easily work alongside that kind of upload, at least not in the current API. For this reason, I think the user should probably get a `ValueError` explaining that `data` may not be a string if the `files` parameter is present.
",Lukasa,skastel
1503,2013-07-30 13:50:55,"@Lukasa I'd actually much prefer a `ValueError` here, it would be clear that something is wrong with the request rather than making the request without uploading the files. So rather than silently ""succeeding"" it would explicitly fail and the raised error could indicate that a `data` was a string instead of a dict. I agree it changes the API, but I don't think this is part of the documented API, is it?
",skastel,Lukasa
1503,2013-07-30 13:53:20,"@skastel part of the documented API or not, it's a behaviour some people may (possibly unintentionally) rely on. People do weird things with code you release and they don't always tell you until you change it. This is a backwards incompatible change either way you slice it unfortunately.
",sigmavirus24,skastel
1503,2013-07-30 13:54:53,"@sigmavirus24 is right. We're fascistic about the API. It should be set in stone outside of major version increases. Happily, 2.0 isn't too far away, so this is well timed.
",Lukasa,sigmavirus24
1503,2013-08-01 01:22:34,"@Lukasa recommendation?
",kennethreitz,Lukasa
1503,2013-08-01 06:02:06,"I'm strongly +1 on changing this to throw an explanatory `ValueError`. If @skastel is happy to make that change, I'll be totally happy with merging this. =)
",Lukasa,skastel
1498,2016-05-18 14:45:54,"@Lukasa To sum up, i send file with some parameters, on server side according to the request parameters i start to read file **or** return a json with code 200. UWSGI doesn't like when the body is not consumed and raise error (IncompleteRead, i deal with it).
Request returned by server is ok (200) :



But i can't read `status_code` or json ...
",yoyoprs,Lukasa
1498,2016-05-18 16:28:07,"@Lukasa I just noticed that the variable `r` is no longer found in the namespace
No NameError **!**

tcpdump (no reset):



traceback :


",yoyoprs,Lukasa
1498,2016-05-18 16:43:44,"@Lukasa You are right, now i get `status_code` and `ChunkedEncodingError` exception is raised when i try to read `r.json()`. This behaviour suits me. Thank you :)
",yoyoprs,Lukasa
1498,2016-05-18 17:58:14,"@Lukasa One last question: I think there is a bug when i try to read a custom header (the problem seems to be string encoding)

Example OK (original custom content: _TODO.md_)



Example KO (original custom content: _hackiñg.txt_)



Some times (in second case) code block on printing `Transfer-Encoding` header
I use python 2.7
",yoyoprs,Lukasa
1497,2013-07-29 05:10:19,"@sigmavirus24 Yeah, I thought about that. Unfortunately, users can send directly into a Transport Adapter (it's a supported flow), and if they did that then they would lose this validation.
",Lukasa,sigmavirus24
1495,2014-10-31 20:42:20,"@Lukasa
Thank you very much. As mostly it was not the problem of missing ressources or bad dokumentation.
",sharpshadow,Lukasa
1493,2014-01-15 09:15:37,"I actually want to revisit this issue.

A not-uncommon use case of Requests is to open connections to download _part_ of a response, without downloading the whole body. In that case, to ensure that the connection pooling logic of `Session` objects still works, you need to make sure you call `Response.close()`. In this sort of use-case it does actually seem not-unreasonable to turn `Response` objects into context managers. @sigmavirus24, thoughts?
",Lukasa,sigmavirus24
1493,2014-01-16 03:04:36,"@pepijndevos It's not very hard to make objects context managers. (Not to sound like a pompous ass who's done this a million times)

@Lukasa I think I know what you're talking about -- there have been a fair number of bug reports/stackoverflow questions about this. I'm willing (because people seem to find `Response#close` annoying) to bend on this. I still think that logically it's a bit of nonsense but from a purely utilitarian perspective, it makes sense.
",sigmavirus24,pepijndevos
1493,2014-01-16 03:04:36,"@pepijndevos It's not very hard to make objects context managers. (Not to sound like a pompous ass who's done this a million times)

@Lukasa I think I know what you're talking about -- there have been a fair number of bug reports/stackoverflow questions about this. I'm willing (because people seem to find `Response#close` annoying) to bend on this. I still think that logically it's a bit of nonsense but from a purely utilitarian perspective, it makes sense.
",sigmavirus24,Lukasa
1493,2014-01-16 03:06:58,"@pepijndevos I totally missed what you meant (contextlib provides a lot of things to help _make_ context managers). I didn't realize that it would close the thing. On that topic I'm 100% for documenting that pattern instead of making the `Response` object a context manager unto itself.
",sigmavirus24,pepijndevos
1487,2013-08-01 01:14:09,"Yes. It does. Thank you @dpursehouse 
",sigmavirus24,dpursehouse
1486,2013-07-23 16:03:22,"@Lukasa love it :)
",kennethreitz,Lukasa
1481,2013-07-23 15:15:44,"Thanks so much for doing the detective work @t-8ch! The interface is very clear, you cannot use `Response.raw` unless `stream=True`, and that has been the case for a very long time. The `stream` parameter was introduced in 1.0.0 and the note you're referencing was also added in the same release.

With such a note in place, Requests was within its rights to do whatever it wants to the `raw` parameter when `stream` isn't set to `True`. I recommend opening a PR on easywebdav to fix this problem.

Thanks for raising this issue @skestle, and thanks so much for your investigation @t-8ch! :star2: :cake: :star2:
",Lukasa,t-8ch
1481,2015-01-13 04:21:33,"@bryanhelmig your problem here is entirely unrelated to the bug originally reported. The original report is about consuming from requests' API first and then consuming from the raw attribute. This seems different to me even thought the message from the gzip library is the same
",sigmavirus24,bryanhelmig
1480,2013-08-13 20:00:55,"@Lukasa I said in my last message that's what I did.

Just to clarify - I did an `rm -fr venv && virtualenv venv`
",jsullivanlive,Lukasa
1480,2013-08-13 20:22:13,"These tracebacks make it look like you have an SSL problem. I shall now invoke Requests' resident SSL specialist by saying his name three times: @t-8ch @t-8ch @t-8ch 
",Lukasa,t-8ch
1476,2013-07-21 13:54:51,"Actually, @Lukasa, you can do that for me with this issue. :-P
",sigmavirus24,Lukasa
1476,2013-07-26 15:48:20,"@kennethreitz is this good for merging?
",sigmavirus24,kennethreitz
1475,2013-07-20 20:09:08,"@sigmavirus24 i agree, but it's so easy, might as well. It's one line :)
",kennethreitz,sigmavirus24
1475,2013-08-02 02:48:40,"@kennethreitz indeed the best problem ever. Fwiw I'd be happy to help with httpbin's PRs.

Also @kracekumar we can rebuild him faster, stronger, better. ;)
",sigmavirus24,kennethreitz
1475,2013-08-02 02:48:40,"@kennethreitz indeed the best problem ever. Fwiw I'd be happy to help with httpbin's PRs.

Also @kracekumar we can rebuild him faster, stronger, better. ;)
",sigmavirus24,kracekumar
1475,2013-08-02 11:14:45,"Uh, @kennethreitz, I think you may have merged #1509 into this instead of the other way around...

**EDIT**: No, I'm just an idiot.
",Lukasa,kennethreitz
1473,2013-08-01 01:28:41,"@kennethreitz it's failing on travis because there's a related PR on HTTPbin that needs to be merged for the tests to pass. This doesn't need to be rebased at the moment, there's no merge conflicts according to GitHub.
",sigmavirus24,kennethreitz
1469,2013-07-19 13:13:00,"Hi @Lukasa, thank you for your reply =)
I already used the way you suggest. For small files that does work, but for large ones python crashes after 99% memory usage..
The idea was to use upload streaming without copying the whole file to memory:
http://docs.python-requests.org/en/latest/user/advanced/#streaming-uploads
",timmyschweer,Lukasa
1469,2013-07-19 13:46:09,"@Lukasa i already tried that! then the boundary is missing =/
",timmyschweer,Lukasa
1469,2013-07-19 15:14:46,"@Lukasa thank you for your brain muscle action!
I investigated the apache commons Fileupload class and it ONLY accepts multipart uploads with an boundary.. Due the fact that i can't write an FileUpload class that can handle streams without boundary i don't have a solution for my problem..

Is there another way of using



without copying the file to memory?
",timmyschweer,Lukasa
1469,2013-07-19 18:45:47,"okay, would never have thought it would work that simple... Dependent on the mediatype I'm using the file upload multipart parser or directly writing the HTTP stream to disk.

@Lukasa  thx for your help!

Here's my java restlet code:


",timmyschweer,Lukasa
1466,2013-07-19 11:12:27,"Brilliant stuff @s7v7nislands, thanks so much! :stars: 
",Lukasa,s7v7nislands
1465,2013-07-19 13:06:07,"Hi @dpursehouse, thanks for this PR!

I think this would be better placed as a subheading under the `Basic Authentication` section. Do you mind moving it?
",Lukasa,dpursehouse
1465,2013-07-19 13:41:55,"@Lukasa No problem.  I just added a new commit.
",dpursehouse,Lukasa
1464,2013-07-19 08:19:51,"Awesome, thanks for this @dpursehouse! :star2: 
",Lukasa,dpursehouse
1461,2013-07-17 16:48:54,"@t-8ch did you mean for us to add it to the copy of `cacert.pem` in requests or just for @heetderks to do that for himself? If the former, I'm not sure we should be adding extra certs to that file, especially if it seems to be a server issue.
",sigmavirus24,t-8ch
1459,2013-07-15 17:53:46,"I agree with @t-8ch's first two points. (It's worth noting that the first point actually requires upstream work.) I'm not really bothered about his third, I'd be happy to leave it as-is.

On top of that, I'd want to consider:
- Provide a `Session.build_request()` function (or equivalent). The biggest problem with preparing `Request`s yourself is that you have to reimplement portions of `Session.request()`. We should make it as easy as possible to not do that.
",Lukasa,t-8ch
1459,2013-07-16 02:32:35,"Also, I've been meaning to take a crack at #1166 and @Lukasa's suggestion to provide `Session.build_request()` would make that much much easier. `mock` makes it nice but making the methods as patchable as possible is also very nice.

For the record, I haven't looked closely at any of the vcr clones, but none of them really work the way I want them too. (Yes I know, that means I should make my own. But time is precious now that I have a real job.)
",sigmavirus24,Lukasa
1459,2013-09-04 03:14:41,"@sigmavirus24 Yep, I fully agree.

I'd just like to get the go-ahead from @kennethreitz before fully continuing, in the event he thinks it's not deserving of being put into requests core.
",Anorov,kennethreitz
1459,2013-09-04 03:14:41,"@sigmavirus24 Yep, I fully agree.

I'd just like to get the go-ahead from @kennethreitz before fully continuing, in the event he thinks it's not deserving of being put into requests core.
",Anorov,sigmavirus24
1459,2013-09-05 06:52:17,"@kevinburke That looks awesome. Great job! It'll resolve some of our open issues, like #1577.

We need to propose our API changes. I really like the current clean form of `timeout`, so I think the best API is to do the following:
- Add a `requests.utils.Timeout` class (which is a simple subclass of the `urllib3` timeout class.
- Extend the `timeout` parameter to be a number or optionally an instance of the `Timeout` class.

@kevinburke, when this gets merged you should let me know, and I can either implement the Requests' side myself or code review your version of it. =)
",Lukasa,kevinburke
1459,2013-09-08 13:53:47,"@kevinburke request:

Allow the Timeout class's `__init__` method to take an instance of itself and handles `None`. If it does, we can always just easily do (on our end):



If you can do that :heart:
",sigmavirus24,kevinburke
1459,2013-09-08 14:17:30,"@sigmavirus24 The only issue there is that None has a specific meaning for timeouts - it implies never timeout, vs. some sentinel object which signifies ""use the system default timeout""
",kevinburke,sigmavirus24
1459,2013-09-08 18:05:28,"@kevinburke since this is going into 2.0 which do you think is better. Currently I believe we do not operate with any default timeout (i.e., None) so keeping that behaviour would not be a big deal. Using some ""system default"" (which I frankly wasn't aware existed) would be a breaking API change and would be okay since 2.0 allows for those.

I personally am in favor of keeping the current behaviour but I'm open to arguments for using a system default.
",sigmavirus24,kevinburke
1457,2013-07-13 09:09:57,"@t-8ch Response to your questions:




",liquidscorpio,t-8ch
1457,2013-07-13 09:16:20,"@t-8ch Let me try.
",liquidscorpio,t-8ch
1457,2013-07-13 09:21:23,"Ok, I cannot reproduce this on a fresh Fedora install, which means it's either a problem with your system setup or your network. I'm going to let @t-8ch take the lead on the system setup end because he knows it better than me. =)
",Lukasa,t-8ch
1457,2013-07-13 09:31:18,"@Lukasa Thanks for the help. I am setting up a virtenv for py3 and trying it out as per @t-8ch's sugesstion.
",liquidscorpio,t-8ch
1457,2013-07-13 09:31:18,"@Lukasa Thanks for the help. I am setting up a virtenv for py3 and trying it out as per @t-8ch's sugesstion.
",liquidscorpio,Lukasa
1457,2013-07-13 09:34:51,"@t-8ch Using python3, the problem persists


",liquidscorpio,t-8ch
1457,2013-07-15 08:56:22,"@t-8ch 
I messed up my Fedora 18 installation and did a clean install of F19; the problem persists. (Note: also persist on Mint 15). Here is the output of your code:


",liquidscorpio,t-8ch
1457,2013-07-15 10:48:35,"HEAD requests are not supposed to return any content in their body, so this is expected.
@Lukasa is right. Your installation of requests is seriously broken.
What do `requests.__file__` and `requests.__dict__` contain?
",t-8ch,Lukasa
1457,2013-07-17 17:39:22,"As and when I do figure out the situation, I will post it here. BTW, Thanks @Lukasa and @t-8ch, you have been quite helpful. :)
",liquidscorpio,t-8ch
1457,2013-07-17 17:39:22,"As and when I do figure out the situation, I will post it here. BTW, Thanks @Lukasa and @t-8ch, you have been quite helpful. :)
",liquidscorpio,Lukasa
1457,2013-08-01 11:20:08,"All right, now I have managed to get the last code posted by @t-8ch (using urllib3) to work properly and I am not getting any error even on https. So I guess my installation is now fine.

But still the original problem with requests persists. 
",liquidscorpio,t-8ch
1457,2013-08-02 01:32:16,"@drepo then please clarify what you mean by this:

> All right, now I have managed to get the last code posted by @t-8ch (using urllib3) to work properly and I am not getting any error even on https. So I guess my installation is now fine.

Because it sounds to me that the installation of requests is fine. The last code block @t-8ch posted involved trying to get `requests.__file__` and `requests.__dict__` which you couldn't previously do. Can you print that output now?
",sigmavirus24,t-8ch
1457,2013-08-02 01:38:53,"@sigmavirus24 That post was in reference to https://github.com/kennethreitz/requests/issues/1457#issuecomment-20958172 suggesting that my request installation is not good. But now the code is returning as expected.

However I am still facing the problem that I am stated in the first post.


",liquidscorpio,sigmavirus24
1453,2013-07-11 08:21:23,"Hi @alanhamlett, thanks for raising this issue!

Unfortunately, this would not be simple to do. Requests doesn't handle any of the low-level HTTP connection processing, we pass that off to [urllib3](https://github.com/shazow/urllib3). urllib3 also appears not to have any explicit hostname resolution calls in it, which means that it passes that job off to httplib in the standard library.

If httplib provided that feature this would be easy enough to plumb through, but it doesn't. This means any change made in urllib3 would have to circumvent the standard httplib hostname resolution behaviour, which would be pretty awkward. Altogether I doubt there's going to be much appetite for this feature.
",Lukasa,alanhamlett
1452,2013-07-10 08:21:15,"Hi @s7v7nislands, thanks for opening this issue!

We're always open to having documentation updates! If you feel the need to make this change, we would expect Python 2 to be the version in the documentation.
",Lukasa,s7v7nislands
1452,2013-07-10 13:40:47,"@s7v7nislands basically I think it somewhat safe to assume that people using python 3000 realize that print is now a function that requires parentheses. As for displaying data, you can use `b'[{""repository"": ""kennethreitz/requests""}]'` because on python 2 `b'string' == 'string'` unlike python 3000 so that distinction is actually good to point out.

I guess, I'm saying that the better way is to put as much subtext in as possible (if that makes sense). I've had ""improve requests documentation"" on my todo list for a while now but I keep finding it hard to find the time. I know I won't have time this weekend but ideally next weekend I might be able to get to some of this. We really need to explicitly define the behaviour differences on python 2 and python 3. For simple examples, you can stick to whichever you feel is best. Python 2 (or python 3.3) might give you the most flexibility so you can clearly illustrate  ""HEY THIS IS UNICODE!"" with a `u'string'`.

And either way, it's good practice whether writing python 2 or 3 to use parentheses when writing print statements (especially if you want your code to work on both without much effort) so tacitly enforcing that with users is a good thing. :) In all the examples for github3.py (for [example](http://github3py.readthedocs.org/en/latest/#example)) I use syntax which I'm certain will work on python 2.6+ since the library works on python 2.6+. 
",sigmavirus24,s7v7nislands
1450,2013-07-10 07:10:19,"Thanks for the pull request @matthewlmcclure! Unfortunately, I think Ian is right on this one. We haven't had this feature for a long time and it's not really one that's missed.
",Lukasa,matthewlmcclure
1449,2013-07-09 20:54:43,"To elaborate on what @t-8ch has just said, this is a known bug in Requests and has been around for some time (see #1359). Until we manage to get proper HTTPS proxying support in urllib3 there is very little we can do about it. Try following @t-8ch's suggestions for the moment.
",Lukasa,t-8ch
1449,2013-07-10 02:36:35,"I know this isn't exactly helpful but thank you for such an excellent issue report. One thing I'd ask to see, out of some level of curiosity, is which version of openssl you're running. While @t-8ch and @lukasa are 100% correct, I wonder of this isn't an SSL issue being masked by proxies.
",sigmavirus24,t-8ch
1449,2013-07-10 03:03:28,"As a member of a debug team of a broad software stack, I understand the 
agony of trying to find the root cause of bugs from customers that don't 
submit detailed bug reports. It took a good chunk of time out of my 
lunch break to prepare it, but I just thought that making it easier on 
you guys would speed things along. I really appreciate the requests 
library and how easy it makes Python web interactions. I'd like to thank 
you guys for all the work you do to keep this open source project active 
and going in the right direction. If I do find a work around, I'll be 
sure to post it here so others can find it.

Feel free to close this if you deem it necessary.

On 7/9/2013 7:36 PM, Ian Cordasco wrote:

> I know this isn't exactly helpful but thank you for such an excellent 
> issue report. One thing I'd ask to see, out of some level of 
> curiosity, is which version of openssl you're running. While @t-8ch 
> https://github.com/t-8ch and @lukasa https://github.com/lukasa are 
> 100% correct, I wonder of this isn't an SSL issue being masked by proxies.
> 
> —
> Reply to this email directly or view it on GitHub 
> https://github.com/kennethreitz/requests/issues/1449#issuecomment-20718442.
",kylestev,t-8ch
1445,2013-07-31 03:35:00,"I actually want this too.

To clarify the use case, I expect to be able to do something like this.



However, `Session.send` doesn't apply its configuration (e.g. `auth`) that way. Essentially, I'd like the ""thing that creates request objects"" independent of the Session.

In our real code, `FooApiRequests` represents a larger API request-factory creating many kinds of requests; and whose requests may get reused elsewhere for things like automated-retries-after-auth, etc.

I'd be _basically_ okay doing this:



but I think this would be even better:



In which `Session.prepare_request()` creates a `PreparedRequest` whose settings have been merged with the session's.

I'd be okay making that change and submitting a PR if it seems like a good solution. It's a non-breaking API change. @sigmavirus24 and @Lukasa, if I were to go ahead with that, would it be likely to be merged in? Any suggestions?
",erydo,Lukasa
1445,2013-07-31 03:35:00,"I actually want this too.

To clarify the use case, I expect to be able to do something like this.



However, `Session.send` doesn't apply its configuration (e.g. `auth`) that way. Essentially, I'd like the ""thing that creates request objects"" independent of the Session.

In our real code, `FooApiRequests` represents a larger API request-factory creating many kinds of requests; and whose requests may get reused elsewhere for things like automated-retries-after-auth, etc.

I'd be _basically_ okay doing this:



but I think this would be even better:



In which `Session.prepare_request()` creates a `PreparedRequest` whose settings have been merged with the session's.

I'd be okay making that change and submitting a PR if it seems like a good solution. It's a non-breaking API change. @sigmavirus24 and @Lukasa, if I were to go ahead with that, would it be likely to be merged in? Any suggestions?
",erydo,sigmavirus24
1442,2013-07-05 20:29:39,"Hi @jvanasco, thanks for opening this issue!

Have you tried `RequestsCookieJar.set_cookie()`? This should do your cookie adding. As for retrieving cookies, you should be able to iterate over the `RequestsCookieJar` to get the cookies out. Does that seem like what you need?
",Lukasa,jvanasco
1442,2013-07-06 20:57:12,"@sigmavirus24  done - https://github.com/kennethreitz/requests/issues/1443
",jvanasco,sigmavirus24
1440,2013-07-01 20:38:27,"Hi @fcurella! Thanks so much for the pull request! This is good work. :cake:

The relevant section of RFC 6265 is this one:



This suggests to me that the cookie value includes the double-quotes. Otherwise, the spec would read:



Of course, reading RFCs is a dark art, and so I could be wrong here. Nevertheless, I think we're safest if we assume that the quotes should be included. That way, if the remote end considers them semantically meaningful, it'll get them back; and if it doesn't, it should be stripping them (since it added them in the first place).

I do, however, agree that we shouldn't be _escaping_ them in the cookie values. =)
",Lukasa,fcurella
1440,2013-07-15 13:47:15,"@Lukasa: I've added https://github.com/kennethreitz/requests/pull/1440/files#L2R173 already, but I can write more. What kind of tests would like fro me to add?
",fcurella,Lukasa
1440,2013-07-15 13:51:57,"Ah, sorry, I totally missed that.

@kennethreitz this is good-to-go. =)
",Lukasa,kennethreitz
1439,2013-06-28 07:55:43,"This looks excellent @voberoi, thank you! :cake:
",Lukasa,voberoi
1437,2013-06-27 18:18:51,"Thanks so much @lukaszb! :cake:
",Lukasa,lukaszb
1434,2013-06-25 20:53:10,"Hi @chinux23, thanks for raising this issue! I agree, that looks fairly clearly wrong to me. I don't have time to fix it right now, so you should feel free to open a Pull Request if you want to fix it yourself. Otherwise, I'm marking this as Contributor Friendly until I can get around to it. =)
",Lukasa,chinux23
1433,2013-06-25 08:06:52,"Hi @jaraco, thanks for opening this issue! =)

I'll begin by addressing your actual problem. Simply put, the easiest way to resolve the password after the server has prompted for it is to not attach an auth handler to the original request, then check the response code:



If you feel as though this has too much boilerplate for you (which it would do if you do this a lot), take a look at what the Digest Auth handler does to handle 401 responses. You can easily write a Basic Auth handler that exhibits the same behaviour:



These should both allow the kind of delayed activation you want, and indeed the `handle_401()` hook should be able to block as long as necessary (e.g. to accept user input).

Hopefully these address your specific problem. Now I'll address your more philosophical questions.

One of the things that Requests desperately tries to avoid is the complexity of urllib2's password API. It's not that we don't think separating credential management and authentication is _ipso facto_ a bad idea, but the API is dire.

Requests is pragmatic. Design decisions are made with simplicity and pragmatism as the primary two concerns. This means we have to answer two questions:
1. Can this separation of concerns be implemented in a way that doesn't inconvenience the vast majority of users, who honestly couldn't care less?
2. Is this separation of concerns actually worth having in the library proper?

I'm not sure about (1), though my gut feeling is 'no'. I'm certainly open to someone proving me wrong. More importantly, I don't know that I think the answer to (2) is 'yes'. I'm prepared to be convinced, but at the moment I'm pretty comfortably leaning towards this being something that people who want it have to implement themselves.
",Lukasa,jaraco
1433,2013-06-25 16:23:01,"@Lukasa 
Thanks so much for the detailed and informative response.

I had gone down the 'handle_401' route, but didn't like it because it didn't seem reusable and seemed to combine too many factors (authentication, request/response workflow, password management), so that's why I abandoned it and ended up here.

I thought maybe I was just framing the problem wrong and sure enough, your simple example works very nicely for my needs.

Modeled after your example, I [implemented this](https://bitbucket.org/jaraco/lpaste/src/4e127b565867eb149b62d753d9c467cbfd196453/lpaste/lpaste.py?at=default#cl-139), which greatly simplified the implementation.

Regarding the separation of concerns, I believe it should be possible to provide an extensible mechanism for auth that separate concerns but degrade nicely to the simple, default pattern. I'll have to defer that work for later, though, and leave this issue closed in the meantime.
",jaraco,Lukasa
1427,2013-06-21 16:50:22,"Thanks @Lukasa.  I found out a way to do by importing CookieJar, Cookie, and cookies.  I like your way better :)  However, with your way I was not able to specify the ""port_specified"", ""domain_specified"", ""domain_initial_dot"" or ""path_specified"" attributes.  The ""set"" method does it automatically with default values.   I'm trying to scrape a website and their cookie has different values in those attributes. As I am new to all of this I'm not sure if that really matters yet.   I was able to make this work with your help. :)

   my_cookie = {
        ""version"":0,
        ""name"":'COOKIE_NAME',
        ""value"":'true',
        ""port"":None,
//       ""port_specified"":False,
        ""domain"":'www.mydomain.com',
//       ""domain_specified"":False,
//        ""domain_initial_dot"":False,
        ""path"":'/',
//        ""path_specified"":True,
        ""secure"":False,
        ""expires"":None,
        ""discard"":True,
        ""comment"":None,
        ""comment_url"":None,
        ""rest"":{},  
        ""rfc2109"":False
    }

s = requests.Session()
s.cookies.set(**my_cookie)

note: the '//' were replacements for the '#'  (comment out) because they were causing funky formatting in this post.
",mrfatboy,Lukasa
1424,2013-06-17 20:49:03,"Cool, no worries. :)

!m @sigmavirus24 
",whit537,sigmavirus24
1420,2013-06-14 16:30:37,"The example you give @Lukasa looks like this is part of RFC 6570 and is supported in [uritemplate.py](/sigmavirus24/uritemplate).

If they can come up with templates for the URIs they're handling, they can pass the construction off to my library and then pass the string returned directly into requests. Unfortunately the library isn't well-documented at the moment, but it is well-tested and it follows the RFC to the 't'. @jase1987 you should look into ""Level 3"" expansions for semi-colon prefixed paths. It should fit your needs well. An example can be found in my tests [here](https://github.com/sigmavirus24/uritemplate/blob/master/test_uritemplate.py#L131). The template which is the key for the dictionary and the expected values are what should be of interest to you.
",sigmavirus24,Lukasa
1419,2013-06-15 05:50:42,"@Lukasa is correct :)

Thanks @kevinburke!
",kennethreitz,kevinburke
1419,2013-06-15 05:50:42,"@Lukasa is correct :)

Thanks @kevinburke!
",kennethreitz,Lukasa
1418,2013-06-12 08:09:22,"@t-8ch Thanks for pointing it out, when I get my internet connection back I'll do just that. =)
",Lukasa,t-8ch
1415,2013-06-23 17:14:58,"@Lukasa 
You linked to the master branch which is a moving target which made your link obsolete by now... Please remember to always link to specific revision/tag :)

@olemoudi 
Merging headers often is asking for trouble. See http://bugs.python.org/issue1660009 for details. This is fixed in Python 3 - headers are not being merged. See http://lists.w3.org/Archives/Public/ietf-http-wg/2013JanMar/0016.html for an example of problems with merged headers.
",piotr-dobrogost,Lukasa
1414,2013-06-12 18:12:15,"@Lukasa feel free to merge something like this next time :)
",kennethreitz,Lukasa
1409,2013-06-12 15:11:14,"Sorry @rsalmond, I lost track of this briefly.

I think @sigmavirus24 has basically got the right idea here. JSON has to be the way to go, and your two options there are either to send multipart data with JSON in one part and the file in another, or to JSONify the whole thing. I'd go with his idea for now.

Btw, on the topic of nested dictionaries working without files, I don't see that behaviour:


",Lukasa,sigmavirus24
1408,2013-06-07 10:21:18,"Hi @wasw100, thanks for raising this pull request!

Unfortunately, it's not clear to me whether we want to continue supporting adding Morsels to `RequestsCookieJars`. Can I get a call on this @kennethreitz?
",Lukasa,wasw100
1408,2013-06-07 10:32:36,"Hi @Lukasa , thanks for your reply.

If we handler response cookie by ourself, Morsels to RequestCookieJar is usefull.
I use these methods in other projectes.
",wasw100,Lukasa
1405,2013-06-04 08:00:41,"Hi @borfig, thanks for this pull request!

As @t-8ch said, in its current form we can't accept this Pull Request. We do not make local changes to urllib3, so the urllib3 portions of this work would have to be contributed separately upstream. You should feel free to do that if you'd like to. As for why we have vendored urllib3 instead of using a git submodule, I'll point you at the discussion in #1384 which went over the decision in great detail. (Short answer: the standard Python repository structure does not lend itself to being a git submodule.)

Approaching this more generally, Requests has very strong opinions about SSL. Even if this feature was added to urllib3, I don't think we'd add it to the functional Requests API. Instead, I think we'd expect people to use Transport Adapters (as @t-8ch has demonstrated) to get that functionality.
",Lukasa,t-8ch
1400,2013-06-08 08:04:53,"@jam _bump_. :smile:
",Lukasa,jam
1397,2013-06-06 21:44:02,"@Lukasa Perhaps requests could wrap the exception and display that? `The server specified a chunked Transfer-Encoding response but did not send chunked data.` or something along those lines.
",Anorov,Lukasa
1397,2013-07-25 14:39:34,"Interesting! This is some awesome detective work @kracekumar! Does look like a very specific bug here, but I'm still not convinced we can neatly work our way around it.
",Lukasa,kracekumar
1396,2013-06-10 13:33:14,"Not sure what path I'd use for httpie's requirements file; there doesn't seem to be one I can spot on the PyPI page for the package. However, on a whim, I went ahead & just tried `pip install --upgrade httpie` again, and now it's working like a charm.

@sigmavirus24, I suspect you're right, that I was ending up at a mirror that was out-of-date, and now I'm either pulling from PyPI directly, or that mirror or mirrors have caught up.

Gonna close this out, as it appears to have resolved itself independently. Thanks all for your suggestions/help!
",jeffbyrnes,sigmavirus24
1391,2013-05-29 09:04:12,"Hi @revolunet, thanks for the fix!

Can I suggest that, instead of using the phrase ""status between 400 and 600"", you use the phrase ""a 4XX client error or 5XX server error response""? It makes it clearer that these errors are categories, not a continuum of numbers.
",Lukasa,revolunet
1391,2013-05-29 09:56:40,"thanks @Lukasa you're perfectly right. i'll close this one and issue another PR
",revolunet,Lukasa
1390,2016-06-09 04:27:02,"We have some unofficial plans to unveil something around async within the next year, @Lukasa and I were discussing this at PyCon.

It's something that will take some time and a lot of energy to introduce, and would no longer be a ""tacked on"" bit of functionality like the original ""requests.async"" (now known as ""grequests"") was. 
",kennethreitz,Lukasa
1390,2016-06-09 04:30:46,"I added some comments here, which I will re-iterate: https://github.com/kennethreitz/requests/issues/1390#issuecomment-224797206

---

We have some unofficial plans to unveil something around async within the next year, @Lukasa and I were discussing this at PyCon.

It's something that will take some time and a lot of energy to introduce, and would no longer be a ""tacked on"" bit of functionality like the original ""requests.async"" (now known as ""grequests"") was.
",kennethreitz,Lukasa
1390,2016-06-09 04:54:57,"@kennethreitz @Lukasa It's something that I seriously need so am happy to contribute.  Would you be willing to take help contributing to it?  I am CTO of my company and have been using python for years.
",smorin,kennethreitz
1390,2016-06-09 04:54:57,"@kennethreitz @Lukasa It's something that I seriously need so am happy to contribute.  Would you be willing to take help contributing to it?  I am CTO of my company and have been using python for years.
",smorin,Lukasa
1390,2016-06-09 07:36:17,"@Lukasa I seem to recall it sounding like defereds (perhaps with a slight (upcoming) alteration) would be the best implementation approach for us?
",kennethreitz,Lukasa
1390,2016-06-09 08:33:40,"@kennethreitz The answer to that is _maybe_. The problem with using Deferreds is that Deferreds don't make things magically async: they're literally just fancy callback containers. We can almost certainly _use_ Deferreds to build an abstraction layer on top of multiple concurrency models if that's a route we want to go.

This is a problem that really needs to be approached _very_ carefully, because as long as Python 2.7 is around we cannot unconditionally expect that the standard library will have an event loop inside it that we can use. That represents a concern: we need a way to execute _without_ an event loop. This is why Deferreds are attractive: they can execute in just such a manner because they're _literally_ just callback holders.

So one approach here would be to write an entirely callback-based HTTP/1.1 + HTTP/2 client library that uses Deferreds (probably on top of @njsmith's [h11](https://github.com/njsmith/h11) and @python-hyper's [h2](https://github.com/python-hyper/h2)).

We could then try to think about how we can integrate that into the various different event loops, though again, I'm not 100% certain of how we'd do that best. _Probably_ we'd have to write (sigh) an event-loop abstraction layer that basically defines certain functions as returning deferreds (""data_from_network""), and that then plugs into the relevant event loops. In practice, though I've said ""event loops"", we really only need two implementations to get started: one that implements the Twisted/asyncio ""protocol"" notion, and one that is synchronous and blocking (so that we continue to work in the absence of an event loop). In fact, we could even just start with the second one.

This is one of those things that it'd be really interesting to get someone like @glyph to weigh in on, in part because @glyph is strongly incentivised to help us get this right (no more treq!) and in part because he's a lot smarter than me. Certainly, however, I'd love to have a version of requests that you can either use synchronously in its current form without noticing the difference _or_ that you can use on top of ${EVENT_LOOP}. Requests provides enough goodness that we should really reimplement as little of it as possible on different platforms.
",Lukasa,kennethreitz
1390,2016-06-09 17:18:59,"@Lukasa @kennethreitz Guess the question is exactly what to support.  In a ideal world it would be a universal design that's elegant to support both, in practice if you wanted to support both a event loop model and a twisted model, then maybe it would be better to support two models one for each.  Alternatively I think a lot of people are using (Gevent/Asyncio/Trollius) so maybe work on a implementation that supports that then see how the model can be altered to include twisted?

Above you mention ""In practice, though I've said ""event loops"", we really only need two implementations to get started: one that implements the Twisted/asyncio ""protocol"" notion, and one that is synchronous and blocking (so that we continue to work in the absence of an event loop)""

My initial thoughts are is there a reason to majorly reimplement or alter the existing requests library aka (""one that is synchronous and blocking"") and just maybe cleanly refactor things under the hood so the can be shared between the ""synchronous"" implementation and the ""async"" implementation?  Or is there a motivating factor beyond that I am not aware of?

@smorin 
",smorin,kennethreitz
1390,2016-06-09 17:18:59,"@Lukasa @kennethreitz Guess the question is exactly what to support.  In a ideal world it would be a universal design that's elegant to support both, in practice if you wanted to support both a event loop model and a twisted model, then maybe it would be better to support two models one for each.  Alternatively I think a lot of people are using (Gevent/Asyncio/Trollius) so maybe work on a implementation that supports that then see how the model can be altered to include twisted?

Above you mention ""In practice, though I've said ""event loops"", we really only need two implementations to get started: one that implements the Twisted/asyncio ""protocol"" notion, and one that is synchronous and blocking (so that we continue to work in the absence of an event loop)""

My initial thoughts are is there a reason to majorly reimplement or alter the existing requests library aka (""one that is synchronous and blocking"") and just maybe cleanly refactor things under the hood so the can be shared between the ""synchronous"" implementation and the ""async"" implementation?  Or is there a motivating factor beyond that I am not aware of?

@smorin 
",smorin,Lukasa
1390,2016-06-11 02:55:28,"It's at least an interesting exercise to try and imagine what the core of requests would look like if written in the style of h2/h11. Even if in the end it turns out to not be the best approach -- it's certainly more complex than any of the existing examples, so I guess we'd at least learn something :-). (And hey @Lukasa, what's this backsliding to concrete I/O APIs as soon as the problem gets more complicated? ;-))

Wildly sketching, since I have no idea how the internals of requests actually look in any kind of detail: I guess the basic I/O operations that requests needs to do are to request a connection (possibly with TLS and possibly with its own hacks to the TLS handshake logic), to do reads and writes on a connection, maybe to set socket options, and to close a connection. And while in h2/h11 the basic state object is a Connection that manages a single socket, for requests it would be a Session that manages a socket pool.

And we'd want to make it possible to wrap this in different APIs like:
- `blocking_session.get(...)` -> returns a `BlockingResponse`
- `await coroutine_session.get(...)` -> returns a `CoroutineResponse`
- `twisted_session.get(...)` -> returns a `Deferred` that when fired returns a `TwistedResponse`

(the different response classes would differ in the API they exposed for streaming reads of the response body)

Since the user is going to be calling methods on individual response wrapper objects, probably we want to let each of these wrappers handle the I/O coupling for each connection. So maybe the API would look something like this:



I dunno, maybe this is only interesting to me :-). Certainly it would be a lot of work, but the result might be rather nice...
",njsmith,Lukasa
1390,2016-06-11 21:32:13,"> Ah, right, I'm assuming that requests dropping support for python 2 is still like... 3-5 years away (is that wrong?)

Heh, that's part of what this issue is about. =D

I think this whole issue is going to be part of my list of ""things I want to deal with in the next year"". It's my genuine belief that Requests needs its own async story. Right now I'm mostly interested in soliciting opinions about how we could do it (which is why I also tagged @glyph and @shazow).

However, from my perspective, all options @kennethreitz will entertain are on the table. If @kennethreitz is willing to consider dropping Python 2 support at some time soon, I'm willing to consider that approach, even if we don't end up doing it. 
",Lukasa,kennethreitz
1390,2016-06-11 21:32:13,"> Ah, right, I'm assuming that requests dropping support for python 2 is still like... 3-5 years away (is that wrong?)

Heh, that's part of what this issue is about. =D

I think this whole issue is going to be part of my list of ""things I want to deal with in the next year"". It's my genuine belief that Requests needs its own async story. Right now I'm mostly interested in soliciting opinions about how we could do it (which is why I also tagged @glyph and @shazow).

However, from my perspective, all options @kennethreitz will entertain are on the table. If @kennethreitz is willing to consider dropping Python 2 support at some time soon, I'm willing to consider that approach, even if we don't end up doing it. 
",Lukasa,shazow
1390,2016-06-17 03:34:45,"@kennethreitz - so are you thinking purely in terms of asyncio.Future objects?  You... _might_ be able to vendor in trollius to get a basic asyncio event loop going.
",glyph,kennethreitz
1390,2016-07-09 02:34:19,"Since @Lukasa was so kind in his assessment of my abilities I figure I should weigh in pretty substantively.  (As always, my estimate last month of ""a couple of days"" was hilariously optimistic, given that I had another conference and a week of vacation to do after that...)

Let me first address some of the questions around the possibility of using `Deferred` directly to facilitate asynchrony.

There's a stand-alone version of Deferred available from &lt;https://pypi.python.org/pypi/deferred&gt;.  I've recently been added as a package index owner, and we (Twisted generally) plan to actively maintain this going forward, and hopefully split it out of Twisted entirely.  This is a much smaller dependency than asyncio or one of its various clones, mostly due to the fact that it does not do any I/O; it's _just_ the callback abstraction.  Right now it still includes stuff like `DeferredFilesystemLock` but we are probably going to remove that and leave it in Twisted proper.

There are two problems that `Deferred` could potentially solve within `requests`.  Let me start with the simpler one: providing a front-end for Twisted - replacing `treq`.  Whatever asynchronous solution you end up going with, it should be easy enough to have a `requests.frontend.twisted` which translates whatever not-yet-available-result.

The other, more substantive one, is providing a common abstraction between the different layers within requests to compose with each other.  @Lukasa laid out some of these layers [in this comment](https://github.com/kennethreitz/requests/issues/1390#issuecomment-225361421).  My understanding is pretty superficial, but at the highest level, `requests.get` wraps `Session.request` and expects it to block, and then returns its result.  What you would be using `Deferred` for here would be to have a single return type which could stand in for ""expects it to block"", and anything that currently blocks could simply start returning a `Deferred`, which would be fired by the external I/O machinery feeding bytes into it somehow, h11-style.

If you have to continue supporting Python 2, `Deferred` is definitely a friendlier abstraction for this than a `Future`-alike, since `asyncio.Future`'s extremely spare API is specifically designed to be a primitive that you interact with indirectly, behind the syntactic convenience of `await`.  (I think it's still a better internal abstraction )

The question then is, having retrofitted everything internally to use this new non-blocking abstraction, how do you still support blocking code?  This is actually a bit more generic than just `Deferred`, because any non-blocking approach will have the same problem, but I'll discuss it in the language of `Deferred` because it provides an easier shorthand :).  This breaks down into two sub-problems: how do you deal with external callers calling into e.g. `requests.get`, and how do you facilitate plugging in a blocking implementation of something that lives in the middle of the stack?

For the former, the answer is really simple: you just have a wrapper that lives at the edge, which presents a blocking API, and when called, dispatches into an event loop which just does I/O until the `Deferred` returned by the lower level has fired.  Doing this in a natively asynchronous program is problematic because the whole point is you want to share an event loop and you don't want to use it re-entrantly (and Twisted, in particular, has never really been retrofitted all the way to make loops easy to instantiate, although we'll get there one day).  But your ""event loop"" can just be a little function that does blocking `socket.recv` / `socket.send` and stuffs the results into the underlying I/O layer; you don't need full-blown event-driven concurrency in this layer.

To plug something in in the middle is a bit more complex in implementation, but essentially the same conceptually: at every possible public integration point, you have a wrapper for the layer below like the one I just described, and you have a wrapper for the layer above which just calls the blocking thing and then wraps the result in an already-fired `Deferred`; this is sort of what [`succeed`](https://twistedmatrix.com/documents/16.2.0/api/twisted.internet.defer.html#succeed) is for.

The original pioneer of the [synchronous `Deferred` approach](https://github.com/radix/synchronous-deferred), @radix, would tell you to use his newer library, [effect](https://github.com/python-effect/effect), to thread the non-blocking needle through the core of `requests`.  And he might even be right!  This is really a matter of taste; functionally the way you'd be writing code and dealing with the edges of the system where things become blocking or attach to a concrete event loop is similar.  Effect has a bit more action-at-a-distance which can be a little tricky to reason about - which is by design, the separation of intents and performers is one of its architectural features - but which also facilitates dispatching to different backends which requests might need.

I'll probably have more thoughts on this later, but I should probably yield the floor at this point - we've still got 11 months before this needs to roll out, right? :)
",glyph,Lukasa
1384,2013-05-24 11:41:39,"Git submodules aren't even just a bad idea for the reason you posted @Lukasa, they include the entire repository not just the library we're looking for. In other words, let's say we vendored urllib3 as a submodule, the structure would be as follows



So then we wouldn't be able to do: `from .packages import urllib3` or `from .packages import charade`. We couldn't even do `from .packages.urllib3 import urllib3` because the repository itself is not a package.
",sigmavirus24,Lukasa
1384,2013-05-24 11:56:25,"@kennethreitz: Of course not. =) I was explaining why we do it, rather than suggesting that we'd ever change. Given @tasuk is relatively new to this, though, it seemed like explaining why lots of libraries do this.
",Lukasa,kennethreitz
1384,2013-05-24 11:57:28,"@Lukasa it could be fixed by structuring the project like logilab structures theirs (example: [astng](https://bitbucket.org/logilab/astng/src)) but I really really hate that structure.

And @kennethreitz, we're just trying to teach our new friend. (At least I hope he'll be our friend.)
",sigmavirus24,kennethreitz
1384,2013-05-24 11:57:28,"@Lukasa it could be fixed by structuring the project like logilab structures theirs (example: [astng](https://bitbucket.org/logilab/astng/src)) but I really really hate that structure.

And @kennethreitz, we're just trying to teach our new friend. (At least I hope he'll be our friend.)
",sigmavirus24,Lukasa
1384,2013-05-24 13:13:21,"@Lukasa, thank you for a very comprehensive answer!

So, basically, dependency management is also broken in Python :)

Coming from an inferior language where we're facing similar problems, I was hoping someone solved dependencies in Python. Maybe it's one of those problems that can never really be solved. I guess you'd have to be able to have several versions of one package installed in parallel and choose which one you're importing. Polluting imports with version numbers doesn't seem like the right thing to do though.

As for submodules, I know they're problematic (been there seen that), but I don't see why the package root shouldn't be the project root (though Python people don't seem to do that). That looks like lesser evil than including whole 3rd party libs in your repo. I'm a bit surprised that's standard practice in Python-world.

It's an interesting situation, each of the solutions sucks in its own specific way. In PHP world, we use Composer (our version of ""pip -r""). Sometimes we get impossible-to-resolve conflicts and sometimes dependency updates break stuff (though there should be no api changes for minor releases), but we learned to live with it.

One more question &ndash; what is the reason to use requirements.txt for `pytest` and `invoke`, but not for `urllib3`?

@sigmavirus24 yes I'll be your github-friend, @kennethreitz thanks for the cookie!
",tasuk,kennethreitz
1384,2013-05-24 13:13:21,"@Lukasa, thank you for a very comprehensive answer!

So, basically, dependency management is also broken in Python :)

Coming from an inferior language where we're facing similar problems, I was hoping someone solved dependencies in Python. Maybe it's one of those problems that can never really be solved. I guess you'd have to be able to have several versions of one package installed in parallel and choose which one you're importing. Polluting imports with version numbers doesn't seem like the right thing to do though.

As for submodules, I know they're problematic (been there seen that), but I don't see why the package root shouldn't be the project root (though Python people don't seem to do that). That looks like lesser evil than including whole 3rd party libs in your repo. I'm a bit surprised that's standard practice in Python-world.

It's an interesting situation, each of the solutions sucks in its own specific way. In PHP world, we use Composer (our version of ""pip -r""). Sometimes we get impossible-to-resolve conflicts and sometimes dependency updates break stuff (though there should be no api changes for minor releases), but we learned to live with it.

One more question &ndash; what is the reason to use requirements.txt for `pytest` and `invoke`, but not for `urllib3`?

@sigmavirus24 yes I'll be your github-friend, @kennethreitz thanks for the cookie!
",tasuk,Lukasa
1384,2013-05-24 13:13:21,"@Lukasa, thank you for a very comprehensive answer!

So, basically, dependency management is also broken in Python :)

Coming from an inferior language where we're facing similar problems, I was hoping someone solved dependencies in Python. Maybe it's one of those problems that can never really be solved. I guess you'd have to be able to have several versions of one package installed in parallel and choose which one you're importing. Polluting imports with version numbers doesn't seem like the right thing to do though.

As for submodules, I know they're problematic (been there seen that), but I don't see why the package root shouldn't be the project root (though Python people don't seem to do that). That looks like lesser evil than including whole 3rd party libs in your repo. I'm a bit surprised that's standard practice in Python-world.

It's an interesting situation, each of the solutions sucks in its own specific way. In PHP world, we use Composer (our version of ""pip -r""). Sometimes we get impossible-to-resolve conflicts and sometimes dependency updates break stuff (though there should be no api changes for minor releases), but we learned to live with it.

One more question &ndash; what is the reason to use requirements.txt for `pytest` and `invoke`, but not for `urllib3`?

@sigmavirus24 yes I'll be your github-friend, @kennethreitz thanks for the cookie!
",tasuk,sigmavirus24
1384,2013-05-24 13:19:06,"Actually @tasuk it isn't necessarily common practice to vendor dependencies. @kennethreitz uses it here because of how incredibly popular requests is and what a headache it would be to have different versions of dependencies floating around.

On the other hand, most of my libraries (the ones that have dependencies) use pip to install the dependencies since (for the most part) the libraries I use I actively help develop, so I can anticipate changes that my cause issues.

As for requirements.txt, we use that for [Travis CI](travis-ci.org) to install dependencies for testing requests. Those aren't installation dependencies at all. Since that's only used for testing, and we only tests requests, we have no need to use our vendored libraries' requirements.txt files simply because we aren't running their test suites.

Some people us requirements.txt to also determine what needs to be installed for their package in general, but that isn't it's only use, just like not all projects vendor dependencies. :)
",sigmavirus24,kennethreitz
1384,2013-05-24 13:24:51,"@sigmavirus24 ah cool, that explains everything!
",tasuk,sigmavirus24
1384,2013-05-24 13:28:15,"@sigmavirus24 has covered most of the points I was going to. =)

Like him, my projects overwhelmingly use Pip to manage dependencies, for the exact same reason. I have written a few requests plugins, and because I am a requests contributor I can predict problems that might come up. Though even then, I pin to a minimum Requests version, so I'm not totally immune. =P

You raised making the package root be the project root. There is a very good reason we don't do that in Python, and that reason is because in Python, folders define your namespaces. If you look at the Requests project root, you'll see it looks like this:



If we put an `__init__.py` here, turning this into the package root, the following lines of code would become possible:



We don't want people to be able to import any of these things: they aren't part of the library itself. But we need to be able to keep them under version control as well, because they are still very important. For this reason, we use a subfolder as the root of our package, which allows us to easily manage the other package metadata.

(A note: the fact that we have our testing dependencies in `requirements.txt` annoys me: I don't do it in my own projects. However, it's really minor so I just let it go. =) )
",Lukasa,sigmavirus24
1384,2013-05-24 19:41:32,"@Lukasa, that all makes perfect sense.

Even in PHP-land it's usual to separate things in a similar way (see e.g. [guzzle](https://github.com/guzzle/guzzle)). But we have funky class-loading mechanisms instead of adhering to the directory structure for namespacing, allowing us to be a bit more lenient about which files go where.

To reiterate my previous point - wouldn't it be wonderful if pip was able to manage packages locally in a recursive way? (""I need tag 3.1 of package 'x' and commit 45fe8a2 of package 'y', can you put them in requests/packages for me?"")
",tasuk,Lukasa
1384,2013-05-28 20:22:28,"@Lukasa 

> A note: the fact that we have our testing dependencies in `requirements.txt` annoys me (...)

I'm glad it annoys you as it annoys me, too :) If these are testing only dependencies I think the file should be named `requirements-testing.txt` or something similar to make it clear what kind of dependencies it enumerates.
",piotr-dobrogost,Lukasa
1380,2013-05-22 21:09:50,"Thank you @sigmavirus24. I'm aware of this workaround, I brought this up only because it broke existing code of mine (so possibly others' who haven't updated to 1.2.x).

I'm :+1: for a `__serialize__`. Magic methods are sometimes nice.
",woozyking,sigmavirus24
1380,2013-05-22 21:18:13,"I'm with @sigmavirus24 here. This is a pain, but the Case-Insensitive Dict is awesome.

As for magic methods, I think JSON will never add one. JSON should be round-trippable, e.g: `json.loads(json.dumps(obj)) == obj`. Allowing arbitrary serialisation breaks that functionality.

@woozyking, for what it's worth, if you find yourself doing this a lot, you can use a [custom JSON encoder with the `default` method defined`](http://docs.python.org/2/library/json.html#json.JSONEncoder.default). Define the `default` method such that it turns the Case-Insensitive Dict into a dict. Slightly less code repetition if you want it.

Sorry that we caused you this inconvenience, but I think things are going to stay as they are. Thanks so much for raising the issue though! :cake:
",Lukasa,sigmavirus24
1380,2013-05-22 21:20:50,"@sigmavirus24 a quick test with the latest simplejson indicates that they don't have built-in resolution for this case.

@Lukasa fully understand.
",woozyking,Lukasa
1380,2013-05-22 21:20:50,"@sigmavirus24 a quick test with the latest simplejson indicates that they don't have built-in resolution for this case.

@Lukasa fully understand.
",woozyking,sigmavirus24
1380,2013-05-22 21:58:56,"@Lukasa that's true. But it would still be convenient. :-P
",sigmavirus24,Lukasa
1374,2013-05-21 19:21:47,"@Lukasa OK, I will.  Sorry for misleading you by my comment.
",papaeye,Lukasa
1374,2013-05-21 19:25:11,"@papaeye: Not at all! I mislead myself by being stupid. =D
",Lukasa,papaeye
1372,2013-05-21 21:25:30,"@kennethreitz are you using a clean virtual environment to test it? Using both old and new versions of virtualenv I found the same results: https://gist.github.com/sigmavirus24/5623374
",sigmavirus24,kennethreitz
1367,2013-11-05 04:00:02,"@ionrock why not do something like [Betamax](https://github.com/sigmavirus24/betamax) instead of trying to use pickle?
",sigmavirus24,ionrock
1367,2013-11-05 22:38:57,"@sigmavirus24 Thanks for pointing out [Betamax](https://github.com/sigmavirus24/betamax)! I'd argue the serialize/deserialize_response functions would be a great addition to the Response object. If the goal is to avoid pickle, this seems like a great option. Sometimes pickle is a good option though, so I still believe it is worthwhile to add the functionality. I've tested the patch @tanelikaivola and they work well. What else would need to be done to potentially get them merged? Obviously some tests would be helpful. I'd also be happy to see about adding the serialize/deserialize code from Betamax if that would be alright with @sigmavirus24. 

Let me know if there is a good way to proceed. I'd be happy to put the code together. 

@sigmavirus24 thanks again for the Betamax suggestion. I will be switching [Cache Control](https://github.com/ionrock/cachecontrol) to use that methodology.
",ionrock,sigmavirus24
1367,2013-11-06 03:28:06,"@sigmavirus24 Fair enough. I'm most definitely not the expert on everything that a Response object, so I'm happy to take your word for it. When I looked at what a response object contained it was semi-complex as it was a wrapper around a urllib3 HTTPResponse, which is in turn a wrapper around a httplib response, which assumes the content comes directly from a socket (not a file like object). 

With that being the case, it seems to make sense to support pickling of Response object. Again, I'm happy to write some tests for @tanelikaivola's patches if there is a consensus. Otherwise, I'd like to understand where it falls short. At the very least I'd like to try and fix it for myself. 

Thanks for the discussion!
",ionrock,sigmavirus24
1367,2013-11-08 12:41:42,"@ionrock :shipit: 
",sigmavirus24,ionrock
1367,2013-11-08 23:19:38,"@sigmavirus24 I have no clue what a detective squirrel means (that is a detective squirrel right?), but I'll assume there will be comments later ;)

The tests are pretty thin, so I'm happy to add more tests. That patch is just my baseline.
",ionrock,sigmavirus24
1364,2013-06-07 05:07:30,"@sigmavirus24 - this has now been added in shazow/urllib3#187.  Is there a regular ""timetable"" for when you pull in a new `urllib3`?  Or is it just whenever they have a release?
",eteq,sigmavirus24
1363,2013-05-16 17:22:29,"@sigmavirus24 does 6e76ab7 do the job?  I'm relatively new to git so it wouldn't surprise me if I mucked something up.
",dave-shawley,sigmavirus24
1363,2013-05-16 17:39:33,"@dave-shawley it most certainly does! Thank you kind sir!

:+1: for @kennethreitz when he comes around these parts again
",sigmavirus24,dave-shawley
1363,2013-05-17 15:50:25,"Congrats @dave-shawley :cake:
",sigmavirus24,dave-shawley
1359,2013-07-19 09:20:08,"@schlamar I've just tried and it works! Great!


",astratto,schlamar
1357,2013-05-12 00:58:38,"I'm personally in favor of #1327 because it doesn't require re-sorting and reversing the list of adapter prefixes every-time we make a request (we use `get_adapter` every time we call `request` which is every time we make a request). Sorry @Zoramite 
",sigmavirus24,Zoramite
1357,2013-05-12 01:00:30,"@sigmavirus24 no worries :) That is a better approach, I should have done a search for it before just blindly fixing it :)
",Zoramite,sigmavirus24
1357,2013-05-12 01:04:15,"Thanks for the request anyway @Zoramite ! Your other one looks good though. :cake:
",sigmavirus24,Zoramite
1356,2013-05-12 08:29:05,"I think this PR is in great shape, but for reasons that are nothing to do with the code I'm slightly more lukewarm to it than @sigmavirus24 is. I consider subclassing the Transport Adapter to be a legitimate part of Requests' API: the documentation talks about it, some interfaces require it, etc. etc.

With that said, I'm +0.5 on this. I think it'd be a good addition to the library and it doesn't look like it'll break anything, but I don't think there is anything wrong with subclassing the adapter either. On balance I'd rather take it than leave it. That said, I don't want adding arguments to the constructor of the adapter to become a trend. =P
",Lukasa,sigmavirus24
1356,2013-05-12 17:01:24,"@Lukasa but sub-classing the adapter just to rewrite one (or two methods) to include an extra parameter seems a bit like over-kill to me. There are times when subclassing is appropriate and times when it isn't.

We may be better off doing something else though:

What if we rewrite `HTTPAdapter` to be more intelligent. We'll modify the `__init__` method to take `**kwargs` that are meant only for the `poolmanager` initialization, so something like:



Because really we have never used `self.config` on the `HTTPAdapter`, nor have we ever really needed to set `_pool_connections` or `_pool_maxsize` twice (which we do both in `__init__` and `__setstate__`). `init_poolmanager` really doesn't need parameters to work well.
",sigmavirus24,Lukasa
1353,2013-05-07 19:07:44,"@Lukasa why you no protect the critical variable?
",sigmavirus24,Lukasa
1353,2013-05-07 23:39:49,"@Lukasa You are still reading in chunks and you have no control while that is going on, and timing of your socket close request depends on state of communication, not good. Imagine a long polling, push message use case, or connection about to timeout in 10 seconds, still you cannot drop it.

For sockets, simply calling close from another thread is a common practice, method should be thread safe of course.
",mua,Lukasa
1347,2013-05-04 08:17:06,"@sigmavirus24: If it is that's awesome.
",Lukasa,sigmavirus24
1338,2013-05-01 18:44:56,"@Lukasa rebase ;)
",sigmavirus24,Lukasa
1338,2013-05-01 19:11:23,"@Lukasa the tests fail on python3 where you have `'accept'.encode('ascii')` because of the new CaseInsensitiveDict (#1339)
",sigmavirus24,Lukasa
1338,2013-05-01 20:51:23,"@cdunklau That was a great idea, and is now done.
",Lukasa,cdunklau
1338,2013-06-21 17:20:55,"@Lukasa It's a shame this PR is a breaking API change in Python 3. I've really been looking forward to the next major release of requests when you can finally include it. A big thank you for the excellent work!
",paparomeo,Lukasa
1336,2013-05-01 16:45:02,"New option after a [conversation](https://botbot.me/freenode/python-requests/msg/2960213/) with @gazpachoking (which I would link to if botbot.me was working; _edit_ added the link since it's now up and running):

Since extracting the cookies before the hook could result in cookies being stored that hook authors might not want stored, we can safely move the extraction to after all redirects have been handled because then the history will be available. We also know that the history and the responses in the history will be correct because if there were redirects, the hooks are called on those responses too. So we can do something akin to:



This way the cookies are set and expired in the right order and only the cookies the hook author wants us to see are extracted. We're in the clear with this because the Auth Handlers properly also manage history (at least the built-in ones do and requests-kerberos does too. I'll check ntlm later).
",sigmavirus24,gazpachoking
1336,2013-05-02 07:22:58,"@sigmavirus24: If I understand this right, we'd also have to add code to `DigestAuth` to propagate cookies for the redirects it handles. Correct?
",Lukasa,sigmavirus24
1336,2013-05-02 13:56:06,"@Lukasa the cookie extraction would take place on the history. DigestAuth won't require any changes at all. It already maintains history and we'll be parsing that history. 
",sigmavirus24,Lukasa
1336,2013-05-04 20:54:38,"@sigmavirus24 I think so, yeah.
",Lukasa,sigmavirus24
1336,2013-05-28 17:04:09,"@Lukasa's edit is exactly what's going on. I'm currently in the middle of a move and don't have time to work on this at the moment. Anyone who wants to tackle this using @gazpachoking's and my instructions above can and I'll be happy to code-review it if you want me to do so before sending a pull request.
",sigmavirus24,gazpachoking
1336,2013-05-28 17:04:09,"@Lukasa's edit is exactly what's going on. I'm currently in the middle of a move and don't have time to work on this at the moment. Anyone who wants to tackle this using @gazpachoking's and my instructions above can and I'll be happy to code-review it if you want me to do so before sending a pull request.
",sigmavirus24,Lukasa
1336,2013-06-02 18:30:26,"Could anyone review this change ? Perhaps @sigmavirus24 ?
",elricL,sigmavirus24
1336,2013-06-06 08:36:55,"Sorry @ericL, both @sigmavirus24 and I have just moved house and don't have domestic internet connections. In addition, I also get lousy mobile data reception in my new house. I can only get EDGE, which means I can't really tether to my mobile phone either.

I think we'll just have to sit on this for a little while until @sigmavirus24 and I have better access and more time.
",Lukasa,sigmavirus24
1335,2013-06-08 10:13:12,"@Lukasa thank you! Btw, you accidentally pushed a merge conflict :(
",va1en0k,Lukasa
1334,2013-05-30 21:07:55,"@kennethreitz Can you restart https://travis-ci.org/kennethreitz/requests/builds/7518571 ? It appears I can't do it without making another commit.
",rcarz,kennethreitz
1334,2013-06-06 08:48:10,"@rcarz That's odd, vanilla requests throws exceptions when I do a get on `HTTP://www.google.com/`. TravisCI does too.

I'm not sure you're right, @Anorov. As @rcarz points out, this change here only affects redirects. You can see this by looking at the Travis CI output for this change, where a test with upper-case scheme fails.

#1385 is more comprehensive than this fix. I need to sit down and confirm whether #1385 covers all cases this fix does.
",Lukasa,rcarz
1334,2013-06-06 16:55:44,"@Lukasa Ah yeah, you're right, sorry. I only looked at the changes and not the full file.

@rcarz Web servers and DNS servers should always be treating hostnames case insensitively, yes. So it doesn't really matter if `requests` lowercases it or not. I think it would just go along well, aesthetically, if the scheme is also lowercased.

Either way, I think this should all be resolved with one pull (whether it be this one or the other, I'm not sure). I think all schemes should be strictly lowercased early on, before processing any further.

If you place the changes made in this commit, essentially `url = '%s://%s' % (scheme.lower(), uri)`, somewhere early on in `Session.send`, I think every bird would be killed with one stone (so to speak). Anyone disagree?
",Anorov,rcarz
1334,2013-06-06 16:55:44,"@Lukasa Ah yeah, you're right, sorry. I only looked at the changes and not the full file.

@rcarz Web servers and DNS servers should always be treating hostnames case insensitively, yes. So it doesn't really matter if `requests` lowercases it or not. I think it would just go along well, aesthetically, if the scheme is also lowercased.

Either way, I think this should all be resolved with one pull (whether it be this one or the other, I'm not sure). I think all schemes should be strictly lowercased early on, before processing any further.

If you place the changes made in this commit, essentially `url = '%s://%s' % (scheme.lower(), uri)`, somewhere early on in `Session.send`, I think every bird would be killed with one stone (so to speak). Anyone disagree?
",Anorov,Lukasa
1333,2013-04-29 19:47:51,"@sigmavirus24 So it doesn't break pickles. The loaded session pickle just uses the original's dict:


",cdunklau,sigmavirus24
1333,2013-04-30 08:28:35,"Yeah, I was under the impression that doing what @gazpachoking described would cause a `ValueError`, but (totally insanely IMHO) it doesn't. So we should just match that behaviour.
",Lukasa,gazpachoking
1328,2013-04-26 15:55:05,"Thanks @gazpachoking you rock! :cake:
",sigmavirus24,gazpachoking
1327,2013-04-25 12:58:53,"After testing it, it is probably not going to be as painless as this.

:+1: Nice work @ambv 
",sigmavirus24,ambv
1327,2013-04-25 14:15:38,"@kennethreitz the idea behind this is that you want to ensure the longest match is the first you'll find. So if you have an adapter for `http://example.com/endpoint1` and `http://example.com` and you issue a reqeust for `http://example.com/endpoint1/resource` you're going to get the first one and find it faster. Of course for the simplest case your performance is sort of awful, but most of those cases won't see people using adapters and as Lukasa mentioned, I doubt anyone is going to have too many adapters mounted in the first place. :)
",sigmavirus24,kennethreitz
1327,2013-04-25 14:31:40,"@sigmavirus24's explanation was always my understanding of how you intended transport adapters to behave. If mount-order becomes the condition, then your adapters become surprising. For instance, changing the `Session` constructor from 



to:



causes all your HTTPS traffic hits the wrong adapter. There's no functional effect because all that logic is handled underneath, but it sure is strange.
",Lukasa,sigmavirus24
1327,2013-04-25 17:35:09,"So if we do order added then we'd have to do something else when we find the adapter @kennethreitz that would be akin to what @Lukasa's last pull request did. So the benefit of ordering would make no difference.

This just ensures that any adapters you have with freakishly long prefixes are matched first and returned.
",sigmavirus24,kennethreitz
1327,2013-04-25 17:35:09,"So if we do order added then we'd have to do something else when we find the adapter @kennethreitz that would be akin to what @Lukasa's last pull request did. So the benefit of ordering would make no difference.

This just ensures that any adapters you have with freakishly long prefixes are matched first and returned.
",sigmavirus24,Lukasa
1327,2013-05-01 18:19:11,"Rebased the PR.

The status here, as described by @sigmavirus24 in [comment 6](https://github.com/kennethreitz/requests/pull/1327#issuecomment-17009475) and confirmed by @Lukasa in [comment 7](https://github.com/kennethreitz/requests/pull/1327#issuecomment-17010424), is that the existing prefix-based `mount` syntax



always suggested that the longest match will be used, for example `get('http://github.com/about/us/')` will always match rule 3 (never the default http:// nor rules 1 or 2), `get('http://gittip.com')` will always match rule 1 (never the default http://), and so on.

The `mount()` invocation order does not matter, only the length of the matching prefix.
",ambv,Lukasa
1327,2013-05-01 18:19:11,"Rebased the PR.

The status here, as described by @sigmavirus24 in [comment 6](https://github.com/kennethreitz/requests/pull/1327#issuecomment-17009475) and confirmed by @Lukasa in [comment 7](https://github.com/kennethreitz/requests/pull/1327#issuecomment-17010424), is that the existing prefix-based `mount` syntax



always suggested that the longest match will be used, for example `get('http://github.com/about/us/')` will always match rule 3 (never the default http:// nor rules 1 or 2), `get('http://gittip.com')` will always match rule 1 (never the default http://), and so on.

The `mount()` invocation order does not matter, only the length of the matching prefix.
",ambv,sigmavirus24
1324,2013-04-26 10:57:12,"@gazpachoking seems to have fixed this.
",sigmavirus24,gazpachoking
1321,2013-04-24 13:27:08,"@sigmavirus24 Thank you. I will fix it ASAP.
",shaung,sigmavirus24
1321,2013-04-24 15:25:28,"@sigmavirus24 Fixed :-)
",shaung,sigmavirus24
1321,2013-05-01 19:22:18,"@sigmavirus24 Just tested against the new master after my PR was merged, this issue is not fixed.
",cdunklau,sigmavirus24
1321,2013-05-02 05:28:00,"@kennethreitz Thanks! I'm going to do it tonight.
",shaung,kennethreitz
1321,2013-05-02 15:57:19,"@kennethreitz  Ready for merging (=´ー｀)ノ
",shaung,kennethreitz
1320,2013-04-24 15:32:33,"Actually, @ambv is right. Here's the source code for `get_adapter()`:



This is awkward, because I've provided Transport Adapter information in the past that directly contradicts this behaviour. I think we need to fix this, because the docs behaviour should be correct. I'm happy to take a swing at this.
",Lukasa,ambv
1320,2013-04-24 15:52:20,"I am sincerely sorry @ambv. That'll teach me to work from memory ever again. 

Here are my thoughts about this with the code above:
- We could collect a list of matching adapters instead of returning the first one we find. The problem is then deciding which adapter to use
- We could maintain two separate adapters registries: 1) user-created 2) default. The user-created adapters would be the first to be searched through and if there's a match in them we could then return that. If none of those match we would then search the default adapters and if nothing matches from there raise the `InvalidSchema` error. To preserve the API we could make `adapters` a property. The `@adapters.setter` method would then only set adapters on the user-created dictionary. The returned information would then be best represented as a list of two-tuples where the user-created items come first and is then followed by the default. This gives an intuitive idea of the overall ordering of discovery of adapters. This, however, would break the case where someone tries to do `session.adapters['http://']`
- We could create our own `AdaptersRegistry` object which behaves like a dictionary, i.e., has the `__setitem__`, `__getitem__`, `get`, `set`, &c., methods, and does the search for us. Then we just maintain that as the `adapters` attribute.

I could be vastly over-thinking the problem though.
",sigmavirus24,ambv
1319,2013-04-25 20:21:28,"Yup, upgrading to 1.2 fixed it. (Sorry for the noob issue.) Thanks, @Lukasa. 
",jmakeig,Lukasa
1317,2013-04-23 14:09:29,"@sigmavirus24 dependency walker gives me two missing dlls, but irrevelant ones.
I agree with you, and came to the conclusion that py2exe is having some problems packing requests dependencies.
Running py2exe indicates some missing modules, corresponding to requests modules (urllib3, etc.)
Using modulefinder I've found where the import of those missing modules are. The issue seems to be that requests is using try statements to test python version (2 or 3) and use the corresponding import.
Now, I'm using python 2.7 and this should be working. But it's not. And I cannot figure out why or how to fix it without messing woth requests source code...

@dmckeone : Wich version of Python are you using?
",fedeanna,sigmavirus24
1316,2013-05-01 19:38:05,"@sigmavirus24 No problem!  I've actually never done a rebase before.  I think it went ok, but let me know if I should do something differently.
",dmckeone,sigmavirus24
1316,2013-05-01 21:04:57,"Well, looks like the rebase didn't go properly.  `from_key_val_list()` should be returning an `OrderedMultiDict`.  I'm going to review all the changes again.

@sigmavirus24 Since this has gotten a little messy with my screwed up rebase, should I just create a separate pull request and close this one?
",dmckeone,sigmavirus24
1316,2013-05-01 21:30:12,"@gazpachoking Yes, definitely.  That was actually my first step.
",dmckeone,gazpachoking
1316,2013-05-01 22:55:30,"@cdunklau I also noticed that `CaseInsensitiveDict` wasn't caught at any place that checked `isinstance( ... , dict)`, so I altered all of those checks to use `collections.Mapping` instead.  As for the sequence check, that looks ok to me as well, happy to integrate it in this PR, but for now I haven't included it.
",dmckeone,cdunklau
1316,2013-05-01 23:46:52,"@cdunklau After some more thought about your changes, I'm not sure if it would handle all inputs in the same way as the previous code.  If a `value` was a generator, then `all()` would exhaust it, and the setting of `item` would not work correctly.  Perhaps that is why `list()` is used?  Your checks for `basestring` are good though, so I included those in a new commit.
",dmckeone,cdunklau
1316,2013-05-02 00:51:01,"@cdunklau Agreed.  Made the commit.
",dmckeone,cdunklau
1316,2013-05-02 01:11:47,"@cdunklau Updated.  I had initially kept the same Exception message on purpose, but since it all needs to be updated, I suppose it is better to update the language.
",dmckeone,cdunklau
1316,2013-05-02 05:11:50,"Discussed a bit more with @cdunklau on IRC, we are thinking that keys from arguments in a request should still continue to override the same keys (rather than add more of those keys) from the session param.

Still not sure if there should be some way to specifically add to the existing session keys for a given request, maybe if the request argument is explicitly a multidict.

Other opinions are certainly needed, as I'm not sure if I'm considering all use cases. I'm thinking though that the most common would be to want to override a session key rather than add a second same key to a dict.
",gazpachoking,cdunklau
1316,2013-05-02 07:27:45,"I agree with @gazpachoking and @cdunklau, overriding already set keys is the way to go. Sadly, that'll need special-case code.
",Lukasa,gazpachoking
1316,2013-05-02 07:27:45,"I agree with @gazpachoking and @cdunklau, overriding already set keys is the way to go. Sadly, that'll need special-case code.
",Lukasa,cdunklau
1315,2013-05-01 18:37:48,"@kennethreitz done. 

Let me know if something is wrong, I'm doing rebase first time.
",reclosedev,kennethreitz
1308,2013-04-12 14:13:20,"@Lukasa Thanks for the quick answer. I've looked at your post and I think it's a good start to solve my problem maybe (which is setting the ciphers, not the SSL version :p)

The VerifiedHTTPSConnection wraps the socket and that's where I need to provide ciphers (this is the only way apparently). I'm not sure i'll be able to create a pool and then use it in a adapter, all without having to copy/paste existing code (I'm looking for a clean way of doing it)

Many thanks again
",damiengermonville,Lukasa
1308,2013-04-15 07:39:50,"@shazow I'm afraid that adjustments have to be made in VerifiedHTTPSConnection but also in urllib3/util.py ssl_wrap_socket() so it can give the ciphers argument to ssl.wrap_socket().
",damiengermonville,shazow
1308,2017-02-23 13:05:54,"@Lukasa, I had to enable only the AES256-SHA in my case. 



I just copy / pasted the original solution. ",athoik,Lukasa
1307,2013-04-11 21:30:01,"@sigmavirus24 That's not how character encodings work. Encoding as CP-1252 gives a series of bytes: decoding those bytes as UTF-8 will give you another 'unicode' string, but its characters will be meaningless. If you encode to CP-1252, you can only ever meaningfully decode as CP-1252.
",Lukasa,sigmavirus24
1305,2013-04-11 12:31:10,"+1 to this and @Lukasa 's suggestion
",sigmavirus24,Lukasa
1305,2013-04-11 15:19:06,"@Lukasa Good idea. I'm not too good at copy, I'd probably just confuse the hell out of somebody; so if anybody has suggestions as to what to update the preamble to, I'm open to suggestions :D
",michaelhelmick,Lukasa
1305,2013-04-11 22:39:04,"@kennethreitz Do _yoooou_ have suggestions on how to introduce `requests_oauthlib` into the docs :smirk: 
",michaelhelmick,kennethreitz
1305,2013-04-12 00:22:06,"@kennethreitz I've made changes to the Other Authentication section, let me know what you think!
",michaelhelmick,kennethreitz
1303,2013-04-10 17:26:08,"@Lukasa False https://github.com/kennethreitz/requests/blob/master/requests/sessions.py#L121
",kennethreitz,Lukasa
1303,2014-02-12 02:35:39,"@danc86 for what it's worth, @Lukasa took care of that for you. =)
",sigmavirus24,Lukasa
1300,2013-04-10 14:36:28,"@sigmavirus24 this issue is different than #1301. Here, I just expected the `max_redirects` work per request, because it is the obvious way. I don't want to create a `requests.Session` to be able to set `max_redirects`.
",iurisilvio,sigmavirus24
1300,2013-04-10 14:44:07,"To continue what @Lukasa explained, `max_retries` was an option pre 1.x but was explicitly removed with malice of forethought during the refactor. Really, when you make a specific request there are things that pertain to that specific request, e.g., certificate verification, whether that request should follow redirects, etc. but not how many times that request should be sent. A request is a single object. A browser may retry a request a maximum number of times, but a request is not a browser, a session is the closest thing to a browser you will ever find in requests (but it is not a browser).

That said, this is a feature request and combined with the fact this feature was already removed and that we're in a feature freeze this will remain closed.
",sigmavirus24,Lukasa
1297,2013-04-14 16:54:46,"@ssbarnea's pull request was merged closing this issue.
",sigmavirus24,ssbarnea
1296,2013-04-08 17:03:44,"Good point @sigmavirus24. Fixed my wording.
",sursh,sigmavirus24
1294,2014-10-21 12:42:15,"@sigmavirus24 

Here's a gist that I created to give more context: https://gist.github.com/jgillmanjr/fb4431bec403de48cf22

I ultimately realized that it was the cert on the testing box that wasn't passing verification. But I'd still like to know if I was trying to get the exception properly or if the exception that is being returned could use some fixing.

Thanks again!
",jgillmanjr,sigmavirus24
1294,2014-10-22 22:19:55,"@sigmavirus24 Not a problem.

It seems when I  run it from the interactive shell, it sort of works:



However, it looks as though I'm using 2.3.0

/// Break ///

Upgraded to 2.4.3 and still seeing the same thing.
",jgillmanjr,sigmavirus24
1294,2014-10-23 12:03:21,"@sigmavirus24 Well, I _may_ have figured it out.

I forgot that I actually initially installed pip via apt. This subsequently required the package `python-requests` which was only at version 2.3.0. I realized this once I started getting issues with requests after I did a pip upgrade on it and pip proceeded to not work very well any more.

Now that I've removed the packages, installed pip via the `get-pip.py` script, I am not recieving any more errors:



I might have to report a bug to the package managers.

Sorry for the time sink!

-Jason
",jgillmanjr,sigmavirus24
1294,2014-10-23 13:17:02,"@sigmavirus24 If you care to weigh in, here is a link to the bug report: https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=766481
",jgillmanjr,sigmavirus24
1294,2014-10-23 15:37:24,"@sigmavirus24 Thanks as usual for bringing me in! :)
I have investigated a bit and the problem seems related to pyasn1. See my reply
https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=766481#10
Unfortunately I can check deeply right now...
",eriol,sigmavirus24
1294,2014-11-02 02:35:39,"@sigmavirus24 It seems to be somting related to pyasn1: I can reproduce on a virtualenv and @jgillmanjr confirmed it. Can I link the Debian Bug with this one? Or you prefer a new issue since the TypeError is only due pyasn1?
",eriol,sigmavirus24
1294,2014-11-02 03:09:45,"@sigmavirus24 `<class 'requests.packages.urllib3.exceptions.SSLError'>` is what I get when I run `print type(e.message)`
",jgillmanjr,sigmavirus24
1292,2014-06-26 07:45:10,"@Lukasa: Sorry for posting irrelevant comment but i was facing problem, thats why i have posted my cUrl call.
",miteshsc,Lukasa
1289,2013-07-22 07:52:40,"@Lukasa Thanks for the suggested solution. Although, I'm not using SSL in my requests. 
",PerfectedTech,Lukasa
1289,2013-07-22 09:10:04,"@Lukasa, It very well could be an error with the script and not the library. The odd thing here is that the address in question is completely accessible.

Here's a traceback:


",PerfectedTech,Lukasa
1289,2013-07-22 14:33:39,"The issue was related to urllib3 and AWS ELB. It was resolved when routing the request without the ELB. Hardly a desirable option... Regardless, thanks for your help @Lukasa .

""First, ELB has internal ""feature"" of closing all incoming HTTP connections that do not get response within 60 seconds. I.e. if your client executes GET ... and waits for more then 60 seconds - ELB will close the connection. This timeout is currently unconfigurable through amazon API and even not advertised in Amazon docs. ""
http://tech.zarmory.com/2013/02/handling-tmeouts-with-amazon-elastic.html
",PerfectedTech,Lukasa
1289,2013-08-22 07:50:56,"@Lukasa No. Just a locally run Python script that does simple HTTP requests. I'm on a Kubuntu GNU/Linux x86_64 desktop.
",mnemonicflow,Lukasa
1289,2013-08-22 11:42:54,"@Lukasa I've reinstalled requests after I deleted it first, and installed it with <b>sudo pip install git+git://github.com/kennethreitz/requests.git</b> . I get exactly the same TypeError, that is: 


",mnemonicflow,Lukasa
1289,2013-08-22 11:49:01,"@Lukasa httplib.HTTPConnection.getresponse() doesn't accept any keyword arguments in Python 3.3 

See:
1) http://docs.python.org/3.3/library/http.client.html?highlight=httpconnection#http.client.HTTPConnection.getresponse
2) http://hg.python.org/cpython/file/3.3/Lib/http/client.py#l1101

The comments are incorrect in /usr/local/lib/python3.3/dist-packages/requests/packages/urllib3/connectionpool.py line 290:



I'll try to send a fix to urllib3.
",mnemonicflow,Lukasa
1289,2013-12-23 23:49:13,"See what I don't get is that most these seem to be URLs that are unreachable at times, and I've tried IPs locally that are unreachable but otherwise _might_ work. I never even get close to the `getresponse` code, and I have the same concern as you @Lukasa .
",sigmavirus24,Lukasa
1289,2013-12-23 23:53:54,"@bodiam The comment is misleading. Python 3.3 doesn't take keyword arguments on `getresponse()`, which would also throw a `TypeError`. That causes the `TypeError` reported in all of these stack traces. But it should be immediately caught and passed into the next block. 

For some _insane_ reason that seems not to be happening here. This bothers me because I've never seen it, @sigmavirus24 has never seen it, and when I try isolated tests that attempt to reproduce this problem I can't do it.

This seems impossible, which means it's a really nasty bug.
",Lukasa,sigmavirus24
1289,2013-12-23 23:59:48,"@Lukasa You're right. The comment is incorrect, it always goes to the second (the 2.6) part, I assume because the conn is a http.client.HTTPConnection (I checked, it is)

I'm also not sure why this error happens, but like I said, it seems to only happen in case of a connection error. I'll take a look once it occurs again, hopefully within the next days! Thanks so far guys!
",bodiam,Lukasa
1289,2013-12-28 19:00:22,"Yeah, that's roughly what I thought you'd say. It's a shame this trace back is so misleading. I'll investigate whether there is some way to strip the irrelevant bits of the TB in situations like this, because @sigmavirus24 and I have spent many more hours than I'd like chasing this issue. 
",Lukasa,sigmavirus24
1289,2016-04-26 19:31:22,"Thanks @Lukasa for response
I check with Jira API, they are in denial that they have changed anything about those apis.
I want to double check from my side that it Is nothing related to python (which is owned by me), there is no setting in python which I should use to correct this.. Before I can ask Jira guys again firmly that everything Is correct from my side. 

So was looking for expert knowledge.

BTW.. this is the header I am using to call those Jira restful APIs
",sandeepnassa,Lukasa
1286,2013-04-16 18:02:30,"@Lukasa, the binary can't hear you.
",sigmavirus24,Lukasa
1285,2013-04-02 22:00:59,"@sigmavirus24 
Could you provide more details/a transcription of the IRC conversation?
I created a really minimal VM of ArchLinux (base installation + python from the repos + requests from pip and from the repos) and everything worked fine.
",t-8ch,sigmavirus24
1285,2013-04-02 22:06:59,"@t-8ch

The following is the start of the conversation: 
https://botbot.me/freenode/python-requests/msg/2523881/

You can see I worked through it with upmauro until we realized that his 
version of python 3 had been compiled without support for `ssl`.  
(https://botbot.me/freenode/python-requests/msg/2523881/ to save you some 
reading)

I know that in one of the recent changes I saw in urllib3 the following:



I suspect the fact that no exception was raised was because of the silent 
failure of importing the `ssl` module. I obviously have to attempt to build a 
system in which the ssl module would be unavailable in a version of python to 
be absolutely certain, but judging by the behaviour it seems a fair 
conclusion.
",sigmavirus24,t-8ch
1284,2013-04-02 13:31:37,"@KamilSzot why would you want the unprepared request in the first place? You can do this very easily on your own and that aside, you can not send an unprepared request to `Session#send` or the underlying adapter. The changes don't make any sense from that point of view. Beyond that, the ability to return the unsent request was **deliberately** removed during the refactor for the 1.x release
",sigmavirus24,KamilSzot
1284,2013-04-02 18:42:15,"@KamilSzot would you mind closing this?
",sigmavirus24,KamilSzot
1281,2013-04-02 12:11:31,"@kennethreitz why reintroduce this behaviour? It was a documentation mistake on my part.
",sigmavirus24,kennethreitz
1273,2013-04-03 13:28:35,"I agree with @lukasa. In fact to expand a bit on his point about the last statement I'll say this: 

We emulate browser behaviour in some instances because in spite of the relevant RFC some servers will only behave ""correctly"" where ""correctly"" is how browsers expect it to behave. In those cases, RFC be damned, that's how we have to behave but at no point is requests a replacement for a browser, or a programmatic browser. Were that the case, we would have to keep a session history tracing back to the very first request. We provide histories for individual requests because being able to know that a redirect occurred is very important. It isn't so important on a session.

I was and remain frankly -0 on this because this existed well into 1.x (I think until 1.1.0) but when we asked if anyone was using it, no one replied until now. If it isn't used, it is just causing code-smell and will only cause confusion for future editors of and contributors to the project.

@Lukasa's solution is elegant and works very well. Even easier might be a hook like so:



Admittedly I don't have the time to test that, but I'm 90% sure it will work. :)
",sigmavirus24,Lukasa
1273,2013-04-03 17:38:58,"Thanks for the examples; I'll consider them if this PR doesn't go through.

@sigmavirus24: When/where was it asked if anyone was using this response property? I didn't even hear about it until I started pulling straight from github (rather than PyPI).
",rowedonalde,sigmavirus24
1270,2013-03-29 13:18:31,"@Lukasa 

Thanks ;)
Glad to be helpful.
",makto,Lukasa
1269,2013-03-29 11:34:04,"@Lukasa 

Thank you for your reply :)
Session object works well most of the time on my machine. This exception occurs only in rare cases.
I don't think it has anything to do with the installation.
Never mind, it didn't bother me right now. Just want to help find some potential bugs :)
",makto,Lukasa
1269,2013-03-29 12:50:55,"@makto could you perhaps give us more relevant information, such as the version of requests you're using and the context (in code) of the exception?
",sigmavirus24,makto
1269,2013-03-29 13:15:25,"@sigmavirus24 
I traced the error and thought it could be brought by `max_redirects`'s missing in **attr**.
I pickled the session object first and then unpickled it. I believe the max_redirects is missing during the process.

See my [pull request](https://github.com/kennethreitz/requests/pull/1270)
",makto,sigmavirus24
1265,2013-04-08 17:35:36,"- What does the following code show you (in the relevant venv)?
  (And does the file exist)



_Edit:_
Do you have `REQUESTS_CA_BUNDLE` set in your environment?
<\/edit>
- Does this happen with plain requests (`requests.get(""https://httpbin.org"")`?
- Where did you get python27 from?
- Does it work with the default python26?
- Which version of bandersnatch are you using?
- Does your bandersnatcher change the location of the cert bundle? (via the `verify=` parameter)

I can't reproduce this.

@sigmavirus24 My ubuntu container was slow to install :-)
",t-8ch,sigmavirus24
1258,2013-03-22 21:40:16,"@sigmavirus24 something like the following:


",geoff-kruss,sigmavirus24
1255,2013-03-22 16:02:32,"@Lukasa I can't wait until all the chunks are there, because this is a file upload server and it's imperative that only the current chunk from the client stays in memory at any one given time. If they were all being saved, writing a generator or a file-like object would be a cakewalk. 

@sigmavirus24 I can't reliably provide `Content-Length`, as I'm accepting uploads from users and I can't know for certain the size of the files they're uploading. They could easily upload a 4GB file and lie to me about the content length being 1024B. 

Your example makes a lot more sense of the problem than I've seen so far, so kudos on that! Essentially, it would seem that I'd have to implement a blocking queue and a thread to make this happen:



Something like the above would seem to be able to solve my problem, right? Essentially, when a new file comes down the pipe, an upload request is generated in another thread so that it won't block the main thread. Immediately after being created, it attempts to get from the queue, which blocks because it's empty. Then, when the upload handler receives a chunk, it puts it into the queue, which is read by the request. The put will block if there's more than one chunk in the queue, so this will manage memory well. This will continue until the upload is done. Once it's done, we simply set `is_running` to `False`, which breaks the iterator's loop and the thread (presumably) exits gracefully.

The only problem I can see so far is trying to handle the case in which a request failed. We'd have to check in the `on_file_chunk` method and break the whole process if the request failed. Does this seem to work? Like I said, I'm  a bit new at this whole generator thing, but I understand threading fairly well. 
",naftulikay,Lukasa
1255,2013-03-22 16:02:32,"@Lukasa I can't wait until all the chunks are there, because this is a file upload server and it's imperative that only the current chunk from the client stays in memory at any one given time. If they were all being saved, writing a generator or a file-like object would be a cakewalk. 

@sigmavirus24 I can't reliably provide `Content-Length`, as I'm accepting uploads from users and I can't know for certain the size of the files they're uploading. They could easily upload a 4GB file and lie to me about the content length being 1024B. 

Your example makes a lot more sense of the problem than I've seen so far, so kudos on that! Essentially, it would seem that I'd have to implement a blocking queue and a thread to make this happen:



Something like the above would seem to be able to solve my problem, right? Essentially, when a new file comes down the pipe, an upload request is generated in another thread so that it won't block the main thread. Immediately after being created, it attempts to get from the queue, which blocks because it's empty. Then, when the upload handler receives a chunk, it puts it into the queue, which is read by the request. The put will block if there's more than one chunk in the queue, so this will manage memory well. This will continue until the upload is done. Once it's done, we simply set `is_running` to `False`, which breaks the iterator's loop and the thread (presumably) exits gracefully.

The only problem I can see so far is trying to handle the case in which a request failed. We'd have to check in the `on_file_chunk` method and break the whole process if the request failed. Does this seem to work? Like I said, I'm  a bit new at this whole generator thing, but I understand threading fairly well. 
",naftulikay,sigmavirus24
1255,2013-03-25 16:09:05,"@sigmavirus24 In your example, how then would I pass data received in `on_file_chunk` to `generate_data`?

Since I imagine that the call to `requests.put` blocks until completed, I would imagine that this code would essentially immediately hang. Looks like I'm working with threads.
",naftulikay,sigmavirus24
1252,2013-03-27 21:30:27,"@Lukasa We recently had a discussion about header encoding in [shazow/urllib3#164](https://github.com/shazow/urllib3/pull/164#issuecomment-15366629) at which you might want to look.
",t-8ch,Lukasa
1252,2013-03-28 02:40:12,"If we ensure everything is bytes, this will work well for Python 2 because `str`s are `bytes` objects. In python 3 this seems to produce an issue like @t-8ch mentioned. Naturally it's perfectly fine for there to be multiple header values and there are no bizarre characters in headers (and cannot be if I remember the spec properly) so the coercion to whatever will be fine. You might think this falls on our shoulders because it doesn't seem that too many urllib3 users have reported this issue, but you're wrong.

The problem with doing this is exactly the case where we're reading binary data which is a very common use case. If we're provided a file (or file-like object). We have no way of knowing if it's binary data or not and images and the like can't be coerced to text. This makes me think that the burden lies on urllib3 to coerce everything together.

Either way, I feel obligated to leave [this](http://nedbatchelder.com/text/unipain.html) behind.
",sigmavirus24,t-8ch
1252,2013-03-28 10:34:26,"Yeah, I was excluding Requests' behaviour for a moment, and just trying to nail down what urllib3 should be doing. Then we could change Requests to program to that interface. =)

Thomas, I'm also quite happy with your proposal there. If @shazow thinks that's the way it should go, the fix belongs outside urllib3. :fireworks:
",Lukasa,shazow
1252,2013-03-28 16:47:29,"@sigmavirus24 I got about 9 hours, haha. And alright, and if he doesn't bump on your first request; we'll start a trending topic on Twitter ;D
",michaelhelmick,sigmavirus24
1252,2013-03-30 05:05:55,"@sigmavirus24 I'd like to treat urllib3 as more of an expected-input-expected-output library, and Requests to do the ""do silly thing to input to make behaviour more user-friendly"" stuff. Does that make sense?
",shazow,sigmavirus24
1252,2013-04-02 18:26:31,"Possibly in order to punish us (:wink:), requests-oauthlib does not work on Python3 if you upload files. That's because Requests uses `encode_multipart_formdata` from urllib3, which returns the content-type as bytes. @shazow: is that intentional? If so, I can work around it here. If not, I can offer you a PR to fix it.
",Lukasa,shazow
1252,2013-05-15 09:08:18,"@Lukasa, thank you. When I can use `requests.post()` I use it :)
",marselester,Lukasa
1247,2013-03-19 20:10:47,"+1 to @Lukasa's comment :)
",kennethreitz,Lukasa
1247,2013-03-21 12:59:52,"I'll third @Lukasa's comment.
",sigmavirus24,Lukasa
1246,2013-04-16 13:33:58,"@Lukasa 
""Explicit is better than implicit"" does not really apply to this at all. If someone downloads RELEASE then it's because he wants RELEASE and he knows that he will get RELEASE and what RELEASE means. Let me summarize with another quote - ""All problems in computer science can be solved by another level of indirection""
",piotr-dobrogost,Lukasa
1246,2013-04-16 13:44:32,"More simply put, this is @kennethreitz's project and he can do as he pleases and he doesn't need to explain his actions to anyone. If he would rather not do this then that is his call.
",sigmavirus24,kennethreitz
1246,2013-04-16 15:53:13,"@sigmavirus24 <3
",kennethreitz,sigmavirus24
1242,2013-03-11 14:25:43,"Sorry @Lukasa I've been pretty sick for the past few days. I'll take a look later this week I hope. I would think though that the standard string operations would be enough and provide less overhead than a regular expression. 
",sigmavirus24,Lukasa
1242,2013-03-12 12:20:09,"@sigmavirus24 All this can be done with standard string operations, but you would have to implement parts of what the re module already has. It would be easy if we could just do a s_auth.lower() on the whole thing and replace what we need, but the case on the rest of the string is important.

The way I see it there are 2 choices. Either we use the re module, or we add code to do a case insensitive replace in requests itself. Basically its a choice between overhead (which I'm not convinced is much) and code complexity. :)
Pisses me off that web server developers don't read the damn standards before implementing something :angry: . 

P.S. Get better soon! :D
",gabriel-samfira,sigmavirus24
1242,2013-03-22 19:52:01,"@kennethreitz you need to update .travis.yml
",sigmavirus24,kennethreitz
1242,2013-03-28 03:55:54,"@kennethreitz this looks good.
",sigmavirus24,kennethreitz
1241,2013-03-19 14:00:49,"@schlamar actually I believe urllib3 handles getting it via the environment variables for us, so there really shouldn't be an API change necessary.

I'm also :-1: on this
",sigmavirus24,schlamar
1236,2013-06-13 21:22:16,"Thanks, @miikka. I notice that part of the documentation is inside a ""Note:"" -- to me it looks like that's basically an implementation detail. I'd think when I specify a timeout I want it to be a timeout on the whole thing, so maybe in this case the documentation can simply be changed to match reality?

Note that I don't think the above affects the issue in any way -- we still want this to be a `requests.Timeout`.
",benhoyt,miikka
1236,2013-06-14 08:18:41,"Yeah @miikka, you're right, it isn't true. Sometimes.

We confusingly use `timeout` in two different ways. For non-chunked requests, we use `timeout` as a parameter on the underlying HTTP connection, which applies it just as httplib would. For chunked uploads, we use it as a timeout on getting the connection from the connection pool, and then apply no timeout to the connection itself (see #1422). I'll look into getting this mess sorted out, but if you want to try to tackle it yourselves please do. =)
",Lukasa,miikka
1236,2014-08-20 23:43:18,"Thanks @miikka for the explanation that let me reliably reproduce this error using HTTPBin's Drip endpoint:



I'm working around as @kaisarea suggested, catching both the `RequestException` exception and the `socket.timeout`.
",johnboxall,miikka
1236,2015-08-21 14:00:08,"Ok @Lukasa , thanks! I need to implement a ""retry"" logic and I can't use urllib3 retries because I need more control. So I needed to know which exceptions should I catch, but I'll catch `Exception` anyway and catch others to do custom stuff for each one.
",fjsj,Lukasa
1236,2015-12-03 08:28:00,"@Lukasa We use 2.8.1 from https://pypi.python.org/pypi/requests
",sbv-csis,Lukasa
1233,2013-03-06 17:58:57,"Yeah, and re @schlamar's concern: https://github.com/shazow/urllib3/pull/56#issuecomment-14412603 would indicate we'd have to wait longer than I think you'd like to.
",sigmavirus24,schlamar
1233,2013-03-27 18:30:15,"Tonight, I'm going to put together a changelog as best as I can. I post it here, and after @Lukasa wraps up #1252 I think we should cut 1.2 tonight.
",sigmavirus24,Lukasa
1227,2013-03-02 09:47:53,"I'm +0 on this. As you said @dmedvinsky, it's a minor refactor. I have no objection to it being in the library, and it's an internal API really, so not too problematic. I'm happy to go with whatever @kennethreitz feels is right here.
",Lukasa,dmedvinsky
1222,2013-02-28 22:11:06,"Ah @t-8ch sorry for the confusion. I must have misread @Lukasa's response to #1221 last night (or had some rather wild dreams of what it said). Thanks for the correction.
",sigmavirus24,t-8ch
1221,2013-03-11 22:19:07,"@dalanmiller what version are you trying to install?
",sigmavirus24,dalanmiller
1220,2013-03-01 23:54:46,"Ah I see the issue. @Lukasa's comment could be losely misunderstood as meaning setting `allow_redirects` to False would raise an exception when a redirect was found. A pattern that could work for you in it's stead is this:



Where I'm making an assumption that you're returning the response without any evidence of that ;)
",sigmavirus24,Lukasa
1220,2013-03-01 23:57:15,"@sigmavirus24 you've implemented pretty much exactly what I have on my screen right now!

Feel free to close this ticket if you don't think the variable naming is misleading, my immediate concern is addressed.
",mjallday,sigmavirus24
1220,2013-03-02 09:45:27,"@sigmavirus24: [We already do](https://github.com/kennethreitz/requests/blob/master/requests/models.py#L656).
",Lukasa,sigmavirus24
1219,2013-03-03 06:27:22,"Uh, @kennethreitz, did you intend to merge this? Your comment seems like you weren't going to, but then you kinda did...
",Lukasa,kennethreitz
1215,2013-02-25 10:32:57,"@sigmavirus24 sure, this once :)
",kennethreitz,sigmavirus24
1213,2013-02-25 10:30:46,"I agree with @Lukasa. 

However, the reverse lookup is something I've wanted to add for a while:


",kennethreitz,Lukasa
1211,2013-02-27 18:41:00,"@bluefoxicy @Lukasa is this still an open issue? It looks like its being resolved for the opener offline.
",phildini,Lukasa
1211,2013-02-27 18:47:00,"@phildini, @bluefoxicy is simply providing sensitive information to @Lukasa so that he can get a better idea of what is happening in his specific case. The issue of dumping content is a moot one, IMO, but do-able as @Lukasa suggested. This feature has been requested several times before (unsuccessfully obviously) and combining its track record with our feature freeze, it likely won't be added to requests.

It could be done in the form of a response hook (beyond what @Lukasa mentioned), e.g.,



Also since @bluefoxicy has claimed this to be invalid on his own accord, I'm closing this issue.
",sigmavirus24,Lukasa
1209,2013-02-22 06:58:39,"@Lukasa 
And how do you explain this:


",schlamar,Lukasa
1209,2013-02-22 08:02:20,"@Lukasa After reading http://curl.haxx.se/mail/lib-2010-03/0018.html I would say that the current behavior is broken in requests! Doing `GET https` on a proxy is non-standard behavior.

However it should be fixed with https://github.com/shazow/urllib3/issues/139 (assuming the cert verification issue will be solved) so I think this issue can be left as closed.
",schlamar,Lukasa
1209,2013-02-22 08:11:16,"@schlamar: I think you're right too, though I don't have the time right now to really explore it.

As and when urllib3 gets sorted I'll probably give you a nudge because it sounds like your network is set up to test this. =D
",Lukasa,schlamar
1209,2013-02-22 17:18:58,"@Lukasa Please also nudge me :)

Being able to access an HTTPS site through an HTTP(S) proxy is a critical requirement for me, and I have the environment to test it.

FYI, my workaround right now is to read stdout from `curl` in a subprocess. Yuck :p
",maxbane,Lukasa
1209,2013-02-26 22:07:45,"@schlamar That would be one option. I actually ended up using pycurl, the python bindings for libcurl.
",maxbane,schlamar
1209,2013-03-04 07:49:32,"@Lukasa urllib3 seems to be fixed with https://github.com/shazow/urllib3/pull/139. But requests on top of it is still not doing the certificate verification. See my comment (which includes a fully automated test): https://github.com/shazow/urllib3/pull/139#issuecomment-14367341 Any idea?

@maxbane Mind if you run the test mentioned above on your setup? :)
",schlamar,Lukasa
1209,2013-03-04 08:28:50,"@Lukasa Found the issue, see my changes at https://github.com/schlamar/requests/tree/new-urllib3-api.
",schlamar,Lukasa
1209,2013-07-19 08:02:11,"@maxbane @ciphercast There is a requests HTTPS proxy ready version to test, can you give it a try: https://github.com/shazow/urllib3/pull/170#issuecomment-21234629

@sigmavirus24 Anyone else on your list? :)
",schlamar,sigmavirus24
1208,2013-02-27 16:10:28,"@Wilfred, I'm not sure why you submitted a pull request. Also I'm not sure why this issue stayed open when there were two sufficient solutions for fixing this.
",sigmavirus24,Wilfred
1208,2013-02-27 16:34:14,"@sigmavirus24 The two solutions given are not documented anywhere. It would be nice to at least document how to do this.

That being said, it's a shame that a simple GET to a URL becomes ~5 lines of code in requests v1.0, when it was just one line before.
",Wilfred,sigmavirus24
1204,2013-02-20 16:03:38,"@t-8ch this is probably just due to not sending 'CONNECT'
",sigmavirus24,t-8ch
1204,2013-02-21 09:47:48,"Excellent, that demonstrates the bug.

As @t-8ch and @sigmavirus24 pointed out, we currently cannot use HTTPS over proxies because we do not support the CONNECT verb. This is a known bug, and we're waiting on urllib3 before we can resolve it.
",Lukasa,t-8ch
1204,2013-02-21 09:47:48,"Excellent, that demonstrates the bug.

As @t-8ch and @sigmavirus24 pointed out, we currently cannot use HTTPS over proxies because we do not support the CONNECT verb. This is a known bug, and we're waiting on urllib3 before we can resolve it.
",Lukasa,sigmavirus24
1198,2013-04-09 09:49:14,"@Lukasa, I'm not sure you do need to consider that -- Kenneth said [here](https://github.com/kennethreitz/requests/pull/1219) that Requests explicitly shouldn't support retries as part of its API.
",benhoyt,Lukasa
1198,2013-04-09 23:05:47,"@sigmavirus24, I agree string parsing in exceptions is a terrible idea. However, urllib3's MaxRetryError already exposes a `reason` attribute which contains the underlying exception (see [source code](https://github.com/kennethreitz/requests/blob/master/requests/packages/urllib3/exceptions.py#L38)). So you can get what you want with `e.args[0].reason`.

So continuing with the example above, `e.args[0].reason` is an instance of `socket.error`:


",benhoyt,sigmavirus24
1198,2014-02-08 13:18:06,"@Lukasa thanks, I'm refreshing myself on the backlog here. If I get a chance to dive in I will definitely reach out.
",ksnavely,Lukasa
1198,2014-02-19 11:54:42,"@shazow great, I'd be game to if I can find the cycles. I'll ping if I have anything.
",ksnavely,shazow
1195,2013-02-14 14:01:10,"@Lukasa 

What's wrong with designating one branch, where development of new features could be halted without hindering development in other branches?
",piotr-dobrogost,Lukasa
1193,2013-02-17 22:55:30,"So it turns out that there is no simple way to get around this Syntax Error. `six` has a good solution, but we currently don't vendor it in. The way I see it, we have three options:
1. Use the version of `six.py` in urllib3. This is bad: `six.py` is an implementation detail in urllib3, which is itself an implementation detail of Requests. -1
2. Add the bit of `six.py` that we need to our `compat.py`. This has a minimal effect on our code, but is a bit weird. +0
3. Vendor in a copy of `six.py`. This is the cleanest, but adds a whole new Python file from which we only need one line. Probably a small perf hit too. +0

@kennethreitz, @sigmavirus24: I don't know which is best from options 2 and 3. Thoughts?
",Lukasa,sigmavirus24
1192,2013-02-14 19:33:21,"@t-8ch: Unfortunately, changes to the API are not allowed in minor version bumps, and we're not going to go to v2.0.0 for quite a while. Additionally, I would argue that the [feature freeze](http://docs.python-requests.org/en/latest/dev/todo/#feature-freeze) means that the API is essentially frozen as well.
",Lukasa,t-8ch
1192,2013-02-14 19:51:43,"@t-8ch: Requests' feature freeze is perpetual, there is no plan to exit the freeze. =) Your arguments are totally understandable, but the API is unlikely to be changed in any way from now on, and certainly not as major a change as that one.
",Lukasa,t-8ch
1192,2013-02-15 00:02:10,"FWIW, I believe that HTTPS proxies will work once fine once https://github.com/shazow/urllib3/pull/139 is merged. cc @t-8ch 
",wolever,t-8ch
1191,2013-02-15 00:12:02,"@sigmavirus24 looks good :)
",nathankot,sigmavirus24
1189,2013-02-14 03:15:22,"@Lukasa you can also try sigmavirus24/requests @fix1189
",sigmavirus24,Lukasa
1189,2013-02-17 01:17:07,"@gazpachoking, you're quite right, this has been resolved. Thanks guys! 
",Lukasa,gazpachoking
1188,2013-02-14 00:25:16,"@Lukasa In that case an update to the documentation is warranted—the front page of the project says “thread-safe” which I assumed meant, you know, the whole project. :)
",brandon-rhodes,Lukasa
1187,2013-02-14 03:58:54,"@kennethreitz we could maintain backwards compatibility by keeping the old exceptions and sub-classing them for the new ones. It's ugly and I don't like it, but we would be able to raise ""properly"" named exceptions whilst maintaining backwards compatibility.
",sigmavirus24,kennethreitz
1187,2013-02-14 19:00:58,"@untitaker the only catch is that the error strings for these errors use ""Schema"" and should be changed to ""Scheme"" to be consistent, which is why I suggested sub-classing the originals.
",sigmavirus24,untitaker
1183,2013-02-12 17:35:52,"So @mkomitee, I've thought about this some more. Where are you concerned about redirects? In the initial request? If so, I understand your concern. If you're instead concerned about redirects after the 401 has been handled, you can prevent those yourself. You have access over the response at that point I believe.
",sigmavirus24,mkomitee
1183,2013-02-12 17:51:04,"I'm concerned about it in the initial request.

On Tue, Feb 12, 2013 at 12:36 PM, Ian Cordasco notifications@github.com
wrote:

> ## So @mkomitee, I've thought about this some more. Where are you concerned about redirects? In the initial request? If so, I understand your concern. If you're instead concerned about redirects after the 401 has been handled, you can prevent those yourself. You have access over the response at that point I believe.
> 
> Reply to this email directly or view it on GitHub:
> https://github.com/kennethreitz/requests/issues/1183#issuecomment-13444844
",mkomitee,mkomitee
1183,2013-02-12 18:41:56,"Ah, I see what you mean. We resolve redirects before dispatching hooks. If that came before the redirect resolution, that would mean you could catch the redirect, correct?

Semantically I think it should be there anyway. If a redirect is resolved, every request except the first has the hook called on it (because each subsequent request calls `send` with `allow_redirects=False`). It should before the resolution unless @kennethreitz or @Lukasa disagree.
",sigmavirus24,Lukasa
1183,2013-02-12 23:22:11,"@mkomitee I have no real opinion about forcing Auth providers to accept all of those parameters, but I doubt it will be accepted. On that topic though, `allow_redirects` should be handled by the Session without a doubt. The Auth providers should have no reason to worry about that. I can understand needing the `proxies`, `timeout`, `stream` and `cert` parameters, but I thoroughly disagree that it is the responsibility of the auth provider to handle  redirects. With that said (twice), there shouldn't be a need to send the `Session` object itself.
",sigmavirus24,mkomitee
1183,2013-02-13 01:45:30,"If authentication is required for the redirect, the 401 goes to the auth provider and there's no way back to the redirect handling code. That's all out of scope for this issue though.

On Tue, Feb 12, 2013 at 6:22 PM, Ian Cordasco notifications@github.com
wrote:

> ## @mkomitee I have no real opinion about forcing Auth providers to accept all of those parameters, but I doubt it will be accepted. On that topic though, `allow_redirects` should be handled by the Session without a doubt. The Auth providers should have no reason to worry about that. I can understand needing the `proxies`, `timeout`, `stream` and `cert` parameters, but I thoroughly disagree that it is the responsibility of the auth provider to handle  redirects. With that said (twice), there shouldn't be a need to send the `Session` object itself.
> 
> Reply to this email directly or view it on GitHub:
> https://github.com/kennethreitz/requests/issues/1183#issuecomment-13465713
",mkomitee,mkomitee
1182,2013-02-12 04:43:36,"@Lukasa thanks for reminding me about that issue. I would love that. I could get stuff working on PythonAnywhere then. :)
",sigmavirus24,Lukasa
1182,2013-02-13 16:30:36,"@t-8ch we do very little work with proxies. If I remember our code correctly, we just pass it to urllib3 and they handle all of that for us. I guess we _could_ validate it before passing it to urllib3 though. I'll defer to @kennethreitz and @Lukasa about whether we should be doing that or not though.
",sigmavirus24,t-8ch
1182,2013-02-13 16:30:36,"@t-8ch we do very little work with proxies. If I remember our code correctly, we just pass it to urllib3 and they handle all of that for us. I guess we _could_ validate it before passing it to urllib3 though. I'll defer to @kennethreitz and @Lukasa about whether we should be doing that or not though.
",sigmavirus24,Lukasa
1182,2013-02-13 21:44:56,"I think @t-8ch is probably right. It's a quick fix, I'll open a PR and see what Kenneth thinks.
",Lukasa,t-8ch
1181,2013-02-12 06:52:53,"@Lukasa
Thanks! Please check the fix.
",denis-ryzhkov,Lukasa
1181,2013-02-12 09:08:03,"@kennethreitz

`latin1` (aka ISO-8859-1) is recommended for header **values** by the very HTTP/1.1 spec
http://tools.ietf.org/html/rfc2616#section-4.2
- Header **name** is always `ascii`:


- Header **value** is `latin1` by default, but may be any other charset too:



This RFC is MIME for headers: http://tools.ietf.org/html/rfc2047

Example of encoded value:



It can be decoded with:
http://docs.python.org/2/library/email.header.html#email.header.decode_header

So it's a bit more complex than just `latin1`.

I guess it is a good idea to merge the fix for header **name** first,
and then return to header **value** issues.
",denis-ryzhkov,kennethreitz
1177,2013-02-11 23:00:39,"You can see that here:



That's the last line of the error stack from above. As @Lukasa pointed out, in the end the whole request has to be serialized as a string. It doesn't matter that there are headers in between, because if you do `unicode + str` the result will be `unicode`. So as long as there is at least one `unicode` element before the message body, the message body will have to be converted to `unicode` too. Basically, a single `unicode` element in URL, fragment, query string, headers or body ""infects"" all other elements.

In any case, the specific problem I had can be solved in a client library, so that's what I did and submitted a pull request there.
",shezi,Lukasa
1176,2013-02-09 23:52:11,"

**Verify return code: 9 (certificate is not yet valid)**

Thanks @t-8ch!  

The time was wrong, after setting up ntp everything works.
",athoik,t-8ch
1176,2013-02-10 00:14:33,"Thanks @t-8ch for helping @athoik clear this up.
",sigmavirus24,t-8ch
1173,2013-02-10 22:19:10,"Rebased. And @kennethreitz I reconstruct a `PreparedRequest` because previously the original was what we made the `request` attribute on the `Response` object. This is just to keep backwards compatibility and not surprise anyone relying on that.
",sigmavirus24,kennethreitz
1171,2013-02-08 09:16:34,"Uh, hang on. Two different users have posted in this issue. The traceback in the original post does not indicate what the URL was, so I can't tell whether this is one issue or two. @norey @t-8ch, are you two talking about the same problem, and if so, how on earth do you know? :confused: 
",Lukasa,t-8ch
1171,2013-02-08 09:17:47,"In fact, rereading this, there's simply no way these are the same issue. @norey claims that **any** URL fails, which is clearly not a header related problem. @t-8ch, can you please open a new issue instead of hijacking this one? =)
",Lukasa,t-8ch
1171,2013-02-08 09:44:16,"I just tried requesting Google and some other URLs, and it worked fine. But that URL still gives me the same error... I don't get the reason especially when @sigmavirus24 didn't have any issues.
",norey,sigmavirus24
1171,2013-02-08 09:51:51,"@Lukasa the one shown in the paste I posted.

I just tried it on Python2.7.3 and it works !! but not on Python3.3 !! 
Looks like something FUBARed at my end but can't seem to locate it.
",norey,Lukasa
1171,2013-02-08 10:40:51,"I get the same output ( @Lukasa's output of 3.3)  on my Linux box running Python 3.2.3

So seems that it's an issue in Python 3, right ?
",norey,Lukasa
1171,2013-02-08 13:03:27,"Right, so @norey, your exception seems to be specific to you. @sigmavirus24 and I are getting the ""can't parse headers"" issue.
",Lukasa,sigmavirus24
1171,2013-02-08 20:02:24,"@t-8ch if this isn't standards compliant report it upstream at bugs.python.org please. You seem to have a good understanding of what is happening.

On a separate topic, @norey, you said this happens with any URL you use. Were you exaggerating or were you being serious? The reason I ask is because using requests on python 3.3 works perfectly fine for me.
",sigmavirus24,t-8ch
1171,2013-02-08 20:39:12,"@sigmavirus24 I actually tried with a couple of URLs not everything, so I just assumed. But I tried again on Google and it works fine the other URL that gives the exception is ( it is the same server I guess, giving that crap)



Ok now, let's disregard its behavior and concentrate on the exception I am getting, what is its reason? (it is in my first post). 
",norey,sigmavirus24
1171,2013-02-09 01:19:48,"@norey I didn't mean you should close this, just that the evidence so far is that some people don't know how to run servers. If you can find other sites that do this, feel free to contact @t-8ch or myself via email, or just post it back here.
",sigmavirus24,t-8ch
1170,2013-02-07 20:37:33,"@kennethreitz but some things are much harder/tricky to change after you've prepared the requests, and even so the way to modify is to overload the send method and modify the argument it receives. that's ugly

@sigmavirus24 yes you can do that yourself by inheriting the Session class and replacing the request method with a similar functionality, but thats also ugly -- this method is quite long and does a number of things and I don't duplicate the whole damn thing.

all i'm asking it to take the 10 lines that create the requests before 'preparing it' and extract it to a separate method. overloading such method would be the equivalent for the old pre_request hook

and also, some documentation on this in the hooks page would help :)
",grzn,kennethreitz
1170,2013-02-07 20:37:33,"@kennethreitz but some things are much harder/tricky to change after you've prepared the requests, and even so the way to modify is to overload the send method and modify the argument it receives. that's ugly

@sigmavirus24 yes you can do that yourself by inheriting the Session class and replacing the request method with a similar functionality, but thats also ugly -- this method is quite long and does a number of things and I don't duplicate the whole damn thing.

all i'm asking it to take the 10 lines that create the requests before 'preparing it' and extract it to a separate method. overloading such method would be the equivalent for the old pre_request hook

and also, some documentation on this in the hooks page would help :)
",grzn,sigmavirus24
1166,2013-03-04 21:51:03,"@schlamar if I remember correctly, this has been brought up before. Look up CherryPy or Twisted on the issue tracker. We could easily mock responses and test the like. It isn't that difficult. @kanzure and I do the same for our projects.
",sigmavirus24,schlamar
1166,2013-07-20 21:07:59,"@sigmavirus24, well, while you're at it, please deprecate my requestions library. I made some poor decisions in its implementation that I think you can just copy. I know you're probably just copying the vcr gem from ruby, but maybe you can add in those decorators for mocking out tests etc.
",kanzure,sigmavirus24
1165,2013-02-06 19:00:15,"@kennethreitz Can you elaborate on what you want contributing.md to be, and how it should relate to the current dev docs?
",Lukasa,kennethreitz
1163,2013-02-05 17:06:53,"@sigmavirus24 according to that documentation (and the pull request), `danger_mode` caused `raise_for_status` to be called at the end of any request. If this were the default behavior in 1.1.0, my example above would have thrown an `HTTPError: 404 Client Error: Not Found`.

It seems that the default behavior in 1.1.0 is `safe_mode`, but there is no ability to use the equivalent of `danger_mode`.
",zwass,sigmavirus24
1163,2013-02-05 17:25:48,"That's equivalent to re-adding part of the configuration API. It's also a new 
feature and as @kennethreitz mentioned, we aren't adding new features.
",sigmavirus24,kennethreitz
1161,2013-02-08 03:42:56,"@kennethreitz, PreparedRequests don't use the CaseInsensitiveDict currently. If you would like it to, let me know.
",sigmavirus24,kennethreitz
1159,2013-02-01 16:50:31,"@kennethreitz I'll check why we set it if they set it themselves.
",sigmavirus24,kennethreitz
1155,2013-02-20 14:33:15,"@sigmavirus24: The `multi=True` argument allows not just ordered items in the dictionary, but also multiple keys in the dictionary.  If the `multi=True` argument is not passed, then all duplicate keys are removed when iterating over the the keys/items/values.  It's required for me because I'm working with a legacy system that requires ordered, duplicate GET parmeters.  Something like: http://www.example.com/item?a=1&b=steve&a=2&b=mary&a=3&b=bob
",dmckeone,sigmavirus24
1155,2013-04-16 18:17:22,"@sigmavirus24 Agree about the MultiDict.items(); it is odd, and I don't know why it's like that.  Not changing the API makes sense too.

So, currently Requests does this (Please feel free to correct me if some of this is wrong): 
- `OrderedDict` for everything, instantiated by `from_key_val_list()`
- Multi-value is supported by passing an iterable value
- `merge_kwargs` in `sessions.py` seems to rely on unique, case-independent keys  (It wouldn't concat multi-keys)

@sigmavirus24 Could you do a quick explanation why `OrderedDict` is used in place of dict?  Everything I've seen seems to not care all that much about order in the parameters.  Is this a relic of when requests used to accept a list of 2-tuples (as the docstring in `from_key_val_list()` seems to indicate)?

To me, the ideal would be to use `MultiDict` internally in `from_key_val_list()`, with some kind of session/response based switch that could use `OrderedMultiDict` in it's stead.  This would mean:
- Changing `from_key_val_list()` to interpret any non-`MultiDict` dictionaries as `MultiDict` (e.g. change `OrderedDict` to `Dict`
- Multi-value would still work, because we aren't affecting the way values are stored by using `MultiDict` (just enabling multi-key)  
- `merge_kwargs` in sessions.py would need some kind of concatenation behaviour if order mattered, and `MutliDict` would need to be case insensitive.
",dmckeone,sigmavirus24
1155,2013-04-16 18:42:58,"Er, yeah, `items` (old habits…)

Okay! I'll take a stab today or tomorrow. And if anything we owe you & @kennethreitz!
",ihodes,kennethreitz
1155,2013-04-18 04:38:12,"Hah we just did the exact same thing, sans the test (though I just fiddled with the OrderedMultiDict, setting the multi param's default to `True` in `iteritems` and `items`…). Unfortunately, I messed up something with the copy-paste or something and have been in traceback hell. 

I read your changes, and they look like we all agreed on above; why not submit a PR? @sigmavirus24, could you CR this? Awesomesauce.
",ihodes,sigmavirus24
1155,2013-04-18 14:37:24,"Since everyone seems to like it, I don't mind keeping at it.   @sigmavirus24 if you find anything then let me know, and I'll be happy to add it in.   I'll go fix that test first.  Would you prefer I do a Pull Request after that, or continue changes on this branch as you do CR?

Are there any recommendations for doing performance profiling for Requests?  Is there a benchmark that is commonly used?

As far as Python 3 support goes, I tested it under Python 3.3 and everything seems to behave the same as 2.7.  I was mostly talking about Werkzeug's lack of Python 3 support.  For example, should we keep all the iter...() methods around in MultiDict and OrderedMultiDict?  There is a BadRequestKeyError that is currently just assigned to KeyError.  In Werkzeug that is used to indicate that a bad request was sent, but since Requests is a little different, perhaps we should change the name or throw a different exception?
",dmckeone,sigmavirus24
1155,2013-04-18 20:13:32,"> I don't mind keeping at it.

I just didn't want to overwork you :)

You can start a pull request and then as you push to it just ping me to do more code review. It'll also get @Lukasa and @kennethreitz's attention.

> Are there any recommendations for doing performance profiling for Requests?

No... but let's make one!

>  For example, should we keep all the iter...() methods around in MultiDict and OrderedMultiDict?

If I remember correctly, those handle the work for `.items` on both. I'm 100% for renaming `iteritems` to `items` though. Also for setting the default for `multi` to `True`. (Like @ihodes did)

> perhaps we should change the name or throw a different exception?

I'm okay with throwing a plain ol' `KeyError`. That seems logical and is something we could reasonable expect.

So to review, the following `request` parameters need to use an `OrderedMultiDict`:
- [x] params
- [ ] data
- [ ] files

And it would be ideal if both `request`s and `response`s use a `MultiDict` for headers, i.e., convert headers to a `MultiDict` when taking them from the user and when returning a response use a `MultiDict` to hold the headers present. I doubt any server is using a header more than once, but it's part of the spec that we can send more than one, so we may as well be safe and expect the more horribly written servers to think they can return one.

Thanks again @dmckeone :cake:
",sigmavirus24,kennethreitz
1155,2013-04-18 20:13:32,"> I don't mind keeping at it.

I just didn't want to overwork you :)

You can start a pull request and then as you push to it just ping me to do more code review. It'll also get @Lukasa and @kennethreitz's attention.

> Are there any recommendations for doing performance profiling for Requests?

No... but let's make one!

>  For example, should we keep all the iter...() methods around in MultiDict and OrderedMultiDict?

If I remember correctly, those handle the work for `.items` on both. I'm 100% for renaming `iteritems` to `items` though. Also for setting the default for `multi` to `True`. (Like @ihodes did)

> perhaps we should change the name or throw a different exception?

I'm okay with throwing a plain ol' `KeyError`. That seems logical and is something we could reasonable expect.

So to review, the following `request` parameters need to use an `OrderedMultiDict`:
- [x] params
- [ ] data
- [ ] files

And it would be ideal if both `request`s and `response`s use a `MultiDict` for headers, i.e., convert headers to a `MultiDict` when taking them from the user and when returning a response use a `MultiDict` to hold the headers present. I doubt any server is using a header more than once, but it's part of the spec that we can send more than one, so we may as well be safe and expect the more horribly written servers to think they can return one.

Thanks again @dmckeone :cake:
",sigmavirus24,Lukasa
1155,2013-04-18 21:32:33,"I'll continue on with what I have for params, data and files.  I'll do the Pull Request and make sure that data and files are supported as well.  

Is requests attempting to have any kind of parity with werkzeug's structures?  While this was a good proof-of-concept to see if MultiDict would work, I don't think its a good idea to change the internal structure of them too much (flipping the default, for example).  Ideally, one day these structures will be in a common place that both Requests and Werkzeug could use (alluded to by @kennethreitz's [blog post](http://kennethreitz.org/exposures/the-future-of-python-http)).  I know that from my perspective it would be great if I could pass a Werkzeug `MultiDict` and have it work in Requests as-is.  That way the meaning of `MultiDict` isn't fragmented, and we can end up with a de-facto standard `MultiDict`.

When it comes to the implementation of the headers, I'm less sure.  I can see the use case for params and data, and I can take a look at files because I can envision it being the same, but I'm not sure I want to change it for headers.  Does the current implementation not work in some way that `MultiDict` would solve?  Implementing this `CaseInsensitiveOrderedMultiDict` just seems like we'd be diverting too far from simple, and too far from existing structures, and doing so for very little benefit.  So if someone else who knows why this would be important wants to implement it, I'm happy to leave it to them.
",dmckeone,kennethreitz
1155,2013-04-19 18:57:16,"@sigmavirus24 I added the pull request and came up with a blend that I think gets both of us what we want.  I set all the `multi=False` kwargs to be `multi=True`. However, I don't think it will be too big of an issue for users, because, unless I missed something, the `MultiDict`s are mostly internal to how Requests processes sessions.  The only way you can ""see"" them is by interrogating body in the prepared request afterwards.  `params=` is the exception, because you can set it on the `Session`.

Additionally, in all places that take in a `MultiDict`, I removed all explicit checks for `isinstance(d, MultiDict)`, and replaced it with a `try: except TypeError:` or a check for the needed attribute, `iterlists`.

I also added a couple extra tests to prove the ordering in `data=` and `files=`
",dmckeone,sigmavirus24
1155,2013-04-21 17:23:43,"@Lukasa I took a look through and I believe that my changes to `params=`, `data=`, and `files=` are all safe from an API perspective:  
- `PreparedRequest` does not expose `params`, `data` or `files`
- `Session` has a `params` attribute that I initialize to OrderedMultiDict, but it is hidden because it isn't declared in the `__attrs__`
-  `Request` has `params`, `data`, and `files` exposed, but the user can't get to that because of `__attrs__`

In the future, I don't think there is any way around an API change for `headers` (which is why I shied away from that change), because a `dict`, or in this case a `CaseInsensitiveDict`, just can't represent multiple keys.  So that implementation will require some thinking about the API, and hopefully a structure more dedicated to handling HTTP Headers;  maybe just a `dict`-like class called `HTTPHeader` that has the behaviour of the earlier mentioned `CaseInsensitiveOrderedMultiDict`, but is clearly bound to conform to HTTP Header behaviours and rules.  After all, `CaseInsensitiveDict` is really only used for headers anyway; perhaps it's best to call a spade a spade?
",dmckeone,Lukasa
1155,2013-04-21 19:22:23,"@dmckeone I think either you or I am misunderstanding the role of `__attrs__`. As far as I know, it does not hide any information from the user but presents to libraries like `multiprocessing` and `pickle` what they need to properly serialize and reconstruct those objects. So, yes, in fact `params` is entirely exposed to the user. There would be no other point in having it. :)

These were all planned and endorsed changes by @kennethreitz. And to a degree they're quite necessary to make requests entirely functional. Leaving `headers` as is, right now, seems okay to me. I'm just not sure how well having the `Session.headers` attribute as a `MultiDict` will work.
",sigmavirus24,kennethreitz
1155,2013-04-21 19:53:24,"@sigmavirus24 is correct, `__attrs__` does not limit exposure of those parameters. In particular, `Session.params`, `Request.params`, `Request.data` and `Request.files` are deliberately exposed parts of the API, intended to be used by users.

I think if we're going to make backward-incompatible changes to the API we should do it in one fell swoop. I'm still remembering the less-than-positive initial reaction to turning `Response.json` into a method, and this change is far more wide-ranging than that one.

Changing any of these parameters, especially if `multi=True` is the default (which it probably should be), is a backward-incompatible change. **I'm all for doing it**, as I think the current behaviour could be improved, but any implementation needs to very carefully planned, and needs to be clearly documented and accompanied by a decent version number bump.
",Lukasa,sigmavirus24
1155,2013-04-21 20:54:27,"@sigmavirus24 @Lukasa Thanks guys.  Mea culpa about the `__attrs__`, you're right.

> Changing any of these parameters, especially if multi=True is the default (which it probably should be), is a backward-incompatible change

You may be right (and certainly would be if `headers=` was included), but I believe its only backwards incompatible with the current change if you tried to use assume order or duplicate values, and since that wouldn't be possible in older versions due to the use of `dict`, I don't think it's incompatible in a breaking way.  

I'm thinking of cases like this (where `request` is an instance of `Request`):
- `isinstance(request.params, dict)` 
- `request.params['param']` for single and iterable values.  
- `request.params == {'param': 1}`

Have I missed a case here, or is it a reasonable expectation to say that, if you want to use ordered duplicate `params`, `data`, or `files` then 1.2X (or whatever version this ends up in) is your baseline?  If you don't use ordered or duplicate `params`, `data`, or `files` then things should work identically for all versions.

So with that in mind, how would you like to proceed with this?  Should I be incorporating `headers=` into my branch and targeting a full version release with docs and all that, or is the pull request I submitted a way to start that works well enough, and then headers can be approached separately.  I'm fine with either, or something else entirely, but since I'm new to this project I'm happy to follow your more experienced lead.
",dmckeone,Lukasa
1155,2013-04-21 20:54:27,"@sigmavirus24 @Lukasa Thanks guys.  Mea culpa about the `__attrs__`, you're right.

> Changing any of these parameters, especially if multi=True is the default (which it probably should be), is a backward-incompatible change

You may be right (and certainly would be if `headers=` was included), but I believe its only backwards incompatible with the current change if you tried to use assume order or duplicate values, and since that wouldn't be possible in older versions due to the use of `dict`, I don't think it's incompatible in a breaking way.  

I'm thinking of cases like this (where `request` is an instance of `Request`):
- `isinstance(request.params, dict)` 
- `request.params['param']` for single and iterable values.  
- `request.params == {'param': 1}`

Have I missed a case here, or is it a reasonable expectation to say that, if you want to use ordered duplicate `params`, `data`, or `files` then 1.2X (or whatever version this ends up in) is your baseline?  If you don't use ordered or duplicate `params`, `data`, or `files` then things should work identically for all versions.

So with that in mind, how would you like to proceed with this?  Should I be incorporating `headers=` into my branch and targeting a full version release with docs and all that, or is the pull request I submitted a way to start that works well enough, and then headers can be approached separately.  I'm fine with either, or something else entirely, but since I'm new to this project I'm happy to follow your more experienced lead.
",dmckeone,sigmavirus24
1155,2013-04-21 21:13:54,"Not a worry @dmckeone . I get confused plenty about stuff too. @Lukasa usually corrects me kindly. :)

I think for the current pull request you can leave out `headers` if you're that uncomfortable with it, but it has to be implemented before the next release. 

Part of the documentation will have to include a section about how the `MultiDict` works with respect to `__setattr__` and `__getattr__`. That (if I remember correctly) is non-obvious behaviour and in this instance I have no criticism of this aspect of the API. :P
",sigmavirus24,Lukasa
1155,2013-04-21 21:20:37,"Getting stuff wrong in public is basically the definition of OSS development, as far as I can tell. =P I certainly do it enough.

I think my concern primarily applies to changing `headers`. We don't expect `param`, `data` or `files` to be mutated by the library, so the user should get out whatever the hell they put in. However, we mutate `headers` quite a bit. If we change what we do there, it's definitely breaking.

I'm with @sigmavirus24 on this: I want to change it, and I think we have to change it, but changing headers is breaking. We might want to consult with @kennethreitz before we go charging ahead on that part of it, to see if he wants to hold off until some later release.

And in case I've seemed a little blunt in this conversation (reading over it again I sure feel like I did), I want to be clear: your work is excellent, and I want to thank you for taking it on. =)
",Lukasa,kennethreitz
1155,2013-04-21 21:20:37,"Getting stuff wrong in public is basically the definition of OSS development, as far as I can tell. =P I certainly do it enough.

I think my concern primarily applies to changing `headers`. We don't expect `param`, `data` or `files` to be mutated by the library, so the user should get out whatever the hell they put in. However, we mutate `headers` quite a bit. If we change what we do there, it's definitely breaking.

I'm with @sigmavirus24 on this: I want to change it, and I think we have to change it, but changing headers is breaking. We might want to consult with @kennethreitz before we go charging ahead on that part of it, to see if he wants to hold off until some later release.

And in case I've seemed a little blunt in this conversation (reading over it again I sure feel like I did), I want to be clear: your work is excellent, and I want to thank you for taking it on. =)
",Lukasa,sigmavirus24
1155,2013-04-21 22:19:35,"@sigmavirus24 Fully documenting how MultiDict behaves is probably a good idea when we do `headers`.  As far as `headers` goes, I'm not necessarily uncomfortable with the actual work, just the implication of the API change and the associated fallout if it were to be done anything less than perfect.  In that way I like @Lukasa's idea to get consensus first, and do it as part of a unique version number.

@Lukasa I actually like direct tone because its clearer, so I didn't mind the bluntness.  Thanks for the compliment as well!

So just to summarize what I think I've read here for everyone's sake (there is quite a long comment thread here): 

MultiDict support will come in two stages: 
1) #1316 for `params`, `data`, and `files` in something relatively soon
2) Full documentation of OrderedMultiDict, as well as a future class that combines the behaviours of OrderedMultiDict with CaseInsensitiveDict, and an implementation of the earlier mentioned future class under the `headers` kwarg. All of which happening under a unique version that incorporates an anticipated API change.
",dmckeone,Lukasa
1155,2013-04-21 22:19:35,"@sigmavirus24 Fully documenting how MultiDict behaves is probably a good idea when we do `headers`.  As far as `headers` goes, I'm not necessarily uncomfortable with the actual work, just the implication of the API change and the associated fallout if it were to be done anything less than perfect.  In that way I like @Lukasa's idea to get consensus first, and do it as part of a unique version number.

@Lukasa I actually like direct tone because its clearer, so I didn't mind the bluntness.  Thanks for the compliment as well!

So just to summarize what I think I've read here for everyone's sake (there is quite a long comment thread here): 

MultiDict support will come in two stages: 
1) #1316 for `params`, `data`, and `files` in something relatively soon
2) Full documentation of OrderedMultiDict, as well as a future class that combines the behaviours of OrderedMultiDict with CaseInsensitiveDict, and an implementation of the earlier mentioned future class under the `headers` kwarg. All of which happening under a unique version that incorporates an anticipated API change.
",dmckeone,sigmavirus24
1155,2013-05-11 19:52:28,"> Sorry for the delay in this comment.  Things have been busy.

Take your time. @kennethreitz is in transit so there's no rush. Also, I 
wouldn't blame you if you wanted to wait for those other pull requests to be 
merged before submitting a new PR. That seems like the most sane thing to do 
at the moment in my humble opinion.
",sigmavirus24,kennethreitz
1155,2013-05-11 20:25:04,"@sigmavirus24 That is what I will likely do.  I will begin work on making the required `MultiDict` changes and tests in the next few days and then just rebase everything once those pull requests go in.
",dmckeone,sigmavirus24
1155,2013-05-13 15:31:58,"@Lukasa @sigmavirus24 My mistake.  I had closed the PR on purpose, but not this.   Thanks for re-opening.
",dmckeone,Lukasa
1155,2013-05-13 15:31:58,"@Lukasa @sigmavirus24 My mistake.  I had closed the PR on purpose, but not this.   Thanks for re-opening.
",dmckeone,sigmavirus24
1155,2014-09-10 20:03:20,"Thanks for the thorough reply, @Lukasa. Great that requests' headers representation is not as wrong as I was afraid of. I'd think this would be worth documenting though, given that so many other libraries use a data structure which preserves multiple headers more closely to how they were sent, leading other requests users to maybe ask you the same thing I did.

Also worth logging a separate bug for joining multiple cookie headers with semicolons?
",requiredfield,Lukasa
1155,2016-07-28 20:57:10,"Thanks @Lukasa. I do like what I'm seeing wrt to the [HTTPHeaderDict](https://github.com/shazow/urllib3/blob/8513dcb2e2f3bf7b53452ddeb22e6e0c322c0e22/urllib3/_collections.py#L101) class. Guess I'll do some digging to see if that would handle sending repeats one per line.

Cheers.
:tophat:
",BigBlueHat,Lukasa
1154,2013-01-31 13:46:36,"I whole-heartedly agree with @Lukasa
",sigmavirus24,Lukasa
1150,2013-01-29 19:25:03,"To clarify @sigmavirus24's point, we only default to ISO-8859-1 in [some cases](http://docs.python-requests.org/en/latest/user/advanced/#encodings).
",Lukasa,sigmavirus24
1147,2013-01-27 19:20:16,"Hm, are you sure this works with urllib3? I can understand why it would work for httplib. The import is right there, but I see no import of socket that would be reachable from simply import urllib3.

And yeah @kennethreitz I think it would be great too, that's why I'm exploring it.
",sigmavirus24,kennethreitz
1147,2013-01-27 19:27:32,"Yeah @kennethreitz that would always be the ideal case. :)

@infodox, in a clean virtualenv I just installed urllib3 (1.5 from pypi) and downloaded the socks.py file.

This is my interactive session:



Are you sure it works fine on urllib3?

It might work if you did: `socks.wrapmodule(urllib3.connectionpool)` and `socks.wrapmodule(urllib3.util)` but it definitely doesn't work using only urllib3.

As for us, I think we could import socket from `urllib3.connectionpool` and it would work since that is the first import in `__init__.py`, I'm just not sure if this would pollute the `requests` namespace.
",sigmavirus24,kennethreitz
1147,2013-02-09 23:17:52,"@infodox, having given it some thought, I honestly think that `socks.py` could be written better. In short, if they want to patch the socket module, they're probably going to want to do so for the entire process, not just for one module. They should be replacing the `socket` entry in `sys.modules`, not trying to rely on a module having `socket` in their namespace. As such, I'm +1 on closing this unless @kennethreitz wants to add a reference to the `socket` stdlib to `requests`, i.e., in `requests/__init__.py` putting `import socket`.
",sigmavirus24,kennethreitz
1146,2013-01-28 15:58:26,"@sigmavirus24 

Because one blank line is a good way of separating blocks of code inside a function. In light of this separating functions with only one blank line doesn't separate them enough.
",piotr-dobrogost,sigmavirus24
1144,2013-01-26 15:13:29,"This is what I see pre #1142



And after #1142 



So @Lukasa is correct, and this is a duplicate.
",sigmavirus24,Lukasa
1143,2013-01-26 17:07:12,"Thanks @sigmavirus24!
",Lukasa,sigmavirus24
1143,2013-01-28 18:36:30,"@kennethreitz: Rebased. =)
",Lukasa,kennethreitz
1142,2013-01-26 04:57:16,"Ah, @kennethreitz beat you to it. MOAR COMMITS!
",theaeolianmachine,kennethreitz
1141,2013-01-24 18:23:52,"@Lukasa line or chunk
",kennethreitz,Lukasa
1140,2013-01-25 03:50:48,"The test failure lies in unittest in python 2.6 not having the `assertIn` method. I'll work from @ralphbean's pr and add a work around tomorrow.
",sigmavirus24,ralphbean
1138,2013-01-24 18:45:37,"@sigmavirus24 Your point about streaming responses is correct; I'm not sure that a simple 'end - start' timer is useful for streaming responses _anyway_, but it IS useful for everything else (which is what I need).

As far as whether it's better as a method or a property - I honestly don't care. It's such a small change that if it gets rewritten to be a single property instead of a method, hooray! All I want is the ability to _get_ this information, from requests, without having to patch it myself every time I use it. @kennethreitz didn't seem to be convinced that this functionality even _belongs_ in requests, so I'm hoping that even if this particular commit doesn't get accepted, the general idea is considered to be worthwhile and some kind of implementation makes its way in.
",clee,sigmavirus24
1138,2013-01-26 15:47:27,"@kennethreitz which start and finish are you talking about? The locals in the adapters file?
",sigmavirus24,kennethreitz
1133,2013-01-24 13:55:48,"@alanhamlett I think he means customizing the the Request class itself and using that instead of the one that comes with requests.
",sigmavirus24,alanhamlett
1133,2013-01-24 15:19:56,"@sigmavirus24 exactly what I was saying.

The primary issue I see today is that if I want to either manipulate a Request instance or subclass the Request object and do my own thing I lose the functionality of the `Session.request` method. It would seem pretty terrible to have to reimplement that method in an ad hoc way just for things like timeouts on the request.

The broader point I'm driving at is `Session.request` explicitly instantiates `Request` which means a caller can't control the behavior of that object. It would really be a shame to have to implement a custom `Session.request` with what would effectively be almost an entire repetition of the request method's logic save for one line's difference.

Unless I'm missing something, this seems to be encouraging some non-idiomatic solutions so perhaps it would be nice to allow a caller to tell Session which Request object to use?
",maxcountryman,sigmavirus24
1133,2013-01-24 17:42:16,"@kennethreitz yes but there's a caveat to that, unless I'm misunderstanding: if you want the wrapper around `send`, i.e. `request` and its helpers, then you have to write your own `Session.send` in order to manipulate the `Request`/`PreparedRequest`, yes?
",maxcountryman,kennethreitz
1133,2013-01-24 17:49:28,"@kennethreitz where do you have access to `Session.request` then? The request has to be sent via `s.send` which eschews all the cool logic happening in `s.request`, right? I can't bootstrap the `Session` with my `Request` instance so I don't see how I can ever use `s.request`.
",maxcountryman,kennethreitz
1133,2013-01-24 18:05:10,"@maxcountryman, that's the cool part. All s.request does is create a Request object, which you already did :)
",kennethreitz,maxcountryman
1133,2013-01-24 18:08:27,"@kennethreitz that ""sugar"" is really handy to have around and it's also expected by anyone who uses the requests standard API, e.g. via `requests.request`. So for instance, say you want to pass `allow_redirects`, unfortunately if I have to invoke `s.send` directly this means I need to write my own wrapper around `s.send` which would basically only change one line of code in `s.request`, i.e. the `req = Request()` line. Am I totally off the mark here?
",maxcountryman,kennethreitz
1133,2013-01-24 18:12:56,"@kennethreitz how can you pass `allow_redirects` to `s.send`? Unless I'm totally missing something, the answer is you can't, because `s.request` does more than just construct a `Request` object. It does neat things like extrapolate on that object to follow redirects, it handles cookies, etc. If I want all that, i.e. if I want to write a custom interface around a Request object that allows the caller to pass in the exact same params they could to `requests.request` then I have to reimplement `s.request` because there's just one line of code there that is fixed and I can't touch, namely `req = Request()`. If all `s.request` did was construct a `Request` object then that would be fine, but from what I can see there's a bunch of stuff happening in its scope which I am in need of. :)
",maxcountryman,kennethreitz
1133,2013-01-24 18:15:00,"@kennethreitz: He wants to use what is essentially a subclass of the `Request` class, which presumably implements different logic for either `Request.prepare` or one of the data fields.

We can solve this, however, by using MAGIC!



Done. Hell yeah. :sunglasses:
",Lukasa,kennethreitz
1133,2013-01-24 18:15:26,"@maxcountryman It's a parameter of send.


",kennethreitz,maxcountryman
1133,2013-01-24 18:17:10,"@kennethreitz I get a keyword error on that...


",maxcountryman,kennethreitz
1133,2013-01-24 18:18:21,"@kennethreitz heh, yeah it's quite unfortunate. :p
",maxcountryman,kennethreitz
1133,2013-01-24 18:18:42,"@kennethreitz: Nope.


",Lukasa,kennethreitz
1133,2013-01-24 18:22:51,"Awesome! Thanks for bearing with me @kennethreitz. :) I may have a chance to make a pull request later on if someone doesn't beat me to it.
",maxcountryman,kennethreitz
1133,2013-01-24 20:23:21,"@maxcountryman, @kennethreitz and I discussed this on IRC (https://botbot.me/freenode/python-requests/1756572/ from the highlighted line, down) and there are some changes that need to be made (which should have taken place during the refactor) which are going to be made now. So you might want to hold off on working on adapters and custom Request objects just yet.
",sigmavirus24,kennethreitz
1133,2013-01-24 20:23:21,"@maxcountryman, @kennethreitz and I discussed this on IRC (https://botbot.me/freenode/python-requests/1756572/ from the highlighted line, down) and there are some changes that need to be made (which should have taken place during the refactor) which are going to be made now. So you might want to hold off on working on adapters and custom Request objects just yet.
",sigmavirus24,maxcountryman
1133,2013-01-24 20:24:32,"@sigmavirus24 Thanks for the update, most appreciated! I'll hold off for now.
",maxcountryman,sigmavirus24
1133,2013-01-25 05:13:53,"So one way of doing this, as you can see from that IRC discussion is to do:



Which should satisfy your needs but which is ugly and horribly complicated. You shouldn't need to care about preparing all of those items. So I'm wondering if there is room for a new Session level method that will pretend to act like `request` but will do this for you and accept `Request` or `reparedRequest` objects. If it takes the former, then it could be the basis for our `request` method. Make sense @kennethreitz ?
",sigmavirus24,kennethreitz
1133,2013-01-28 17:37:26,"@sigmavirus24 that seems good to me. What does @kennethreitz think?
",maxcountryman,kennethreitz
1133,2013-01-28 17:37:26,"@sigmavirus24 that seems good to me. What does @kennethreitz think?
",maxcountryman,sigmavirus24
1129,2013-01-23 13:23:14,"Well said, @Lukasa. 

Thanks for the pull request, but I won't be accepting this at this time. You're not the first person to send this, actually :)
",kennethreitz,Lukasa
1129,2013-01-23 18:44:59,":+1: go get 'em @alanhamlett
",sigmavirus24,alanhamlett
1124,2013-01-22 16:15:47,"@Lukasa you are absolutely correct in your deduction: I am in fact using Python 2.7.x.

That is terribly unfortunate apropos of urllib3 and a little surprising considering how widespread Python 2.x usage still is...
",maxcountryman,Lukasa
1124,2013-01-22 18:00:39,"@maxcountryman: The reason the changes have not yet been made in urllib3 is that the SNI hostname parameter is not exposed in the default library ssl module before Python 3.2. To implement this behaviour would require doing something intelligent with PyOpenSSL, a third party module. I'm beginning to think about doing this, but I certainly won't start before the weekend. =)
",Lukasa,maxcountryman
1124,2013-01-26 17:51:56,"@Lukasa you're right. It is exclusive to python 2.x (I tested on 2.6). Testing again on python 3.2 works just fine.
",sigmavirus24,Lukasa
1123,2013-01-23 17:19:53,"@andrewjesaitis, I have to look up where I think I did this then, because I'm getting the same thing on 2.6. :/ Sorry for the confusion.

And yeah, placing it on the auth in the pattern I mentioned above would work. Inside the if-statement you increment it by one, outside you set it to 1. That way if the auth is passed around (say in a session) it won't have incorrect counts.
",sigmavirus24,andrewjesaitis
1123,2013-01-23 18:43:51,"@kennethreitz looks good to me. :+1:
",sigmavirus24,kennethreitz
1123,2013-01-26 05:00:31,":sparkles: @sigmavirus24 has spoken! :sparkles:
",kennethreitz,sigmavirus24
1119,2013-01-21 13:43:50,"@sigmavirus24 that's exactly what i was thinking.

Need to think about if more. @Lukasa makes a point.
",kennethreitz,Lukasa
1119,2013-01-21 13:43:50,"@sigmavirus24 that's exactly what i was thinking.

Need to think about if more. @Lukasa makes a point.
",kennethreitz,sigmavirus24
1119,2013-01-21 14:55:50,"I agree with @Lukasa 's point, and we've already broken the pre 1.x behaviour, so restoring it is not a must. Raising a `ValueError` seems to be the pythonic thing to do here, otherwise.
",sigmavirus24,Lukasa
1119,2013-01-26 15:48:53,"@kennethreitz: You didn't seem to have checked in any code for this, so I decided to give you an easy option. =)

I feel like the `getattr` check is overbroad, users might want to use custom PreparedRequest objects where the `prepare()` method simply mutates the current object as opposed to returning a new one. Normally I wouldn't use it, but as we're testing for 'user makes a boo-boo', maybe we should use it.
",Lukasa,kennethreitz
1119,2013-01-28 14:44:24,"@Lukasa I rebased and made the changes for you [here](https://github.com/sigmavirus24/requests/tree/lukasa/diags)
",sigmavirus24,Lukasa
1119,2013-02-09 23:19:14,"@Lukasa rebase again please? ;P
",sigmavirus24,Lukasa
1116,2013-01-19 18:12:56,"@Lukasa just use the `io` module. It has `StringIO` in it in python 2.6-3.3
",sigmavirus24,Lukasa
1116,2013-01-19 18:16:27,"@sigmavirus24:



The problem isn't in my code, it's that stupid StringIO _does_ have the method, but unconditionally throws exceptions. Given that Requests tries to use it, that's a problem. =P
",Lukasa,sigmavirus24
1116,2013-01-23 20:59:22,"@Lukasa 

As to type of this header, I agree, it should not be unicode. More generally no header should be unicode. Actually, thinking of it, no header in prepared request should be unicode; there could be some value in leaving unicode in unprepared request.
",piotr-dobrogost,Lukasa
1116,2013-01-24 12:43:03,"@kennethreitz Yeah, I was wondering about that. It seems like headers on the `Request` object should be free to be either unicode or bytes. With that said, somewhere before urllib3 we should get rid of any unicode headers, in order to avoid problems like #1082. Maybe `prepare_headers` should turn them into bytes?
",Lukasa,kennethreitz
1109,2014-09-30 08:57:08,"@kennethreitz : I just came across this while looking for a way to do this.  Can you explain why allowing requests to open file scheme URLs would be a ""conceptual flaw""?
",BrenBarn,kennethreitz
1109,2014-10-05 16:43:43,"They are also a big security risk for people that expecting to be able to trust that this is an HTTP library :)

Many thanks, @Lukasa :)
",kennethreitz,Lukasa
1099,2013-01-23 13:35:58,"@sprt can you update this? It's a bit out of date now :)
",kennethreitz,sprt
1099,2013-01-23 15:14:18,"@sprt, I'll work on this.
",sigmavirus24,sprt
1099,2013-01-23 16:48:51,"@kennethreitz, @sprt makes a valid point. My concern is dispatching a hook more than once. Someone using requests in @sprt's case has a valid reason to believe the hook should be called which makes it the responsibility of either the adapter's `send` or the session's `send` method. The former also catches the case where a user just instantiates the adapter and uses that directly (which is a bad idea). The latter would allow the hook to be dispatched once, and would remove it from the `Session.request` and `<Adapter>.send`. This I see as being the preferable option, but I wanted to make sure this is okay.

The problem, of course, is that only `request` has the hooks passed by the user. Naturally, this shouldn't be a problem because the prepared request has those hooks and the auth hooks generated by the authentication handler, so we can just rely on that. No where in between are hooks removed. A rogue adapter may do this in its `send` method, but that would be author's problem, not ours.
",sigmavirus24,kennethreitz
1099,2013-01-23 16:48:51,"@kennethreitz, @sprt makes a valid point. My concern is dispatching a hook more than once. Someone using requests in @sprt's case has a valid reason to believe the hook should be called which makes it the responsibility of either the adapter's `send` or the session's `send` method. The former also catches the case where a user just instantiates the adapter and uses that directly (which is a bad idea). The latter would allow the hook to be dispatched once, and would remove it from the `Session.request` and `<Adapter>.send`. This I see as being the preferable option, but I wanted to make sure this is okay.

The problem, of course, is that only `request` has the hooks passed by the user. Naturally, this shouldn't be a problem because the prepared request has those hooks and the auth hooks generated by the authentication handler, so we can just rely on that. No where in between are hooks removed. A rogue adapter may do this in its `send` method, but that would be author's problem, not ours.
",sigmavirus24,sprt
1099,2013-01-24 14:06:49,"@sprt those changes pass with your tests. If you like them merge them into your branch, they'll be added to this pull and @kennethreitz could accept it.
",sigmavirus24,kennethreitz
1099,2013-01-24 14:06:49,"@sprt those changes pass with your tests. If you like them merge them into your branch, they'll be added to this pull and @kennethreitz could accept it.
",sigmavirus24,sprt
1099,2013-01-24 18:04:10,"@sigmavirus24 was this not ready? send another pr :)
",kennethreitz,sigmavirus24
1099,2013-01-24 21:24:53,"Ah indeed, forgot about mounting!

Then I suggest @kennethreitz edit [his blog post](http://www.kennethreitz.com/announcing-requests-v100.html) (cf. ""Connection Adapters"") :)
",sprt,kennethreitz
1090,2013-01-10 20:59:48,"WOW! Thank you @sigmavirus24! I am a moron! The netflix api does call them params, it was my stupid mistake. I can't believe how long I was stymied by this, when as usual the answer was right in front of me.
",mikofski,sigmavirus24
1087,2013-01-08 03:50:00,"@sigmavirus24 I noticed the issue with the title of this article:

http://www.theverge.com/2013/1/4/3836944/robot-band-compressorhead-plays-motorhead-ace-of-spades

a few days ago, it was reporting an encoding of `iso-8859-1`. I don't see the issue now, however. It would be a little weird if they just happened to fix the issue since I last looked... but I don't have a better explanation. Content-Type is now:

`Content-Type: text/html; charset=utf-8`
",akavlie,sigmavirus24
1087,2013-01-10 08:21:12,"@kennethreitz understood after reviewing your comments in #156. A mention of the function in the docs would be useful.
",akavlie,kennethreitz
1085,2013-01-06 04:27:07,"Heh, good catch. Would have been good if @Lukasa or I had actually looked at the PR. ;)
",sigmavirus24,Lukasa
1084,2013-01-23 23:07:10,"@sigmavirus24

> If I understand correctly, the behaviour on redirect should be that we don't follow the redirect but instead just return the 307 response to the user, correct?

Not quite. Section [`7.4. Redirection 3xx`](http://trac.tools.ietf.org/html/draft-ietf-httpbis-p2-semantics-21#section-7.4) in the current draft from httpbis working group states

>   This class of status code indicates that further action needs to be
>    taken by the user agent in order to fulfill the request.  If the
>    required action involves a subsequent HTTP request, it MAY be carried
>    out by the user agent without interaction with the user if and only
>    if the method used in the second request is known to be ""safe"", as
>    defined in Section 5.2.1.

So speaking simply in case of 307 status code in reply to request with unsafe method (method other than the GET, HEAD, OPTIONS, and TRACE) we MAY NOT carry the redirect without interaction with the user. Now, the question is how to define _interaction with the user_ for http library. I agree with @jaraco saying that we should _require that the request be configured to ""re-post on 307""._

> Are there any other cases like this? 

Yes, soon to be accepted 308 status code - look for `draft-reschke-http-status-308-07.txt` at http://www.rfc-editor.org/cluster_info.php?cid=C160



Side note: using `codes.moved, codes.found` etc. in the source code instead of codes' numeric values does not help in quickly identifying these codes.
",piotr-dobrogost,sigmavirus24
1084,2013-01-23 23:07:10,"@sigmavirus24

> If I understand correctly, the behaviour on redirect should be that we don't follow the redirect but instead just return the 307 response to the user, correct?

Not quite. Section [`7.4. Redirection 3xx`](http://trac.tools.ietf.org/html/draft-ietf-httpbis-p2-semantics-21#section-7.4) in the current draft from httpbis working group states

>   This class of status code indicates that further action needs to be
>    taken by the user agent in order to fulfill the request.  If the
>    required action involves a subsequent HTTP request, it MAY be carried
>    out by the user agent without interaction with the user if and only
>    if the method used in the second request is known to be ""safe"", as
>    defined in Section 5.2.1.

So speaking simply in case of 307 status code in reply to request with unsafe method (method other than the GET, HEAD, OPTIONS, and TRACE) we MAY NOT carry the redirect without interaction with the user. Now, the question is how to define _interaction with the user_ for http library. I agree with @jaraco saying that we should _require that the request be configured to ""re-post on 307""._

> Are there any other cases like this? 

Yes, soon to be accepted 308 status code - look for `draft-reschke-http-status-308-07.txt` at http://www.rfc-editor.org/cluster_info.php?cid=C160



Side note: using `codes.moved, codes.found` etc. in the source code instead of codes' numeric values does not help in quickly identifying these codes.
",piotr-dobrogost,jaraco
1084,2013-01-26 17:48:24,"@cbare you're correct about that call to `request` not being complete.

So let me just walk through the steps of the request before submitting a pull request to fix that.

In a normal case (not chunked encoding), the user calls `requests.post(url, data={'key': 'value'}, files={'foo': open('foo', 'rb')})`. In this case, the `Request` object is created and prepared turning into a `PreparedRequest` which is what we receive as `req` in `resolve_redirects`. This is stored in `req.body`. Since this is prepared, we can do this (in `resolve_redirects`):



This works because when `data` receives a string, it sends that.

The problematic case is when the user is using chunked encoding (I think). The problem is, I'm not entirely sure what happens with chunked encoding at the moment. Maybe @kennethreitz can explain how that works because I haven't presently looked at it at all.

I could be wrong and it could all be handled as one case though.
",sigmavirus24,cbare
1084,2013-01-26 20:56:23,"@sigmavirus24 

> And by implementing it themselves, they could use hooks

Hooks are for custom actions not mandated by any RFC. We should not force users to write/use hooks to accomplish something specified in the RFC. Also there's no collection of commonly needed hooks packaged with Requests which makes any solution based on hooks more problematic for users.

@cbare

> the call to the request method in lines 122-135 of sessions.py doesn't propagate the body of the original request

Body will have to be kept when supporting 307/308.
",piotr-dobrogost,sigmavirus24
1084,2013-01-26 20:56:23,"@sigmavirus24 

> And by implementing it themselves, they could use hooks

Hooks are for custom actions not mandated by any RFC. We should not force users to write/use hooks to accomplish something specified in the RFC. Also there's no collection of commonly needed hooks packaged with Requests which makes any solution based on hooks more problematic for users.

@cbare

> the call to the request method in lines 122-135 of sessions.py doesn't propagate the body of the original request

Body will have to be kept when supporting 307/308.
",piotr-dobrogost,cbare
1084,2013-02-11 22:25:43,"@kennethreitz 

`allow_redirects` is about allowing redirects in general but in case of 307/308 status codes web browser asks user about permission that's why we have to similarly ""ask"" user of the library.
",piotr-dobrogost,kennethreitz
1084,2013-02-13 12:56:44,"So with my second to last pull request merged, we now re-post the data on 307. I missed that it should also be on 308, so if @kennethreitz doesn't mind I might just push the one-line fix for that instead of issuing a PR.

If I understand correctly, this issue would then be fixed. If users want to be ""asked"" they will just have to pass False to `allow_redirects` which will need to be documented. 
",sigmavirus24,kennethreitz
1084,2013-02-13 16:03:35,"@sigmavirus24 

As I wrote in my last comment 

> `allow_redirects` is about allowing redirects in general but in case of 307/308 status codes web browser asks user about permission that's why we have to similarly ""ask"" user of the library.

We need another param to let users decide if they want to follow 307/308 with unsafe http method.
",piotr-dobrogost,sigmavirus24
1075,2013-01-10 04:26:39,"@kennethreitz 
Let me clarify the reason In my understanding the reason is my patch has a dependency on ""netaddr"" module. Is it correct? If so this is the policy of this module and I am happy to honor that.

Lastly, do you have a plan to support x.y.z.w/prefix form in no_proxy?
",amotoki,kennethreitz
1074,2013-01-08 09:57:11,"@MicksMix: You've actually confused two different issues, which is totally understandable. =)

#905 is specifically a problem with HTTPS over proxies: in particular, urllib3 does not use the CONNECT verb. _This_ issue is affecting all proxies, and is a result of the refactor.

@trentvb, @erikcw, @mishari, @MicksMix: Some similar issues were raised earlier: see #1056 and #1058. I submitted a fix for those in PR #1060, which has not yet made it to PyPI. Try using the version of Requests from `master` and see if that solves your problem.
",Lukasa,erikcw
1074,2013-01-08 14:02:06,"@Lukasa Thanks!
",MicksMix,Lukasa
1071,2012-12-28 15:36:03,"@Lukasa I'll merge if you remove the print statement :)
",kennethreitz,Lukasa
1058,2012-12-22 12:12:55,"@sigmavirus24 is right. I've just posted a possible fix in #1056, so please follow the discussion there. Thanks for the report!
",Lukasa,sigmavirus24
1053,2013-01-19 17:11:26,"@Lukasa There are a bunch of other outstanding pulls, some of which may address this. I haven't gone through my backlog in a while. :( But you're welcome to throw something in just in case.
",shazow,Lukasa
1053,2013-02-09 03:19:25,"@socketubs, I think @Lukasa is close to fixing this for you. (Just to keep you up to date)
",sigmavirus24,Lukasa
1053,2013-02-09 14:17:56,"I think we're just waiting for @shazow to merge a change over on urllib3 before we pull that in.
",sigmavirus24,shazow
1051,2012-12-21 13:25:48,"I'm not saying we ignore it, I'm just saying it isn't against spec. And yeah, I agree that the 503 looks like it's a malformed request error. I'll mock up conditional addition for GETs tonight and see if @kennethreitz wouldn't mind the minor extra complexity.
",sigmavirus24,kennethreitz
1050,2012-12-19 21:54:21,"@kennethreitz I love this refactor more and more.

@PaulMcMillan can you post the call you're making or an example of it? This fix was introduced ~0.14.1

And by example, I mean: fake header and fake URI
",sigmavirus24,PaulMcMillan
1050,2012-12-19 21:54:21,"@kennethreitz I love this refactor more and more.

@PaulMcMillan can you post the call you're making or an example of it? This fix was introduced ~0.14.1

And by example, I mean: fake header and fake URI
",sigmavirus24,kennethreitz
1050,2013-01-24 12:37:48,"I'm closing this as resolved by v1.0.4. =) @PaulMcMillan, I understand your worries about the maturity of v1, but there's not much I can do about it other than to say that any bugs that are in v0.14 will never be fixed. Bugs in v1 will be. =) Thanks for raising this issue!
",Lukasa,PaulMcMillan
1049,2012-12-19 21:34:11,"@Lukasa  import it :) 
",kennethreitz,Lukasa
1049,2012-12-19 21:35:59,"C'mon @Lukasa you need some sleep brother.
",sigmavirus24,Lukasa
1049,2012-12-19 21:56:02,"@Lukasa take a break from everything, you deserve it. :fistbump:
",sigmavirus24,Lukasa
1049,2012-12-22 10:55:45,"@kennethreitz, what's the state of this? Got `basestring` in now, so all should be good. =)
",Lukasa,kennethreitz
1045,2012-12-19 20:31:49,"@slingamn :heart: 

Quite right, the cookiejar should be empty. I clearly need to take a nap. Or a holiday.

Not a bug! Thanks for the report, @odedgolan, I'm now closing this. =)
",Lukasa,slingamn
1045,2012-12-19 20:45:05,"True, it is the standard mechanism for deleting cookies. 
This is why it needs to be included in the CookieJar so it can be sent in turn to a browser, and the browser can delete its local cookie.

I do not wish to use Requests as a full implementation of a browser but as a vessel for HTTP requests. 
@Lukasa @slingamn What do you think? Don't you think I'm right on this?

Thank you,
Oded.
",odedgolan,slingamn
1045,2012-12-19 20:45:05,"True, it is the standard mechanism for deleting cookies. 
This is why it needs to be included in the CookieJar so it can be sent in turn to a browser, and the browser can delete its local cookie.

I do not wish to use Requests as a full implementation of a browser but as a vessel for HTTP requests. 
@Lukasa @slingamn What do you think? Don't you think I'm right on this?

Thank you,
Oded.
",odedgolan,Lukasa
1045,2012-12-20 06:18:16,"+1 for @sigmavirus24 here; it sounds like you want to pass through the original headers, including the Cookie header.

However, if you really want to do this via `CookieJar`, you can write your own `CookieJar` class that implements the policy you want and then pass it in to Requests via the `cookies` kwarg. Requests has its own `RequestsCookieJar` but it's only a way to provide a convenient external API, internally we should be able to work with any `CookieJar` subclass.
",slingamn,sigmavirus24
1045,2012-12-20 07:55:14,"@sigmavirus24 non taken, no worries. 

I cannot just pass the header, I need some manipulation ability on the cookies.

Subclassing CookieJar sounds like the right solution but passing it to Requests via cookies kwarg will set the ""to send"" CookieJar, not the ""to receive"" CookieJar.
",odedgolan,sigmavirus24
1041,2012-12-19 02:34:30,"@bboe your server and your test works fine.
",sigmavirus24,bboe
1041,2012-12-19 02:35:21,"@sigmavirus24 Do you mean you are unable to reproduce the problem?
",bboe,sigmavirus24
1041,2012-12-19 02:37:00,"@bboe exact opposite. I was just going to edit that since I realized it was a bit vague. Even calling release_conn doesn't work.
",sigmavirus24,bboe
1041,2012-12-19 02:43:14,"@sigmavirus24 Oh okay good. I was hoping I wouldn't have to find the set of platform specific settings to reproduce :) Does this mean you are going to look into this a bit then?
",bboe,sigmavirus24
1041,2012-12-19 02:52:25,"@bboe looking into it right now.
",sigmavirus24,bboe
1041,2012-12-19 03:06:25,"@sigmavirus24 Thanks for the quick fix. I'll use that for now.
",bboe,sigmavirus24
1041,2012-12-19 03:10:24,"@bboe glad to help. I'm trying to figure out what the best approach might be with urllib3 for now and how to test it.
",sigmavirus24,bboe
1041,2012-12-19 03:48:50,"@sigmavirus24 I'm thinking the issue should be fixed in urllib3. See https://github.com/kennethreitz/requests/blob/master/requests/models.py#L466

It seems that the entire content is received by requests, thus urllib3 should be aware of whether or not the connection is reusable as this point. If the connection is not re-usable, the socket should be closed.
",bboe,sigmavirus24
1041,2013-01-23 13:40:13,"@bboe your PR on urllib3 was accepted and we just updated urllib3 in requests, care to close this? 
",sigmavirus24,bboe
1034,2012-12-18 16:16:32,"> Kenneth thinks this API is better/cleaner.

@kennethreitz 

Do you, really? If yes, can you reference any articles proposing this or libraries doing this?
",piotr-dobrogost,kennethreitz
1034,2012-12-18 22:42:08,"> can you stop harassing me?

@kennethreitz 

I can't stop doing something I never started :) Relax. Whenever somebody states (Lukasa in this case) what the other person thinks (you in this case) I want to confirm if it's really the case. That's the reason I asked if you really think this API is better/cleaner. Not using parameters in a constructor of rather complex object in object orientated language is novelty to me. That's the reason I asked about some references. As everybody can see both questions are natural and I have no idea why did you come up with this harassment thing.
",piotr-dobrogost,kennethreitz
1034,2014-10-13 13:41:54,"@schlamar I would also remind you of the fact that attacks on other contributors are not welcome here. If you aren't going to be cordial towards @piotr-dobrogost and every other contributor to requests, you will not [be welcomed](http://www.kennethreitz.org/essays/be-cordial-or-be-on-your-way) here. No contributor to this project has the right to attack another and it will not be tolerated.
",sigmavirus24,schlamar
1031,2012-12-18 14:52:26,"Always welcome @kennethreitz 
",sigmavirus24,kennethreitz
1028,2013-02-15 20:19:13,"Just weighing in as I only noticed this change today. First, @kennethreitz, I totally agree with your reasons for changing it and I'm glad you did! This is a good upgrade.

That said, I might have preferred a staged transition, like:

v 0.15.0:
- `response.json` is renamed to `response.json_property`
- `response.json_method` is the method version of `response.json_property`
- `response.json` is a new property that issues a DeprecationWarning and returns `response.json_property`

End user actions: replace `response.json` with `response.json_property` throughout their code. Other co-installed projects designed for 0.14.1 can keep using `response.json` for now. Installations can upgrade from 0.14.1 to 0.15.0 without breakage.

v 1.0.0:
- `response.json` now points to `response.json_method`

End user actions: replace `response.json_property` with `response.json()` throughout their code. Other co-installed projects designed for 0.15.0 can keep using `response.json_property` for now. Installations can upgrade from 0.15.0 to 1.0.0 without breakage.

v 1.1.0:
- `response.json_property` and `response.json_method` are removed

I find myself in the situation where it's easy enough to upgrade the code I'm writing to use the 1.0.0 API (which I think is great!), but I have to coordinate with other departments so that we all make the from from 0.14.1 to 1.0.0 at the same moment. There's no intermediate state that supports both the old and new styles at the same time.
",kstrauser,kennethreitz
1025,2012-12-17 23:03:42,"So @sigmavirus24 it seems that GAE support is deprecated :(
",PanosJee,sigmavirus24
1025,2012-12-17 23:08:31,"Thanks @shazow , I ll try to patch it first!
",PanosJee,shazow
1023,2012-12-19 01:44:49,"@sigmavirus24 Cool!!! 1.0.3 works fine with my code, thx for your great help. But encode function's code is used for commercial encrypted file, I can't post it, sorry. 
",myzhan,sigmavirus24
1018,2012-12-17 18:13:08,"@kennethreitz Well, that just isn't going to do. All code must be shipped at once. No :cake: for you. :rage4: 
",michaelhelmick,kennethreitz
1018,2012-12-17 18:13:24,"@Lukasa Sure.
",michaelhelmick,Lukasa
1005,2012-12-17 22:37:57,"@sigmavirus24 yep, I saw that. Just saying to highlight it in the porting doc :)
",woozyking,sigmavirus24
995,2012-12-12 09:56:28,"@shazow Last I checked, @kennethreitz's 'official' position was that we must support 2.7 and 3.3, and anything else is a bonus.
",Lukasa,kennethreitz
995,2012-12-12 09:56:28,"@shazow Last I checked, @kennethreitz's 'official' position was that we must support 2.7 and 3.3, and anything else is a bonus.
",Lukasa,shazow
995,2012-12-13 18:10:31,"@shazow see you over at shazow/urllib3#9 ...
",fhsm,shazow
995,2012-12-16 06:04:27,"@kennethreitz I've had a look around urllib3 and started working on getting `source_address` into the API. Making it work is easy, making it fail nicely is much harder. How would you envision exposing `source_address` in the the requests' API? 

I'm struggling with a good way to avoid making an API that invites user's to over specify the outbound sockets used by the connection pool such that they are slammed with _error 98_'s and we are slammed with all sorts of impossible to reproduce issues around socket allocation/release.
",fhsm,kennethreitz
992,2012-12-06 21:56:27,"@mwielgoszewski just to elaborate on @kennethreitz's comment.

With the `pre_fetch=False` option, your solution will not work for `__len__` when the server doesn't provide a Content-Length header. As for iterating over the response, there's far more to a response than just the content. So iterating over a response makes little sense in my personal opinion. As it is, the API for iterating over the content is complete for both values of `pre_fetch`.

Stick around, I'm sure there's far more you can contribute elsewhere in the project.
",sigmavirus24,kennethreitz
992,2012-12-06 21:56:27,"@mwielgoszewski just to elaborate on @kennethreitz's comment.

With the `pre_fetch=False` option, your solution will not work for `__len__` when the server doesn't provide a Content-Length header. As for iterating over the response, there's far more to a response than just the content. So iterating over a response makes little sense in my personal opinion. As it is, the API for iterating over the content is complete for both values of `pre_fetch`.

Stick around, I'm sure there's far more you can contribute elsewhere in the project.
",sigmavirus24,mwielgoszewski
990,2012-12-06 18:23:54,"Thanks for the report @ekratskih, and thanks for taking this off our hands @shazow! :cake:

Closed as 'not our fault'.
",Lukasa,shazow
989,2012-12-06 22:14:54,"@slingamn yes, the first item on your list I believe is related
",sigmavirus24,slingamn
989,2012-12-06 22:34:26,"@sigmavirus24: Good point well made. =D

Nevertheless, I'd be inclined to go slightly smaller. 512 is tempting.
",Lukasa,sigmavirus24
988,2012-12-22 11:15:36,"I'm sure @shazow would be delighted to have extra logging in urllib3. =D
",Lukasa,shazow
982,2012-12-23 11:32:02,"@vlaci 
Have you seen https://github.com/shazow/urllib3/blob/master/dummyserver/server.py?
",piotr-dobrogost,vlaci
981,2012-12-01 20:35:02,"@sigmavirus24, he referenced #769 in his post, he knows. =)
",Lukasa,sigmavirus24
976,2012-11-29 20:34:57,"Yeah I'm working on a single fork of chardet that works under both python 2 and 3. That's over at sigmavirus24/charade (pull requests welcome) and that will become the new standard. @kennethreitz pulled in a non-functional version to replace chardet with. Any imports referring to chardet should now be broken and replaced by charade. For right now, the charade tests are failing unfortunately and I hope to have them passing in the majority soon. The latest version should be entirely functional.
",sigmavirus24,kennethreitz
975,2013-01-23 18:11:32,"@sigmavirus24: I'd forgotten about this, so thankyou! I'll take a look at it tonight and see how problematic it's likely to be.
",Lukasa,sigmavirus24
975,2013-01-23 19:20:41,"Should auth persist across redirects to subdomains? What about from one subdomain to another? Put another way, should auth persist in these situations (not likely to happen in real life)?

http://google.com/ -> http://mail.google.com/
http://www.google.com/ -> http://mail.google.com/

Thoughts @sigmavirus24 @kennethreitz?
",Lukasa,kennethreitz
975,2013-01-23 19:20:41,"Should auth persist across redirects to subdomains? What about from one subdomain to another? Put another way, should auth persist in these situations (not likely to happen in real life)?

http://google.com/ -> http://mail.google.com/
http://www.google.com/ -> http://mail.google.com/

Thoughts @sigmavirus24 @kennethreitz?
",Lukasa,sigmavirus24
975,2013-01-23 20:46:15,"@kennethreitz, while I like that I think if we see a 307 (after a POST) we should just return that response as if the user had specified allow redirects to be False. It's safer and it follows spec.
",sigmavirus24,kennethreitz
975,2013-01-23 21:04:15,"@kennethreitz; Are you happy to make that the policy? If so, I'm happy to simply say that the user should disallow redirects and not write any code. =P
",Lukasa,kennethreitz
972,2012-11-28 03:15:11,"@matthewlmcclure try out my branch, this should fix your issue.
",sigmavirus24,matthewlmcclure
970,2012-12-15 21:11:21,"@sigmavirus24 the best part is that it's fully mutable :)
",kennethreitz,sigmavirus24
968,2012-11-27 18:47:55,"@kennethreitz: I can't see that requirement in the License instructions...
",Lukasa,kennethreitz
968,2012-11-27 18:50:10,"@Lukasa hmm, it's a long document. I might be crazy ;)
",kennethreitz,Lukasa
968,2012-11-27 18:53:26,"Humorously enough @kennethreitz, you might be thinking of the GPL's requirement to have itself in **every** file ;)
",sigmavirus24,kennethreitz
968,2012-11-27 19:05:52,"@Lukasa I think it's appropriate.

What shall we call it? Chardetter? FuckUnicode?
",kennethreitz,Lukasa
968,2012-11-27 19:10:07,"CharStar? Pointers and whatnot :P

@shazow I was thinking of that too ;)
",sigmavirus24,shazow
968,2012-11-27 19:34:15,"@Lukasa and all they saw when reading your comment was ""Don't bother doing this"" 
",sigmavirus24,Lukasa
967,2012-11-27 18:45:29,"Thanks @kennethreitz.
",matthewlmcclure,kennethreitz
964,2012-11-27 12:09:08,"@Lukasa that's exactly the intention: :cocktail: 
",kennethreitz,Lukasa
963,2012-11-27 16:56:06,"@Lukasa for the time being I thinik it's fine to add it to the Requests documentation since that's where everyone will be looking for the time being.

@matthewlmcclure it won't hurt to submit a PR. On the other hand, afaik, oauthlib doesn't have OAuth2 completely finished. Their API might be finished but having information in the tutorial about that may be misleading for the moment. As for a start-to-finish tutorial using the Twitter API as an example, perhaps we could get Twython's permission to excerpt/adapt some of their code for the example?
",sigmavirus24,Lukasa
963,2012-11-27 16:56:06,"@Lukasa for the time being I thinik it's fine to add it to the Requests documentation since that's where everyone will be looking for the time being.

@matthewlmcclure it won't hurt to submit a PR. On the other hand, afaik, oauthlib doesn't have OAuth2 completely finished. Their API might be finished but having information in the tutorial about that may be misleading for the moment. As for a start-to-finish tutorial using the Twitter API as an example, perhaps we could get Twython's permission to excerpt/adapt some of their code for the example?
",sigmavirus24,matthewlmcclure
963,2012-12-19 05:38:30,"@r1chardj0n3s it'll be updated shortly. The new library that provides the OAuth class is `requests-oauthlib`
",kennethreitz,r1chardj0n3s
962,2012-11-26 20:24:54,"@Lukasa I should have added that I'm +10 on waiting for @kennethreitz to weigh in. ;)
",sigmavirus24,Lukasa
961,2012-11-25 22:25:21,"@Lukasa : I'm not surprised the web site is misbehaving (I should have probed more thoroughly)  --  thanks for the mitigation tip.
",inactivist,Lukasa
961,2012-11-25 22:31:39,"@Lukasa: Thanks for the positive feedback.  I aims to please!

I'm definitely a requests newbie, but so far I love it.  And I have to say, the response here (on a Sunday, no less!) was freaking awesome.  Thanks, @all!
",inactivist,Lukasa
959,2012-11-26 01:12:26,"@kennethreitz [PEP386](http://www.python.org/dev/peps/pep-0386/) actually allows for the syntax I suggested. See this [section](http://www.python.org/dev/peps/pep-0386/#the-new-versioning-algorithm) in specific.
",sigmavirus24,kennethreitz
957,2012-11-25 09:34:32,"@sigmavirus24 Re: handle Python 2 and 3, I'd like to suggest [Pythonbrew](https://github.com/utahta/pythonbrew).
",Lukasa,sigmavirus24
957,2012-11-25 12:34:33,"@Lukasa very interesting. Thanks! I'll look more closely at it later tonight or tomorrow. 
",sigmavirus24,Lukasa
956,2012-11-26 19:01:12,"I'm not that surprised that it broke. =(

When @idan can work out what the break is I'll open another PR to fix the issue.
",Lukasa,idan
953,2012-11-26 08:44:39,"@kennethreitz 

I'm curious why didn't you merge my 6 months old pull request #671 which solves problem for all cases but you merged this one which does not? Thanks.
",piotr-dobrogost,kennethreitz
951,2012-11-24 11:24:35,"@Lukasa @kennethreitz #939 solves these problems without any major disadvantage I can think of. Furthermore it has the slight advantage of letting the package maintainers easily add separate versions of any other package `requests` may depend on in the future without having to follow this same forking approach everytime. But this is just my humble opinion, of course :-)
",ghost,kennethreitz
945,2012-11-18 21:21:28,"@Lukasa py.test is still the best :)
",piotr-dobrogost,Lukasa
944,2012-11-19 01:38:46,"@kennethreitz is the plan to fix it for 3.2, or to wait until Py3.3?
",sybrenstuvel,kennethreitz
944,2012-11-23 10:24:06,"Had to be done, I just didn't want to be the one to do it because @kennethreitz might make me maintain it. The idea of working with OAuth keeps me up at night.
",Lukasa,kennethreitz
944,2012-11-23 10:29:55,"@sybrenstuvel: Sucks to be you. =D

@kennethreitz: Mm, there's a lot on your plate. I tried to go through and close a few issues last week, because it had gotten a bit out of control. ATM I'm trying to triage new issues so I can keep the trivial ones off your radar.
",Lukasa,kennethreitz
944,2012-11-23 10:31:17,"@Lukasa you're doing an absolutely kick-ass job, and you've made this project twice as sustainable as it already was. This Thanksgiving, I'm thankful for you :P

:sparkles: :cake: :sparkles:
",kennethreitz,Lukasa
944,2012-11-23 10:33:08,"@Lukasa haha thanks :P

@kennethreitz and @Lukasa thank you both for the great lib! My work has become a lot easier thanks to you guys.
",sybrenstuvel,kennethreitz
944,2012-11-23 10:33:08,"@Lukasa haha thanks :P

@kennethreitz and @Lukasa thank you both for the great lib! My work has become a lot easier thanks to you guys.
",sybrenstuvel,Lukasa
944,2012-11-23 11:03:56,"@sybrenstuvel: In all seriousness, I run around, patch small holes, close issues and write docs. The general awesomeness of this library was in place before I got to this project, I'm just basking in reflected glory. =)

@kennethreitz: Thanks Kenneth, always affirming to know that my contribution is valuable. =D This thanksgiving (and the one before, and the one before that...) I'm thankful for your ludicrous productivity. If I was half as productive as you I'd be way better paid. =P
",Lukasa,kennethreitz
938,2012-11-15 20:03:16,"As @kennethreitz pointed out, the org needs a logo. I'm design-blind and have no taste, so someone else should take that on. =D
",Lukasa,kennethreitz
936,2012-12-02 19:28:08,"@sigmavirus24 is correct
",kennethreitz,sigmavirus24
935,2012-11-10 17:38:01,"@kennethreitz 
What's great about using `files` as the name of the parameter for sending multipart data? Multipart data has nothing to do with sending files. Using (abusing) `files` to send data when there already exists `data` param is illogical. The fact it's wrong comes from the fact that every file is kind of data but data is not always a file. If anything we could expect users to pass files to `data` param but not the other way around like it is now. Sending files may require giving more metadata than sending plain data so it's kind of ok to have additional `files` param for this purpose. But there's no justification for forcing users to pass their data to `files` param if all they want is to send _data_ not files. `Files` param is currently abused to send multipart data only because it happens that uploading files uses multipart transfer encoding in http which is just implementation detail and as such shouldn't be exposed in any way through the API.

Also, could you elaborate on what's wrong with tuples as `files` value?
",piotr-dobrogost,kennethreitz
935,2016-01-31 07:58:37,"@kennethreitz It already exists [here](http://toolbelt.readthedocs.org/en/latest/uploading-data.html#streaming-multipart-data-encoder).
",Lukasa,kennethreitz
934,2012-11-13 00:50:56,"Use git-svn @mastahyeti ;)
",sigmavirus24,mastahyeti
934,2012-11-15 11:02:17,"@Lukasa I'd feel much more comfortable with that myself :)
",kennethreitz,Lukasa
934,2012-11-15 11:08:19,"Cool. I'll have a think this evening about how best to handle this. Maybe something in the docs.

In the meantime, @mastahyeti, how do you feel about acting as maintainer of a Requests NTLM authentication package?
",Lukasa,mastahyeti
934,2012-11-15 13:12:01,"Sounds like a good idea, @sigmavirus24. I won't look into it until I finish work, but that combined with some documentation might be a good way to keep everything together.
",Lukasa,sigmavirus24
934,2012-11-15 14:09:22,"Documentation makes everything better. Let's just wait on @kennethreitz to make the org if he agrees. 
",sigmavirus24,kennethreitz
934,2012-11-15 14:18:59,"@kennethreitz Yeah, that occurred to me too. I wouldn't have proposed moving it: it's yours, and besides, we'd break ALL THE LINKS. I then thought about mirroring requests, but that's also a bad idea: there should only be one place on Github where you can go for requests itself.

Otherwise, that might be a decent idea. What auth stuff would we move? (This, obviously, but what else would you like to pull out?)
",Lukasa,kennethreitz
934,2012-11-15 14:23:33,"_/me casually deletes out-of-date post._

Awesome, sounds good to me. @mastahyeti, you cool with maintaining a repo there instead of this pull request?
",Lukasa,mastahyeti
934,2012-11-15 14:35:41,"@mastahyeti, you should have push access to the repo now :)
",kennethreitz,mastahyeti
927,2012-11-24 03:47:42,"@Lukasa there shouldn't be such a parameter ;)
",sigmavirus24,Lukasa
927,2012-11-24 08:45:39,"@Lukasa Yes, it does, thank you for thorough answer. And i just tested, the #44 would be satisfied now that requests actually send Basic right away. Everything is consistent now.

Thank you.
",temoto,Lukasa
924,2012-11-25 17:17:43,"

@kennethreitz
But this is not peeking what OP askes for but reading which would influence data returned by the api like `.content` etc., right? Is the untold assumption that a client must remember to prepend data read outside of the api to what api returns later?

It looks like it would be possible to do with slightly modified version of [`tee`](http://docs.python.org/2/library/itertools.html#itertools.tee) from `itertools`. The modification would be to allow for removing given deques so that they would not receive and accumulate further data.

UPDATE

This is not as straightforward as `iter_content` is a generator not an iterator. Still I think it's doable by making `Response.raw` an iterator and teeing it.
",piotr-dobrogost,kennethreitz
917,2012-11-23 19:38:38,"I think passing params without values should be supported by `params` parameter. Currently setting param's value to `None` in `params` dict makes Requests skip adding the param to a query string. I propose to change this behavior so that in such case param is added without value. @kennethreitz What do you think?
",piotr-dobrogost,kennethreitz
917,2012-11-26 07:56:58,"> it would be nice to be able to express the contents of the query string in the `params` argument

Totally agree. It's misleading at best to have to embed query string into url in spite of the fact there's `params` param. Making `params` support pre-formatted query string goes in right direction. However having one parameter for both makes it impossible to have both at the same time. I would suggest adding new parameter `query_string` and combining contents of it with contents of `params` and the query string part of the url itself.
@kennethreitz Is this ok?
",piotr-dobrogost,kennethreitz
913,2012-11-16 22:29:09,"@kennethreitz I don't see `encode_urls` anywhere.
",sigmavirus24,kennethreitz
910,2012-10-26 17:31:54,"@sigmavirus24 Thank you for your prompt reply.

I'm realizing that I wasn't clear enough about what I'm trying to accomplish.  Sorry about that.

We are a PaaS provider. I'm writing a client for the public api of our service.  ( I left the api key and secret in the gist since they are from a dev server running on my laptop. ) 

One of the call I have to make is a POST to http://localhost:8003/jobs.  This call requires a job parameters JSON data in the request body such as:



and also requires a signed oauth header.  Our api implement 2-legged oauth with header signature type.

I tried bunch of different variations:

---

if I pass a simple dictionary structure as data such as:



requests generate the following request:



It includes an Authorization header, set the Content-Type to application/x-www-form-urlencoded and some data in the body as I would expect.

---

if I replace the simple dictionary structure above with something more complex, such as:



requests generates a traceback:



---

if i pass JSON encoded string as data such as: 



requests generate the following request:



It contains the JSON data as string but no Authorization header and not content-type

---

if i pass JSON encoded string as data and specify the content type such as: 



requests generate the following request:



It contains JSON data, Content-Type but no Authorization header

---

So basically the client I'm writing has to be able to send json data and must contain an Authorization header.
Is this possible using the requests module? 

I have successfully built a ruby client that does what I need.  I like working with requests and really want to figure this out so I can do the same with requests.

Thank you for your time
",alex-ethier,sigmavirus24
910,2012-10-26 21:11:59,"Thank you @sigmavirus24.

@kennethreitz, Any thoughts on this? 

Thanks!
A
",alex-ethier,kennethreitz
910,2012-10-26 21:11:59,"Thank you @sigmavirus24.

@kennethreitz, Any thoughts on this? 

Thanks!
A
",alex-ethier,sigmavirus24
910,2012-11-24 20:29:57,"Thankyou @idan! I think this means we've got a bug in the OAuth code here. I'll fix it up. Do you mind if I notify you when I've done it so you can take a quick look?
",Lukasa,idan
910,2012-11-24 20:30:56,"Go for it. Open a ticket on idan/oauthlib :)  

On Saturday, November 24, 2012 at 10:30 PM, Cory Benfield wrote:

> Thankyou @idan (https://github.com/idan)! I think this means we've got a bug in the OAuth code here. I'll fix it up. Do you mind if I notify you when I've done it so you can take a quick look?
> 
> —
> Reply to this email directly or view it on GitHub (https://github.com/kennethreitz/requests/issues/910#issuecomment-10683343).  
",idan,idan
910,2013-02-20 17:27:03,"@idan @Lukasa any opinions?
",sigmavirus24,idan
910,2013-02-20 17:27:03,"@idan @Lukasa any opinions?
",sigmavirus24,Lukasa
910,2013-02-21 10:52:05,"@Lukasa You don't know how happy you've made me by saying ""Goddam I hate OAuth"" :D

http://api.projectplace.com/index.php/DocumentUpload is the one that was giving me problems. The body content is just the contents of a file.



(This works with my local copy `prepare` bodge).
",daycoder,Lukasa
908,2012-10-24 18:26:22,"@bmannix
See issue #361.

@kennethreitz
How about contrib module?
",piotr-dobrogost,kennethreitz
907,2012-11-28 13:31:49,"@Lukasa ironic you say that given how many people's Arch installs get fubarred because of the rolling release model that has notoriously had all sorts of problems.
",sigmavirus24,Lukasa
907,2012-11-28 19:47:48,"@Lukasa that's a much better explanation than I wrote. +1 good sir
",sigmavirus24,Lukasa
907,2012-11-29 13:13:06,"I agree with @slingamn and have a feeling the whole discussion here went in the wrong direction. It's not task of any Python library to set variables such as `PYTHONDONTWRITEBYTECODE` similar to how it's not its task to configure logging. Both are tasks of users of the library. Conclusion, `PYTHONDONTWRITEBYTECODE` should have never been manipulated by Requests. This pull removes manipulation so it fixes the problem. We should merge this and close this issue as fixed.
",piotr-dobrogost,slingamn
907,2012-11-29 13:29:44,"@sigmavirus24: I :heart: you
",kennethreitz,sigmavirus24
907,2012-11-29 13:55:10,"@sigmavirus24 how close is it to being in a usable state?
",kennethreitz,sigmavirus24
907,2012-11-29 13:58:37,"Hopefully ~1 week. I haven't changed anything that would change the code, I'm 
just doing the 2to3 work and making it so that flake8 won't kill vim whenever 
I open a file (when possible). Also there's not a single test for this, so 
having an easy guide as to it working in python3 won't help.

I'm going to reference that guide from Dive into Python3 though to make sure I 
haven't missed anything. And be sure, the latter part (pep8-ifying the files) 
is the time consuming part.
On Thu, Nov 29, 2012 at 05:55:21AM -0800, Kenneth Reitz wrote:

> @sigmavirus24 how close is it to being in a usable state?
> 
> ---
> 
> Reply to this email directly or view it on GitHub:
> https://github.com/kennethreitz/requests/pull/907#issuecomment-10847816
",sigmavirus24,sigmavirus24
905,2012-11-26 19:03:57,"@sigmavirus24: +1.

The problem is in urllib3, not Requests. I'm only leaving the issue open to attempt to prevent too many people opening duplicates. Problems in urllib3 should be raised against and resolved in urllib3.
",Lukasa,sigmavirus24
905,2012-11-27 23:07:51,"@Lukasa I was under the impression this was already fixed in urllib3 as per https://github.com/shazow/urllib3/pull/68#issuecomment-10127574 and Requests just needs to be updated with the latest version of urllib3.
",machinae,Lukasa
905,2012-11-28 00:41:33,"That's quite wrong but understandable 

Ilya Lichtenstein notifications@github.com wrote:

> @Lukasa I was under the impression this was already fixed in urllib3 as
> per https://github.com/shazow/urllib3/pull/68#issuecomment-10127574 and
> Requests just needs to be updated with the latest version of urllib3.
> 
> ---
> 
> Reply to this email directly or view it on GitHub:
> https://github.com/kennethreitz/requests/issues/905#issuecomment-10781919
",sigmavirus24,Lukasa
895,2012-11-27 19:03:56,"@kennethreitz don't forget to reopen this one too ;)
",sigmavirus24,kennethreitz
882,2012-10-07 18:52:43,"> I see no need to split.

I don't quite catch what you mean @kennethreitz 
",sigmavirus24,kennethreitz
882,2012-11-24 11:57:26,"@kennethreitz: Do we want to fix this, or just stop claiming that we support Python 3.1, seeing as we don't test on it anyway?
",Lukasa,kennethreitz
882,2012-11-27 02:04:10,"Well in that case, perhaps removing the guarantee of 3.1 compatibility is the way to go. It would seem @Lukasa nipped this in #955 though, so this should be fixed. @Arfrever care to confirm?
",sigmavirus24,Lukasa
882,2012-11-27 16:28:41,"@Lukasa well explained. Thank you.
",sigmavirus24,Lukasa
879,2013-10-28 22:50:53,"Hi @Lukasa , I was having this issue with Requests 1.2. After reading this thread, I thought it will go away on upgrading to 2.0.1 (which contains your fix). However, it did not fix it for me. 
The only way I got around it was to add the following code: 


",tanmay9,Lukasa
875,2012-11-27 04:18:07,"So to flesh out @vlaci's proposed patch:



should be feasible? I don't have a proxy to test against.

@vlaci if I'm misspeaking please pipe up. It'd even be awesome if you submitted the Pull Request to fix this.
",sigmavirus24,vlaci
875,2012-11-27 16:44:59,"@vlaci awesome. Thanks for finding, reporting and fixing this.
",sigmavirus24,vlaci
871,2012-10-01 16:52:57,"@yegle care to update?
",kennethreitz,yegle
868,2012-09-26 19:32:04,"> Well that was painful

@sigmavirus24 What do you mean? Btw, thanks for creating this issue.
",piotr-dobrogost,sigmavirus24
868,2012-09-29 18:49:51,"Agreed @shazow, I'm probably going to change some of them back and squash the commits before turning that branch into a Pull Request which is probably a while off.
",sigmavirus24,shazow
868,2012-09-29 19:07:21,"@shazow Then please show us which ones are good and why? :) Btw, yours _is_foo helper_ idea was heading in right direction. This is what ABCs in collections module are for although it looks like they specify too few interfaces.
",piotr-dobrogost,shazow
868,2012-09-29 21:35:29,"@kennethreitz
People use different containers - see for example http://sebsauvage.net/python/snyppets/index.html#dbdict If api is restricted only to standard ones then people can't use their custom made containers.
",piotr-dobrogost,kennethreitz
868,2012-09-29 21:54:46,"@shazow 
I meant the ones we are talking here :) Like checking for dict, list, set etc.
",piotr-dobrogost,shazow
868,2012-09-29 21:56:04,"@piotr-dobrogost A big chunk of the results in @sigmavirus24's grep were for backwards compat. Another chunk was the base case reduction to 'str', which otherwise uses duck typing as you prefer. :) I suspect about half of the rest could be removed/improved.
",shazow,sigmavirus24
867,2012-09-26 19:42:17,"@sigmavirus24 sounds reasonable!
",sweenzor,sigmavirus24
864,2012-09-23 23:48:31,"I'm with @shazow on this one, and I hadn't even seen it in use with PHP until I saw it [here](http://stackoverflow.com/questions/353379/how-to-get-multiple-parameters-with-same-name-from-a-url-in-php).
",sigmavirus24,shazow
856,2012-09-17 18:00:54,"On an entirely unrelated note @mastahyeti , it appears your commiter email address and the address you used to sign up for GitHub are not one and the same. You can add that email address to your GitHub profile if you want the committer/author on the diffs to point back to you.
",sigmavirus24,mastahyeti
851,2012-09-24 16:10:15,"@joequery not your fault ;)
",kennethreitz,joequery
851,2013-01-21 21:07:33,"@joequery: Is your specific issue that you need to force SSLv3 on Ubuntu?
",Lukasa,joequery
851,2013-01-21 21:15:47,"@joequery If you are able to run (or update to) requests 1.0, https://github.com/kennethreitz/requests/issues/1083#issuecomment-11853729 might be helpful.
",joeshaw,joequery
851,2013-01-21 21:23:22,"@joequery: You aren't creating a stir. =) If you're having a problem, we want to fix it.

#1083 absolutely does **not** work pre version 1. I should know, I wrote it. =) In fact, the first sentence reads:

> Requests does not support doing this before version 1.

The examples provided are for post v1.

You are correct that this is not in the docs, and I don't think it belongs there. If you want a more detailed explanation, I have a [blog post about it](http://lukasa.co.uk/2013/01/Choosing_SSL_Version_In_Requests/).
",Lukasa,joequery
849,2012-09-12 17:39:15,"I actually think this is because `dict.items()` returns an iterator now instead of a list like in python2. This should be fixable by doing `list(dict.items())` but I could be wrong. Another option would be to do:



Since you're not changing the dictionary you're iterating over (although this is just an expansion on what @barberj was say).
",sigmavirus24,barberj
847,2013-06-19 08:05:44,"I'm glad you got something working for your use case, @mlissner!

I doubt Requests will ever have direct support in the main library for the `file:` URL scheme. Aside from Kenneth's point above about it being a security risk, Requests is strictly an HTTP library. `file:` URLs do not use HTTP, so they're outside the Requests use case.

If you wanted a more transparent example, you should be able to use something like this:



Mounting that transport adapter _should_ (I haven't tested it) provide you with the behaviour you want.
",Lukasa,mlissner
847,2013-06-19 19:15:05,"Thanks @Lukasa. I gave that a whirl and tried to make it work for a bit, but I ran into an error I can't get past.

This line is the one causing trouble: https://github.com/kennethreitz/requests/blob/master/requests/cookies.py#L109

From what I can tell, setting response.raw to a file or StringIO object won't work because: 



I'm not sure how to proceed with this, though I read your FTP Adapter code and blog entry (but there's not much else on transport adapters yet!). I'm tempted to say that not allowing file-like objects is a bug, but I know that's been closed before as a wont-fix. I've also gotten the impression that _original_response shouldn't be used for anything (though it is used in cookie.py). So...I'm confused how to make this work properly, whether _original_response shouldn't be used the way it is, and what the proper fix for this should be.
",mlissner,Lukasa
847,2013-06-19 19:37:56,"@sigmavirus24 I took a look at requestions, but don't see any use of adapters (or workarounds) in there. Regardless, I'd rather avoid the dependency if possible.
",mlissner,sigmavirus24
844,2012-09-07 07:05:33,"@slingamn thanks so much for stepping up with this ;)
",kennethreitz,slingamn
844,2012-09-07 15:48:01,"@shazow Because the synchronous network read call will not return until the specified amount of bytes has been read or the stream is closed. Thus, is you use a 1K read size and the server returns lines averaging 100 bytes, you will have to wait for the server to return at least 11 lines before `iter_lines` return any lines and your Python code is executed. If one line is sent every minute, it will take 11 minutes before your code is even aware of the first line sent by the server.
",mponton,shazow
844,2012-10-04 08:42:48,"@slingamn Could you elaborate on point 3 from your initial list?
",piotr-dobrogost,slingamn
844,2013-01-23 08:25:02,"@slingamn pointed out that there are a few issues here that are still unresolved. Sometime today I'll go through and work out which ones still haven't been done.
",Lukasa,slingamn
844,2013-02-10 22:56:32,"@slingamn #4's a documentation problem and needs to be corrected.
",kennethreitz,slingamn
844,2013-02-11 23:22:17,"I would like to jump in because it appears that a bit of the above discussion is based upon a false premise: that a synchronous network receive of _n_ bytes will block until at least _n_ bytes are received (or, presumably, the socket has closed).

This is not, in fact, the case, and Unix network programming would be a shambles if it were — think of the disaster that every network program would face: applications would have to make the horrible decision to either read data byte-by-byte, or block indefinitely if they overestimated, even very slightly, the amount of data about to arrive on a socket. Network programmers would all stand impaled upon the horns of a dilemma. Everyone might use Windows for network programming instead. :)

But, fortunately, the plain normal vanilla blocking synchronous version of the `recv()` call is merciful to us: it only blocks waiting as long as _no data at all_ is ready to be consumed! As soon as _even a single byte_ of data arrives in an incoming packet, your beefy call to `recv(1048576)` will return _that single byte_ and your program is off and running again. There is _no penalty_ for giving `recv()` permission to return lots of data if a lot of data arrives in a single packet, or arrives in several packets while the operating system is still getting around to scheduling your thread again.

It is true that, for those rare exceptional cases where you really _want_ to stay blocked because you really know that you need _n_ bytes before you can do anything useful, there exists a POSIX flag `MSG_WAITALL` that you can pass to `recv()` so that your program really does block until lots of data arrives. But that is a very rare flag to specify, for all of these reasons that we have put on the table here.

So what is the problem here, you ask?

Well, @mponton actually lets the cat out of the bag without knowing it! Look carefully at this phrase from his reply to @shazow:

“…the synchronous network read call…”

“Read” call? What? Who would do a `read()` call on a socket? While POSIX does allow `read()` and `write()` on sockets for those extremely rare cases where you pass a socket to a library that is only designed for talking to files, it's not something you would ever do in a network program—you would lock yourself up waiting for _n_ bytes to arrive even if less than _n_ bytes were available immediately! Who would do _that?_

Why, the author of `httplib`, of course!

Yes, that's right. Instead of simply sitting in a tidy standard `recv()` loop and slowly filling a buffer until an end-of-line is visible, the author of `httplib` **wraps the socket in a fake file-like object with `makefile()` and completely abandons all of the benefits of network programming under Unix!**

They gain a tiny bit of convenience — and maybe, way back when it was written, C-level performance? — by having a Python file-like object watch for the end-of-line character for them. But they **completely disabled the ability to stream live data from the network** by making this choice, probably because they were operating in an era when people read and wrote network payloads whole anyway.

I recommend that Requests move off of `httplib`. It does very, very little for you; it could be re-implemented much more simply with `recv()` and `send()` in a day or two, given the simplicity of HTTP. There is no reason why Python or Requests shouldn't have line-based or content-based iteration that returns the moment that enough data has arrived to satisfy the caller; you just need to move off of a broken implementation that wantonly imposes the semantics of the `read()` call on what is really a socket capable of doing `recv()` if you'll just ask nicely!
",brandon-rhodes,shazow
844,2013-02-12 00:33:16,"@shazow: I'm all for that! Let me know if you get off the ground with it and I'll do my best to help out.
",Lukasa,shazow
844,2013-02-12 03:15:04,"@slingamn There is a danger here: once the socket has been wrapped with a file-like object by `makefile()`, there is a chance that the stdio buffering will have read a bit _past_ the end of the first line and, after returning that line to you, will hold in its buffer a good bit of data that lay _after_ the line ending.

Usually this is no problem, as when you next issue `readline()` or `read()` the file buffering simply draws data from its buffer first before then turning to the file descriptor for more.

But if you try to go _around_ the stdio input buffering and read directly from the socket, you will be missing the chunk of data (if any) that had been read past the end of the line! Unfortunately, stdio provides no way to introspect a `FILE *` object and rescue the contents of its buffer without also probably inducing the punishment of a blocking `read()`. So once you have even for a moment used a file-like object to do your reading for you, you are committed: you cannot return to raw socket I/O.

It might be possible to craft your own file-like object; I found, for example, that `pywsgi.py` in `gevent` let me provide my own `rfile` object that implements `read()` and `readline()` on top of a socket, and the problem was solved.

It is also possible that you might want a way around the `FILE *` problem, or even a guarantee that no data will be trapped there. I am often wrong about such things, and you may see a way out where I have not yet.

But I think the whole issue is what lead the Python 3 people to ditch `FILE *` I/O entirely and write a new dedicated Python buffering subsystem directly atop POSIX system calls. If only we were working under Python 3, we might not even face these issues. Alas that all of our favorite toys keep us toiling under Python 2!
",brandon-rhodes,slingamn
844,2013-02-12 06:28:24,"@brandon-rhodes `makefile()` is implemented entirely in Python (in 2.7 at least) and is a wrapper around the low-level socket object. It is not a FILE \* object so it has nothing to do with stdio. Its internal buffering is implemented using StringIO. It has a `readline()` method that does exactly what we need (i.e. use `recv()` to get data from the network into a buffer and return immediately when an EOL is encountered) for `iter_lines()` (minus unicode decoding).

@slingamn The file-like object in `Response.raw` is a `urllib3` `HTTPResponse` wrapping a `httplib` `HTTPResponse`. The later uses `makefile()` to wrap the socket object but does no expose that socket object and thus it is not possible to call `recv()` from a `Response` object (well, unless you dig deep, see below).

As an example, here's an ugly kludge that would use an ""optimized"" way to read lines from the network:



Here's an example of the result when reading 10 lines generated at 5 seconds interval with a buffer size of 10K:

Original `iter_lines` (stuck in `read()` for 50 seconds):



""Fixed"" `iter_lines` (properly streaming every 5 seconds):



I'm sorry if my previous comments on network I/O might have been overly simplistic, but in the current context, they seemed right enough.

@brandon-rhodes Is right, the proper way of fixing this would be to either use `recv()` directly or use the `makefile()`-generated file-like object's `readline()` (which does exactly what Brandon describes). Considering that `httplib`'s HTTPResponse class does not ""officialy"" expose the file-like object it uses, using it for our own purpose may not be a good idea. This would be solved if `urllib3` stopped using `httplib` and properly exposed the methods/properties we need to fix this issue correctly but this would require some surgery...
",mponton,slingamn
844,2013-02-12 09:54:29,"@kennethreitz when you say that item 4 [here](https://github.com/kennethreitz/requests/issues/844#issuecomment-12587649) is a documentation issue, do you mean that there is currently no implementation of chunked encoding for incoming requests, and that the documentation needs to be changed to clarify that?
",slingamn,kennethreitz
844,2013-02-12 17:14:23,"@brandon-rhodes Well, thanks for the kind words! I did not know older versions of Python implemented the file-like object that way. Interesting. Glad they changed it.

@slingamn `line.rstrip()` will remove all newline characters and whitespaces from the right. This could change the lines in an undesirable way for some clients (maybe someone does expect/want right padding whitespace). I wonder if `iter_lines` should have kept the line break in the first place now... Just so it behaves more like the `file` iterator. A bit late now though... :-)

As for considering my kludgy example a fix, I'm a bit concerned :-) I suppose the `httplib` `HTTPResponse.fp` member probably won't change nor be removed as @brandon-rhodes said. I am more concerned by the access to `urllib3`'s `HTTPResponse._fp` member. Not only is this one underscore-prefixed, `urllib3` is still in active development. Maybe access to an exposed `fp` member (in the short term directly represented by the `httplib`'s own `fp`) could be provided officially by `urllib3` by @shazow ? (This would have implication for the future support/implementation of this now public member though...)

Also, please note that I did not validate any of this on Python 3. I'm still using 2.7 (yes, I'm late to the party, I know). Maybe the socket file-like object implementation or `httplib` module differ on this version.
",mponton,shazow
844,2013-02-12 17:14:23,"@brandon-rhodes Well, thanks for the kind words! I did not know older versions of Python implemented the file-like object that way. Interesting. Glad they changed it.

@slingamn `line.rstrip()` will remove all newline characters and whitespaces from the right. This could change the lines in an undesirable way for some clients (maybe someone does expect/want right padding whitespace). I wonder if `iter_lines` should have kept the line break in the first place now... Just so it behaves more like the `file` iterator. A bit late now though... :-)

As for considering my kludgy example a fix, I'm a bit concerned :-) I suppose the `httplib` `HTTPResponse.fp` member probably won't change nor be removed as @brandon-rhodes said. I am more concerned by the access to `urllib3`'s `HTTPResponse._fp` member. Not only is this one underscore-prefixed, `urllib3` is still in active development. Maybe access to an exposed `fp` member (in the short term directly represented by the `httplib`'s own `fp`) could be provided officially by `urllib3` by @shazow ? (This would have implication for the future support/implementation of this now public member though...)

Also, please note that I did not validate any of this on Python 3. I'm still using 2.7 (yes, I'm late to the party, I know). Maybe the socket file-like object implementation or `httplib` module differ on this version.
",mponton,slingamn
844,2013-02-12 19:39:12,"@shazow I guess since `requests` and `urllib3` are such good neighbours (and `requests` packages its own version), there is little benefit to make that official and instead, should `urllib3` internals change later, only `requests` will have to be adapted. I just feel dirty when I have to use `something._xyz` :-)

@slingamn On that previous `rstrip()` comment: I had a brain fart. We can simply specify `\n\r` to `rstrip()`. Dhuh! (Apparently I don't use `rstrip()` often enough...)

If everyone agrees this is an acceptable solution, I can make a pull-request in the next few days for it.
",mponton,shazow
844,2013-02-12 19:39:12,"@shazow I guess since `requests` and `urllib3` are such good neighbours (and `requests` packages its own version), there is little benefit to make that official and instead, should `urllib3` internals change later, only `requests` will have to be adapted. I just feel dirty when I have to use `something._xyz` :-)

@slingamn On that previous `rstrip()` comment: I had a brain fart. We can simply specify `\n\r` to `rstrip()`. Dhuh! (Apparently I don't use `rstrip()` often enough...)

If everyone agrees this is an acceptable solution, I can make a pull-request in the next few days for it.
",mponton,slingamn
844,2013-02-12 22:21:38,"Agreed that ideally `iter_lines` would return strings terminated by the original newline character(s), since that's how the file iterator works. @kennethreitz would this be too big of an API break to introduce?
",slingamn,kennethreitz
844,2013-04-17 07:26:43,"@Lukasa not unless it's the default.

Try this:

`requests.get('http://www.stkierans.org/')`
",akavlie,Lukasa
844,2013-04-18 04:49:04,"@Lukasa Thanks for all the digging in... I've seen lots of pointless iframes and other horrible practices on various church sites, so this does not suprise me.

I'm targeting a lot of sites with requests in this application, and specifying overrides for bad behavior like this isn't very realistic at this point.

Is it reasonable to expect Requests to catch and wrap an exception like this? It looked to me like Requests itself had a bug.
",akavlie,Lukasa
844,2013-04-20 06:07:26,"@Lukasa It would indeed be great if IncompleteRead was wrapped. Currently, when calling requests.get or requests.post, I need to catch both requests.exceptions.RequestException and httplib.IncompleteRead, which does not make sense. IncompleteRead should be turned into a RequestException or its subclass. 
",jpaalasm,Lukasa
832,2012-10-27 16:37:02,"> These libraries are vendored to make it possible to easily vendor Requests itself.

@kennethreitz 
How about changing the way vendorization is done by including dependencies in a zip and adding an option to setup.py to unzip them? This way vendorization could be sort of opt-in.
",piotr-dobrogost,kennethreitz
832,2012-10-27 16:57:50,"It's really not that complicated or that big of a deal.

On Oct 27, 2012, at 11:37 AM, Piotr Dobrogost notifications@github.com wrote:

> These libraries are vendored to make it possible to easily vendor Requests itself.
> 
> @kennethreitz 
> How about changing the way vendorization is done by including dependencies in a zip and adding an option to setup.py to unzip them? This way vendorization could be sort of opt-in.
> 
> —
> Reply to this email directly or view it on GitHub.
",kennethreitz,kennethreitz
830,2012-09-11 03:05:51,"Thanks for the awesome patch! Would you mind resending it with the advise given by @sigmavirus24? Thanks!
",kennethreitz,sigmavirus24
823,2012-08-30 13:14:05,"Thanks @Lukasa for centralizing the discussion. I'm personally a little torn about the `save_as` function for the Response object. It could easily become unwieldy. For example, what if someone also wants to save the response headers? Then they have to rewrite this recipe for themselves and the function serves no benefit. Still others might want as much information from the Response object as possible including the url they used (if they're using this towards logging). I think it is certainly useful, I just think it could introduce some possible unnecessary arguments in the future over how much it should include. Besides, developers can easily write a function for themselves to dump the content that they want.

I would lean on the side of simplicity and leave it out.

As for adding tests for `iter_content()`, I'm surprised it didn't already exist. :+1: for catching that.
",sigmavirus24,Lukasa
823,2012-08-30 13:26:52,"(Thanks @Lukasa for fixing my messy pullrequest)

I see your reasoning, but because of the prefetch=False gotcha I didn't get it right the first time (I needed to look at the source of iter_content() to figure out how to use it). Having a save_as() which Just Works seems to be nice, if only as an example for anyone to make their own. An other option would be to add it to the documentation as 'recipe'.
",alicebob,Lukasa
821,2013-05-13 03:04:55,"@sigmavirus24 I am using the same code as original poster: 



Without proxy:



Headers (via WireShark):



So it's not happening without proxy.
",oliverjanik,sigmavirus24
817,2012-08-28 13:53:33,"@kennethreitz 

> Thanks for the contribution, but I don't really see how modifying the very simple dictionary is difficult at all.

Hmmm, maybe I'm missing some easy way to do this? Or maybe you're not familiar with or recalling the recent changes in how `params` is handled? (see @sigmavirus24's comment also)

`params` **used to** be a dict until very recently. Now it is a list of tuples. It's not rocket science of course to append to a list of tuples -- the issue is that it's tricky for folks to write code that reliably handles `params` when they don't know which version of requests is installed. Not impossible, but harder than it needs to be. Here's what I have in my client code now:



If I had the `add_params` method then this simply becomes:


",msabramo,kennethreitz
817,2012-08-28 13:53:33,"@kennethreitz 

> Thanks for the contribution, but I don't really see how modifying the very simple dictionary is difficult at all.

Hmmm, maybe I'm missing some easy way to do this? Or maybe you're not familiar with or recalling the recent changes in how `params` is handled? (see @sigmavirus24's comment also)

`params` **used to** be a dict until very recently. Now it is a list of tuples. It's not rocket science of course to append to a list of tuples -- the issue is that it's tricky for folks to write code that reliably handles `params` when they don't know which version of requests is installed. Not impossible, but harder than it needs to be. Here's what I have in my client code now:



If I had the `add_params` method then this simply becomes:


",msabramo,sigmavirus24
817,2012-08-28 17:34:22,"@msabramo that was my initial thought as well, yes.
",kennethreitz,msabramo
817,2012-08-28 17:37:38,"@shazow :cake:
",kennethreitz,shazow
817,2012-08-28 20:29:47,"@sigmavirus24 no fault needs to be had, just improvements to a great pull request :)
",kennethreitz,sigmavirus24
815,2012-08-26 16:47:26,"@kennethreitz So fast!!!!!
",ayanamist,kennethreitz
804,2012-08-25 14:34:30,"Fixed by @weak, thanks!
",kennethreitz,weak
804,2012-08-25 14:34:57,"@sigmavirus24 feel free to send your final version when it's ready :)
",kennethreitz,sigmavirus24
800,2012-08-24 15:26:05,"@Lukasa Thank you. Either form is accepted. If the issue can be fixed, i don't care what and how is modified.
",ayanamist,Lukasa
800,2012-08-25 14:37:49,"@ayanamist hmm, this seems like an odd way to fix the problem. I need to get a better understanding of why this is necessary. Any additional info would be great!
",kennethreitz,ayanamist
799,2012-08-20 05:38:27,"@joequery You should be making this pull request against urllib3, not requests :)
",kennethreitz,joequery
799,2012-09-24 16:01:57,"@kennethreitz :O You merged urllib3 pull request code into requests. Naughty.
",shazow,kennethreitz
799,2014-01-07 15:41:44,"I'm a little confused as to the status of what happened, here. My perception is that one change was made to urllib (allowing a SSL version to be passed) and that this PR was reverted, leaving only the @joequery fork of requests, where the only real change was to attempt SSL3 and _then_ SSL23.

There still exists a problem where any error will be caught and another connection reattempted using SSL23, sometimes or always causing the ""unknown protocol"" error. This can simply be fixed by reraising the exception unless the message matches the TLS-version error:



I'd submit an issue, but apparently forks can't have issues.

@joequery I think your fork might still be the only solution. Please confirm. If so, please consider the fix, above.
",dsoprea,joequery
799,2014-01-07 15:57:51,"@dsoprea Please think very carefully before commenting on two-year-old issues. There was definitely a better, less-noisy way to handle providing your input.

Your reading of this issue is wrong. This pull request contained only changes to urllib3, which is a separate module to requests, maintained elsewhere. We do not accept code changes in urllib3 in this repository, as we simply bring in urllib3 wholesale. Any urllib3 changes therefore need to be made at the core repository, shazow/urllib3. This means even if we wanted to, we could not consider this fix, because it's not our fix to make.

Kenneth accidentally merged this fix when he intended to close it, so he also reverted it.

If you'd carefully checked the internet, you'd know that there is an accepted way to choose the SSL version used by Requests. It is documented [in this blog post](https://lukasa.co.uk/2013/01/Choosing_SSL_Version_In_Requests/) (now more than a year old), [this StackOverflow question](http://stackoverflow.com/questions/14102416/python-requests-requests-exceptions-sslerror-errno-8-ssl-c504-eof-occurred), [this Requests issue](https://github.com/kennethreitz/requests/issues/1083), and will be implemented in [this open-source library](https://github.com/sigmavirus24/requests-toolbelt).
",Lukasa,dsoprea
799,2014-01-07 18:47:53,"I did a search for the bug that I was having, and I ended-up in that PR. I
apologize for posting the comment. Joe seemed to be fixing a bug that I
encountered first thing in the morning, and it seemed the most relevant to
a bug that never got fixed. I did not notice the timestamp, and got lost in
all of the various cross-linking. I appreciate the links in your response.

I would caution you against being so abrasive. I'll be careful to respond
to you as you have responded to me, should you ever have a question about
anomalous behavior in one of my projects that you might suddenly find
yourself relying on.

On Tue, Jan 7, 2014 at 10:58 AM, Cory Benfield notifications@github.comwrote:

> @dsoprea https://github.com/dsoprea Please think very carefully before
> commenting on two-year-old issues. There was definitely a better,
> less-noisy way to handle providing your input.
> 
> Your reading of this issue is wrong. This pull request contained only
> changes to urllib3, which is a separate module to requests, maintained
> elsewhere. We do not accept code changes in urllib3 in this repository, as
> we simply bring in urllib3 wholesale. Any urllib3 changes therefore need to
> be made at the core repository, shazow/urllib3. This means even if we
> wanted to, we could not consider this fix, because it's not our fix to make.
> 
> Kenneth accidentally merged this fix when he intended to close it, so he
> also reverted it.
> 
> If you'd carefully checked the internet, you'd know that there is an
> accepted way to choose the SSL version used by Requests. It is documented in
> this blog posthttps://lukasa.co.uk/2013/01/Choosing_SSL_Version_In_Requests/(now more than a year old), this
> StackOverflow questionhttp://stackoverflow.com/questions/14102416/python-requests-requests-exceptions-sslerror-errno-8-ssl-c504-eof-occurred,
> this Requests issue https://github.com/kennethreitz/requests/issues/1083,
> and will be implemented in this open-source libraryhttps://github.com/sigmavirus24/requests-toolbelt
> .
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/pull/799#issuecomment-31749628
> .
",dsoprea,dsoprea
799,2014-01-08 16:51:59,"@dsoprea My fork existed back when documentation concerning choosing SSL versions wasn't as easy to find. I still use my fork from time to time when I deal with scraping sites which exclusively deal with sslv3, because I'm lazy and don't feel like doing that mounting process :P

I'll make the update you provided anyway, so thanks for that. I also didn't think your initial comment was rude or inconsiderate at all, and I don't see how commenting on an issue similar to your own is ""noisy"".
",joequery,dsoprea
799,2014-01-08 17:09:26,"He's a good guy. I was confused and annoyed by the presence of no seemingly
standard, staightforward solution, and he was just annoyed by something
being brought-up that seemed a bit more straightforward to him, and
resurrected an old thread.

I saw a reference by Ken to ""transport adapters"" on another thread, and
then, through more searches than I would've hoped, finally found my way to
the ""transport adapters"" section in the docs. I then just overrode
_init_poolmanager_, and added ssl_version as a parameter.

I think that HTTPAdapter should have this already, but I can't imagine it
wouldn't be there unless there was an exceptional reason.

I'll propose a PR that introduces an SSL example in the ""transport
adapters"" section of the documentation that I referred-to, above.

Dustin

On Wed, Jan 8, 2014 at 11:52 AM, Joseph McCullough <notifications@github.com

> wrote:
> 
> @dsoprea https://github.com/dsoprea My fork existed back before
> documentation concerning choosing SSL versions wasn't as easy to find. I
> still use my fork from time to time when I deal with scraping sites which
> exclusively deal with sslv3, because I'm lazy and don't feel like doing
> that mounting process :P
> 
> I'll make the update you provided anyway, so thanks for that. I also
> didn't think your initial comment was rude or inconsiderate at all, and I
> don't see how commenting on an issue similar to your own is ""noisy"".
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/pull/799#issuecomment-31852884
> .
",dsoprea,dsoprea
799,2014-01-08 17:27:01,"@joequery As @dsoprea just said, we both managed to get our wires crossed and weren't entirely our best selves. These things happen, apologies all round and everyone's friends again. =)

@dsoprea You mention that you're not sure why the SSL version isn't available on the default `HTTPAdapter`. This is a fair question. The short answer is that we don't want to provide options on the `HTTPAdapter` for _every single argument_ urllib3 takes on any of the functions we call. That will lead to a nightmare god-object, and worse, will lead to us duplicating the default values that are already in urllib3. That's a potential source of many many bugs.

With that in mind, we had to pick which attributes we thought were important enough to specify explicitly and which we thought weren't. This was an arbitrary decision, and Kenneth made it as he saw fit. In principle the `ssl_version` should never need to be set: it looks like the bug we're encountering here is related tangentially to buggy versions of OpenSSL (see a very recent discussion at #1847 and #1850, where the tone of the conversation provides an idea of how frustrating the maintainer + contributors find this SSL bugginess). This makes `ssl_version` a prime candidate to simply leave at its default urllib3 value.

This dovetails with the fact that the Transport Adapter interface is _intended_ to be an inheritance-based interface, not a function-parameters one. Again, [I've written briefly about this](https://lukasa.co.uk/2013/04/Requests_Two_APIs/) (though if you read the `SSLAdapter` post you'll find this retreads a lot of ground).

If you strongly disagree with this design decision, I can get Kenneth to weigh in on it, but we're strongly resisting changes to Requests' interfaces.

As for proposing an SSL Transport Adapter example in the documentation, I think we'd be happy to have it. =) I've documented it myself in a number of places that aren't the official documentation, and it's now present in what is likely to be a very popular companion library, but it also makes a good example.
",Lukasa,dsoprea
799,2014-01-08 17:27:01,"@joequery As @dsoprea just said, we both managed to get our wires crossed and weren't entirely our best selves. These things happen, apologies all round and everyone's friends again. =)

@dsoprea You mention that you're not sure why the SSL version isn't available on the default `HTTPAdapter`. This is a fair question. The short answer is that we don't want to provide options on the `HTTPAdapter` for _every single argument_ urllib3 takes on any of the functions we call. That will lead to a nightmare god-object, and worse, will lead to us duplicating the default values that are already in urllib3. That's a potential source of many many bugs.

With that in mind, we had to pick which attributes we thought were important enough to specify explicitly and which we thought weren't. This was an arbitrary decision, and Kenneth made it as he saw fit. In principle the `ssl_version` should never need to be set: it looks like the bug we're encountering here is related tangentially to buggy versions of OpenSSL (see a very recent discussion at #1847 and #1850, where the tone of the conversation provides an idea of how frustrating the maintainer + contributors find this SSL bugginess). This makes `ssl_version` a prime candidate to simply leave at its default urllib3 value.

This dovetails with the fact that the Transport Adapter interface is _intended_ to be an inheritance-based interface, not a function-parameters one. Again, [I've written briefly about this](https://lukasa.co.uk/2013/04/Requests_Two_APIs/) (though if you read the `SSLAdapter` post you'll find this retreads a lot of ground).

If you strongly disagree with this design decision, I can get Kenneth to weigh in on it, but we're strongly resisting changes to Requests' interfaces.

As for proposing an SSL Transport Adapter example in the documentation, I think we'd be happy to have it. =) I've documented it myself in a number of places that aren't the official documentation, and it's now present in what is likely to be a very popular companion library, but it also makes a good example.
",Lukasa,joequery
791,2012-08-18 20:31:57,"@kennethreitz @Lukasa should I start a new branch off of develop and cherry pick my commits on top of it to avoid the merges? Besides, one of the merges would not have been able to be automatic, so it would have required a merge commit anyway.
",sigmavirus24,Lukasa
791,2012-08-18 22:34:40,"@kennethreitz done.
",sigmavirus24,kennethreitz
790,2012-08-20 07:17:25,"@idan I understand the complete workflow, but the design purpose make all things complicated.

You can see how kennethreitz/requests use requests, and i agree what they do.

If i pass the auth to requests, i think requests should do all auth stuffs, but not like current situation, i must manually add oauth signature. If requests needs manual operation, why i pass the auth arguments to it?

Passing dict object as post data parameters is a normal demand, but not doing something special!!! Every developer are doing like this after read your documentation. Passing the auth and thinking auth function will do things automatically is also a normal thought. If i want to do something special, i will do it before pass the whole thing into requests.

The content-type is set by your code not mine, and the conditional which ignore the post data is also part of your logical but not mine. So what a special thing you think may happen is made by your code but not mine.

> but only if the content-type header is set to ""application/x-www-form-urlencoded"", otherwise the request body is ignored for the purpose of signing.

Why you ignore to sign? What's the reason? So i must sign the request by myself although i told the requests ""plz do the auth work""? Does your thought make things simpler or more complicated?
",ayanamist,idan
790,2012-08-20 07:51:18,"In fact, https://github.com/kennethreitz/requests/blob/develop/requests/models.py#L514 here set content-type to 'application/x-www-form-urlencoded' and if it's not set, it will still be set on https://github.com/kennethreitz/requests/blob/develop/requests/auth.py#L97
So you are doing duplicate work and make the whole thing really complicated.

And if someone set incorrect content-type(anything other than 'multipart/form-encoded') and post a file, you just let the incorrect thing pass by?

So what's your root design purpose @idan ? If you think developers will do things special, you should remove the code which add content-type automatically, or remove the content-type detect conditions in order to make signing work.
",ayanamist,idan
790,2012-08-20 09:11:43,"Relax, @ayanamist, @idan is agreeing with you. He said that the parameters should be taken as input to the signing function only if the `Content-Type` header is `application/x-www-form-urlencoded`. If you pass in a dict, such a header is set, so we should be signing it, but we aren't. **This is a bug.**

As for ""why is the request body ignored for signing in other cases"", it's because the specification says to ignore it. This isn't Requests' decision, it's just what the specification says to do.
",Lukasa,idan
790,2012-08-20 09:11:43,"Relax, @ayanamist, @idan is agreeing with you. He said that the parameters should be taken as input to the signing function only if the `Content-Type` header is `application/x-www-form-urlencoded`. If you pass in a dict, such a header is set, so we should be signing it, but we aren't. **This is a bug.**

As for ""why is the request body ignored for signing in other cases"", it's because the specification says to ignore it. This isn't Requests' decision, it's just what the specification says to do.
",Lukasa,ayanamist
790,2012-08-25 14:47:48,"@kennethreitz Thank you very much :-)
",ayanamist,kennethreitz
785,2012-08-25 14:51:10,"@sigmavirus24 :+1:
",kennethreitz,sigmavirus24
785,2012-09-06 14:27:23,"Once @kennethreitz has time to review #833, I'll start working on this. I have a feeling opening a branch for this would cause a merge conflict if I were to have two Pull Requests that are ignorant of each other for the same file. Could be wrong though. Also, I'm in no rush since I'm fairly busy and I know @kennethreitz is more busy than I am with conferences and whatnot. Just wanted to keep @flub updated.
",sigmavirus24,kennethreitz
783,2012-08-19 00:45:05,"@doda @Lukasa I agree :)
",kennethreitz,Lukasa
782,2012-08-25 15:00:12,"@Lukasa want to send a pull request? :)
",kennethreitz,Lukasa
781,2012-08-25 14:36:46,"@slingamn if you do that, I'll accept it :)
",kennethreitz,slingamn
776,2012-08-13 15:34:25,"@Lukasa It appears that `hostname` is already available on `_p`, from the previous `urlparse()`.

So `_p.hostname.endswith` should be the new callable to pass to `map`, in my opinion.
",Pewpewarrows,Lukasa
773,2012-08-11 12:33:20,"I think neither I nor @slingamn are saying this pull request is anything other than brilliant. We just don't understand how the tests were passing to begin with. I always test locally before I submit a pull request, including in 2.6, and never had the tests fail.

For me, this is just a curiosity, nothing more. =)
",Lukasa,slingamn
773,2012-08-14 03:54:05,"@radomir: @Lukasa is right, this change was great. I was just worried that the testing infrastructure for Requests was broken somehow. But `test_requests_ext.py` is intentionally excluded, so that explains it. Thanks for your contribution!
",slingamn,Lukasa
771,2012-08-11 14:21:54,"Using @slingamn's Gist, I can also reproduce the bug.

Using Python with the following build info:



And using the `develop` branch of Requests @ 27b55a7, I get the output:


",Lukasa,slingamn
771,2012-09-10 12:10:34,"@slingamn my first instinct about the docs telling you to use shutdown first is that it changes the behavior of the socket just like shutdown would do with [C sockets](http://beej.us/guide/bgnet/output/html/singlepage/bgnet.html#closedown). I would agree with you that the shutdown call will not likely help, but it is worth trying.
",sigmavirus24,slingamn
771,2012-09-20 08:35:08,"@shazow interesting.

It is sufficient for this ticket for `_fp.fp` to be set to None, as it should be after the call to `r.content`. I think we'd only need to have urllib3 set `raw._fp` to None as well if we were trying to release the socket without reading all the content.

However, I suspect this is not a good use case; if you try to reuse a connection while it still has unread data from the last request, I think it may not work correctly. (Possibly depending on implementation details of the server.) Thoughts?
",slingamn,shazow
771,2012-10-20 01:14:23,"@slingamn thanks for reminding me about 223 ;)
",sigmavirus24,slingamn
761,2012-11-28 13:33:55,"Ahh understood, thanks for that @sigmavirus24.
",llama,sigmavirus24
761,2013-09-24 01:09:44,"I'm using requests version 1.2.0 and seeing the same problem: `requests` crashes with `SSLError` on https://selectedpapers.net, whereas `openssl s_client -connect selectedpapers.net:443` verifies the certificate successfully (and any browser not IE on XP accepts the certificate;  here's another example of the same problem: http://hearsum.ca/blog/python-and-ssl-certificate-verification/).  I searched the urllib3 issue tracker but couldn't find a clear match to this bug. 

@geier @sigmavirus24 can you give a link to the urllib3 issue that tracks this?  I'd like to see whether urllib3 has actually fixed this, and if so why requests 1.2.0 still doesn't work for this https request.
",cjlee112,sigmavirus24
761,2013-09-24 04:51:08,"@sigmavirus24 Thanks for looking into this!!  I wasn't looking for a workaround, but rather trying to find the urllib3 issue number that tracks the resolution of this bug.   You guys said ""it looks like this is an urllib3 problem, will take it there"" but gave no details, and hunting around there I couldn't find a clear match.  I think those details should be entered here, for the record, since it strongly affects `requests` users.  For example, if `urllib3` (for whatever reason) ends up not truly fixing this bug, it will (continue to) be a big issue for people trying to use `requests`.  So the `requests` issue tracker should have an explicit record of what `urllib3` issue number(s) address this.
",cjlee112,sigmavirus24
757,2012-08-03 22:56:03,"@kennethreitz , the problem is not how browsers work, there are some APIs which use OAuth and need specifically to receive `%20` as the scape character for `space` according to the [RFC 5849](http://tools.ietf.org/html/rfc5849#section-3.6)

This should be an available option, as if it is done other way, it would result in a 401 response
",dlitvakb,kennethreitz
757,2013-01-08 10:28:38,"@sigmavirus24 



Out of curiosity; how can user know what is safe to change after the request had been prepared? It looks like playing with fire :)
",piotr-dobrogost,sigmavirus24
749,2013-01-21 20:17:44,"@shazow: Presumably you have no interest in making urllib3 depend on PyOpenSSL?
",Lukasa,shazow
749,2013-02-09 23:13:36,"It looks like @t-8ch is knocking it out of the park with the work on urllib3. I'll let him tell us when this can be closed.
",sigmavirus24,t-8ch
749,2013-06-10 13:48:01,"@pythonmobile If that's the case, and you have a good reproducible test case, please open an issue highlighting it on urllib3. Requests has no SNI-specific code (as far as I know), so a fix there will fix us as well.

Unless urllib3 is fixed already, which seems to happen a lot these days. Shazow _et. al._ are pretty awesome. @t-8ch, have you guys seen/fixed this already?
",Lukasa,t-8ch
749,2013-06-11 17:25:12,"While more tests for requests certainly wouldn't hurt, as @Lukasa said, you'll need to write a test for urllib3 if you'd like to see this actually fixed. :)

We already have many tests in a similar vein, ping me if you need any help.
",shazow,Lukasa
749,2013-08-27 22:10:55,"@t-8ch @pythonmobile  : Can you guys look at this thread as well: https://github.com/kennethreitz/requests/issues/1522#issuecomment-22443282

It would be nice to get these requests in a unit test inside urllib3 or requests. 
",pikumar,t-8ch
749,2013-08-28 07:49:21,"The problem with getting these as a unit test is that they only appear in very specific configurations. For instance, I have never been able to reproduce either the second half of this issue or #1522 on any of my systems, whether it's Windows, OS X or Ubuntu. That's why I asked @pythonmobile for a reproducible unit test. If we can find one, we can develop against it, otherwise we're stuck hoping that @t-8ch continues to be a genius and magically fix all our problems up at `urllib3`.

NB: @t-8ch, I don't think I thank you enough for your work, both here and at `urllib3`. You're awesome. =D :cake: :pineapple: :banana: :cookie: :beers: 
",Lukasa,t-8ch
749,2013-08-29 15:50:48,"@t-8ch Do it. :+1: 
",Lukasa,t-8ch
747,2012-08-04 00:50:32,"I started working on this and wrote a regression test that exposes the problem, but I wanted to get your opinion on how you want this fixed @kennethreitz . My first question is do you want the content-type header to be text/plain or do you just want it unset? The second question is in how to fix it. The issue is that the compat.py rebinds str to be the unicode class so in models.py line 496 where there is a check for `instanceof(self.data, str)` where str is actually unicode and not the __builtin__.str. Should I just add in another isinstance check for the __builtin__ str type or do something else? Let me know how you would like this fixed and I can put in a pull request.
",volker48,kennethreitz
746,2012-07-27 16:10:05,"@Lukasa if you edit the verbosity of the tests in a local branch of yours and set up Travis with it, you should be able to get more information on your own from Travis.
",sigmavirus24,Lukasa
741,2013-01-08 20:42:42,"@karlcow

_And my server implementation is wrong as it should send a list of csv for the header Link:._

I think you are wrong here. Firstly, there seems to be no change in rules regarding multiple header fields with the same field name between RFC 2616 and the draft of httpbis you cited. The wording has changed but the sense is the same. Secondly, [RFC 5988 section 5](http://tools.ietf.org/html/rfc5988#section-5) defines the value of `Link` header as a list of comma separated values (`Link           = ""Link"" "":"" #link-value`) which means, according to the rules you cited, that there CAN be multiple `Link` headers and that they CAN be merged into one, comma separated list of values.
",piotr-dobrogost,karlcow
741,2013-01-09 01:57:24,"@sigmavirus24 because it is still not sure sure if it has be reopened or not.  Trying to understand the issue first. In the other hand, if there is a better place for it. I will be happy to discuss it there. 
",karlcow,sigmavirus24
740,2012-07-27 05:28:31,"Closing this, @r1chardj0n3s thanks for the patch! Can you send again with only 6f0a224? 
",kennethreitz,r1chardj0n3s
738,2012-10-02 21:06:21,"@kennethreitz Could you point us to some details on this known problem?
",piotr-dobrogost,kennethreitz
737,2012-07-27 05:43:53,"@Lukasa :+1:
",kennethreitz,Lukasa
731,2012-07-18 14:44:35,"Thanks guys!

And thanks for the rundown @Lukasa :)
",kennethreitz,Lukasa
724,2012-07-27 05:25:54,"thanks for the fix for this issue @muhtasib and @kennethreitz  . 
",amenasse,kennethreitz
724,2012-07-27 05:25:54,"thanks for the fix for this issue @muhtasib and @kennethreitz  . 
",amenasse,muhtasib
722,2012-07-12 23:36:28,"@gulopine Ah, thanks! I couldn't find my comment in 684! Haha, I was looking for it. And what does Linkedin use? Query or body? 
",michaelhelmick,gulopine
719,2012-07-12 18:55:10,"@kennethreitz Then what does the max_retries do in requests.defaults?

http://docs.python-requests.org/en/latest/api/#configurations
",JakeAustwick,kennethreitz
713,2012-07-14 02:25:32,"_nod_ We can do this, but it'll be easiest with a rewritten http client library. I've got one that's mostly API compatible with httplib (http://code.google.com/p/httpplus/) that I'm writing as part of my 20% work on Mercurial that we could try.

Note that the flow proposed in the initial bug isn't strictly RFC compliant - you can't wait indefinitely for the response to the Expect: header. You have to wait an unspecified delay and then continue optimistically, as servers SHOULD (not MUST) understand Expect: headers. Reverse proxies are allowed to strip Expect headers entirely, and many do so.

@kennethreitz I don't have any particular time to dedicate to this, but I'm happy to chat more about what we can do to make this better. Are you going to be at OSCON?
",durin42,kennethreitz
713,2013-01-23 14:02:11,"@kennethreitz do you have support for streaming uploads? I've not followed development of requests closely.
",durin42,kennethreitz
713,2016-10-08 15:38:40,"@sigmavirus24 what do you mean ""never exposes""? We are sending data directly to socket, and then reading response [0]. We can read and write anything we want. This may be very simple patch, just few more lines and tests.

[0] https://github.com/kennethreitz/requests/blob/v2.11.1/requests/adapters.py#L434-L465
",redixin,sigmavirus24
711,2012-07-12 20:55:51,"@steveklabnik I suppose Link headers aren't going anywhere just because of HAL. 

One question you might be more familiar with .. is the rel attribute a namespace of sorts for links? So suppose you had two ;rel=profile headers should one of them be ignored? I'm asking because the implementation would be nice as an attribute accessor like so:



HTML pages can link to multiple stylesheets, as is common.
",jokull,steveklabnik
711,2012-07-13 02:53:51,"@kennethreitz Good point about headers vs body. Are you looking for contributors or suggested implementations? 

One more reference: the [official Link Relations](http://www.iana.org/assignments/link-relations/link-relations.xml).
",jokull,kennethreitz
700,2012-11-27 18:46:44,"@kennethreitz : Are you abandoning plans to add caching support, or is this work continuing elsewhere?
",inactivist,kennethreitz
700,2012-11-27 20:21:27,"@kennethreitz : Thanks.  Actually, I was interested in helping with the effort, as I'd like to take advantage of caching.  What can I do to assist?
",inactivist,kennethreitz
700,2014-02-24 20:40:19,"@sigmavirus24

Ahh. Sorry about that.

I went looking but only managed to find a cache implementation which was purely timer based (no ETags, no `If-Modified-Since`) so I assumed they didn't exist and this was the most promising avenue.

It's quite likely I just screwed up my search keywords somehow but it'd still be a good idea to to add some kind of ""Related Projects"" list to the wiki.

@Lukasa Thanks! That couldn't be more perfect.
",ssokolow,Lukasa
700,2014-02-24 20:40:19,"@sigmavirus24

Ahh. Sorry about that.

I went looking but only managed to find a cache implementation which was purely timer based (no ETags, no `If-Modified-Since`) so I assumed they didn't exist and this was the most promising avenue.

It's quite likely I just screwed up my search keywords somehow but it'd still be a good idea to to add some kind of ""Related Projects"" list to the wiki.

@Lukasa Thanks! That couldn't be more perfect.
",ssokolow,sigmavirus24
700,2014-02-24 21:20:15,"Yes, thanks, @Lukasa, I searched recently and missed Cache-Control as well.  If it's not there now, why not mention it in the Requests docs?  (Hmm.... pull request, anyone?)
",inactivist,Lukasa
700,2014-02-24 22:29:11,"What about on the wiki then?

It does match the wiki's ""either hasn't made into the documentation yet, or doesn't belong there"" criterion and given the following points, it's pretty intuitive that wiki contents are community-supplied and neither endorsed nor supported by the project:
- The project doesn't use the wiki for its website or documentation.
- The project's documentation is much more polished than the wiki
- It's pretty well understood that, when a project has both Sphinx documentation **and** a wiki, the wiki is there to take advantage of the ""anyone can edit"" aspect of wiki-ness.
- The Wiki's ""Home"" page specifically encourages people to add things.

(I'd add it myself, but I don't know all of the ""several options"" that @sigmavirus24 mentioned existing. I'd rather have all known caching extensions listed, both to let people make their own decisions and to further drive home the ""this is not advice"" point.)
",ssokolow,sigmavirus24
700,2014-02-24 22:34:55,"Good points, @Lukasa -- totally understandable.  I admit it's been a little while (a month?) since I last researched Requests caching; I suspect I developed a blindness towards your (deprecated) project so I probably didn't click the link in my last search and thus didn't see your mention.  My bad.

I do like the idea of listing all known 'compatible' caching solutions on the wiki, along with the standard disclaimers.
",inactivist,Lukasa
700,2014-04-15 23:44:52,"@anentropic also https://github.com/ionrock/cachecontrol (same as the bitbucket repo you linked)
",sigmavirus24,anentropic
700,2014-04-16 08:24:03,"@sigmavirus24 ah yes, that looks like the best one to me so far
",anentropic,sigmavirus24
696,2013-11-18 08:36:51,"@Lukasa Thanks :) Sorry for posting here, will take care from next time.
",bhoomit,Lukasa
691,2012-07-30 12:10:19,"@kennethreitz I got bitten by this today. To me the behavior of `response.json` is very unintuitive. If I use `response.json` that's because I expect the response to actually be json-formatted. And I expect this call to fail in case it's not proper json. Say an API I use is mis-behaving and returns weird errors, if an exception occurs here I know something is wrong. Otherwise I just get a `None` object, things start to fail elsewhere in my system and I need to figure out why an object is `None` instead of a proper api response and where this comes from.

This is also inconsistent with python's `json.loads`.

At the very least this property should be renamed to `try_json` or `json_or_none` / `json_maybe` ;). But the current implementation is weak. When you say:

> there simply isn't any JSON, so the value is None

If I translate this to reading a file, this could also mean `open(filename, 'r')` returning `None` if the file doesn't exist. What you're currently saying is basically ""if you expect json data. don't use `response.json`"", which makes me wonder why there is such an API in the first place.

Sorry to be that guy, I'm rather strongly opposed to silent errors because they tend to catch you by surprise at the wrong moment. Hope you'll reconsider this change.

Thanks!
",brutasse,kennethreitz
691,2012-07-30 13:01:36,"@brutasse If you keep the request object then you can test if `r.json` is None. If so, then check the response object for the content of the response (e.g., `r.content`) and that will tell you what was returned by the API. I'm handling things that way myself.
",sigmavirus24,brutasse
691,2012-07-30 13:17:51,"@sigmavirus24 I know I can do it this way but it doesn't address the fact that the API is misleading: I expect some decoding happen and if the content is malformed then this decoding should fail loudly. I'd rather use json.loads on the response content directly but again, why is there a helper for json decoding if you have to hack around it to even notice there was a decoding error…

To quote pep20, ""Errors should never pass silently unless explicitly silenced."" I know this silencing is documented but I wouldn't qualify it as explicit.
",brutasse,sigmavirus24
691,2012-08-05 20:12:06,"I'm agree with @brutasse and @merwok 
",GMLudo,brutasse
688,2012-06-21 19:21:24,"Works fine for me. Check your `proxy` variable is actually the dictionary you want to pass, e.g.:



Make sure that you have the full schema present.

EDIT: Just did a copy-paste of @kennethreitz's code, and noticed that it just made standard requests. The issue seems to be that there is a colon in the dict key that shouldn't be there. Try removing that colon.
",Lukasa,kennethreitz
684,2012-06-20 20:45:05,"Yes! Thank you.
I was actually just going to make an issue. 

I was updating a lib and was wondering why my signatures were invalid when passing the `params` kwarg and were valid when I just did `http://api.example.com/method/?myquerystring=parameters` and thought of this issue to be the only explanation 

@gulopine Think you can get @travisbot to successfully pass?

 :+1:
",michaelhelmick,gulopine
684,2012-06-29 16:19:45,"@kennethreitz @gulopine I feel like this didn't fix the issue. I went to update `Twython` to use



instead of 



and the request failed, response was None :(
",michaelhelmick,kennethreitz
684,2012-06-29 16:19:45,"@kennethreitz @gulopine I feel like this didn't fix the issue. I went to update `Twython` to use



instead of 



and the request failed, response was None :(
",michaelhelmick,gulopine
677,2012-06-21 01:07:39,"The comment I deleted stands: I ran the tests on my machine with python 3000 and nose and got the same errors on my upstream branch as I get on my branch. I'll dig in to see if I can figure out where these tests are breaking.

**Update**: I tried running the tests individually, e.g.,



Only the last one passes with python3 on my machine on my branch tracking develop

**Update #2** After getting the tests working (thanks again @kennethreitz), I have a feeling the failure is related to nose and not requests or my changes. From what I can tell, the exception is occurring because of my having OpenDNS installed and running. This causes an error to be raised in the test for invalid content which causes the exception in nosetests. @jpellerin, @kumar303, any clues about the message above?
",sigmavirus24,kennethreitz
677,2012-06-21 18:40:25,"I disagree, the tests are legitimately failing. The four tests that are erroring out provide `data` as a string. In Python 3, strings implement the `__iter__` method. The same problem occurs with byte arrays in Python 3, which also implement `__iter__`. The tests will pass if you adjust your conditional to:



That said, the conditional becomes less nice in that form. I'm ok with having the `else` block all in one place, but I think this is about what PEP 20 suggests. I don't know which form fits PEP 20 better, and @kennethreitz has a better understanding of it than I do.
",Lukasa,kennethreitz
677,2012-06-21 18:53:40,"@Lukasa I was trying to find the bug using virtualenvs as well and couldn't get any better output than I described.

I'll take your word for it even though I don't understand why `not isinstance(data, str) and not isinstance(data, bytes)` would be necessary - neither has an `__iter__` attribute.

And yeah, if this is in fact the case, I'm all in favor of keeping it in the old style because that conditional would be horribly ugly. I just vastly dislike having so many return statements in one function.

Anyway, the other commit 0624b04 should still be considered since it's only a documentation change.

Sorry for the confusion
",sigmavirus24,Lukasa
677,2012-06-21 18:57:11,"@sigmavirus24 Try opening a Python3 interpreter and typing the following:



It should return True. Strings and Bytes both have `__iter__` methods in Python 3, which is why your tests fail in Python 3. No need to apologise, I didn't know Python 3 strings had `__iter__` methods either, until I did my investigation.
",Lukasa,sigmavirus24
677,2012-06-21 19:01:41,"Huh. When I was testing the hooks section the API wrapper I'm working on 
this morning, I thought I saw that this was passing on python 3 and 
failing on python 2. 

_shrug_. I'll close this and re-work my fork to only have the 
documentation change (and maybe one more that I promised @kennethreitz).

Thanks again for your help @Lukasa
",sigmavirus24,kennethreitz
677,2012-06-21 19:01:41,"Huh. When I was testing the hooks section the API wrapper I'm working on 
this morning, I thought I saw that this was passing on python 3 and 
failing on python 2. 

_shrug_. I'll close this and re-work my fork to only have the 
documentation change (and maybe one more that I promised @kennethreitz).

Thanks again for your help @Lukasa
",sigmavirus24,Lukasa
671,2012-11-23 21:03:59,"@sigmavirus24
Yes, I believe it fixes #888 too.
",piotr-dobrogost,sigmavirus24
664,2012-06-19 13:20:52,"Thanks @shazow @kennethreitz!
",lsemel,shazow
658,2012-07-03 06:16:27,"@kennethreitz ?
",schlamar,kennethreitz
655,2012-06-25 14:18:16,"@shazow I think the problem is that I am doing HTTPS over an HTTP proxy. It seems that this is fixed in #478. 

@kennethreitz I think this pull request hadn't made it into requests, right?
",schlamar,kennethreitz
655,2012-06-25 14:18:16,"@shazow I think the problem is that I am doing HTTPS over an HTTP proxy. It seems that this is fixed in #478. 

@kennethreitz I think this pull request hadn't made it into requests, right?
",schlamar,shazow
655,2012-06-25 16:42:58,"@kennethreitz Not sure what you merged, but if it was anything to do with this bug, it probably isn't a good idea. :)

urllib3 has some related pull requests and an in-progress branch but none of them are ready (and in fact has known bugs).
",shazow,kennethreitz
647,2012-07-19 15:17:24,"Can't wait for this to be pulled, so that I can pull. :)

Thank you, @ncoghlan for the write-up.
",jpmens,ncoghlan
633,2012-05-31 15:39:24,"Hey Kenneth, I would recommend against wrapping the whole method within a try/except ValueError block. I think my pull request #641 should be sufficient in addressing @tzuryby's issue. If there is another edge case that would raise an exception here, it would be valuable for us to understand why it did so.  Catching the exception and continuing would prevent us or the developer from even knowing there was an issue with the URI.
",mwielgoszewski,tzuryby
632,2012-05-29 23:41:37,"@amalakar :cake: :)
",kennethreitz,amalakar
632,2012-05-30 00:00:08,"Yay! Thanks @kennethreitz for the encouraging words. We can close Issue #505 now. I need to find the next bug to fix now. Feel free to assign/recommend something :)
",amalakar,kennethreitz
630,2012-05-23 16:05:08,"@tzuryby so make a pullrequest !
",toutouastro,tzuryby
627,2012-11-26 22:24:32,":heart: @Lukasa 
",sigmavirus24,Lukasa
627,2012-11-26 23:17:32,"@Lukasa 
Nice find. Generally there's asymmetry between the first request and all subsequent ones. Notice history is not passed, either.
",piotr-dobrogost,Lukasa
627,2012-11-26 23:45:14,"@sigmavirus24, @Lukasa , @piotr-dobrogost, you are awesome.
Now I see, it was worth to write the test code.

I found that the file, I was using for testing, got removed from my AWS bucket (I forgot about testing purpose of it).

The file is back, so it shall return some content as soon as this solution comes into effect.

I will have to study your hooks and this stuff, which I am not so familiar with yet, but probably not today, as we have here deep night 00:44 a.m CET.

Thanks for your effort.
",vlcinsky,Lukasa
627,2012-11-26 23:45:14,"@sigmavirus24, @Lukasa , @piotr-dobrogost, you are awesome.
Now I see, it was worth to write the test code.

I found that the file, I was using for testing, got removed from my AWS bucket (I forgot about testing purpose of it).

The file is back, so it shall return some content as soon as this solution comes into effect.

I will have to study your hooks and this stuff, which I am not so familiar with yet, but probably not today, as we have here deep night 00:44 a.m CET.

Thanks for your effort.
",vlcinsky,sigmavirus24
627,2012-11-27 09:56:57,"@sigmavirus24: Actually you don't always have to return the `Request`, as I didn't do it locally. =) The docs aren't super clear on this, but my reading of them is that if you don't return anything from the hook then the variable passed into the hook is used. Because Python is primarily pass-by-reference, when I pop the item from the dict it affects the version of the dict in the original `Request`. Admittedly it's not super explicit and so returning the `Request` is better form, but it does work.

@vlcinsky: Please note that there is probably a bug in Requests that will prevent that hook from functioning as written. When we resolve that, I will post some sample code that functions correctly. =)
",Lukasa,sigmavirus24
627,2012-11-27 14:40:50,"@piotr-dobrogost :-)
My question regarding filing a bug has two sides:

#### Shall it be filed at all?

In case, the problem is almost fixed and about to be committed and adopted into code, then fileing a new issue is not worth to do.

#### Who is the best person to write such a bug report

I do my best to describe things, I am at least familiar with or where I can provide good quality description.

But with hook being ignored - I did not even try to use one so far and I would be talking about completely new topic to me.

I agree with @sigmavirus24 we could keep this issue as discussed bug report **hooks are not passed to a new request on redirect**

I will edit my original description to point to this aspect, if anyone is interested in detailing, do it.

@all this is my first experience with Github issue cooperation incorporating more then two persons - I love it. So fast, so effective. It is like chatting while being quite productive.
",vlcinsky,sigmavirus24
627,2012-11-27 16:27:41,"Also, @Lukasa TIL
",sigmavirus24,Lukasa
627,2012-11-27 20:14:25,"Maybe I'm missing something here but do we know how major browsers behave in various scenarios described by @vlcinsky in this thread? I think we don't and as long as we don't find out and make Requests follow their behavior the issue of sending authorization data at right moment still exists.

@sigmavirus24 

> history can't be passed to even the first request, but it isn't exactly important.

It should be passed to the first request (being empty at this moment of course) and all subsequent requests. Every request should have knowledge about history as this could be needed to make decisions in hooks for example.
I did not imply it's important in this particular issue but it's logical to note that here, as this is yet another missing piece of data not propagated to subsequent requests similarly to hooks.
If you agree I'd like to open issue for this.
",piotr-dobrogost,sigmavirus24
627,2012-11-28 09:28:06,"@Lukasa I see, you prefer issues being closed :-).

But: you agree, the Auth handler is doing things wrong - and you prefer to keep the issue closed.

Isn't this sort of issue maintainance anti-pattern?

Is anyone supposed to review closed issues after 1.0.0 is completed? Are we hoping, someone will just remember it?

To me, milestone ""later"" being assigned to this issue seems natural. And as 1.0.0 is on the table, it is quite likely, it would be closed even before 1.0.0 is completed.

You and others put some effort on analysing and resolving current problem (so there is some temporary solution in devel branch now), I put some effort on providing bug report and proposal for handling authorization. I think, having this issue open and living under ""later"" milestone would ensure, this effort will not be forgotten and is more likely to contribute to higher usability and quality of Requests in future.

Other option would be to extract remaining issues from this bug (handling Authorization header with redirect and History not being kept over border of redirect), assign them to ""later"" milestone and close this bug, as there is temporary solution available.
",vlcinsky,Lukasa
624,2013-04-18 02:00:10,"@kennethreitz any chance for a Portuguese repository? :)
",duailibe,kennethreitz
624,2013-04-18 02:00:59,"@duailibe are you volunteering? :)
",kennethreitz,duailibe
624,2013-04-18 02:01:56,"@kennethreitz yes! :)
",duailibe,kennethreitz
624,2013-04-18 02:06:37,"@duailibe thanks so much! Just push here :) https://github.com/requests/requests-docs-pt
",kennethreitz,duailibe
624,2013-04-18 02:10:51,"@kennethreitz awesome! Thanks! :cake: 
",duailibe,kennethreitz
624,2013-04-18 17:43:36,"Hi @omederos. I think regular git-flow / pull requests would be the way to go.
I think I pushed the one document that was left, and I honestly don't know
what's next. Let's see if we can get some pointers from @kennethreitz  for the
other translators.

Thanks,

Sergio
",serpulga,kennethreitz
624,2013-04-19 19:13:04,"@Kwpolska all set! Let me know when you have an update so I can make it official :)

https://github.com/requests/requests-docs-pl

Thanks so much :)
",kennethreitz,Kwpolska
624,2014-01-24 15:23:05,"Can I ask what does it need to put the spanish documentation online, @kennethreitz ? 
(Besides some update, it has 9 months old, as I see here: https://github.com/requests/requests-docs-es)
",esparta,kennethreitz
624,2015-01-28 00:12:15,"@deusExCore start a fork and start translating the documentation in place. When we can grab his attention, we'll have @kennethreitz add you to the requests organization and help you move your fork to github.com/requests/requests-docs-tr and then he, or I will have to set up DNS to resolve that for you
",sigmavirus24,kennethreitz
624,2015-05-28 13:15:56,"Hi guys, here you have a candidate for translating the docs to Italian!
@kennethreitz @sigmavirus24 if it's a good thing, I can start working on that on a fork
",csparpa,kennethreitz
624,2015-05-28 13:15:56,"Hi guys, here you have a candidate for translating the docs to Italian!
@kennethreitz @sigmavirus24 if it's a good thing, I can start working on that on a fork
",csparpa,sigmavirus24
624,2015-05-30 19:10:23,"@csparpa go for it!
",kennethreitz,csparpa
624,2015-06-01 18:25:27,"@kennethreitz would you setup a specific repo under https://github.com/requests for the IT translation? I'll wait for that if you're going to create it
",csparpa,kennethreitz
624,2015-06-01 19:09:32,"@csparpa please start translating and as soon as it's mostly done is when we usually import your fork into the @requests org
",sigmavirus24,csparpa
624,2015-06-01 19:43:39,"Got it, thanks ;)

Claudio Sparpaglione
http://csparpa.github.io
On 1 Jun 2015 9:10 pm, ""Ian Cordasco"" notifications@github.com wrote:

> @csparpa https://github.com/csparpa please start translating and as
> soon as it's mostly done is when we usually import your fork into the
> @requests https://github.com/requests org
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/624#issuecomment-107673876
> .
",csparpa,csparpa
624,2015-06-14 12:33:45,"Hi @sigmavirus24 I'm done with the Italian translation - this is on a dedicated branch on my fork. What's the next step? Thanks
",csparpa,sigmavirus24
624,2015-06-14 16:43:13,"So now's the time when @Lukasa or @kennethreitz should add you (@csparpa) to the translators team. They also need to set up a repository for the translation and import it from your fork. I'll set up the stuff so that it appears on python-requests.org, once they've taken care of that. (I'm not an owner/admin on the @requests org so I can't do that stuff for you.)
",sigmavirus24,kennethreitz
624,2015-06-14 16:43:13,"So now's the time when @Lukasa or @kennethreitz should add you (@csparpa) to the translators team. They also need to set up a repository for the translation and import it from your fork. I'll set up the stuff so that it appears on python-requests.org, once they've taken care of that. (I'm not an owner/admin on the @requests org so I can't do that stuff for you.)
",sigmavirus24,csparpa
624,2015-06-14 16:43:55,"@sigmavirus24 That's a terrible oversight, I'll add you as an admin as well.
",Lukasa,sigmavirus24
624,2015-06-14 16:50:39,"@csparpa You are now the proud father of [one repository of Italian-translated docs](https://github.com/requests/requests-docs-it)!
",Lukasa,csparpa
624,2015-06-15 10:31:59,"@Lukasa I'll keep on working on the translation (there are lots of typos and possible rephrasings).
Can you add the repo https://github.com/requests/requests-docs-it to the Translation Team repositories?Thanks
",csparpa,Lukasa
624,2015-06-15 10:34:51,"@csparpa Sorry, done =)
",Lukasa,csparpa
624,2015-08-24 17:01:21,"That's a bit more @sigmavirus24's area of expertise. Ping ping. =)
",Lukasa,sigmavirus24
624,2015-08-24 18:38:15,"@carlosvargas sorry about that. I didn't realize we were missing that. I just set up the CNAME for that so it should hopefully be fixed shortly
",sigmavirus24,carlosvargas
624,2015-08-24 18:45:59,"@sigmavirus24 can I take the chance to notify that on the main Requests
page (http://www.python-requests.org/en/latest/) there is no link to the
Italian translation of the documentation (http://it.python-requests.org/) ?
;-)

Thank you

Cla
On 24 Aug 2015 8:38 pm, ""Ian Cordasco"" notifications@github.com wrote:

> @carlosvargas https://github.com/carlosvargas sorry about that. I
> didn't realize we were missing that. I just set up the CNAME for that so it
> should hopefully be fixed shortly
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/624#issuecomment-134332797
> .
",csparpa,carlosvargas
624,2015-08-24 18:45:59,"@sigmavirus24 can I take the chance to notify that on the main Requests
page (http://www.python-requests.org/en/latest/) there is no link to the
Italian translation of the documentation (http://it.python-requests.org/) ?
;-)

Thank you

Cla
On 24 Aug 2015 8:38 pm, ""Ian Cordasco"" notifications@github.com wrote:

> @carlosvargas https://github.com/carlosvargas sorry about that. I
> didn't realize we were missing that. I just set up the CNAME for that so it
> should hopefully be fixed shortly
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/624#issuecomment-134332797
> .
",csparpa,sigmavirus24
624,2015-08-25 00:57:56,"@csparpa I think that's merely a matter of sending a PR here to add it. ;)
",sigmavirus24,csparpa
624,2015-08-25 07:20:13,"Of course ;) I just wanted to notify the issue so it gets tracked somewhere
but I had better track it on a github issue instead of here... Sorry guys,
I'll do it! Ciao

Cla
On 25 Aug 2015 2:58 am, ""Ian Cordasco"" notifications@github.com wrote:

> @csparpa https://github.com/csparpa I think that's merely a matter of
> sending a PR here to add it. ;)
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/624#issuecomment-134431167
> .
",csparpa,csparpa
624,2016-03-02 14:05:04,"@kennethreitz @Lukasa @sigmavirus24 
I'm a native speaker of Chinese (Simplified) and Chinese (Traditional) .
Current translation is kinda outdated. Could you please add me to the organization, so that I can work on it?
",caizixian,kennethreitz
624,2016-03-02 14:05:04,"@kennethreitz @Lukasa @sigmavirus24 
I'm a native speaker of Chinese (Simplified) and Chinese (Traditional) .
Current translation is kinda outdated. Could you please add me to the organization, so that I can work on it?
",caizixian,Lukasa
624,2016-03-02 14:05:04,"@kennethreitz @Lukasa @sigmavirus24 
I'm a native speaker of Chinese (Simplified) and Chinese (Traditional) .
Current translation is kinda outdated. Could you please add me to the organization, so that I can work on it?
",caizixian,sigmavirus24
614,2012-05-15 18:43:17,"@ib-lundgren 

3.3 will [support](http://docs.python.org/dev/whatsnew/3.3.html#pep-414-explicit-unicode-literals) explicit unicode literals. 
",piotr-dobrogost,ib-lundgren
608,2012-05-14 16:33:46,"@kracekumar no, this is a backwards incompatible change.
",kennethreitz,kracekumar
606,2012-08-26 19:20:57,"@davidfischer > What version of openssl are you using? Is your connection using TLSv1?

$ openssl version
OpenSSL 0.9.8r 8 Feb 2011

EDITED to correct:

> > > ssl.OPENSSL_VERSION
> > > 'OpenSSL 0.9.7l 28 Sep 2006'

$ python -V
Python 2.7.1

Looks like TLSv1 from what I can tell...
$ openssl s_client -connect www.torproject.org:443 | grep Protocol
depth=2 /C=US/O=DigiCert Inc/OU=www.digicert.com/CN=DigiCert High Assurance EV Root CA
verify error:num=20:unable to get local issuer certificate
verify return:0
    Protocol  : TLSv1
...

And to verify:

> > > import requests
> > > requests.get('https://www.torproject.org/projects/torbrowser.html.en')
> > > Traceback (most recent call last):
> > >   File ""<stdin>"", line 1, in <module>
> > >   File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/requests-0.13.9-py2.7.egg/requests/api.py"", line 65, in get
> > >     return request('get', url, *_kwargs)
> > >   File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/requests-0.13.9-py2.7.egg/requests/safe_mode.py"", line 39, in wrapped
> > >     return function(method, url, *_kwargs)
> > >   File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/requests-0.13.9-py2.7.egg/requests/api.py"", line 51, in request
> > >     return session.request(method=method, url=url, **kwargs)
> > >   File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/requests-0.13.9-py2.7.egg/requests/sessions.py"", line 252, in request
> > >     r.send(prefetch=prefetch)
> > >   File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/requests-0.13.9-py2.7.egg/requests/models.py"", line 632, in send
> > >     raise SSLError(e)
> > > requests.exceptions.SSLError: [Errno 1] _ssl.c:499: error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown protocol
",von,davidfischer
606,2012-08-26 19:40:49,"@davidfischer Thanks for the pointer about the old version of openssl, updating to 2.7.3 with the latest Mac installer fixed it.


",von,davidfischer
601,2012-11-26 21:21:13,"@slingamn do #611 and #962 satisfy your desire for better documented testing practices?
",sigmavirus24,slingamn
598,2012-05-09 09:54:07,"@reclosedev Interesting, but not the problem I'm having ;-)
@piotr-dobrogost Okay, I suppose it is my setup indeed:



I'm on Ubuntu 12.04, behind a proxy (which is known to do funny things), could that be the problem? If so, why does it work with `urllib2`, but not with `requests` and `urllib`?
",maebert,reclosedev
595,2012-05-08 20:26:58,"@reclosedev 

Nice work.

> Yes, but `get` is patched in this testcase

... and all sync tests are being run using async api as well. As your two requests in the async test are sequential you might want to replace sync test with the async one (which tests more) and remove the latter. You might also want to reference request from reply instead the other way around in case the link in this direction is removed in the future :)

Btw, please remember not to link to moving target (develop) as this doesn't make sense.
",piotr-dobrogost,reclosedev
583,2012-05-08 16:00:24,"@joshimhoff go for it :)
",kennethreitz,joshimhoff
583,2012-05-10 18:11:16,"@joshimhoff fantastic work.
",kennethreitz,joshimhoff
578,2012-05-15 14:52:54,"@slingamn I'll work on a patch for this, if no one else is doing it.
",joshimhoff,slingamn
578,2012-05-15 19:21:36,"@slingamn 
Sorry for not responding earlier. I don't think I'll find the time for this soon. If @joshimhoff is willing, that would be great.

@piotr-dobrogost
I see two use-cases for `RequestsCookieJar`:
- One is the dev interested in a single domain, in which case `dict(res.cookies)` would be sufficient.
- The other is the dev interested in multiple domains, in which case it is necessary to be explicit.

I think I would default to the first case, but force the dev to use the second when necessary to avoid ambiguity. In other words, when using the simple API would cause a conflict, raise an exception.



I'm not sure if it would be better to raise the exception when more than one domain/path is stored in the jar, or only when there is an actually conflicting cookie. I would say the former to favor explicitness. It doesn't make much sense to have a `dict` of cookies from multiple domains.

The simple API would have `res.cookies` act as a standard `dict` object, including the `.get` method. I would then add a special API for `Cookie` objects:
- `.get_cookie(name, domain, path, ...)` returns a `Cookie` object.
- `.set_cookie(name, value, domain, path, ...)` sets/creates a `Cookie`.
- `.get_dict(domain, path, ...)` returns a `name:value` dict of cookies that match the specified requirements.
- perhaps some useful info methods as well, like `.list_domains`.

Anyway, it's not a simple problem. It's worth some discussion to get it right.
",dhagrow,joshimhoff
578,2012-05-15 19:21:36,"@slingamn 
Sorry for not responding earlier. I don't think I'll find the time for this soon. If @joshimhoff is willing, that would be great.

@piotr-dobrogost
I see two use-cases for `RequestsCookieJar`:
- One is the dev interested in a single domain, in which case `dict(res.cookies)` would be sufficient.
- The other is the dev interested in multiple domains, in which case it is necessary to be explicit.

I think I would default to the first case, but force the dev to use the second when necessary to avoid ambiguity. In other words, when using the simple API would cause a conflict, raise an exception.



I'm not sure if it would be better to raise the exception when more than one domain/path is stored in the jar, or only when there is an actually conflicting cookie. I would say the former to favor explicitness. It doesn't make much sense to have a `dict` of cookies from multiple domains.

The simple API would have `res.cookies` act as a standard `dict` object, including the `.get` method. I would then add a special API for `Cookie` objects:
- `.get_cookie(name, domain, path, ...)` returns a `Cookie` object.
- `.set_cookie(name, value, domain, path, ...)` sets/creates a `Cookie`.
- `.get_dict(domain, path, ...)` returns a `name:value` dict of cookies that match the specified requirements.
- perhaps some useful info methods as well, like `.list_domains`.

Anyway, it's not a simple problem. It's worth some discussion to get it right.
",dhagrow,slingamn
578,2012-05-15 19:49:42,"@joshimhoff go for it :-)
",slingamn,joshimhoff
565,2012-04-26 20:59:16,"@slingamn I'll update the repo to pass today :)
",kennethreitz,slingamn
565,2012-05-02 00:22:30,"@dhagrow and @slingamn  are my heroes.

:sparkles: :cake: :sparkles:
",kennethreitz,slingamn
559,2012-05-06 08:29:42,"@barberj there is some (likely trivial) merge conflict now; can you update? Sorry.
",slingamn,barberj
559,2012-05-06 21:54:50,"@slingamn done
",barberj,slingamn
559,2012-05-06 22:02:45,"@barberj hey sorry, it looks like the merge is messed up; upstream changes since your branch are showing as part of your branch.

Can you try this?


",slingamn,barberj
559,2012-05-06 22:16:52,"@slingamn its not letting me push...
! [rejected]        issue_526 -> issue_526 (non-fast-forward)
",barberj,slingamn
559,2012-05-06 22:19:47,"@slingamn looks like that did it. thanks
",barberj,slingamn
559,2012-05-06 23:26:44,"@slingamn good call. obvious test case that i missed... mmm cake.
",barberj,slingamn
557,2012-12-13 00:06:15,"@slingamn is it possible @viciu doesn't have that path? Maybe some other package he's installed removed it/replaced it/moved it?
",sigmavirus24,slingamn
557,2013-01-21 20:04:48,"@slingamn: Given #1065 (which was opened after a discussion on #1033), can we close this?
",Lukasa,slingamn
557,2013-01-26 18:06:17,"@Lukasa I'm for closing it. 
",sigmavirus24,Lukasa
556,2012-04-22 11:49:16,"@barberj I don't know—that's the point. I'm not sure what the correct approach is for py2 AND py3.
",idan,barberj
555,2012-04-19 01:32:33,"@kennethreitz: Can you suggest a workaround?
",bhadra,kennethreitz
552,2012-04-23 10:39:30,"@kennethreitz, quick question.  Do you consider `DEFAULT_CA_BUNDLE_PATH` part of the supported API now?  I'm just wondering if I can call `assert` against it, and expect code to carry on working for the foreseeable future.  Not asking for promises, just a nod ;)

Perhaps, the One Blessed Method for disabling the `certifi` support should be stated in a packaging document?  I'd be surprised if the changes here allowed packagers to use the package as-is, and a continuing variety of solutions is quite difficult to support for naïve `requests` users like me.
",JNRowe,kennethreitz
552,2012-04-23 14:51:26,"@JNRowe Yeah, it's tricky. Perhaps, as the administrator, you could `export REQUESTS_CA_BUNDLE` in `/etc/profile.d` or something similar? That would globally override local installations of `certifi`.

What further changes do you think would be required for packaging?
",slingamn,JNRowe
552,2012-04-23 19:04:35,"_EDIT_: Oh wow, sorry guys.  I didn't realise this comment was so long, it'll teach me to keep to the tiny textbox next time.

> @JNRowe Yeah, it's tricky. Perhaps, as the administrator, you could `export REQUESTS_CA_BUNDLE` in `/etc/profile.d` or something similar? That would globally override local installations of `certifi`.

Good idea, and it would work for managed desktop and server deployment.

My question was more concerned with a stable method for testing the environment we're running under on client systems.  I guess if `requests.utils.get_os_ca_bundle_path()` is part of the stable API we could check that, `.startswith('/etc/')` is most of the way there for me.

It is far better than the current situation, needing a lot of code pasting to check for various changes.  On Debian,  the `where()` call is just replaced with `/etc/ssl/certs/ca-certificates.crt`.  On Gentoo, `certifi` is installed with a symlink to the system certs, which necessitates a call to `os.path.realpath()`.

> What further changes do you think would be required for packaging?

Without trying to be deliberately obtuse I'd say ""ask the packagers"".  I may be totally wrong, the next round of distributor packages could accept this behaviour and ship vanilla packages.  The only reason I doubt it is the automagic nature of the changes, the certs being chosen depending on whether another package is currently installed system-wide or available in `$PYTHONUSERBASE` or somewhere else seems a little off.

Don't get me wrong, I think you and @kennethreitz came to the right conclusion for the common case.  Most people simply don't care about this stuff that deeply, and the ones that do will patch the behaviour anyway.  That is why I resigned myself to checking at runtime, and asked for a little confirmation on an acceptable way to do it.
",JNRowe,JNRowe
552,2012-04-23 19:04:35,"_EDIT_: Oh wow, sorry guys.  I didn't realise this comment was so long, it'll teach me to keep to the tiny textbox next time.

> @JNRowe Yeah, it's tricky. Perhaps, as the administrator, you could `export REQUESTS_CA_BUNDLE` in `/etc/profile.d` or something similar? That would globally override local installations of `certifi`.

Good idea, and it would work for managed desktop and server deployment.

My question was more concerned with a stable method for testing the environment we're running under on client systems.  I guess if `requests.utils.get_os_ca_bundle_path()` is part of the stable API we could check that, `.startswith('/etc/')` is most of the way there for me.

It is far better than the current situation, needing a lot of code pasting to check for various changes.  On Debian,  the `where()` call is just replaced with `/etc/ssl/certs/ca-certificates.crt`.  On Gentoo, `certifi` is installed with a symlink to the system certs, which necessitates a call to `os.path.realpath()`.

> What further changes do you think would be required for packaging?

Without trying to be deliberately obtuse I'd say ""ask the packagers"".  I may be totally wrong, the next round of distributor packages could accept this behaviour and ship vanilla packages.  The only reason I doubt it is the automagic nature of the changes, the certs being chosen depending on whether another package is currently installed system-wide or available in `$PYTHONUSERBASE` or somewhere else seems a little off.

Don't get me wrong, I think you and @kennethreitz came to the right conclusion for the common case.  Most people simply don't care about this stuff that deeply, and the ones that do will patch the behaviour anyway.  That is why I resigned myself to checking at runtime, and asked for a little confirmation on an acceptable way to do it.
",JNRowe,kennethreitz
552,2012-04-23 19:33:36,"@JNRowe Thanks, these are really good observations. I'll get in touch with the guy who maintains `requests` for Fedora and see if this change is sufficient for him to package `python-requests` with a dependency on `ca-certificates`.

I understand now why you wanted to be able to test `DEFAULT_CA_BUNDLE_PATH`. I'd put in a tentative +1 for making this part of the API, but my understanding of how stable that would be is shaky at best :-)

By the way, do you know what the standard path to the CA certificates is on Gentoo, and the name of the package that provides them?
",slingamn,JNRowe
552,2012-04-23 20:22:32,"Fedora maintainer here, @slingamn this is a clean approach, i was lucky that people allowed me to package python-certifi, i would be happy to remove any dependency on certifi and default to system CA certs.
",sagarun,slingamn
543,2012-04-10 04:22:02,"This literally appears to have been made with requests in mind (and is mentioned in the examples as an acceptable project). Input @shazow, if you don't mind?
",TkTech,shazow
541,2012-05-03 07:27:09,"@slingamn you're a machine
",kennethreitz,slingamn
539,2012-05-07 00:19:21,"@kennethreitz Well, of course it will be. You're reading ""chunks"" of 1 byte at a time. However, when we talked about changing the default `chunk_size`, it was for `iter_lines()`, not `iter_chunk()` (see my pull request). The reason was to allow `iter_lines()` to behave like most people using requests would expect it to behave: Read lines as they come in. If you do use `iter_lines()` to read from a streaming API like the Twitter API, and it was used with an `chunk_size` of 10K, you may have to wait a freaking long time before you ever get any line.

That said, I'm open to alternatives, but unless you start polling the socket in non-blocking mode, the `read(chunk_size)` will block until it get `chunk_size` or the connection is closed AFAIK. Please let me know if I'm missing something.

@piotr-dobrogost If you know of another implementation to read lines as they come in from a socket while using blocking I/O, please feel free to let me (or us) know. I'm not trying to sell my quick fix to anyone, I'm simply trying to find a way to have `iter_lines()` behave like (I hope) it was meant to behave.

@kennethreitz Also, if you feel `chunk_size` should not be set to 1 in the distribution, please change it back, I don't want to impose on anybody. I can specify it in my code but I'm pretty sure you'll get regular ""issues"" opened about this thing by people wondering why their ""lines"" are not coming in when sent by the server as expected. You already had two soon after the initial code change, @gwrtheyrn and I.
",mponton,kennethreitz
522,2012-11-26 23:28:21,"@idan @kennethreitz you never answered my question, and beyond that accepting update sequences everywhere seems a bit unreasonable
",sigmavirus24,kennethreitz
522,2012-11-26 23:28:21,"@idan @kennethreitz you never answered my question, and beyond that accepting update sequences everywhere seems a bit unreasonable
",sigmavirus24,idan
520,2012-05-06 19:08:15,"@shazow 

Can you clarify the reasons for defaulting `prefetch` to off instead of on?

So --- defaulting `keep_alive=True` in the context of a session makes sense. But in the context of the vanilla API, that is to say, a plain `requests.get` or `requests.post`, there doesn't seem to be any mechanism for reusing the keep-alive connections, because an ephemeral `Session` object is created and it is given an ephemeral `PoolManager` in `init_poolmanager()`.
",slingamn,shazow
520,2012-05-06 19:29:56,"Is an O(1) leak really a leak? ;)

I'm -0 on `prefetch=True` just because I feel the current settings cover the +80% use case, but I wouldn't shed any tears. Up to @kennethreitz, I'd say.
",shazow,kennethreitz
520,2012-05-06 20:45:16,"@slingamn keep-alive should be on, we're using it for 301s and the like.
",kennethreitz,slingamn
520,2013-10-15 00:52:20,"@Lukasa thanks for the tip.  After more research I am pretty sure this is a problem with the server side code in etcd.

Thanks!
",rca,Lukasa
520,2014-02-24 21:40:56,"@Lukasa Thanks for the quick response, and you miss understood my question, sorry for not being clear, what i meant is actually what is described in this bug report. 

If i have in my script a call like this `request.get(....)`, this call will create a new Session object which create a new connection pool, and from this later a connection will be created/borrowed to send my GET request, this connection will not be closed by the server (b/c of keep-alive, assuming that the server understand it), so we got now one connection in the pool that is still open and ready to be re-used right ?

But how about if i do another `requests.get(...)`This will also create another session which mean another connection pool and like before the result will be one connection from this new pool that is still open and ready to be re-use, but as you can see they will not be re-used because in the end the session object is not re-used.

So the result now is that we have 2 connection open to our server which will not be close until my program stop (clean up by the kernel !?) or the server clean them up (if it's support a keep alive timeout), so you can imagine what will happen if i have a lot of `requests.get(...)` calls (as an example).

First of all does this make sense ? If so what what is the guide line of using ephemeral session ? if this laters will not clean up after them self, unless i call `session.close(...)` (which will close opened connection from the pool), and if the close call is needed why wasn't it part of the api shortcuts ? And wouldn't it make more sense to disable connection pooling in a use case like this one ?
",mouadino,Lukasa
513,2012-04-02 11:20:51,"its working now thanks a lot @BYK @johtso 
",morogoro,johtso
513,2012-04-02 22:28:30,"@johtso That was exactly what I was looking for, thanks for the quick reply!
",rbui,johtso
511,2012-03-24 18:27:55,"Is there any chance of getting commit 8c5d06148bfa94ba87f369202b909d8c79897568 pulled, @kennethreitz?
",kvonhorn,kennethreitz
507,2012-03-23 21:40:37,"@johtso 
As I don't get the above description, could you please write a short example showing what you have in mind ?
",piotr-dobrogost,johtso
507,2012-03-27 07:46:42,"@kennethreitz 

How do you like this?
",piotr-dobrogost,kennethreitz
507,2012-07-31 20:39:29,"@kennethreitz 

Why did you close this?
",piotr-dobrogost,kennethreitz
505,2012-04-26 12:32:43,"@johtso are you working on this?
",barberj,johtso
505,2012-04-26 22:40:41,"@barberj I'm afraid I haven't had a chance to work on this, no.
",johtso,barberj
505,2012-04-29 22:00:49,"@berkerpeksag awesome! The second one (with `config`) is definitely best
",kennethreitz,berkerpeksag
505,2012-04-30 07:10:31,"@kennethreitz thanks!
",berkerpeksag,kennethreitz
503,2012-06-13 12:37:37,"@plaes Awesome, thanks for the quick response.
",kylerob,plaes
502,2012-04-07 17:31:40,"@maxcountryman right now, original/modified values are all mixed together. Its unclear. `body` needs to be called `content`. 
",kennethreitz,maxcountryman
502,2012-04-07 23:19:38,"@maxcountryman 

I'm talking about passing the original value of `content_type` to hooks not the ability to modify it. Passing content and not passing content type feels wrong.
",piotr-dobrogost,maxcountryman
499,2013-04-25 16:20:19,"@jmoiron @kennethreitz I'm sorry, but how can it be currently done?
Building API client for another service that uses signature-based authentication and when I'm trying to add parameters by editing r.params, I receive:
AttributeError: 'PreparedRequest' object has no attribute 'params'
",pitsevich,kennethreitz
499,2013-04-25 17:28:34,"@sigmavirus24 thanks, this worked!
",pitsevich,sigmavirus24
498,2012-03-19 14:10:32,"@kennethreitz How do you want to handle merging in the latest urllib3? Once that's dealt with, I can make the necessary changes to make `requests` compatible with App Engine on Python 2.7.
",singingwolfboy,kennethreitz
498,2012-03-31 03:00:36,"@shazow I'm a bit of a noob but could try my hand at a merge.... this would be incredibly helpful to those of us in the python environment on app engine!
",rdixit,shazow
498,2012-03-31 03:13:48,"Hey, I just tried this again with the current master branch. Looks like the same AttributeError referenced in the above stackoverflow post, which @shazow mentioned probably has to do with using the local filesystem for certain things in Requests. The local filesystem is not available on AppEngine- I tend to use StringIO in a lot cases where I'd relied on the file system in the past, but not sure how how helpful that is.
",rdixit,shazow
498,2012-03-31 07:23:09,"@shazow, I choose you!

![](http://4.bp.blogspot.com/_J0fSQHG901k/Sw40rKFnN8I/AAAAAAAAAT4/cLXi4xhQTNQ/s1600/ash+trowing+pokeball.png)
",kennethreitz,shazow
498,2012-03-31 21:41:52,"@shazow if only we knew a Google Employee...
",kennethreitz,shazow
493,2012-03-18 23:16:20,"Btw, @kennethreitz: Requests still does not support AppEngine, since you're depending on various filesystem things for configuration magic that AppEngine doesn't have. Not sure if you want to reopen this bug.
",shazow,kennethreitz
493,2012-03-18 23:25:20,"@shazow those should all be supplementary. I guess I should fire up an app engine app and find out :)
",kennethreitz,shazow
493,2012-03-18 23:26:03,"@kennethreitz Here's a failure for example: http://stackoverflow.com/questions/9762685/using-the-requests-python-library-in-google-app-engine/9763217
",shazow,kennethreitz
482,2012-03-15 17:42:58,"As @kennethreitz knows, I'm preparing to use requests in a different project, and was testing out the GitHub API. The following is something I did from the python command line interpreter:


",sigmavirus24,kennethreitz
482,2012-05-17 18:40:20,"@kennethreitz: Whereabouts in the docs did you want these? The only place that currently exists that looks suitable is docs/quickstart.rst, but that document is getting increasingly large. Should we consider busting the verbs out into a new file? Or are you happy to extend the quickstart? Either way, I'm happy to write these and add them into #619.
",Lukasa,kennethreitz
482,2013-11-20 14:29:47,"@Lukasa the emails I received from @RAINCEN had the same message that @Fighter42 posted about 20 times (of which I deleted all but one). I have to wonder if these are just spam accounts or if perhaps this is related to [recent attempts to brute force passwords on accounts](https://github.com/blog/1698-weak-passwords-brute-forced). Regardless, it's probably advisable to just ignore everyone who makes similar comments on this issue. It's been closed for so long.
",sigmavirus24,Lukasa
480,2012-03-31 15:21:37,"@kennethreitz If I'm understanding you, I don't think this issue is solved from my perspective. Let me give you my use case.

When I fetch an HTML resource with requests, I want to parse the HTML with the encoding as it was intended. That means:
1. Looking in the HTTP header.
2. Looking in the meta tag.
3. Making a dumb guess using something like chardet.

_Setting_ the encoding on a request doesn't help me there, because I want to get the natural encoding response. And if it failed to get it from the headers, I can continue down the line and look at the meta tag.

Right now in my code I'm working around this by just checking to see if requests responded with ISO-8859-1 and ignoring it because I don't have any guarantees it was actually there. The better solution for me would be for this method to return None.
",umbrae,kennethreitz
478,2012-03-15 15:53:55,"@shazow Sounds good man. @wolever can you make sure you are happy with this code and approach, if so I will make the necessary fork/mods. ---edited---
",foxx,shazow
468,2012-03-17 22:24:28,"@pydanny @kennethreitz I'm very aware of what oauthlib is. I totally understand its advantages, and let me insist that I've never wanted to state it's a bad project or idea. It also will ease releasing providers for different Python frameworks, something as I said is missing and that is a hard  task.

When I started requests-oauth, I wanted to add OAuth consumer ability to requests, which I guess we agree it's the future of http libraries in Python. 

I guess what I don't understand is why oauthlib started from scratch, when there were very good projects with lots of corner cases covered. The code base of those projects was a good starting point. Projects like mine or python-oauth2 (mine was based on it in the beginning), could have been refactored into a independent library. I'm guessing with the entrance of these great new devs, this is now a reality, as they are capturing their know-how in oauthlib, great.

> All of the excellent work you've done for bringing oauth into requests could be immensely useful to the development of oauthlib, which is the best thing going forward for the Python community as a whole. Not just requests.

Thanks. I'm working hard on trying to maintain my hook up to date as you know, as this is the most used choice for requests and several people depend on my development to continue working on their projects.

> The reason your pull requests as of late have been sitting there is because they are great changes that need to happen and can't be merged in immediately because of the fundamental changes they make. Normally it'd be easier, but there's underway a rearchitecture of requests / flask / werkzeug / cache / httplib / dns security libraries. This is all pretty much undocumented at this point, but I hope to fix that this week. Lots of great progress happened at PyCon.

I'm looking forward to seeing what this is all about. It felt that 27 days for getting an answer on an issue that was blocking the development of an important plugin of the application, was too long, considering 7 days before, I got an answer on an other issue #468. I know myself how busy we can all be with our lives and open source all at the same time. In your case maintaining big projects such as requests, it's even more demanding.

What I'm saying is that at the moment, requests-oauth is what people are using, until OAuth gets merged into core, there needs to be a way I can keep development alive. If I stop maintaining today my hook because something is superseding it, there will be a gap of time in which people will have no choice. I depend now on you to be able to maintain it.

Cheers,
Miguel
",maraujop,kennethreitz
467,2012-03-02 06:34:37,"@kennethreitz Yes guy, it has been fixed, and you are really efficient :)
",reorx,kennethreitz
465,2012-07-26 12:32:41,"Yes, but this solution is still cleaner (and IMO easier) than doing the decompression at a second place because this is already built-in in requests.

But I agree with you in general, a `r.file` (or something like this) has much more use cases than `r.raw`. So I would like to see this included in requests, too. @kennethreitz 
",schlamar,kennethreitz
465,2013-02-07 19:16:56,"@Lukasa is correct. `content` should always be a bytestring (in Python 3 it's an explicit bytestring; in Python 2 str == bytes). The only item that is not a bytestring is `text`.
",sigmavirus24,Lukasa
465,2013-03-19 06:38:15,"@kennethreitz any news on this? This is a pretty serious design bug and it's best to sort it out early. The more code gets written to work around it, the more costly it becomes for everyone.
",scoder,kennethreitz
465,2013-03-19 08:52:20,"@Lukasa
I can't really see how filing the bug against urllib3 would fix the API of requests, at least not all by itself.

And I agree that your ""use case"" is contrieved. As I said, if the client cannot positively control the compression on the server side (and it disable it, but not reliably enable it), so relying on it to be able to save a compressed file to disk is, well, not so interesting.
",scoder,Lukasa
465,2013-03-19 08:55:45,"@schlamar
I agree that it can be read as such. I assure you that I'm fine with anything that solves this problem. If opening a new ticket is required in order to get there, so be it.
",scoder,schlamar
465,2013-03-19 10:24:54,"> Although, does that actually work? I.e. are the decompressors stateful and incremental? 

Oh, doesn't seem so. I didn't read the docstring.

However, here is my proposal:
1. Patch urllib3 so that `HTTPResponse.read` works with `amt` and `decode_content` concurrently.
2. Make HTTPResponse._decode_content a public member (so you can do `response.raw.decode_content = True` instead of patching the `read` method).
3. Drop decompression in requests completely by using `decode_content=True` in `iter_content`

@Lukasa I think this won't violate the feature freeze, right?
",schlamar,Lukasa
465,2013-03-19 12:19:24,"@schlamar: In principle, sure. As long as the API remains unchanged, internal changes _should_ be ok, and I'd be +1 on this one. However, bear in mind that I'm not the BDFL, =)
",Lukasa,schlamar
458,2012-03-29 07:35:27,"@kennethreitz - what if I explicitly want it off?
",yuvadm,kennethreitz
458,2012-03-31 10:44:55,"@kennethreitz for example, if I'm on a server with limited resources and I know for sure that I'm making a single connection that should immediately be closed.
",yuvadm,kennethreitz
441,2012-02-21 17:59:37,"Like @umbrae said, this is an HTTP library, not an HTML library.
",kennethreitz,umbrae
440,2012-02-21 21:35:11,"The reason it's necessary/desired is that [RFC 2965](http://www.ietf.org/rfc/rfc2965.txt) requires that a cookie value either be a token, [which may not contain an '=' character](http://www.ietf.org/rfc/rfc2068.txt), or a quoted string. If the server isn't quoting the string, the server is non-compliant. As @kennethreitz says, Oreos already allows non-RFC-compliant characters in cookie keys because they're commonly sent out.
",Lukasa,kennethreitz
436,2012-02-28 15:51:39,"@RonnyPfannschmidt Thanks for the insight Ronny.
",umbrae,RonnyPfannschmidt
436,2012-03-08 01:29:17,"@RonnyPfannschmidt care to add it? :)
",kennethreitz,RonnyPfannschmidt
429,2012-03-08 03:09:14,"Also @mgiuca, thank you very much for raising the issue about the + and %20 stuff, our patch was failing on several tests due to this problem - so you've saved us a lot of time :) Thank you again.
",foxx,mgiuca
429,2012-03-08 04:10:43,"@mgiuca good to know :)
",kennethreitz,mgiuca
429,2012-03-31 06:32:51,"@mgiuca we could really use your help with a new collaboration with @mitsuhiko to merge requests and werkzeug. Would you like to help?
",kennethreitz,mgiuca
429,2012-03-31 07:08:58,"@mgiuca It'll likely be over the next month or two :)
",kennethreitz,mgiuca
427,2012-02-19 13:29:22,"@kennethreitz Hm, I thought you would consider adding Python 2.5 support again.. at least that's what I understood when I asked you  about it regarding requests adoption in pip. Mind elaborating?
",jezdez,kennethreitz
427,2012-02-19 13:46:17,"@kennethreitz Not sure what more is needed than a pip developer saying that requests adoption requires 2.5. A public statement?
",jezdez,kennethreitz
427,2012-08-24 10:27:30,"@Lukasa Good idea +1
",jezdez,Lukasa
426,2012-02-15 00:17:49,"@mgiuca ah - i see what you mean, looks like it is a completely different issue, thanks!
",foxx,mgiuca
426,2016-01-30 18:35:45,"@Lukasa , because some website do have a percentage sign in their get parameters so I kind of need to avoid the percentage sign to be encoded.
",dorafmon,Lukasa
417,2012-02-09 19:31:22,"> We shouldn't be relying on urllib3 for decompression right now, actually.
> The fact that we are is a bug.

@kennethreitz I would agree. FWIW, I've come across some issues using requests (urllib3) with some gzipped responses.
",JeffPaine,kennethreitz
417,2012-02-10 00:44:31,"@kennethreitz Any reasonable guesses as to when it might be squashed? Thanks very much.
",JeffPaine,kennethreitz
388,2012-01-26 18:59:12,"@kennethreitz What if I added in using the 2to3 into the Makefile so we could generate a Python 3 compatible install? The vast majority of users are still on the Python 2.x line and will be for the foreseeable future.

Requests is great and I would like to see it replace all the other cruft out there by being available everywhere. It would be nice if I could run it on cPython 2.5 and Jython, it limits my use of it on the systems I use as I can't migrate programs across environments.
",seanjensengrey,kennethreitz
386,2012-01-25 15:32:58,"@shazow +1
",kennethreitz,shazow
378,2012-01-22 17:32:58,"@johtso Strange thing. I got notification about your post which begins with _Ah yes, of course, forgot about 0. What about False? if (v is None) or (v is False): v =_ but I don't see your post from notification...
",piotr-dobrogost,johtso
377,2012-04-09 03:43:11,"Fixed thanks to @newmaniese 
",kennethreitz,newmaniese
369,2012-02-11 23:55:25,"@kennethreitz but you seem to be indicating that it is correct behaviour for most (or even some) cases. I can't see how it could ever be right, so it bothers me to see it even implemented (regardless of whether you can turn it off).
",timbertson,kennethreitz
369,2012-02-12 10:33:48,"@kennethreitz What do you mean by optimised? Quicker? Or restricted to non-ASCII characters?

I admit I hadn't heard of IRIs before now, but from a quick skim of the spec converting IRIs sounds like it must not escape an existing percent character, which the current code is doing.
",timbertson,kennethreitz
362,2012-02-01 17:12:23,"@kennethreitz For which? How do you implement proxies? Are you using ProxyManager?
",shazow,kennethreitz
362,2012-02-01 17:13:31,"@shazow https://github.com/kennethreitz/requests/blob/develop/requests/models.py#L447
",kennethreitz,shazow
362,2012-02-01 17:19:41,"@kennethreitz Thanks! Hmm. The underlying HTTPSConnectionPool should be making a CONNECT.

Are you sure you're actually setting it to HTTPS? Because if you look at the example...



Both of the proxies are http and port 80, so urllib3 has no way of knowing it's HTTPS unless you do it explicitly.

Anyways, if it is urllib3's fault, if someone writes a failing test in urllib3, I'll look into it this weekend. :)

Something between...
https://github.com/shazow/urllib3/blob/master/test/with_dummyserver/test_https.py and
https://github.com/shazow/urllib3/blob/master/test/with_dummyserver/test_socketlevel.py
",shazow,kennethreitz
361,2012-01-19 20:49:54,"@kennethreitz Note that this would be a trivial user addition if multiple hooks were supported, which would be a nice way to add this for people who care without adding two syscalls (albeit an extremely cheap call on modern Unix variants) per request for everyone.
",acdha,kennethreitz
361,2012-01-19 20:51:00,"@acdha multiple hooks would be huge. I want. Badly.
",kennethreitz,acdha
361,2012-01-23 07:59:16,"@acdha multiple hooks are in!
",kennethreitz,acdha
359,2012-02-28 14:15:36,"@RonnyPfannschmidt I'm looking into debugging issue #436 with generate_chunked and noticed you merged this. I was wondering if you had any insight on what the trouble is there?
",umbrae,RonnyPfannschmidt
358,2012-08-09 14:22:47,"What's the state of this @kennethreitz ? Because it looks done to me...
",Lukasa,kennethreitz
335,2012-02-23 18:59:28,"Hmmm, just encountered a CookieError =/, Is there any way to just ignore all cookie errors @kennethreitz? 

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Library/Python/2.6/site-packages/requests/api.py"", line 79, in get
    :param url: URL for the new :class:`Request` object.
  File ""/Library/Python/2.6/site-packages/requests/api.py"", line 66, in request
    """"""Sends a HEAD request. Returns :class:`Response` object.
  File ""/Library/Python/2.6/site-packages/requests/sessions.py"", line 191, in request

  File ""/Library/Python/2.6/site-packages/requests/models.py"", line 462, in send
    r = self.proxy_auth(self)
  File ""/Library/Python/2.6/site-packages/requests/models.py"", line 242, in _build_response
    except KeyError:
  File ""/Library/Python/2.6/site-packages/requests/models.py"", line 462, in send
    r = self.proxy_auth(self)
  File ""/Library/Python/2.6/site-packages/requests/models.py"", line 186, in _build_response
    response.cookies = cookies
  File ""/Library/Python/2.6/site-packages/requests/models.py"", line 169, in build
    response.status_code = getattr(resp, 'status', None)
  File ""/Library/Python/2.6/site-packages/requests/packages/oreos/core.py"", line 19, in dict_from_string
    c.load(s)
  File ""/Library/Python/2.6/site-packages/requests/packages/oreos/monkeys.py"", line 641, in load
    map(Cookie.__setitem__, d.keys(), d.values())
  File ""/Library/Python/2.6/site-packages/requests/packages/oreos/monkeys.py"", line 674, in __ParseString
    M[ K ] = _unquote(V)
  File ""/Library/Python/2.6/site-packages/requests/packages/oreos/monkeys.py"", line 594, in __set
    def __set(self, key, real_value, coded_value):
  File ""/Library/Python/2.6/site-packages/requests/packages/oreos/monkeys.py"", line 468, in set
    if key.lower() in self._reserved:
requests.packages.oreos.monkeys.CookieError: Illegal key value: 10:54:14&CookieExpireDate
",dalanmiller,kennethreitz
335,2012-02-23 20:21:55,"@dalanmiller 
How can we reproduce this?
",piotr-dobrogost,dalanmiller
335,2012-02-23 21:11:03,"@dalanmiller What version of requests are you using? When I hit http://nordstrom.com/, I find the illegal cookie key to be ""13:05:29&CookieExpireDate"", which is illegal because it contains colons. This was fixed in commit f72c13f .
",Lukasa,dalanmiller
335,2012-02-23 21:18:54,"@Lukasa According to pip I'm using Requests 0.10.4



As well:



Results in 



I don't even need cookies for what I'm trying to do, is there anyway I can just set something to ignore them and errors?
",dalanmiller,Lukasa
335,2012-02-23 21:24:33,"The two failures given by @umbrae fail because the first one contains parentheses and the second one contains a question mark. I'm reluctant to just keep patching up failures when they arise, I'd be more inclined to rewrite the code to correctly handle all of the cookies. I'm looking to get around to it sometime in the next few days, but if someone else wants to tackle it they're welcome.

@dalanmiller Odd. If I launch a new virtualenv, pip install requests and then perform a get on http://nordstrom.com/ , I don't get a CookieError, and pip claims I'm running 0.10.4 as well.
",Lukasa,umbrae
335,2012-02-23 21:24:33,"The two failures given by @umbrae fail because the first one contains parentheses and the second one contains a question mark. I'm reluctant to just keep patching up failures when they arise, I'd be more inclined to rewrite the code to correctly handle all of the cookies. I'm looking to get around to it sometime in the next few days, but if someone else wants to tackle it they're welcome.

@dalanmiller Odd. If I launch a new virtualenv, pip install requests and then perform a get on http://nordstrom.com/ , I don't get a CookieError, and pip claims I'm running 0.10.4 as well.
",Lukasa,dalanmiller
335,2012-02-23 21:29:47,"@Lukasa Tried using a new virtualenv, then installing requests via pip and everything worked perfectly fine like you did. 

Going to try uninstalling / reinstalling requests.

Thank you for all your help! 

p.s.

A `pip uninstall requests` and then a `pip install requests --upgrade` did the trick! Now working! Thank you @Lukasa, @KennethReitz, @piotr-dobrogost. 
",dalanmiller,Lukasa
333,2012-01-05 18:29:58,"@gazpachoking Yeah, a bit torn on the name. `raise_status` or `always_raise` may be better.
",bryanhelmig,gazpachoking
333,2012-01-05 18:44:04,"@bryanhelmig Heh, both of those name have actually been in my code at some point.
@kennethreitz Yeah, now that you mention it as a complement to `safe_mode` I like it a bit better. I just feel like the name doesn't imply that it is going to raise errors. Feels more like it would do something like prefetch mode to me.
",gazpachoking,kennethreitz
333,2012-01-05 18:44:04,"@bryanhelmig Heh, both of those name have actually been in my code at some point.
@kennethreitz Yeah, now that you mention it as a complement to `safe_mode` I like it a bit better. I just feel like the name doesn't imply that it is going to raise errors. Feels more like it would do something like prefetch mode to me.
",gazpachoking,bryanhelmig
333,2012-01-05 19:30:41,"@kennethreitz Let me know if this is enough accept the pull, I'm happy to do any additional legwork.
",bryanhelmig,kennethreitz
333,2012-01-05 19:31:47,"@bryanhelmig I'd be happy to pull now, but I'll give you a sparkly piece of cake if you add it to the docs :)
",kennethreitz,bryanhelmig
333,2012-01-05 19:34:20,"@kennethreitz I do love cake. :-) Where in the docs is this appropriate to mention? I assume the makefile will grab the appropriate docstring and add to http://docs.python-requests.org/en/latest/api/#configurations automatically?
",bryanhelmig,kennethreitz
333,2012-01-05 19:35:16,"@kennethreitz Would http://docs.python-requests.org/en/latest/user/quickstart/#errors-and-exceptions be appropriate?
",bryanhelmig,kennethreitz
333,2012-01-05 19:37:30,"@bryanhelmig even better!
",kennethreitz,bryanhelmig
333,2012-01-05 20:14:07,"@kennethreitz Added something rather minor.

On another note, it might be a good idea to flesh out the configuration section of the advanced page at some point in the future, as it just points to the API docs right now.
",bryanhelmig,kennethreitz
326,2017-02-25 16:25:58,"@kennethreitz Thanks, I had same issue and is resolved by that command
`pip install requests` just saved my life.",swhshamsi,kennethreitz
306,2012-01-24 13:32:17,"@shazow - asking here rather than filing an issue on urllib3, as I'm looking for clarification.

I've just run into this exact issue myself. Seeing as how it's been marked closed 'urllib3 bug', would you like an issue opened for it? I'm not seeing one currently.

**Edit:** For reference, urllib2 does import fine in this environment and throws a urllib2.URLError: ""urlopen error unknown url type: https"" Regular http connections work.
",pudquick,shazow
304,2012-06-29 18:47:36,"@ionrock: this looks perfect. I think I'm going to try to merge this into the codebase ;)
",kennethreitz,ionrock
304,2012-06-29 18:56:54,"@kennethreitz That is great news. Please let me know if I can help. I'm happy to fork and try to merge it myself. I also plan on adding the etag and if-\* header support, which I'm happy to submit as a patch later if need be. 
",ionrock,kennethreitz
304,2012-06-29 18:58:21,"@ionrock Fantastic! Start watching the #700 pull request, where i'll be working on it. It'll be using the cachecore caching interfaces.
",kennethreitz,ionrock
295,2012-05-15 13:55:06,"@kennethreitz, If I wanted to take a look at implementing this and contributing it to requests, where would you suggest I start. I don't want to step on any toes :).
",cpatrick,kennethreitz
295,2013-01-26 21:50:38,"@sigmavirus24 

I don't see how making it possible to stream a request by passing generator/iterator as `data` param's value could magically change how another part of api (`files` param) works.
",piotr-dobrogost,sigmavirus24
281,2012-04-20 21:08:35,"@slingamn

From https://github.com/sashahart/cookies

> This doesn't compete with the cookielib (http.cookiejar) module in the Python standard library, which is specifically
> for implementing cookie storage and similar behavior in an HTTP client such as a browser.

As to

> rather than an external cookie library that seems to have no real community and only one maintainer.

I wouldn't judge based on community's size or number of maintainers. Wouldn't Requests community become this library's community the moment Request starts using it?
",piotr-dobrogost,slingamn
281,2012-04-20 21:19:52,"@kennethreitz 

What do you mean by _The module is practically non-existant._?
",piotr-dobrogost,kennethreitz
272,2011-11-17 09:54:27,"Also as @juanriaza says, this is not an API specific thing, Rdio fails the same way and it was working before.
",maraujop,juanriaza
269,2011-11-20 13:11:31,"@kennethreitz: You mean `allow_redirects`?

You should read [this piece](http://codesearch.google.com/#OAMlx_jo-ck/src/net/url_request/url_request.cc&l=701) of the Google Chrome code:



This is also how all other browsers are doing it. I think it makes more sense to follow that instead of doing it different than everybody else.

Also, as said, some websites/servers even break otherwise because they expect this behavior. (They return 302 instead of a 303 because 303 was too less supported a while ago and browsers do the expected behavior on 302 anyway.)

Also, it was reported [here](https://github.com/szechuen/CCC-Presale/issues/3#issuecomment-2759174) that Requests indeed did it that way a while ago, at least in 0.7.4. It just broke since 0.8.0.
",albertz,kennethreitz
269,2011-11-29 22:13:11,"@dstufft: It's quite common to send 302 redirects after a successful POST. Perhaps you could argue that sites doing this are technically broken, but it's how many sites currently work. And it's how many popular frameworks, such as Django, suggest on handling successful POSTs.

I'm all for doing things ""the right way"". But I get the feeling that `strict_mode` will be set to `False` more often than not.
",andymccurdy,dstufft
265,2013-01-29 16:37:28,"@untitaker: Just because the RFC allows it doesn't stop it being a bug. =)
",Lukasa,untitaker
265,2013-01-29 16:38:29,"@Lukasa If this is not a misbehavior, how would this be a bug then?
",untitaker,Lukasa
239,2012-04-30 20:32:11,"@acdha setting:



before I make any requests still results in the same error, but I think this isn't an option for my implementation as all the requests I'm making will require a redirect =/ 
",dalanmiller,acdha
239,2012-04-30 20:43:11,"@dalanmiller How are you processing your responses? I was previously using `async.map` with a response hook and it _appears_ to be more stable using a simple loop over `async.imap`:


",acdha,dalanmiller
239,2012-04-30 22:29:34,"@acdha 

I was just using a for loop through a url list and doing a request.get on each with my settings and such. 



I tried using your paste and it works for about 50 requests in my 900 length list, until I start to get ""max retries errors exceeded with url"" for the rest.  This is a pretty standard error though for hitting the same domain repeatedly though, no? 
",dalanmiller,acdha
239,2012-07-03 09:18:10,"For me the 'Too many open files' error occurred after downloading exactly 1k files. My solution was to disable keep-alive property, ever getting requests in chunks (@acdha thank you for the hint). `lsof -p PID | wc -l` shows a non-increasing number of connections during the execution.



[1] chunking: http://stackoverflow.com/a/312464
",lmillefiori,acdha
239,2012-09-22 10:43:29,"@kennethreitz What's the urllib3's issue number?
",piotr-dobrogost,kennethreitz
239,2013-08-10 13:59:03,"@Lukasa this was definitely fixed in urllib3 as I was part of the discussion. With an inclination towards being conservative in my estimate, I would say it's there since requests 1.2.x if not 1.1.x.
",sigmavirus24,Lukasa
239,2013-11-29 16:33:20,"@tardyp also, how did you install requests? I think all of the OS package maintainers strip out urllib3. If they don't keep that up-to-date and you're using an old version, that could be the cause instead. If you're using pip, then feel free to open a new issue to track this with instead of adding discussion onto this one.
",sigmavirus24,tardyp
239,2013-11-29 16:35:17,"I installed with pip, but I use python 2.6, I've seen fix on python2.7 for
this bug. Do you monkeypatch for older version?

Pierre

On Fri, Nov 29, 2013 at 5:33 PM, Ian Cordasco notifications@github.comwrote:

> @tardyp https://github.com/tardyp also, how did you install requests? I
> think all of the OS package maintainers strip out urllib3. If they don't
> keep that up-to-date and you're using an old version, that could be the
> cause instead. If you're using pip, then feel free to open a new issue to
> track this with instead of adding discussion onto this one.
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/issues/239#issuecomment-29526302
> .
",tardyp,tardyp
239,2013-11-29 16:53:04,"@tardyp please open a new issue with as much detail as possible including whether the requests you're making have redirects and whether you're using gevent. Also, any details about the operating system and an example of how to reproduce it would be fantastic.
",sigmavirus24,tardyp
239,2016-01-23 10:21:51,"@Lukasa  i need you help 。 i use eventlet + requests，that always create so many sock that can't identify protocol 。 my requests is 2.4.3, is eventlet + requests cause this problem? 
",mygoda,Lukasa
239,2016-01-23 13:10:58,"@Lukasa  thank you。  i think my issue is similar with [this](https://gist.github.com/tamiel/1512329)。 my project is [pyvmomi](https://github.com/vmware/pyvmomi). that connection is long-connection. i always confused why can hold so many can't identify protocol  sock
",mygoda,Lukasa
232,2011-10-31 17:04:23,"Yea, thoughts like ""man, that @kennethreitz is such a jerk!"". ;)

---

A weak Monday morning attempt to recapture my IRC arguments:

There are, IMO, two types of failure modes in this kind of library: network failures, such that you never connect to or hear back from the HTTP server; and HTTP failures, such that you successfully get back a non-200 response.

Kenneth's default is to suppress exceptions, return a valid Request object, and require the user to explicitly ask about exceptions via `Request.raise_for_<whatever>`. (IIRC he does this for both types of failures, but had a thought to create a 2nd `Request.raise_*` method so users could tell the failure types apart.)

My opinion is that this is un-Pythonic and surprising; somebody new to the library but not new to Python will be confused upon getting a successful-but-empty `Request` object, even if the server was down/DNS was down/bad URL/etc.

Furthermore, one will get a ""farther down the road"" traceback (e.g. `None has no attribute 'blah'` or `IndexError`s or etc etc) when trying to use the `Request` object as if it had succeeded, e.g. using its `.content` or `.raw` attributes, which is more confusing and harder to debug than an obvious, at-connection-time `NetworkError` or whatnot.

Such error suppression needs to have a serious, obvious benefit to offset the negative effects of forcing them to appear in strange ways farther down the stack. I don't see such a benefit here, other than Kenneth's noted use case of making N requests in a row/simultaneously, and not wanting connection errors to abort the entire run.

In that situation, my opinion is one should use an opt-in, ""please suppress errors, I will check for them myself"" setting, and that it should not be the default behavior.
",bitprophet,kennethreitz
223,2011-11-29 16:39:05,"@kennethreitz ok yeah snap... with `data` it works... thanks and sorry for the trouble!
",zoranzaric,kennethreitz
223,2012-05-02 10:59:03,"@slingamn Ah, maybe it's fixed in trunk then. I'm just using the version installed via pip, i.e. 0.11.2.
",aknuds1,slingamn
223,2012-05-02 16:31:38,"@slingamn It's the desired output, yes, but why is there a difference towards WCF when I define the content-length header myself?

I mean, httpbin.org reports that the request has defined content-length as ""0"", even though the same request directed at WCF fails with error 411.
",aknuds1,slingamn
223,2012-05-04 06:46:48,"@slingamn Yes, it seems appropriate to me to file a bug against Python, considering this is a real world problem. I suspect it's actually IIS 7.5 that rejects the requests without defined content-length, rather than WCF, since these rejected requests never show up in my WCF log. I've enabled logging of all levels and malformed messages etc, so I'm quite sure these client errors would be logged had they been raised by WCF (as opposed to IIS), the way that for instance invalid service operation invocations are.

I found a StackOverflow [question](http://stackoverflow.com/questions/5915131/can-i-send-an-empty-http-post-webrequest-object-from-c-sharp-to-iis) on this problem re. IIS, which confirms that this server does indeed require the content-length header on POST requests.

Would you like to file the bug against Python, or should I?
",aknuds1,slingamn
223,2012-05-04 17:36:58,"@slingamn Which HTTP methods do you think content-length should be defined for? POST and PUT? I've only seen it mentioned so far that content-length should be defined for requests that intend to place something on the server, and I'm hardly the HTTP expert myself.
",aknuds1,slingamn
223,2012-09-21 14:25:29,"Also, just realized this had bitten me a while back [here](https://github.com/sigmavirus24/github3.py/blob/master/github3/models.py#L120). If @kennethreitz still wants a PR attaching Content-Length to everything, I'd be happy to do so.
",sigmavirus24,kennethreitz
223,2012-10-18 17:28:17,"@kennethreitz where should I branch from, develop or adapters?
",sigmavirus24,kennethreitz
223,2012-11-27 01:59:30,"@kennethreitz Looks like the content-length is being set in case of GET request as well. Where the data being passed is going as query params in the URL. This could be a 400 Bad Request case (which is what is happening for me).

Following is an example of headers sent:
{'Content-Length': u'33', 'Content-Type': 'application/x-www-form-urlencoded', 'Accept-Encoding': 'gzip, deflate, compress', 'Accept': '_/_', 'User-Agent': 'python-requests/0.14.2 CPython/2.7.3 Darwin/10.8.0'}

And this was for a GET request.
",amalakar,kennethreitz
223,2012-11-27 02:12:24,"Can you provide the call @amalakar? This must be a mistake on my part and I'd like to fix it.
",sigmavirus24,amalakar
223,2012-11-27 02:16:24,"To clarify @amalakar, testing against HTTPBIN, with `requests.get('http://httpbin.org/get', params={'foo': 'bar'})` I get in the response Headers like:



which is a bit bizarre but probably a bug on HTTPBIN's end. (The fact that Content-Length is `u''` is what I find bizarre.)
",sigmavirus24,amalakar
179,2012-07-27 06:13:25,"@sigmavirus24 That'd be wonderful! Looking forward to it :)
",kennethreitz,sigmavirus24
179,2012-07-31 15:47:13,"@sigmavirus24, hmm, what operating system are you running? what version of python?

can you try `make simple`
",kennethreitz,sigmavirus24
179,2012-07-31 15:48:58,"Oh, that's interesting. I remember what you're talking about @sigmavirus24 , but it's not the exact same failure case. The intermittent error I was seeing was associated with httpbin.org's /redirect loop occasionally returning 503s. [Relevant url](https://github.com/kennethreitz/requests/pull/746#issuecomment-7308242).

EDIT: too slow twice in a row. I'll leave this here for posterity.
",Lukasa,sigmavirus24
179,2012-07-31 16:13:54,"Slackware 13.37 (x86_64) w/ python 2.6.6 but it was my error that the repository wasn't entirely up-to-date. I hope to finish `_encode_files` by the end of today to make sure it works properly. `_encode_data` already uses/accepts a list of k/v tuples and shouldn't really need to be changed unless I'm missing something.

Sorry to bother you @Lukasa.
",sigmavirus24,Lukasa
179,2012-08-02 14:29:02,"@kennethreitz Anything other than the following that need this functionality?

In `Session.request`:
- `headers` **done**
- `cookies`
- `proxies` **done**
- `config`
- `cert`
- `params` **done**
",sigmavirus24,kennethreitz
179,2012-08-02 21:14:47,"@sigmavirus24, `params`, `data` :)
",kennethreitz,sigmavirus24
179,2012-09-03 20:52:54,"""files"" could benefit from the magic treatment too. Happy to chip in test cases & fix on top of @sigmavirus24 work if it helps.
",CraigJPerry,sigmavirus24
179,2012-09-11 03:18:34,"@kennethreitz I'm pretty sure you can close this. I'm almost entirely certain that this is resolved as per the last PR of mine you merged (the one in reference to #817)
",sigmavirus24,kennethreitz
171,2013-12-19 13:13:51,"@ssbarnea This issue is more than two years old: it has nothing to do with the problem you're currently experiencing. =) Can you open a new issue to track your problem please?
",Lukasa,ssbarnea
171,2013-12-19 17:24:22,"@ssbarnea 
It most probably a buggy server doing wrong stuff when spoken to in specific SSL/TLS versions like described in #1567 (and several more).
Try to force a different version like it is explained [here](http://lukasa.co.uk/2013/01/Choosing_SSL_Version_In_Requests/) (some more kudos to @Lukasa)
",t-8ch,Lukasa
171,2013-12-19 17:24:22,"@ssbarnea 
It most probably a buggy server doing wrong stuff when spoken to in specific SSL/TLS versions like described in #1567 (and several more).
Try to force a different version like it is explained [here](http://lukasa.co.uk/2013/01/Choosing_SSL_Version_In_Requests/) (some more kudos to @Lukasa)
",t-8ch,ssbarnea
171,2013-12-19 17:46:32,"@ssbarnea You'll find that the limitation isn't in Requests but in Python. Currently no shipped versions of Python have support for any version of TLS more recent than V1.0 as you can see in the table [here](http://docs.python.org/3.3/library/ssl.html#ssl.wrap_socket).

Python 3.4 (not yet shipped, no guarantees that Requests is compatible with it) allows support for TLS1.2 and TLS1.1, so you can try with that. Otherwise, you'll have to allow TLSv1 to allow Requests through.
",Lukasa,ssbarnea
166,2011-09-17 17:00:41,"@mrtazz, can you send another one?
",kennethreitz,mrtazz
161,2011-11-15 23:34:59,"@kennethreitz I think this problem is still there, I'm getting a lot TooManyRedirects errors.

The fix by @jerem worked for me, but had to adapt it a little bit to the latest Requests version.
",michielgardner,kennethreitz
136,2011-08-31 05:18:55,"@densh, I've considered this before, but I like being able to 'import requests' directly from `test_requests.py` without having have requests installed into a site-packages w/ `setup.py develop`.

If there's an elegant way to accomplish both at the same time, I'm game.
",kennethreitz,densh
136,2011-08-31 07:42:33,"@kennethreitz: You shouldn't really assume PYTHONPATH=`pwd` anyway (since different flavors of python won't like running the same bytecode) - so if you want to create the subdir, either inject path to `ls -d build/lib-*/` depending on python version or more commonly let the user decide by setting PYTHONPATH. I guess one could say that the latter is already in play (disregarding moving tests) :-)
",jbergstroem,kennethreitz
115,2011-08-23 08:19:51,"@kennethreitz: It would have been good if you had written here that you've done the merge here: https://github.com/kennethreitz/requests/commit/8fbb1e6d97cda90d588d4263a18906a52d147fba#requests/utils.py
",jedie,kennethreitz
115,2011-08-23 13:28:42,"@monkeython excellent point. Let's just stick to headers. This is an HTTP library after all, not HTML :)
",kennethreitz,monkeython
114,2011-08-15 15:37:46,"@jedie: I can make it an OrderedDict, so the order won't be lost. 

Making the raw request headers available is interesting. Same with the response. 

**Random ideas:** 

Raw property on the headers would be useful: `r.headers.raw` and `r.request.headers.raw` perhaps.
",kennethreitz,jedie
96,2011-08-12 19:29:32,"Just ran into this bug today, thanks for the fix @jeremys
",kyleconroy,jeremys
91,2011-08-13 17:03:33,"@jerem: This is fixed now!
",kennethreitz,jerem
90,2011-08-06 13:12:01,"Thanks @moliware, this is what I did.
",janrito,moliware
90,2011-08-06 15:02:52,"@moliware: Excellent! I'm going to add this to the upcoming cookbook :)
",kennethreitz,moliware
79,2011-06-24 16:22:58,"I believe this is correct behaviour, and a good example of why using fragments in this way is a really bad idea.

> Fragment identifiers have a special role in information retrieval systems as the primary form of client-side indirect referencing, allowing an author to specifically identify aspects of an existing resource that are only indirectly provided by the resource owner.  As such, the fragment identifier is not used in the scheme-specific processing of a URI; instead, the fragment identifier is separated from the rest of the URI prior to a dereference, and thus the identifying information within the fragment itself is dereferenced solely by the user agent, regardless of the URI scheme. - [RFC 3986](http://tools.ietf.org/html/rfc3986)

**Edit**: Damn, @kennethreitz, you fast!
",jgorset,kennethreitz
71,2011-06-23 07:42:34,"It should also be noted that `data` is not an available keyword on the `get` function, which should be the primary method of making GET requests. 

@jgorset, are you using `request.request` directly? I view that as an internal-only function.
",kennethreitz,jgorset
71,2011-06-23 07:45:30,"@kennethreitz,

if `request` is an internal function, I don't think it should be defined in the `api` module. I'd argue that it's [very useful](https://github.com/jgorset/facepy/blob/master/facepy/graph_api.py#L106), though.
",jgorset,kennethreitz
71,2011-06-23 07:48:19,"@jgorset, can you hop on freenode #python-requests? 
",kennethreitz,jgorset
64,2013-04-23 07:17:33,"You should probably talk to @kennethreitz about whether he's prepared to dual-license Requests, either in general or in specific cases.
",Lukasa,kennethreitz
61,2011-06-17 06:57:10,"In all honesty, @kennethreitz, it is I who should've updated the docs when I committed that. ;-)
",jgorset,kennethreitz
61,2011-06-22 01:05:11,"@jgorset: feel free to :)
",kennethreitz,jgorset
47,2013-02-07 16:03:53,"@Lukasa I suspect we'll be able to repurpose at least the setup parts of the Zephyr test suite we're working on over the next couple of weekends, we'll definitely be in touch about that :)
",eichin,Lukasa
34,2011-05-20 17:00:13,"@jgorset already done :)

**EDIT:** You already know :P
",kennethreitz,jgorset
30,2012-01-24 12:44:51,"@kennethreitz that is to check server cert against a CA bundle or i'm missing something?

I've found traces of key/cert pair in packages, but not a mention of this in requests itself. Looks like there's some non-obvious way to actually pass client stuff to server as an auth token.
",wiz,kennethreitz
26,2011-04-22 07:47:21,"@jgorset thanks! I'll try to take a look at this and add some tests later today
",kennethreitz,jgorset
23,2011-05-16 07:23:23,"Nice work, @kennethreitz!
",jgorset,kennethreitz
23,2011-05-16 08:07:15,"Thanks @kennethreitz!
",esaurito,kennethreitz
3933,2017-03-21 20:19:21,"Thanks for this report! It's very detailed, which is a good start. I'd like to confirm some things:

1. You used 3.5.2 and 3.5.3, with libssl 1.0.1 and 1.1.0 respectively. You said, however, that ""upgrading libssl didn't fix the problem"". Did that mean just moving to the latest libssl in those releases, or moving 1.0.1 to 1.1.0 on 3.5.2.
2. Does bug 2 go away when you remove pyopenssl?
3. Does this reproduce with a simpler scenario? That is, if you change the response to, for example, be a simple 200 OK? What about if you change the SSL method to SSLv23_METHOD?
4. Does this reproduce on different distributions? Your docker image is Debian stretch: if you use, say, Fedora Rawhide, does that problem go away?

I'm not super well at the moment so I probably won't be looking at this in the next day or two, so if anyone else wants to dive in then feel free. Also, I'm going to CC some folks associated with PyOpenSSL: @hynek @alex @reaperhulk.",Lukasa,alex
3922,2017-03-14 15:40:47,"These CI failures aren't your fault. @kennethreitz, looks like our CI setup has busted again: want to take a look?",Lukasa,kennethreitz
3897,2017-03-01 17:33:33,"@davidsoncasey, glad we're able to get all of your work merged, thanks again!

@Lukasa, 3.0-HISTORY is updated :)",nateprewitt,Lukasa
3888,2017-02-24 17:40:34,"Hi there @atleta,

First, RFC 2616 has been made obsolete by RFCs [7230][], [7231][], [7232][], [7233][], [7234][], and [7235][].

As you'll see when you familiarize yourself with those RFCs, the *default* encoding for Headers is no longer ISO-8859-1 (a.k.a., Latin-1).

I suspect that if you have a specific encoding you want to do you should disallow redirects and take control over URL encoding yourself. @Lukasa may disagree, though.

[7230]: https://tools.ietf.org/html/rfc7230
[7231]: https://tools.ietf.org/html/rfc7231
[7232]: https://tools.ietf.org/html/rfc7232
[7233]: https://tools.ietf.org/html/rfc7233
[7234]: https://tools.ietf.org/html/rfc7234
[7235]: https://tools.ietf.org/html/rfc7235",sigmavirus24,Lukasa
3886,2017-02-23 22:53:28,"> The fact that this counts as regressing code review is stupid but there we are.

How do you mean, @Lukasa?

This looks generally fairly good, but I'm not sure I'm entirely clear. We're changing the ordering of the response history? Is there a good reason why? Has it been wrong all this time?",sigmavirus24,Lukasa
3885,2017-02-22 20:59:35,"@Lukasa I see a line like this already in the code base



as a top level definition. I've attempted to mock those, but my attempts so far have had some code smells. I've added a new commit that attempts to mock those in the least smelly way, but it's not ideal. Can you help me iron these out so they aren't so strange?

My approaches so far:
1. I set `current_time` in the class and mock it using `mocker.patch.object`. If I set it as a staticmethod or a classmethod, I'm not able to replace the function with a lambda w/o any args or with one anonymous arg. I'd rather not set it as an instance method of the class as that's a bit strange.
1. I use `mocker.patch` and set `current_time` at the global level of requests.structures. The only problem then is I have to import requests.structures (or just requests.structures.current_time), and pyflakes complains about an unused import.

Would appreciate any suggestions.",davidfontenot,Lukasa
3884,2017-02-22 08:58:01,"Thanks for this! I think this is absolutely worth doing but there's an element of style consideration here, so I'm going to let @kennethreitz make the call on this one.",Lukasa,kennethreitz
3874,2017-02-14 16:38:40,"Yep @sigmavirus24, `response` and `history` were added to 3.0.0 in commits by Kenneth.

Edit: 66eedec to be specific.",nateprewitt,sigmavirus24
3874,2017-02-21 18:13:30,"I think it's good to go @jvanasco, we were just waiting for confirmation from @kennethreitz. @Lukasa we can probably just merge though since this isn't actually a change, yeah?",nateprewitt,kennethreitz
3868,2017-02-11 21:17:14,"@Lukasa added a changelog entry to 3.0-HISTORY. Also the Travis build fails, but it doesn't appear like it's because of my change... not sure how to investigate...",vmalloc,Lukasa
3856,2017-02-08 11:14:52,"Oh boy, you are going to get @Lukasa screaming after you soon :) From this email https://lwn.net/Articles/643399/ it is intentional that requests take all sort of types for the ""param"" value. If something needs a fix, that probably would be Requests documentation.

Also, if you have not already, read #3855 ",JordanP,Lukasa
3851,2017-02-03 17:14:31,"@kennethreitz, it looks like the Pipfile was rebuilt yesterday with the latest version of `pipenv`. We'll need to remove the version we had pinned now that kennethreitz/pipenv#90 is resolved.",nateprewitt,kennethreitz
3835,2017-01-26 21:18:43,@Lukasa I believe there were issues with ONLY doing 401 as noted in #3772. The related RFCs allow for the WWW-Authenticate header in a non-401 response which the client SHOULD respond to if received. I think the conclusion we came to is to support 4XX response codes.,nateprewitt,Lukasa
3830,2017-01-24 23:13:07,"This fixes the issue that was encountered this morning during the 2.13.0 release.

The source specified in the original Pipfile was for an incorrect pypi endpoint. This didn't matter until pipenv 2.6 was released, specifically [cb22a12](https://github.com/kennethreitz/pipenv/commit/cb22a129eae8c8e800e603c38bf1fe04d420fbde), which started using the provided `source` value.

@kennethreitz I used `pipenv lock` with pipenv 3.0.0 to generate the new lock file which I'm assuming is what we want here.",nateprewitt,kennethreitz
3827,2017-01-24 12:44:51,For now we'll just pin it so the builds keep working: @kennethreitz should feel free to remove the pin and investigate when he has some time.,Lukasa,kennethreitz
3817,2017-01-13 20:55:32,"Ok, @alex and @reaperhulk really need to sign off on this before I consider merging it. ;)",Lukasa,alex
3807,2017-01-11 01:12:05,"Hey @gilessbrown, thanks for opening this issue! This behaviour was exposed by a change made in (327512f) which removed exception handling for this case. It currently only exists in the 2.12.x releases, so using Requests 2.11.1 should work for you.

While you're seeing this behaviour in Requests, it actually seems to be how we're handling chunked responses without a body in urllib3. shazow/urllib3#990 introduced an attempt to catch this problem but the check is just slightly off. While it verifies the existence to `fp`, it doesn't check that `fp` isn't `None`.

I think the simple fix here is to change [the check](https://github.com/shazow/urllib3/blob/master/urllib3/response.py#L525) in urllib3 to `return getattr(self._fp, 'fp', None) is not None` which should give us what we actually want. It verifies that `fp` both exists, and is not the default `None` value.

If @Lukasa or @sigmavirus24 are in agreement, we can address this over in urllib3.",nateprewitt,Lukasa
3807,2017-01-11 01:12:05,"Hey @gilessbrown, thanks for opening this issue! This behaviour was exposed by a change made in (327512f) which removed exception handling for this case. It currently only exists in the 2.12.x releases, so using Requests 2.11.1 should work for you.

While you're seeing this behaviour in Requests, it actually seems to be how we're handling chunked responses without a body in urllib3. shazow/urllib3#990 introduced an attempt to catch this problem but the check is just slightly off. While it verifies the existence to `fp`, it doesn't check that `fp` isn't `None`.

I think the simple fix here is to change [the check](https://github.com/shazow/urllib3/blob/master/urllib3/response.py#L525) in urllib3 to `return getattr(self._fp, 'fp', None) is not None` which should give us what we actually want. It verifies that `fp` both exists, and is not the default `None` value.

If @Lukasa or @sigmavirus24 are in agreement, we can address this over in urllib3.",nateprewitt,sigmavirus24
3789,2016-12-23 17:33:05,@Lukasa Please let me know your thought on this. I think it should address your concern in my previous Pull Request.,moin18,Lukasa
3789,2016-12-26 18:09:14,"@sigmavirus24  Not sure why it is showing the status as ""Changes requested"" even though they are fixed in commit: https://github.com/kennethreitz/requests/pull/3789/commits/965eb3d1525e3d384623c0267654c2f07acc4aea

May be it due to additional import of `sys` (needed but was missing in previous PR)",moin18,sigmavirus24
3789,2017-01-10 20:36:31,@sigmavirus24  I have fixed the comments you mentioned. Please can you review them once more? :),moin18,sigmavirus24
3789,2017-01-10 20:55:01,"@moin18 would you be willing to rebase this rather than merging master into this branch? Alternatively, @Lukasa how do you feel about squash merging this?",sigmavirus24,Lukasa
3770,2016-12-15 02:06:09,"So we are not doing client auth here.

And the openssl client fails for me because there's no local certificate in my bundle for the issuer of that revoked certificate:



> tl;dr it seems that revoked ssl certs aren't being rejected, which could be an issue

This does seem, at this point, true. That said, I believe this is because requests (and openssl) don't by default turn on OCSP which is what we'd need to use to *detect* a revoked certificate.

There are lots of opinions about OCSP and certificate revocation, but one prevailing one is that it doesn't work very well (which isn't an excuse to not support it). Either way, I believe to support it, we need some work in the standard library ssl library and in cryptography to allow us to add that support to PyOpenSSL.

I'm not 100% confident in any of this, though, so I'd advise we wait until @Lukasa is up or someone like @reaperhulk can take a look.

---

Thanks for reporting this @mendaxi ",sigmavirus24,Lukasa
3761,2016-12-09 15:25:22,"@Lukasa I should have learned by now that when I feel the need for emojis, my changes are overzealous. Ready for another peek :)",nateprewitt,Lukasa
3760,2016-12-09 14:44:18,r4r @Lukasa @nateprewitt ,sigmavirus24,Lukasa
3760,2016-12-09 14:44:18,r4r @Lukasa @nateprewitt ,sigmavirus24,nateprewitt
3760,2016-12-09 14:53:29,"@Lukasa when you get another second to merge master into Proposed/3.0.0, I'll throw up the other patch.",nateprewitt,Lukasa
3758,2016-12-09 14:28:28,"@sigmavirus24 you're suggesting using `warnings.warn` to actively notify the user about the deprecation, correct? I can add that and the test here in a moment if you're not already working on it.",nateprewitt,sigmavirus24
3757,2016-12-12 14:26:32,"@Lukasa, I think this is ready for a peek whenever you've got a moment :)",nateprewitt,Lukasa
3753,2016-12-08 04:00:47,"I've got a feeling this may fall under Requests' policy of ""All header values must be a string, bytestring, or unicode."", since the Authorization field generated is a header, and the `auth` keyword is even discussed in the [Custom Headers](http://docs.python-requests.org/en/master/user/quickstart/#custom-headers) docs.

@Lukasa may have different feelings on this, but trying to permit non-string values here seems to have the same ambiguity issues we've tried to avoid elsewhere. Just like how `None` hasn't been permitted because `None` is cast to a string, when the user much more likely wants `""""`, which will generate different auth values.",nateprewitt,Lukasa
3752,2016-12-07 20:52:07,"This sounds like this is a bug in the standard library, actually: it seems like they introduced an infinite recursion when setting options directly. @tiran?",Lukasa,tiran
3745,2016-12-02 19:31:54,"This is really quite unexciting: #2431 had a tentative +1 from @Lukasa, pending a rebase and retargeting against 3.0.0, but it’s been sat unloved for nearly eight months. 😢 

This patch does the required rebase and targets the 3.0.0 branch.",alexwlchan,Lukasa
3739,2016-11-30 23:52:35,"Hey @obestwalter, I talked to @Lukasa about this a few weeks ago. To quote a bit of his response:

>Requests moved away from Travis because it made a lot of live network requests in its test suite and Travis had frequent network outages that caused real problems with the test run. 

While a lot of these issues have been resolved, I think the general sentiment is Travis generates a fair amount of noise for the repo owner and has historically had quirks. Until recently, Requests ran CI on a Jenkins server, but that's since been removed.

As for tox, I think most of Requests' contributors are running a local tox.ini file for testing changes. If you run that in conjunction with something like [pyenv](https://github.com/yyuu/pyenv), you have an easy local equivalent of a Travis environment which has worked for the most part the last several months.

Note this may no longer be the state of things, I just wanted save Lukasa from having to reiterate what he's conveyed recently via other channels.

Here's the last couple of Kenneth's comments on the state of Requests' CI/Travis in 2016 too. [[Feb](https://github.com/kennethreitz/requests/pull/2991#issuecomment-178813436)] [[Apr](https://github.com/kennethreitz/requests/pull/3096#issuecomment-211701179)]",nateprewitt,Lukasa
3739,2016-12-01 08:18:37,@obestwalter All of these decisions are fundamentally @kennethreitz's: the other maintainers have no objection to re-adding Travis support and a toxfile.,Lukasa,kennethreitz
3738,2016-11-30 21:22:10,"This reverts commit 34af72c87d79bd8852e8564c050dd7711c6a08d6.

This commit was added by @tiran to try to avoid the IDNA-encoding logic that we added in v2.12.0. I believe that the *other* workarounds we merged for that are sufficient, and this change broke docker-py and probably broke others.

@tiran, can you confirm that your code continues to function with this change reverted? I'd like to be able to merge this and ship a v2.12.3 if at all possible.

Resolves #3735.",Lukasa,tiran
3738,2016-12-01 08:04:03,"@mshahpalerra As to the ETA of v2.12.3, it will be as soon as I can get confirmation from @tiran that the problem he was originally trying to fix is still fixed in this branch. If it isn't, then it will be as soon as I can work with him to build a new patch that avoids this specific issue.",Lukasa,tiran
3735,2016-12-01 08:22:34,"In the short term, I think we need to back out the breakage: kicking things over with @graingert convinced me that ultimately, whether we ever intended docker-py's usage of Requests to actually work, it *has* worked for more than three years. If we're going to break that, we should break it on purpose, rather than by accident.

So this issue remains worth discussing because we should solidify our position on this kind of thing for 3.0.0. Either we'll want to add explicit support for having HTTP-like schemes, or we'll want to add support for custom schemes to register URL processing handlers, or we'll want to drop support entirely as we (accidentally) did in v2.12.2. We should discuss what of those we think we want to do, and make sure there is a transition plan in place for projects like docker-py.",Lukasa,graingert
3734,2016-11-30 15:10:21,"Reported by @graingert.

The patch filed by @tiran in #3713 seems to have broken docker-py. They're using a custom URL scheme (`http+docker`), which we previously applied URL preparation to but now do not. This has therefore busted what they were up to in v2.12.2.

I'm not immediately sure that we have a good way out of this. Prior to this patch docker-py users were *probably* at risk of encountering issues because the HTTP requests to the Docker API can be routed across unix domain sockets, which may entirely fail to contain hostnames and also fail to IDNA-encode. So I'm not sure that we don't need docker-py to route around this a different way.",Lukasa,tiran
3734,2016-11-30 15:10:21,"Reported by @graingert.

The patch filed by @tiran in #3713 seems to have broken docker-py. They're using a custom URL scheme (`http+docker`), which we previously applied URL preparation to but now do not. This has therefore busted what they were up to in v2.12.2.

I'm not immediately sure that we have a good way out of this. Prior to this patch docker-py users were *probably* at risk of encountering issues because the HTTP requests to the Docker API can be routed across unix domain sockets, which may entirely fail to contain hostnames and also fail to IDNA-encode. So I'm not sure that we don't need docker-py to route around this a different way.",Lukasa,graingert
3734,2016-11-30 15:17:26,"What is unclear to me is exactly *why* this skip is happening. @graingert, if you can run your example under pdb can you set a breakpoint at `requests.sessions.Session.request` and run `print url` when it breaks?",Lukasa,graingert
3717,2016-11-23 03:48:05,"Minor example update and rewording in response to the continued discussion after merging #3704. @afeld, if you're inclined to comment, does this change still address your original concerns?",nateprewitt,afeld
3716,2016-11-22 20:13:55,"@PatriotRDX so this is a bit harder because this is a reflection of the README.rst at the time of release. `pip install requests` is the correct command for installing requests on your machine. If you require an older version, the standard usage of pip has you specify a specific version number (`requests==2.2.1`) or a range (`requests>=2.6.0`). You can find more documentation on pip [here](https://pip.pypa.io/en/stable/).

This should be specified by whichever package you're trying to use in either the requirements.txt file, or setup.py. If it isn't, that should probably be brought up with the package maintainer.

@Lukasa or Kenneth may have different opinions on retroactively updating older release docs, but I'm not sure it's likely.",nateprewitt,Lukasa
3695,2016-11-17 15:37:08,"> An alternative solution to this is to say that if the idna module fails to encode we'll just go ahead and try to encode as ASCII. If that works then we shrug our shoulders and say everything is probably ok, and if it fails we catch that and throw InvalidURL. @sigmavirus24, how does that sound?

([comment](/kennethreitz/requests/issues/3683#issuecomment-261240279))

Do we want to hold here until a decision is made on this so we're not having to back things out?
",nateprewitt,sigmavirus24
3686,2016-11-15 18:59:25,"Thanks for opening this issue, @afaicode. This is currently being tracked in #3683 which will be the official issue for further discussion. I believe @Lukasa has stated underscores in host names are not behaviour we likely want to support, but you're welcome to raise objections in the appropriate issue.
",nateprewitt,Lukasa
3685,2016-11-15 18:42:52,"@jaraco I believe what you're seeing is shazow/urllib3#1025. It has been patched on master for urllib3 but was caught after 1.19 released. This may be enough of an issue to warrant bumping urllib3 to 1.19.1 and adding it to a 2.12.1 release? Until then, you'll likely need to use 2.11.1 unless @sigmavirus24 has a solution via requests-toolbelt.
",nateprewitt,sigmavirus24
3669,2016-11-10 15:33:34,"This is a continuation of the original PR (#3595) to address how we set Session cookies in Requests. The PR had enough disagreement on how this should be addressed that I felt it was better to move this into a discussion.

As noted in #3595, Requests currently allows the user to provide cookies as a dictionary. While we've allowed this for individual requests, because their domain is (mostly) scoped, there are some real security concerns for supporting this for Sessions.

The issues
=========

1. If the user sets Session's `cookies` attribute to a `dict`, it will happily accept it, but crashes when you try to send anything. To correct this issue, the cookies need to be supplied as a `CookieJar` instance as noted in the [documentation](http://docs.python-requests.org/en/master/user/advanced/#session-objects). This is unideal because we allow users to use an idiom supported everywhere else in the API, and then fail later with a fairly unhelpful exception.

2. #3595 automated this suggested process from the documentation, as we do elsewhere in Requests, but it allows you to easily set session-wide cookies that aren't bounded by a domain. This means it's easy to unintentionally send sensitive cookie information to unintended recipients.

3. The currently proposed solution of using `add_dict_to_cookiejar` or `cookiejar_from_dict` provides no extra protection against the security issues raised in #3595.

Solutions(?)
==========

1. I think this is a two step process. For Requests 2.x, I'm suggesting we add a warning to the user when they set Session `cookies` attribute to a `dict`. This points them at the documentation so they can at least *try* to do the right things. Otherwise, they're none the wiser until their program crashes with an error in a separate module and only minor clues on the cause. In 3.0.0, I think this warning should be raised an actual exception since we don't support cookies as dicts for Sessions.

2. (and also 3.) If we're going to require the extra step of making the user explicitly use `add_dict_to_cookiejar` or `cookie_from_dict`, then let's make them useful. They currently don't provide any benefit over the dictionary approach in #3595. We can emulate some of the functionality of how a browser handles cookies by allowing users to supply cookie parameters on a per-dictionary basis. This is similar to cookies set by individual page requests.



This is a relatively simple changed as shown in an initial mockup in e2a4f9f & 4454849. This would need to be accompanied by a documentation update (something like 3e4e5b7) to explain how to do this properly.


----
@sigmavirus24, when you've got a moment, could you confirm this issue properly encapsulates your concerns and I didn't leave anything out? As for the possible solutions, are any of these amenable? I know your work in #2714 will help this a little bit with the default cookie policy change, but we still need to provide a way for the user to easily adhere to the safeguards we're putting in place.",nateprewitt,sigmavirus24
3655,2016-10-31 19:15:34,"Alright @Lukasa, I think everything in that last round should be addressed. A couple of notes on some decisions I made in these tweaks.

Python 2.7 decided it needs to return a long from `tell` instead of an int like every other version. I've adjusted for that by casting it to int because we can't test for long in Py3.

I didn't declare the ""sentinel object"" as a global because I couldn't come up with a scenario where it was equivocal to anything in the sessions module. Simply setting it to object accomplishes what we were looking for but perhaps there's a caveat I'm missing here?

~~**Tests.** I went a little crazy. I have a few ""lifecycle"" tests, but found a snag when adding `test_rewind_body_failed_tell`. I realized things were failing in different ways (`_body_position` was never being set with the custom test objects) but the error messages produced were still ""correct"". In order to prevent accidental regressions, I add specific tests for setting `_body_position` and failure conditions in `rewind_body` by manually setting `_body_position`. Let me know if you want these collapsed back.~~

Edit: I went ahead and expanded the test objects to simplify the testing.
",nateprewitt,Lukasa
3655,2016-11-01 14:06:07,"Here's another round of changes @Lukasa.

I moved the `rewind_body` to a util function but when I started looking at moving tests over to `test_utils` they're going to bring a lot of import overhead with them. I can chop them up into individual `_body_position` and `rewind_body` tests and divide them up accordingly. Otherwise, we can leave them where they are.
",nateprewitt,Lukasa
3651,2016-10-27 01:58:38,"when the http response like this:
`HTTP/1.1 401 Unauthorized\r\n
Server: DNVRS-Webs\r\n
Content-Type: text/html\r\n
WWW-Authenticate: Basic realm=""DVRNVRDVS""\r\n
WWW-Authenticate: Digest realm=""DVRNVRDVS"", domain=""::"", qop=""auth"", nonce=""2c4158c9159934737e69aa698494ae04"", opaque="""", algorithm=""MD5"", stale=""FALSE""\r\n`
I have print the response headers ,`{'Server': 'DNVRS-Webs', 'Content-Type': 'text/html', 'WWW-Authenticate': 'Basic realm=""DVRNVRDVS"", Digest realm=""DVRNVRDVS"", domain=""::"", qop=""auth"", nonce=""2c4158c9159934737e69aa698494ae04"", opaque="""", algorithm=""MD5"", stale=""FALSE""'}`,is this a bug? @nakajima @alex @mkomitee @thedaniel 
",beruhan,alex
3651,2016-10-27 01:58:38,"when the http response like this:
`HTTP/1.1 401 Unauthorized\r\n
Server: DNVRS-Webs\r\n
Content-Type: text/html\r\n
WWW-Authenticate: Basic realm=""DVRNVRDVS""\r\n
WWW-Authenticate: Digest realm=""DVRNVRDVS"", domain=""::"", qop=""auth"", nonce=""2c4158c9159934737e69aa698494ae04"", opaque="""", algorithm=""MD5"", stale=""FALSE""\r\n`
I have print the response headers ,`{'Server': 'DNVRS-Webs', 'Content-Type': 'text/html', 'WWW-Authenticate': 'Basic realm=""DVRNVRDVS"", Digest realm=""DVRNVRDVS"", domain=""::"", qop=""auth"", nonce=""2c4158c9159934737e69aa698494ae04"", opaque="""", algorithm=""MD5"", stale=""FALSE""'}`,is this a bug? @nakajima @alex @mkomitee @thedaniel 
",beruhan,mkomitee
3651,2016-10-27 01:58:38,"when the http response like this:
`HTTP/1.1 401 Unauthorized\r\n
Server: DNVRS-Webs\r\n
Content-Type: text/html\r\n
WWW-Authenticate: Basic realm=""DVRNVRDVS""\r\n
WWW-Authenticate: Digest realm=""DVRNVRDVS"", domain=""::"", qop=""auth"", nonce=""2c4158c9159934737e69aa698494ae04"", opaque="""", algorithm=""MD5"", stale=""FALSE""\r\n`
I have print the response headers ,`{'Server': 'DNVRS-Webs', 'Content-Type': 'text/html', 'WWW-Authenticate': 'Basic realm=""DVRNVRDVS"", Digest realm=""DVRNVRDVS"", domain=""::"", qop=""auth"", nonce=""2c4158c9159934737e69aa698494ae04"", opaque="""", algorithm=""MD5"", stale=""FALSE""'}`,is this a bug? @nakajima @alex @mkomitee @thedaniel 
",beruhan,nakajima
3651,2016-10-27 01:58:38,"when the http response like this:
`HTTP/1.1 401 Unauthorized\r\n
Server: DNVRS-Webs\r\n
Content-Type: text/html\r\n
WWW-Authenticate: Basic realm=""DVRNVRDVS""\r\n
WWW-Authenticate: Digest realm=""DVRNVRDVS"", domain=""::"", qop=""auth"", nonce=""2c4158c9159934737e69aa698494ae04"", opaque="""", algorithm=""MD5"", stale=""FALSE""\r\n`
I have print the response headers ,`{'Server': 'DNVRS-Webs', 'Content-Type': 'text/html', 'WWW-Authenticate': 'Basic realm=""DVRNVRDVS"", Digest realm=""DVRNVRDVS"", domain=""::"", qop=""auth"", nonce=""2c4158c9159934737e69aa698494ae04"", opaque="""", algorithm=""MD5"", stale=""FALSE""'}`,is this a bug? @nakajima @alex @mkomitee @thedaniel 
",beruhan,thedaniel
3634,2016-10-24 14:29:27,"@Lukasa I've adjusted the test.
",jeremycline,Lukasa
3634,2016-10-25 00:40:36,"@Lukasa no worries. I haven't really worked with pytest and didn't see examples of patching in the existing tests so I got lazy :frowning_face:.
",jeremycline,Lukasa
3627,2016-10-18 17:18:25,"This is a minor fix in the same vein as #3591. This function calls `update` on the CookieJar which only exists on `RequestsCookieJar` not the standard library `cookielib.CookieJar`. While this function will now be somewhat trivial, this will ensure it maintains backwards compatibility. It may be worth discussing removal in the future.

I have a test as well @Lukasa but it seems a bit overkill. I can include it if you'd like though.
",nateprewitt,Lukasa
3627,2016-10-27 18:42:01,"Hey @Lukasa, just pinging on this when you've got a moment. This should be the last problematic use of `update` on CookieJars in the codebase.
",nateprewitt,Lukasa
3625,2016-10-21 07:20:34,"Ok, rather than block on @sigmavirus24 having time to swing back to this, I'll merge this now. @sigmavirus24 can provide feedback whenever he gets time back. Thanks for the work @mie00!
",Lukasa,sigmavirus24
3616,2016-10-12 16:21:01,"As has been noted on the [Python Security SIG mailing list](https://mail.python.org/pipermail/security-sig/2016-October/000122.html), Python currently only includes the deprecated IDNA 2003 codec in the standard library. This is problematic, because IDNA 2003 is forbidden for some ccTLDs, such as `.de`: instead, the newer IDNA 2008 standard should be used. That standard is implemented in the PyPI package `idna`.

If we're going to support IDNA 2008, I would like it to not be optional: it leads to a fairly substantial and difficult to debug change, so it should be supported fully. To do that and keep in touch with Kenneth's wishes regarding dependencies, we'd have to vendor it.

I'd like contributors and packagers, particularly @eriol, @ralphbean, and @sigmavirus24 to weigh in with their thoughts here. What are your thoughts here? I'd also, if possible, like to hear from @kjd: while `idna` is licensed under a BSD-like license that should be broadly compatible with our own Apache 2.0 and so should present no legal blockers to vendoring, I'd still rather do it with the original author's permission than without it.
",Lukasa,ralphbean
3616,2016-10-12 16:21:01,"As has been noted on the [Python Security SIG mailing list](https://mail.python.org/pipermail/security-sig/2016-October/000122.html), Python currently only includes the deprecated IDNA 2003 codec in the standard library. This is problematic, because IDNA 2003 is forbidden for some ccTLDs, such as `.de`: instead, the newer IDNA 2008 standard should be used. That standard is implemented in the PyPI package `idna`.

If we're going to support IDNA 2008, I would like it to not be optional: it leads to a fairly substantial and difficult to debug change, so it should be supported fully. To do that and keep in touch with Kenneth's wishes regarding dependencies, we'd have to vendor it.

I'd like contributors and packagers, particularly @eriol, @ralphbean, and @sigmavirus24 to weigh in with their thoughts here. What are your thoughts here? I'd also, if possible, like to hear from @kjd: while `idna` is licensed under a BSD-like license that should be broadly compatible with our own Apache 2.0 and so should present no legal blockers to vendoring, I'd still rather do it with the original author's permission than without it.
",Lukasa,eriol
3616,2016-10-12 16:21:01,"As has been noted on the [Python Security SIG mailing list](https://mail.python.org/pipermail/security-sig/2016-October/000122.html), Python currently only includes the deprecated IDNA 2003 codec in the standard library. This is problematic, because IDNA 2003 is forbidden for some ccTLDs, such as `.de`: instead, the newer IDNA 2008 standard should be used. That standard is implemented in the PyPI package `idna`.

If we're going to support IDNA 2008, I would like it to not be optional: it leads to a fairly substantial and difficult to debug change, so it should be supported fully. To do that and keep in touch with Kenneth's wishes regarding dependencies, we'd have to vendor it.

I'd like contributors and packagers, particularly @eriol, @ralphbean, and @sigmavirus24 to weigh in with their thoughts here. What are your thoughts here? I'd also, if possible, like to hear from @kjd: while `idna` is licensed under a BSD-like license that should be broadly compatible with our own Apache 2.0 and so should present no legal blockers to vendoring, I'd still rather do it with the original author's permission than without it.
",Lukasa,sigmavirus24
3616,2016-10-12 21:47:35,"Hey, @ralphbean has asked me to take over being the point of contact for Fedora packaging interactions.

I'm fine with this and I'm also fine with it being non-optional.
",jeremycline,ralphbean
3595,2016-09-23 15:51:56,"Right now you can provide a dictionary to the `cookies` param almost everywhere in Requests and it will be converted into a RequestsCookieJar (aka ""just work""). The one place this isn't happening is with Session, which will accept a dictionary without complaint but then fail when you try to send a PreparedRequest. 

I discussed wanting to make this part of the API a bit more predictable with @Lukasa and this was a suggested fix for the problem. I did fair amount of testing outside of the included test and I don't believe this will adversely affect functionality. The one case I found is where someone is providing a CookieJar-like object that doesn't inherit from `cookielib.CookieJar`, and doesn't have an `__iter__` method. This seems like an unlikely case and wasn't _really_ supported before, so I wouldn't consider this breaking.
",nateprewitt,Lukasa
3595,2016-09-30 13:47:26,"@sigmavirus24, if we're planning on enforcing cookies requiring domains with your work in #2714, then I'd agree this PR may not be needed. If we don't intend to deprecate the use of dict->cookies though, I do think this should work the same everywhere. The documentation is already a bit opaque on this, and my original suggestion to @Lukasa was to update it to reflect how updating cookies currently works with a Session. If we want to make a note of this change in the documentation, I think simply adding ""As of 2.12"" at the beginning should be sufficient.

I can make the changes for `self.cookies` to `self._cookies` if there's a consensus to move forward with this. I'm not sure I'm clear on the answer to that at present though. I'll wait for a go ahead from you and @Lukasa before I make any further changes.
",nateprewitt,Lukasa
3595,2016-09-30 13:47:26,"@sigmavirus24, if we're planning on enforcing cookies requiring domains with your work in #2714, then I'd agree this PR may not be needed. If we don't intend to deprecate the use of dict->cookies though, I do think this should work the same everywhere. The documentation is already a bit opaque on this, and my original suggestion to @Lukasa was to update it to reflect how updating cookies currently works with a Session. If we want to make a note of this change in the documentation, I think simply adding ""As of 2.12"" at the beginning should be sufficient.

I can make the changes for `self.cookies` to `self._cookies` if there's a consensus to move forward with this. I'm not sure I'm clear on the answer to that at present though. I'll wait for a go ahead from you and @Lukasa before I make any further changes.
",nateprewitt,sigmavirus24
3595,2016-10-17 06:34:43,"@sigmavirus24 @Lukasa, if we don't want to address this issue by allowing dictionaries, perhaps we should at least raise an exception when someone tries to assign one? I'm not particularly keen on this approach, but the current point of failure doesn't generate a helpful error.
",nateprewitt,Lukasa
3595,2016-10-17 06:34:43,"@sigmavirus24 @Lukasa, if we don't want to address this issue by allowing dictionaries, perhaps we should at least raise an exception when someone tries to assign one? I'm not particularly keen on this approach, but the current point of failure doesn't generate a helpful error.
",nateprewitt,sigmavirus24
3595,2016-10-17 07:22:11,"@nateprewitt I think @sigmavirus24 was tentatively in favour of this change, at least as a temporary stopping-off point before something better. He was just asking for a change in the descriptor protocol.
",Lukasa,sigmavirus24
3594,2016-09-20 16:24:26,"Merging change from #3576 into the proposed/3.0.0 branch as per @sigmavirus24's request.
",nateprewitt,sigmavirus24
3576,2016-09-17 09:04:26,"@sigmavirus24 Would you like to take a look at this?
",Lukasa,sigmavirus24
3563,2016-09-10 17:25:14,"@Lukasa, here's a bit of a hiccup I'd appreciate some input on. Currently all data reads in Requests end up using `itercontent`. 

Currently, regardless of whether or not the content is streamed, we end up initially running it through the `generate` function inside `itercontent`. This function assumes we're using Transfer-Encoding: chunked if it's called, and raises `ChunkedEncodingError` wrapping urllib3's `ProtocolError`. This is clearly incorrect because we're not chunked for responses with Content-Length but I'm not sure how best to catch this. 

We could parse the exception string for 'IncompleteRead', but that seems clunky to me. We could also add a conditional inside to raise `ProtocolError` on `stream=False` and `ChunkedEncodingError` on `stream=True` but that may be subtle enough to confuse since it's the same base error. The last option I can think of is removing `ChunkedEncodingError` completely and just raising the unmodified `ProtocolError` instead. Thoughts?
",nateprewitt,Lukasa
3554,2016-09-05 18:51:18,"This implements the proposed solution to #3538 by falling back from Unicode to ISO-8859-1 for raw reason decoding.

This is a relatively trivial fix, and I wasn't sure if you wanted to waste bandwidth fixing, @mitsuhiko. I had a brief chat with @Lukasa who said to toss this up, but I'll gladly drop it in favor of #3538, if you had plans to update.
",nateprewitt,mitsuhiko
3554,2016-09-05 18:51:18,"This implements the proposed solution to #3538 by falling back from Unicode to ISO-8859-1 for raw reason decoding.

This is a relatively trivial fix, and I wasn't sure if you wanted to waste bandwidth fixing, @mitsuhiko. I had a brief chat with @Lukasa who said to toss this up, but I'll gladly drop it in favor of #3538, if you had plans to update.
",nateprewitt,Lukasa
3554,2016-09-05 20:19:27,"@sigmavirus24 are you happy with this?
",Lukasa,sigmavirus24
3552,2016-09-05 16:53:35,"Glad I could help and thanks @Lukasa 
",Trodis,Lukasa
3548,2016-09-01 09:16:31,"@shazow @Lukasa @mcescalante @borbamartin

Hi guys;

I just tried to extract data by using the URL of the webservices; moreover, I used the requests package however it returned the error ""TypeError: **str** returned non-string (type SysCallError)"" according to that; I just read the section for the issue at GitHub and upgraded the requests package. However, it returned another error. Before updating the code worked for an hour and stopped and gave the error as I shared ""TypeError: **str** returned non-string (type SysCallError)"" however right now it does not work; moreover jit just returns the traceback:

Traceback (most recent call last):
  File ""C:\Users\may\Desktop\Documents\BOUN-CSE\Master Thesis\Project Code Files\pubchem\PubchemMapping.py"", line 35, in <module>
    activities = new_client.activity.filter(target_chembl_id__in=[target['target_chembl_id'] for target in targets])
  File ""C:\Python27\lib\site-packages\chembl_webresource_client\query_set.py"", line 114, in next
    self.chunk = self.query.get_page()
  File ""C:\Python27\lib\site-packages\chembl_webresource_client\url_query.py"", line 390, in get_page
    res = session.post(self.base_url + '.' + self.frmt, data=data, timeout=self.timeout)
  File ""C:\Python27\lib\site-packages\requests\sessions.py"", line 522, in post
    return self.request('POST', url, data=data, json=json, *_kwargs)
  File ""C:\Python27\lib\site-packages\requests_cache\core.py"", line 126, in request
    *_kwargs
  File ""C:\Python27\lib\site-packages\requests\sessions.py"", line 475, in request
    resp = self.send(prep, **send_kwargs)
  File ""C:\Python27\lib\site-packages\requests_cache\core.py"", line 97, in send
    response, timestamp = self.cache.get_response_and_time(cache_key)
  File ""C:\Python27\lib\site-packages\requests_cache\backends\base.py"", line 70, in get_response_and_time
    if key not in self.responses:
  File ""C:\Python27\lib_abcoll.py"", line 388, in **contains**
    self[key]
  File ""C:\Python27\lib\site-packages\requests_cache\backends\storage\dbdict.py"", line 163, in **getitem**
    return pickle.loads(bytes(super(DbPickleDict, self).**getitem**(key)))
  File ""C:\Python27\lib\copy_reg.py"", line 50, in _reconstructor
    obj = base.__new__(cls, state)
TypeError: ('dict.**new**(HTTPHeaderDict): HTTPHeaderDict is not a subtype of dict', <function _reconstructor at 0x03039570>, (<class 'requests.packages.urllib3._collections.HTTPHeaderDict'>, <type 'dict'>, {'date': ('date', 'Thu, 01 Sep 2016 08:24:03 GMT'), 'transfer-encoding': ('transfer-encoding', 'chunked'), 'vary': ('vary', 'Accept'), 'server': ('server', 'gunicorn/19.1.1'), 'x-chembl-retrieval-time': ('x-chembl-retrieval-time', '0.00829792022705'), 'connection': ('connection', 'Keep-Alive'), 'cache-control': ('cache-control', 's-maxage=30000000, max-age=30000000'), 'x-chembl-in-cache': ('x-chembl-in-cache', 'True'), 'content-type': ('content-type', 'application/json')}))

Can you help to solve the problem please ?

All the Best;
Aziz
",MehmetAzizYirik,shazow
3548,2016-09-01 09:16:31,"@shazow @Lukasa @mcescalante @borbamartin

Hi guys;

I just tried to extract data by using the URL of the webservices; moreover, I used the requests package however it returned the error ""TypeError: **str** returned non-string (type SysCallError)"" according to that; I just read the section for the issue at GitHub and upgraded the requests package. However, it returned another error. Before updating the code worked for an hour and stopped and gave the error as I shared ""TypeError: **str** returned non-string (type SysCallError)"" however right now it does not work; moreover jit just returns the traceback:

Traceback (most recent call last):
  File ""C:\Users\may\Desktop\Documents\BOUN-CSE\Master Thesis\Project Code Files\pubchem\PubchemMapping.py"", line 35, in <module>
    activities = new_client.activity.filter(target_chembl_id__in=[target['target_chembl_id'] for target in targets])
  File ""C:\Python27\lib\site-packages\chembl_webresource_client\query_set.py"", line 114, in next
    self.chunk = self.query.get_page()
  File ""C:\Python27\lib\site-packages\chembl_webresource_client\url_query.py"", line 390, in get_page
    res = session.post(self.base_url + '.' + self.frmt, data=data, timeout=self.timeout)
  File ""C:\Python27\lib\site-packages\requests\sessions.py"", line 522, in post
    return self.request('POST', url, data=data, json=json, *_kwargs)
  File ""C:\Python27\lib\site-packages\requests_cache\core.py"", line 126, in request
    *_kwargs
  File ""C:\Python27\lib\site-packages\requests\sessions.py"", line 475, in request
    resp = self.send(prep, **send_kwargs)
  File ""C:\Python27\lib\site-packages\requests_cache\core.py"", line 97, in send
    response, timestamp = self.cache.get_response_and_time(cache_key)
  File ""C:\Python27\lib\site-packages\requests_cache\backends\base.py"", line 70, in get_response_and_time
    if key not in self.responses:
  File ""C:\Python27\lib_abcoll.py"", line 388, in **contains**
    self[key]
  File ""C:\Python27\lib\site-packages\requests_cache\backends\storage\dbdict.py"", line 163, in **getitem**
    return pickle.loads(bytes(super(DbPickleDict, self).**getitem**(key)))
  File ""C:\Python27\lib\copy_reg.py"", line 50, in _reconstructor
    obj = base.__new__(cls, state)
TypeError: ('dict.**new**(HTTPHeaderDict): HTTPHeaderDict is not a subtype of dict', <function _reconstructor at 0x03039570>, (<class 'requests.packages.urllib3._collections.HTTPHeaderDict'>, <type 'dict'>, {'date': ('date', 'Thu, 01 Sep 2016 08:24:03 GMT'), 'transfer-encoding': ('transfer-encoding', 'chunked'), 'vary': ('vary', 'Accept'), 'server': ('server', 'gunicorn/19.1.1'), 'x-chembl-retrieval-time': ('x-chembl-retrieval-time', '0.00829792022705'), 'connection': ('connection', 'Keep-Alive'), 'cache-control': ('cache-control', 's-maxage=30000000, max-age=30000000'), 'x-chembl-in-cache': ('x-chembl-in-cache', 'True'), 'content-type': ('content-type', 'application/json')}))

Can you help to solve the problem please ?

All the Best;
Aziz
",MehmetAzizYirik,Lukasa
3545,2016-09-20 19:05:55,"@Lukasa whenever you've got a chance, could I get your thoughts on this?
",nateprewitt,Lukasa
3543,2016-08-27 11:36:36,"@kennethreitz This one is for you, my friend.
",Lukasa,kennethreitz
3539,2016-08-26 12:32:36,"@Lukasa I was certain this was described elsewhere, am I wrong?
",sigmavirus24,Lukasa
3535,2016-08-24 18:04:53,"This is a follow up on @jseabold's work in #3339. These last minor changes should fix the issues with return values of `seek` between Python 2 and Python 3.
",nateprewitt,jseabold
3535,2016-08-26 01:20:32,"Ok, cool, I like this. @sigmavirus24?
",Lukasa,sigmavirus24
3535,2016-09-01 22:31:59,"@sigmavirus24, wanted to ping in case this fell off the queue. No rush, just checking in :)
",nateprewitt,sigmavirus24
3527,2016-08-21 22:21:13,"Thanks for this @michelsch92!

Some notes. Firstly, I'm afraid to say that #3518 was already resolved by #3527. I'm sorry about that, the timing was just a bit unlucky.

Secondly, the response helpers, I think we don't want to put them into the core requests library itself. Generally speaking we try to keep the surface area of the API down, because the more methods and properties that there are on the object the harder it gets to understand. However, @sigmavirus24 may be interested in having those properties in a headers helper object in the requests-toolbelt.

Unfortunately, that means this patch won't be accepted at this time. This has nothing to do with the quality of your work: altogether it was high quality work, and we'd have been pleased to accept it. Please do keep looking around for ways to contribute, because you're most welcome here!
",Lukasa,sigmavirus24
3526,2016-08-21 17:02:52,"This seems reasonable to me. @Lukasa thoughts?
",sigmavirus24,Lukasa
3516,2016-08-17 10:23:22,"@alochym01 If you don't want it, why is it set in your environment? Regardless, if you use a session with `trust_env=False` that problem will no longer occur, though you'll lose your environment HTTP proxy. 

However, there does seem like there is a bug here: requests will prefer `all_proxy` to a scheme-specific proxy. I think that's a bad idea. @sigmavirus24, should we re-order that?
",Lukasa,sigmavirus24
3514,2016-08-15 09:08:00,"@kennethreitz, this is all we need from a boring administrative perspective. If you merge this PR, you only need to do three things to release:
1. Update the version number to 2.11.1.
2. Put the date you do this in the changelog instead of the XX.
3. Push the release.

It'd be good to get this release out this week: it should put the nastiest bugs of 2.11 behind us and get people back to working again. =)
",Lukasa,kennethreitz
3514,2016-08-15 10:20:06,"@kennethreitz let's :shipit:
",sigmavirus24,kennethreitz
3513,2016-08-14 20:08:58,"Various doc updates for clarity. Some of these may not match @kennethreitz's aesthetic sensibilities, which may require an inversion of the proposed change. I was mainly trying to make things consistent, input on preferences would be greatly appreciated.

Some notes:
- I think at least the faq.rst changes _need_ to be merged because there is still live documentation stating that Requests supports 3.1 and 3.2. I can open a separate PR if we want those changes quicker.
- What was ScraperWiki, is now found at quickcode.io and appears to either be a paid or account-walled service. Wayback Machine is showing the site as a free service when the documentation was created, so I'm not sure if Kenneth still wants this included. At the very least, the URL should be updated.
- ~~Does Requests support pypy 2.2 officially? faq.rst says yes, but it's other versions were wrong.~~
- While I realize it's common to use `class.method` as a pattern when describing code, I felt like `r.json` is ambiguous when referenced next to `r.text`. Calling exactly `r.json` won't function as described in the provided `ValueError` example. Explicitly defining the call as `r.json()` reduces the chance of misinterpretation.
- api.rst has a pared down set of Exceptions and classes. I assume this is to keep extraneous information from bloating the page to a point of being unhelpful. I do think it may be worth including `BaseAdapter` for easy reference since anyone looking into a custom _non-HTTP_ Transport Adapter will likely need to start there.
",nateprewitt,kennethreitz
3507,2016-08-11 23:54:54,"Hey @ppolewicz, I believe you can find answers addressing this in #3479, primarily @Lukasa's comment [here](https://github.com/kennethreitz/requests/issues/3479#issuecomment-238633044).

pip is currently unable to filter out libraries based on python version, but it was also [noted](https://github.com/kennethreitz/requests/issues/3479#issuecomment-238640627) in #3479 that this may be available in the _distant_ future. I'm not positive on when 3.2 was officially deprecated for Requests (it was before 2.11.0) but it was removed from the trove list in setup.py in May 2013. Unfortunately, I don't believe there's any intent to reintroduce support for 3.2, per @sigmavirus24 [comments](https://github.com/kennethreitz/requests/issues/3479#issuecomment-238640113).
",nateprewitt,Lukasa
3507,2016-08-11 23:54:54,"Hey @ppolewicz, I believe you can find answers addressing this in #3479, primarily @Lukasa's comment [here](https://github.com/kennethreitz/requests/issues/3479#issuecomment-238633044).

pip is currently unable to filter out libraries based on python version, but it was also [noted](https://github.com/kennethreitz/requests/issues/3479#issuecomment-238640627) in #3479 that this may be available in the _distant_ future. I'm not positive on when 3.2 was officially deprecated for Requests (it was before 2.11.0) but it was removed from the trove list in setup.py in May 2013. Unfortunately, I don't believe there's any intent to reintroduce support for 3.2, per @sigmavirus24 [comments](https://github.com/kennethreitz/requests/issues/3479#issuecomment-238640113).
",nateprewitt,sigmavirus24
3507,2016-08-12 00:36:36,"If @Lukasa wants I'd be happy to create a PR for thisfor 2.11.1
",TetraEtc,Lukasa
3507,2016-08-12 11:23:07,"> I understand that 3.2 is no longer supported by requests, but pip being broken as it is with the version mapping causes trouble for some people.

I'm sorry, I don't quite understand what you are referring to @ppolewicz. 

> I think a minor change of 2 LOC to prevent this would be acceptable?

@ppolewicz except that minor change implies Python 3.2 support. Python 3.2 as a product itself is unsupported. The number of users it has according to PyPI is incredibly small. I don't see a whole lot of value in changing those two lines, but @Lukasa and @kennethreitz may disagree. I'm not against it, I just don't agree that we should be implicitly supporting an unsupported version that is mostly not used (regardless of how easy it is).
",sigmavirus24,kennethreitz
3493,2016-08-10 17:22:28,"@saveman71, I only bring up the optimization because @sigmavirus24 has [brought it up before](https://github.com/kennethreitz/requests/pull/3362#discussion_r68398047). If I'm misunderstanding his original intent though, I vote we completely remove the declaration, in favor of `prepared_request.headers`. `headers` currently doesn't provide a lot of use in `resolve_redirects`.

Edit: Clarity and Link
",nateprewitt,sigmavirus24
3493,2016-08-11 17:03:14,"Alright, fab, I'm happy with this. @sigmavirus24 you ok with us merging this for 2.11.1?
",Lukasa,sigmavirus24
3493,2016-08-12 08:02:06,"Alright, all is well. @sigmavirus24, you want this in 2.11.1?
",Lukasa,sigmavirus24
3489,2016-08-09 18:17:19,"This is a continuation of @keyan's work in #2757 and addresses issue #1558 regarding pickling Request objects (specifically `PreparedRequest` objects in this PR).

These tests ensure that a standard `PreparedRequest`, a `PreparedRequest` with a file object as the body, and a `PreparedRequest` with hooks defined outside of the `locals` scope will all pickle properly.

This also works for data passed to the `json` parameter and I can add my test for that as well if it's deemed helpful.

Hooks defined inside of a method (such as in the [original test](https://github.com/kennethreitz/requests/pull/2757/files#diff-56c2d754173a4a158ce8f445834c8fe8R843) by @keyan) will fail because you can't pickle local objects. Is this a use case that needs to be handled?",nateprewitt,keyan
3489,2016-11-09 22:10:00,"@Lukasa any thoughts here? We still have edge cases where if the `data` param is a generator or file-like object, it can't be pickled. Is there any utility in adding this subset of tests, or should #1558 be closed as can't fix?
",nateprewitt,Lukasa
3486,2016-08-09 13:51:45,"I'm good with this. How about you @Lukasa? (To be honest, I would have rathered let GitHub generate the revert than have it done artisanally.)
",sigmavirus24,Lukasa
3481,2016-08-09 08:37:36,"Aha, got it.

`decode_unicode=True` was enhanced in the last release to perform the same auto-detection of encoding as `text` does for Requests. Unfortunately, that may involve a call to `apparent_encoding`, which streams in the entire request body and attempts to determine the encoding.

This was done as part of #3362 in an attempt to resolve #3359.

Frankly, this leads me to suspect that perhaps we should revert that change. If `apparent_encoding` is going to read the whole response body then there is genuinely no point in falling back to `apparent_encoding` as part of `iter_*`: any reason to use the method has been totally lost.

I wonder if we should instead revert the fix in #3362, and then do one of the following three things:
1. Use UTF-8 as an unconditional fallback if no specific encoding is declared.
2. Raise an exception if no specific encoding is declared.
3. Allow `decode_unicode` to take an encoding name which is used as a fallback.

@kennethreitz @sigmavirus24?
",Lukasa,kennethreitz
3481,2016-08-09 08:37:36,"Aha, got it.

`decode_unicode=True` was enhanced in the last release to perform the same auto-detection of encoding as `text` does for Requests. Unfortunately, that may involve a call to `apparent_encoding`, which streams in the entire request body and attempts to determine the encoding.

This was done as part of #3362 in an attempt to resolve #3359.

Frankly, this leads me to suspect that perhaps we should revert that change. If `apparent_encoding` is going to read the whole response body then there is genuinely no point in falling back to `apparent_encoding` as part of `iter_*`: any reason to use the method has been totally lost.

I wonder if we should instead revert the fix in #3362, and then do one of the following three things:
1. Use UTF-8 as an unconditional fallback if no specific encoding is declared.
2. Raise an exception if no specific encoding is declared.
3. Allow `decode_unicode` to take an encoding name which is used as a fallback.

@kennethreitz @sigmavirus24?
",Lukasa,sigmavirus24
3479,2016-08-09 17:46:55,"@huseyinyilmaz That is not possible, I'm afraid: you'd need to ask @dstufft to provide tools to do that. It's a nice idea, certainly, although it requires much more formalisation of the trove classifiers system that currently exists.
",Lukasa,dstufft
3478,2016-08-08 22:47:04,"We have not supported Python 3.2 for ... over a year? I don't think we'll accept this. @kennethreitz, thoughts?
",sigmavirus24,kennethreitz
3475,2016-08-08 12:39:59,"@kennethreitz, this is all we need from a boring administrative perspective. If you merge this PR, you only need to do three things to release:
1. Update the version number to 2.11.
2. Put the date you do this in the changelog instead of the XX.
3. Push the release.

@sigmavirus24 I'm inclined to want to let all currently open PRs remain open rather than try to rush merge any: they can always wait for a 2.11.1 or 2.12 without any risk.
",Lukasa,kennethreitz
3475,2016-08-08 12:39:59,"@kennethreitz, this is all we need from a boring administrative perspective. If you merge this PR, you only need to do three things to release:
1. Update the version number to 2.11.
2. Put the date you do this in the changelog instead of the XX.
3. Push the release.

@sigmavirus24 I'm inclined to want to let all currently open PRs remain open rather than try to rush merge any: they can always wait for a 2.11.1 or 2.12 without any risk.
",Lukasa,sigmavirus24
3463,2016-07-31 07:36:07,"So I'm generally ok with this, but I'd like @sigmavirus24 and @kennethreitz to have a look.
",Lukasa,kennethreitz
3463,2016-07-31 07:36:07,"So I'm generally ok with this, but I'd like @sigmavirus24 and @kennethreitz to have a look.
",Lukasa,sigmavirus24
3444,2016-07-27 06:05:09,"Thanks for this! 

It's my view that this is overkill: generally speaking, users that require this function can implement it themselves, and the majority of users don't need it. But I'll leave that decision up to @kennethreitz.
",Lukasa,kennethreitz
3444,2016-07-27 13:47:42,"@Lukasa further, in v1.0 didn't @kennethreitz completely tear out what logging requests did provide because people were constantly trying to add tons more logging on top of what we provided? I feel like were were to accept this we'd be in the same situation.

Further, what's being logged here is a very clear overload of information for any logging that I would view as reasonable for requests. This seems to be working very well in Café as it is and I see little reason to include this in Requests.

I appreciate your offer of the code @seemethere, but I'm very strongly against this. If you want review on the code anyway, I'm happy to provide that in a separate forum.
",sigmavirus24,kennethreitz
3439,2016-07-23 19:16:06,"To be clear, I assigned @kennethreitz to this because I think this is something he's going to care about and have opinions about. I'm fine with this but I'll leave it to him to merge.
",sigmavirus24,kennethreitz
3427,2016-07-20 16:39:57,"This is a combination of two passes over the code to help cleanup some of the coding style differences that have creeped in over time. It's mostly cosmetic but I think it'll help reduce inconsistencies going forward by having the code be an example for future contributors. Per @Lukasa's request, I'm going to break this up into two pull requests. One involving docstrings and the other involving pep8 fixes. I don't know that these changes are necessarily ""correct"", I simply chose what appeared to be the most common occurrence in the code. I'm sure @kennethreitz will have opinions on which choices are best.
",nateprewitt,kennethreitz
3427,2016-07-20 16:39:57,"This is a combination of two passes over the code to help cleanup some of the coding style differences that have creeped in over time. It's mostly cosmetic but I think it'll help reduce inconsistencies going forward by having the code be an example for future contributors. Per @Lukasa's request, I'm going to break this up into two pull requests. One involving docstrings and the other involving pep8 fixes. I don't know that these changes are necessarily ""correct"", I simply chose what appeared to be the most common occurrence in the code. I'm sure @kennethreitz will have opinions on which choices are best.
",nateprewitt,Lukasa
3426,2016-07-20 16:08:10,"This spawned off of [an article](https://hynek.me/articles/hasattr/) @Lukasa brought up in another PR. These changes should fix the pitfalls with using `hasattr` by converting their logic to use `getattr` instead.
",nateprewitt,Lukasa
3390,2016-07-07 15:32:45,"👍 from me. @Lukasa ?
",sigmavirus24,Lukasa
3389,2016-07-07 10:14:24,"Yeah, this makes me somewhat uncomfortable. My _suspicion_ is that the LGPL doesn't allow what we're doing here, but IANAL. Fundamentally the decision here is with @kennethreitz. If it were me, though, I'd chat with Van Lindberg and see what he says, and then potentially take action to make chardet an optional dependency (like PyOpenSSL), rather than a bundled one.
",Lukasa,kennethreitz
3388,2016-07-07 13:19:38,"Cool, LGTM! :sparkles: @sigmavirus24?
",Lukasa,sigmavirus24
3388,2016-07-07 13:27:44,"While you're here @sigmavirus24 and @Lukasa, I threw together [another commit](https://github.com/nateprewitt/requests/commit/be31a90906deb5553c2e703fb05cf6964ee23ed5) related to this to encapsulate the type error thrown by `re` when non-strings are passed to `check_header_validity`. This is to make it clear that the behaviour is to be expected, but may be overkill with this documentation now. Any thoughts on if this would be worth opening? 
",nateprewitt,sigmavirus24
3387,2016-07-05 16:06:41,"This will fix issue #3386 with non-string/buffer header values, but there may be some more discussion to be had here. This is a passthrough for non-string/buffers which means all other datatypes will skip the regex check. I wasn't able to create the header split issue with most of the other standard datatypes I checked (lists, sets, dicts), so this _should_ be safe with the passthrough. 

I think an alternative, and possibly better solution is to cast everything that isn't a byte string as a `str`, whether that be standard, or `encode('latin-1')`. This will ensure the check still runs and doesn't modify the value in the actual `Requests.headers` value. This is probably closer to what @sigmavirus24 already did [here](https://github.com/kennethreitz/requests/pull/866/files#diff-5956087d5835a57d9ef6fff974f6fd9bR273).
",nateprewitt,sigmavirus24
3386,2016-07-05 15:07:50,"@wut0n9 This behavioural change is not in v2.10.0, it's in the current master branch.

However, that's a real bug: #3366 has regressed this. @nateprewitt, are you interested in trying to update with a fix for this?
",Lukasa,nateprewitt
3385,2016-07-05 09:47:23,"Cool, I'm happy with this as-is. I'd like @sigmavirus24 to ACK as well, if possible.
",Lukasa,sigmavirus24
3383,2016-07-01 23:42:41,"@Lukasa @sigmavirus24 

I tried running the tests on the proposed/3.0.0 branch, and found that they did not run. So I changed a couple of variable names to fix the obvious errors. If you prefer InvalidSchema and MissingSchema I can change those back, I changed the imports to match the names of the exceptions as they were defined.

After these changes, there is still one failing test. I'll try to take a look and see if I can debug it. But this was low hanging fruit.
",davidsoncasey,Lukasa
3383,2016-07-01 23:42:41,"@Lukasa @sigmavirus24 

I tried running the tests on the proposed/3.0.0 branch, and found that they did not run. So I changed a couple of variable names to fix the obvious errors. If you prefer InvalidSchema and MissingSchema I can change those back, I changed the imports to match the names of the exceptions as they were defined.

After these changes, there is still one failing test. I'll try to take a look and see if I can debug it. But this was low hanging fruit.
",davidsoncasey,sigmavirus24
3370,2016-07-01 13:24:27,"I'm happy with this, but would like @sigmavirus24 to take a look as well.
",Lukasa,sigmavirus24
3370,2016-07-01 21:58:06,"Ok, I'm still happy, but would still like @sigmavirus24 to weigh in. 
",Lukasa,sigmavirus24
3362,2016-06-28 18:05:05,"@Lukasa this looks good to me. What are your thoughts?
",sigmavirus24,Lukasa
3361,2016-06-23 06:06:56,"PreparedRequests should be suitable for accomplishing what you're trying to accomplish.

I see where you're coming from, and thank you for your contribution. However, in short, no. :)

@sigmavirus24 & @Lukasa are welcome to add additional thoughts, if desired.
",kennethreitz,Lukasa
3361,2016-06-23 06:06:56,"PreparedRequests should be suitable for accomplishing what you're trying to accomplish.

I see where you're coming from, and thank you for your contribution. However, in short, no. :)

@sigmavirus24 & @Lukasa are welcome to add additional thoughts, if desired.
",kennethreitz,sigmavirus24
3357,2016-06-21 16:19:04,"@sigmavirus24 mentioned that in auth.py, the magic method **ne** is technically not needed since != is implied from definition of **eq**. Would this be considered a useful update?
",ueg1990,sigmavirus24
3338,2016-11-08 06:19:48,"@Lukasa @nateprewitt I sort of forgot that this PR has been hanging out - is this still something we'd like to merge in? I can see about getting the conflicts fixed. Sorry I sort of lost momentum working on requests, I switched jobs and don't get time at work for open source projects any more.
",davidsoncasey,Lukasa
3338,2016-11-08 06:19:48,"@Lukasa @nateprewitt I sort of forgot that this PR has been hanging out - is this still something we'd like to merge in? I can see about getting the conflicts fixed. Sorry I sort of lost momentum working on requests, I switched jobs and don't get time at work for open source projects any more.
",davidsoncasey,nateprewitt
3338,2016-11-08 07:25:27,"@davidsoncasey, no worries, thanks for taking the time to check back in! 😀

 @Lukasa may have more on this but I think moving [these tests](https://github.com/davidsoncasey/requests/blob/60e0349ce265378b127ae51e1853533b57eaac38/tests/test_requests.py#L1241-L1270) into a separate PR against master will be the most immediate benefit. This will show that the issue is currently fixed on master and allow us to close out #3066.

The work you did for `prepare_body` and `prepare_content_length` simplifies a lot of the logic and would be great for proposed/3.0.0. We'll need to merge master into the proposed/3.0.0 branch in this repo and then have you rebase your commits on top of it. I did some local testing and got this patch merged and working relatively pain free.

I'm also happy to help with any of the reshuffling of your commits if needed, just let us know.
",nateprewitt,Lukasa
3338,2017-02-10 17:14:06,"@davidsoncasey, checking in again. It looks like @kennethreitz may want to start cleaning some of these older PRs up. There's still a lot of useful stuff in here that didn't make it into master but should be in 3.0.0. Would you be interested in bringing this branch up to date and fixing a few things? If not, I'll tidy up your commits and wrap this up.",nateprewitt,kennethreitz
3297,2016-06-09 05:02:07,"Urllib3 has an exception with the same name, but it is not actually related to `requests.exceptions.HTTPError`. I [searched this repository](https://github.com/kennethreitz/requests/search?utf8=%E2%9C%93&q=httperror) (well, I used grep) for HTTPError and the only place I can see where requests itself uses it is with `raise_for_status`.

I had a discussion with @Lukasa briefly on IRC about this. As far as I can tell, `requests.exceptions.HTTPError` is only used for this purpose. However, if you think you'll use it for other purposes, another alternative is to have something like an `HTTPStatusCodeError` (a subclass of HTTPError) that is specifically for `raise_for_status`.
",davidfischer,Lukasa
3296,2016-07-04 13:30:36,"Cool, that's a good spot.

Yes, there is a minor bug in `resolve_redirects`. Specifically, while `resolve_redirects` attempts to remove proxy information, it cannot actually tell the difference between a proxy that was passed in via the command-line API or from the session and one that was extracted from the environment. This is because `resolve_redirects` is passed the _computed_ `proxies` argument, not the _user's_ `proxies` argument.

With the way the code in requests is structured, this is a very difficult problem to solve. One option is to hang the original `proxies` kwarg off the `Request` object: this will allow `rebuild_proxies` to essentially re-calculate the proxies argument. Another option is to suggest that the `NO_PROXY` environment variable overrides the user proxies argument for redirects: this is out of line with what we normally do, so I'm inclined to not do this. A third option is to try to do something wacky with storing the original `proxies` kwarg value and passing it to `Session.send` so that we can pass it to `resolve_redirects`, but that seems kind of nutty.

Does anyone else have an opinion on how to go about doing this that doesn't suck as much as the three I have just mentioned? @sigmavirus24?
",Lukasa,sigmavirus24
3289,2016-06-07 14:26:27,"This looks reasonable to me aside from my one note about warning spam. @sigmavirus24, what do you think?
",Lukasa,sigmavirus24
3287,2016-06-06 02:10:11,"In requests 2.5.2 `requests.packages.__init__` added code to allow `requests.packages.urllib3` to not exist, and it would fall back to using `urllib3`.

No doubt this works in some cases, as the people on #2375 were happy with it, but it doesnt work with pyopenssl, resulting in no SNI/etc.

`requests.packages.__init__` only injects `urllib3` as `requests.packages.urllib3`.

However all of the other modules within urlllib3 will have different copies in the namespace `requests.packages.urllib3`.

`requests.packages.urllib3.contrib.pyopenssl` then writes to a separate copy of `requests.packages.urllib3.util` and `requests.packages.urllib3.connection`, and these changes are never seen by `urllib3`.

I encountered this on debian stretch & sid's `python-requests` 2.10.0-1 with `python-urllib3` 1.15.1 when running on Python 2.7.6 (Ubuntu Trusty), and haven't retested yet on a later Python 2.7.  @eriol
#3286 fixes the problem for me.

I note that Fedora's latest `python-requests` no longer unbundles urllib3.
",jayvdb,eriol
3287,2016-06-17 12:14:26,"@eriol +1  I think pip has done the best job of making devendorizing easier for downstream redistributors.  My deltas are very small and now that it's been in place for a while, I haven't encountered any issues with it.  It's possible I'm missing something, so pinging @dstufft for additional thoughts.
",warsaw,dstufft
3250,2016-05-31 21:25:26,"Hrm, this seems to be a problem with the RTD docs build. @sigmavirus24, any ideas?
",Lukasa,sigmavirus24
3236,2016-06-21 02:18:40,"@eriol I'm so sorry. I think I lost track of this while I was travelling to PyCon. I expect the same happened to @Lukasa 
",sigmavirus24,Lukasa
3223,2016-05-28 22:04:07,"So the fact that these are all `getaddrinfo` failures makes me think that this might be some sort of DNS caching. I don't think Python 3.4 does that, so I'm guessing Fedora is doing it. I also don't think there's a way that Requests can work around this. And I don't know Fedora very well but @ralphbean does. Maybe he can help us out here?
",sigmavirus24,ralphbean
3216,2016-05-24 07:02:09,"There's one small note from @sigmavirus24 but I'd be happy to merge this when that's resolved.
",Lukasa,sigmavirus24
3196,2016-05-16 17:19:29,"Yeah, that's definitely the case, but I think the two URLs are semantically identical.

Regardless, I _also_ think that we have a question to ask ourselves (we being @sigmavirus24 and I), which is: why does `resolve_redirects` not end up calling `prepare_url`?
",Lukasa,sigmavirus24
3195,2016-05-16 11:04:51,"Cool, I'm :+1: on this for now. I'd like another :+1: from @sigmavirus24 before we merge, but I think this is good.
",Lukasa,sigmavirus24
3192,2016-05-14 01:12:25,"ping @Lukasa @sigmavirus24 

I think I addressed the comments from #3185.

Oddly, I cannot make the test suite hang indefinitely when I run it locally, but that's what's happening in jenkins. :confused: Maybe you can spot something I'm missing?
",brettdh,Lukasa
3192,2016-05-14 01:12:25,"ping @Lukasa @sigmavirus24 

I think I addressed the comments from #3185.

Oddly, I cannot make the test suite hang indefinitely when I run it locally, but that's what's happening in jenkins. :confused: Maybe you can spot something I'm missing?
",brettdh,sigmavirus24
3192,2016-05-17 07:10:39,"Ok, cool, I'm happy with this. Assuming @sigmavirus24 is as well, he may merge.
",Lukasa,sigmavirus24
3186,2016-05-11 19:12:34,"`Response.content` [iterates over the response data in chunks of 10240 bytes](https://github.com/kennethreitz/requests/blob/87704105af65b382b86f168f6a54192eab91faf2/requests/models.py#L741). The number 10240 was set in commit [`62d2ea8`](https://github.com/kennethreitz/requests/commit/62d2ea8).

After tracing the source code of urllib3 and httplib, I can’t see a reason for this behavior. It all ultimately goes through httplib’s [`HTTPResponse.readinto`](https://hg.python.org/cpython/file/87130512ef34/Lib/http/client.py#l469), which automatically limits the read size according to `Content-Length` or the `chunked` framing.

Therefore, it seems that, if you simply set `CONTENT_CHUNK_SIZE` to a much larger number (like 10240000), nothing should change, except `Response.content` will become more efficient on large responses.

**Update:** it seems like httplib allocates a buffer of the requested size (to be read into), so simply setting `CONTENT_CHUNK_SIZE` to a large value will cause large chunks of memory to be allocated, which is probably a bad idea.

This is not a problem for me and I have not researched it thoroughly. I’m filing this issue after investigating [a Stack Overflow question](http://stackoverflow.com/questions/37135880/python-3-urllib-vs-requests-performance) where this caused an unexpected slowdown for the poster, and a subsequent [IRC exchange](https://botbot.me/freenode/python-requests/2016-05-11/?msg=65874287&page=1) with @Lukasa. Feel free to do (or not do) whatever you think is right here.
",vfaronov,Lukasa
3185,2016-05-11 13:09:31,"ping @Lukasa 
",brettdh,Lukasa
3184,2016-05-11 04:01:14,"@Lukasa Here's an alternative fix to #3066 (discussed in PR #3181), where I refactored `PreparedRequest.prepare_body` and `PreparedRequest.prepare_content_length` so that `prepare_content_length` is always called.

My only question is in the case when the body is a stream (which it is in the case that brought up this issue) that the current position will always be 0 when the length is calculated. Previously, in cases where `prepare_auth` was not called, the content length was being calculated with `super_len`, now it would be calculated using `self.headers['Content-Length'] = builtin_str(max(0, end_pos - curr_pos))`. As far as I can tell, this will give the same result (as long as the current position is 0), and this is how it's been computed in cases where authorization is being used. But I'm curious if you have thoughts about this.
",davidsoncasey,Lukasa
3184,2016-05-11 08:06:41,"So, I think in general I like this better. However, your point about `super_len` is a real one.

I think the solution here is to rewrite `prepare_content_length` to have it always call `super_len`. `super_len` does use `tell` to do a little dance here, so I think it's probably going to be good enough: @sigmavirus24, does that seem right to you?
",Lukasa,sigmavirus24
3184,2016-05-11 16:09:34,"Ok, I think I'm happy with this, though I'd like @sigmavirus24 to review before we merge. =)
",Lukasa,sigmavirus24
3179,2016-05-05 07:22:38,"Huh. I feel like maybe we should just change `Response.content` to never be `None`. It's certainly surprising to me that it can be. @kennethreitz @sigmavirus24?
",Lukasa,kennethreitz
3179,2016-05-05 07:22:38,"Huh. I feel like maybe we should just change `Response.content` to never be `None`. It's certainly surprising to me that it can be. @kennethreitz @sigmavirus24?
",Lukasa,sigmavirus24
3176,2016-05-04 19:29:39,"This is up to @kennethreitz, but I'm disinclined to want to add this: I think I'd rather factor out the logic in `SessionRedirectMixin` as we've done with a few other bits to allow easier subclassing, rather than add further keyword arguments.
",Lukasa,kennethreitz
3174,2016-05-03 19:33:20,"Actually, before we conclude this is a urllib3 bug, we need to have a discussion about what the API promises. So let me explain what the problem is.

urllib3 attempts to ensure that connections are thrown away if a problem occurs. This is implemented using a context manager in urllib3 (`_error_catcher`) that checks certain errors and then determines whether the block terminated cleanly. If it did not, it forcibly closes the connection before returning the connection object to the pool to be reopened.

That object treats _any_ exception as an error case that leaves the connection in an indeterminate state. That is mostly a good thing, except for one particularly awkward exception: [`GeneratorExit`](https://docs.python.org/2/library/exceptions.html#exceptions.GeneratorExit). This exception is raised when the `close()` method on a generator is called, and is thrown to allow things like `finally` blocks to execute properly.

_One_ case where this is called is when a generator is garbage collected. That happens in your code, because you call `iter_content` multiple times, once for each tag you search for. When that happens you spin up several generators (`iter_content` returns a generator, and `stream` returns a generator, and `read_chunked` also returns a generator, so there's a chain of at least three generators in this case). Because you don't save the return value from `iter_content`, that generator chain gets leaked. This causes the `read_chunked` generator to throw `GeneratorExit`, which causes the urllib3 `_error_catcher` to conclude that the connection was not left in a clean state and terminates it.

There are therefore a few questions:
1. Should `_error_catcher` consider `GeneratorExit` an error case, or special case it? I'm not sure: the question is whether the connection will get cleaned up properly in situations where the generator really is leaked. Currently my assumption is that it won't, and so `GeneratorExit` really is an error case.
2. Is it safe to open multiple versions of the generators `iter_content`, `stream`, and `read_chunked`? They don't make it clear. `iter_lines` in requests is clear that it is _not_ safe to do that, but the rest are left ambiguous. We need to make a call, where I suspect the answer boils down to whether `decode_content` is `True`: if it is, there is a state object that gets lost when that generator leaks. Given that requests essentially always sets `decode_content` to `True`, I think that means it's also not safe to repeatedly call `iter_content`.

@ducu In the short term, you can fix this by saving off the result of `iter_content` somewhere on your `Summary` object and then re-using that, rather than repeatedly re-calling that method. That logic will _definitely_ work, and we can work out whether or not the alternative _should_ work.

Can I get input from @kennethreitz, @shazow, and @sigmavirus24 on this please?
",Lukasa,kennethreitz
3174,2016-05-03 19:33:20,"Actually, before we conclude this is a urllib3 bug, we need to have a discussion about what the API promises. So let me explain what the problem is.

urllib3 attempts to ensure that connections are thrown away if a problem occurs. This is implemented using a context manager in urllib3 (`_error_catcher`) that checks certain errors and then determines whether the block terminated cleanly. If it did not, it forcibly closes the connection before returning the connection object to the pool to be reopened.

That object treats _any_ exception as an error case that leaves the connection in an indeterminate state. That is mostly a good thing, except for one particularly awkward exception: [`GeneratorExit`](https://docs.python.org/2/library/exceptions.html#exceptions.GeneratorExit). This exception is raised when the `close()` method on a generator is called, and is thrown to allow things like `finally` blocks to execute properly.

_One_ case where this is called is when a generator is garbage collected. That happens in your code, because you call `iter_content` multiple times, once for each tag you search for. When that happens you spin up several generators (`iter_content` returns a generator, and `stream` returns a generator, and `read_chunked` also returns a generator, so there's a chain of at least three generators in this case). Because you don't save the return value from `iter_content`, that generator chain gets leaked. This causes the `read_chunked` generator to throw `GeneratorExit`, which causes the urllib3 `_error_catcher` to conclude that the connection was not left in a clean state and terminates it.

There are therefore a few questions:
1. Should `_error_catcher` consider `GeneratorExit` an error case, or special case it? I'm not sure: the question is whether the connection will get cleaned up properly in situations where the generator really is leaked. Currently my assumption is that it won't, and so `GeneratorExit` really is an error case.
2. Is it safe to open multiple versions of the generators `iter_content`, `stream`, and `read_chunked`? They don't make it clear. `iter_lines` in requests is clear that it is _not_ safe to do that, but the rest are left ambiguous. We need to make a call, where I suspect the answer boils down to whether `decode_content` is `True`: if it is, there is a state object that gets lost when that generator leaks. Given that requests essentially always sets `decode_content` to `True`, I think that means it's also not safe to repeatedly call `iter_content`.

@ducu In the short term, you can fix this by saving off the result of `iter_content` somewhere on your `Summary` object and then re-using that, rather than repeatedly re-calling that method. That logic will _definitely_ work, and we can work out whether or not the alternative _should_ work.

Can I get input from @kennethreitz, @shazow, and @sigmavirus24 on this please?
",Lukasa,shazow
3174,2016-05-03 19:33:20,"Actually, before we conclude this is a urllib3 bug, we need to have a discussion about what the API promises. So let me explain what the problem is.

urllib3 attempts to ensure that connections are thrown away if a problem occurs. This is implemented using a context manager in urllib3 (`_error_catcher`) that checks certain errors and then determines whether the block terminated cleanly. If it did not, it forcibly closes the connection before returning the connection object to the pool to be reopened.

That object treats _any_ exception as an error case that leaves the connection in an indeterminate state. That is mostly a good thing, except for one particularly awkward exception: [`GeneratorExit`](https://docs.python.org/2/library/exceptions.html#exceptions.GeneratorExit). This exception is raised when the `close()` method on a generator is called, and is thrown to allow things like `finally` blocks to execute properly.

_One_ case where this is called is when a generator is garbage collected. That happens in your code, because you call `iter_content` multiple times, once for each tag you search for. When that happens you spin up several generators (`iter_content` returns a generator, and `stream` returns a generator, and `read_chunked` also returns a generator, so there's a chain of at least three generators in this case). Because you don't save the return value from `iter_content`, that generator chain gets leaked. This causes the `read_chunked` generator to throw `GeneratorExit`, which causes the urllib3 `_error_catcher` to conclude that the connection was not left in a clean state and terminates it.

There are therefore a few questions:
1. Should `_error_catcher` consider `GeneratorExit` an error case, or special case it? I'm not sure: the question is whether the connection will get cleaned up properly in situations where the generator really is leaked. Currently my assumption is that it won't, and so `GeneratorExit` really is an error case.
2. Is it safe to open multiple versions of the generators `iter_content`, `stream`, and `read_chunked`? They don't make it clear. `iter_lines` in requests is clear that it is _not_ safe to do that, but the rest are left ambiguous. We need to make a call, where I suspect the answer boils down to whether `decode_content` is `True`: if it is, there is a state object that gets lost when that generator leaks. Given that requests essentially always sets `decode_content` to `True`, I think that means it's also not safe to repeatedly call `iter_content`.

@ducu In the short term, you can fix this by saving off the result of `iter_content` somewhere on your `Summary` object and then re-using that, rather than repeatedly re-calling that method. That logic will _definitely_ work, and we can work out whether or not the alternative _should_ work.

Can I get input from @kennethreitz, @shazow, and @sigmavirus24 on this please?
",Lukasa,sigmavirus24
3131,2016-04-26 14:53:18,"If you're using a `file://` URI, you're never going to talk to something over the network using HTTP (for example). I'll talk to @msabramo about fixing unixsocket as this problem will persist otherwise for several people who will not get this fix.
",sigmavirus24,msabramo
3131,2016-04-26 15:57:36,"Ah so @msabramo should be overriding that too if he continues to subclass the adapter. (I missed the usage in the `request_url` method because GitHub search hasn't indexed that yet apparently.)
",sigmavirus24,msabramo
3108,2016-04-21 15:22:47,"I'm happy with this! Go for it @kennethreitz, merge if you'd like to. =D
",Lukasa,kennethreitz
3100,2016-04-17 20:03:22,"@kennethreitz, @sigmavirus24 Thanks! 👍 
",hitstergtd,sigmavirus24
3089,2016-05-21 17:07:27,"if found a simple way to  achieve my target.



it work well.
Thanks all of you.
@kennethreitz 
@Lukasa 
@sigmavirus24 
",liuyang007,kennethreitz
3082,2016-04-08 20:40:11,"I think this works well, at least in the basic case. As you correctly pointed out, there's a question about how this will work with redirects, but that's harder for us to test at this moment (though we'll get there shortly I hope).

@kennethreitz, want a final review here?
",Lukasa,kennethreitz
3082,2016-04-11 17:43:20,"Gah, @sigmavirus24 beat me to it
",kennethreitz,sigmavirus24
3079,2016-04-08 09:24:52,"@tzickel As far as I know, BytesIO implements `tell()` on all Python versions, so the simplest thing to do is to add logic that does `tell()`, `seek()`, `tell()`, `seek()`. Doing that, incidentally, would allow us to also record the current location in the file-like-object so that we can safely rewind.

@sigmavirus24, you wrote the original `getvalue()` call: what are the odds you remember the rationale at the time?
",Lukasa,sigmavirus24
3079,2016-04-08 14:15:35,"1. Shouldn't requests have an option to make 302 and 307 act the same (instead of as 303 as it does by default) ? The HTTP 1.1 spec actually meant that 302 should be like 307 by default (yet most browsers treat it like a 303).
2. I'm still waiting for the input from @sigmavirus24 because that is important memory-wise.
",tzickel,sigmavirus24
3070,2016-03-29 17:08:43,"This is an entirely reasonable suggestion.

Honestly, I'm not averse to doing it: there are definitely worse things to do than this. I'd be open to providing a default timeout. However, I don't think we can do it until 3.0.0.

Of course @kennethreitz as keeper of the spirit of requests has got the final say on this, and should definitely weigh in.
",Lukasa,kennethreitz
3065,2016-03-24 14:10:48,"Well that's bizarre. Does `pip freeze` show that ndg-httpsclient is installed?

@dstufft, any theories about this weirdness?
",Lukasa,dstufft
3060,2016-03-18 10:01:21,"So, I'm totally happy with this code change: I think it's a reasonable refactoring that helps improve some clarity, _as well_ as allow @benweatherman to perform the override you want.

I'm also delighted to see some extra tests added for this. I think this change is wonderful! Pinging @kennethreitz or @sigmavirus24 for an extra round of review before merging.
",Lukasa,kennethreitz
3060,2016-03-18 10:01:21,"So, I'm totally happy with this code change: I think it's a reasonable refactoring that helps improve some clarity, _as well_ as allow @benweatherman to perform the override you want.

I'm also delighted to see some extra tests added for this. I think this change is wonderful! Pinging @kennethreitz or @sigmavirus24 for an extra round of review before merging.
",Lukasa,sigmavirus24
3059,2016-03-17 19:56:34,"Cool, I'm happy with this. @sigmavirus24 `ProxyError` is a `ConnectionError` subclass, so I think this can go in the next minor release. Agreed?
",Lukasa,sigmavirus24
3052,2016-03-15 21:08:17,"So to be clear, I am now as I was then open to removing simplejson as an option entirely, but @kennethreitz wanted it there. =)
",Lukasa,kennethreitz
3050,2016-03-13 12:26:00,"And yeah, in first place, thanks for great library, @kennethreitz and all the contributors :+1: 
",alexanderad,kennethreitz
3049,2016-03-13 11:23:35,"This also seems reasonable to me, but @Stranger6667 is sat right next to me. ;) @sigmavirus24/@kennethreitz, mind doing an extra review for me? This is +1 from me.
",Lukasa,kennethreitz
3049,2016-03-13 11:23:35,"This also seems reasonable to me, but @Stranger6667 is sat right next to me. ;) @sigmavirus24/@kennethreitz, mind doing an extra review for me? This is +1 from me.
",Lukasa,sigmavirus24
3049,2016-03-13 11:37:35,"Hello folks seems somehow I got placed on this email alias By mistake.
Anyway I can be removed.

Thank you
-Ryan
On Sun, Mar 13, 2016 at 7:24 AM Cory Benfield notifications@github.com
wrote:

> This also seems reasonable to me, but @Stranger6667
> https://github.com/Stranger6667 is sat right next to me. ;)
> @sigmavirus24/@kennethreitz https://github.com/kennethreitz, mind doing
> an extra review for me? This is +1 from me.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/pull/3049#issuecomment-195938314
> .
",ryandebruyn,kennethreitz
3049,2016-03-13 11:37:35,"Hello folks seems somehow I got placed on this email alias By mistake.
Anyway I can be removed.

Thank you
-Ryan
On Sun, Mar 13, 2016 at 7:24 AM Cory Benfield notifications@github.com
wrote:

> This also seems reasonable to me, but @Stranger6667
> https://github.com/Stranger6667 is sat right next to me. ;)
> @sigmavirus24/@kennethreitz https://github.com/kennethreitz, mind doing
> an extra review for me? This is +1 from me.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/pull/3049#issuecomment-195938314
> .
",ryandebruyn,sigmavirus24
3048,2016-03-13 11:13:15,"@sigmavirus24 @kennethreitz Can one of you two do a separate code review of this? @Stranger6667 is here with me at PyCon SK writing some of these tests. I'm happy with these, but it'd be good if one of you two gave the ok/not-ok.
",Lukasa,kennethreitz
3048,2016-03-13 11:13:15,"@sigmavirus24 @kennethreitz Can one of you two do a separate code review of this? @Stranger6667 is here with me at PyCon SK writing some of these tests. I'm happy with these, but it'd be good if one of you two gave the ok/not-ok.
",Lukasa,sigmavirus24
3048,2016-03-15 13:51:17,"Hello @Lukasa @sigmavirus24 !
I've updated this PR :)
",Stranger6667,sigmavirus24
3038,2016-03-08 08:32:42,"FWIW, I don't think any of those issues actually indicate that preservation of order on the wire is important. It _is_, but not for any of those reasons. The reason it's important is that headers can in principle be split up into multiple instances if the header is a comma-separated list, and maintaining the order of those parts is important (or the header ends up taking on a different meaning).

The fundamental reason this doesn't work is because we store the headers in our `CaseInsensitiveDict`, which does not preserve order. At this point we may want to look harder at replacing our own implementation with the one from urllib3, which _does_ preserve order.

Thoughts @kennethreitz, @sigmavirus24?
",Lukasa,kennethreitz
3038,2016-03-08 08:32:42,"FWIW, I don't think any of those issues actually indicate that preservation of order on the wire is important. It _is_, but not for any of those reasons. The reason it's important is that headers can in principle be split up into multiple instances if the header is a comma-separated list, and maintaining the order of those parts is important (or the header ends up taking on a different meaning).

The fundamental reason this doesn't work is because we store the headers in our `CaseInsensitiveDict`, which does not preserve order. At this point we may want to look harder at replacing our own implementation with the one from urllib3, which _does_ preserve order.

Thoughts @kennethreitz, @sigmavirus24?
",Lukasa,sigmavirus24
3036,2016-03-07 08:33:52,"Resolves #3035.

I'm not actually sure that this approach is the right one: I'm inclined to say that, when an exception is hit from `tell()`, that we may want to assume we don't know the length at all (return length 0) and use chunked-transfer encoding. That's a particularly good idea in this case, as frequently stdin has an unknown length altogether.

Thoughts on that point @sigmavirus24 and @jkbrzt?
",Lukasa,sigmavirus24
3033,2016-03-05 07:11:26,"I'm really intrigued by this idea. @kennethreitz?
",Lukasa,kennethreitz
3033,2016-03-05 09:02:55,"Transifex has a really nice permissions system, you can set it to auto
update the source translation from a url and a useful API.

On Sat, 5 Mar 2016 17:11 Cory Benfield notifications@github.com wrote:

> I'm really intrigued by this idea. @kennethreitz
> https://github.com/kennethreitz?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/3033#issuecomment-192600901
> .
> 
> ## 
> 
> _Stewart Polley_
> Head of Technical Support
> stewart.polley@elvanto.com | www.elvanto.com
",ElvantoStewart,kennethreitz
3029,2016-03-03 22:26:47,"Hi @vaibhavkaul,

Thanks for sending a pull request. I'm afraid we likely won't be accepting it. Here's why:
- This is drastically backwards incompatible behaviour. 
- It takes a great deal of freedom away from the user which we have intentionally placed into their hands.
- This code does not just change this for the `multipart/form-data` case.
- This also prevents people from writing API fuzzers which provide incorrect content-type headers for a totally different kind of content and drastically reduces requests flexibility.
- This affects the cases where people provide something to `data=` or `json=` with a custom content-type header where they expect their explicit decision to specify that header to be honored.

We could consider this for version 3.0.0 but I think it would not fit there either for several of the reasons listed above and also mostly due to the fact that we hope to not break things too drastically in 3.0.

I won't close this just yet because I hope @Lukasa and @kennethreitz will weigh in as well.

Cheers,
Ian
",sigmavirus24,kennethreitz
3029,2016-03-03 22:26:47,"Hi @vaibhavkaul,

Thanks for sending a pull request. I'm afraid we likely won't be accepting it. Here's why:
- This is drastically backwards incompatible behaviour. 
- It takes a great deal of freedom away from the user which we have intentionally placed into their hands.
- This code does not just change this for the `multipart/form-data` case.
- This also prevents people from writing API fuzzers which provide incorrect content-type headers for a totally different kind of content and drastically reduces requests flexibility.
- This affects the cases where people provide something to `data=` or `json=` with a custom content-type header where they expect their explicit decision to specify that header to be honored.

We could consider this for version 3.0.0 but I think it would not fit there either for several of the reasons listed above and also mostly due to the fact that we hope to not break things too drastically in 3.0.

I won't close this just yet because I hope @Lukasa and @kennethreitz will weigh in as well.

Cheers,
Ian
",sigmavirus24,Lukasa
3029,2016-03-04 13:47:45,"@kennethreitz thoughts?
",sigmavirus24,kennethreitz
3029,2016-03-04 16:25:18,"@sigmavirus24 I made the change so it doesn't affect other things besides files.

> Sometimes overriding requests default behaviour is correct for user

Yes, which which is why `sometimes` should not dictate default behavior.

> Will make the correct body with an incorrect header. 

I dont think thats true since for the body to be parseable by any standard multi-part library (cgi etc) it would need to have the correct header. Just having the data in the body does not make it correct. I would argue the body is wrong in this case for a multipart request.

My motive right now is not to get this merged in at all. Happy to close the PR. But we should be clear about what the issue is here. Specifically I would like to address things like:

> That means that one of the cases above prevents the user from doing exactly what they intend to do

and 

> then changing the behaviour to differently silently subvert expectations isn't a great direction

The specific part of code I am referring to deals [exclusively with Multipart Files](https://github.com/kennethreitz/requests/blob/master/requests/models.py#L445-L447). The whole [idea of adding a `file=` param](http://docs.python-requests.org/en/master/user/quickstart/#post-a-multipart-encoded-file) is to encode the request as a multi-part request. `Most` developers using that code path clearly want the request to be posted as multipart.

Multipart requests must [have a boundary specified in the content-type](https://www.w3.org/Protocols/rfc1341/7_2_Multipart.html), if this is missing, the whole purpose for passing a `files=` param is defeated. This is no longer a multipart request. This is not a `correct body with the wrong header`. Clearly the `requests` framework [does not allow a user to pass their own boundary](https://github.com/kennethreitz/requests/blob/master/requests/models.py#L156) (notice the missing boundary kwarg) and use that. It `always` relies on a auto generated boundary string. We should either give people the ability to be in control of the `content-type` (by letting them specify a boundary string) or do the right think in the library.

> In general the requests header dictionary lets people do stupid things. If the user wants to change content-length, they can. If they want to change transfer-encoding, they can

I agree. Devs do stupid things. But then why make [this](https://github.com/kennethreitz/requests/blob/master/requests/models.py#L434-L438) raise an exception. There should at least be a warning somewhere about the request not being encoded as a multi part request if the headers being passed include a `content-type`, even if that content type is `multipart/form-data`.

I completely agree that we should not make this change if you feel this breaks the existing API, so maybe this should be considered for a future release or maybe not. I'll leave it up to @kennethreitz.
",vaibhavkaul,kennethreitz
3023,2016-02-18 17:34:55,"I'm going to leave this up to @kennethreitz. As you stated in #3022, this is totally unnecessary, but neither is it a maintenance burden, so I have no objection to it. Thanks for your contribution! :cake:
",Lukasa,kennethreitz
3022,2016-02-21 01:31:49,"@kennethreitz Resolved this by changing the docs (see PR #3023).

Looks like http://docs.python-requests.org/en/master/api/#exceptions is still missing ConnectTimeout and ReadTimeout. Should they be added to `__init__.py`?
",jtpereyda,kennethreitz
3009,2016-03-06 19:57:56,"Extra datapoint, I've been bitten by this too while interacting with the API of a Spanish-language server. The correct fixes here are, I believe:
1. Properly decode the HTTP Reason Phrase field as described by @denis-ryzhkov in [#1181](https://github.com/kennethreitz/requests/pull/1181#issuecomment-13423623) for header values.
   - According to [RFC2616](https://tools.ietf.org/html/rfc2616#section-6.1.1), Reason Phrase is `TEXT` which defaults to `latin1` and supports other characters under RFC2047.
2. Store the `.reason` attribute as unicode if it isn't already.
3. In `.raise_for_status` use `%r` as @Lukasa suggests (since exception messages are supposed to be bytestrings).
",erydo,denis-ryzhkov
3006,2016-02-12 05:28:54,"Any idea? @Lukasa 
",caizixian,Lukasa
3006,2016-02-21 00:26:22,"@Lukasa @shazow urllib3 always has a home at http://ci.kennethreitz.org, if desired!
",kennethreitz,shazow
3002,2016-02-09 20:55:19,"So this was probably introduced by shazow/urllib3#708. Looks like we'll have to revert that @shazow.

@pneumaticdeath Why Python version are you using?
",Lukasa,shazow
2997,2016-02-05 19:52:19,"Good spot! I suspect this might be the result of the 301 cache misbehaving somewhat. @sigmavirus24, does that sound right to you?
",Lukasa,sigmavirus24
2996,2016-02-05 13:19:26,"Pinging @kennethreitz again for review on test code.

Thanks for this @Stranger6667!
",Lukasa,kennethreitz
2992,2016-02-03 15:07:23,"For general code review I'm not best placed to do it because I suggested the approach to begin with, so let's tag in @sigmavirus24 and @kennethreitz as well.
",Lukasa,kennethreitz
2992,2016-02-03 15:07:23,"For general code review I'm not best placed to do it because I suggested the approach to begin with, so let's tag in @sigmavirus24 and @kennethreitz as well.
",Lukasa,sigmavirus24
2991,2016-02-02 13:12:00,"Thanks for this! I'll handle answering some questions, but I know that @kennethreitz has got opinions about some of the changes you're making here so I'll let him weigh in to do the actual review work.

To your first question, I suspect that test is just poorly written.

To Python 3.2 support, requests explicitly considers Python 3.2 a best-effort platform. There's no reason to continue to support it formally, because Python 3.2 is no longer supported by the PSF: the last release was in 2014. The only reason we appear to be compatible with Python 3.2 is because pip needed to support it, and even pip is planning to drop support.

To Jython: we'd like to work on Jython, but it's moderately painful for us to get a CI suite for it. It's not unreasonable to run the tests against Jython though.

Re: Travis. We moved away from Travis because our test suite used to run against the real httpbin instance, and Travis' network access was sufficiently flaky that our tests failed a lot. We've therefore been using Jenkins for a while. However, now that our tests are entirely offline I'm interested in moving back to Travis. Of course, that's @kennethreitz's call.

For coverage, I see no reason to use anything but coverage.py, but again, this is @kennethreitz's area.
",Lukasa,kennethreitz
2991,2016-02-02 20:52:32,"Regarding Travis: we used to utilize Travis, but I quickly grew frustrated with it reporting false negatives for our test suite on a regular basis. I was also frustrated by them randomly removing a few older versions of Python one day, as well. I was also frustrated by all the email notifications it sent me (many regarding forks of kennethreitz/requests). I'm also not a fan of its UI. :)

That was a long time ago, and I'm sure it's improved quite a bit since then. 

Our current CI integration with my Jenkins server seems to be suiting our needs perfectly fine. I personally quite enjoy having a private Jenkins server. However, I wouldn't be against considering a switch back to Travis if there was an obvious benefit to myself, @Lukasa, or @sigmavirus24. I'm not aware of any. Are there any?
",kennethreitz,sigmavirus24
2987,2016-01-31 08:41:47,"Would be great to include the functionality of `requests_toolbelt.utils.formdata.urlencode` in Requests. 

http://toolbelt.readthedocs.org/en/latest/formdata.html

Thoughts? 

/cc @Lukasa @sigmavirus24 
",kennethreitz,Lukasa
2987,2016-01-31 08:41:47,"Would be great to include the functionality of `requests_toolbelt.utils.formdata.urlencode` in Requests. 

http://toolbelt.readthedocs.org/en/latest/formdata.html

Thoughts? 

/cc @Lukasa @sigmavirus24 
",kennethreitz,sigmavirus24
2987,2016-01-31 08:55:03,"No objection from me, but the tool belt is @sigmavirus24's baby so he may have a stronger position,
",Lukasa,sigmavirus24
2973,2016-01-20 13:54:30,"/cc @dstufft (re: https://github.com/pypa/pip/issues/3384#issuecomment-173202320) although I'm starting to suspect that that is a version of requests packaged by the distro that is causing wichert's problems.
",sigmavirus24,dstufft
2972,2016-01-20 02:21:44,"@Lukasa I'm +0.5 on this. The reason I'm not a full +1 is that we might start leaking sockets this way.
",sigmavirus24,Lukasa
2969,2016-01-24 04:24:04,"Okay that went quicker than expected, based on extrapolation of past activity on my pull request. :) My changes are now integrated into a requests-toolbelt branch, conditioned against a hypothetical 2.10 version of requests containing the latest urllib3 functionality. I assume once requests cuts this release, @sigmavirus24 can then integrate the branch into mainline requests-toolbelt.

I'll leave it to you guys to determine the time-sensitivity of appengine functionality.
",mikelambert,sigmavirus24
2966,2016-01-11 17:15:47,"It's been raised repeatedly, mostly by people using Linux systems, that it's annoying that requests doesn't use the system trust store and instead uses the one that certifi ships. This is an understandable position. I have some personal attachment to the certifi approach, but the other side of that argument definitely has a reasonable position too. For this reason, I'd like us to look into whether we should use the system trust store by default, and make certifi's bundle a fallback option.

I have some caveats here:
1. If we move to the system trust store, we must do so on _all_ platforms: Linux must not be its own special snowflake.
2. We must have broad-based support for Linux and Windows.
3. We must be able to fall back to certifi cleanly.

Right now it seems like the best route to achieving this would be to use [certitude](https://github.com/python-hyper/certitude). This currently has support for dynamically generating the cert bundle OpenSSL needs directly from the system keychain on OS X. If we added Linux and Windows support to that library, we may have the opportunity to switch to using certitude.

Given @kennethreitz's bundling policy, we probably cannot unconditionally switch to certitude, because certitude depends on cryptography (at least on OS X). However, certitude could take the current privileged position that certifi takes, or be a higher priority than certifi, as an optional dependency that is used if present on the system.

Thoughts? This is currently a RFC, so please comment if you have opinions. /cc @sigmavirus24 @alex @kennethreitz @dstufft @glyph @reaperhulk @morganfainberg
",Lukasa,kennethreitz
2966,2016-01-11 17:15:47,"It's been raised repeatedly, mostly by people using Linux systems, that it's annoying that requests doesn't use the system trust store and instead uses the one that certifi ships. This is an understandable position. I have some personal attachment to the certifi approach, but the other side of that argument definitely has a reasonable position too. For this reason, I'd like us to look into whether we should use the system trust store by default, and make certifi's bundle a fallback option.

I have some caveats here:
1. If we move to the system trust store, we must do so on _all_ platforms: Linux must not be its own special snowflake.
2. We must have broad-based support for Linux and Windows.
3. We must be able to fall back to certifi cleanly.

Right now it seems like the best route to achieving this would be to use [certitude](https://github.com/python-hyper/certitude). This currently has support for dynamically generating the cert bundle OpenSSL needs directly from the system keychain on OS X. If we added Linux and Windows support to that library, we may have the opportunity to switch to using certitude.

Given @kennethreitz's bundling policy, we probably cannot unconditionally switch to certitude, because certitude depends on cryptography (at least on OS X). However, certitude could take the current privileged position that certifi takes, or be a higher priority than certifi, as an optional dependency that is used if present on the system.

Thoughts? This is currently a RFC, so please comment if you have opinions. /cc @sigmavirus24 @alex @kennethreitz @dstufft @glyph @reaperhulk @morganfainberg
",Lukasa,dstufft
2966,2016-01-11 17:15:47,"It's been raised repeatedly, mostly by people using Linux systems, that it's annoying that requests doesn't use the system trust store and instead uses the one that certifi ships. This is an understandable position. I have some personal attachment to the certifi approach, but the other side of that argument definitely has a reasonable position too. For this reason, I'd like us to look into whether we should use the system trust store by default, and make certifi's bundle a fallback option.

I have some caveats here:
1. If we move to the system trust store, we must do so on _all_ platforms: Linux must not be its own special snowflake.
2. We must have broad-based support for Linux and Windows.
3. We must be able to fall back to certifi cleanly.

Right now it seems like the best route to achieving this would be to use [certitude](https://github.com/python-hyper/certitude). This currently has support for dynamically generating the cert bundle OpenSSL needs directly from the system keychain on OS X. If we added Linux and Windows support to that library, we may have the opportunity to switch to using certitude.

Given @kennethreitz's bundling policy, we probably cannot unconditionally switch to certitude, because certitude depends on cryptography (at least on OS X). However, certitude could take the current privileged position that certifi takes, or be a higher priority than certifi, as an optional dependency that is used if present on the system.

Thoughts? This is currently a RFC, so please comment if you have opinions. /cc @sigmavirus24 @alex @kennethreitz @dstufft @glyph @reaperhulk @morganfainberg
",Lukasa,alex
2966,2016-01-11 17:15:47,"It's been raised repeatedly, mostly by people using Linux systems, that it's annoying that requests doesn't use the system trust store and instead uses the one that certifi ships. This is an understandable position. I have some personal attachment to the certifi approach, but the other side of that argument definitely has a reasonable position too. For this reason, I'd like us to look into whether we should use the system trust store by default, and make certifi's bundle a fallback option.

I have some caveats here:
1. If we move to the system trust store, we must do so on _all_ platforms: Linux must not be its own special snowflake.
2. We must have broad-based support for Linux and Windows.
3. We must be able to fall back to certifi cleanly.

Right now it seems like the best route to achieving this would be to use [certitude](https://github.com/python-hyper/certitude). This currently has support for dynamically generating the cert bundle OpenSSL needs directly from the system keychain on OS X. If we added Linux and Windows support to that library, we may have the opportunity to switch to using certitude.

Given @kennethreitz's bundling policy, we probably cannot unconditionally switch to certitude, because certitude depends on cryptography (at least on OS X). However, certitude could take the current privileged position that certifi takes, or be a higher priority than certifi, as an optional dependency that is used if present on the system.

Thoughts? This is currently a RFC, so please comment if you have opinions. /cc @sigmavirus24 @alex @kennethreitz @dstufft @glyph @reaperhulk @morganfainberg
",Lukasa,sigmavirus24
2966,2016-01-11 17:37:49,"I think the system trust stores (or not) essentially boils down to whether you want requests to act the same across platforms, or whether you want it to act in line with the platform it is running on. I do not think that _either_ of these options are wrong (or right), just different trade offs.

I think that it's not as simple on Windows as it is on Linux or OSX (although @tiran might have a better idea). I _think_ that Windows doesn't ship with all of the certificates available and you have to do something (use WinHTTP?) to get it to download any additional certificates on demand. I think that means that a brand new Windows install, if you attempt to dump the certificate store will be missing a great many certificates.

On Linux, you still have the problem that there isn't one single set location for the certificate files, the best you can do is try to heuristically guess at where it might be. This gets better on Python 2.7.9+ and Python  3.4+ since you can use `ssl.get_default_verify_paths()` to get what the default paths are, but you can't rely on that unless you drop 2.6, 2.7.8, and 3.3. In pip we attempt to discover the location of the system trust store (just by looping over some common file locations) and if we can't find it we fall back to certifi, and one problem that has come up is that sometimes we'll find a location, but it's an old outdated copy that isn't being updated by anything. People then get really confused because it works in their browser, it works with requests, but it doesn't in pip.

I assume the fall back to certifi ensures that things will still work correctly on platforms that either don't ship certificates at all, or don't ship them by default and they aren't installed? If so, that's another possible niggle here that you'd want to think about. Some platforms, like say FreeBSD, don't ship them by default at all. So it's possible that people will have a requests using thing running just fine without the FreeBSD certificates installed, and they then install them (explicitly or implicitly) and suddenly they trust something different and the behavior of the program changes.

Anyways, the desire seems reasonable to me and, if all of the little niggles get worked out, it really just comes down to a question of if requests wants to fall on the side of ""fitting in"" with a particular platform, or if it wants to prefer cross platform uniformity.
",dstufft,tiran
2966,2016-01-11 22:22:45,"> I think that it's not as simple on Windows as it is on Linux or OSX (although @tiran might have a better idea). I think that Windows doesn't ship with all of the certificates available and you have to do something (use WinHTTP?) to get it to download any additional certificates on demand. I think that means that a brand new Windows install, if you attempt to dump the certificate store will be missing a great many certificates.

You are right about this.  I have verified it on my nearly-pristine Windows VM; in a Python prompt, I do:



and get ""21"".  Visit some HTTPS websites, up-arrow/enter in the python interpreter, and now I get ""23"".
",glyph,tiran
2966,2016-01-29 14:58:56,"@mwcampbell PyOpenSSL does _not_ support this, and never will, because PyOpenSSL is a thin wrapper library around OpenSSL, and so doesn't support the relevant APIs.

However, I recently got merged into cryptography the relevant bindings for OS X (pyca/cryptography#2683), and I believe that a cryptography with that change in it has been released now. I've also used those bindings to successfully use OS X to validate a certificate chain. A similar approach can probably be used on Windows and the cryptography developers have expressed a willingness to bind the appropriate functions. Given that PyOpenSSL depends on cryptography, this is far and away the simplest route.

My current proposal is to add the relevant functionality into the urllib3 PyOpenSSL shim. I've briefly discussed this with @shazow, who was open to the idea. Then, urllib3 would allow `urllib3.contrib.pyopenssl.inject_into_urllib3()` to take a parameter (`system_trust=True`, defaulting to `False`) that will automatically use the system trust store instead of OpenSSL on the relevant platforms.

The question then would become whether the requests project can come to consensus on setting that parameter to `True`. =) We should burn that bridge when we get to it: for now, I'd like to get the building blocks in place.

In an ideal world we'd actually pull the OS-specific logic out into its own library, so that it can be meaningfully tested, but that's a pretty tricky goal. On the other hand, if we can pull it off then we have both provided a really useful service to the Python community in general _and_ potentially helped move towards #2118 by providing a PyOpenSSL `SSLContext` equivalent. This I think would be my preferred outcome.
",Lukasa,shazow
2953,2016-04-06 19:14:24,"We need an updated urllib3. I think @shazow is expecting to release this week.
",Lukasa,shazow
2947,2015-12-28 11:20:19,"Ugh, yeah, good spot. This isn't really ideal: while what occurs here is kinda exactly what you'd expect if you have an understanding of the system, it provides a nice shiny footgun for developers who aren't expecting it, potentially allowing for header injection if they're passing user-provided data directly to a different service. We should probably remove it.

In general, I'm inclined to want to throw errors here, resisting the temptation to guess, but @sigmavirus24 or @kennethreitz may disagree and instead want us to simply strip line terminating characters from headers.
",Lukasa,kennethreitz
2947,2015-12-28 11:20:19,"Ugh, yeah, good spot. This isn't really ideal: while what occurs here is kinda exactly what you'd expect if you have an understanding of the system, it provides a nice shiny footgun for developers who aren't expecting it, potentially allowing for header injection if they're passing user-provided data directly to a different service. We should probably remove it.

In general, I'm inclined to want to throw errors here, resisting the temptation to guess, but @sigmavirus24 or @kennethreitz may disagree and instead want us to simply strip line terminating characters from headers.
",Lukasa,sigmavirus24
2941,2015-12-20 10:15:40,"Hi @alison985!

Currently our relatively minor code of conduct is [defined here](http://docs.python-requests.org/en/latest/dev/contributing/). There is interest amongst some of the maintainers in moving to something based on the [Contributor Covenant](http://contributor-covenant.org/), given that both myself and @sigmavirus24 maintain personal projects that use it, but I'm unwilling to change that by fiat: I'd want consensus from the whole team.

Regardless, I believe that the Contributor Covenant _implicitly_ applies to this project, and certainly that's the standard to which the maintainers hold themselves.
",Lukasa,sigmavirus24
2939,2015-12-19 14:34:36,"A web server MUST NOT respond with multiple location header fields. From [RFC 7320 Section 3.2.2](https://tools.ietf.org/html/rfc7230#section-3.2.2):

> A sender MUST NOT generate multiple header fields with the same field name in a message unless either the entire field value for that header field is defined as a comma-separated list [i.e., #(values)] or the header field is a well-known exception (as noted below).

There is exactly _one_ well-known exception: `Set-Cookie`. `Location` is not `Set-Cookie`, so that doesn't apply. So it would only be acceptable to send multiple `Location` headers if the syntax allowed multiple comma-separated values. `Location` is defined in [RFC 7231 Section 7.1.2](https://tools.ietf.org/html/rfc7231#section-7.1.2):

>    Location = URI-reference
> 
> The field value consists of a single URI-reference.

The server in this example is violating the HTTP specification. In the face of such violation, a user-agent is simply not required to take any specific action. In particular, while the example you provided above has a very simple course of action (both URLs are the same), if both URLs were _not_ the same requests would be unable to take any sensible action: the situation is too ambiguous to allow a correct decision.

At the level of requests actually catching this and treating it as an error condition is tricky, because requests only sees the header string `http://foo.com,http://foo.com`, which perversely does parse as a valid URL in the case of most Python libraries because in principle the comma is an allowed character in the host part of the URL. This puts us in a bit of a bind. However, `urllib3`'s `HTTPHeaderDict` structure could enforce this requirement and treat this case as an error on receiving it. That might be a sensible way to go, but it's us going out of our way to provide a slightly nicer error when a server does something it really really shouldn't do. @sigmavirus24, thoughts?
",Lukasa,sigmavirus24
2937,2015-12-18 09:56:33,"This release is scheduled for Monday. I'm aiming to set this up now so that it's an easy release to push on Monday.

@shazow, what are the odds that we can get a urllib3 patch release 1.13.1 containing the fix for shazow/urllib3#761 by Monday? I'd owe you lots of :custard: if we could get it! :heart:
",Lukasa,shazow
2937,2015-12-18 14:19:59,"@sigmavirus24 Yeah, good question, but I think I am, given that the urllib3 release would itself be a patch release. Any concerns there? @ralphbean @eriol?
",Lukasa,ralphbean
2937,2015-12-18 14:19:59,"@sigmavirus24 Yeah, good question, but I think I am, given that the urllib3 release would itself be a patch release. Any concerns there? @ralphbean @eriol?
",Lukasa,eriol
2931,2015-12-16 15:16:38,"@untitaker has confirmed this fix works for them.
",Lukasa,untitaker
2929,2015-12-16 13:50:15,"@sigmavirus24 You're probably best placed to look at this, but my suspicion is that there's an unexpected interaction between the toolbelt's MultipartEncoder and the fix we put in for calculating the actual length of file-like objects that are not seeked to the beginning. That said, I can't find anything in that.
",Lukasa,sigmavirus24
2927,2015-12-15 14:59:42,"@sigmavirus24 Can you confirm for me that you're happy with these release notes?
",Lukasa,sigmavirus24
2919,2015-12-07 08:07:42,"Yeah, so how we approach this is basically dependent on whether we think it's acceptable for `to_native_string()` to go in `encode_params`. I personally think that's the best place for it, because it allows us to avoid duplicating the large amount of knowledge `encode_params` has about how this variable is structured. However, IIRC @sigmavirus24 originally objected to that idea so I'd like to hear from him.
",Lukasa,sigmavirus24
2915,2015-12-04 08:22:53,"Feedback is welcome! :grinning: 
## Before with @kennethreitz's sphinx theme

![before_kr_sphinx_theme](https://cloud.githubusercontent.com/assets/954858/11584924/f6321290-9a1c-11e5-890c-11a20cf3ff8d.png)
## After with alabaster sphinx theme

![alabaster_theme_for_requests](https://cloud.githubusercontent.com/assets/954858/11672701/a4a17548-9dc7-11e5-8038-d2ca698b8aa9.png)
",ArcTanSusan,kennethreitz
2915,2015-12-04 08:25:56,"\o/ You are a :star: @onceuponatimeforever!

For posterity, this resolves #2779. Based on the quick screenshot provided above it looks like the only thing that gets lost here is that the Gumroad button isn't rendering out correctly.

@kennethreitz, you'll need to review this because the look of the site is an important part of your personal branding, but FWIW I'm :+1: and totally delighted about it.
",Lukasa,kennethreitz
2915,2015-12-04 12:51:45,"Thanks @onceuponatimeforever! This looks good to me.

I'm going to sit on it to confirm that @kennethreitz is happy with it. (I'm confident he will be.)
",Lukasa,kennethreitz
2912,2015-12-02 08:36:58,"This is not a bug in URL parsing: quite the opposite.

Requests does its best to normalise URLs where it is safe to do so. This is necessary to ensure that various stages of URL building work appropriately, and that we don't accidentally end up with invalid headers (e.g. multiple question marks or none at all when parameters are present). A trailing question mark with no parameters is superfluous (and arguably somewhat invalid), so requests kindly strips it off for you. This is entirely intentional.

For what it's worth, my opinion is that this API is poorly designed. It relies on the idea that no middle-box will rewrite that URL to remove the empty query part, which is a risk. Generally speaking, the better API would be to have a sub-resource or a proper query string (e.g. `/api/item.json?metadata=true`, or `/api/item/metadata.json`). This API design is _super_ fragile, and I guarantee that it'll be prone to breaking in mysterious ways. I doubt we're the only framework that strips it. In fact, anything that relies on Python's standard `urlsplit` function will probably do exactly what we do and ignore the query part.

**However**, both browsers and curl will, if requested, send the empty query string. Having chatted to @bagder I don't think this is a deliberate decision on the part of those entities, so much as it just happens to fall out of the way they handle query strings.

Given that we have a duty to be better than the Python standard library, that means I think we should come up with a way to make it possible, at least when using the PreparedRequest API. Sadly, `urlsplit` makes this _really_ hard for us, because it doesn't have a difference in the query portion of the URL for `http://http2bin.org/get' and`http://http2bin.org/get?`. That makes our lives really tricky because I'd rather not hand roll URL parsing if I can possibly avoid it.

I think the only way we can fix this is to change the library we use to parse URLs to one that can safely inform us of the difference between these two cases. @sigmavirus24, do you think your URL parsing library would do better here?

I think this is a reasonable request, but we can't get to it before 3.0.0 because moving to a new URL parsing library will probably break a whole lot of stuff.
",Lukasa,sigmavirus24
2912,2015-12-02 11:34:51,"Your understanding is correct, at least as far as the _standard library_ URL parser goes. That is not necessarily _generally_ true of URL parsers, which is why I asked @sigmavirus24 if his URL parser has the same behaviour or not. In particular, it's not clear to me that an absent query part is semantically identical to an _empty_ query part: Appendix A of RFC 3986 does appear to allow zero-length query strings.

Again, I think this is _strictly_ acceptable, but it's likely to be extremely fragile.
",Lukasa,sigmavirus24
2897,2016-03-01 08:29:25,"I believe this is generally going in the right direction, but now that @kennethreitz is more active he may want to see if this is something he wants. @sigmavirus24 and I are definitely :+1: on this for improving our testing, but it's up to Kenneth.
",Lukasa,kennethreitz
2896,2015-11-24 12:46:04,"@BraulioVM That's actually intentional: sending a chunked zero-length stream is fine, there won't be a problem there. That way, the code is clearer. =)

So, this change looks good to me, but I'll let @sigmavirus24 review it.
",Lukasa,sigmavirus24
2896,2015-12-02 08:09:20,"@BraulioVM Nope, just waiting for @sigmavirus24 to get enough time to swing by and take a look at it. =)
",Lukasa,sigmavirus24
2886,2015-11-17 15:21:25,"This is definitely a duplicate but I don't have the time to find the exact bug right now. The fix is already merged in urllib3 iirc but @Lukasa can correct me.
",sigmavirus24,Lukasa
2872,2015-11-10 22:28:56,"Thanks for this report. This comes because we calculate the length of the StringIO, but don't account for where it is seeked to. 

As I see it we have two options: we can either always `seek(0)` before sending a body, or we can use `tell()` to adjust the content length. 

On balance, I think the first is probably better: we already seek(0) in some cases (e.g. auth handlers), so this would be consistent. The downside is that it makes it harder for someone to upload a partial file from disk, though I suspect that's a minor use-case. 

@sigmavirus24 thoughts?
",Lukasa,sigmavirus24
2870,2015-11-10 12:49:34,"Originally reported by @fasaxc. Interested parties: @sigmavirus24 @dcramer @eriol @ralphbean @warsaw 

We added support for a new form of unbundling logic in #2567. Unfortunately, this seems not to work properly. Specifically, the presence of requests in a system makes it impossible to catch urllib3 exceptions correctly: see [this Stack Overflow question](https://stackoverflow.com/questions/33516164/why-cant-i-catch-this-python-exception) for more.

This problem seems to specifically affect the urllib3 sub-packages, not urllib3 itself. One proposed change is to add every urllib3 sub-package that gets imported to the list of modules requests explicitly imports in this stub file. This is pretty gross, but might work. Does anyone have other suggestions?
",Lukasa,ralphbean
2870,2015-11-10 12:49:34,"Originally reported by @fasaxc. Interested parties: @sigmavirus24 @dcramer @eriol @ralphbean @warsaw 

We added support for a new form of unbundling logic in #2567. Unfortunately, this seems not to work properly. Specifically, the presence of requests in a system makes it impossible to catch urllib3 exceptions correctly: see [this Stack Overflow question](https://stackoverflow.com/questions/33516164/why-cant-i-catch-this-python-exception) for more.

This problem seems to specifically affect the urllib3 sub-packages, not urllib3 itself. One proposed change is to add every urllib3 sub-package that gets imported to the list of modules requests explicitly imports in this stub file. This is pretty gross, but might work. Does anyone have other suggestions?
",Lukasa,eriol
2870,2015-11-10 12:49:34,"Originally reported by @fasaxc. Interested parties: @sigmavirus24 @dcramer @eriol @ralphbean @warsaw 

We added support for a new form of unbundling logic in #2567. Unfortunately, this seems not to work properly. Specifically, the presence of requests in a system makes it impossible to catch urllib3 exceptions correctly: see [this Stack Overflow question](https://stackoverflow.com/questions/33516164/why-cant-i-catch-this-python-exception) for more.

This problem seems to specifically affect the urllib3 sub-packages, not urllib3 itself. One proposed change is to add every urllib3 sub-package that gets imported to the list of modules requests explicitly imports in this stub file. This is pretty gross, but might work. Does anyone have other suggestions?
",Lukasa,sigmavirus24
2870,2015-11-10 14:29:56,"So 2.6.x has the old VendorAlias logic from pip. 2.7.0 had nothing but some distros (Debian specifically, more specifically @warsaw) backported the patch from @untitaker that was released in 2.8.0. So version information is **very** important here.
",sigmavirus24,untitaker
2870,2015-11-10 15:51:23,"On Nov 10, 2015, at 06:24 AM, Cory Benfield wrote:

> Oh that's a good point, did this get backported into the distros?

Don't forget, @eriol is really doing most of excellent work on this package
for Debian!

Our 2.8.1-1 removed the devendorizing patch we were carrying separately, so
these days we are really only carrying a few deltas from upstream:

https://anonscm.debian.org/cgit/python-modules/packages/requests.git/tree/debian/patches

Of course, we prefer to reduce the differences between the Debian version and
upstream.
",warsaw,eriol
2870,2015-11-10 15:54:56,"Lukasa, who do you mean exactly when you say ""the whole team""? Isn't Kenneth some sort of BDFL, so his decision would suffice?

On 10 November 2015 16:51:56 CET, Barry Warsaw notifications@github.com wrote:

> On Nov 10, 2015, at 06:24 AM, Cory Benfield wrote:
> 
> > Oh that's a good point, did this get backported into the distros?
> 
> Don't forget, @eriol is really doing most of excellent work on this
> package
> for Debian!
> 
> Our 2.8.1-1 removed the devendorizing patch we were carrying
> separately, so
> these days we are really only carrying a few deltas from upstream:
> 
> https://anonscm.debian.org/cgit/python-modules/packages/requests.git/tree/debian/patches
> 
> Of course, we prefer to reduce the differences between the Debian
> version and
> upstream.
> 
> ---
> 
> Reply to this email directly or view it on GitHub:
> https://github.com/kennethreitz/requests/issues/2870#issuecomment-155460186

## 

Sent from my phone. Please excuse my brevity.
",untitaker,eriol
2870,2015-11-11 09:11:32,"@Lukasa I'm sad to confirm but yes, the problem actually exists in Debian testing (with the new import machinery).
I tried the same setup of https://github.com/kennethreitz/requests/issues/2867 using only system packages (so I took python-etcd from sid) and I can confirm it.
I looked at python-etcd code, but the problem was already explained  by @fasaxc and is about exceptions.

On Debian this is what I got:



I'm real sorry I did not noticed when we discussed https://github.com/kennethreitz/requests/pull/2567.

I'm not neither an import logic expert, but your proposal seems the only way to fix this, without unvendoring. Maybe we can ask to @brettcannon if he can take a look at this: I can volunteer to recap all the story so far.

Also, I just want to say thanks to you, @sigmavirus24 and @kennethreitz because although you can just mark this as wontfix, we started working together talking without forget that we are all human beings. 
So, thank you.
",eriol,kennethreitz
2870,2015-11-17 14:14:16,"So I'm enumerating the options and asking questions to make sure I understand correctly.

> you can just patch every module in `urllib3`

As in, ensuring that `sys.modules` has both `urllib3.submodule` and `requests.packages.urllib3.submodule`? I think this is the least magic way and given that @Lukasa and I are both core developers of urllib3, we will catch any new submodules that need to be added to requests' patching logic. I'm most strongly in favor of this one.

> Another option is a custom importer that handles just this case, but I don't know if you want such a magical solution.

We had that option and it broke many a thing. I'm not in favor of going back down that route.

> insert an object into `sys.modules` who uses `__getattribute__` to direct to the proper module and handle all magical tweaks to `sys.modules`. 

Hm. This sounds like something in the vein of option 2 but a little less magic-y. It still feels like some abuse of the module system and like it might cause us problems. If we come up with something like this, I would appreciate it if @fasaxc and @eriol would commit to testing this with python-etcd and making sure things still aren't broken before shipping a release with it.

> And lastly, the simplest solution is either let Debian handle their own desire to monkeypatch vendored code or stop vendoring stuff entirely.

Breaking this into two sub-suggestions:
- ""let Debian handle their own desire to monkey patch vendored code"" No. I don't want @eriol to have to figure that out. Vendoring is a position of this project. As much as I dislike downstreams unvendoring things, it isn't the individual maintainer's fault or decision and I don't want to push this work onto their backs.
- ""stop vendoring stuff entirely"" Many people constantly badger (or even, at times, yell) this at us (I know you're not Brett) but it simply just won't happen like that (if it happens at all).

---

My 2¢:

We can take a short term solution of the first option for a `2.8.2` release and get that updated patch to @eriol, @warsaw, @ralphbean, etc. so distros can fix their packages while taking a look at the feasibility and reliability of option 3. I still prefer the first option to the third, but the third may have benefits I'm not seeing due to the bad taste left in my mouth by the `VendorAlias` (a.k.a., option 2).
",sigmavirus24,ralphbean
2870,2015-11-17 15:36:29,"On Nov 17, 2015, at 06:14 AM, Ian Cordasco wrote:

> As in, ensuring that `sys.modules` has both `urllib3.submodule` and
> `requests.packages.urllib3.submodule`? I think this is the least magic way
> and given that @Lukasa and I are both core developers of urllib3, we will
> catch any new submodules that need to be added to requests' patching
> logic. I'm most strongly in favor of this one.

It's mildly disconcerting that a library would fiddle with another library's
sys.modules namespace, but I agree that this is probably the least magical
(and thus most likely to work) way.  Maybe we can convince @brettcannon to
build a nicer foolproof <wink> module alias system for 3.6. :)

> We can take a short term solution of the first option for a `2.8.2` release
> and get that updated patch to @eriol, @warsaw, @ralphbean, etc. so distros
> can fix their packages while taking a look at the feasibility and reliability
> of option 3. I still prefer the first option to the third, but the third may
> have benefits I'm not seeing due to the bad taste left in my mouth by the
> `VendorAlias` (a.k.a., option 2).

Yep, I suspect that #1 may be the best approach, and that #3 would be
difficult to debug if things go south.  It may be that no solution is perfect,
given Python's current import semantics and implementation, in which case
doing the best you can with the least magic (i.e. most discoverable and
debuggable) would suck less.

Thanks!
",warsaw,ralphbean
2868,2015-11-09 15:06:41,"If I know @jamielennox, they're working on SAML auth plugin. Part of (some of) the SAML authentication flow(s) includes cookies being sent that aren't meant to be persisted on the session.

I think what @jamielennox wants is for those cookies that are meant to be part of an intermediate step in authentication not to be persisted. After a series of redirects, we process everything and update the Session cookie jar with the cookies from individual responses in the history. I suspect @jamielennox was removing the cookies individually from the intermediate responses hoping that would prevent them from being persisted on the Session but that isn't working because we don't use those attributes when extracting cookies to a session cookie jar. (Further, this is likely a plugin for OpenStack.)

I don't think there is necessarily a bug here. We're doing the right thing for the 99% case.

@jamielennox I suspect the cookie names are predictable and you could have a CookieJar that ignores those cookie values (along the lines of a far more selective ForgetfulCookieJar which @ceaess is adding to the requests-toolbelt).
",sigmavirus24,ceaess
2863,2015-11-06 17:16:29,"So, @shazow, I think really we need to adjust the connection pooling logic in urllib3 a bit. Keying connections off the URL isn't really enough: for HTTPS connections we need to key them off the various connection state parameters as well (cert_reqs, ca_certs, ca_cert_dir, client cert parameters). That's pretty gross because it's a major API change.

Thoughts?
",Lukasa,shazow
2861,2015-11-07 10:21:08,"I honestly don't know what to do about this. To get a test that confirms we don't break this we need either to change the WSGI server used by pytest-httpbin (the only options are ones that are _really_ heavyweight and probably not suitable) or start writing new tests that don't use a httpbin at all.

The second is plenty do-able, it just requires some py.test test fixtures and some socket work. But it makes me a bit nervous given @kennethreitz's attitude to changing our test layout in the past: I'd want to move all the tests to a `test/` subdirectory so that I can add the necessary `conf.py` file, and also start writing new tests that explicitly use sockets rather than a web server.

@kennethreitz, do you have any objection to me adding some more thorough testing around here? The reality is that our chunked upload logic is _entirely_ untested, and has been since it was originally written. That is a recipe for accidental breakage, and it makes me really nervous.
",Lukasa,kennethreitz
2836,2015-10-20 14:46:59,"Ah, yes, I see the problem now.

We have a similar issue with proxy settings, where proxies set in the environment can override proxies set on the session. I'm beginning to wonder if we can fix two problems in one go here, by taking the `trust_env` block of `merge_environment_settings` and moving it below the `merge_setting` lines in that function.

@causton81 This is definitely a real bug, but we need a bit of time to work out what the fix is and what branch it'll go on to (while this is strictly a bug fix, it's been in the product long enough that it's also an API compatibility break and needs to be dealt with cautiously).

@sigmavirus24 What do you think about my proposed rearrangement of `merge_environment_settings`?
",Lukasa,sigmavirus24
2833,2015-10-17 08:40:29,"Iiiinteresting. So @causton81, I think you're right. Diving into the `httplib` source code shows this:



So, I think you're right and this doesn't work. I think this would be best implemented in `urllib3`, frankly: in urllib3 they can take advantage of the `HTTPResponse.length` parameter that `httplib` maintains (but is weirdly refusing to use) to make sure this works in almost all cases. @shazow does that sound reasonable to you?
",Lukasa,shazow
2829,2015-10-15 14:02:13,"This seems like a sensible change to me, I'm :+1:. @sigmavirus24?
",Lukasa,sigmavirus24
2825,2015-10-14 12:24:27,"Resolves #2725.

@sigmavirus24 I'm wondering if we should add a warning to `super_len` for whenever we decide on the length of the file using `os.fstat`, if the file has not been opened in binary mode. It should be easy enough to do. Thoughts?
",Lukasa,sigmavirus24
2822,2015-10-12 16:36:11,"The tag not existing is something @Lukasa  can fix immediately
",sigmavirus24,Lukasa
2821,2015-10-12 09:52:41,"@sigmavirus24 I'd like to slip this into 2.8.1, which I plan to release tomorrow. Mind doing a quick review before then?
",Lukasa,sigmavirus24
2818,2015-12-01 22:27:48,"Yup. We're on track, but @shazow is taking a well-deserved birthday break so it'll be a little bit. 
",Lukasa,shazow
2816,2015-10-09 14:48:30,"/cc @ralphbean @eriol

OpenStack has encountered yet another breakage caused by people combining `pip` installed and system packages: specifically, updating urllib3 via `pip` when requests/urllib3 are already installed via the system packages.

@dstufft tells me that, if the system packages correctly populate the `install_requires` part of `setup.py`, `pip` would respect those requirements. Previously this wasn't possible because requests would include mid-release versions of urllib3, but as of v2.6.2 our [new release rules](http://docs.python-requests.org/en/latest/community/release-process/) mean that we only use proper urllib3 releases.

For this reason, I think it would be extremely good if downstream unbundlers (most notably Fedora and Debian), when they unbundle requests, would populate setup.py with the correct `install_requires`. For 2.8.0, that would be `urllib3==1.12`. For 2.7.0, that would be `urllib3==1.10.4`. This should ensure that `pip` won't casually install a version of urllib3 that will break the system requests package.

Thoughts?
",Lukasa,ralphbean
2816,2015-10-09 14:48:30,"/cc @ralphbean @eriol

OpenStack has encountered yet another breakage caused by people combining `pip` installed and system packages: specifically, updating urllib3 via `pip` when requests/urllib3 are already installed via the system packages.

@dstufft tells me that, if the system packages correctly populate the `install_requires` part of `setup.py`, `pip` would respect those requirements. Previously this wasn't possible because requests would include mid-release versions of urllib3, but as of v2.6.2 our [new release rules](http://docs.python-requests.org/en/latest/community/release-process/) mean that we only use proper urllib3 releases.

For this reason, I think it would be extremely good if downstream unbundlers (most notably Fedora and Debian), when they unbundle requests, would populate setup.py with the correct `install_requires`. For 2.8.0, that would be `urllib3==1.12`. For 2.7.0, that would be `urllib3==1.10.4`. This should ensure that `pip` won't casually install a version of urllib3 that will break the system requests package.

Thoughts?
",Lukasa,eriol
2816,2015-10-09 14:48:30,"/cc @ralphbean @eriol

OpenStack has encountered yet another breakage caused by people combining `pip` installed and system packages: specifically, updating urllib3 via `pip` when requests/urllib3 are already installed via the system packages.

@dstufft tells me that, if the system packages correctly populate the `install_requires` part of `setup.py`, `pip` would respect those requirements. Previously this wasn't possible because requests would include mid-release versions of urllib3, but as of v2.6.2 our [new release rules](http://docs.python-requests.org/en/latest/community/release-process/) mean that we only use proper urllib3 releases.

For this reason, I think it would be extremely good if downstream unbundlers (most notably Fedora and Debian), when they unbundle requests, would populate setup.py with the correct `install_requires`. For 2.8.0, that would be `urllib3==1.12`. For 2.7.0, that would be `urllib3==1.10.4`. This should ensure that `pip` won't casually install a version of urllib3 that will break the system requests package.

Thoughts?
",Lukasa,dstufft
2816,2015-10-11 15:21:27,":heart: Indeed, thanks so much (and thanks @warsaw and @dstufft as well!).
",Lukasa,dstufft
2812,2015-10-08 13:11:04,"@sigmavirus24 How do you feel about removing this in 3.0.0?
",Lukasa,sigmavirus24
2811,2015-10-08 13:06:18,"Also cc'ing @shazow 
",sigmavirus24,shazow
2808,2015-10-07 21:15:55,"I could be mistaken but I'm rather certain that we don't accept additions to the certificate bundle directly. They have to go through [certifi](/certifi/python-certifi) first. @Lukasa am I right?
",sigmavirus24,Lukasa
2807,2015-10-07 16:03:46,"It's certainly the _coded_ behaviour: if you don't use something that goes through `Session.request` then you never get the environment variables.

However, I think this is unexpected. It doesn't seem like there's any reason we couldn't move the call to `Session.merge_environment` into `Session.send`, so I'm inclined to say we should do that. Unfortunately, it's a bit backward-incompatible IMO, so I'd want to sit on this until 2.9.0 or, more likely, 3.0.0. Thoughts @sigmavirus24?
",Lukasa,sigmavirus24
2804,2015-10-05 16:13:21,"@sigmavirus24 Happy for you to merge this when you think things look good.
",Lukasa,sigmavirus24
2786,2015-09-27 00:47:24,"Paging @shazow to make sure we don't step on any toes here with urllib3 by accident.
",sigmavirus24,shazow
2785,2015-09-26 23:40:12,"Eh, I'm 50/50 on this. I agree the default user-agent isn't ideal. I don't agree that this is a large problem. I'd like to hear @Lukasa's opinion on this but he's away on vacation (as I should be).
",sigmavirus24,Lukasa
2785,2015-09-30 15:33:26,"My general position on this is that removing the kernel version is a good idea and we should do it. Other UA strings do not include it, and it's unlikely to be of much use in any role other than attacking a Linux kernel directly, so I'd be happy to strip it.

I'm +0 on removing the Python version, though we need to confirm with @dstufft that pip's not relying on our use of it. I don't believe it's a serious attack vector, but neither is it information that it's vital to be sending in the UA string.

IMO the biggest security risk in there is actually the _requests_ version, and that's the one thing it's hard to justify removing. ;)
",Lukasa,dstufft
2773,2015-09-12 18:35:48,"@jwilk This is certainly a thing we could do. However, it's a bizarrely application specific fix that will not actually help in a lot of cases because applications that use requests will need to opt-in to that functionality. This means they need to know enough to do that, which is not likely.

Really from a security perspective we should switch to disable netrc auth by default (a change that would need to wait until a 3.0.0 release because it's backwards incompatible, though potentially something worth doing).

In the short term, you will get more security either by not using `~/.netrc` files at all (thereby removing the source of the vulnerability altogether) or by constructing AppArmor profiles that limit access to the file to those applications you have pre-authorized to use it.

@sigmavirus24 For the longer term, I'm open to swapping our default here, which is arguably somewhat insecure, though I also just think people shouldn't be writing their passwords down anywhere at all, at least not in plaintext.
",Lukasa,sigmavirus24
2771,2015-09-11 16:40:54,"This is not a totally unreasonable thing to do, though it's totally tragic that we need it: if we didn't support Python 2.6 this wouldn't be a problem. @sigmavirus24?
",Lukasa,sigmavirus24
2771,2015-11-05 10:05:49,"@shazow If I open a PR that does this for urllib3, would you merge it?
",Lukasa,shazow
2763,2015-09-08 16:37:08,"Ugh, you're right, our code sets data to the empty dict. Nevermind, I rescind that feedback. This LGTM, @sigmavirus24?
",Lukasa,sigmavirus24
2741,2015-08-25 07:26:40,"This looks reasonable enough to me.

Thoughts @sigmavirus24?
",Lukasa,sigmavirus24
2722,2015-08-14 19:34:13,"Ok, I think that approach sounds reasonable to me: I think this feature request requires relatively little code and enables a fairly useful use-case. Thoughts from the other maintainers? /cc @kennethreitz @sigmavirus24
",Lukasa,kennethreitz
2722,2015-08-14 19:34:13,"Ok, I think that approach sounds reasonable to me: I think this feature request requires relatively little code and enables a fairly useful use-case. Thoughts from the other maintainers? /cc @kennethreitz @sigmavirus24
",Lukasa,sigmavirus24
2699,2015-07-30 21:28:40,"@sigmavirus24 Sounds good to me.

Separately, urllib3 may want a fix for this, because it is _also_ affected. /cc @shazow
",Lukasa,shazow
2699,2015-08-31 07:00:46,"@sigmavirus24 Ah, crap, that doesn't work. Observe the majesty of Python:



I wonder if we should explicitly disallow anything that compares numerically equal to zero, because setting the socket to non-blocking mode is a wacky and bizarre thing to want to do. Alternatively, we could coerce anything that compares numerically equal to zero to `None` instead, which makes the concrete assertion that when you said `timeout=0` or `timeout=False` what you meant was ""don't time out"".

@shazow @kevinburke It's my assertion that we shouldn't let anyone set `0` as their timeout for either connection or read timeouts, because it doesn't mean what they think it means. We should either coerce to `None` or reject outright. Do either of you disagree?
",Lukasa,kevinburke
2688,2015-07-24 06:55:41,"Ah, I see, the problem is that there's a non-ascii character in the location that requests is installed in. That makes sense.

So the path we generate is generated by doing `os.path.join(os.path.dirname(__file__), 'cacert.pem')`. Given that requests hasn't touched this at all, it looks like it's possible to get this wrong.

I'd like you to quickly `cd` into the directory that wakatime is in, and run the following in a Python interpreter and show me the output (censoring your username):



Right now this looks to me like the SSL library is not capable of handling Windows paths properly. @tiran?
",Lukasa,tiran
2688,2015-07-24 07:12:03,"Right, so this error seems to be entirely in the stdlib. For some reason we're not able to pass a path that we got from the stdlib _to_ the stdlib. I'm hoping that when @tiran wakes up he'll confirm that I'm right, and then we can chase this up in the stdlib.
",Lukasa,tiran
2684,2015-07-22 05:59:26,"@sigmavirus24 the link you posted is interesting, but I don't see that it addresses:

> For instance, we can no longer return the response to the caller then inspect status_code.

The linked issue had an insightful explanation by @jvanasco of why this feature is needed, then no response... The issue was closed.

If `requests` means to live up to **http for humans**, then... (quoting the first post):

In keeping with simple is beautiful, it would be terrific if we could do:
`requests.get(url, max_response_size=1024*1024)`
",boolbag,jvanasco
2682,2015-07-21 11:30:38,"Thanks for the suggestion!

The supported way to do this in requests is to use a HTTP adapter. I notice you've spotted this but consider it to be a 'hack'. It's not: the requests project considers the HTTP adapter a first-class part of our interface.

The change is very simple:



This is a very simple adapter and could easily be dropped in. We may even want to add it to the requests-toolbelt: a pull request there would be very welcome. (/cc @sigmavirus24)

Thanks for the suggestion!
",Lukasa,sigmavirus24
2678,2015-07-18 16:20:06,"Here's hoping http://ci.sigmavir.us/job/requests-real/ picks up on this. @kennethreitz's CI is not picking up on the new commits.
",sigmavirus24,kennethreitz
2678,2015-07-29 15:39:00,"what does @mitsuhiko think?
",kennethreitz,mitsuhiko
2676,2015-07-17 10:48:21,"The change you've made is in urllib3. @shazow may want this change, in which case we should accept it over at the main urllib3 repository. If he does not want it, we should instead move it into the requests code.
",Lukasa,shazow
2674,2015-07-17 08:33:52,"Partially resolves #1572: ""urllib3 exceptions passing through requests
API"". #1572 

Inspired from @Lukasa's previous 2605be11d82d42438ac7c3993810c955bde74cef.

This is my first PR to requests library; feel free to give me feedback. I generally have a fast response time. Also, available on IRC and Twitter (@ArcTanSusan).
",ArcTanSusan,Lukasa
2674,2015-07-17 08:41:24,"Aw no, we lost my awesome work?

![image](https://cloud.githubusercontent.com/assets/1382556/8743585/e94a1910-2c67-11e5-9387-f09a00a9b26c.png)

All joking aside, this looks good to me, I'd be happy to merge it. However, it's an API change, so it _at least_ needs to go into 2.8.0 and may need to go into 3.0.0. @sigmavirus24?
",Lukasa,sigmavirus24
2670,2015-07-15 16:42:19,"We will not be turning on Travis CI. ci.kennethreitz.org just needs a proper kick. /cc @kennethreitz 
",sigmavirus24,kennethreitz
2666,2015-07-13 13:29:33,"@Lukasa how does this look to you? Looks fine to me. I don't mind a couple extra tests personally.
",sigmavirus24,Lukasa
2661,2015-07-03 16:05:07,"/cc @shazow 
",Lukasa,shazow
2656,2015-06-29 04:02:25,"Cool. LGTM. I'll let @Lukasa weigh in.

Thanks @dpursehouse 
",sigmavirus24,Lukasa
2655,2015-06-28 15:58:44,"Resolves #2653.

This is one of those annoying changes that's almost impossible to test because of the sheer complexity of our redirect handling code. This also doesn't make it any simpler, sadly.

As to what version we merge this into, I proposed it against the master branch. I don't think it belongs in 3.0.0 (`resolve_redirects` isn't really part of our public API), but it could definitely break people's stuff. Next minor release feels appropriate, but I'd like to hear what you think @sigmavirus24.
",Lukasa,sigmavirus24
2651,2015-06-25 06:59:47,"I can see some value in this. For API reasons it could only ever work with the 'list of tuples' approach, but I'd be ok with us adding support for this. @sigmavirus24?
",Lukasa,sigmavirus24
2648,2015-06-22 20:36:36,"Seems reasonable enough to me. I'm happy to take this. @sigmavirus24?
",Lukasa,sigmavirus24
2646,2015-06-21 14:22:24,"/cc @neosab @Lukasa 
",sigmavirus24,Lukasa
2645,2015-06-19 18:29:16,"# Preamble

Please note that this a request for comments, not something that we will _definitely_ do in Requests.
# Problem

Right now we have an attribute defined on a Response object, `ok`. This attribute [currently](https://github.com/kennethreitz/requests/blob/9bbab338fdbb562b923ba2d8a80f0bfba697fa41/requests/models.py#L617..L623) calls `self.raise_for_status()` and catches the [exception](https://github.com/kennethreitz/requests/blob/9bbab338fdbb562b923ba2d8a80f0bfba697fa41/requests/models.py#L825..L837) raised. `raise_for_status` appropriately only raises an exception for status codes in the 4xx or 5xx range. This means that a Response with a status code in the 2xx and 3xx range will be ""ok"". The problem is that ""ok"" has a certain association in HTTP with 2xx responses, specifically 200 responses. This _may_ (I haven't looked to see if anyone has had problems with this) be misleading, especially if the user is combining their usage of the `ok` attribute with `allow_redirects=False`.
# Proposed Solution

Instead of using `raise_for_status` to determine the ""ok-ness"" of a response, we should compare the status code of the response directly. This will do two things:
1. It will narrow the definition of `Response.ok`
2. It will make using `Response.ok` faster. Currently we add a new stack, potentially throw and catch an exception, and then return. With a simple comparison, we could just do: `return 200 <= self.status_code < 300` which is much faster. This argument, however, doesn't hold much weight for me personally.

I'd really love @Lukasa and @kennethreitz's opinions here as well as anyone else willing to share their opinion publicly.
",sigmavirus24,kennethreitz
2645,2015-06-19 18:29:16,"# Preamble

Please note that this a request for comments, not something that we will _definitely_ do in Requests.
# Problem

Right now we have an attribute defined on a Response object, `ok`. This attribute [currently](https://github.com/kennethreitz/requests/blob/9bbab338fdbb562b923ba2d8a80f0bfba697fa41/requests/models.py#L617..L623) calls `self.raise_for_status()` and catches the [exception](https://github.com/kennethreitz/requests/blob/9bbab338fdbb562b923ba2d8a80f0bfba697fa41/requests/models.py#L825..L837) raised. `raise_for_status` appropriately only raises an exception for status codes in the 4xx or 5xx range. This means that a Response with a status code in the 2xx and 3xx range will be ""ok"". The problem is that ""ok"" has a certain association in HTTP with 2xx responses, specifically 200 responses. This _may_ (I haven't looked to see if anyone has had problems with this) be misleading, especially if the user is combining their usage of the `ok` attribute with `allow_redirects=False`.
# Proposed Solution

Instead of using `raise_for_status` to determine the ""ok-ness"" of a response, we should compare the status code of the response directly. This will do two things:
1. It will narrow the definition of `Response.ok`
2. It will make using `Response.ok` faster. Currently we add a new stack, potentially throw and catch an exception, and then return. With a simple comparison, we could just do: `return 200 <= self.status_code < 300` which is much faster. This argument, however, doesn't hold much weight for me personally.

I'd really love @Lukasa and @kennethreitz's opinions here as well as anyone else willing to share their opinion publicly.
",sigmavirus24,Lukasa
2641,2015-06-15 06:39:01,"Thanks for this @duanhongyi!

However, I'm :-1: on this idea. My primary objection is that this adds implicit global state. Generally speaking I don't think libraries should maintain any internal state at all except what is truly necessary for their function. Wherever possible we should provide 'state objects' to users that the user has control over, ensuring that users own the lifetimes of objects: we already do this.

I think having a pool of sessions in the background is a bad idea. It'll lead to bug reports from users who aren't expecting it and it'll lead to complaints from users who don't like the memory profile it causes. Most importantly, it makes it _very very difficult_ to obtain reproducible behaviour out of the top-level API, because what exactly happens on a given request is actually dependent on the entire lifetime of the program up until that point. `requests.get()` may or may not work depending on whether there are cookies present in the particular `Session` you're using. It may or may not work depending on whether the remote server supports keepalive connections (a surprising number don't handle it well).

I'll let @sigmavirus24 express an opinion as well, but I'm sorry, I doubt we'll merge this.
",Lukasa,sigmavirus24
2639,2015-06-12 20:27:50,"Hmm, this one is really tricky.

I think the best fix here is actually in urllib3: we should catch `UnicodeDecodeError`. However, if we catch it, we should return `result`. The rationale is that the user has already provided us with a bytestring, so they presumably think they know what they're doing. @shazow, thoughts?
",Lukasa,shazow
2636,2015-06-12 02:10:48,"I've seen this and wasn't able to come up with a simple case to reproduce it. /cc @shazow since we discussed it
",sigmavirus24,shazow
2633,2015-06-09 10:38:31,"It absolutely can, but you have to build the URL yourself. If you ask requests to encode the parameters for you (using params) then we'll throw the anchor away.

It seems like a reasonable request to keep hold of that anchor: @sigmavirus24?
",Lukasa,sigmavirus24
2630,2015-06-05 12:45:27,"> Correct me if I'm wrong, (I posted my question right before going to sleep) but this doesn't actually run the tests with py.test. Don't we need a py.test setuptools command in the code and registered as test_class or something like that?

Probably

> Further, doesn't setuptools still use unverified TLS to download the test dependencies when someone who doesn't already have pytest installed runs python setup.py test?

I was told at DjangoCon EU that setuptools has been improved in that regard: @dstufft?
",Lukasa,dstufft
2621,2015-06-01 16:47:09,"You are entirely correct. This likely applies a bit more broadly than requests: we pulled this algorithm out of the standard library, which means it is likely affected. Similarly, I checked `service_identity`, and I believe it may also not handle this appropriately.

Can I get some input from @reaperhulk, @hynek, and @tiran about whether that assessment is right?
",Lukasa,tiran
2621,2015-06-01 22:16:08,"Ok then, so I think the first priority here is working out whether or not this is a problem in the stdlib as well. @tiran?
",Lukasa,tiran
2617,2015-05-28 17:09:51,"This fixes #2613. I believe that this technically constitutes a backward-incompatible API change, so I've proposed this against 3.0.0. Let me know if you disagree @sigmavirus24 (the other option is really that this is a bugfix), and I'll propose against master.
",Lukasa,sigmavirus24
2615,2015-05-26 13:23:54,"No objection from me: @sigmavirus24?
",Lukasa,sigmavirus24
2605,2015-05-18 17:00:36,"Removing the arguments to the `Session` constructor was a deliberate decision made by @kennethreitz in 92355ada54a3a19341f7fbc65a8bc50816858c63. In this issue I cannot pretend to speak for him, but I believe in his view this was done for cleanliness (which seems reasonable to me). We have no plans to change that portion of the API at this time. Sorry! :smile:
",Lukasa,kennethreitz
2602,2015-05-14 17:41:11,"That is, assuming, @Lukasa has no objections
",sigmavirus24,Lukasa
2598,2015-05-13 06:08:49,"As discussed in the comments for ab84f9be5740d4649d734e73b84f17f85e52ffc9, this code block is necessary thanks to our shiny `json` parameter.

As a temporary workaround, this reinstates the code Kenneth hates so much so that the builds start working again. @kennethreitz feel free to replace this with something else longer term if you still hate it. :grin:
",Lukasa,kennethreitz
2595,2015-05-12 16:58:04,"Aha, this is interesting. I also cannot reproduce this, so it's actually important that we're on GAE. It seems like this is going to be caused by GAE being different to stdlib Python.

This means, unfortunately, that this _is_ a urllib3 issue (requests is unlikely to be at fault here), and in particular I think GAE is screwing this up. Unfortunately, it's screwing it up _silently_, which is doubly bad.

@shazow What's your position on GAE support in urllib3? I know requests considers it an unsupported platform...
",Lukasa,shazow
2586,2015-05-03 13:43:44,"Ok, I believe we've left enough time for the chunked stuff to bake, and no further issues seem to have come out of the woodwork. At this point, I think we should cut 2.7.0 and begin the switchover to our new release process.

@shazow are you open to tagging a new urllib3 release, and if so what commit would you like us to use? If you don't really care, by default we'd use shazow/urllib3@74073791a3429d9b9f375563954f57f2181599dc.

Things to do:
- [x] Update urllib3.
- [x] Confirm that the #2455 problem sites still work.
- [x] Update changelog.
- [ ] Tag.
- [ ] Push release.
- [ ] Have a drink.
",Lukasa,shazow
2586,2015-05-03 13:51:35,"In my ideal world we'd update the `certifi` bundle as well, but I'm now strongly averse to trying to do too much with these releases. @sigmavirus24?
",Lukasa,sigmavirus24
2586,2015-05-03 14:13:02,"Hm, this downgraded the embedded version string of urllib3.
I think it is confusing to have a quite arbitrary version in the bundled library.

Thoughts:
- Wait for a @shazow to cut a new release and update the version in urllib3 itself, then pull that.
- Rewrite the version string to be actual the commit id.
",t-8ch,shazow
2567,2015-04-24 11:08:06,"@sigmavirus24 You own this logic for the most part: how does this look?
",Lukasa,sigmavirus24
2567,2015-04-24 18:19:11,"From my POV this seems safe. @mitsuhiko might have better feedback
",dcramer,mitsuhiko
2567,2015-05-10 15:43:41,"@untitaker did you mean @mitsuhiko? I would hope a direct ping would suffice.

---

@Lukasa I would argue this is probably, on the whole, better than our previous meta_path hackery for several reasons:
1. It provides the same functionality
2. It's significantly simpler (simple is better than complex)
3. It allows for all of this to work in the case where requests is vendored without its vendored dependencies (see @untitaker's use of `__name__` when adding the alias to `sys.modules`)
4. It doesn't mess with other meta_path plugins (e.g., PyInstaller, the hack that @dcramer and @mitsuhiko are using, etc.)
5. It falls back in the correct order
6. It's far easier to explain to someone
7. When urllib3 is not vendored, the following works as we'd like it to:
   
   

I have yet to test this with PyInstaller, but the root of the problem there was the fact that our meta_path plugin was in the wrong place relative to the multiple plugins that PyInstaller uses. So with that removed, this should just work. I'm also confident that if @mitsuhiko and @dcramer test this with their code that hacks the meta_path, then they'll not see any problems.
",sigmavirus24,mitsuhiko
2567,2015-05-10 16:04:52,"@sigmavirus24

> @untitaker did you mean @mitsuhiko? I would hope a direct ping would suffice.

Unfortunately I don't think so. I haven't heard back from him either, perhaps
@dcramer can ping him about that?

---

@eriol BTW this should also remove the need to rewrite every import statement
inside requests itself.
",untitaker,mitsuhiko
2567,2015-05-10 17:34:27,"@sigmavirus24 I'm not disputing better at all. =) What I'd like to do is to take all reasonable precautions to reduce the risk of deploying this fix. For example, can @eriol and @ralphbean confirm that their package building functions correctly with this patch?

Basically, rushing helps nobody, and I'd like to try to begin a run of stable requests releases if at all possible. The last run of four-or-five broken releases in a row is bad, and we need to not get in the habit of doing that.
",Lukasa,ralphbean
2567,2015-05-27 13:02:45,"We're still hoping to have @eriol and/or @ralphbean take a swing at this code and confirm it's working for them.
",Lukasa,ralphbean
2567,2015-06-30 16:00:56,"@untitaker to be honest, there are some differences. @dstufft mentioned that his patch includes something that handles `from pip._vendor import requests` as well as `import pip._vendor.requests` but I'm not sure we need that because I think ours just works as it's written. Perhaps @dstufft could explain more about what he found to be necessary in his patch that didn't work with this (assuming he tried this one in pip)
",sigmavirus24,dstufft
2567,2015-07-29 15:51:39,"Since @kennethreitz just pinged me in #2678 i want to give some comments about this.  Given all the crappy solutions about this debian bundling thing, this is probably the best so far.  My original complaint was about the badly broken import hook.  This one comes with it's own insanity.

For a start: if someone imports `from requests.packages.urllib3 import submodule` and then later someome imports `from urllib3 import submodule` (with the patch in place), then `submodule.__name__` is `requests.packages.urllib3.submodule` and not `urllib3.submodule`.  This will break pickle in urllib3 as an example.  Since you are already down so far the ""we don't care about any of this"" route I think this is the closest you will get to achieving your goal.

I still argue that what you should be doing is just stop this vendoring.  None of this patching business makes any sense.

There be many more dragons.
",mitsuhiko,kennethreitz
2567,2015-07-29 16:57:09,"As far as I understand the main advantage Kenneth sees in vendoring urllib3 is that a minimal installation can be done by just unpacking a tarball.

Perhaps there is a way to provide this comfort without making urllib3 a subpackage of requests?

On 29 July 2015 17:52:10 CEST, Armin Ronacher notifications@github.com wrote:

> Since @kennethreitz just pinged me in #2678 i want to give some
> comments about this.  Given all the crappy solutions about this debian
> bundling thing, this is probably the best so far.  My original
> complaint was about the badly broken import hook.  This one comes with
> it's own insanity.
> 
> For a start: if someone imports `from requests.packages.urllib3 import
> submodule` and then later someome imports `from urllib3 import
> submodule` (with the patch in place), then submodule.**name** is
> `requests.packages.urllib3.submodule` and not `urllib3.submodule`. 
> This will break pickle in urllib3 as an example.  Since you are already
> down so far the ""we don't care about any of this"" route I think this is
> the closest you will get to achieving your goal.
> 
> I still argue that what you should be doing is just stop this
> vendoring.  None of this patching business makes any sense.
> 
> There be many more dragons.
> 
> ---
> 
> Reply to this email directly or view it on GitHub:
> https://github.com/kennethreitz/requests/pull/2567#issuecomment-125996644

## 

Sent from my phone. Please excuse my brevity.
",untitaker,kennethreitz
2565,2015-04-24 09:50:01,"Chunked is exactly what I expected.

Can you try hitting any URL that does not return chunked data and check whether you still see this problem? @sigmavirus24 @shazow Looks like the chunked-encoding logic has a problem with proxy connections.
",Lukasa,shazow
2565,2015-04-24 09:50:01,"Chunked is exactly what I expected.

Can you try hitting any URL that does not return chunked data and check whether you still see this problem? @sigmavirus24 @shazow Looks like the chunked-encoding logic has a problem with proxy connections.
",Lukasa,sigmavirus24
2559,2015-04-22 20:08:03,"/cc @kennethreitz 
",Lukasa,kennethreitz
2558,2015-04-22 21:27:34,"> Seemingly both packages are doing similar things, barring that requests tries to replace meta_path at one point.

@dstufft can speak best to why we replace it, but I believe we only filter out instances of our own hook to prevent infinite recursion when first attempting to import the name given to us.

That shouldn't be a big problem here though because the import error looks like it's probably coming directly out of Django.

Either way this will be fixed by 3e3fc76 in 2.6.1
",sigmavirus24,dstufft
2556,2015-04-22 14:12:32,"LGTM, but it updates urllib3, so we need to co-ordinate with @shazow to make sure he's comfortable with pushing a new release of urllib3 as well.

Thoughts @shazow?
",Lukasa,shazow
2556,2015-04-22 14:12:45,"Courtesy nod to @shazow that we'd like to release soon so you might get bugged by downstream redistributors
",sigmavirus24,shazow
2556,2015-04-22 16:14:42,"Also, a head's up to @eriol and @ralphbean that we're planning a release in the event they would like to prepare for that.
",sigmavirus24,ralphbean
2556,2015-04-22 16:14:42,"Also, a head's up to @eriol and @ralphbean that we're planning a release in the event they would like to prepare for that.
",sigmavirus24,eriol
2556,2015-04-22 21:30:06,"@ralphbean @eriol after a discussion today, we've decided that the VendorAlias work we had in requests was causing too many problems. If you'd like to include it as a patch for Fedora/Debian we're :+1: on that, but we won't be shipping it. If you find problems with it in the future, I'll be happy to hack on it with y'all to make sure it's robust, thread-safe, etc. and will continue to work for you and for our (people using requests on Fedora/Debian) users.
",sigmavirus24,ralphbean
2556,2015-04-22 21:30:06,"@ralphbean @eriol after a discussion today, we've decided that the VendorAlias work we had in requests was causing too many problems. If you'd like to include it as a patch for Fedora/Debian we're :+1: on that, but we won't be shipping it. If you find problems with it in the future, I'll be happy to hack on it with y'all to make sure it's robust, thread-safe, etc. and will continue to work for you and for our (people using requests on Fedora/Debian) users.
",sigmavirus24,eriol
2556,2015-04-22 21:31:33,"I also want to find a sustainable long-term solution to this problem. I want to grab some of @dstufft's time (or anyone who really understands python's import machinery) to work out how we can do this long term.
",Lukasa,dstufft
2554,2015-04-21 14:57:39,"It would be nice to be able to prefix session URLs to avoid repetitive code.

Consider the following code;



This could look a lot prettier by doing;



In the mean time, I was able to achieve this by using a subclass;



@kennethreitz would such a PR be accepted? Or does anyone know of a better way this could be done?
",foxx,kennethreitz
2554,2015-04-21 15:28:58,"I agree that `prefix_url` is not the most flexible solution, and hooks/subclasses would be better suited, as suggested by @kennethreitz in #133.

However it's not immediately clear from the docs how to do this, therefore I'd like to propose some sort of docs patch to make this clearer. Would such a patch be accepted?
",foxx,kennethreitz
2554,2015-04-21 16:12:20,"Again, I'd argue this is a common use case which genuinely improves code quality, and is at least worthy of a docs patch. However if @kennethreitz votes down on this, then so be it.
",foxx,kennethreitz
2554,2015-06-05 15:59:50,"Apologies for the slow response on this. After trying several different approaches, it seems the most flexible and cleanest approach is subclassing.

Here's an example that meets our particular needs of prepending a URL;



As for documentation, I'm thinking perhaps a ""recipes"" page of patterns and user contributed subclasses for achieving common goals, which aren't appropriate for inclusion into the core.

Thoughts @kennethreitz / @sigmavirus24 ?
",foxx,kennethreitz
2552,2015-04-20 19:39:16,"I see no good reason to leave those as `None`-able defaults. I think we could get away with patching this in a minor release, because while it's _technically_ backward-incompatible, in practice I don't think anyone can be correctly relying on this behaviour. @sigmavirus24?
",Lukasa,sigmavirus24
2550,2015-04-13 15:40:17,"Beginnings of work to have a list of recommended third-party (fourth-party?) packages.

@sigmavirus24 @kennethreitz feel free to push on top of this branch to add other things if you want.
",Lukasa,kennethreitz
2550,2015-04-13 15:40:17,"Beginnings of work to have a list of recommended third-party (fourth-party?) packages.

@sigmavirus24 @kennethreitz feel free to push on top of this branch to add other things if you want.
",Lukasa,sigmavirus24
2550,2015-04-14 01:37:38,"@kennethreitz added the requests-toolbelt to the sidebar a while back. Would it be best to move it here?
",sigmavirus24,kennethreitz
2538,2015-04-07 11:34:33,"I'm afraid we can't support you with the `.deb`, because we don't build or maintain that package, @ralphbean does. I recommend raising a bug with debian if you're having trouble.

When you say you've tried the install methods on our website, which methods have you actually tried and how did they fail?
",Lukasa,ralphbean
2538,2015-04-07 13:26:25,"> I'm afraid we can't support you with the .deb, because we don't build or maintain that package, @ralphbean does.

Just a correction:  I do not support the debian redistribution, @eriol does.  I work on the `.rpm` packaging for Fedora and EPEL.  Regardless, thanks for the ping!
",ralphbean,ralphbean
2538,2015-04-07 13:26:25,"> I'm afraid we can't support you with the .deb, because we don't build or maintain that package, @ralphbean does.

Just a correction:  I do not support the debian redistribution, @eriol does.  I work on the `.rpm` packaging for Fedora and EPEL.  Regardless, thanks for the ping!
",ralphbean,eriol
2538,2015-04-07 13:39:56,"I think @lukasa meant to ping @eriol
",sigmavirus24,eriol
2536,2015-04-06 18:10:05,"I've done the documentation changes suggested in #2532.

I also documented a way to work around this in the docstring, but I'm not sure if it'd be good to do so. (As it may lead to hacked-together solutions) Then again, other solutions like creating a brand new `Request` might lead to worse problems. (e.g.: loss of data)

@sigmavirus24, what do you think?
",smiley,sigmavirus24
2535,2015-04-06 14:03:00,"_/me forgot to make this last night_ Sorry @Lukasa 
",sigmavirus24,Lukasa
2523,2015-04-02 20:33:56,"@Lukasa , @kennethreitz - Per your suggestion, I just updated the PR with commit e65360d.
Thanks for your input.
",exvito,kennethreitz
2523,2015-04-02 20:33:56,"@Lukasa , @kennethreitz - Per your suggestion, I just updated the PR with commit e65360d.
Thanks for your input.
",exvito,Lukasa
2519,2015-03-30 17:24:44,"Hi @tdussa,

In the future, please ask _questions_ on [StackOverflow](https://stackoverflow.com/questions/tagged/python-requests). Quoting from [the docs](http://docs.python-requests.org/en/latest/api/?highlight=cert).

> cert – (optional) if String, path to ssl client cert file (.pem). If Tuple, (‘cert’, ‘key’) pair.

So if you have the key file, you can use that. @t-8ch can correct me if I'm wrong, but I don't believe we (or urllib3) support sending a passphrase. I think the lack of support is specifically a limitation of the way the SSL module [loads verification data](https://docs.python.org/3/library/ssl.html?highlight=ssl#ssl.SSLContext.load_verify_locations).
",sigmavirus24,t-8ch
2518,2015-03-25 17:20:49,"@sigmavirus24 I though it could be improved a bit. Tell me what you think.
",deronnax,sigmavirus24
2517,2015-03-24 15:32:20,"Also, ping @shazow 
",sigmavirus24,shazow
2516,2015-03-24 14:09:57,"This is a very deliberate import situation. requests does not support a version of Python that lacks the standard library version. simplejson is imported first specifically because there are people who would rather it be used than the standard library's json module. As such I'm strongly -1 on including this change, but I'd still like to hear @Lukasa's thoughts.
",sigmavirus24,Lukasa
2516,2015-03-24 16:31:39,"So I looked into the bug on simplejson and I can't reproduce it. Also, I forgot to mention earlier that this change is functionally equivalent to only ever importing the standard library's `json` module (for interested parties).

I'd still like input from @Lukasa although I'm not rather convinced that this isn't a bug requests needs to be concerned about.
",sigmavirus24,Lukasa
2504,2015-03-19 11:34:03,"LGTM. The one thing I'm thinking about is whether we want to allow for forward compatibility by actually storing these kwargs in a dictionary for resolve_redirects, but I haven't decided yet. I'll let @sigmavirus24 make a call here.
",Lukasa,sigmavirus24
2497,2015-03-15 17:28:25,"In the wake of CVE-2015-2296 we should take another look at our policy for patching, releasing and publicising security vulnerabilities like this one.

Although we did a great job of responding quickly and pushing out a new release, we can definitely improve. I've received feedback from a couple of places that wanted to point out ways we could improve, and I'd like to solicit public feedback from anyone who has an interest to ensure that we're doing the best we possibly can with this sort of thing.

The goal here is for me to write up a document that explains, step-by-step, our policy with security issues. This will be posted publicly and we'll use it as a reference for any future events.

Below is the list of points people have raised:
- We probably shouldn't announce these vulnerabilities on weekends. Several people have products and projects that require them to evaluate vulnerabilities as soon as they become aware of them, and forcing those people to work on weekends does not engender positive feelings towards us.
- We released and announced before we had a CVE number. This might not have been the best thing for us to do, particularly as it will have made it a bit more difficult for people to keep track of what was going on.

If you have any other suggestions, please post them below.

I'm going to ping some specific interested parties to ensure they see this: @sigmavirus24 @dstufft @eriol @ralphbean
",Lukasa,ralphbean
2497,2015-03-15 17:28:25,"In the wake of CVE-2015-2296 we should take another look at our policy for patching, releasing and publicising security vulnerabilities like this one.

Although we did a great job of responding quickly and pushing out a new release, we can definitely improve. I've received feedback from a couple of places that wanted to point out ways we could improve, and I'd like to solicit public feedback from anyone who has an interest to ensure that we're doing the best we possibly can with this sort of thing.

The goal here is for me to write up a document that explains, step-by-step, our policy with security issues. This will be posted publicly and we'll use it as a reference for any future events.

Below is the list of points people have raised:
- We probably shouldn't announce these vulnerabilities on weekends. Several people have products and projects that require them to evaluate vulnerabilities as soon as they become aware of them, and forcing those people to work on weekends does not engender positive feelings towards us.
- We released and announced before we had a CVE number. This might not have been the best thing for us to do, particularly as it will have made it a bit more difficult for people to keep track of what was going on.

If you have any other suggestions, please post them below.

I'm going to ping some specific interested parties to ensure they see this: @sigmavirus24 @dstufft @eriol @ralphbean
",Lukasa,eriol
2497,2015-03-15 17:28:25,"In the wake of CVE-2015-2296 we should take another look at our policy for patching, releasing and publicising security vulnerabilities like this one.

Although we did a great job of responding quickly and pushing out a new release, we can definitely improve. I've received feedback from a couple of places that wanted to point out ways we could improve, and I'd like to solicit public feedback from anyone who has an interest to ensure that we're doing the best we possibly can with this sort of thing.

The goal here is for me to write up a document that explains, step-by-step, our policy with security issues. This will be posted publicly and we'll use it as a reference for any future events.

Below is the list of points people have raised:
- We probably shouldn't announce these vulnerabilities on weekends. Several people have products and projects that require them to evaluate vulnerabilities as soon as they become aware of them, and forcing those people to work on weekends does not engender positive feelings towards us.
- We released and announced before we had a CVE number. This might not have been the best thing for us to do, particularly as it will have made it a bit more difficult for people to keep track of what was going on.

If you have any other suggestions, please post them below.

I'm going to ping some specific interested parties to ensure they see this: @sigmavirus24 @dstufft @eriol @ralphbean
",Lukasa,sigmavirus24
2497,2015-03-15 17:28:25,"In the wake of CVE-2015-2296 we should take another look at our policy for patching, releasing and publicising security vulnerabilities like this one.

Although we did a great job of responding quickly and pushing out a new release, we can definitely improve. I've received feedback from a couple of places that wanted to point out ways we could improve, and I'd like to solicit public feedback from anyone who has an interest to ensure that we're doing the best we possibly can with this sort of thing.

The goal here is for me to write up a document that explains, step-by-step, our policy with security issues. This will be posted publicly and we'll use it as a reference for any future events.

Below is the list of points people have raised:
- We probably shouldn't announce these vulnerabilities on weekends. Several people have products and projects that require them to evaluate vulnerabilities as soon as they become aware of them, and forcing those people to work on weekends does not engender positive feelings towards us.
- We released and announced before we had a CVE number. This might not have been the best thing for us to do, particularly as it will have made it a bit more difficult for people to keep track of what was going on.

If you have any other suggestions, please post them below.

I'm going to ping some specific interested parties to ensure they see this: @sigmavirus24 @dstufft @eriol @ralphbean
",Lukasa,dstufft
2497,2015-03-17 20:09:25,"Alright, thanks for your feedback all! Can you please take a look at the diff attached to this issue and confirm that it's to your liking, and that you're happy with this commitment? All feedback valued! 

One particular note: I'm not sure about having a public list of who we notify downstream, but at the same time I worry about losing track of it if we don't record it somewhere. Thoughts there? @sigmavirus24?
",Lukasa,sigmavirus24
2490,2015-03-14 12:12:56,"So this is probably a stupid question, but does that server actually work? As you discussed with @mjpieters on the question, this doesn't happen with our typical test server. Is there a server we can actually reproduce this with?
",sigmavirus24,mjpieters
2489,2015-03-14 11:31:11,"Per a discussion @sigmavirus24 and I had on the mailing list, this change ensures that we correctly record cookie properties based on the original request, rather than the mutated version.
",Lukasa,sigmavirus24
2485,2015-03-12 01:55:42,"@Lukasa feel free to merge this so we can release 2.5.5 tomorrow.
",sigmavirus24,Lukasa
2477,2015-03-06 18:53:23,"Eh, fine by me. @sigmavirus24?
",Lukasa,sigmavirus24
2476,2015-03-06 15:21:31,"This is a tricky interaction with iterators. Most iterators don't quite behave the same way. What you need to do is the following



That works fine for me. I'm not confident this is a bug because `iter_lines` is allowed to make some assumptions about how it's used. I'll wait to see if @Lukasa agrees with me.
",sigmavirus24,Lukasa
2472,2015-03-04 19:54:57,"Thanks for this!

I can see no particular reason to be concerned about this patch, it seems totally reasonable to me. I'll obviously wait for @sigmavirus24 to hop in. It would also be extremely helpful to get someone who is deploying requests in anger to weigh in here.
",Lukasa,sigmavirus24
2469,2015-03-03 15:32:27,"eventlet shouldn't be affecting the import of netrc. I'm also not certain if eventlet has special case handling for requests. All eventlet should be patching is socket or httplib (or both) as far as I know, so this seems like a bug for eventlet, not us. I don't use eventlet with requests, nor does @Lukasa so neither of us can tell you that this isn't going to break anything (i.e., that the warning is okay). So if you want assurances, you'll need to search elsewhere (e.g., ask a question on StackOverflow and tag it with eventlet so that someone more familiar with eventlet will answer your question).
",sigmavirus24,Lukasa
2468,2015-03-03 07:51:21,"This looks great, I see no reason not to merge this. :cake:

@sigmavirus24, I'll let you pull the trigger.
",Lukasa,sigmavirus24
2466,2015-02-28 16:53:27,"Closes #2465 
Closes #2470 

/cc @dstufft Let me know if I should send this to pip as well
",sigmavirus24,dstufft
2466,2015-03-01 05:31:44,"Also many thanks to @dstufft for helping with the final solution.
",sigmavirus24,dstufft
2466,2015-03-01 15:04:15,"@ralphbean @eriol if you two wouldn't mind testing this on Fedora and Debian respectively, I'd greatly appreciate it. (Alternatively, if y'all have scripts that I could test this with on the respective distros, I can spin up cloud images to test with.)
",sigmavirus24,ralphbean
2466,2015-03-01 15:04:15,"@ralphbean @eriol if you two wouldn't mind testing this on Fedora and Debian respectively, I'd greatly appreciate it. (Alternatively, if y'all have scripts that I could test this with on the respective distros, I can spin up cloud images to test with.)
",sigmavirus24,eriol
2466,2015-03-03 15:08:51,"@Lukasa should I just merge this myself to push 2.5.4 out the door?
",sigmavirus24,Lukasa
2465,2015-02-28 16:30:04,"So this is sort of a bug with that import machinery and sort of a bug with python 2 and sort of a bug with chardet. On the bright side, there's a fix for it that can be performed in chardet and then vendored into requests, etc. On the not-so-bright side, this _can_ affect urllib3 as well.

I was trying to debug https://github.com/jakubroztocil/httpie/issues/315 with pdb to figure out why I was seeing a different error and ran into an issue with this import logic trying to import `requests.packages.urllib3.pdb` because on Py2, `import pdb` is treated as a implicit relative import first and then a non-relative import second. (Woo, thanks Python 2.) The temporary work-around was to add `from __future__ import absolute_import` to the top of the file I was trying to debug in. This, of course, could be applied to urllib3 and chardet both. I think the better option is to attempt to fix the import machinery stolen wholesale from pip.

@dstufft definitely understands this code better than I do, but as I understand it now: we stop trying to import it at L83 if the `__import__(name)` (which in these cases are `chardet.sys` and `urllib3.pdb`) fails. Instead, I think we need to figure out how to try one last case to actually mimic the regular import machinery. I can imagine more complex import failures, like seeing something like `chardet.os.path` fail, so something like



Does that make sense?
",sigmavirus24,dstufft
2465,2015-03-31 01:00:25,"So I read a bit of the PEP that explains the part `sys.meta_path` plays and talked to @dstufft a bit on IRC. Provided the way PyInstaller registers it's import hooks on the meta path you can see that it registers it's ""BuiltinImporter"" first. Since the first hook to return a module wins, we probably want to register our hook first. This allows Python 2's behaviour of try an implicit relative import then try an absolute import to kind of just work.

As a quick fix, I'm going to change our behaviour from appending to the meta path to inserting at the start of it. We kind of want that behaviour anyway.
",sigmavirus24,dstufft
2462,2015-03-12 01:56:09,"/cc @dstufft 
",sigmavirus24,dstufft
2456,2015-02-24 02:42:39,"@Lukasa generated the certificate with mkcert.org if I recall correctly but I'm not sure the list of certificates he used. It seems we just need to add thawte
",sigmavirus24,Lukasa
2455,2015-02-24 01:18:54,"@t-8ch seems related to the urllib3 patch to disable built-in hostname verification possibly.
",sigmavirus24,t-8ch
2455,2015-03-01 20:24:59,"**Update**

There is actually an API available in OpenSSL to tell it to use the shortest trust path, however it's only available in OpenSSL 1.0.2+. In addition there was apparently another patch applied to OpenSSL that is unreleased which will cause OpenSSL to fall back to trying shorter paths if it can't validate the path served by the the site operator.

So I think the best course of action right now is to get Python patched so that it will use the ""shortest trust path"" API if it's available. @alex has uploaded a patch to [Python's #23476](http://bugs.python.org/issue23476) which does that. I can't think of any scenario where that would break things so that seems like an easy win to me. However that's only going to ""fix"" things for people using 2.7.10 and 3.4.4, both of which are not immediately on the horizon. Even with that fix, it's still going to require a newer version of OpenSSL unless we can convince the downstream distributors to backport the OpenSSL patch to add that API, as well as backport the Python patch which will use the new API in OpenSSL if possible.
",dstufft,alex
2455,2015-03-02 08:42:48,"Thanks @bagder.

Just to update based on where we got to last night in IRC:

@alex's patch would fix the problem in 2.7.10 and 3.5 (assuming it gets merged), as well as possibly a later version of 3.4.

We can do this more generally by ORing in `X509_V_FLAG_TRUSTED_FIRST` numeric constant into the `SSLContext.verify_flags` field:



This will only work on Python versions that a) have an SSL context (2.7.9, 3.2 and onwards), and b) are using OpenSSL 1.0.2 or later. This satisfies a large number of users, but lots of users don't have access to either or both of those things.
",Lukasa,alex
2455,2015-04-23 15:22:46,"/cc @kennethreitz: this is the issue that tracks the work we need to do for certifi, btw.
",Lukasa,kennethreitz
2450,2015-02-19 19:21:42,"No objection from me. @sigmavirus24?
",Lukasa,sigmavirus24
2448,2015-02-17 23:27:55,"So I think the fundamental problem here is that neither requests nor urllib3 have any way of knowing what's happening with the actually underlying TCP connection. urllib3 at its best wraps sockets to enable TLS but doesn't really directly manage sockets beyond that very thoroughly.

It isn't as if urllib3 or requests is acknowledging the FIN that it receives. That's generally how the socket behaves without us having to do that. I think a way around this is to enable TCP Keep-Alive but I'm not entirely convinced this is a solution that needs to live in requests proper.

I think @Lukasa is on vacation or just generally taking a break so I'll have to look into this more tonight. Thanks for the very very detailed issue @pddenhar!
",sigmavirus24,Lukasa
2444,2015-02-11 18:58:36,"We actually will pull in tip when we do a release. Speaking of which @Lukasa when do you think we should do a release? There's extra header awesomeness that's unreleased in urllib3 (merged last night) that I'd like to grab before doing a release.
",sigmavirus24,Lukasa
2434,2015-02-02 19:42:24,"I wonder if this shouldn't go into urllib3 instead, thereby giving the benefit there too. @shazow? @sigmavirus24?
",Lukasa,shazow
2434,2015-02-02 19:42:24,"I wonder if this shouldn't go into urllib3 instead, thereby giving the benefit there too. @shazow? @sigmavirus24?
",Lukasa,sigmavirus24
2431,2016-04-06 19:25:35,"This PR hasn't seen any love in a long time. @ianepperson, would mind performing a rebase to make this once-again mergable? 

@sigmavirus24 @Lukasa +1? -1? I want to get this merged or closed out. 
",kennethreitz,Lukasa
2427,2015-04-06 03:32:08,"I'm comfortable with this. Sorry for the delay @luozhaoyu. I lost track of this.

Thoughts @Lukasa?
",sigmavirus24,Lukasa
2424,2015-01-25 17:22:50,"Let's CC some people whose opinions we care strongly about:

@shazow @kevinburke @dstufft @alex @sigmavirus24
",Lukasa,sigmavirus24
2424,2015-01-25 17:22:50,"Let's CC some people whose opinions we care strongly about:

@shazow @kevinburke @dstufft @alex @sigmavirus24
",Lukasa,alex
2424,2015-01-25 17:22:50,"Let's CC some people whose opinions we care strongly about:

@shazow @kevinburke @dstufft @alex @sigmavirus24
",Lukasa,shazow
2424,2015-01-25 17:22:50,"Let's CC some people whose opinions we care strongly about:

@shazow @kevinburke @dstufft @alex @sigmavirus24
",Lukasa,dstufft
2424,2015-01-25 17:22:50,"Let's CC some people whose opinions we care strongly about:

@shazow @kevinburke @dstufft @alex @sigmavirus24
",Lukasa,kevinburke
2424,2015-01-25 17:34:43,"My 2 $CURRENCY's worth:

I would be disinclined to do it. I think being outside the standard library has given us the freedom to make choices that benefit our users without being stuck behind core dev's policies for version support and release. It allows us to respectfully disagree with the priorities of core dev. And it allows us to make decisions that are ideological, which has been the lifeblood of this project for more than three years.

I think the reality is that if this module enters the standard library the current core team will move on from it. I certainly have little interest following it into the quagmire that is core dev. The most likely to steward requests in the stdlib is @sigmavirus24, and he's just one man. That loss of direction will inevitably lead to an erosion of the library's interface over time, and I think that would be a tragic thing.

The only thing that being in the standard library gives us is our time back. That's a good reason to put it there, if that's what you think it needs, but I don't think we should pretend that it will make the library or the Python ecosystem any better.
",Lukasa,sigmavirus24
2424,2015-01-25 18:11:17,"> I think the reality is that if this module enters the standard library the current core team will move on from it. I certainly have little interest following it into the quagmire that is core dev. The most likely to steward requests in the stdlib is @sigmavirus24, and he's just one man. That loss of direction will inevitably lead to an erosion of the library's interface over time, and I think that would be a tragic thing.

I would wander into the stdlib to try to help, but given the fact that exactly one of I don't know how many previous patchsets I've submitted has been accepted and one other _reviewed_ makes me wary of wanting to bother with that process. I know the core devs are entirely swamped by more important things. I also know someone else has decided randomly that they want to maintain httplib/http but they're clearly not suited for the job (yet) and I don't have the patience to work on httplib when patches that both @Lukasa and I sit around, unreviewed, and not cared about (when they fix pressing issues with the library).

I'd probably end up just forking requests to continue using it.

> requests is absolutely unsuitable for stdlib inclusion for the many reasons stated before me. The urllib3 dependency alone is a complete showstopper; we don’t want it to got to die in the stdlib.

It's always been a contention of @kennethreitz (and therefore, the project as a whole) that urllib3 is an implementation detail. Many of requests' biggest features are handled entirely by urllib3, but it doesn't mean they couldn't be reimplemented with care into truly dependency-less library.

Regarding the chardet dependency: it's been nothing but a headache to us (and to me specifically). It used to have separate codebases for py2 and py3 until I got it into a single codebase library (which has only in the last several months been merged back into chardet proper). The library is slow and a huge memory hog (which angers many people to the point of yelling at us here on the issue tracker). It's not entirely accurate and Mozilla's universalchardet that it is modeled after has all but been abandoned by Mozilla. So removing chardet would probably be a net positive anyway.

Regarding whether we should do this or not, I'm frankly unconcerned. Whatever would be in the stdlib would end up being requests in API only. The Python 3 adoption rate is slow enough that I don't think people will be meaningfully affected by this for the next N years (where N is the globally unknown number of years for 3.5 to be used in production by corporations).

And like I said, I'd probably end up just forking requests or using urllib3 directly at that point.
",sigmavirus24,sigmavirus24
2423,2015-01-24 07:53:11,"This isn't our fault I don't think: your exception comes from pip. @dstufft?
",Lukasa,dstufft
2423,2015-01-24 16:20:51,"Yeah, so I'm pretty sure this is one of those cases where we shouldn't have let @jschneier remove that import in `requests.compat` (our fault) mixed with the constant of ""`sudo pip install x` is evil; use virtualenvs"".

We _could_ add the import back, but I would then mark it for deletion in 3.0 so this problem would just pop up again. So I'm going to close this since there's no solution that won't result in further breakage of other packages down the road.
",sigmavirus24,jschneier
2416,2015-01-21 02:41:48,"So we [rebuild](https://github.com/kennethreitz/requests/blob/d2d576b6b1101e2871c82f63adf2c2b534c2dabc/requests/sessions.py#L168..L176) the `Cookie` header on a redirect as a matter of not exposing the cookie to people who shouldn't see it. Without a cookiejar to rebuild _from_ we won't persist the header through the redirect. We already [sanitize authorization](https://github.com/kennethreitz/requests/blob/d2d576b6b1101e2871c82f63adf2c2b534c2dabc/requests/sessions.py#L215) on redirect to different hosts, so we _could_ do the same here.

I want @Lukasa's thoughts on this though before I continue with work for this.

I, for one, think Cookie headers are in the same class as Content-Length and Host headers. They can be set by users but they really _shouldn't_ be set by users because it can do bad things if not handled with care.
",sigmavirus24,Lukasa
2413,2015-01-19 13:35:11,"@Lukasa This is what I get for working on this way too late last night.

@arthurdarcet thanks. Done.
",sigmavirus24,arthurdarcet
2409,2015-01-16 08:53:50,"Alright, @sigmavirus24, I've thought about this overnight and I can't think of a good fix that uses the current code and doesn't commit wild layering violations. So here's my proposal.

We should rewrite the redirect cache.

The core problem is, fundamentally, that the redirect cache's implementation is misguided. I've come to this position because of closer reading of RFC 7231. It has the following two stanzas [under the 301 code](https://tools.ietf.org/html/rfc7231#section-6.4.2) that I find to be interesting:

> [A]ny future references to this resource ought to use one of the enclosed URIs. Clients with link-editing capabilities ought to automatically re-link references to the effective request URI to one or more of the new references sent by the server, where possible

and

> A 301 response is cacheable by default; i.e., unless otherwise indicated by the method definition or explicit cache controls.

This is a weird combination of text. If the 301 is supposed to cause a rewrite of the URL, why would I cache it?

My suspicion is that the correct flow in requests is actually not a 'redirect' cache, but a 301 cache, that literally returns 301 responses without contacting the network. This will lead to the correct flow in requests: everything looks exactly the same from the perspective of the upper layers of code, the response just came in really fast.

This is backed up by examining what Chrome does, which is serve 301s from its cache (click to see better):

![301](https://cloud.githubusercontent.com/assets/1382556/5773563/dc5f2bdc-9d5b-11e4-8581-5f83e76d8526.png)

I think now, as I thought then, that the 301 cache should not have been merged in its proposed form. I now think more strongly than that: we have to change it. I have three options on the table:
1. Remove the redirect cache entirely. People who want that function should use [CacheControl](https://github.com/ionrock/cachecontrol), as we have said many, many times. This will lead to the flow through requests I mentioned above, resolve this bug (and some others that are inevitably lying around), and provide additional benefits _on top_ of the standard redirect cache.
2. Replace the redirect cache with a special-case cache for 301s that operates at the `HTTPAdapter` layer. This is tricky, because caching 301s needs to obey all the rules for caching (another thing our 'redirect cache' doesn't do), including `Cache-Control` and `Vary` headers. We'd probably need to write about 30% of the CacheControl library, which raises my third option;
3. Bring CacheControl (or someone else) into the fold. Bless a requests caching solution as 'the one true way' to cache and bring it in tree, or use it as an external `pip` dependency, but include it by default. This will fix our 301 bugs and give us caching out-of-the-box by default.

Thoughts would be extremely appreciated at this time. Especially from @sigmavirus24, @kennethreitz, @shazow, @ionrock. Note also the discussion that occurred in #2095: my reading of it suggests that @johnsheehan, @dstufft, and @dknecht favoured my option 1, and @dolph favoured something closer to my option 3 (or possibly 2).
",Lukasa,kennethreitz
2409,2015-01-16 08:53:50,"Alright, @sigmavirus24, I've thought about this overnight and I can't think of a good fix that uses the current code and doesn't commit wild layering violations. So here's my proposal.

We should rewrite the redirect cache.

The core problem is, fundamentally, that the redirect cache's implementation is misguided. I've come to this position because of closer reading of RFC 7231. It has the following two stanzas [under the 301 code](https://tools.ietf.org/html/rfc7231#section-6.4.2) that I find to be interesting:

> [A]ny future references to this resource ought to use one of the enclosed URIs. Clients with link-editing capabilities ought to automatically re-link references to the effective request URI to one or more of the new references sent by the server, where possible

and

> A 301 response is cacheable by default; i.e., unless otherwise indicated by the method definition or explicit cache controls.

This is a weird combination of text. If the 301 is supposed to cause a rewrite of the URL, why would I cache it?

My suspicion is that the correct flow in requests is actually not a 'redirect' cache, but a 301 cache, that literally returns 301 responses without contacting the network. This will lead to the correct flow in requests: everything looks exactly the same from the perspective of the upper layers of code, the response just came in really fast.

This is backed up by examining what Chrome does, which is serve 301s from its cache (click to see better):

![301](https://cloud.githubusercontent.com/assets/1382556/5773563/dc5f2bdc-9d5b-11e4-8581-5f83e76d8526.png)

I think now, as I thought then, that the 301 cache should not have been merged in its proposed form. I now think more strongly than that: we have to change it. I have three options on the table:
1. Remove the redirect cache entirely. People who want that function should use [CacheControl](https://github.com/ionrock/cachecontrol), as we have said many, many times. This will lead to the flow through requests I mentioned above, resolve this bug (and some others that are inevitably lying around), and provide additional benefits _on top_ of the standard redirect cache.
2. Replace the redirect cache with a special-case cache for 301s that operates at the `HTTPAdapter` layer. This is tricky, because caching 301s needs to obey all the rules for caching (another thing our 'redirect cache' doesn't do), including `Cache-Control` and `Vary` headers. We'd probably need to write about 30% of the CacheControl library, which raises my third option;
3. Bring CacheControl (or someone else) into the fold. Bless a requests caching solution as 'the one true way' to cache and bring it in tree, or use it as an external `pip` dependency, but include it by default. This will fix our 301 bugs and give us caching out-of-the-box by default.

Thoughts would be extremely appreciated at this time. Especially from @sigmavirus24, @kennethreitz, @shazow, @ionrock. Note also the discussion that occurred in #2095: my reading of it suggests that @johnsheehan, @dstufft, and @dknecht favoured my option 1, and @dolph favoured something closer to my option 3 (or possibly 2).
",Lukasa,dstufft
2409,2015-01-16 08:53:50,"Alright, @sigmavirus24, I've thought about this overnight and I can't think of a good fix that uses the current code and doesn't commit wild layering violations. So here's my proposal.

We should rewrite the redirect cache.

The core problem is, fundamentally, that the redirect cache's implementation is misguided. I've come to this position because of closer reading of RFC 7231. It has the following two stanzas [under the 301 code](https://tools.ietf.org/html/rfc7231#section-6.4.2) that I find to be interesting:

> [A]ny future references to this resource ought to use one of the enclosed URIs. Clients with link-editing capabilities ought to automatically re-link references to the effective request URI to one or more of the new references sent by the server, where possible

and

> A 301 response is cacheable by default; i.e., unless otherwise indicated by the method definition or explicit cache controls.

This is a weird combination of text. If the 301 is supposed to cause a rewrite of the URL, why would I cache it?

My suspicion is that the correct flow in requests is actually not a 'redirect' cache, but a 301 cache, that literally returns 301 responses without contacting the network. This will lead to the correct flow in requests: everything looks exactly the same from the perspective of the upper layers of code, the response just came in really fast.

This is backed up by examining what Chrome does, which is serve 301s from its cache (click to see better):

![301](https://cloud.githubusercontent.com/assets/1382556/5773563/dc5f2bdc-9d5b-11e4-8581-5f83e76d8526.png)

I think now, as I thought then, that the 301 cache should not have been merged in its proposed form. I now think more strongly than that: we have to change it. I have three options on the table:
1. Remove the redirect cache entirely. People who want that function should use [CacheControl](https://github.com/ionrock/cachecontrol), as we have said many, many times. This will lead to the flow through requests I mentioned above, resolve this bug (and some others that are inevitably lying around), and provide additional benefits _on top_ of the standard redirect cache.
2. Replace the redirect cache with a special-case cache for 301s that operates at the `HTTPAdapter` layer. This is tricky, because caching 301s needs to obey all the rules for caching (another thing our 'redirect cache' doesn't do), including `Cache-Control` and `Vary` headers. We'd probably need to write about 30% of the CacheControl library, which raises my third option;
3. Bring CacheControl (or someone else) into the fold. Bless a requests caching solution as 'the one true way' to cache and bring it in tree, or use it as an external `pip` dependency, but include it by default. This will fix our 301 bugs and give us caching out-of-the-box by default.

Thoughts would be extremely appreciated at this time. Especially from @sigmavirus24, @kennethreitz, @shazow, @ionrock. Note also the discussion that occurred in #2095: my reading of it suggests that @johnsheehan, @dstufft, and @dknecht favoured my option 1, and @dolph favoured something closer to my option 3 (or possibly 2).
",Lukasa,ionrock
2409,2015-01-16 08:53:50,"Alright, @sigmavirus24, I've thought about this overnight and I can't think of a good fix that uses the current code and doesn't commit wild layering violations. So here's my proposal.

We should rewrite the redirect cache.

The core problem is, fundamentally, that the redirect cache's implementation is misguided. I've come to this position because of closer reading of RFC 7231. It has the following two stanzas [under the 301 code](https://tools.ietf.org/html/rfc7231#section-6.4.2) that I find to be interesting:

> [A]ny future references to this resource ought to use one of the enclosed URIs. Clients with link-editing capabilities ought to automatically re-link references to the effective request URI to one or more of the new references sent by the server, where possible

and

> A 301 response is cacheable by default; i.e., unless otherwise indicated by the method definition or explicit cache controls.

This is a weird combination of text. If the 301 is supposed to cause a rewrite of the URL, why would I cache it?

My suspicion is that the correct flow in requests is actually not a 'redirect' cache, but a 301 cache, that literally returns 301 responses without contacting the network. This will lead to the correct flow in requests: everything looks exactly the same from the perspective of the upper layers of code, the response just came in really fast.

This is backed up by examining what Chrome does, which is serve 301s from its cache (click to see better):

![301](https://cloud.githubusercontent.com/assets/1382556/5773563/dc5f2bdc-9d5b-11e4-8581-5f83e76d8526.png)

I think now, as I thought then, that the 301 cache should not have been merged in its proposed form. I now think more strongly than that: we have to change it. I have three options on the table:
1. Remove the redirect cache entirely. People who want that function should use [CacheControl](https://github.com/ionrock/cachecontrol), as we have said many, many times. This will lead to the flow through requests I mentioned above, resolve this bug (and some others that are inevitably lying around), and provide additional benefits _on top_ of the standard redirect cache.
2. Replace the redirect cache with a special-case cache for 301s that operates at the `HTTPAdapter` layer. This is tricky, because caching 301s needs to obey all the rules for caching (another thing our 'redirect cache' doesn't do), including `Cache-Control` and `Vary` headers. We'd probably need to write about 30% of the CacheControl library, which raises my third option;
3. Bring CacheControl (or someone else) into the fold. Bless a requests caching solution as 'the one true way' to cache and bring it in tree, or use it as an external `pip` dependency, but include it by default. This will fix our 301 bugs and give us caching out-of-the-box by default.

Thoughts would be extremely appreciated at this time. Especially from @sigmavirus24, @kennethreitz, @shazow, @ionrock. Note also the discussion that occurred in #2095: my reading of it suggests that @johnsheehan, @dstufft, and @dknecht favoured my option 1, and @dolph favoured something closer to my option 3 (or possibly 2).
",Lukasa,shazow
2409,2015-01-16 08:53:50,"Alright, @sigmavirus24, I've thought about this overnight and I can't think of a good fix that uses the current code and doesn't commit wild layering violations. So here's my proposal.

We should rewrite the redirect cache.

The core problem is, fundamentally, that the redirect cache's implementation is misguided. I've come to this position because of closer reading of RFC 7231. It has the following two stanzas [under the 301 code](https://tools.ietf.org/html/rfc7231#section-6.4.2) that I find to be interesting:

> [A]ny future references to this resource ought to use one of the enclosed URIs. Clients with link-editing capabilities ought to automatically re-link references to the effective request URI to one or more of the new references sent by the server, where possible

and

> A 301 response is cacheable by default; i.e., unless otherwise indicated by the method definition or explicit cache controls.

This is a weird combination of text. If the 301 is supposed to cause a rewrite of the URL, why would I cache it?

My suspicion is that the correct flow in requests is actually not a 'redirect' cache, but a 301 cache, that literally returns 301 responses without contacting the network. This will lead to the correct flow in requests: everything looks exactly the same from the perspective of the upper layers of code, the response just came in really fast.

This is backed up by examining what Chrome does, which is serve 301s from its cache (click to see better):

![301](https://cloud.githubusercontent.com/assets/1382556/5773563/dc5f2bdc-9d5b-11e4-8581-5f83e76d8526.png)

I think now, as I thought then, that the 301 cache should not have been merged in its proposed form. I now think more strongly than that: we have to change it. I have three options on the table:
1. Remove the redirect cache entirely. People who want that function should use [CacheControl](https://github.com/ionrock/cachecontrol), as we have said many, many times. This will lead to the flow through requests I mentioned above, resolve this bug (and some others that are inevitably lying around), and provide additional benefits _on top_ of the standard redirect cache.
2. Replace the redirect cache with a special-case cache for 301s that operates at the `HTTPAdapter` layer. This is tricky, because caching 301s needs to obey all the rules for caching (another thing our 'redirect cache' doesn't do), including `Cache-Control` and `Vary` headers. We'd probably need to write about 30% of the CacheControl library, which raises my third option;
3. Bring CacheControl (or someone else) into the fold. Bless a requests caching solution as 'the one true way' to cache and bring it in tree, or use it as an external `pip` dependency, but include it by default. This will fix our 301 bugs and give us caching out-of-the-box by default.

Thoughts would be extremely appreciated at this time. Especially from @sigmavirus24, @kennethreitz, @shazow, @ionrock. Note also the discussion that occurred in #2095: my reading of it suggests that @johnsheehan, @dstufft, and @dknecht favoured my option 1, and @dolph favoured something closer to my option 3 (or possibly 2).
",Lukasa,sigmavirus24
2408,2015-01-14 14:35:00,"We do not use `qop=auth-int`, we don't support it. When given multiple `qop` values like that in an auth challenge we're allowed to select one, and we have: `auth`.

You're right, however, about the fix. If we've chosen `auth` that's what we should use in our calculation. The simplest fix should be to pull the calculation of `noncebit` down into the only place it's used.

That said, can RFCreader @sigmavirus24 confirm that this is correct WRT the specs?
",Lukasa,sigmavirus24
2408,2015-01-14 14:58:30,"> That said, can RFCreader @sigmavirus24 confirm that this is correct WRT the specs?

Is this the new requests/urllib3 thing? If so, I thoroughly approve.
",sigmavirus24,sigmavirus24
2399,2015-01-06 11:01:34,"(Note that doing this might belong in urllib3: @shazow?)
",Lukasa,shazow
2388,2014-12-22 23:23:49,"Thanks for this!

The logic doesn't look quite right. Here, if we get an empty section from a generator, we stop sending the chunked body altogether. That doesn't seem like the correct logic to me. We should really do one of two things:
1. Throw an exception. This means that users need to not send us empty chunks. This has the advantage of correctness, in that users are forced to choose what the correct behaviour is for empty chunks. It has the disadvantage of being annoying.
2. Skip the chunk. We cannot emit it, that would be incorrect, so the only way to 'muddle on through' is to skip it. This has the advantage of continuing to work in the majority of cases, and the disadvantage of being a bit surprising in the remainder, where we send something other than what the user provided us.

If we want to do option 2, that means changing `break` for `continue`. @sigmavirus24, thoughts?
",Lukasa,sigmavirus24
2378,2014-12-12 09:40:28,"Hello,

I'm moving some Python code from `urlopen` to `requests` (because it will be easier for me to add a db cache (SQLite) mechanism using requests-cache)

The code I want to modify looks like



I don't find any equivalent to `readlines()`

Any idea ?

I have found a generator solution like



but requests_cache doesn't seems to like this stream parameter
see https://github.com/reclosedev/requests-cache/issues/33

So there is 2 solutions (and both can be valuable).
1. fix requests-cache : that's what I'm asking to @reclosedev
2. provide a request readlines() method (or give a tip to have something like that which will return a list and not a generator)

That will be very nice.

Thanks for requests... that's a great lib for humans
",femtotrader,reclosedev
2375,2014-12-09 02:58:42,"Fedora and Debian have both recently added symlinks to their distributions of requests so people can do `from requests.packages import urllib3` but they seem to still rewrite our import statements. So the following situation is possible:
- User registers adapter for custom scheme
- User does the following:
  
  
- When they do `s.get('glance+https://...')` they see a `KeyError` because the urllib3 we're using in requests is not the one they've imported (according to `sys.modules`).

Pip works around this with the copied machinery. The tests seem to work just fine for me. I still need to test this with our vendored packages removed and urllib3/chardet installed separately.

/cc @dstufft @eriolv @ralphbean 
",sigmavirus24,ralphbean
2375,2014-12-09 02:58:42,"Fedora and Debian have both recently added symlinks to their distributions of requests so people can do `from requests.packages import urllib3` but they seem to still rewrite our import statements. So the following situation is possible:
- User registers adapter for custom scheme
- User does the following:
  
  
- When they do `s.get('glance+https://...')` they see a `KeyError` because the urllib3 we're using in requests is not the one they've imported (according to `sys.modules`).

Pip works around this with the copied machinery. The tests seem to work just fine for me. I still need to test this with our vendored packages removed and urllib3/chardet installed separately.

/cc @dstufft @eriolv @ralphbean 
",sigmavirus24,dstufft
2375,2014-12-09 14:40:29,"@eriolv question: Since the symlink is in place (it may just be Fedora that has this in place), is there any chance of the imports that import from `.packages` inside of requests could not be rewritten? 

If not, can the imports not be rewritten after we ship this patch? (After we've updated it to give proper attribution to @dstufft and pypa/pip) The crux of this issue is that sys.modules is incorrectly populated and needs to work a certain way for users to not run into surprises like this.

[/Edit - I submitted my comment too soon]
And I appreciate your collaboration @eriolv. That's why I pinged you immediately. I wanted to make you aware of this from the start and get your feedback as well as @Lukasa's and @ralphbean's
",sigmavirus24,ralphbean
2372,2014-12-08 09:53:19,"Interesting. The Debian bug shows the time is in `cryptography`. Ping @alex?
",Lukasa,alex
2371,2015-03-07 19:24:05,"@gardenia I haven't had a chance to absorb all of this, but thank you so much for your effort and work here. @shazow perhaps you'd find @gardenia's research interesting.
",sigmavirus24,shazow
2365,2014-12-01 23:21:24,":cake: LGTM.

Ready when you are @kennethreitz.
",Lukasa,kennethreitz
2365,2014-12-01 23:22:14,"@kennethreitz wanted to get this out today and added both of us to PyPI as maintainers so I'm going to wrap this up right now.
",sigmavirus24,kennethreitz
2362,2014-12-01 19:51:59,"Keys to the kingdom for @Lukasa and @sigmavirus24.

This includes....
- [ ] DNSimple collab access]
- [x] PyPi collab access

I think that's it...
",kennethreitz,Lukasa
2362,2014-12-01 19:51:59,"Keys to the kingdom for @Lukasa and @sigmavirus24.

This includes....
- [ ] DNSimple collab access]
- [x] PyPi collab access

I think that's it...
",kennethreitz,sigmavirus24
2362,2014-12-01 20:44:27,"I'm not sure we need op on the channel. I've never seen any problems arise in there. 



Should allow @Lukasa and I to OP ourselves with ChanServ when necessary. Ideally not having OP on by default will be a positive impact on the channel's atmosphere 
",sigmavirus24,Lukasa
2362,2014-12-03 02:25:06,"@sigmavirus24 now has access to dnsimple. @Lukasa, can you tell me your dnsimple email address? 
",kennethreitz,Lukasa
2356,2014-11-25 19:34:12,"This bug is exactly the same as #1360, with one key difference: here, the server isn't percent-encoding percent signs. This is not valid HTTP, and we're totally allowed to fail here according to RFC 7231:

> Note: Some recipients attempt to recover from Location fields that are not valid URI references.  This specification does not mandate or define such processing, but does allow it for the sake of robustness.

However, I wonder if we can do better. Specifically, I wonder if we can update our `requote_uri` function to allow us to attempt to unquote it, and if that fails because of invalid percent-escape sequences we can just use the URL unchanged. That probably covers most of our bases, and it's gotta be better than failing hard like we do now.

@sigmavirus24, thoughts?
",Lukasa,sigmavirus24
2355,2014-11-25 00:18:28,"This adds support for unix domain sockets by using a slightly modified version of the unix domain socket adapter from the [docker-py](https://github.com/docker/docker-py) project:

https://github.com/docker/docker-py/blob/master/docker/unixconn/unixconn.py

The modifications are to make the adapter more usable in a greater variety of contexts by getting rid of the context of a `base_url` for the adapter and inventing a URL syntax that allows embedding the socket path into the URL. Basically the socket name goes in the netloc (host) part of the URL, but it's URL-encoded so that slashes become `%2F`'s so that the slashes don't interfere with separating the netloc and path parts of the URLs. This was roughly inspired by URL syntaxes that are mentioned in the following comments:
- http://daniel.haxx.se/blog/2008/04/14/http-over-unix-domain-sockets/
- http://lists.w3.org/Archives/Public/uri/2008Oct/0000.html

So for example if I had a server listening on `/tmp/profilesvc.sock`:



I can access it as follows:



In case you're curious about the motivation for adding unix domain socket support, it was inspired by:

https://github.com/jakubroztocil/httpie/issues/209

Cc: @shin-, @jakubroztocil, @monsanto, @np, @nuxlli, @matrixise, @remmelt
",msabramo,jakubroztocil
2355,2014-11-25 02:09:42,"So the fundamental thing to remember about requests is that it is an HTTP library which means we strive to be in compliance with RFC 7230, 7231, 7232, 7233, 7234, and 7235 and other RFCs that relate to how we have to transmit the data or communicate with the server (etc. etc. etc.). We primarily handle HTTP, we don't handle arbitrary URLs. That's a job for a library that will handle arbitrary URLs. (Hint: this was the point of urllib/urllib2)

Note, while people use requests for `unix://` or `file://`, we do not explicitly support these protocols purposefully. Our scope is very narrowly defined to 98% of our users concerns: HTTP. Those protocols are currently used through transport adapters and the adapters that provide this functionality are packaged separately.

If copying and pasting this code around is becoming a problem for projects (like httpie, docker-py, or whomever else) the solution is not to upstream the code here to be maintained by @Lukasa and me. The solution is to find people who care about maintaining it and make it its own library. If you think this code is too small for a library on its own, consider the fact that many of the auth libraries (kerberos, ntlm, etc.) are all similar in size to this code and are all packaged separately.

**tl;dr** We have no interest in making this a feature of the project or maintaining this code when the maintainers will not use it actively enough to be comfortable maintaining it.
",sigmavirus24,Lukasa
2353,2014-11-24 08:33:11,"I think this is safe to take. @sigmavirus24?
",Lukasa,sigmavirus24
2352,2014-11-22 18:11:47,"I really don't think this belongs in the quickstart section frankly. It isn't something most people will need as soon as they start using requests. It's more of an advanced usage piece of information, but even then our Advanced section is already quite large and overloaded. I'm not entirely convinced this shouldn't be it's own section though. I'd love to see more of the recipes you think belong there before we do that though and I'd would **really** like @Lukasa's input when they have a minute or two to spare.
",sigmavirus24,Lukasa
2349,2014-11-18 04:46:01,"> any way to add a test exposing this issue?

Besides the test that failed in the first place? Jenkins isn't running for a reason only @kennethreitz can determine. Had @Lukasa or I been running the tests locally before approving PRs, it probably wouldn't have reached this point.
",sigmavirus24,kennethreitz
2349,2014-11-18 04:46:01,"> any way to add a test exposing this issue?

Besides the test that failed in the first place? Jenkins isn't running for a reason only @kennethreitz can determine. Had @Lukasa or I been running the tests locally before approving PRs, it probably wouldn't have reached this point.
",sigmavirus24,Lukasa
2347,2014-11-17 04:40:53,"Given that requests doesn't handle sockets itself, I think this is a feature you should be requesting in urllib3. Before you open an issue there, I wonder if @shazow would care to express their opinion here first.
",sigmavirus24,shazow
2345,2014-11-16 15:52:34,"Note to @kennethreitz, we **can not** do a release until we fix this.
",sigmavirus24,kennethreitz
2339,2014-11-14 11:03:01,"This feature really belongs in another library altogether. The problem is that the HTTP parsing logic in requests is in `httplib`, which is well down our stack and outside our control. This means we can't pull it out into common code.

_However_, @dstufft has been working on a formal HTTP parsing library which might be usable as a base for this use-case. What do you think, @dstufft?
",Lukasa,dstufft
2336,2014-11-13 15:53:37,"That's an interesting question. @kevinburke, got any idea?
",Lukasa,kevinburke
2336,2014-11-13 19:47:13,"So... this is an error. 

The gist of it is, with HTTPS connections connect() gets called earlier in the urlopen() process than with a HTTP connection. Specifically, for HTTPS _new_conn will call _prepare_conn, which calls connect() on the socket, where for HTTP, the connect() call is delayed all the way until httplib.request.

urllib3 sets the connection timeout until after calling _get_conn, but before httplib.request, which explains why the timeout isn't set.

The obvious question is why isn't this an issue when you're not using a proxy? Well, prepare_conn only attempts the connect() if you're using a proxy, with the following note:



I'll discuss with @shazow and see if we can figure out a good course of action.
",kevinburke,shazow
2336,2014-12-03 15:03:39,"We haven't shipped the fix for this yet. Ping @shazow and @sigmavirus24

On Wednesday, December 3, 2014, Zhang Yibin notifications@github.com
wrote:

> I also encoutered a deadlock problem caused by this similar timeout issue.
> 
> What I did is sending a GET request to some site (like http://jsonip.com)
> using an HTTP proxy to get proxy's public ip address, the code is like:
> 
> s = requests.Session()
> s.proxies = {'http': 'http://xx.xx.xx.xx:xxx', 'https': 'http://xx.xx.xx.xx:xxx'}
> r = s.get('http://jsonip.com', timeout=5)
> r.raise_for_status()
> ...
> 
> After deadlock, the stack trace of the problmatic thread is (other threads
> are waiting for its completion):
> 
> File: ""C:\Python27\lib\threading.py"", line 783, in __bootstrap
>   self.__bootstrap_inner()
> File: ""C:\Python27\lib\threading.py"", line 810, in __bootstrap_inner
>   self.run()
> File: ""C:\Python27\lib\threading.py"", line 763, in run
>   self.__target(_self.__args, *_self.__kwargs)
> File: ""C:\Python27\lib\site-packages\concurrent\futures\thread.py"", line 73, in _worker
>   work_item.run()
> File: ""C:\Python27\lib\site-packages\concurrent\futures\thread.py"", line 61, in run
>   result = self.fn(_self.args, *_self.kwargs)
> File: ""E:\zbb\projects\crawler\new\zbb-crawler\zbb_proxymanager\app\proxy_filter.py"", line 45, in test
>   ip = self.public_ip.get(host)
> File: ""E:\zbb\projects\crawler\new\zbb-crawler\zbb_proxymanager\utils.py"", line 91, in get
>   ip = func(session, timeout)
> File: ""E:\zbb\projects\crawler\new\zbb-crawler\zbb_proxymanager\get_ip.py"", line 43, in get_ip_4
>   r = session.get('http://jsonip.com', timeout=timeout)
> File: ""C:\Python27\lib\site-packages\requests\sessions.py"", line 473, in get
>   return self.request('GET', url, *_kwargs)
> File: ""C:\Python27\lib\site-packages\requests\sessions.py"", line 461, in request
>   resp = self.send(prep, *_send_kwargs)
> File: ""C:\Python27\lib\site-packages\requests\sessions.py"", line 599, in send
>   history = [resp for resp in gen] if allow_redirects else []
> File: ""C:\Python27\lib\site-packages\requests\sessions.py"", line 192, in resolve_redirects
>   allow_redirects=False,
> File: ""C:\Python27\lib\site-packages\requests\sessions.py"", line 573, in send
>   r = adapter.send(request, **kwargs)
> File: ""C:\Python27\lib\site-packages\requests\adapters.py"", line 370, in send
>   timeout=timeout
> File: ""C:\Python27\lib\site-packages\requests\packages\urllib3\connectionpool.py"", line 513, in urlopen
>   conn = self._get_conn(timeout=pool_timeout)
> File: ""C:\Python27\lib\site-packages\requests\packages\urllib3\connectionpool.py"", line 231, in _get_conn
>   return conn or self._new_conn()
> File: ""C:\Python27\lib\site-packages\requests\packages\urllib3\connectionpool.py"", line 717, in _new_conn
>   return self._prepare_conn(conn)
> File: ""C:\Python27\lib\site-packages\requests\packages\urllib3\connectionpool.py"", line 690, in _prepare_conn
>   conn.connect()
> File: ""C:\Python27\lib\site-packages\requests\packages\urllib3\connection.py"", line 217, in connect
>   self._tunnel()
> File: ""C:\Python27\lib\httplib.py"", line 752, in _tunnel
>   (version, code, message) = response._read_status()
> File: ""C:\Python27\lib\httplib.py"", line 365, in _read_status
>   line = self.fp.readline(_MAXLINE + 1)
> File: ""C:\Python27\lib\socket.py"", line 476, in readline
>   data = self._sock.recv(self._rbufsize)
> 
> I think it's the same bug discussed in this issue, right? The version of
> Requests is 2.5.0.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/2336#issuecomment-65412519
> .

## 

## 

Kevin Burke
phone: 925.271.7005 | twentymilliseconds.com
",kevinburke,shazow
2336,2016-07-23 15:21:36,"Hooray! So, that points to a bug in the Ubuntu version of this package. @eriol, you might want to be aware of this.
",Lukasa,eriol
2334,2015-04-03 12:18:42,"@vincentxb : thanks for your feedback. You are correct. @tardyp made the same comment in #2523. I'll go ahead and extend the per-thread storage to the other state attributes.
",exvito,tardyp
2332,2014-11-12 14:58:25,"This is related to https://github.com/kennethreitz/requests/issues/2329

Just implemented @Lukasa's solution and added test.
",asnelzin,Lukasa
2331,2014-11-12 14:26:42,"Thanks for raising this!

Unfortunately, this bug lives a long way down the stack, in `httplib`. Specifically, `httplib` never checks the socket to see if there is any data to read before sending the body.

What you've actually stumbled upon, however, is one of the trickiest parts of HTTP/1.1: how do we know we have a response to read? It's not enough to poll for data after we send the headers, because the response might be in flight and we could miss it. So we'd have to poll after each data send. That's not enough either: it's possible that we could send some data, causing the server to emit the response and RST packet that reaches us _after_ we poll (so not marking the socket readable) but _before_ we send again, therefore still causing the `Connection reset by peer` error. This gets more likely in the event of threading etc. causing us to have to wait.

Handling all of this properly is extremely tricky and `httplib` just doesn't do it, so while we're using `httplib` we cannot get around it. One day I'd like to replace `httplib`, but until that happens we're between a rock and a hard place.

An interesting idea is whether we can take the socket with us when we throw these exceptions, so that the user can read from them if they want to. However, that breaks our connection re-use where we attempt to re-use socket objects: we can't do that if the user has a handle to them.

I'm tempted to call this a ""can't fix"". @sigmavirus24?
",Lukasa,sigmavirus24
2329,2014-11-11 22:35:35,"Eh, I'm on the fence. I don't think requests should be able to generate any invalid input provided to it by the user. We're in the position of making user's lives easier, and sometimes that means ignoring their mistakes.

However, this violates our general principle of leaving the user's headers where they are. That means I'm +0.5 on fixing it. The fix is easy: in `PreparedRequest.prepare_content_length`, prevent falling into the last `elif` block by adding `and self.headers.get('Content-Length') is None`.

@sigmavirus24, does that sound reasonable?
",Lukasa,sigmavirus24
2313,2014-10-29 18:13:27,"Oh, yes, I remember now.

POSTing files with unicode filenames is awkward, because you didn't say what text encoding you want us to use. There's a spec for this, which we implement, but relatively few others do it and many servers don't understand it.

My suggested workaround would be to set the filename yourself using whatever encoding you choose. Unfortunately, that doesn't work:



The problem here seems to be [this line](https://github.com/kennethreitz/requests/blob/master/requests/packages/urllib3/fields.py#L37). This unconditional call to encode will actually cause an implicit call to `str.decode` on Python 2, which breaks for non-ascii characters. @shazow, you prepared to consider that a bug?
",Lukasa,shazow
2308,2014-10-27 18:16:12,"And this is going to cause a merge conflict with #2216. If @Lukasa doesn't object, we can merge this now and I'll update #2216 soon there after with a merge of master.
",sigmavirus24,Lukasa
2287,2015-04-07 13:50:06,"@Lukasa is it still worth keeping this open?
",sigmavirus24,Lukasa
2283,2014-10-15 02:25:24,"I'm not sure that not using it by default is enough - if an attacker can interrupt the TLS v1 protocol and force the connection to downgrade to ssl v3, it's enough to read the plaintext.

I am not sure how to test this but I am reading through the code to try and understand it. Would appreciate some help from @t-8ch @alex @reaperhulk @dreid .. :)
",kevinburke,alex
2283,2014-10-15 02:25:24,"I'm not sure that not using it by default is enough - if an attacker can interrupt the TLS v1 protocol and force the connection to downgrade to ssl v3, it's enough to read the plaintext.

I am not sure how to test this but I am reading through the code to try and understand it. Would appreciate some help from @t-8ch @alex @reaperhulk @dreid .. :)
",kevinburke,t-8ch
2282,2014-10-14 15:17:43,"@kennethreitz you okay with this?
",sigmavirus24,kennethreitz
2273,2014-10-10 02:07:53,"Since @kennethreitz is very passionate about the docs, I want to make sure he's okay with this before we merge it @alex 
",sigmavirus24,kennethreitz
2273,2014-10-10 02:07:53,"Since @kennethreitz is very passionate about the docs, I want to make sure he's okay with this before we merge it @alex 
",sigmavirus24,alex
2272,2014-10-11 13:22:32,"I haven't run the code yet, but this can (probably) be solved in a few ways:
1. Use grequests because it looks to do The Right Thing
2. Figure out a way to make gevent monkey patch the `queue` module with it's [own](http://www.gevent.org/gevent.queue.html) (although I have yet to test that this will work)
3. (And this will be the easiest) Use a separate session for each ""thread"". The problem is that the `LifoQueue` from the standard library is **threadsafe** but that does not imply gevent-safe. This will ensure separate queues for your program.

We _could_ add something like





But we probably won't. The reason this might work is because when using `gevent.monkey.patch_all()` you're patching the threading library and so gevent will ensure that this will not hang. I don't have the time to test all of this, but connection pooling is almost certainly the problem here. @shazow do you have any thoughts on this matter?
",sigmavirus24,shazow
2268,2014-10-05 23:56:27,"Address @kevinburke's concerns
",sigmavirus24,kevinburke
2263,2014-10-03 18:50:43,"This looks great @daftshady, thanks! :cake: I'll let @sigmavirus24 review, but I'm :+1:.
",Lukasa,sigmavirus24
2262,2014-10-02 23:19:13,"Hello,

I've looked through open and closed issues but haven't seen this raised before. Sorry if I'm bringing up something that's already been addressed. That said, here we go:

I'm running a flask-restful app and returning JSON responses to requests. Flask-restful helpfully encodes the response body as UTF-8 and sets the Content-Type to ""application/json"". When i make a request to my server using requests, requests can't properly detect the encoding and it looks like this is because I haven't set the charset on the Content-Type header. However, according to [RFC 4627](http://www.ietf.org/rfc/rfc4627.txt) the encoding for application/json responses is either UTF-8, UTF-16, or UTF-32. The default is UTF-8, but you can detect which one of the three encodings is being used by looking at the first four bytes as mentioned in [RFC 4627](http://www.ietf.org/rfc/rfc4627.txt).

When requests receives a response with a Content-Type header of application/json shouldn't it try to detect the encoding as UTF-8, UTF-16, or UTF-32 and set the encoding on the response object accordingly? This would help greatly with using `response.text`. Right now I'm setting the encoding on the response object manually. 

Just to note: `response.json()` works perfectly so I realize this is a bit of a corner case since users will probably just deserialize the response anyway. 

Thanks for your time, and all your awesome work. I absolutely love requests.
Mike

Possibly Related Issues: #1665 #1467 
Reference to @mitsuhiko's opinion on the matter: https://github.com/mitsuhiko/flask/issues/454#issuecomment-4578200
",noseworthy,mitsuhiko
2258,2014-10-04 14:04:15,"@Lukasa @kevinburke care to take another look at this?
",sigmavirus24,kevinburke
2258,2014-10-04 14:04:15,"@Lukasa @kevinburke care to take another look at this?
",sigmavirus24,Lukasa
2258,2014-10-05 16:49:29,"@willingc thank you so much for your hard work on this. :cake: (Those emoji from @kennethreitz are for you ;))
",sigmavirus24,willingc
2255,2014-09-29 06:57:07,"Yeah, this is almost certainly connection pooling. urllib3 puts the connection back when SSL errors are raised, but I really don't know if we can safely do that. @shazow, @t-8ch, what are your thoughts about not re-using connections that raise SSL errors?
",Lukasa,shazow
2255,2014-09-29 06:57:07,"Yeah, this is almost certainly connection pooling. urllib3 puts the connection back when SSL errors are raised, but I really don't know if we can safely do that. @shazow, @t-8ch, what are your thoughts about not re-using connections that raise SSL errors?
",Lukasa,t-8ch
2249,2014-09-25 20:22:05,"This _looks_ right, but it's hard for me to know because URLs are a crazy mess. I'll let @sigmavirus24 review this, he knows URLs better than I do.
",Lukasa,sigmavirus24
2249,2014-09-26 14:52:49,"So if we used a better (read: less forgiving) URL parser library we wouldn't need to split on `@` (like we do [here](https://github.com/kennethreitz/requests/pull/2249/files#diff-5956087d5835a57d9ef6fff974f6fd9bR687)). In principle this works fine. In reality, we should get something like `rfc3986` into acceptable shape for @shazow or just make the `Url` object in `urllib3` more RFC compliant (whichever works better) and use that for parsing the URL and reconstructing it. In short, you have 5 major components of the URL:



And `authority` which we're dealing with right now has 3 sub-components so a URL would look like:



`rfc3986` would split this up and allow you to replace `userinfo` with `None` and then reconstruct the URL. For a quick fix, this is great. I'd rather not have so much URL/URI parsing logic in requests though, this is an HTTP library not an HTTP + URL + ... library.
",sigmavirus24,shazow
2244,2014-09-23 14:43:52,"But yes, I suspect we could subclass `RedirectLoopError` from `TooManyRedirects` and raise that so as to save the calls to the network that would otherwise cause us to exceed the max number of retries. I'm not opposed to that. I'd like to hear other opinions though from @fcosantos and @RuudBurger (and anyone else that has one).

@kennethreitz thoughts on saving people from hitting the network more than necessary when we can detect an endless redirect loop?
",sigmavirus24,kennethreitz
2241,2014-09-22 16:39:40,"Let's have a subclass of `RequestException`. My proposed name is `BodyConsumedError`: @sigmavirus24, do you have a better one?
",Lukasa,sigmavirus24
2238,2014-09-20 22:22:20,"Thanks for this!

I think we like the ability to pass non-strings to `prepare_url`. It allows people to use custom url-building classes without running into trouble. It's worth noting that you change also breaks Python 2 behaviour: previously a Python 2 `str` would get lifted to a Python 2 `unicode`, which it now doesn't do.

I think we can get around this by simply special-casing string types. The logic is really:



I think that's really the logic we want here. @sigmavirus24, thoughts?
",Lukasa,sigmavirus24
2235,2014-09-19 06:18:42,"@sigmavirus24, can I get your thoughts here?
",Lukasa,sigmavirus24
2228,2014-09-16 15:26:37,"Thanks for the suggestion! However, I disagree.

In many cases, users find it valuable to add a timeout to their calls. This is entirely reasonable. However, the quickstart code is not an example of 'general use', it's intended to be illustrative and educational. We should not obscure the pedagogical point in the pursuit of some form of pure correctness.

There is a [timeout section](http://docs.python-requests.org/en/latest/user/quickstart/#timeouts) in the quickstart docs, which we believe to be sufficient. Do you disagree?

As for mentioning the default socket timeout, I hadn't spotted that. That may actually be a bug. @sigmavirus24, what do you think: should we use the default socket timeout when no value is provided? If so, how do users specify they want no timeout at all? Am I overthinking the whole thing?
",Lukasa,sigmavirus24
2228,2014-09-16 20:59:02,"I feel like `timeout=0` is probably clear enough.

I think we stopped using it when @kevinburke did his really major timeout work, and we didn't spot that urllib3 uses a sentinel object where we use the `None` pattern. Just some crossed wires.
",Lukasa,kevinburke
2222,2014-09-12 02:43:06,"@jvantuyl
Can you remember and elaborate on why you've made this change (https://github.com/kennethreitz/requests/commit/b149be5d864cc7f65ac22113db3d0e4d2ed2b49e)?
",blueyed,jvantuyl
2218,2014-09-11 12:21:27,"Yeah I just found that myself and saw @Lukasa's [comment](https://github.com/requests/requests-oauthlib/pull/147#issuecomment-55255020). That's exactly how we feel and further this already exists elsewhere. Finally, URLObject's (un)license would prohibit us from actually being able to vendor it in this case. I don't believe it's compatible with Apache v2. [rfc3986](/sigmavirus24/rfc3986) would be vendorable but even so, we won't accept this functionality in requests.
",sigmavirus24,Lukasa
2214,2014-09-09 22:24:09,"At this point it's fairly obvious that @Lukasa and I are -1 on this feature. @kennethreitz @shazow any opinions?
",sigmavirus24,kennethreitz
2214,2014-09-09 22:24:09,"At this point it's fairly obvious that @Lukasa and I are -1 on this feature. @kennethreitz @shazow any opinions?
",sigmavirus24,shazow
2210,2014-09-07 17:03:09,"@Lukasa wrote the fix in #2207 

With this, the expected behaviour is restored (a `TooManyRedirects` error);


",sigmavirus24,Lukasa
2209,2014-09-07 15:07:43,"So the calculation of the elapsed time per-request is done in `requests/sessions.py` and hasn't changed between 2.2.0 and 2.4.0. It still does this:



This is entirely related to something that @kevinburke pointed out though. In [HTTPAdapter#send](https://github.com/kennethreitz/requests/blob/master/requests/adapters.py#L315) we do not use the `stream` parameter any longer. In 2.2.0, if stream was not truthy, at the end of the `send` method we would do `r.content` which would consume everything. I'm not sure when or why that was removed, but I'll check into that.

**Edit** This check for `stream` is also not present in [2.3.0](https://github.com/kennethreitz/requests/blob/6366d3dd190a9e58ca582955cddf7e2ac5f32dcc/requests/adapters.py#L380..L388). It is present in [2.2.1](https://github.com/kennethreitz/requests/blob/33735480f77891754304e7f13e3cdf83aaaa76aa/requests/adapters.py#L393)
",sigmavirus24,kevinburke
2209,2014-09-07 15:21:22,"So the actual change was made by @schlamar in https://github.com/kennethreitz/requests/commit/59c8d813818110aac29fd104c2fa012387c2004c. It was part of https://github.com/kennethreitz/requests/pull/1944. Now to figure out why that was moved. It's not apparent from the bug report that it was necessary to move it to `Session#send`.
",sigmavirus24,schlamar
2209,2014-09-07 15:27:38,"Final comment of the morning (as I have other things I have to do), I'm not sure there is anything preventing us from moving that consumption back to `HTTPAdapter#send`. I'm genuinely interested in @schlamar's reason for moving it though if they can remember it.

Also, @heyman thank you for changing the test style. We may not end up merging this right away, but your cooperation and bug report are much appreciated.
",sigmavirus24,schlamar
2206,2014-09-05 20:36:43,"There is a good reason not to do it on the module scope. That function is so rarely used by 98% of requests users that triggering the compile when they do `import requests` will hurt their performance for no real benefit. That said, how would you determine which was the first encoding in the body? You could do something like:



But that seems kind of arbitrary. On the one hand, returning all of them is also arbitrary but if we had written it the way you want, someone else would have wanted us to write it the way it is currently written.

To address your concern about `get_encoding_from_headers` that is now not conformant with RFC 7230 and we have plans to change that in #2086. Right now, we _could_ accept that pull request but I'm not sure if it's desirable. I'll have to think about it and wait for @Lukasa would have to weigh in too.
",sigmavirus24,Lukasa
2204,2014-09-04 19:11:06,"@kevinburke Did retry logic change to not raise MaxRetryErrors when `retries=0`?
",Lukasa,kevinburke
2203,2014-09-04 18:45:01,"@kevinburke Done. =)
",Lukasa,kevinburke
2200,2014-09-03 10:06:40,"On the topic of requests not mapping exceptions to its own exceptions...

`requests.packages.urllib3.exceptions.ProtocolError` is not caught. Reproduction: invalid status line.

Server:
`$ echo | nc -l 8000`

requests:



Output: <class 'requests.packages.urllib3.exceptions.ProtocolError'>

I would expect this to be mapped to a requests exception.

Connection refused comes out also as ProtocolError.

This is requests 2.4.0.

Ping @Lukasa , @sigmavirus24 

Also happens with connection refused. This should be covered by requests unittests in the future.


",joneskoo,Lukasa
2200,2014-09-03 10:06:40,"On the topic of requests not mapping exceptions to its own exceptions...

`requests.packages.urllib3.exceptions.ProtocolError` is not caught. Reproduction: invalid status line.

Server:
`$ echo | nc -l 8000`

requests:



Output: <class 'requests.packages.urllib3.exceptions.ProtocolError'>

I would expect this to be mapped to a requests exception.

Connection refused comes out also as ProtocolError.

This is requests 2.4.0.

Ping @Lukasa , @sigmavirus24 

Also happens with connection refused. This should be covered by requests unittests in the future.


",joneskoo,sigmavirus24
2199,2014-09-04 04:18:20,"Nevermind, I asked @r1chardj0n3s and he assuaged my concerns.
",sigmavirus24,r1chardj0n3s
2195,2014-08-30 13:24:11,"I really have no opinion on the name. I would think `with_sni` (i.e., `pip install requests[with_sni]` reads well) would be most descriptive of the use case for the largest number of users, but `pyopenssl` is fine too. I'll wait for @kennethreitz to decide.
",sigmavirus24,kennethreitz
2189,2014-08-29 18:59:37,"The only person who can fix this is @kennethreitz 
",sigmavirus24,kennethreitz
2185,2014-08-27 12:22:05,"My code



raises an exception socket.error; I would expect it to be mapped to requests.exceptions instead.



@Lukasa, since you fixed another exception handling issue already, would you care to take a look at some point? Easy enough to work around but should be fixed to maintain a human API.
",joneskoo,Lukasa
2185,2014-08-27 12:23:32,"Agreed. Either we should catch it ourselves, or urllib3 should catch this stuff from read or stream. @shazow?
",Lukasa,shazow
2183,2014-08-26 19:20:57,"@kennethreitz?
",Lukasa,kennethreitz
2180,2014-08-26 05:41:07,"Good catch! @sigmavirus24, you know the GitHub API best, what endpoint should we hit?
",Lukasa,sigmavirus24
2177,2014-08-23 20:17:21,"@kennethreitz, we're going to need your input on this one.
",Lukasa,kennethreitz
2176,2014-08-26 19:15:58,"I would petition that we wait until @willingc can get her `json` parameter pull request in good shape for us to review.
",sigmavirus24,willingc
2173,2014-08-22 08:45:23,"Concerning #2169, i get a problem with automatic keep alive.
@sigmavirus24 has commented this saying that was in documentation but can have been lost.
",tychotatitscheff,sigmavirus24
2169,2014-08-26 19:25:37,"I wonder if @kevinburke or someone else removed it in one of their PRs modifying the default headers we set
",sigmavirus24,kevinburke
2169,2014-08-27 15:04:17,"Seems legit.
Good luck and please let me know if you need my help for anything.
I just poke @shazow, so he can read the wrong (imao) rewrapping of `SocketError` to `ProxyError` (first post) and say me if i shall open an issue on https://github.com/shazow/urllib3
Cheers
",tychotatitscheff,shazow
2159,2014-08-02 14:35:22,"My only hesitation is @kennethreitz's previous resistance to the idea. Perhaps with the relatively recent release of the new version of HTTP/1.1 (which removes the necessity to support HTTP/0.9) he will be more willing.
",sigmavirus24,kennethreitz
2159,2014-08-02 15:18:54,"I'm confident I could sell him on the idea. =)

> On 2 Aug 2014, at 10:35, Ian Cordasco notifications@github.com wrote:
> 
> My only hesitation is @kennethreitz's previous resistance to the idea. Perhaps with the relatively recent release of the new version of HTTP/1.1 (which removes the necessity to support HTTP/0.9) he will be more willing.
> 
> —
> Reply to this email directly or view it on GitHub.
",Lukasa,kennethreitz
2158,2014-08-01 18:20:47,"Hey @zapman449!

Thanks for your suggestion! We really appreciate the fact that you took the time to file this. Unfortunately we're not adding anything new to the requests API at this time.

I'll wait for @Lukasa to chime in before closing this though.
",sigmavirus24,Lukasa
2155,2014-08-01 14:56:06,"So this actually leads to something that's been rattling around in my head for a while and which I haven't proposed yet because it would be a significant API change.

I don't like the fact that we suggest people use `r.raw` because it's an object which we don't document and it's an object provided by `urllib3` (which we've claimed in the past is more of an implementation detail). With that in mind, I've been toying with the idea of providing methods on a `Response` object which just proxy to the `urllib3` methods (`read` would just proxy to `raw.read`, etc.). This gives us extra flexibility around `urllib3` and allows us to handle (on behalf of the users) an API change in `urllib3` (which historically has almost never been a problem, so there isn't any urgency in that).

With that said, we already have enough methods on a Response object in my opinion and growing our API isn't ideal. The best API is the API from which there's nothing left to remove. So I'm continuously on the fence about this.

---

> This assumption was also at the core of my original request to default decode_content to True. Of course now that I see what a leaky abstraction this is, I'm no longer suggesting that.

For others who find this and may not be certain why this is true, allow me to explain.

There are several users of requests who turn off automatic decompression to validate the length of a response, or to do other important things with it. One consumer of the former kind is OpenStack. Many of the OpenStack clients validate the `Content-Length` header sent to the client and the actual length of the body received. To them, handling decompression is a fair trade-off to be certain they're receiving and handling a valid response.

Another consumer is Betamax (or really any tool that (re)constructs Response objects) because when it is handling the full process of making a totally valid response, it needs the content to be in the compressed format.

I'm sure there are others that neither @Lukasa or I know about that also rely heavily on this behaviour.
",sigmavirus24,Lukasa
2148,2014-07-24 21:31:13,"@sigmavirus24 @alex Fixed. Not sure what I was thinking, there.
",romanlevin,alex
2146,2014-07-23 06:14:57,"I have a background worker that makes many http requests to various APIs. I've found that every time my job runs, a few requests appear to just ""hang"" for quite a long time.
I've investigated further and it appears the requests are not actually hanging, but my python code ""hangs"" when it invokes `OpenSSL's` `SSL_CTX_load_verify_locations` function.

I forked `requests` and added some more logging. The last entry to be logged in my code is `begin: ssl_wrap_socket: load verify locations` and that corresponds to [this line](https://github.com/WiFL-co/requests/blob/master/requests/packages/urllib3/util/ssl_.py#L118).

One possible conclusion is that SSL_CTX_load_verify_locations is hanging because of parallel file access or the version of OpenSSL on my Heroku dyno has a bug. FYI the `requests` library is calling python's `load_verify_locations` which is what then calls `OpenSSL`. I am not invoking these functions manually.

[I've created a question on StackOverflow](http://stackoverflow.com/questions/24628866/why-does-my-program-hang-after-urllib3-logs-starting-new-https-connection) that provides extensive information. I am using the following dependencies:



@Lukasa has been really helpful thus far, and I've been able to narrow it down. I figured i'd throw this issue up and see if anyone else has some insight.
",scottccoates,Lukasa
2142,2014-07-21 18:46:08,"It seems urllib3 changed the timeout logic such that, we now have a failing test (`test_stream_timeout`). We now receive a `MaxRetryError` instead of a `TimeoutError` from urllib3. Either we have to adjust or this is a bug that needs to be fixed in urllib3.

Thoughts @shazow @lukasa?
",sigmavirus24,shazow
2142,2014-07-21 18:59:08,"Don't think it's strictly a urllib3 bug: I seem to recall @shazow had a comment in his urllib3 code wondering why `Timeout`s weren't wrapped in `MaxRetryError`s.

As for what to do, I'm not sure. Check what the wrapped exception is, maybe?
",Lukasa,shazow
2141,2014-07-18 17:16:21,"This looks good to me. We're already using the print function in several places on the advanced.rst page. Any objections @Lukasa ?
",sigmavirus24,Lukasa
2129,2014-07-22 20:23:04,"There are some nice changes in here. Looking forward to hearing what @sigmavirus24 and @Lukasa have to say :)
",kennethreitz,Lukasa
2118,2014-07-02 16:11:52,"> The requests library seems to grow more and more keyword arguments to try to provide all of the flexibility that SSL users need.

I'm not sure if you mean the fundamental `get/post/put/delete/head/options/request` methods/functions, but if that is what you're referring to, those haven't changed since before 1.0 was released. If you mean that there's an increasing amount of flexibility within adapters to fine tune this, then yes, you're correct.

> If the requests library under Python 3 started supporting a context= parameter like the Standard Library protocols, then users could fine-tune their encryption settings without requests having to become more complicated.

We're very unlikely to start supporting a keyword argument on only one version of Python. `urllib3` will discard keyword arguments on specific versions of Python, but we have yet to do that.

In general I like the idea, but so long as we will support Python 2 (unless a backport of the SSL module can be successfully provided), we will have trouble supporting this. Further, we're ignoring whether or not @kennethreitz would even want to add this keyword argument. I for one would be in favor of adding it (when we can support all Python versions equally), but I don't get to make these decisions ;).

That said, I'll have to investigate if urllib3 will support `SSLContext` objects (because I frankly don't remember). I'll look into that tonight unless @shazow chimes in before then. I'm also willing to do the work to allow urllib3 to accept them.
",sigmavirus24,kennethreitz
2118,2014-07-02 16:11:52,"> The requests library seems to grow more and more keyword arguments to try to provide all of the flexibility that SSL users need.

I'm not sure if you mean the fundamental `get/post/put/delete/head/options/request` methods/functions, but if that is what you're referring to, those haven't changed since before 1.0 was released. If you mean that there's an increasing amount of flexibility within adapters to fine tune this, then yes, you're correct.

> If the requests library under Python 3 started supporting a context= parameter like the Standard Library protocols, then users could fine-tune their encryption settings without requests having to become more complicated.

We're very unlikely to start supporting a keyword argument on only one version of Python. `urllib3` will discard keyword arguments on specific versions of Python, but we have yet to do that.

In general I like the idea, but so long as we will support Python 2 (unless a backport of the SSL module can be successfully provided), we will have trouble supporting this. Further, we're ignoring whether or not @kennethreitz would even want to add this keyword argument. I for one would be in favor of adding it (when we can support all Python versions equally), but I don't get to make these decisions ;).

That said, I'll have to investigate if urllib3 will support `SSLContext` objects (because I frankly don't remember). I'll look into that tonight unless @shazow chimes in before then. I'm also willing to do the work to allow urllib3 to accept them.
",sigmavirus24,shazow
2118,2014-07-02 16:39:48,"> Further, we're ignoring whether or not @kennethreitz would even want to add this keyword argument.

By **no means** did I intend to ignore @kennethreitz’s wishes! Alas. Opening this issue was, I had thought, precisely the means by which Mr. Reitz’s wishes could become known. Would there have been a more appropriate forum for asking my question?

> In this I agree with you.

Thank you, @Lukasa! Sorry if I worded the issue awkwardly and it required multiple read-throughs.

>  I think requests should continue its proud tradition of solving the 80% use-case … I'd never accept adding a `ciphers` kwarg to the main API … This would allow us to write something like the `SSLAdapter`

So the keyword arguments to `get()` and the other functions are not “kitchen sink” collections of everything that _could_ be specified, but a smaller collection of settings, and users are intended to step back and create adapters for more difficult cases? Then you are correct that what I probably need is an adapter that accepts an `SSLContext` for building connections!

The `urllib3` library accepts an `ssl_version` keyword? That, I fear, is a first step towards insanity, and a course which can be stopped by turning to `SSLContext`. Because the next logical step after `ssl_version` (which is really setting what OpenSSL calls the “protocol” version, from what I can see?) is adding a `ciphers` keyword, and then a `dh_params` keyword, and then `ecdh_curve`, and so forth.

In fact, what I really probably want is an adapter that does not even know that `SSLContext` exists, but simply accepts that I have gotten ready an object with a `wrap_socket()` method that it can use when it is ready to negotiate an encrypted connection. That way, as long as an SSL library that I want to use in the future also offers a `wrap_socket()` method (think of PyOpenSSL, or that new `cryptography` project), then it would automatically work if dropped in to the transport object.
",brandon-rhodes,kennethreitz
2117,2014-07-03 10:50:41,"Recommend we open a bug report on urllib3 if @shazow agrees.
",Lukasa,shazow
2115,2014-07-02 15:31:28,"Assigned to @Lukasa for review
",sigmavirus24,Lukasa
2111,2014-06-26 18:22:57,"Unless @kennethreitz disagrees, I think this can be closed.
",sigmavirus24,kennethreitz
2110,2014-06-24 16:59:57,"I wonder if this has anything to do with Connection Pooling. @shazow?
",Lukasa,shazow
2105,2014-06-21 01:00:08,"@robvdl it does not say that it ""does not support Python 3.4"" it is merely an omission that we do in fact support 3.4. I'd like to wait for @kennethreitz to add a Python 3.4 builder to the Jenkins server before adding it however.
",sigmavirus24,kennethreitz
2095,2014-06-12 12:47:33,"Thanks for this!

I have mixed feelings. In general, this seems really helpful, but it also seems like it could introduce difficult-to-trace bugs when working with unhelpful upstream websites. Altogether I'm tempted to err in favour of the change though.

It'll need a unit test though: mind writing one?

In the meantime, @kennethreitz, you interested in this change?
",Lukasa,kennethreitz
2095,2014-06-12 13:02:11,"I'm :-1: on this change if only because I can imagine consumers like CloudFlare and Runscope relying on the fact that we follow the redirects each time. Sure this provides a speed-up and performance enhancement but I'm not convinced this won't make some people's lives much harder and less pleasant. Before we move on we should at least gather a lot more feedback from the people who rely so heavily on requests.

@dolph, @johnsheehan, @bryanhelmig, @dstufft, @mtourne, @dknecht can any of you weigh in or have someone more appropriate way in please?
",sigmavirus24,dstufft
2095,2014-06-12 13:02:11,"I'm :-1: on this change if only because I can imagine consumers like CloudFlare and Runscope relying on the fact that we follow the redirects each time. Sure this provides a speed-up and performance enhancement but I'm not convinced this won't make some people's lives much harder and less pleasant. Before we move on we should at least gather a lot more feedback from the people who rely so heavily on requests.

@dolph, @johnsheehan, @bryanhelmig, @dstufft, @mtourne, @dknecht can any of you weigh in or have someone more appropriate way in please?
",sigmavirus24,bryanhelmig
2095,2014-06-12 17:38:04,"@ericfrederich It does control whether we follow redirects. `resolve_redirects` returns a generator, so if `allow_redirects` is false that generator doesn't get consumed and no redirects get followed.

The key thing to note is that requests is not, in fact, a browser, and there are times users are going to want more control. With that said, the bigger problem is that we're strongly resistant to adding more fields to the `Session`. That's why I wanted @kennethreitz's insight, just to see if he thinks this is a reasonable exception.
",Lukasa,kennethreitz
2087,2014-06-07 08:56:07,"As a companion to #2086, this updates all the other references to RFC 2616 in our codebase and documentation to their new locations. Gonna let @sigmavirus24 review this rather than merge it myself (which I'd normally do) just so he can sanity-check.
",Lukasa,sigmavirus24
2086,2014-06-07 06:54:12,"For a long time we've had a fallback value in `response.encoding` of `ISO-8859-1`, because RFC 2616 told us to. RFC 2616 is now obsolete, replaced by RFCs 7230, 7231, 7232, 7233, 7234, and 7235. The authoritative RFC on this issue is RFC 7231, which has this to say:

> The default charset of ISO-8859-1 for text media types has been removed; the default is now whatever the media type definition says.

The media type definitions for `text/*` are most recently affected by RFC 6657, which has this to say:

> In accordance with option (a) above, registrations for ""text/*"" media types that can transport charset information inside the corresponding payloads (such as ""text/html"" and ""text/xml"") SHOULD NOT specify the use of a ""charset"" parameter, nor any default value, in order to avoid conflicting interpretations should the ""charset"" parameter value and the value specified in the payload disagree.

I checked the registration for `text/html` [here](https://www.iana.org/assignments/media-types/media-types.xhtml#text). Unsurprisingly, it provides no default values. It does allow a charset parameter which overrides anything in the content itself.

I propose the following changes:
1. Remove the ISO-8859-1 fallback, as it's no longer valid (being only enforced by RFC 2616). We should _definitely_ do this.
2. Consider writing a module that has the appropriate fallback encodings for other `text/*` content and use them where appropriate. This isn't vital, just is a ""might be nice"".
3. Begin checking HTML content for meta tags again, in order to appropriately fall back. This is controversial, and we'll want @kennethreitz to consider it carefully.
",Lukasa,kennethreitz
2086,2014-06-08 14:56:11,"> Remove the ISO-8859-1 fallback, as it's no longer valid (being only enforced by RFC 2616). We should definitely do this.

I agree that we should remove the fallback. I do wonder how we should handle `Response#text` in the event that the server does not specify a charset (in anyway, including the meta tags of the body). Should we disable `Response#text` conditionally either through an exception or something else? Not doing so will rely more heavily on chardet, which I have decreasing confidence in given the number of new encodings it does not detect.

> Consider writing a module that has the appropriate fallback encodings for other text/\* content and use them where appropriate. This isn't vital, just is a ""might be nice"".

Given that this is not guaranteed to be included in requests, I'm fine with adding it to the toolbelt, that said. I'm also okay with making this a separate package so users can just use that with out having to install the rest of the toolbelt. That, however, is a separate discussion.

> Begin checking HTML content for meta tags again, in order to appropriately fall back. This is controversial, and we'll want @kennethreitz to consider it carefully.

We still have a method to do this in `utils`, right? I don't like the idea in the slightest, but it won't cost extra effort. That said, we have to make sure any charset provided in the media type takes precedence.
",sigmavirus24,kennethreitz
2086,2014-06-08 16:18:06,"Agreed from a correctness perspective, but I wonder if @kennethreitz is going to want it from a usability perspective.
",Lukasa,kennethreitz
2071,2014-05-28 01:27:48,"@Lukasa this is the actual behaviour. I tested on Python 2 and 3 and this only happens on Python 3 due to [these lines](https://github.com/kennethreitz/requests/blob/6c72509f5bb0e9cd8fad64a44ba99687ed044771/requests/models.py#L434..L439) I'm pretty sure we don't want this behaviour by default. I'm going to work on this some tonight and post my progress before heading to sleep.
",sigmavirus24,Lukasa
2065,2014-05-26 14:25:46,":+1: I'm in favor of this change. The only thing that would be better would be if @kennethreitz added support to Jenkins for 3.2, 3.4, and pypy 2.2 :)
",sigmavirus24,kennethreitz
2064,2014-05-26 13:37:14,"I could be wrong, but I think I remember @dstufft saying that pip support for 3.2 wasn't a high priority. That aside, I'm not quite sure why we support using simplejson. If @mgeisler hadn't linked to #710 I would have guessed it was to support Python 2.5 (which we no longer do).
",sigmavirus24,dstufft
2061,2014-05-23 16:40:54,"@shazow, are you open to doing part 2?
",Lukasa,shazow
2050,2014-05-18 01:42:48,"This is great work @Hasimir! I really appreciate your effort in putting this together.

Personally, I think this is better suited to a blog post and not the documentation. I'll leave this open, though, until @Lukasa wakes up. :)
",sigmavirus24,Lukasa
2049,2014-05-17 22:42:52,"Basically identical to @sigmavirus24's suggested code - thanks for that pointer. Just added docstring.

There aren't tests for particular Adapter implementations, and I've not added any at this stage.
I have tested it with the following (which achieves what I want):


",codedstructure,sigmavirus24
2048,2014-05-17 20:42:42,"I think there's no good reason not to do this, and it would definitely make this easier. @sigmavirus24?
",Lukasa,sigmavirus24
2045,2014-05-16 14:26:47,"Thanks so much for this! :cake:

This looks like a good catch. I think the generator created in `Response.iter_content` should probably be looking for Timeout errors, both from urllib3 and from the socket module, and should catch and wrap them. @sigmavirus24?
",Lukasa,sigmavirus24
2038,2014-05-12 18:47:22,":+1: :cake:

@kennethreitz Just so you're aware, the reason this has been opened is because of shazow/urllib3#385, which is a nastly TLS-related bug on Python 3.4.1. This will be a big problem for `pip` in addition to our users, so we'll want to think hard about pushing a new release. Let me know if you decide to do that soon and @sigmavirus24 and I will tidy up the code ready for a release.
",Lukasa,kennethreitz
2038,2014-05-12 18:47:22,":+1: :cake:

@kennethreitz Just so you're aware, the reason this has been opened is because of shazow/urllib3#385, which is a nastly TLS-related bug on Python 3.4.1. This will be a big problem for `pip` in addition to our users, so we'll want to think hard about pushing a new release. Let me know if you decide to do that soon and @sigmavirus24 and I will tidy up the code ready for a release.
",Lukasa,sigmavirus24
2038,2014-05-12 18:59:41,"@Lukasa @sigmavirus24 alright, let's get a release ready. :)
",kennethreitz,sigmavirus24
2029,2014-05-01 09:54:44,"I've honestly got no idea. @t-8ch, does the underlying ssl library not time out based on the socket timeout?
",Lukasa,t-8ch
2027,2014-04-29 15:51:43,"Hi @gauravp2003,

Much of the documentation around requests, including the [README](https://github.com/kennethreitz/requests#contribute), suggests that the issue tracker is for bugs or feature requests, not questions. If you have questions, the should be asked on [StackOverflow](https://stackoverflow.com/questions/tagged/python-requests). @Lukasa, someone else, or I will answer it there.

Thanks for your interest in requests,
",sigmavirus24,Lukasa
2025,2014-05-02 19:07:59,"@willingc and I are working on this.
",sigmavirus24,willingc
2021,2014-04-26 13:29:49,"LGTM. Merge when ready @kennethreitz 
",sigmavirus24,kennethreitz
2018,2014-04-26 22:50:42,"To be clear for those who aren't sure, the way @ouroborus' suggestion differs from the current logic is that we take the proxies from the request, then apply proxies from the environment, then finally apply proxies from the `Session`.

I'm open to re-ordering the precedence of the priorities. @sigmavirus24, thoughts?
",Lukasa,sigmavirus24
2013,2014-04-23 01:39:47,"Hey @cheecheeo, thanks for opening this!

Your code is fine and the idea is valid. The problem is that this makes the rest of the library inconsistent and it is _not_ desired behaviour. We have no intent to implement this and we have never intended for the a Request's representation to be used like this.

I'll wait for @Lukasa to weigh in, but I'm pretty sure we will not be accepting this contribution.

Cheers
",sigmavirus24,Lukasa
2010,2014-04-18 16:30:32,"As @kennethreitz and I discussed at PyCon, this is a first pass at a rewrite of part of the Philosophy section of the docs, attempting to explain Requests' slightly unusual management style. I'd like some feedback.

Some ideas:
- should @shazow be mentioned? I felt like he should but I wasn't sure how best to explain the relationship.
- is there anything major that I've missed?
",Lukasa,kennethreitz
2010,2014-04-18 16:30:32,"As @kennethreitz and I discussed at PyCon, this is a first pass at a rewrite of part of the Philosophy section of the docs, attempting to explain Requests' slightly unusual management style. I'd like some feedback.

Some ideas:
- should @shazow be mentioned? I felt like he should but I wasn't sure how best to explain the relationship.
- is there anything major that I've missed?
",Lukasa,shazow
2006,2014-04-16 20:03:11,"This is an interesting idea @untitaker! It does feel like it'd be useful.

I wonder if we should trial it in [the toolbelt](https://github.com/sigmavirus24/requests-toolbelt) first, see if it's useful. @sigmavirus24?
",Lukasa,sigmavirus24
2002,2014-04-14 21:32:06,"Yeah I'm :+1: for removing this. @Lukasa thoughts? 

I know it would have to wait for requests 3.0 but it might still be good to have a wishlist for 3.0 
",sigmavirus24,Lukasa
2002,2014-04-14 21:50:16,"I just chatted with Kenneth about this, he definitely doesn't like it. I think we should be leaving this on a wishlist for 3.0, but @kennethreitz might disagree.
",Lukasa,kennethreitz
1999,2014-04-12 18:37:01,"This looks great to me, thanks! :cake: @kennethreitz, are you happy with this?
",Lukasa,kennethreitz
1996,2014-04-07 21:18:43,"Hmm, yup, from looking at the code it's clear that we no longer guess the MIME type of the uploaded file. I wonder if that was deliberate.

Looks like it went away in af4fb8cedca7c331b8c914a40c477a2cb02055e1 (part of #1640) when we switched to explicitly using `RequestField` objects. Assuming we're happy to guess MIME types I'm happy to put it back. @kennethreitz, should we be guessing MIME types on multipart upload parts?
",Lukasa,kennethreitz
1996,2014-10-10 19:09:49,"Hey @kennethreitz I was rummaging through issues and we never did get an answer from you. Was it intentional that content type guessing was removed? There's an API for the user to specify it now explicitly so we just might want to document this.
",sigmavirus24,kennethreitz
1995,2014-04-07 06:14:08,"I'm happy to do this as well. @kennethreitz, do you want to do this?

Would be nice if we could have a bit of the docs that talks about building the most secure possible form of requests, including stuff like installing OpenSSL from Homebrew and then building against that.
",Lukasa,kennethreitz
1995,2014-07-01 00:54:26,"@kennethreitz any update on this? 
",sigmavirus24,kennethreitz
1991,2014-04-05 14:52:14,"We should remove all reference of the mirrors. PyPI is at the point now where the mirrors are entirely unnecessary thanks to the use of a CDN by the infrastructure team. @dstufft can speak more to this.
",sigmavirus24,dstufft
1989,2014-04-03 00:05:43,"Just a doc change. If anyone thinks they deserve separate items let me know. /cc @dstufft
",sigmavirus24,dstufft
1987,2014-04-01 13:01:06,"I'm basically in favour of this, and I'm happy to do the legwork required for it, but I want to know if @kennethreitz wants it or not.
",Lukasa,kennethreitz
1980,2014-03-29 09:05:48,"Wow, this is a serious chunk of work: thankyou!

I have to get around to reviewing this, which I haven't done yet, but it seems like it's a good enhancement. I'm not going to speculate on how @kennethreitz will want to handle the API change: whether he'll want extra properties or to hold off until Semver lets us make this backwards incompatible change.
",Lukasa,kennethreitz
1980,2014-03-29 15:31:21,"I haven't looked at this at all and don't have any opinion yet. I just think the important context here is that (if I remember correctly) @kennethreitz was explicit in his desire to only handle the case where there's a single link for a given relation type. I'll have to find the corresponding issues/PRs to double check though. 

In general I'm always :+1: for conforming to RFCs. As a side note, if this is rejected, I would be more than happy to accept this as a feature for the [toolbelt](/sigmavirus24/toolbelt).
",sigmavirus24,kennethreitz
1979,2014-03-31 15:40:55,"Aha, ok.

I don't think auth handlers can handle an auth challenge that occurs _after_ a redirect: I think we lose track of the auth handler. @sigmavirus24 does this match your reading of the code?

If that's the case, do we consider that behaviour a bug?
",Lukasa,sigmavirus24
1976,2014-03-26 13:38:02,"In related news, @kennethreitz is there an email address that the Jenkins server emails when tests fail? If not, could you add @Lukasa and/or me to the notifications? Assuming Jenkins runs on pushes to master (i.e., when you merge a PR) I would have seen this and fixed it sooner.
",sigmavirus24,kennethreitz
1973,2014-03-24 20:55:44,"Hmm, @shazow, thoughts?
",Lukasa,shazow
1972,2014-03-23 16:39:33,"@sigmavirus24 Sure updated now
",avidas,sigmavirus24
1972,2014-03-23 16:49:03,":+1: :cake: Thanks. Assigning @kennethreitz since @Lukasa and I agree this is ready to merge.
",sigmavirus24,kennethreitz
1972,2014-04-04 16:05:28,"Will you merge this @kennethreitz ?
",avidas,kennethreitz
1968,2014-03-23 11:34:08,"This is really up to @kennethreitz, but I suspect he's going to want to avoid the extra procedural overhead.
",Lukasa,kennethreitz
1967,2014-03-20 14:36:30,"So in that situation we should have dropped the connection, which is what I'd expect if we'd exhausted it on the read (which we do).

@shazow, want to run your eye over this quickly?
",Lukasa,shazow
1963,2014-03-23 14:54:08,"@kennethreitz since you're around, care to weigh in on this one?
",sigmavirus24,kennethreitz
1962,2014-03-15 16:36:18,"@Lukasa I was considering calling `list(r.history)` instead of `tuple(r.history)` but I cannot see a case there where it wouldn't be a list.
",sigmavirus24,Lukasa
1962,2014-03-23 14:54:20,"@kennethreitz thoughts?
",sigmavirus24,kennethreitz
1951,2014-03-12 19:26:15,"This pull request falls into three commits.

The first is a refactoring of the `get_environ_proxies` method to make it possible to evaluate whether a given URL is in the NO_PROXY list.

The second is a refactoring of the `resolve_redirects` method to move rebuilding the `Authorization` header to its own method.

Both of these commits are intended to set the stage for the third, which is a new method that re-evaluates proxy configuration on a redirect, and re-evaluates proxy authorization as well.

I don't want this merged yet, but it's a pretty sizeable change and I'd like to get eyes on it as early as possible. The key problem right now is that I don't have tests, though it should be possible for me to test this fairly easily. I'll want to add tests before we merge this.

@sigmavirus24, do you mind taking a look?
",Lukasa,sigmavirus24
1949,2014-03-12 18:36:54,"Can I get review from @sigmavirus24 and @ionrock?
",Lukasa,sigmavirus24
1948,2014-03-12 11:20:34,"And, in fact, @ceaess has already provided a Pull Request that fixes this in #1935, which will be in the next release.
",Lukasa,ceaess
1947,2014-03-12 07:02:13,"@kennethreitz I'm open to doing the work here if this is something you think you'd like.
",Lukasa,kennethreitz
1945,2014-03-10 22:05:45,"Thanks for this!

I have no strong opinions, it looks fine. =) +0.5. @sigmavirus24?
",Lukasa,sigmavirus24
1945,2014-03-11 13:36:46,"I have a couple concerns. The first is that you're introducing yet another way for the user to set the CA bundle. You did not state that in your description. I assume you want this in order to specify the path that you would otherwise specify to `verify` as a plain string.

That noted, I'm not comfortable with verify having a trinary value (its current state) let alone having 4 possible values (`True`, `False`, `'/path/to/bundle'`, and dictionary of **connection** options). Let's be clear, you are attaching these to the connection. We've held in the past that these are Transport Adapter concerns. Whilst being less invasive and a less severe API change, I'm still -0. I still feel that if you have all of the information for each URL you could structure it in such a way that a custom transport adapter could use it and attach it to the connection.  The current API affords for what you want, just not in the simplest way possible. Especially given Kenneth's hesitancy to tie requests too closely to urllib3's API, I'm hesitant to be positive about this.

If @kennethreitz likes the change though my only request would be that you use `#get` on the verify dictionary like so:



This is far better than first checking for the value first and then assigning it. Each of those assigned values will be `None` if in fact the parameter is not provided or the dictionary is simply empty.
",sigmavirus24,kennethreitz
1945,2014-10-10 15:16:45,"@schlamar in this case, if I were you, I would do the following:
1. Use @t-8ch's [StackOverflow answer](http://stackoverflow.com/a/22794281/1953283) to create a new Adapter
2. Register the adapter on your session like so:


",sigmavirus24,t-8ch
1944,2014-03-10 07:24:49,"This is my proposal which fixes #1939 completely.

@sigmavirus24 missed one possible exception in #1940 (ChunkedEncodingError) and didn't handle RuntimeError correctly (because this means that the content is already consumed).

This also addresses the issue that a decoding error is already thrown in Adapter.send by moving the content reading part at the end of Session.send.
",schlamar,sigmavirus24
1944,2014-03-10 15:54:51,"This looks reasonable to me. I'll let @sigmavirus24 take a look too.
",Lukasa,sigmavirus24
1944,2014-03-24 16:26:40,"I suspect that `r.transfer_encoding` would be very useful for `pip`. I seem to recall that they've had some trouble with servers sending `.tar.gz` files with `Content-Encoding: gzip` set, causing `r.content` to save the ungzipped tar file. @dstufft, does that ring a bell?
",Lukasa,dstufft
1941,2014-03-06 16:31:13,"Not that I am aware of.

@sigmavirus24, is this worth putting in the toolbelt? `flatten_and_form_encode`?
",Lukasa,sigmavirus24
1940,2014-03-05 03:08:13,"This fixes #1939 and part of @zackw's rehaul of redirection.

To better test this, I'd like to add Betamax as a test dependency. Also, as a separate PR I plan to kill the usage of `RuntimeError` in `Response#content`. That's awful and the community consensus is that nothing should ever raise that. 
",sigmavirus24,zackw
1940,2014-03-08 02:32:36,"> The actual point of raising the exception is here: https://github.com/kennethreitz/requests/blob/master/requests/adapters.py#L391

@schlamar we cannot catch that in `Session#resolve_redirects`. To catch it in the adapter would be a serious change in behaviour. I wonder why we consume all of the content in the adapter though. If there's a decoding error, we don't even attempt to return a Response to the user. That may be fodder for an entirely different issue though. If @Lukasa is satisfied with this as an immediate solution to what was reported, I am too. But I want to investigate that other piece in the adapter as well.
",sigmavirus24,Lukasa
1939,2014-03-07 14:09:14,"Interesting. It's very likely that this is a duplicate of https://github.com/shazow/urllib3/issues/206 / https://github.com/kennethreitz/requests/issues/1472 because there is also an 301 redirect in the curl output. @shazow 
",schlamar,shazow
1935,2014-02-28 16:12:05,"Hey @Lukasa can we have some :eyes: on this? I paired with @cjstapleton on this PR this morning. Let us know what you think.
",sigmavirus24,Lukasa
1934,2014-02-28 14:30:45,"@Lukasa this is essentially a documentation change. I included the other information about cacert.pem but I'm not sure it is entirely necessary in the NOTICE file. I'll happily remove it if you share the same doubts.
",sigmavirus24,Lukasa
1930,2014-02-26 07:29:07,"> Session.send now offers a new mode, `iter_redirects=True`, in which
> it returns an iterator over redirects instead of the first response. [...] If both allow_redirects=False and iter_redirects=True are specified, allow_redirects=False wins.

Aah! Nope, that's not the way to do this. That last sentence should be a bit of a clue: your arguments to a function should never be fighting each other.

More importantly, function argument should not change the type of the returned value. This makes it substantially more difficult to reason about the function. If you really want to do this then you're going to have to have a new function call here, rather than a new parameter.

@kennethreitz We're going to want your insight here. =)
",Lukasa,kennethreitz
1926,2014-02-19 16:13:25,"I don't have the time to find the spec but I think Headers are supposed to be encoded as latin strings (and that's how basic authentication and digest authentication is specified for the server). If Python cannot coerce your unicode credentials to Latin we should raise an exception. UnicodeEncodeError is a good one in my opinion. I don't like the idea of adding yet another exception.

I agree that this doesn't give the user a great deal of information, but at the same time, this is a very accurate message.

@Lukasa should we be checking credentials ahead of time? The problem with that is the fact that we gleam authentication credentials from the Session too. There's no way to check this except in the preparation of the request. If we inherit from the UnicodeDecodeError, we could raise a new exception with the original message attached. It might be more informative to the user, `AuthenticationEncodeError`? Or perhaps a more generic `HeaderEncodeError`?
",sigmavirus24,Lukasa
1921,2014-02-14 22:16:24,"Assigning to @Lukasa for review and once past that, I will assign it to Kenneth.
",sigmavirus24,Lukasa
1914,2014-02-13 07:40:03,"Thanks for raising this!

We actually quite like this behaviour, though: it means you can examine the exception object we wrapped. I'm going to let @sigmavirus24 weigh in here, but I think we're happy as-is.
",Lukasa,sigmavirus24
1913,2014-02-11 21:24:06,"Wow, this is a substantial change, and an impressive bit of work. I'm not going to dive into code review at this moment for two reasons: I don't have time, and I think we'll want @kennethreitz involved early in the discussion on this pull request.

Nevertheless, there'll definitely be a couple of code review comments if we decide to move ahead with this. I'm mostly interested to see what Kenneth has to say about the idea though.
",Lukasa,kennethreitz
1913,2014-02-12 08:49:35,"I've glanced through the code review comments stuff so far, and wanted to talk more generally about how this relates to our API freeze.

@zackw has said in one of his comments that he believes these changes would be welcome extensions to the API for anyone who has to manually handle redirects. That's probably true. However, our API freeze policy does not say that we are freezing the API ""except when it'll be useful for people if it was extended"". By default, the Requests answer to API changes will always be ""no"". The reason this issue is still open and being discussed is because we think there is potentially enough value here that we want to look at it in depth. Please don't assume that we hate your PR, @zackw, we're just starting from a very conservative position.

Next, there is a question about how much affordance we should give to people who circumvent Requests' redirection policy. The general Requests policy on this sort of thing (see also: `PreparedRequest` objects) is that if you don't like the way Requests does it you should do it yourself. We have made concessions here in the past, but not many and always under substantial duress.

Again, I'm going to hold off more dramatic code review until @kennethreitz gives an idea of whether he's likely to want this change at all.
",Lukasa,kennethreitz
1912,2014-02-11 20:08:25,"@zackw actually this is the right direction. On any response that did have history previously we were returning tuples and on any without we were returning an empty list. The fact of the matter is that history on a response should be immutable. For that to be the case, it should be a tuple. I also don't quite understand your arguments. If you're manually processing redirections or using a hook you will now get a tuple after this change, as you should have in the first place. That does raise a good point that hook authors will expect a list though (if they're writing hooks dealing with manual processing of redirecitons).

@Lukasa that makes this (sort of) a backwards incompatible change.
",sigmavirus24,Lukasa
1907,2014-03-26 09:45:46,"_Use by default SSL CA certificate bundle from the platform_ is what OP proposes in this issue and that's something I totally agree with. It feels so natural to me that I find it difficult someone could oppose this, yet alone argue that not doing this is better experience. Yet this is what I see here and that's why I decided to share my opinion with you.
@Lukasa states

> I don't think we should change which certificates we use based on whether or not you install an optional dependency. Our current behaviour WRT pyopenssl is to enable additional features without changing our current behaviour. That's what you should be aiming for here.

This argument is misguided here as pretty much everyone agrees that when you're dealing with security you should **by default** be as secure as you are able to be, given the environment you operate in. This means using system CA certificates by default (and fallback to bundled ones if it's not possible or very hard to do) and not bundled ones. I think one might argue that security begs for policy of graceful 
degradation.

Also @Lukasa states

> Surely you'd want to ensure that all your users have the exact same certs on all platforms, to avoid annoying platform-specific bugs?

Surely, if the only goal is to _avoid annoying platform-specific bugs_ then yes. But if the goal is to make requests secure by default (and this is in line with _HTTP library for humans_ motto) then surely you would want to ensure that all your users have _the best certs available on their systems_.

Additional bonus is that you save packagers from having to patch this (mis)feature.

I'm curious what @dstuff and @t-8ch think.
",piotr-dobrogost,t-8ch
1906,2014-02-07 13:37:31,"I agree that it's a real problem. I do not necessarily agree that Requests needs a workaround for every bug in any of our dependencies. 

What is not clear to me at this time is how severe this bug is, how widespread it is, and how easy it is to work around. It's also not easy for me to find those things out today: I'm at a conference and won't have time near a laptop. It would be useful if @sigmavirus24 could take a look: otherwise I'll have to dig into it tomorrow. 

The key thing to know is that OpenSSL has had other bugs in the past that we haven't worked around in the core Requests code. Most notably, 0.9.8f (I think) that ships on OS X 10.8 has a similar bug in it that we have never worked around. 

We are not required to fix every bug in all of our dependencies, especially implicit ones. We _may_ fix this one, but I don't know yet. 

Note that another workaround is to use a different version of OpenSSL. Just saying. 
",Lukasa,sigmavirus24
1905,2014-02-07 06:25:27,"I understand fixing AppEngine is not something you intend to pursue. In case it helps anyone else, though, I think I have found the cause: line 172 [here](https://github.com/kennethreitz/requests/pull/1892/files#diff-28e67177469c0d36b068d68d9f6043bfR172). It's my impression that [get_netrc_auth essentially fails on AppEngine](https://github.com/kennethreitz/requests/pull/1709/files#diff-5956087d5835a57d9ef6fff974f6fd9bL97), and without that, authentication will not survive redirects, resulting in infinite recursive redirects to any endpoint that requires Auth (not just POSTs). 

I'm not sure, but the absence of the feature mentioned here: https://github.com/kennethreitz/requests/pull/1892#issuecomment-33730591 , which at the time seemed like a ""nice-to-have"", may be a factor. 

@kennethreitz had seemed [concerned](https://github.com/kennethreitz/requests/pull/1892#issuecomment-33821403) that making this design decision for the developer might have adverse consequences... I wonder if there are others outside of the GAE ecosystem who would prefer Authentication tokens survive across redirects. 

Perhaps an option to allow this would assuage the GAE community as well as those in similar situations?
",rattrayalex,kennethreitz
1905,2014-02-07 07:26:47,"There are absolutely people who would prefer that we keep the auth tokens on over redirects. However, both @sigmavirus24 and I very strongly believe that being insecure by default is dangerous. @kennethreitz has a more nuanced view than we do, and I acknowledge that. However, I think that #1892 is a good change.

However, this bug cannot be #1892 because we haven't released a version with #1892 in it yet. =)
",Lukasa,kennethreitz
1905,2014-02-07 07:26:47,"There are absolutely people who would prefer that we keep the auth tokens on over redirects. However, both @sigmavirus24 and I very strongly believe that being insecure by default is dangerous. @kennethreitz has a more nuanced view than we do, and I acknowledge that. However, I think that #1892 is a good change.

However, this bug cannot be #1892 because we haven't released a version with #1892 in it yet. =)
",Lukasa,sigmavirus24
1903,2014-02-04 10:35:02,"That's not a problem at all. =) In the future, if you're worried about whether a question is a bug or simply a misunderstanding, you can email either myself or @sigmavirus24. =) We both publish our email addresses on GitHub, and we're always happy to take questions.
",Lukasa,sigmavirus24
1899,2014-02-02 17:31:11,"This has been proposed in the past and @kennethreitz vastly dislikes most of pep8 from my experience. I've never been able to tell exactly what style he likes to follow but it's very much his call. Until he can get around to replying to this, I would hold off on your efforts so you do not expend too much energy.

FWIW, As the maintainer of Flake8 as well as this project, I appreciate your desire to make this project more compliant.
",sigmavirus24,kennethreitz
1899,2014-02-02 17:38:34,"Ok, let's wait @kennethreitz's rely. It is interesting that he does't like PEP8 style.
",cli248,kennethreitz
1899,2014-02-02 17:51:50,"Haha, that reminds me Shakespeare's famous remark, **There are a thousand Hamlets in a thousand people's eyes**

I will wait @kennethreitz's reply and see what I should do next. 

BTW: I think @kennethreitz should come up with a style guide for `requests` if he doesn't like pep8. 
",cli248,kennethreitz
1899,2014-02-06 19:52:33,"While it's not a style guide for requests specifically, @kennethreitz 's guide linked above documents many of his opinions on python style in general. Though he does seem to say nicer things about PEP8 there: http://docs.python-guide.org/en/latest/writing/style/#pep-8
",rattrayalex,kennethreitz
1899,2014-02-07 04:52:45,"Ah! Not a lot of @kennethreitz in that ;-P
(tl;dr, none of the recent blames are his)
Thanks!
",rattrayalex,kennethreitz
1897,2014-02-02 09:40:12,"Thanks for this!

However, I'm on the fence. So far we've made a conscious decision not to _officially_ document this support in Requests, because we don't adequately test it. We don't have an SNI environment in our CI system, we've regressed this support in the past and we'll probably do it again.

We should combine that with the fact that Requests is meant to be _for humans_. This SNI support is a frankly-fairly-ugly workaround for a serious failing in Python 2. I'm happy to suggest this as a workaround for people everywhere else on the internet (and I do), but I'm reluctant to add it to the core documentation.

However, if the rest of the team wants to I'm happy to let them, so I'll leave this open until @kennethreitz and @sigmavirus24 take a look.
",Lukasa,kennethreitz
1897,2014-02-02 09:40:12,"Thanks for this!

However, I'm on the fence. So far we've made a conscious decision not to _officially_ document this support in Requests, because we don't adequately test it. We don't have an SNI environment in our CI system, we've regressed this support in the past and we'll probably do it again.

We should combine that with the fact that Requests is meant to be _for humans_. This SNI support is a frankly-fairly-ugly workaround for a serious failing in Python 2. I'm happy to suggest this as a workaround for people everywhere else on the internet (and I do), but I'm reluctant to add it to the core documentation.

However, if the rest of the team wants to I'm happy to let them, so I'll leave this open until @kennethreitz and @sigmavirus24 take a look.
",Lukasa,sigmavirus24
1897,2014-02-04 10:34:12,"Excellent, I'm happy with that.

Normally I'd merge documentation changes myself, but this changes the layout a little bit and I want to chat with @kennethreitz a bit before we go ahead and merge. It will definitely get merged though. =D
",Lukasa,kennethreitz
1896,2014-01-31 13:32:01,"Thanks for this!

I'm -0.5 on this change, because I believe it to be a security risk. It is upsettingly easy to create a situation where such a module is installed that defines a `where()` function that returns arbitrary certificates, and such behaviour would not be noticed. Monkeypatching makes this behaviour harder (though not impossible).

I'm going to wait until @sigmavirus24 weighs in. =)
",Lukasa,sigmavirus24
1893,2014-01-30 17:34:26,"One day, crate.io will return as pypi.python.org. Unless @dstufft explodes in a ball of caremad.
",Lukasa,dstufft
1892,2014-01-29 19:15:37,"This should be a fix for #1885.

@sigmavirus24, can you give me some code review? =)
",Lukasa,sigmavirus24
1892,2014-01-30 20:38:17,"@kennethreitz we absolutely cannot continue reusing authorizations on redirects to sites that are not the same host. With that in mind we almost certainly need to issue a CVE. I'll happily work on that though.

We've been leaking credentials and we need to at least address that. Whether we repopulate the auth after stopping the leak or not is more of a feature decision. I'm sure one of the security experts, like @dstufft would back up @Lukasa and I on that.
",sigmavirus24,dstufft
1888,2014-01-28 17:20:23,"This seems like a reasonable fix to me. Unfortunately, the unit test is not a particularly good one because most people don't use the SNI build of Requests in their development systems. Additionally, we don't do it in our CI server (though we probably should, I'll bring it up with @kennethreitz). Leave it there for now: if we can get it into the CI server it'll be fine, otherwise we'll need another test.
",Lukasa,kennethreitz
1885,2014-01-27 17:54:29,"Thanks for this!

Right now we don't strip authentication information of any kind on redirects. We could, but we can't ask the user for new credentials, we can only really go to `~/. Thoughts @sigmavirus24?
",Lukasa,sigmavirus24
1884,2014-01-27 10:01:29,"Thanks for this! Jython is weird. =)

I think before we merge this we should set up our CI server with a Jython build. If we're going to support Jython, let's do it properly. @kennethreitz do you want to do that yourself, or would you rather one of @sigmavirus24 or I did it?
",Lukasa,kennethreitz
1884,2014-01-27 10:01:29,"Thanks for this! Jython is weird. =)

I think before we merge this we should set up our CI server with a Jython build. If we're going to support Jython, let's do it properly. @kennethreitz do you want to do that yourself, or would you rather one of @sigmavirus24 or I did it?
",Lukasa,sigmavirus24
1884,2014-01-27 14:37:47,"@kennethreitz controls the Jenkins CI server so I would guess he'd be able to set up Jython if desired. FWIW, I would like to support Jython, but I'm curious about some things in the PR. None are really merge blockers.

The only thing that _is_ a merge blocker to me is failing HTTPS tests. If we're going to support Jython, given that we're the only package that does SSL as correctly as possible, I would want that to be fully functional before we ship anything advertising that we support Jython. Otherwise, the greatest draw to requests (beyond its API) is invalid and horribly broken. That raises the question:

How far out is SSL support?
",sigmavirus24,kennethreitz
1882,2014-01-25 19:33:34,"Thanks for this!

This is actually a behaviour in urllib3. I'm going to summon @shazow into this conversation, but I believe this is a deliberate design decision. We can often reuse the socket object to reconnect to the same host without the overhead of recreating the object. This makes this deliberate. I wonder if we can silence the warning.
",Lukasa,shazow
1877,2014-01-23 23:34:26,"Considering you're the first person in over 2 years to request this, it seems as though this is not something that is popular enough to belong in the core of requests. A feature like this would need to be requested a whole lot more for us to even consider adding this method.

On the other hand though, you could totally get this into the [requests-toolbelt](https://github.com/sigmavirus24/requests-toolbelt) which is designed for exactly this kind of thing. This is something that would be awesome to have and if you can put together a PR to handle the parsing, that would be awesome! I'd love to merge it over there provided you also add tests around it.

I'll leave this open to make sure that @Lukasa has a chance to weigh in, but I have a hunch he'll have the same opinion as me.
",sigmavirus24,Lukasa
1876,2014-01-23 03:17:22,"@kennethreitz :cake: :+1: :shipit: 
",sigmavirus24,kennethreitz
1869,2014-01-31 14:34:29,"Let's do it unless @kennethreitz has an object to restricting the library to HTTP/1.1
",sigmavirus24,kennethreitz
1867,2014-01-16 08:39:35,"From #1493. Provides a way to use the `Response` object as a context manager without adding the conceptual overhead to the `Response` class.

I'm still on the fence about whether we should document this or just make `Response`s context managers, but this seems the least controversial option so lets do this first.

Review: @sigmavirus24, @pepijndevos.
",Lukasa,pepijndevos
1867,2014-01-16 08:39:35,"From #1493. Provides a way to use the `Response` object as a context manager without adding the conceptual overhead to the `Response` class.

I'm still on the fence about whether we should document this or just make `Response`s context managers, but this seems the least controversial option so lets do this first.

Review: @sigmavirus24, @pepijndevos.
",Lukasa,sigmavirus24
1865,2014-01-15 13:51:50,"In 3.4 the standard library's previously woeful `ssl` module is being vastly improved, and one of the nice things it is getting is [support for Certificate Revocation Lists](http://docs.python.org/3.4/library/ssl.html#ssl.SSLContext.load_verify_locations). I'd like us to investigate supporting CRLs in versions of Python where they are available. If we can do this, it'll continue the ongoing trend of Requests being the single most secure way to do HTTP in Python.

It's highly probable that this will need to be a two-pronged approach: urllib3 will probably want to support some genericised CRL options, with Requests pasting over the genericness with a built-in ""correct"" behaviour. For that reason, I want to start a discussion with the relevant people to work out how we can do this.

Requests people: @kennethreitz, @sigmavirus24
urllib3 people: @shazow 
Our SSL guy: @t-8ch.

People who might be interested in either helping or providing insight and code review (to make sure we don't get this wrong) are @tiran (heavily involved with the standard library's `ssl` module) and @alex (currently on a big crypto kick). If either of you two are actually not interested that's totally fine. =)

Initial views: how should we split this work up, and how safe can Requests' defaults be?
",Lukasa,kennethreitz
1865,2014-01-15 13:51:50,"In 3.4 the standard library's previously woeful `ssl` module is being vastly improved, and one of the nice things it is getting is [support for Certificate Revocation Lists](http://docs.python.org/3.4/library/ssl.html#ssl.SSLContext.load_verify_locations). I'd like us to investigate supporting CRLs in versions of Python where they are available. If we can do this, it'll continue the ongoing trend of Requests being the single most secure way to do HTTP in Python.

It's highly probable that this will need to be a two-pronged approach: urllib3 will probably want to support some genericised CRL options, with Requests pasting over the genericness with a built-in ""correct"" behaviour. For that reason, I want to start a discussion with the relevant people to work out how we can do this.

Requests people: @kennethreitz, @sigmavirus24
urllib3 people: @shazow 
Our SSL guy: @t-8ch.

People who might be interested in either helping or providing insight and code review (to make sure we don't get this wrong) are @tiran (heavily involved with the standard library's `ssl` module) and @alex (currently on a big crypto kick). If either of you two are actually not interested that's totally fine. =)

Initial views: how should we split this work up, and how safe can Requests' defaults be?
",Lukasa,alex
1865,2014-01-15 13:51:50,"In 3.4 the standard library's previously woeful `ssl` module is being vastly improved, and one of the nice things it is getting is [support for Certificate Revocation Lists](http://docs.python.org/3.4/library/ssl.html#ssl.SSLContext.load_verify_locations). I'd like us to investigate supporting CRLs in versions of Python where they are available. If we can do this, it'll continue the ongoing trend of Requests being the single most secure way to do HTTP in Python.

It's highly probable that this will need to be a two-pronged approach: urllib3 will probably want to support some genericised CRL options, with Requests pasting over the genericness with a built-in ""correct"" behaviour. For that reason, I want to start a discussion with the relevant people to work out how we can do this.

Requests people: @kennethreitz, @sigmavirus24
urllib3 people: @shazow 
Our SSL guy: @t-8ch.

People who might be interested in either helping or providing insight and code review (to make sure we don't get this wrong) are @tiran (heavily involved with the standard library's `ssl` module) and @alex (currently on a big crypto kick). If either of you two are actually not interested that's totally fine. =)

Initial views: how should we split this work up, and how safe can Requests' defaults be?
",Lukasa,tiran
1865,2014-01-15 13:51:50,"In 3.4 the standard library's previously woeful `ssl` module is being vastly improved, and one of the nice things it is getting is [support for Certificate Revocation Lists](http://docs.python.org/3.4/library/ssl.html#ssl.SSLContext.load_verify_locations). I'd like us to investigate supporting CRLs in versions of Python where they are available. If we can do this, it'll continue the ongoing trend of Requests being the single most secure way to do HTTP in Python.

It's highly probable that this will need to be a two-pronged approach: urllib3 will probably want to support some genericised CRL options, with Requests pasting over the genericness with a built-in ""correct"" behaviour. For that reason, I want to start a discussion with the relevant people to work out how we can do this.

Requests people: @kennethreitz, @sigmavirus24
urllib3 people: @shazow 
Our SSL guy: @t-8ch.

People who might be interested in either helping or providing insight and code review (to make sure we don't get this wrong) are @tiran (heavily involved with the standard library's `ssl` module) and @alex (currently on a big crypto kick). If either of you two are actually not interested that's totally fine. =)

Initial views: how should we split this work up, and how safe can Requests' defaults be?
",Lukasa,t-8ch
1865,2014-01-15 13:51:50,"In 3.4 the standard library's previously woeful `ssl` module is being vastly improved, and one of the nice things it is getting is [support for Certificate Revocation Lists](http://docs.python.org/3.4/library/ssl.html#ssl.SSLContext.load_verify_locations). I'd like us to investigate supporting CRLs in versions of Python where they are available. If we can do this, it'll continue the ongoing trend of Requests being the single most secure way to do HTTP in Python.

It's highly probable that this will need to be a two-pronged approach: urllib3 will probably want to support some genericised CRL options, with Requests pasting over the genericness with a built-in ""correct"" behaviour. For that reason, I want to start a discussion with the relevant people to work out how we can do this.

Requests people: @kennethreitz, @sigmavirus24
urllib3 people: @shazow 
Our SSL guy: @t-8ch.

People who might be interested in either helping or providing insight and code review (to make sure we don't get this wrong) are @tiran (heavily involved with the standard library's `ssl` module) and @alex (currently on a big crypto kick). If either of you two are actually not interested that's totally fine. =)

Initial views: how should we split this work up, and how safe can Requests' defaults be?
",Lukasa,shazow
1865,2014-01-15 13:51:50,"In 3.4 the standard library's previously woeful `ssl` module is being vastly improved, and one of the nice things it is getting is [support for Certificate Revocation Lists](http://docs.python.org/3.4/library/ssl.html#ssl.SSLContext.load_verify_locations). I'd like us to investigate supporting CRLs in versions of Python where they are available. If we can do this, it'll continue the ongoing trend of Requests being the single most secure way to do HTTP in Python.

It's highly probable that this will need to be a two-pronged approach: urllib3 will probably want to support some genericised CRL options, with Requests pasting over the genericness with a built-in ""correct"" behaviour. For that reason, I want to start a discussion with the relevant people to work out how we can do this.

Requests people: @kennethreitz, @sigmavirus24
urllib3 people: @shazow 
Our SSL guy: @t-8ch.

People who might be interested in either helping or providing insight and code review (to make sure we don't get this wrong) are @tiran (heavily involved with the standard library's `ssl` module) and @alex (currently on a big crypto kick). If either of you two are actually not interested that's totally fine. =)

Initial views: how should we split this work up, and how safe can Requests' defaults be?
",Lukasa,sigmavirus24
1861,2014-01-13 07:16:20,"Thanks for this!

In principle this looks fine. My only worry is that technically this changes what our multipart requests look like on the wire, and I worry about breaking currently functional code. I'm interested to see if @sigmavirus24 is as worried about that as me.
",Lukasa,sigmavirus24
1861,2014-01-13 09:00:08,"@Lukasa 

> My only worry is that technically this changes what our multipart requests look like on the wire

I don't think it changed anything. A proper guess of file content type shouldn't break anything (I hope).

ping @sigmavirus24 for a code review.
",lepture,sigmavirus24
1861,2014-01-13 10:19:40,"I mean, in the literal sense it definitely changed something.

This:



 is not equal to this:



You're right, this _shouldn't_ break things, but this is the web we're talking about, it definitely has a chance to. =) That's why I want @sigmavirus24 to take a look.
",Lukasa,sigmavirus24
1861,2014-01-14 21:20:33,"@shazow how accurate is this detection?
",kennethreitz,shazow
1861,2014-01-16 08:26:06,"Actually, that's a point. @dstufft, can you give us an idea of what `pip`s release cycle looks like from the perspective of supporting 2.6?
",Lukasa,dstufft
1860,2014-01-12 20:34:24,"That's a good point @Lukasa. I'm guessing @gazpachoking might have a good idea. I'm just too tired to think this hard right now =P
",sigmavirus24,gazpachoking
1858,2014-01-11 10:00:52,"This _should_ resolve #1856. /cc @sigmavirus24 @t-8ch.
",Lukasa,t-8ch
1858,2014-01-11 10:00:52,"This _should_ resolve #1856. /cc @sigmavirus24 @t-8ch.
",Lukasa,sigmavirus24
1858,2014-01-12 14:26:45,"And the other place it's used is here: https://github.com/kennethreitz/requests/blob/ac4e05874a1a983ca126185a0e4d4e74915f792e/requests/models.py#L456 and notice that there's an optional param to that method that is never used if it is ever passed. We should either use it or remove it (not necessarily in this PR though).

Finally, given that the call to `HTTPAdapter#proxy_headers` is wrapped inside an `if proxy:` block, the param it sends to `get_auth_from_url` should never be `None` or `''`. And `prepare_auth` on the `PreparedRequest` is called after `prepare_url` which should blow up if `url` is not a valid `url`. I think we're safe making `get_auth_from_url` a bit less paranoid. As with all of my reviews though, this is totally up to the discretion of @kennethreitz and @Lukasa 
",sigmavirus24,kennethreitz
1858,2014-01-12 14:58:02,"@kennethreitz :shipit: :+1:
",sigmavirus24,kennethreitz
1858,2014-01-13 12:42:17,"@Lukasa @sybeck2k found a problem with this test: https://github.com/kennethreitz/requests/pull/1862/files#diff-56c2d754173a4a158ce8f445834c8fe8R705 can you fix it here too?
",sigmavirus24,sybeck2k
1857,2014-01-10 18:14:42,"After the recent discussion about SSL compression both in shazow/urllib3#109
and kennethreitz/requests#1583 (this issue was about performance reason, it got me to look at other projects, performance is _not_ the reason for this issue) I looked at the behaviour of other popular open source projects. It turned out that the following projects disable the compression by default for [security reasons](https://en.wikipedia.org/wiki/CRIME_(security_exploit\)):
- Nginx, July 2012 ([version 1.2.2](http://nginx.org/en/CHANGES-1.2), [version 1.3.2, the development version to 1.4](http://nginx.org/en/CHANGES-1.4)
- Apache2, ([version 2.2.25](https://httpd.apache.org/docs/2.2/mod/mod_ssl.html#sslcompression), [version 2.4.4](https://httpd.apache.org/docs/2.4/mod/mod_ssl.html#sslcompression))
- The upcoming version 2.7 of the Pound reverse proxy (the `CHANGELOG` file in the source archive: http://www.apsis.ch/pound/Pound-2.7b.tgz)
- [Curl 7.28.1](http://curl.haxx.se/changes.html#7_28_1) July 2012
- CPython >= 3.4 (http://hg.python.org/cpython/rev/98eb88d3d94e)

This are the only projects I have looked at. I am sure we can find more if necessary.
As the stdlib `ssl` module does not allow us to change this parameter before 3.3
I propose to raise the issue on the CPython bug tracker, so that SSL
compression will be disabled by default (with the possibility to manually
enable it on 3.3 and later).

The current handlich of CPython 3.4 and up only disables compression on openssl
1.0 and up, as the relevant constant has not introduced before. However the
nginx changelog claims to also disable compression on earlier versions. I will
look into this.

This issue is meant to gather feedback and momentum before raising the issue with CPython (and maybe also the other implementations)

/cc @lukasa @sigmavirus24 @shazow @alex @jmhodges
",t-8ch,shazow
1857,2014-01-10 18:14:42,"After the recent discussion about SSL compression both in shazow/urllib3#109
and kennethreitz/requests#1583 (this issue was about performance reason, it got me to look at other projects, performance is _not_ the reason for this issue) I looked at the behaviour of other popular open source projects. It turned out that the following projects disable the compression by default for [security reasons](https://en.wikipedia.org/wiki/CRIME_(security_exploit\)):
- Nginx, July 2012 ([version 1.2.2](http://nginx.org/en/CHANGES-1.2), [version 1.3.2, the development version to 1.4](http://nginx.org/en/CHANGES-1.4)
- Apache2, ([version 2.2.25](https://httpd.apache.org/docs/2.2/mod/mod_ssl.html#sslcompression), [version 2.4.4](https://httpd.apache.org/docs/2.4/mod/mod_ssl.html#sslcompression))
- The upcoming version 2.7 of the Pound reverse proxy (the `CHANGELOG` file in the source archive: http://www.apsis.ch/pound/Pound-2.7b.tgz)
- [Curl 7.28.1](http://curl.haxx.se/changes.html#7_28_1) July 2012
- CPython >= 3.4 (http://hg.python.org/cpython/rev/98eb88d3d94e)

This are the only projects I have looked at. I am sure we can find more if necessary.
As the stdlib `ssl` module does not allow us to change this parameter before 3.3
I propose to raise the issue on the CPython bug tracker, so that SSL
compression will be disabled by default (with the possibility to manually
enable it on 3.3 and later).

The current handlich of CPython 3.4 and up only disables compression on openssl
1.0 and up, as the relevant constant has not introduced before. However the
nginx changelog claims to also disable compression on earlier versions. I will
look into this.

This issue is meant to gather feedback and momentum before raising the issue with CPython (and maybe also the other implementations)

/cc @lukasa @sigmavirus24 @shazow @alex @jmhodges
",t-8ch,alex
1857,2014-01-10 18:14:42,"After the recent discussion about SSL compression both in shazow/urllib3#109
and kennethreitz/requests#1583 (this issue was about performance reason, it got me to look at other projects, performance is _not_ the reason for this issue) I looked at the behaviour of other popular open source projects. It turned out that the following projects disable the compression by default for [security reasons](https://en.wikipedia.org/wiki/CRIME_(security_exploit\)):
- Nginx, July 2012 ([version 1.2.2](http://nginx.org/en/CHANGES-1.2), [version 1.3.2, the development version to 1.4](http://nginx.org/en/CHANGES-1.4)
- Apache2, ([version 2.2.25](https://httpd.apache.org/docs/2.2/mod/mod_ssl.html#sslcompression), [version 2.4.4](https://httpd.apache.org/docs/2.4/mod/mod_ssl.html#sslcompression))
- The upcoming version 2.7 of the Pound reverse proxy (the `CHANGELOG` file in the source archive: http://www.apsis.ch/pound/Pound-2.7b.tgz)
- [Curl 7.28.1](http://curl.haxx.se/changes.html#7_28_1) July 2012
- CPython >= 3.4 (http://hg.python.org/cpython/rev/98eb88d3d94e)

This are the only projects I have looked at. I am sure we can find more if necessary.
As the stdlib `ssl` module does not allow us to change this parameter before 3.3
I propose to raise the issue on the CPython bug tracker, so that SSL
compression will be disabled by default (with the possibility to manually
enable it on 3.3 and later).

The current handlich of CPython 3.4 and up only disables compression on openssl
1.0 and up, as the relevant constant has not introduced before. However the
nginx changelog claims to also disable compression on earlier versions. I will
look into this.

This issue is meant to gather feedback and momentum before raising the issue with CPython (and maybe also the other implementations)

/cc @lukasa @sigmavirus24 @shazow @alex @jmhodges
",t-8ch,sigmavirus24
1857,2014-01-10 18:18:09,"Fwiw, urllib3 _just_ merged a PR which disables this by default in Py32+ and PyOpenSSL, big thanks to @dbrgn for bringing it up and writing the patch. https://github.com/shazow/urllib3/pull/309
",shazow,dbrgn
1857,2014-01-10 18:48:17,"I'm 100% in favour of this. 2.7 is clearly accepting some SSL related changes in the next version (see [20207](http://bugs.python.org/issue20207) from @alex), so it doesn't seem unreasonable to make a push for this as well.

Unfortunately, it doesn't save requests/urllib3 _entirely_, because both projects support 2.6 and 2.6 is deader-than-dead.
",Lukasa,alex
1856,2014-01-10 15:56:40,"Thanks for raising this issue!

It's hard to see how unquoting before parsing the URL is really helping us. We can almost certainly defer the unquote step until after parsing the URL, applying unquote only to the username and password. @sigmavirus24, can you think of any reason that's not going to be ok?
",Lukasa,sigmavirus24
1856,2014-01-11 09:10:51,"Ok, so `urlparse` chokes like a champ on a sample URL, and so does urllib3's superior `parse_url` function:





Thoughts? /cc @shazow
",Lukasa,shazow
1853,2014-01-09 16:13:16,"Also, @t-8ch, our resident TLS guru, can you confirm that this is/isn't related to specific installs of OpenSSL?
",Lukasa,t-8ch
1849,2014-01-07 23:15:06,"This isn't necessary since @kennethreitz pulls in a fresh copy of urllib3 before each release I think. But thanks, this will hopefully ensure he pulls in the right version again.
",sigmavirus24,kennethreitz
1847,2014-01-07 19:57:50,"@kennethreitz has previously stated that he views requests as a product and anything else as an implementation detail, so I assumed he'd want to know about this to address this in requests, if at all possible.
",alex,kennethreitz
1846,2014-01-07 17:30:05,"Alright, I'm happy with this. You can ignore TravisCI (I have no idea why it's still running), the relevant CI results are these (all of which passed):

http://ci.kennethreitz.org/job/requests-pr/PYTHON=2.6/167/
http://ci.kennethreitz.org/job/requests-pr/PYTHON=2.7/167/
http://ci.kennethreitz.org/job/requests-pr/PYTHON=3.3/167/
http://ci.kennethreitz.org/job/requests-pr/PYTHON=pypy-1.8/167/

Waiting on @sigmavirus24 to review.
",Lukasa,sigmavirus24
1844,2014-01-09 13:08:04,"@Lukasa Looks good to me. I had the reporter of the bugs test it out and he was able to successfully install stuff without error. So once @kennethreitz cuts a new release (hopefully soon!) pip 1.5.1 can vendor that release and close out those bugs.
",dstufft,kennethreitz
1843,2014-01-07 08:51:59,"I'm obviously tempted to argue that `pip` should be able to spot this and handle it (that is, any compliant server would signal `Content-Encoding: gzip` and `Content-Type: application/x-tar`), but that's not a very helpful way to approach this problem.

In principle we could do that, it would be very easy from our side. @kennethreitz, the API is your call: are you happy to do this?
",Lukasa,kennethreitz
1843,2014-01-07 09:30:30,"Yeah, that was basically the decision I want @kennethreitz to make: do we just say ""tough, use the raw response"", or do we provide the kwarg.
",Lukasa,kennethreitz
1843,2014-01-07 09:32:28,"The current intended API design is to use .raw for this.

## 

Kenneth Reitz

> On Jan 7, 2014, at 3:52 AM, Cory Benfield notifications@github.com wrote:
> 
> I'm obviously tempted to argue that pip should be able to spot this and handle it (that is, any compliant server would signal Content-Encoding: gzip and Content-Type: application/x-tar), but that's not a very helpful way to approach this problem.
> 
> In principle we could do that, it would be very easy from our side. @kennethreitz, the API is your call: are you happy to do this?
> 
> —
> Reply to this email directly or view it on GitHub.
",kennethreitz,kennethreitz
1842,2014-01-06 20:57:50,"@sigmavirus24 and I are in favour of not supporting very old patch releases of 2.6. For perspective, 2.6.0 was released in 2008 and 2.6.2 was released in April 2009.

Obviously, we'll take the fix in urllib3, but I see no reason for Requests to rush into supporting a five year old release of Python. Anyone who felt the need to be on Python 2.6 should have been taking security patches and be at 2.6.9. 
",Lukasa,sigmavirus24
1842,2014-01-06 21:07:06,"Agreed, was mostly creating this so people can hopefully find it on Google
if they run into this problem.

## 

Kevin Burke | Twilio
phone: 925.271.7005 | kev.inburke.com

On Mon, Jan 6, 2014 at 12:58 PM, Cory Benfield notifications@github.comwrote:

> @sigmavirus24 https://github.com/sigmavirus24 and I are in favour of
> not supporting very old patch releases of 2.6. For perspective, 2.6.0 was
> released in 2008 and 2.6.2 was released in April 2009.
> 
> Obviously, we'll take the fix in urllib3, but I see no reason for Requests
> to rush into supporting a five year old release of Python. Anyone who felt
> the need to be on Python 2.6 should have been taking security patches and
> be at 2.6.9.
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/issues/1842#issuecomment-31686263
> .
",kevinburke,sigmavirus24
1834,2013-12-30 17:38:18,"PyPI is a strange beast that is luckily being slowly replaced by [pypa/warehouse](https://github.com/pypa/warehouse). In the past I had issues where I created a package name manually and then metadata from `setup.py` didn't properly appear. I suspect this might be something similar. Regardless only @kennethreitz can change this and it isn't an actual bug in requests.
",sigmavirus24,kennethreitz
1831,2013-12-28 11:20:24,"This in principle looks fine, though I don't know about about intersphinx to be sure. =) Let's see what @sigmavirus24 thinks.
",Lukasa,sigmavirus24
1827,2013-12-24 10:27:53,"Thanks for this!

I like this solution, I think it's the correct approach. We can quite safely overload these kwargs, and I think doing so provides the nicer interface.

Once again, it's annoying that we can't easily test this, but I'm happy with this solution as-is. @sigmavirus24?
",Lukasa,sigmavirus24
1818,2013-12-20 02:33:58,"This sounds like a bug in urllib3. @shazow opinions?
",sigmavirus24,shazow
1818,2013-12-20 02:40:49,"Actually following the conversation on #171 this is not a bug in requests but in Python as @Lukasa explained.
",sigmavirus24,Lukasa
1813,2013-12-18 22:15:35,"@orokusaki In principle there's no reason. I suggest raising the issue on urllib3 and seeing what @shazow thinks. =)
",Lukasa,shazow
1813,2013-12-18 22:56:48,"Ah, so it sounds like maybe a `requests==3.0.0` type of feature, if it were to be added. Thanks for checking on that for me @shazow.

@Lukasa @kennethreitz If more things were delegated to urllib3, it would simplify the requests codebase even more. If either of you have suggestions pertaining a change like this, I'd be more than happy to take a stab at it.
",orokusaki,kennethreitz
1811,2013-12-18 15:25:45,"We have discussed this issue with users several times in the past. A search of the issues that are closed would show that. Our opinion has not changed, nor has our reasoning. By vendoring urllib3 we have a very specific version that we have tested against that may include unreleased bug fixes and does not put pressure on @shazow. It also allows us to checkout a specific version of the repository and just work on it with what existed at the time. It's reliable and it will be how we work for the foreseeable future. I'll leave this open until @Lukasa can make his way around to it, but I'm strongly in favor of closing this.
",sigmavirus24,shazow
1811,2013-12-18 15:25:45,"We have discussed this issue with users several times in the past. A search of the issues that are closed would show that. Our opinion has not changed, nor has our reasoning. By vendoring urllib3 we have a very specific version that we have tested against that may include unreleased bug fixes and does not put pressure on @shazow. It also allows us to checkout a specific version of the repository and just work on it with what existed at the time. It's reliable and it will be how we work for the foreseeable future. I'll leave this open until @Lukasa can make his way around to it, but I'm strongly in favor of closing this.
",sigmavirus24,Lukasa
1810,2013-12-18 14:48:06,"This shouldn't affect any of the tests @kennethreitz and should be merged before the next release.
",sigmavirus24,kennethreitz
1806,2013-12-18 13:40:25,"I'm waiting for @Lukasa to weigh in before closing this.
",sigmavirus24,Lukasa
1805,2013-12-17 15:32:09,"Urgh, this is a cookies problem. Observe:



@gazpachoking, @sigmavirus24, I choose you!
",Lukasa,gazpachoking
1805,2013-12-17 15:32:09,"Urgh, this is a cookies problem. Observe:



@gazpachoking, @sigmavirus24, I choose you!
",Lukasa,sigmavirus24
1802,2013-12-16 14:08:23,"I agree with 100% of what @Lukasa said. In fact, even curl would give you far more information to debug with. _Furthermore_, there are headers that we compute that we don't even set on the `PreparedRequest` object so adding this method to that would be useless too quite frankly. Unfortunately your best bet is something like `mitmproxy` or `wireshark` (locally) or a service like `Runscope` or `httpbin`.

With that said, the both of us are in agreement and I'm certain @kennethreitz will agree with both of us, so I'm going to close this.
",sigmavirus24,kennethreitz
1801,2013-12-16 04:10:00,"Per discussion with @sigmavirus24 in IRC, this PR kills the Timeout class and adds support for connect timeouts via a simple tuple interface: 



Sometimes you try to connect to a dead host (this happens to us all the time, because of AWS) and you would like to fail fairly quickly in this case; the request has no chance of succeeding. However, once the connection succeeds, you'd like to give the server a fairly long time to process the request and return a response. 

The attached tests and documentation have more information. Hope this is okay!
",kevinburke,sigmavirus24
1801,2013-12-16 08:56:58,"In principle I'm +0.5 on this. It's a nice extension. However, as memory serves, @kennethreitz considered this approach and ended up not taking it. It might be that now that someone has written the code he'll be happy with it, but equally, it might be that he doesn't like the approach.

@sigmavirus24 Yeah, `TimeoutSauce` is used for the urllib3 `Timeout` object, because we have our own `Timeout` object (an exception).
",Lukasa,kennethreitz
1801,2013-12-16 14:05:32,"@Lukasa As I understood it @kennethreitz was more concerned with the addition (and requirement) of the `Timeout` class from urllib3. And thanks for clearing up the naming, I still think there has to be a better name for it. (I'm shaving a yak, I know)
",sigmavirus24,kennethreitz
1799,2013-12-14 20:14:48,"I'm something in the region of -0 on this. @sigmavirus24?
",Lukasa,sigmavirus24
1796,2013-12-14 09:41:16,"Thanks for this @martinblech!

Unfortunately, I'm opposed to this change. Not anything to do with the code itself, but I don't think adding this parameter to the `Response` object adds sufficient value to justify including it. If people _really_ want this information they can get it (by running `charade`/`chardet` themselves), and for a significant majority of people this information is simply unnecessary. Requests got where it was today by hiding complexity from users, and I'm in favour of that trend continuing. @sigmavirus24?
",Lukasa,sigmavirus24
1795,2013-12-14 01:01:56,"Ah, good spot. We have mixed feelings about auto detection. It's hard to do right, and no matter how well we do there will always be edge cases which cause problems.

Realistically, there's not a lot we can do about that beyond warn people. Auto detection _usually_ works well enough for people. I'm interested to see what @sigmavirus24 thinks. 
",Lukasa,sigmavirus24
1794,2013-12-13 11:06:27,"This fix is a subset of the fix in #1793. You and @sigmavirus24 should work out which fix we apply (maybe a merged set of the fixes).
",Lukasa,sigmavirus24
1793,2013-12-12 19:04:51,"@Lukasa if you'd like to add tests for this you can. I'm not exactly certain if we already had tests for pickling objects and didn't have the time to check right now. I'll check when I get home though (unless you've already added tests).
",sigmavirus24,Lukasa
1793,2013-12-14 04:30:42,"@Lukasa any further comments?
",sigmavirus24,Lukasa
1790,2013-12-12 06:57:48,"Hey, thanks for thi @kracekumar!

I don't think this is a good idea, however. The key reason is that auto-converting outgoing data is upsettingly magic. As much as possible, what you put in one parameter to the request function (e.g. `headers`) should not affect what happens to something that came in on another parameter (e.g. `data`, or `auth`).

It's also not quite the same as what we do on `Response`s. For a `Response`, we don't automatically convert to JSON unless explicitly asked to by the user: that is, they have to actually call `Response.json()`. That fits the Zen of Python (""Explicit is better than implicit""). For that reason as well, I'd rather that we stick to the current behaviour.

Let's leave this open to see if @sigmavirus24 agrees.
",Lukasa,sigmavirus24
1789,2013-12-11 16:30:35,"Yeah, @t-8ch seems to have the right analysis. In CPython, it takes 29milliseconds per call to `requests.get()`, of which 22ms is `getaddrinfo()`. In PyPy, it takes 47 ms per call to `requests.get()`, of which 35ms is `getaddrinfo()` That accounts for 13 ms of the 18ms difference. I imagine the remaining portion of that time difference is probably because the profiler doesn't play well with PyPy (I seem to recall that being a problem, though @alex will surely be able to correct me if it isn't).
",Lukasa,alex
1787,2013-12-19 08:30:06,"Hang on, it looks to me like urllib3 doesn't catch `socket.timeout` in `Response.read()`. That feels clearly and obviously wrong to me. @shazow, (and @kevinburke since you implemented this), am I missing something here?
",Lukasa,shazow
1787,2013-12-19 08:30:06,"Hang on, it looks to me like urllib3 doesn't catch `socket.timeout` in `Response.read()`. That feels clearly and obviously wrong to me. @shazow, (and @kevinburke since you implemented this), am I missing something here?
",Lukasa,kevinburke
1779,2013-12-05 18:29:19,"I'm not really sure. It's a small change but we would then have to expose it to the user and that's what I'm not exactly a fan of. Certainly they can find it on their own now but that's not the same as exposing it to them which would be encouraging its use. How commonly do requests users set their own User-Agent string and want it to include all of that information? Frankly I'm not convinced it is all that frequently. That said if @kennethreitz wants this I'm perfectly okay with it granted that it handles strings correctly (as @Lukasa already mentioned).
",sigmavirus24,kennethreitz
1776,2013-12-04 12:46:50,"This is just @gazpachoking's PR #1729 in a mergable state and with the PR feedback that @Lukasa left on it.
",sigmavirus24,gazpachoking
1776,2013-12-04 12:46:50,"This is just @gazpachoking's PR #1729 in a mergable state and with the PR feedback that @Lukasa left on it.
",sigmavirus24,Lukasa
1775,2013-12-03 22:37:33,"@kennethreitz can you turn off the Travis integration hook altogether? This is annoying. I'd be willing to guess that they both ran this but Travis ran it last and got precendence over the other.
",sigmavirus24,kennethreitz
1773,2013-12-03 12:44:55,"Fix issue @tardyp reported on #239

We have not been able to reproduce on a simple testcase. We can only make it happen on our complex environment; involving buildbot, twisted and txrequests.

This fix no longer exhibits the issue. Please, give us a feedback whether this solutions is correct.
",vincentxb,tardyp
1772,2013-12-03 16:06:09,"@sigmavirus24 - Pushed the changes we discussed. Let me know if you have anything else!

Thanks!
",mdbecker,sigmavirus24
1769,2013-12-02 10:46:54,"Hi there!

First, the answer. Yes, this is a known issue, and a fix is already present in master.

Now, some notes. Asking this question on Stack Overflow was exactly the right thing to do. When you do that, please wait more than 30 minutes before moving to this issue tracker. Both I and @sigmavirus24 regularly look at the Python-Requests tag on Stack Overflow, so you'd have got our attention before long. Asking the question both there and here is unnecessary duplication.

Secondly, the question 'is this a known issue' is one you should be able to answer just as well as us. It is a known issue: issue #1745 tracked it. A [very simple GitHub search](https://github.com/kennethreitz/requests/search?o=desc&q=cookie+AttributeError&s=created&type=Issues) could have shown you this issue quite quickly. I appreciate that it's not always obvious, but if everyone took five minutes to browse some old related issues I'd save hours of my life over the long term.
",Lukasa,sigmavirus24
1766,2013-11-29 16:36:47,"#1765 was already a PR, why did you open this one? We could have added tests after the fact. Can you rebase this to work off of #1765 so that @bicycle1885 gets some credit?
",sigmavirus24,bicycle1885
1764,2013-11-28 18:36:29,"There are a few nasty bugs that we've fixed since the last release, including the broken SNI support. It'd be good to get another release out. I think a few things have changed dramatically enough that this is probably a minor release (e.g. takes us to 2.1.0). Thoughts? /cc @kennethreitz @sigmavirus24

Still to do:
- [ ] Merge outstanding PRs (definitely ~~#1713, #1657, #1768, and #1766~~; maybe #1729, #1628 and #1743 depending on how @kennethreitz feels).
- [ ] Update changelog to match those PRs.

If nothing else Runscope could do with this release, so it'd be nice to be in good shape.
",Lukasa,kennethreitz
1764,2013-11-28 18:36:29,"There are a few nasty bugs that we've fixed since the last release, including the broken SNI support. It'd be good to get another release out. I think a few things have changed dramatically enough that this is probably a minor release (e.g. takes us to 2.1.0). Thoughts? /cc @kennethreitz @sigmavirus24

Still to do:
- [ ] Merge outstanding PRs (definitely ~~#1713, #1657, #1768, and #1766~~; maybe #1729, #1628 and #1743 depending on how @kennethreitz feels).
- [ ] Update changelog to match those PRs.

If nothing else Runscope could do with this release, so it'd be nice to be in good shape.
",Lukasa,sigmavirus24
1764,2013-11-29 16:59:48,"Looking over the PRs listed above now:
- #1713 :+1: merge it
- #1657 :+1: merge it
- #1729 :-1: do not merge it (I don't fee it is ready yet. @Lukasa and I left feedback that hasn't been addressed)
- #1628 :-1: do not merge it (neither @kennethreitz nor I are very fond of this)
- #1766 :+1: merge it
- #1743 +0 I don't see anything awful about allowing for separate timeouts, it is just the API under question. I've proposed a different way of handling the same feature. I'm not sure this _has_ to be in 2.1 though 
",sigmavirus24,kennethreitz
1764,2013-12-04 01:49:40,"@kennethreitz this PR wasn't finished. There's still stuff missing from the Changelog
",sigmavirus24,kennethreitz
1752,2013-11-22 07:51:22,"This is effecting Pip pretty aggressively. 

Also being tracked by PyPy: https://bugs.pypy.org/issue1645

/cc @Lukasa @sigmavirus24 
",kennethreitz,Lukasa
1752,2013-11-22 07:51:22,"This is effecting Pip pretty aggressively. 

Also being tracked by PyPy: https://bugs.pypy.org/issue1645

/cc @Lukasa @sigmavirus24 
",kennethreitz,sigmavirus24
1743,2013-11-18 08:57:00,"I'm 100% sitting on the fence here. This is an API change, so we need to see if @kennethreitz likes it or not. =)
",Lukasa,kennethreitz
1733,2013-11-09 22:50:42,"Looks fine in principle to me, but I'd want @sigmavirus24 to take a look. =)
",Lukasa,sigmavirus24
1732,2013-11-09 08:57:36,"`urllib3` had a bug, we accidentally pulled it in, we didn't catch it because we don't test with SNI support (should we have that as a separate build?). Happily the awesome @t-8ch has already provided us with a patch, which is already merged, so we don't necessarily have an action on this: it'll be quietly fixed in the next release. Just raising this issue so that people who look on the issue tracker see it.
",Lukasa,t-8ch
1727,2013-11-10 09:35:43,"@sigmavirus24 I really want to add a regression test for this, but we have essentially got no testing infrasrtucture for  proxies. Do you have any bright ideas/will Betamax help with this, or do I need to look into spinning up an open HTTP proxy?
",Lukasa,sigmavirus24
1723,2013-11-04 09:33:02,"Thanks for raising this issue! It looks like @daftshady is already taking a pass at fixing this, so let's see what gets produced. =)
",Lukasa,daftshady
1723,2013-11-04 16:05:44,"@hwkns Yes, something has changed. Specifically, we merged #1338. This added the `to_native_string` method that @daftshady is using in the fix.

Previously, we'd have had to special-case this parameter, which seemed insane to me, especially as it's very easy to simply not use it (e.g. call requests.get or requests.post). However, we can now treat this parameter the way we treat header keys, which is to say that people who use this library have all sorts of ways of handling their internal strings, so we should just fix it up as best we can.

In your particular case I think you're overthinking the issue. If the method name can't come from outside your application (and I hope to god it can't), then there's no reason for you not to just use native strings. You gain nothing by forcing them to unicode, and your code will be cleaner. However, I'm still +1 on taking a fix for this, because it's now so easy to do.
",Lukasa,daftshady
1717,2013-10-30 21:17:56,"Yeah, just a transient problem. As discussed previously, I'm generally happy with this kind of change. It's up to @sigmavirus24 and @kennethreitz to agree/disagree with this.
",Lukasa,kennethreitz
1717,2013-10-30 21:17:56,"Yeah, just a transient problem. As discussed previously, I'm generally happy with this kind of change. It's up to @sigmavirus24 and @kennethreitz to agree/disagree with this.
",Lukasa,sigmavirus24
1717,2013-10-31 00:29:12,"It seems @kennethreitz has warmed to the idea so I'll withdraw my objections once the tests are made a bit better. You should check to make sure the url isn't changed in the future.
",sigmavirus24,kennethreitz
1713,2013-10-30 00:41:20,"I thought @gazpachoking or I had commented the old way of merging these fairly explicitly to talk about accepting a CookieJar. I just can't seem to find what I remember writing. Something like `merge_cookies` or `merge_session_cookies`. 
",sigmavirus24,gazpachoking
1710,2013-10-29 11:05:21,"Closed and reopened to get a rebuild.

The code changes here look totally fine to me, so in principle this is acceptable to merge. I'm also in principle happy with this change: it makes using unusual protocols in Transport Adapters not too big a problem.

However, it's a significant enough diversion that I want to hear from @sigmavirus24 and @kennethreitz first. =)
",Lukasa,kennethreitz
1710,2013-10-29 11:05:21,"Closed and reopened to get a rebuild.

The code changes here look totally fine to me, so in principle this is acceptable to merge. I'm also in principle happy with this change: it makes using unusual protocols in Transport Adapters not too big a problem.

However, it's a significant enough diversion that I want to hear from @sigmavirus24 and @kennethreitz first. =)
",Lukasa,sigmavirus24
1709,2013-10-28 17:21:17,"I am strongly +0 on this. Requests does not support Google App Engine, and there has never been any expectation that it will function correctly on GAE. I'm not opposed to this change, because it's minor and should have no functional effect, but I equally do not want to begin to establish a tradition of establishing special-cases for GAE.

I'll let @sigmavirus24 and @kennethreitz provide their opinions instead.
",Lukasa,kennethreitz
1709,2013-10-28 17:21:17,"I am strongly +0 on this. Requests does not support Google App Engine, and there has never been any expectation that it will function correctly on GAE. I'm not opposed to this change, because it's minor and should have no functional effect, but I equally do not want to begin to establish a tradition of establishing special-cases for GAE.

I'll let @sigmavirus24 and @kennethreitz provide their opinions instead.
",Lukasa,sigmavirus24
1705,2013-10-26 08:21:55,"Thanks for saving us again @shazow and @t-8ch.

Also, cross repo automatic issue closing? Very cool.
",Lukasa,shazow
1705,2013-10-26 08:21:55,"Thanks for saving us again @shazow and @t-8ch.

Also, cross repo automatic issue closing? Very cool.
",Lukasa,t-8ch
1702,2013-10-24 20:24:11,"In response to some of the discussion in #1698. Looking for review from @sigmavirus24. =)
",Lukasa,sigmavirus24
1700,2013-10-24 14:01:45,"TODO: 
- [x] get :+1: from @lukasa and @sigmavirus24 
- [x] update changelog

Fixes #1659
",kennethreitz,sigmavirus24
1700,2013-10-24 14:02:41,"/cc @Lukasa @sigmavirus24 
",kennethreitz,Lukasa
1700,2013-10-24 14:02:41,"/cc @Lukasa @sigmavirus24 
",kennethreitz,sigmavirus24
1696,2013-10-22 20:51:27,"First contribution, half expecting to see that I screwed something up. Thanks a lot @Lukasa for your assistance in IRC.
",canibanoglu,Lukasa
1694,2013-10-20 15:24:17,"Looks like it's 404ing. @kennethreitz?
",Lukasa,kennethreitz
1693,2013-10-20 17:26:08,"I might go through and update all of these to use `#format` instead in the library. It is the ""future"" after all. Any objections @Lukasa @kennethreitz ?
",sigmavirus24,kennethreitz
1690,2013-10-19 01:34:06,"There are no plans for such and I believe it is currently completely untenable. There is no support like this in any other library like requests that I know of and the place to add anything like this would be in shazow/urllib3. I'm not even entirely certain it would be possible to do in urllib3, especially since I find it hard to believe this is even possible with httplib (which urllib3 uses).

I'll leave this open so @Lukasa and @kennethreitz can comment, but not only am I :-1: on the idea in general, I'm also pretty sure it is not technically possible at the moment.
",sigmavirus24,kennethreitz
1690,2013-10-19 01:34:06,"There are no plans for such and I believe it is currently completely untenable. There is no support like this in any other library like requests that I know of and the place to add anything like this would be in shazow/urllib3. I'm not even entirely certain it would be possible to do in urllib3, especially since I find it hard to believe this is even possible with httplib (which urllib3 uses).

I'll leave this open so @Lukasa and @kennethreitz can comment, but not only am I :-1: on the idea in general, I'm also pretty sure it is not technically possible at the moment.
",sigmavirus24,Lukasa
1685,2014-11-07 13:35:55,"@stas I want to address one thing:

> Requests users should be aware of those methods, because in most of the cases the methods are not called implicitly.

Leaving PyPy aside for a moment, those methods shouldn't _need_ to be called explicitly. If the socket objects become unreachable in CPython they will get auto gc'd, which includes closing the file handles. This is not an argument to not-document those methods, but it is a warning to not focus overmuch on them.

We are meant to use a CI, but it appears to be unwell at the moment, and only @kennethreitz is in a position to fix it. He'll get to it when he has time. Note, however, that benchmark tests are extremely difficult to get right in a way that doesn't make them extremely noisy.
",Lukasa,kennethreitz
1685,2014-11-08 16:56:23,"Alright, so the problem explicitly appears to be with the way we handle memory interacting with the PyPy JIT. It might be a good idea to summon in a PyPy expert: @alex?
",Lukasa,alex
1674,2013-10-20 16:42:06,"@jkatzer can you tell us what `self.encoding` is when encountering this issue? Furthermore, can you tell us what `guess_json_utf(self.content)` returns? Something @mjpieters or @sburns contributed seems to be causing the issue here.

The stack trace seems to imply that `self.encoding` is `None` or some other falsey value (e.g., `''`) so we use `guess_json_utf` with `self.content`. `self.content` at that point is the raw bytes object we get from urllib3. So we use `self.content.decode(encoding)` which seems to be what's causing this issue. Judging by the stack trace (again) it seems that `guess_json_utf` is returning `utf8`.

One other note is that on requests master (on python 2.7), when I use `r.json()` the title of this issue comes back replaced like so: `u'""\u010d"" - UTF-8 UnicodeDecodeError'` which if I remember correctly is how the stdlib replaces errors and is a consequence of us always using `errors='replace'`. This suggests that the call to `str.decode` on line 692 needs an `errors='replace'` parameter passed in since that's what we do for `self.text`. 

Objections? I feel like using that particular option is a bad idea but we'd break the API were we to change it now.
",sigmavirus24,sburns
1674,2013-10-20 16:42:06,"@jkatzer can you tell us what `self.encoding` is when encountering this issue? Furthermore, can you tell us what `guess_json_utf(self.content)` returns? Something @mjpieters or @sburns contributed seems to be causing the issue here.

The stack trace seems to imply that `self.encoding` is `None` or some other falsey value (e.g., `''`) so we use `guess_json_utf` with `self.content`. `self.content` at that point is the raw bytes object we get from urllib3. So we use `self.content.decode(encoding)` which seems to be what's causing this issue. Judging by the stack trace (again) it seems that `guess_json_utf` is returning `utf8`.

One other note is that on requests master (on python 2.7), when I use `r.json()` the title of this issue comes back replaced like so: `u'""\u010d"" - UTF-8 UnicodeDecodeError'` which if I remember correctly is how the stdlib replaces errors and is a consequence of us always using `errors='replace'`. This suggests that the call to `str.decode` on line 692 needs an `errors='replace'` parameter passed in since that's what we do for `self.text`. 

Objections? I feel like using that particular option is a bad idea but we'd break the API were we to change it now.
",sigmavirus24,mjpieters
1674,2013-10-21 07:44:13,"It _used_ to fall back to using `self.text` if decoding failed, but [@kennethreitz removed that at some point](https://github.com/kennethreitz/requests/commit/1451ba0c6d395c41f86da35036fa361c3a41bc90), without explanation. `guess_json_utf` assumes that the content is correctly encoded to _a_ UTF codec, and the exception handling would handle the edge cases where a non-RFC-compliant JSON response is to be handled.
",mjpieters,kennethreitz
1671,2013-10-21 10:12:15,"@sigmavirus24 Should we be filing this under our long list of ""Do we trust the Host: header"" bugs?
",Lukasa,sigmavirus24
1669,2013-10-13 08:59:50,"Following #1642, I think we should treat urllib3's pooled connections better when we manage them ourselves. This PR provides the following extra functionality:
- Clean up connections when we hit problems during chunked upload, rather than leaking them.
- Return connections to the pool when a chunked upload is successful, rather than leaking it.

The diff is a little unclear, so let me clarify: I wrapped most of the chunked code in a `try` block, then added a catch-everything exception handler that will attempt to close the connection if we hit it. Otherwise, we'll return the presumably-fine connection to the pool.

I'd like some code review on this. @sigmavirus24, can you confirm this is doing what I think it is, and can you give me some suggestions for how best to test it? @shazow, if you have time, can you confirm that I'm obeying the semantics of the urllib3 connection pool? (e.g. should I check if we get a `Connection: close` header on the response?
",Lukasa,shazow
1669,2013-10-13 08:59:50,"Following #1642, I think we should treat urllib3's pooled connections better when we manage them ourselves. This PR provides the following extra functionality:
- Clean up connections when we hit problems during chunked upload, rather than leaking them.
- Return connections to the pool when a chunked upload is successful, rather than leaking it.

The diff is a little unclear, so let me clarify: I wrapped most of the chunked code in a `try` block, then added a catch-everything exception handler that will attempt to close the connection if we hit it. Otherwise, we'll return the presumably-fine connection to the pool.

I'd like some code review on this. @sigmavirus24, can you confirm this is doing what I think it is, and can you give me some suggestions for how best to test it? @shazow, if you have time, can you confirm that I'm obeying the semantics of the urllib3 connection pool? (e.g. should I check if we get a `Connection: close` header on the response?
",Lukasa,sigmavirus24
1663,2013-11-11 14:12:13,"Looks like the change where @daftshady wasn't expecting to be receiving CookieJar objects instead of dictionaries. There's a fix for this somewhere and should be in 2.0.2 then again, I can't be certain without confirmation from @dmakhno as to what `self.context.cookies` actually is.
",sigmavirus24,daftshady
1662,2013-10-10 22:14:33,"Taking a page out of @mattspitz's book.

https://github.com/kennethreitz/requests/pull/1439
# dreamsdocometrue!
",voberoi,mattspitz
1662,2013-10-10 22:44:20,"I should add, this is partly in jest since I know @mattspitz.

My contribution is probably not as substantial as others', in which case #dreamsdonotcometrueaseasilyasihoped.
",voberoi,mattspitz
1659,2013-10-24 10:57:58,"/cc @dstufft
",kennethreitz,dstufft
1659,2013-10-24 12:07:18,"Tentative new cert is available here:

http://ci.kennethreitz.org/job/ca-bundle/lastSuccessfulBuild/artifact/certs.pem

+1 from @dstufft, @agl, or @tiran would be appreciated :)

Once I receive a +1, a new release will be cut.
",kennethreitz,tiran
1657,2013-10-08 01:01:47,"@Lukasa I assigned this to you for code review, but don't spend too much time on it yet. I've got work to do.
",sigmavirus24,Lukasa
1657,2013-11-27 14:41:57,"@Lukasa @kennethreitz can we get this merged soon? I just realized I'll probably need this for github3.py so that I don't have to do [this](https://github.com/sigmavirus24/github3.py/commit/9a1b4d892a31bd12ee477f39caada76670cfc062#diff-8a0b7651f0a27fb4490e3f2d87206eedR67) ;)
",sigmavirus24,kennethreitz
1654,2013-10-05 14:49:11,"@untitaker I was not objecting to the changes themselves. I have far more to maintain than just requests and I do far more offline than anyone really knows or cares to know about. @kennethreitz has an ostensibly similar schedule (which is why he has minions) and having exact language to review in an email makes our lives far easier. Stressing that point with @riyadparvez will only make future pull requests to this and other projects on his behalf better and perchance allow them to be merged in a quicker fashion.
",sigmavirus24,kennethreitz
1652,2013-10-04 13:05:54,"This looks fine to me. You could potentially just use `if` instead of `elif` here: might be slightly cleaner. @sigmavirus24, thoughts?
",Lukasa,sigmavirus24
1647,2013-10-04 10:21:17,"Ah, yes, ok. The problem is that we pass the full URL to `proxy_from_url`. This ends up calling `urllib3.util.parse_url` which is a bit stupid when it comes to splitting the path.

@shazow Do you want to make `proxy_from_url` better, or do you want Requests to try to sanitise the URL it sends to you?
",Lukasa,shazow
1646,2013-10-03 19:57:03,"Definitely still a urllib3 bug. =) @shazow is playing silly buggers, I think. I've put a reproducible script in the urllib3 issue. So you don't feel like you're being messed around I'm going to leave this issue open, but I'm convinced it's upstream.
",Lukasa,shazow
1640,2013-10-01 00:57:09,"This would have to wait for 3.0 since it's a significant API change unless @kennethreitz or @Lukasa disagree.
",sigmavirus24,kennethreitz
1640,2013-10-01 00:57:09,"This would have to wait for 3.0 since it's a significant API change unless @kennethreitz or @Lukasa disagree.
",sigmavirus24,Lukasa
1640,2013-10-01 08:11:00,"This in principle looks great @abarnert, thanks so much!

We wouldn't need to wait until 3.0, because this change isn't backwards incompatible: adding the ability to pass a 4-tuple can be done in a minor release (e.g. 2.1.0) according to [semver](http://semver.org/).

The real question is whether we think this API extension is the right way to handle it. As you pointed out in #1640, we already poorly document this corner of the API so that definitely has to be fixed.

@kennethreitz: Are you happy with this extension to the API, or would you like to reconsider the multipart file API entirely?
",Lukasa,kennethreitz
1638,2013-10-01 01:57:29,"This is default python `cookielib` behaviour:



This is not an issue in requests. Perhaps we did something different with the Host header previously and possibly ignored it, but that version is so ancient that I don't see this as a real issue. If @kennethreitz and @Lukasa agree they can close this issue.
",sigmavirus24,kennethreitz
1638,2013-10-01 01:57:29,"This is default python `cookielib` behaviour:



This is not an issue in requests. Perhaps we did something different with the Host header previously and possibly ignored it, but that version is so ancient that I don't see this as a real issue. If @kennethreitz and @Lukasa agree they can close this issue.
",sigmavirus24,Lukasa
1634,2013-09-28 15:02:05,"I'm -1 on this personally but @Lukasa and @kennethreitz might have different opinions.
",sigmavirus24,kennethreitz
1634,2013-09-28 15:02:05,"I'm -1 on this personally but @Lukasa and @kennethreitz might have different opinions.
",sigmavirus24,Lukasa
1627,2013-09-26 07:52:25,"Thanks for this!

I'm not in the best place to review this, having not contributed to this section of our codebase, but I can point out a few things. @sigmavirus24 would do a better job of reviewing this change.

Firstly, according to RFC 2617, ['If the ""algorithm"" directive's value is ""MD5-sess"", then A1 is calculated only once - on the first request by the client following receipt of a WWW-Authenticate challenge from the server.  It uses the server nonce from that challenge...'](http://tools.ietf.org/html/rfc2617#page-13). This is something that is very difficult for our current authentication handler system to do, but appears to mean that we are severely limited in our ability to properly execute 'MD5-sess'.

Secondly, the `build_digest_header` method is now _massive_ and quite difficult to follow. I think we should be aiming to refactor this method, either as part of this pull-request or more generally.
",Lukasa,sigmavirus24
1625,2013-09-25 14:49:09,"I'm totally +0 on this, it's up to @kennethreitz. =)
",Lukasa,kennethreitz
1622,2013-09-25 07:48:23,"This is a good catch, and thanks so much for the bug report. The problem is almost certainly in the underlying urllib3 implementation of the proxy behaviour though. Let's not raise an issue there just yet until we can isolate the problem, but I think the fix will have to be in that library.

Hey @schlamar, can you take a look at this?
",Lukasa,schlamar
1622,2013-10-11 11:08:20,"@schlamar Sorry I let this slip past me. The answer there is 'maybe'. @t-8ch was one of the people strongly pushing for explicit proxy schemes: do you have an opinion?
",Lukasa,t-8ch
1621,2013-09-25 01:42:32,"In a similar vein to the migrating to 1.x docs, this section details changes that can break existing code in order to ease the migration to 2.x. I didn't talk about any of the added APIs since those don't really break existing code. Compared with the last major release, there's not much to say.

This is largely based on @Lukasa's [blog post](http://lukasa.co.uk/2013/09/Requests_20/) and the [changelog](https://github.com/kennethreitz/requests/blob/master/HISTORY.rst). I did not know how folks feel about linking to a blog post from the docs so I didn't add the link.
",davidfischer,Lukasa
1618,2013-09-24 11:21:37,"Thanks for raising this issue!

I seem to recall we discussed this in the past, and decided that what we do here is a user-level issue. Some users may absolutely want their authentication headers to redirect off-host. Does that sound right to you @kennethreitz?
",Lukasa,kennethreitz
1617,2013-09-24 02:45:49,"@laruellef yeah I was just about to go looking for that issue. It almost certainly seems related to #1577. We're still waiting on the urllib3 PR that @Lukasa alluded to in that issue so I'm going to close this while we wait for that to work itself out so we can provide that interface to you.

That said, if you run into timeout issues again, and setting it lower doesn't resolve your issue, then open an issue. Otherwise it might be safe to assume it's related to #1577.
",sigmavirus24,Lukasa
1615,2013-09-21 13:13:43,"Thanks for raising this issue!

I don't think I agree with your assessment of what the 'expected behaviour' of this should be. You've set a timeout of zero seconds. That implies that the connection process should time out after zero seconds, e.g. immediately.

I'm a bit mixed here. On the one hand, we're doing exactly what the code asked for. On the other hand, I'd argue that falsy values for timeout should be treated the same as `None`. @sigmavirus24, thoughts?
",Lukasa,sigmavirus24
1612,2013-09-24 02:21:19,"@sanmayaj do I understand correctly that you expect requests to string-ify everything you send in? Why can you not do this yourself on your inputs?

> Be liberal in what you receive and conservative in what you send.

You should be assuming that everything is a string (or byte) that leaves your computer via requests. This includes headers too. Personally I'm -1 on this. Up to @kennethreitz and @Lukasa though
",sigmavirus24,kennethreitz
1612,2013-09-24 02:21:19,"@sanmayaj do I understand correctly that you expect requests to string-ify everything you send in? Why can you not do this yourself on your inputs?

> Be liberal in what you receive and conservative in what you send.

You should be assuming that everything is a string (or byte) that leaves your computer via requests. This includes headers too. Personally I'm -1 on this. Up to @kennethreitz and @Lukasa though
",sigmavirus24,Lukasa
1606,2013-09-17 07:53:39,"Hi @borfig! Thanks for doing this work!

I have no idea if this'll get accepted or not. We're very reluctant to add new things to the Requests functional API at this stage, so this boils down to whether or not @kennethreitz believes this is a feature worth exposing at that level. I simply don't know what he'll decide. =)

Regardless of what Kenneth decides, the patch itself appears to be in good shape, so if he wants the feature this patch is a good solid implementation of it.
",Lukasa,kennethreitz
1602,2013-09-14 03:34:06,"As a side note: the Travis builds will continue to fail on this branch until someone (:eyes: @kennethreitz) deploys the latest version of httpbin. :wink:
",sigmavirus24,kennethreitz
1589,2013-09-11 14:45:08,"I actually need to backtrack on the JSON encoding: we have logic for it in `Response.json()`. On balance, I think that belongs there unless we add more specific behaviour to `Response.text`, in which case it should be moved. The relevant funky details are that you can use any of the UTF- encodings, I think. Don't hold me to that though.
1. You and I disagree on the definition of 'information that is present'. =) I find RFC 2616's statement to be clear and unambiguous: if you serve, for example, `text/plain` with no charset specified, the headers actually do say to use ISO-8859-1. This is because RFC 2616 defines what they say, and that definition clearly states that `text/X` means ISO-8859-1 unless explicitly overridden in the `Content-Type` header.
   
   We should note here that I'm not arguing that HTML has a default of ISO-8859-1, I'm saying that _any_ MIME type beginning `text/` should have that default. HTML is a notable example but it's far from the only one.
   
   As for what the default should be in a given context, that context is defined by the `Content-Type` header, which is what we're examining anyway. No big deal. =)
2. Re: My sample workflow above. I believe it to be correct(-ish) within the parameters of the problem. The only time that it isn't correct is if the headers actually _do_ specify ISO-8859-1. I didn't handle that case because I wrote the code in 10 seconds. =)

My proposal is to add the following logic (in Python, but not directly related to any part of the Requests code):



Does this seem like a sensible set of logic to you?

Final note: I can't guarantee that a pull request that I'm happy with will get incorporated. Requests is ""one man one vote"": Kenneth is the man, he has the vote. I'm already tempted to say that the entire discussion above is an overreach, and that Kenneth will believe that Requests simply should stop caring about `Content-Type` beyond whether it has a `charset` value (that is, removing the ISO-8859-1 default and not replacing it with anything else).

In fact, let's pose him that exact question (there's no way he has time to read the entire discussion above). I'll also get Ian's opinion:

**BDFL Question:**
@kennethreitz: Currently Requests will use `ISO-8859-1` as the default encoding for anything with `Content-Type` set to `text/<something>` and without a `charset` declaration (as specified by RFC 2616). This can cause problems with HTML that uses `<meta>` tags to declare a non-ISO-8859-1 encoding. We can do one of three things:
1. Be smarter. If the `Content-Type` is `text/html` or one of a similar set of families, we can search for a relevant `<meta>` tag.
2. Stay the same.
3. Remove the ISO-8859-1 default and stop pretending we know about `Content-Type`s at all.

Preferences? @sigmavirus24, I'd like your opinion too. =)
",Lukasa,kennethreitz
1589,2013-09-11 14:45:08,"I actually need to backtrack on the JSON encoding: we have logic for it in `Response.json()`. On balance, I think that belongs there unless we add more specific behaviour to `Response.text`, in which case it should be moved. The relevant funky details are that you can use any of the UTF- encodings, I think. Don't hold me to that though.
1. You and I disagree on the definition of 'information that is present'. =) I find RFC 2616's statement to be clear and unambiguous: if you serve, for example, `text/plain` with no charset specified, the headers actually do say to use ISO-8859-1. This is because RFC 2616 defines what they say, and that definition clearly states that `text/X` means ISO-8859-1 unless explicitly overridden in the `Content-Type` header.
   
   We should note here that I'm not arguing that HTML has a default of ISO-8859-1, I'm saying that _any_ MIME type beginning `text/` should have that default. HTML is a notable example but it's far from the only one.
   
   As for what the default should be in a given context, that context is defined by the `Content-Type` header, which is what we're examining anyway. No big deal. =)
2. Re: My sample workflow above. I believe it to be correct(-ish) within the parameters of the problem. The only time that it isn't correct is if the headers actually _do_ specify ISO-8859-1. I didn't handle that case because I wrote the code in 10 seconds. =)

My proposal is to add the following logic (in Python, but not directly related to any part of the Requests code):



Does this seem like a sensible set of logic to you?

Final note: I can't guarantee that a pull request that I'm happy with will get incorporated. Requests is ""one man one vote"": Kenneth is the man, he has the vote. I'm already tempted to say that the entire discussion above is an overreach, and that Kenneth will believe that Requests simply should stop caring about `Content-Type` beyond whether it has a `charset` value (that is, removing the ISO-8859-1 default and not replacing it with anything else).

In fact, let's pose him that exact question (there's no way he has time to read the entire discussion above). I'll also get Ian's opinion:

**BDFL Question:**
@kennethreitz: Currently Requests will use `ISO-8859-1` as the default encoding for anything with `Content-Type` set to `text/<something>` and without a `charset` declaration (as specified by RFC 2616). This can cause problems with HTML that uses `<meta>` tags to declare a non-ISO-8859-1 encoding. We can do one of three things:
1. Be smarter. If the `Content-Type` is `text/html` or one of a similar set of families, we can search for a relevant `<meta>` tag.
2. Stay the same.
3. Remove the ISO-8859-1 default and stop pretending we know about `Content-Type`s at all.

Preferences? @sigmavirus24, I'd like your opinion too. =)
",Lukasa,sigmavirus24
1586,2013-09-09 15:02:00,"Thanks for raising this issue!

Quickly, before I dive into this: the correct solution when Requests is getting in your way like this is to use `Response.raw`. =) 

Before we get into discussing this feature request further, I'm interested to know how this problem bit you. We should only decompress the file if the file is served with a `Content-Encoding` header set to `gzip`. My best guess is that a `.tar.gz` file should be served with no `Content-Encoding` header and `Content-Type: x-gzip`. Do we know if this is a common behaviour with gzipped raw files, or if this was an unusual behaviour from your webserver? /cc @sigmavirus24 
",Lukasa,sigmavirus24
1582,2013-09-08 15:11:47,"This is a good idea @matt-hickford. I see three ways of doing this:
1. Use a `Proxy` class in the `Proxies` dictionary. Definitely do-able, but adds quite a bit of complication into the API. Possible. +0
2. Use proxy-type Auth handlers. Not good. -1
3. Use Transport Adapters.

I'm generally in favour of TAs. In particular, Transport Adapters are really the only thing that knows anything about HTTPS over proxy (the CONNECT verb), which needs to be differently handled from other kinds of messages. To that end, it seems more natural to provide proxy authentication solutions at the Transport Adapter level.

Potentially TAs could take pluggable auth modules, just like individual requests? /cc @sigmavirus24 
",Lukasa,sigmavirus24
1577,2013-09-05 08:16:44,"So, @laruellef, it looks like @kevinburke is doing some work on the `urllib3` side to add better timeout control. When that gets sorted we'll probably try to plumb it through to Requests. I think waiting for that issue (shazow/urllib3#231) to be resolved is the correct next step here.

Thanks for raising this, and keep track of the `urllib3` issue!
",Lukasa,kevinburke
1573,2016-02-24 07:36:00,"@botondus I think I found a simpler way to achieve this with request library. I am documenting this for other people who are facing the issue.

I assume that you have a .p12 certificate and a passphrase for the key.

### Generate certificate and private key.



Well, we are not done yet and we need to generate the key that doesn't require the PEM password every time it needs to talk to the server.

### Generate key without passphrase.



Now, you will have `certificate.pem` and `plainkey.pem`, both of the files required to talk to the API using requests.

Here is an example request using these cert and keys.



Hope this helps:

cc @kennethreitz @Lukasa @sigmavirus24 
",vinitkumar,kennethreitz
1568,2013-09-02 15:06:27,"Thanks for this @alekibango, and thanks for providing a PR!

It's worth noting that this was an issue we already knew about: see #1426. We work very closely with @shazow on his urllib3 project because we build Requests on top of it, and we regularly take newer versions of urllib3 with releases of Requests. I'm really glad you opened this issue, but in future it is worth quickly searching the GitHub issue tracker to see if someone else raised it before you. It would have saved you a little time! :smile:

Thanks again for this!
",Lukasa,shazow
1567,2013-09-02 08:05:34,"So I think this is an SSL problem, because I have no problems running this query on my Windows box. What OS are you using?

Also, let's ping @t-8ch for his expertise here. =)
",Lukasa,t-8ch
1565,2013-09-01 17:12:19,"Pointed out by @dstufft in #1560.
",Lukasa,dstufft
1564,2013-08-31 02:04:08,"Part of the rational for my argument against this is in the issue you provided. To quote ""Tim B"":

> Few mainstream browsers support gzip transfer-encoding.

requests never attempts to mimic anything that isn't in a browser and isn't very common. On the other hand, we also strive to mimic curl(/libcurl) and the fact that curl supports it (according to Tim B in the same comment) would be a point in your favor except that Tim then goes on to say:

> …however, there's no webserver that will serve this.

So I think it is safe to say that we can satisfy the largest number of our uses without it.

To not give you all of the details, however, would be dishonest of me. The Content-Encoding decompression occurs in `urrlib3` if I remember correctly. The correct place to request this would then be `urrlib3`. 

That said, I'm leaving this open so @Lukasa and @kennethreitz can feel free to correct me if I have mistated anything above.
",sigmavirus24,kennethreitz
1564,2013-08-31 02:04:08,"Part of the rational for my argument against this is in the issue you provided. To quote ""Tim B"":

> Few mainstream browsers support gzip transfer-encoding.

requests never attempts to mimic anything that isn't in a browser and isn't very common. On the other hand, we also strive to mimic curl(/libcurl) and the fact that curl supports it (according to Tim B in the same comment) would be a point in your favor except that Tim then goes on to say:

> …however, there's no webserver that will serve this.

So I think it is safe to say that we can satisfy the largest number of our uses without it.

To not give you all of the details, however, would be dishonest of me. The Content-Encoding decompression occurs in `urrlib3` if I remember correctly. The correct place to request this would then be `urrlib3`. 

That said, I'm leaving this open so @Lukasa and @kennethreitz can feel free to correct me if I have mistated anything above.
",sigmavirus24,Lukasa
1561,2013-08-30 15:10:18,"It does. See [L244 of urllib3/response.py](https://github.com/shazow/urllib3/blob/master/urllib3/response.py#L244). I'm not totally convinced this is a bug. For the purposes of the comment, the phrase ""the actual key"" should more correctly read ""the key that was added to the dictionary"". The CID is functioning as intended.

@sigmavirus24, I don't feel like urllib3 is doing the wrong thing here. Do you agree?
",Lukasa,sigmavirus24
1558,2013-08-28 11:34:51,"@ssbarnea To the best of my knowledge we have no plans to release any further 1.2.X point releases. The next planned release is 2.0.0. I'm potentially open to you making a patch against master, but I'd want to check with @kennethreitz first.
",Lukasa,kennethreitz
1558,2013-08-28 12:09:55,"Go for it



Kenneth Reitz

On Wed, Aug 28, 2013 at 7:34 AM, Cory Benfield notifications@github.com
wrote:

> ## @ssbarnea To the best of my knowledge we have no plans to release any further 1.2.X point releases. The next planned release is 2.0.0. I'm potentially open to you making a patch against master, but I'd want to check with @kennethreitz first.
> 
> Reply to this email directly or view it on GitHub:
> https://github.com/kennethreitz/requests/issues/1558#issuecomment-23407592
",kennethreitz,kennethreitz
1558,2013-09-01 18:16:13,"I'm not (quite frankly) convinced that a `Request` object should be entirely pickleable. It is an ephemeral object that we use to construct the `PreparedRequest`. `PreparedRequest` objects do not store a reference to it, so I'm 90% sure it does not need to be pickleable. We should probably make it pickleable anyway though it is a seriously minor introduction of code and it will cover us for the future.

I'm wondering if @shazow would be opposed to making the `urllib3` response object pickleable. If he were to approve, then I think it would make sense to also make the Response object pickleable. 
",sigmavirus24,shazow
1553,2013-08-25 06:16:06,"It would be nice if requests could support automatic proxy detection on Windows.

Both urllib2 and urllib accomplish this by reading windows registry via urllib.getproxies() on Windows and using _scproxy on Mac.

I opened an issue in [urllib3](https://github.com/shazow/urllib3/issues/232) but @shazow doesn't think it belongs there.
",oliverjanik,shazow
1550,2013-08-26 07:48:02,"That's true, it does. Still, I think it'd be better to set it higher up if possible, to make the `PreparedRequest` look as much like what went out on the wire as possible. @kennethreitz may disagree though.
",Lukasa,kennethreitz
1545,2013-08-20 19:16:10,"Hi @Aaron1011! That's a perfectly good idea. However, 100% code coverage has never been an explicit goal of this project. In the absence of a particular code coverage target, I'm not convinced that adding coverage by default is a good plan. I'll defer to @kennethreitz's judgement in this case.

(Side note: You can do your own code coverage testing very easily by installing the [pytest-cov](https://pypi.python.org/pypi/pytest-cov) plugin for py.test. Then testing requests is done by the command `py.test --cov-report term --cov=requests test_requests.py`. This creates a fair amount of noise because it tests our coverage of our vendored libraries (charade and urllib3), which is irrelevant here. Excluding those we're at roughly 75% test coverage, which is pretty solid. Sample output from master:


",Lukasa,kennethreitz
1545,2013-08-20 19:19:20,"In general I'm +1 on coverage reports for all projects. This is 
@kennethreitz's final call though.
",sigmavirus24,kennethreitz
1545,2013-09-01 18:28:42,"@kennethreitz any opinions on this?
",sigmavirus24,kennethreitz
1545,2013-09-24 02:51:09,"Pinging @kennethreitz 
",sigmavirus24,kennethreitz
1542,2013-08-20 02:15:42,"@kennethreitz any idea when Heroku will upgrade to python 2.7.5? I thought it was already using that.
",sigmavirus24,kennethreitz
1542,2014-05-20 16:57:43,"@cvolawless can you try making the same request as if it were a curl request? If you receive the same response as requests, you should contact Heroku support. If you get a different response feel free to open a new issue so @Lukasa and I can be of use. :)
",sigmavirus24,Lukasa
1542,2014-05-20 16:59:21,"@cvolawless also, if you can't discuss the issue publicly, @Lukasa and I both have our emails on our profile pages, so you can send us an email as an alternative for private (confidential) support.
",sigmavirus24,Lukasa
1534,2013-08-17 01:50:45,"This probably doesn't help but a few things:
- Requests is ill-suited for the `file://` scheme.
- The `_original_response` object can be faked in a couple ways but only one way if you're interested in python 2 and 3 support. If the latter is your purpose then checkout sigmavirus24/betamax. I'm using the latter method there. If you have questions, feel free to ask me outside of this message.

I also wouldn't be opposed to documenting the method, but for the most part, I don't see this being a bug. Maybe @Lukasa or @kennethreitz 
",sigmavirus24,kennethreitz
1534,2013-08-17 01:50:45,"This probably doesn't help but a few things:
- Requests is ill-suited for the `file://` scheme.
- The `_original_response` object can be faked in a couple ways but only one way if you're interested in python 2 and 3 support. If the latter is your purpose then checkout sigmavirus24/betamax. I'm using the latter method there. If you have questions, feel free to ask me outside of this message.

I also wouldn't be opposed to documenting the method, but for the most part, I don't see this being a bug. Maybe @Lukasa or @kennethreitz 
",sigmavirus24,Lukasa
1534,2013-08-17 01:55:36,"Heh, @kennethreitz is the one who told me to file it :) I actually have it mocked and it's pretty janky. Simple fix for upstream though and the `file://` url is actually working pretty well, requests seems to handle it fine. The only ""gotchas"" are this one which requires a pretty janky hack to get around, and an error because `file://` urls don't have a hostname but another small hack can fix that too.

FWIW: https://github.com/dstufft/pip/commit/aba6f37d5619b26a4dbc4b2ceb01e9dd2899f5ad
",dstufft,kennethreitz
1534,2013-08-17 02:12:01,"The fix is simple without a doubt and since it seems @kennethreitz seems okay with it, I'll put together a PR with it.

That said, the best workaround is not that janky frankly but you're right in that it shouldn't be necessary. My project needs it though :/
",sigmavirus24,kennethreitz
1532,2013-08-15 07:48:12,"Yeah, I think you're right. @kennethreitz, are you open to me doing this for 2.0?
",Lukasa,kennethreitz
1523,2013-08-12 11:06:28,"So it's worth noting that the default response of this project to any request to change the API is No. An API change request starts with -100 points and needs to justify itself to that level before inclusion.

Sadly, I don't think this is sufficiently useful to justify inclusion. It would get a decent amount of use, but it's not hard to do yourself and not super common (like JSON decoding). I just don't think the API would benefit particularly from its inclusion.

I'll wait until @sigmavirus24 and/or @kennethreitz comments on this, but I'm -1. Sorry. =(

NB: The transformation for anyone that needs it is



Any RFC-2616 compliant Content-Type header should come through that just fine, including no header or an empty header.
",Lukasa,kennethreitz
1523,2013-08-12 11:06:28,"So it's worth noting that the default response of this project to any request to change the API is No. An API change request starts with -100 points and needs to justify itself to that level before inclusion.

Sadly, I don't think this is sufficiently useful to justify inclusion. It would get a decent amount of use, but it's not hard to do yourself and not super common (like JSON decoding). I just don't think the API would benefit particularly from its inclusion.

I'll wait until @sigmavirus24 and/or @kennethreitz comments on this, but I'm -1. Sorry. =(

NB: The transformation for anyone that needs it is



Any RFC-2616 compliant Content-Type header should come through that just fine, including no header or an empty header.
",Lukasa,sigmavirus24
1507,2013-07-31 07:26:36,"Mm. I agree that you can see many situations in which modifying the Request itself creates unexpected behaviour. Conversely, though, I can definitely see situations in which `prepare_request`'s allocation hurts you: specifically, situations when you have only one `Session`. Then you have to absorb two allocations per request instead of one, which seems like a nasty waste of time for the GC.

My engineering inclination is to leave the `Request.copy` method in, and say that by default we don't do any extra allocations, and if you want to you have to do it yourself. This turns your code into:



The problem is, I think that the API isn't as good here. I'm pretty torn. My intuition is that the _general_ use-case does not have the same unprepared `Request` being sent to different `Session`s, so we should optimise for that case. I'm prepared to be wrong here, though. @sigmavirus24, thoughts?

**EDIT**: I should note that while the cost of extra allocations is usually not that high, it's inescapable, while the bug introduced by failing to copy a `Request` can be easily worked around.
",Lukasa,sigmavirus24
1507,2013-08-01 01:12:45,"Here's my hang-up. It consists of two parts:
1. > 90% of our users won't ever need this as such, our existing strategy is to use Request objects as organizational throw away objects, as such we never present them to the user directly as part of a Response. What we do allow is for a user to replicate what we do in order to send a PreparedRequest. So as the API is concerned, the most import objects are the Session and Response object. Next most important are PreparedRequest objects because those are directly exposed via the Response object and finally the Request object. I fail to see how we can not just explicitly document for the user that the Request object will be mutated. We're not using a purely functional language so there's no reasonable expectation that the Request will not be mutated.
2. The second part, contingent on that first, is that when you prepare a request, you're not going to get it back and so it may be mutated. If you're using this advanced API then you should have read the docs where we can explicitly document that those objects will be mutated. Does this hamper your use case? Yes. Does it meet the needs of what is likely > 90% of our users? More emphatically, yes.

I haven't skimmed your PR because it seems (from the conversation that I've read via email) that it's very much in flux. That said, if you can sell @Lukasa and me on why we need Ruby-ish methods here, we can probably sell @kennethreitz and frankly you haven't sold me.
",sigmavirus24,kennethreitz
1505,2013-07-30 17:29:10,"You're quite right, there is not. In no small part this is because this functionality is not exposed (at least not cleanly) in versions of Python before 3.3: additionally, AFAIK, urllib3 does not expose the functionality (@t-8ch knows the TLS support of urllib3 way better than I do, he'll be able to tell you).
",Lukasa,t-8ch
1498,2013-07-28 06:43:35,"This should improve the user experience from #1397.

@sigmavirus24, @kennethreitz: I'm not really happy with the nested try...except blocks here. Suggestions for a better style? Nothing leaps out at me.
",Lukasa,kennethreitz
1498,2013-07-28 06:43:35,"This should improve the user experience from #1397.

@sigmavirus24, @kennethreitz: I'm not really happy with the nested try...except blocks here. Suggestions for a better style? Nothing leaps out at me.
",Lukasa,sigmavirus24
1497,2013-07-28 06:17:58,"This is for 2.0, and is in response to @t-8ch's suggestion that proxies should have explicit schemes instead of guessing (which is stupid).

@sigmavirus24, @kennethreitz: I'm not entirely happy with the validation code being here: let me know if you have a better idea for where it should go.
",Lukasa,kennethreitz
1497,2013-07-28 06:17:58,"This is for 2.0, and is in response to @t-8ch's suggestion that proxies should have explicit schemes instead of guessing (which is stupid).

@sigmavirus24, @kennethreitz: I'm not entirely happy with the validation code being here: let me know if you have a better idea for where it should go.
",Lukasa,t-8ch
1497,2013-07-28 06:17:58,"This is for 2.0, and is in response to @t-8ch's suggestion that proxies should have explicit schemes instead of guessing (which is stupid).

@sigmavirus24, @kennethreitz: I'm not entirely happy with the validation code being here: let me know if you have a better idea for where it should go.
",Lukasa,sigmavirus24
1493,2013-07-25 18:14:22,"Thanks for raising this issue @julie777!

I'm strictly +0 on this. I can see the utility of it, but I don't know that a context manager on `Response` is semantically the right way to approach this issue. If @kennethreitz is +1 on this, though, I think it'd make a good addition to V2.0.
",Lukasa,kennethreitz
1475,2013-07-20 17:04:19,"This PR is a work in progress. It addresses some problems we've got with Exceptions. Currently it contains two fixes:
- Resolves the misspelling of 'scheme', reimplementing the fix proposed in #1187.
- Attempts to provide a fix of #1294.

This second part is of particular interest. I want to know what @sigmavirus24 thinks of this possible fix.
",Lukasa,sigmavirus24
1473,2013-07-20 20:05:32,"@Lukasa some eyes on this early on will be helpful too.
",sigmavirus24,Lukasa
1473,2013-08-01 01:38:26,"Cookies are collected on redirects successfully.

I think @gazpachoking worked on that successfully. This was a similar problem 
that wasn't able to be handled the same way.
",sigmavirus24,gazpachoking
1472,2013-07-19 15:20:41,"I'm prepared to believe this was broken by commit 6d6252aa9fc4bb38d7f68073b41092c31ddb146b, and more specifically [this commit](https://github.com/shazow/urllib3/commit/d00d3052382de69382e21af8d10524e5aefadde8). 

I'm pretty sure this failure is in urllib3, not Requests: can you open this issue over there, and cc @schlamar on it?
",Lukasa,schlamar
1459,2013-07-15 13:56:58,"/cc @Lukasa @sigmavirus24 

What would Requests 2.0 include?
- #1338 
- ...
",kennethreitz,Lukasa
1459,2013-07-15 13:56:58,"/cc @Lukasa @sigmavirus24 

What would Requests 2.0 include?
- #1338 
- ...
",kennethreitz,sigmavirus24
1456,2013-07-13 09:15:04,"Done. By the way, i've seen some status codes that did not match with the ones in the IANA list, but i don't if @kennethreitz did that intentionnaly or if it's just some errors. 

Eg:

`uri_too_long` is set to 122 in requests but the IANA set it to 414 (103 to 199 are unassigned)
",phndiaye,kennethreitz
1445,2013-07-09 09:11:50,"Mm, I guess I can see wanting to do this. I'm still pretty much +0 on it though, so I'll defer to @sigmavirus24 and @kennethreitz.
",Lukasa,kennethreitz
1445,2013-07-09 09:11:50,"Mm, I guess I can see wanting to do this. I'm still pretty much +0 on it though, so I'll defer to @sigmavirus24 and @kennethreitz.
",Lukasa,sigmavirus24
1441,2013-07-04 09:41:43,"As pointed out by @lukesneeringer in #1395, the kwargs in the non-urllib3 branch of the `iter_content` code are urllib3 specific. This PR will finish the split I originally made in #1425.

/cc @sigmavirus24 and @lukesneeringer who have both done work that has been rendered unnecessary (though not useless) by this change.
",Lukasa,sigmavirus24
1436,2013-06-26 09:51:50,"Actually, on second thought, I'm no longer convinced we should change this.

I've moved that code in a local copy of Requests, and while everything works fine it just looks _wrong_ to me. `Session.send()` doesn't really mess about with keyword arguments, but this change would do that.

I'm now tempted to say that if you've circumvented `Session.request()` (in order to prepare Requests yourself), you must make sure you do whatever you need from `Session.request()`. In this case, that's merging cookies and getting data from environment variables.

I'm now +0 on this, so I'll wait until we get input from @kennethreitz and @sigmavirus24.
",Lukasa,kennethreitz
1436,2013-06-26 09:51:50,"Actually, on second thought, I'm no longer convinced we should change this.

I've moved that code in a local copy of Requests, and while everything works fine it just looks _wrong_ to me. `Session.send()` doesn't really mess about with keyword arguments, but this change would do that.

I'm now tempted to say that if you've circumvented `Session.request()` (in order to prepare Requests yourself), you must make sure you do whatever you need from `Session.request()`. In this case, that's merging cookies and getting data from environment variables.

I'm now +0 on this, so I'll wait until we get input from @kennethreitz and @sigmavirus24.
",Lukasa,sigmavirus24
1436,2013-06-26 12:22:12,"Hmm, I'm prepared to be convinced by that line of argument. I'd still like at least one of @kennethreitz and @sigmavirus24 to weigh in, but I think that's reasonable. 
",Lukasa,kennethreitz
1436,2013-06-26 12:22:12,"Hmm, I'm prepared to be convinced by that line of argument. I'd still like at least one of @kennethreitz and @sigmavirus24 to weigh in, but I think that's reasonable. 
",Lukasa,sigmavirus24
1430,2013-06-24 07:55:07,"I'm pretty sure that the API is doing the wrong thing here.

The '+' character has a syntactic meaning in the query string. Spaces may be encoded either to the literal `+` or to `%20`. For this reason, a non-space `+` character _must_ be encoded to `%2B`. Given that the HN API is clearly using the `%20` representation of a space, I think someone needs to tell them that they can't use the `+` like that.

Unfortunately, this makes it a lot harder for us to help you on this problem. You do have some options. One would be to break open the flow, building `PreparedRequests` yourself and adjusting their URLs. You could potentially use @sigmavirus24's [uritemplate](https://github.com/sigmavirus24/uritemplate) library to help with that.
",Lukasa,sigmavirus24
1426,2013-06-18 21:32:03,"`urlparse` from the stdlib keeps the braces.

cc @shazow 
",t-8ch,shazow
1420,2013-06-13 08:16:58,"Hi @jase1987! Thanks so much for this pull request. :cake:

Unfortunately, I think it's really very unlikely that we'll accept this. This has nothing to do with the PR itself: the code is in great shape. However, I don't think we want the feature.

Matrix parameters are really very, very infrequently used. The [URI RFC](http://pretty-rfc.herokuapp.com/RFC3986) does not include them, On top of that, the article you linked to doesn't just say they stopped being supported in '01: they hadn't been supported up until then _either_. I haven't done exhaustive research here, but it's quite possible they were never part of the URI (or URL) standards.

You've also only tackled a subset of the problem. Insanely, matrix parameters can apply to any (and all) portions of the path element, e.g. `http://example.com/res/categories;name=foo/objects;name=green/?page=1`. This is very difficult to cleanly represent in Requests' standard functional API.

Given that they're not part of the standard, are very infrequently used, and are difficult to do 'properly', I think we won't want to put this in mainline Requests. With that said, I encourage you to maintain a downstream fork of Requests that includes them if that would be useful to you.

Also, I'm very sorry that you have to interact with such an unhelpful REST API. =)

I'm going to leave this open until @kennethreitz takes a look, but I'd be surprised if he wanted to add support for matrix parameters.

Again, thanks so much for the work!
",Lukasa,kennethreitz
1419,2013-06-13 08:03:08,"I'm +0 on this, actually. I like having clearer class names, but having the full class name makes it way more obvious how to import the class. If it were me, I'd probably leave it as is, but @kennethreitz will probably disagree with me. =)
",Lukasa,kennethreitz
1418,2013-06-11 19:19:40,"This is a duplicate of #1289. Right?
According to the bugreport you linked it's an issue with the SSL/TLS version requests is using to connect to the server.
@Lukasa wrote a blogpost about customizing the version, requests uses: http://lukasa.co.uk/2013/01/Choosing_SSL_Version_In_Requests/

I modified the code from the blog a bit, so that it works with the current version of requests:
(Try to remove `s.mount()` to break it again)



Btw, I also couldn't connect from my arch box.
And also congratulations for open sourcing your project!
",t-8ch,Lukasa
1418,2013-06-11 21:21:55,"I suspect (I'm not sure) the server is buggy. It sees a TLS version it doesn't recognize and then stops responding, while the client is waiting for a respones, even an error would be better.
I have the same problem on archlinux with openssl 1.0.1.e. Rereading your last bugreport it seems It worked for me back then, with an older version of openssl.
This older version presumably used a lower TLS version, which didn't freak out the server.
So the version is rather too new :smile:

@Lukasa You may want to update your blogpost to include the `block` argument, which is needed nowadays.
",t-8ch,Lukasa
1408,2013-06-07 10:21:18,"Hi @wasw100, thanks for raising this pull request!

Unfortunately, it's not clear to me whether we want to continue supporting adding Morsels to `RequestsCookieJars`. Can I get a call on this @kennethreitz?
",Lukasa,kennethreitz
1408,2013-06-07 10:43:22,"I agree that it's useful, I just wonder how useful it is. Requests does not include plenty of useful code on the grounds that it complicates the API and increases the amount of code we need to maintain. I'm on the fence about this: I don't mind keeping it and I don't mind throwing it away. In situations like that, I'd usually throw it away.

That's why I'm interested to see what @kennethreitz thinks.
",Lukasa,kennethreitz
1405,2013-06-04 12:46:34,"There have been talks in urllib3#140 about a new class `Verification` where users can embed their own verification logic, as urllib3's ssl api is becoming a but unwieldy.
Until this gets implemented it may be possible to change the `assert_hostname` of `urllib3` to disable hostname verification if it's value is the empty string `''`.

cc @shazow
",t-8ch,shazow
1401,2013-06-03 09:03:26,"Hi @foxx, thanks for opening this issue! Let me address these in no particular order.
1. Implementing .read(). Requests explicitly provides the underlying transport library's notion of a response in the `Response.raw` property. If you believe that `decode_content=True` should be the default behaviour (and I agree that it should), I recommend you open a pull request to that effect on [urllib3](https://github.com/shazow/urllib3). I'm sure @shazow will be happy to take a look at it. =)
2. Changing the default read size is a bit of an odder one. Given the point I made in **1**, we probably can't do it in Requests itself. I would also argue that any method named `read()` should behave like a file-like object, which doesn't have a minimum read value. When I get internet in my new house, I'm going to take a look at implementing the idea in shazow/urllib3#186, which should be able to paper over this issue.
3. Default `iter_content()` size is a sore topic around here: see #844, which has still not been completely resolved. Your input is welcome there. =)
4. Where is the memory leak in `iter_content()`?
",Lukasa,shazow
1394,2013-05-31 13:50:51,"@shazow and I have explored adding this several times. Every time, our conclusion is that it's simply too much work with little performance benefit.

Waiting for SPDY now :)
",kennethreitz,shazow
1390,2016-06-09 08:37:22,"Oh, and @shazow, this is something you might care about too, in part because we might be able to make urllib3 do most of the heavy lifting here.
",Lukasa,shazow
1388,2013-05-25 08:03:59,"@Arfrever Unfortunately we rushed the last two releases out due to a pair of nasty bugs, one affecting everyone on 3.3.2 and one affecting anyone using OAuth (both bad). Presumably @kennethreitz was midway through working on something when he published the last two releases, and my badgering him to release fast caused him to lose track of it.

However, the library itself functions fine, only the tests fail to run. Is it sufficient for you to simply change the tests locally?
",Lukasa,kennethreitz
1384,2013-05-24 07:22:17,"Hi, I'm quite new to the world of Python, so might be missing something. When I submitted [this pull request](https://github.com/kennethreitz/requests/pull/1382) the other day, @sigmavirus24's response was surprising. Surely, putting a checkout of a third party library in your repository is not the right way to manage dependencies?

Can't you use [requirements.txt](https://github.com/kennethreitz/requests/blob/master/requirements.txt) to include urllib3 or at least make it a git submodule?
",tasuk,sigmavirus24
1380,2013-05-22 21:02:57,"This is easily worked around by doing:



Technically we could return any object that behaves like a dict without it being a dict, so I don't see this as much of a problem. What would be fantastic is if there were a `__serialize__` method we could declare on objects so that modules like `json` could just reference that as the author of the object intends.

I'll defer to @Lukasa on this one though. I personally don't find this so offensive frankly but I'm not against changing it either.
",sigmavirus24,Lukasa
1373,2013-05-21 08:52:21,"`filepost.encode_multipart_formdata` says all fieldnames should be unicode and currently that is not the case - https://github.com/shazow/urllib3/blob/master/urllib3/filepost.py#L55 - since requests encodes to bytes here - https://github.com/kennethreitz/requests/blob/master/requests/models.py#L108.

Discovered after plenty of fun in https://github.com/requests/requests-oauthlib/pull/43. This should fix https://github.com/kennethreitz/requests/issues/1371.

Tested in 2.6, 2.7 and 3.3.

cc @Lukasa, @michaelhelmick, @sigmavirus24
",ib-lundgren,Lukasa
1373,2013-05-21 08:52:21,"`filepost.encode_multipart_formdata` says all fieldnames should be unicode and currently that is not the case - https://github.com/shazow/urllib3/blob/master/urllib3/filepost.py#L55 - since requests encodes to bytes here - https://github.com/kennethreitz/requests/blob/master/requests/models.py#L108.

Discovered after plenty of fun in https://github.com/requests/requests-oauthlib/pull/43. This should fix https://github.com/kennethreitz/requests/issues/1371.

Tested in 2.6, 2.7 and 3.3.

cc @Lukasa, @michaelhelmick, @sigmavirus24
",ib-lundgren,sigmavirus24
1371,2013-05-21 06:24:55,"Firstly, thanks for opening this issue! This exact bug was found about 8 hours before you opened this, in requests/requests-oauthlib#43. The problem is that all field and file names passed to `encode_multipart_formdata` should be Unicode. As you identified, #1279 caused all of these field and file names to be bytes, not unicode.

@ib-lundgren has proposed a fix across in requests-oauthlib, which involves changing L108 as follows:



If you would like to fix this yourself, you can write a test and apply the fix, then send a Pull Request. Alternatively,@ib-lundgren will probably do that at some point soon. =)
",Lukasa,ib-lundgren
1369,2013-05-20 19:21:58,"@jellyflower Thanks for reporting this bug. It's actually one we already knew about: see requests/requests-oauthlib#43.

I wonder if 3.3.2 has introduced a bug in Requests. @sigmavirus24, I'm leaving this for you. =)
",Lukasa,sigmavirus24
1366,2013-05-19 01:14:35,"/cc @idan
",michaelhelmick,idan
1363,2013-05-16 17:39:33,"@dave-shawley it most certainly does! Thank you kind sir!

:+1: for @kennethreitz when he comes around these parts again
",sigmavirus24,kennethreitz
1356,2013-05-12 01:11:39,"Thanks @sigmavirus24 for the help!

This was causing problems when I was doing threaded gets to a single host. I can get around it by subclassing the adapter and overriding the creation of the connection pool (where I ran into the problem from #1357), but this seems like it should be something you can do without needing to sub class the adapter.
",Zoramite,sigmavirus24
1356,2013-05-12 01:38:27,":cake: Awesome! Sorry it took so long, but this pull request now looks perfect IMO. Hopefully @Lukasa will agree.
",sigmavirus24,Lukasa
1353,2013-05-07 13:31:47,"For everyone else interested, this stems from [this discussion](http://stackoverflow.com/questions/16390243/closing-python-requests-connection-from-another-thread/16400574?noredirect=1#comment23529567_16400574). I'm personally :-1: on the idea, but I'd like to hear @kennethreitz and @Lukasa chime in.
",sigmavirus24,kennethreitz
1353,2013-05-07 13:31:47,"For everyone else interested, this stems from [this discussion](http://stackoverflow.com/questions/16390243/closing-python-requests-connection-from-another-thread/16400574?noredirect=1#comment23529567_16400574). I'm personally :-1: on the idea, but I'd like to hear @kennethreitz and @Lukasa chime in.
",sigmavirus24,Lukasa
1349,2013-05-05 03:13:58,"@Lukasa concerns?
",sigmavirus24,Lukasa
1341,2013-05-01 18:37:33,"Requested by @sigmavirus24.
",cdunklau,sigmavirus24
1340,2013-05-01 16:46:51,"This PR removes Python 3.1 and 3.2 from the trove classifiers in `setup.py` since @kennethreitz decided to drop support for those versions in the rejected PR #1326. 

It also introduces a rudimentary ""tox.ini"" which makes local tests with multiple Python versions trivial:


",ambv,kennethreitz
1339,2013-04-30 20:05:30,"Fixes #649 and #1329 by making Session.headers a CaseInsensitiveDict,
and fixing the implementation of CID. Credit for the brilliant idea
to map `lowercased_key -> (cased_key, mapped_value)` goes to
@gazpachoking, thanks a bunch.

Changes from original implementation of CaseInsensitiveDict:
1.  CID is rewritten as a subclass of `collections.MutableMapping`.
2.  CID remembers the case of the last-set key, but `__setitem__`
   and `__delitem__` will handle keys without respect to case.
3.  CID returns the key case as remembered for the `keys`, `items`,
   and `__iter__` methods.
4.  Query operations (`__getitem__` and `__contains__`) are done in
   a case-insensitive manner: `cid['foo']` and `cid['FOO']` will
   return the same value.
5.  The constructor as well as `update` and `__eq__` have undefined
   behavior when given multiple keys that have the same `lower()`.
6.  The new method `lower_items` is like `iteritems`, but keys are
   all lowercased.
7.  CID raises `KeyError` for `__getitem__` as normal dicts do. The
   old implementation returned `None`
8.  The `__repr__` now makes it obvious that it's not a normal dict.

See PR #1333 for the discussions that lead up to this implementation
",cdunklau,gazpachoking
1338,2013-05-01 19:21:16,"Yeah that fixes it. Travis just has your 2.6 build queued so @kennethreitz might be wary until it completes (not that he need be).
",sigmavirus24,kennethreitz
1336,2013-04-30 08:27:45,"`HTTPDigestAuth.handle_401()` needs to persist cookies. Thanks for reporting this! I'm marking this as contributor friendly, anyone who wants it should take it. I'm looking at @gazpachoking and @sigmavirus24, they're the cookie masters here.
",Lukasa,gazpachoking
1336,2013-04-30 08:27:45,"`HTTPDigestAuth.handle_401()` needs to persist cookies. Thanks for reporting this! I'm marking this as contributor friendly, anyone who wants it should take it. I'm looking at @gazpachoking and @sigmavirus24, they're the cookie masters here.
",Lukasa,sigmavirus24
1334,2013-06-05 23:59:33,"I'm not sure I follow. Per http://www.ietf.org/rfc/rfc2965.txt hostname matching should be case-insensitive. Is that not how it's behaving today? A quick test with uppercase hosts seems to work.

I'm curious the conditions under which #1385 are needed. For example, this prints 200 for me with no changes to requests:



My patch addresses uppercase schemes in the Location header, so it only affects redirects. I did try what @ViktorHaag has proposed, but I seem to remember having issues with a pool manager?
",rcarz,ViktorHaag
1333,2013-04-29 16:59:19,"Another concern, brought up by @gazpachoking: The original implementation preserves the case of the header name first inserted, this implementation does not.
",cdunklau,gazpachoking
1333,2013-04-29 21:56:55,"@piotr-dobrogost There was some back and forth on IRC about it, and I was swayed. The undefined behavior is preferred because:
1. Simplicity. (KISS)
2. More dict-like in some circumstances.

But I can't remember the finer details. @gazpachoking, @Lukasa can you chime in?
",cdunklau,gazpachoking
1333,2013-04-29 21:56:55,"@piotr-dobrogost There was some back and forth on IRC about it, and I was swayed. The undefined behavior is preferred because:
1. Simplicity. (KISS)
2. More dict-like in some circumstances.

But I can't remember the finer details. @gazpachoking, @Lukasa can you chime in?
",cdunklau,Lukasa
1329,2013-04-26 21:03:27,"Ok, so based on a suggestion from @Lukasa in IRC, I'm considering switching `.sessions.Session.headers` over to `.structures.CaseInsensitiveDict`, but it looks like its `.update` method doesn't do what I'd like. However, I don't really know what the sane behavior would be for edge cases where the input mapping has two keys that have the same `.lower()`...

This needs some extra thinking, I'll ponder it over the weekend.
",cdunklau,Lukasa
1324,2013-04-25 03:20:04,"So the code which causes this break was introduced partially because of Transport Adapters and partially because the re-factor broke the old code. This is a far more elegant way of managing cookies (in my opinion) than we used to use and I'm really not 100% comfortable changing it back. However, this is a major break in the API (although entirely unintended and not exactly forseen) so I am guessing that @kennethreitz will be entirely in favor of finding a solution to return to the old behaviour.

This especially is a pain because @gazpachoking and I put a decent amount of effort into restoring cookie persistance and making it elegant. Perhaps he would be interested in tackling this? _Nudge, nudge_
",sigmavirus24,gazpachoking
1324,2013-04-25 03:20:04,"So the code which causes this break was introduced partially because of Transport Adapters and partially because the re-factor broke the old code. This is a far more elegant way of managing cookies (in my opinion) than we used to use and I'm really not 100% comfortable changing it back. However, this is a major break in the API (although entirely unintended and not exactly forseen) so I am guessing that @kennethreitz will be entirely in favor of finding a solution to return to the old behaviour.

This especially is a pain because @gazpachoking and I put a decent amount of effort into restoring cookie persistance and making it elegant. Perhaps he would be interested in tackling this? _Nudge, nudge_
",sigmavirus24,kennethreitz
1323,2013-04-25 07:07:45,"@sigmavirus24: Changes made. =)
",Lukasa,sigmavirus24
1321,2013-05-01 18:43:50,"@cdunklau is this fixed by your pull request?
",sigmavirus24,cdunklau
1321,2013-05-03 18:09:52,"I think #1343 + #1339 will make this pull obsolete to a degree. The other issue is that previously we were merging the arguments in a case insensitive way which was only correct for headers. This is better handled by @gazpachoking's PR.
",sigmavirus24,gazpachoking
1320,2013-04-24 14:41:08,"> This is because there is already a default adapter for 'http://' in the form of requests.adapters.HTTPAdapter. Depending on the (seemingly random) order of keys in the s.adapters dictionary, for some combinations of keys it will work, for others it won't.

**EDIT** None of the information in this comment is correct. There's nothing to see here except my embarrassment.

This has nothing to do with dictionary order. When we look an adapter we're looking for an adapter based on protocol, not hostname. We use `urlparse` to get the scheme (or protocol) and then we look for that in the adapters dictionary. With this in mind you get



And we do `self.adapters.get(uri.scheme)` I believe. You would have to monkey patch `get_adapter` to get the behaviour you want.

That's how we do it now. As for the docs, I have no clue why that example is there because it is just plain wrong. Setting up an adapter for that though would probably be convenient for quite a few people though. One concern I have, though, is that it is a change that sort of breaks the API despite being documented as working that way.

@Lukasa ideas?
",sigmavirus24,Lukasa
1320,2013-04-24 18:12:26,"I think we're totally over-engineering this. If we were going to do this properly we'd implement a trie and cause it to mimic the dictionary interface (not hard).

The reality is, we don't need to. We can assert that the number of transport adapters plugged into any session is likely to be small. If that's the case, we should just do:



This way we don't have to maintain a new data structure. Unless @kennethreitz wants to, of course, in which case I'll happily whip up a pure-Python trie. =)
",Lukasa,kennethreitz
1320,2013-04-25 06:58:08,"One of the valid use cases for adapters is unit testing. Tests should run as fast as possible, spending time sorting adapters in place every time is wasteful. I don't like the approach taken in #1323 because `get_adapter()` is called for every single request.

I'd like @kennethreitz to weigh in here whether he considers session.adapters a public API. For what it's worth this attribute is not listed in the ""Developers Interface"" section of the docs here: http://www.python-requests.org/en/latest/api/#request-sessions
",ambv,kennethreitz
1312,2013-04-14 18:30:16,"`HTTPResponse` is actualy from urllib3 in this case, not urllib2. The simple work around would be to check urllib3 for similar issues and if none exist (open or closed) open a new one to see if @shazow likes the idea of making the Responses usable via select. You might be better off (in the interim) looking into the Respones attributes for something that might allow you to use select. It won't be as elegant as doing `Response.raw` but it shouldn't be worse than `Response.raw.attribute`.
",sigmavirus24,shazow
1311,2013-04-13 16:34:19,"cc @Lukasa for code-review
",sigmavirus24,Lukasa
1308,2013-04-12 15:45:31,"@damiengermonville you might then need to petition over at shazow/urllib3 for an easier way to pass in the ciphers to the VerifiedHTTPSConnection object. We have no clue what your code looks like, but I imagine this might make it easier. It's also something I don't believe @shazow would be that adverse to, but I'm pretty sick at the moment so it's probable that I'm wrong. :)
",sigmavirus24,shazow
1298,2013-04-09 20:03:03,"A documentation Pull Request, from @Lukasa? How unexpected! :grin: 

There comes a point where I should stop writing blog articles about Requests-y things and start actually adding some documentation to Requests. This will add the Transport Adapter to the API documentation and a description of them to the Advanced docs.
",Lukasa,Lukasa
1297,2013-04-09 18:51:25,"Requests had logging before 1.0 and it was torn out during the refactor. Take from that what you will. I'm entirely indifferent on the proposal. It's up to @kennethreitz no matter what.
",sigmavirus24,kennethreitz
1294,2013-04-06 15:08:59,"So while we could introspect those exceptions, we really only re-wrap 
exceptions so that the user doesn't have to do:



Those errors are strictly rising out of the SSL library being used so their 
being re-wrapped as SSLErrors is perfectly valid. What we would rely on is 
urllib3 doing the introspection but I doubt @shazow would want that there 
either. So that you can consider a ""Won't fix"" item.
",sigmavirus24,shazow
1294,2013-06-30 03:32:09,"@kennethreitz @Lukasa @GP89 is there any interest in this issue any longer? I'm not keen on the solution I had started to work out and the issue wasn't really ours in the first place as can be seen by the discussion on e4e7eb8
",sigmavirus24,kennethreitz
1294,2013-06-30 03:32:09,"@kennethreitz @Lukasa @GP89 is there any interest in this issue any longer? I'm not keen on the solution I had started to work out and the issue wasn't really ours in the first place as can be seen by the discussion on e4e7eb8
",sigmavirus24,Lukasa
1289,2013-04-04 19:00:40,"@t-8ch can explain this in detail but upgrading to 1.2.0 should fix all of your problems.
",sigmavirus24,t-8ch
1289,2013-12-28 11:30:07,"Riiiight, now that makes sense.

This is an unforeseen problem to do with how exception tracebacks are being reported in Python 3. [PEP 3134](http://www.python.org/dev/peps/pep-3134/) introduced this 'chaining exceptions' reporting that you can see in @bodiam's traceback. The purpose of this error reporting is to highlight that some exceptions occur in `except` blocks, and to work out what chain of exceptions was hit. This is potentially very useful: for instance, you can hit an exception after destroying a resource and then attempt to use that resource in the `except` block, which hits another exception. It's helpful to be able to see both exceptions at once.

The key is that the `TypeError` raised as the first exception is _unrelated_ to the subsequent ones. In fact, that's the standard control flow in `urllib3`. This means that the real exception that's being raised here is the `request.exceptions.ConnectionError` exception that wraps the `urllib3.exceptions.MaxRetryError` exception being raised in `urllib3`.

This is _not_ a Requests bug, it's just an ugly traceback introduced by Python 3. We can try to reduce the nastiness of it somewhat by refactoring the method in `urllib3` (though I don't believe it's possible: @shazow?), but that'll only remove the `TypeError` from the chain: the rest will stay in place.
",Lukasa,shazow
1287,2013-04-04 16:28:28,"@gazpachoking any insight? I'm going to take a look at this now
",sigmavirus24,gazpachoking
1287,2013-04-04 20:18:35,"So as I mentioned to @gazpachoking on IRC, using pdb, I seem to have traced this to an issue in cookielib which makes absolutely no sense. If this was working prior to 1.x there should be no reason for it to have stopped working.

For future adventurers though here are some relevant snips:


",sigmavirus24,gazpachoking
1286,2013-04-02 19:55:53,"Over the last few months there have been multiple issues raised about problems with string encodings. At best Requests is being inconsistent (#1250), and at worst perfectly valid combinations of input are failing (#1279).

I think it's time for Requests to enforce a very consistent behaviour when it comes to string encodings. We need to establish what format the data will be in when we pass it to urllib3, and where we're going to transform it. We need to be consistent so that when users inevitably encounter errors, we can make it very clear what they should have passed in.

@shazow has said that urllib3 wants the following input: bytes in the body, and native strings everywhere else. This defines our interface to urllib3.

To keep in sync with the above, we need to do the following things.
- On Python 2, any unicode strings anywhere on a `PreparedRequest` other than the body should be encoded as UTF-8. The body should also be encoded as UTF-8 if it is a unicode string (unlikely, but unless we can guarantee it won't be we should do the right thing).
- On Python 3, any bytestrings in a PreparedRequest should be decoded to unicode strings. Because we cannot make any intelligent guess, we will use Python's (stupid, stupid, _stupid_) locale-based decoding. This will almost-certainly throw exceptions if an incorrect encoding has been used, alerting the user.

Thoughts?
",Lukasa,shazow
1286,2013-04-03 19:45:10,"Ok, so I've taken a first-pass at this. Right now this doesn't do anything about coercing the `body` parameter to bytes, which it needs to.

I want lots of feedback on this. In particular, at some point @kennethreitz is going to need to step in and make an API decision. This change is a massive API change and affects the output of the PreparedRequest. I think it's worth it, but if @kennethreitz doesn't then I'm happy to stop work on this.
",Lukasa,kennethreitz
1267,2013-03-28 03:56:26,"@Lukasa if you have a chance, some code review would be great. Thanks
",sigmavirus24,Lukasa
1265,2013-04-08 16:48:41,"Thanks. I'll dig into this later this week but @t-8ch might have some insight on this that could wrap it up sooner.
",sigmavirus24,t-8ch
1265,2013-04-08 16:59:49,"Hm. I do recall there being an issue with versions of OpenSSL older than 1.0.0 but I don't recall if it was directly related to this exact message. Again @t-8ch would know more about it if he has the chance to take a look.
",sigmavirus24,t-8ch
1261,2013-03-25 14:53:08,"So I'm personally in favor of keeping the existing behaviour and trying to make it as clear as possible to users that the behaviour has been changed.

Also, I forgot to include @mkomitee in on this ticket.
",sigmavirus24,mkomitee
1255,2013-03-22 02:48:01,"**tl;dr** It can not be easily added as far as I know and likely won't be considered by Kenneth given we're in a feature freeze.

Something about the API you show in the second snippet is oddly attractive but not at all what is within the realm of possibility for requests. I think a generator is your best bet in this instance but without knowing exactly what you're doing (beyond this) I can't speak as to the need for threading (although frankly I'm unsure of how that would be necessary for a generator).

With respect to the feature freeze please see #1165. As for adding this, all we do in requests is prepare a request to be sent using urllib3. You may be able to do something like this using urllib3 but the initial API will not be as simple as ours. It's a great library though and I think it has more functionality than what requests exposes to be frank. @shazow could definitely confirm or deny the ability to do this there though.
",sigmavirus24,shazow
1252,2013-03-21 12:48:28,"So here's my first guess (but I'll obviously look into this more): I think that OAuth1 might be generating unicode and opening the file in binary form will cause this issue due to how httplib sends the message body. Also this may be related to #1250.

@Lukasa, opinions?
",sigmavirus24,Lukasa
1252,2013-03-27 18:58:09,"So, really, this is because oauthlib is converting all the headers to unicode objects. We're then concatenating these unicode objects with the bytes of the file. Python tries to implicitly decode the bytes into unicode using the locale default codec, and obviously fails.

We can do the 'easy' fix and have `requests-oauthlib` just encode the headers using Latin-1, but that defers the problem. Alternatively, we can do the 'right' thing and take control of header encoding ourselves. I don't know if Kenneth is up for that, though. @kennethreitz, thoughts?
",Lukasa,kennethreitz
1252,2013-03-27 21:55:35,"Cc @shazow
",t-8ch,shazow
1252,2013-03-28 15:10:49,"So this can not block 1.2.0 unless @kennethreitz really wants it to. 

Perhaps to satisfy @michaelhelmick and company we should add a notice to the release that we realize that this is broken and a fix is being worked on in shazow/urllib3
",sigmavirus24,kennethreitz
1252,2013-03-28 16:45:27,"@michaelhelmick I hope you got better sleep than I did. :) And yes, as soon as this gets fixed, I would be certain to bug @kennethreitz about a bump to 1.2.1

And there's no need to apologize.
",sigmavirus24,kennethreitz
1242,2013-03-11 08:59:47,"I hate web server authors. :angry: 

This is a good patch. =) My only question is whether we need the overhead of a regular expression here? @sigmavirus24?
",Lukasa,sigmavirus24
1235,2013-03-10 00:31:38,"I still don't like requests performing this check either honestly. This is never going to affect the headers that we create and users can send whatever they like, especially by modifying a PreparedRequest object. That would also be the only place to add this check if @kennethreitz wants to add it, but I don't see a real need for it.
",sigmavirus24,kennethreitz
1233,2013-03-02 21:05:11,"/cc @sigmavirus24 @slingamn 

Anything pending for a new release? A ton of stuff has happened.

_Note_ I updated the title to reflect what is generally believed to be the correct version we should cut. --Ian
",kennethreitz,slingamn
1233,2013-03-02 21:05:11,"/cc @sigmavirus24 @slingamn 

Anything pending for a new release? A ton of stuff has happened.

_Note_ I updated the title to reflect what is generally believed to be the correct version we should cut. --Ian
",kennethreitz,sigmavirus24
1233,2013-03-03 02:44:05,"A damn lot has happened. Did you want multidicts ready for this? I'm still trying to think of a good way to handle the merging of keyword arguments. Also, I'm missing one error class from Flask, but my branch isn't really ready for a merge at all.

Also pinging the jet-lagged @Lukasa 
",sigmavirus24,Lukasa
1228,2013-03-02 04:14:47,"Maybe. Especially if you did it within the last week or two.

In that case, I'm going to assume that the SESSION ID cookie is set when you originally do a GET on the login page (unless you're setting it manually which it doesn't seem as if you are). A POST to it might not set it properly at the start. In other words, you might need that cookie set in the header when making the post, otherwise it will prevent you from logging in. However, if the above code works (not letting us handle the redirects) that would seem to disprove my idea.

Maybe @Lukasa has some insight into this. I'm 99% sure the logic for this is correct in `resolve_redirects`. :/
",sigmavirus24,Lukasa
1227,2013-03-01 23:58:04,"I'm +1 on this. It definitely seems worth adding to allow for extra information and it isn't so much a change in the API although @Lukasa and @kennethreitz may disagree.
",sigmavirus24,kennethreitz
1227,2013-03-01 23:58:04,"I'm +1 on this. It definitely seems worth adding to allow for extra information and it isn't so much a change in the API although @Lukasa and @kennethreitz may disagree.
",sigmavirus24,Lukasa
1227,2013-03-02 09:47:53,"I'm +0 on this. As you said @dmedvinsky, it's a minor refactor. I have no objection to it being in the library, and it's an internal API really, so not too problematic. I'm happy to go with whatever @kennethreitz feels is right here.
",Lukasa,kennethreitz
1222,2013-02-28 22:11:06,"Ah @t-8ch sorry for the confusion. I must have misread @Lukasa's response to #1221 last night (or had some rather wild dreams of what it said). Thanks for the correction.
",sigmavirus24,Lukasa
1220,2013-02-28 02:05:25,"Hmm. Actually, I think I'm -0.5 on reintroducing the old behaviour. It doesn't cost much and doesn't clutter the API much. We're in [feature freeze](http://docs.python-requests.org/en/latest/dev/todo/#feature-freeze), but if @kennethreitz decides to we might put this back. I'm leaving this for him. =)
",Lukasa,kennethreitz
1219,2013-02-27 23:04:22,"There is nothing wrong with this PR, but it won't be accepted. Requests is in [feature freeze](http://docs.python-requests.org/en/latest/dev/todo/#feature-freeze), and that goes doubly for changes that affect the API. As @sigmavirus24 pointed out in #1208, there are plenty of ways to specify this value.
",Lukasa,sigmavirus24
1218,2013-02-26 17:13:07,"I believe this is a duplicate and related to shazow/urllib3#139

@Lukasa thoughts?
",sigmavirus24,Lukasa
1218,2013-02-26 17:35:29,"No worries. We're just waiting for that pull request to land before we can support this. I believe @t-8ch has made the case that you can use `http://` for your proxy and it should work, but I've no practical knowledge of how proxies work in all candor.
",sigmavirus24,t-8ch
1215,2013-02-24 20:48:44,"I didn't like that pull request anyway. Like we said on that issue, the user should go way out of their way to break things. @kennethreitz do you mind if I revert and push directly to requests? It's really not much of a difficult fix.
",sigmavirus24,kennethreitz
1201,2013-02-19 15:34:49,"@kennethreitz this seems sensible to me. I won't send a Pull Request though until you say so.
",sigmavirus24,kennethreitz
1198,2014-02-09 09:25:32,"Whether or not the fix belongs in urllib3 is totally down to @shazow. Given that urllib3 by default _does_ retry (3 times IIRC), it may be that he wants to keep urllib3's behaviour as is. Pinging him to get his input.

We vendor urllib3 to avoid some dependency issues. Essentially, it means we're always operating against a known version of urllib3. This has been discussed at excruciating length in #1384 and #1812 if you want the gritty details.
",Lukasa,shazow
1198,2014-02-09 13:50:08,"Phew gritty but informative. @shazow these are just a few thoughts I had -- raising a RequestError rather than MaxRetryError as above. Really I think I better understand the MaxRetryError after checking out [urlopen](https://github.com/shazow/urllib3/blob/951ea12ba18103e5434e751246c0895e54fef211/urllib3/connectionpool.py#L382).

Double edit: Really even just a kwarg so one can `raise MaxRetryError(retries=0)` and alter the message on `retries==0`.
",ksnavely,shazow
1198,2014-10-05 17:37:27,"@kevinburke thoughts?
",sigmavirus24,kevinburke
1198,2014-10-05 19:47:08,"Need a little more time

## 

Kevin Burke
phone: 925.271.7005 | twentymilliseconds.com

On Sun, Oct 5, 2014 at 10:37 AM, Ian Cordasco notifications@github.com
wrote:

> @kevinburke https://github.com/kevinburke thoughts?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/kennethreitz/requests/issues/1198#issuecomment-57945403
> .
",kevinburke,kevinburke
1193,2013-02-14 04:23:44,"In #1188 @brandon-rhodes pointed out that we throw some uninformative exceptions when problems arise in urllib3. This would catch the exception and wrap it in a Requests exception, then rethrow it with its original traceback.

I'm on the fence about whether we should do one of the following:
1. Create a new Requests exception for this case (`requests.exceptions.TransportError`?)
2. Import urllib3's exceptions into `requests.exceptions` and export them as our own? (Not great)

Opinions welcome (@sigmavirus24?)
",Lukasa,sigmavirus24
1193,2013-02-17 22:55:30,"So it turns out that there is no simple way to get around this Syntax Error. `six` has a good solution, but we currently don't vendor it in. The way I see it, we have three options:
1. Use the version of `six.py` in urllib3. This is bad: `six.py` is an implementation detail in urllib3, which is itself an implementation detail of Requests. -1
2. Add the bit of `six.py` that we need to our `compat.py`. This has a minimal effect on our code, but is a bit weird. +0
3. Vendor in a copy of `six.py`. This is the cleanest, but adds a whole new Python file from which we only need one line. Probably a small perf hit too. +0

@kennethreitz, @sigmavirus24: I don't know which is best from options 2 and 3. Thoughts?
",Lukasa,kennethreitz
1192,2013-02-14 03:57:17,"Exactly my view. =) I thought I'd open this PR so we can see what it entails, but I think we'll probably not fix what isn't broken. Just want to see what @kennethreitz thinks.
",Lukasa,kennethreitz
1191,2013-02-14 03:34:10,"Fixes #1189 as confirmed by @Lukasa
",sigmavirus24,Lukasa
1189,2013-02-14 00:11:57,"/cc @sigmavirus24 
",Lukasa,sigmavirus24
1188,2013-02-14 04:34:07,"@shazow, we need your help!
",kennethreitz,shazow
1188,2013-02-15 17:36:31,"Sorry I'm off the grid until Tuesday.
- Sent from rogue wilderness wifi
  On Feb 13, 2013 8:34 PM, ""Kenneth Reitz"" notifications@github.com wrote:

> @shazow https://github.com/shazow, we need your help!
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/issues/1188#issuecomment-13532802.
",shazow,shazow
1185,2013-02-13 04:05:15,"Fixes #1183 

/cc @mkomitee
",sigmavirus24,mkomitee
1183,2013-02-12 18:41:56,"Ah, I see what you mean. We resolve redirects before dispatching hooks. If that came before the redirect resolution, that would mean you could catch the redirect, correct?

Semantically I think it should be there anyway. If a redirect is resolved, every request except the first has the hook called on it (because each subsequent request calls `send` with `allow_redirects=False`). It should before the resolution unless @kennethreitz or @Lukasa disagree.
",sigmavirus24,kennethreitz
1182,2013-02-12 04:13:22,"@Miarevo I'm guessing it was either a change in our proxy handling or a change in urllib3's. Nice and specific eh? :-P Frankly though, @Lukasa has been more engaged in the proxy work lately so he might have an idea.
",sigmavirus24,Lukasa
1182,2013-02-13 16:30:36,"@t-8ch we do very little work with proxies. If I remember our code correctly, we just pass it to urllib3 and they handle all of that for us. I guess we _could_ validate it before passing it to urllib3 though. I'll defer to @kennethreitz and @Lukasa about whether we should be doing that or not though.
",sigmavirus24,kennethreitz
1181,2013-02-12 08:28:31,"I don't recall off the top of my head where I've seen this, but I believe latin1 (or something similar) is recommended for header encodings. @mitsuhiko do you know?
",kennethreitz,mitsuhiko
1180,2013-02-11 00:40:52,"Reported by Filefly on IRC and written with help from @gazpachoking.
",sigmavirus24,gazpachoking
1171,2013-02-07 20:04:09,"@shazow any clues?
",sigmavirus24,shazow
1163,2013-02-05 12:28:54,"I suspect it got pulled out during the big refactor. I'm not opposed to having it back, depends what @kennethreitz thinks.
",Lukasa,kennethreitz
1163,2013-02-05 17:21:38,"Hm, the docs didn't indicate that. The problem is that the entire Configuration API was deliberately removed in the refactor. As such, there isn't really a `safe_mode` either, just the in-between that always existed. A request is a request and any exception that occurs during that is raised. I doubt @kennethreitz would like to re-add this (given there is likely not a very elegant way of doing so without adding the Configuration API back), but I'll re-open it to be fair.
",sigmavirus24,kennethreitz
1155,2013-04-16 18:48:49,"Don't forget @Lukasa. He's done and is doing far more than me.
",sigmavirus24,Lukasa
1155,2013-04-16 18:50:39,"(+ @Lukasa!)
",ihodes,Lukasa
1151,2013-01-30 18:54:33,"Yeah, my intuition was correct:



@kennethreitz are you okay with req (or the Request object) being mandatory as a parameter, e.g., `Session.send`'s signature becomes: `def send(prepared_request, request, **kwargs):`?
",sigmavirus24,kennethreitz
1151,2013-01-31 14:51:39,"ping @kennethreitz, thoughts on changing `Session.send`'s signature?
",sigmavirus24,kennethreitz
1148,2013-01-28 17:16:44,"per @kennethreitz's request
",sigmavirus24,kennethreitz
1138,2013-01-24 18:45:37,"@sigmavirus24 Your point about streaming responses is correct; I'm not sure that a simple 'end - start' timer is useful for streaming responses _anyway_, but it IS useful for everything else (which is what I need).

As far as whether it's better as a method or a property - I honestly don't care. It's such a small change that if it gets rewritten to be a single property instead of a method, hooray! All I want is the ability to _get_ this information, from requests, without having to patch it myself every time I use it. @kennethreitz didn't seem to be convinced that this functionality even _belongs_ in requests, so I'm hoping that even if this particular commit doesn't get accepted, the general idea is considered to be worthwhile and some kind of implementation makes its way in.
",clee,kennethreitz
1138,2013-01-25 04:09:10,"Btw, @kennethreitz, as we discussed on IRC, :+1:. :shipit: 
",sigmavirus24,kennethreitz
1125,2013-01-23 02:49:05,"@sigmavirus24, all good?
",kennethreitz,sigmavirus24
1125,2013-01-23 03:43:04,"Seems it to me. I'm convinced it does what he says.

Kenneth Reitz notifications@github.com wrote:

> @sigmavirus24, all good?
> 
> ---
> 
> Reply to this email directly or view it on GitHub:
> https://github.com/kennethreitz/requests/pull/1125#issuecomment-12579122
",sigmavirus24,sigmavirus24
1102,2013-01-15 11:30:17,"It seems likely that `requests/cacerts.pem` is affected by the recent compromise of TURKTRUST certificates. However, I'm not sure what exactly we need to do to fix this. Removing both of the root certs we currently have in there is probably wrong.

Maybe @saschpe can advise?
",slingamn,saschpe
1099,2013-01-23 13:31:08,"@kennethreitz this is definitely a pull request you should merge. It complements #1117 which you already merged.
",sigmavirus24,kennethreitz
1092,2013-01-10 07:04:55,"@kennethreitz Are you closing/reopening in order to restart the Travis build?
",jianli,kennethreitz
1084,2013-01-26 17:48:24,"@cbare you're correct about that call to `request` not being complete.

So let me just walk through the steps of the request before submitting a pull request to fix that.

In a normal case (not chunked encoding), the user calls `requests.post(url, data={'key': 'value'}, files={'foo': open('foo', 'rb')})`. In this case, the `Request` object is created and prepared turning into a `PreparedRequest` which is what we receive as `req` in `resolve_redirects`. This is stored in `req.body`. Since this is prepared, we can do this (in `resolve_redirects`):



This works because when `data` receives a string, it sends that.

The problematic case is when the user is using chunked encoding (I think). The problem is, I'm not entirely sure what happens with chunked encoding at the moment. Maybe @kennethreitz can explain how that works because I haven't presently looked at it at all.

I could be wrong and it could all be handled as one case though.
",sigmavirus24,kennethreitz
1082,2013-01-03 19:23:39,"Related issues: #390, #400, #409, #421, #424.
#400 seems to have the most information.

@kennethreitz
Why was `test_unicode_headers` test (see #400)  and other valuable tests removed?
",piotr-dobrogost,kennethreitz
1082,2013-02-11 12:49:00,"@kennethreitz
This issue is still reproducible with current `requests==1.1.0` and `python==2.7.3`.

I have created a pull-request #1181 that fixes the issue, please check it.
",denis-ryzhkov,kennethreitz
1075,2012-12-31 01:08:30,"Personally, I'm -1 on adding `netparse` as a requirement. Not sure what @kennethreitz will think though.
",sigmavirus24,kennethreitz
1070,2012-12-26 02:26:13,"Yeah, I'm not sure if we need to re-pass the parameters. This is a good catch @alefnula. If you want to ready a commit that removes this, feel free to. I'm sure @Lukasa or @kennethreitz will weigh in after the holidays.
",sigmavirus24,kennethreitz
1070,2012-12-26 02:26:13,"Yeah, I'm not sure if we need to re-pass the parameters. This is a good catch @alefnula. If you want to ready a commit that removes this, feel free to. I'm sure @Lukasa or @kennethreitz will weigh in after the holidays.
",sigmavirus24,Lukasa
1065,2012-12-23 10:44:14,"Given the discussion on #1033, I removed the code support for OS certificate bundles entirely, and indicated where packagers can modify the code to support their environments.

Travis will fail, but that's just because it's red ever since #1064. The tests pass locally.

cc @sagarun @saschpe
",slingamn,saschpe
1053,2013-01-19 13:00:57,"Taking another look at this, I think this fix might belong in urllib3. The `urllib3.poolmanger.ProxyManager` class has a `_set_proxy_headers` method which should probably set the Host header. Does that sound right, @shazow? If it does, I can offer you a PR with the fix.
",Lukasa,shazow
1041,2012-12-19 03:32:25,"Pending shazow/urllib3#132 we can add that line I mentioned after calling `release_conn` and `r.raw.close()`. This should solve the issue completely.

The other thing is that `Response.close()` doesn't return anything since `release_conn` doesn't return anything. If we want, we can also stick `self.raw.close()` in there. Any objections @kennethreitz ?
",sigmavirus24,kennethreitz
1041,2012-12-19 03:52:38,"See the pull request I referenced. @shazow and I are discussing whether this 
behaviour should implicitly be part of the `release_conn` method.
",sigmavirus24,shazow
1035,2012-12-18 14:40:31,"Actually I may have spoken too soon. Looks like @kennethreitz just tagged v1.0.3 which means it is probably on PyPI. Try doing `pip install -U requests` and let us know if this is resolved (which it should be).
",sigmavirus24,kennethreitz
1025,2012-12-17 22:26:15,"See @kennethreitz's comment here: https://github.com/kennethreitz/requests/issues/1008
",sigmavirus24,kennethreitz
1017,2012-12-17 17:58:08,"Yup, that looks wrong.

@kennethreitz, I haven't ramped up on the adaptors stuff yet, but it feels like [line 130 of adapters.py](https://github.com/kennethreitz/requests/blob/master/requests/adapters.py#L130) should read `conn = ProxyManager.(self.poolmanager.proxy_from_url(proxy))`. You'd need to import `ProxyManager` too.

If I get up to speed tonight I'll branch and raise a PR, but I thought I'd put this here as I might not.
",Lukasa,kennethreitz
1008,2013-01-18 15:30:09,"I broke it while enabling SNI in urllib3.
It will be fixed by shazow/urllib3#130 (ping @shazow)
Try removing the `ssl_wrap_socket()` function from `requests.packages.urllib3.util.py`
",t-8ch,shazow
1008,2013-01-28 01:50:56,"Per @sigmavirus24, I'm watching this as a possible root cause of a similar-looking problem in Gittip, hitting Twitter: https://github.com/zetaweb/www.gittip.com/issues/531.
",whit537,sigmavirus24
1001,2012-12-12 17:05:10,"@kennethreitz,

If I'm not mistaken, the Travis CI builds were failing before this pull request. Correct?
",matthewlmcclure,kennethreitz
997,2012-12-12 12:51:17,"Yes, this is definitely a bug. =) Good spot, and excellent detail in your report, thanks so much!

I think Requests' documentation is documenting the behaviour we want. We can get this by adding a function call in line 558 (or some inline code) that puts the scheme into the proxy URL if it's missing. Thus, to carry on the example from [here](http://docs.python-requests.org/en/latest/user/advanced/#proxies), we'd change `""10.10.1.10:3128""` to `""http://10.10.1.10:3128""` and `""10.10.1.10:1080""` into `""https://10.10.1.10:1080""`.

The only worry here is that people might have code that passes their HTTPS traffic through an HTTP proxy, which this would break (unless the expressly indicate the scheme in their proxy URL). @kennethreitz, is this a problem?
",Lukasa,kennethreitz
995,2012-12-11 16:24:07,"We could expose this in the refactor as a connection parameter.

@shazow, what would need to change in urllib3?
",kennethreitz,shazow
991,2012-12-06 16:51:13,"Per @kennethreitz's implied intent in 5c1bc201c46297d39d591e5d30e0e65a989bd22c, this change implements the Apache 2.0 license across requests, in every location I could find a license specified.
",lyndsysimon,kennethreitz
989,2012-12-06 20:52:20,"Change your line:



to:



You'll find this works. =)

@sigmavirus24: IIRC, you worked on the streaming stuff last. Why is the default iter_lines chunk size so large (10240 bytes)? Is there a design decision I don't know about there?
",Lukasa,sigmavirus24
989,2012-12-06 21:52:26,"Also @gdamjan it was already decided if I remember correctly so you can close this if you feel your needs were met.

At this point, with the refactor coming up we could change it to half the current size since we can really announce the breaking changes then. But that still wouldn't fix his problem. To try to phrase this how @kennethreitz will see it, 90% of people's cases will be sufficiently met by this default, and probably 10% will be affected negatively. He likes to ignore that 10% if possible. (Paraphrasing from one of his talks.)
",sigmavirus24,kennethreitz
988,2012-12-05 16:58:31,"Not sure @kennethreitz will approve of the extra parameter. At best you might be able to add that as a configuration parameter. Even so, as I already said, the majority of the trace work only makes sense if it is performed in urllib3. If it doesn't exist there, you'll be hard pressed to see it introduced in requests.
",sigmavirus24,kennethreitz
980,2012-12-01 14:39:18,"I renamed my Github account from @gwrtheyrn to @dbrgn.
",dbrgn,dbrgn
973,2012-11-28 16:39:09,"poke @kennethreitz 
",sigmavirus24,kennethreitz
967,2012-11-27 18:36:24,"@idan?
",kennethreitz,idan
964,2012-11-27 11:27:21,"Good point about its location. Clearly I'm so familiar with the documentation I just assume everyone does what I do and skip straight to Quickstart.

Regarding the tone, I think we have a legitimate difference of opinion here. My reading of that section is that the tone isn't pontifical but informative. It does not say ""Do not use the GPL"", it says ""Don't use the GPL _without thinking_"", which is very different.

That said, I'm happy for @kennethreitz to weigh in here if he feels differently.
",Lukasa,kennethreitz
962,2012-11-26 19:04:46,"@kennethreitz, before you merge this, can we get a consensus on whether we officially support 3.1 and 3.3? If we do, it might be worth getting TravisCI to test on them so we don't regress.
",Lukasa,kennethreitz
962,2012-11-26 20:24:54,"@Lukasa I should have added that I'm +10 on waiting for @kennethreitz to weigh in. ;)
",sigmavirus24,kennethreitz
960,2012-11-25 20:41:08,"@kennethreitz 

How about creating new project requests-hooks with various hooks for requests? I could maintain it if you want.
",piotr-dobrogost,kennethreitz
960,2012-11-25 23:39:31,"@piotr-dobrogost both ;)

Also, if a separate repository is not acceptable to @kennethreitz, my next choice would be to add a sort of ""Recipes"" section to the docs which could include hook recipes.
",sigmavirus24,kennethreitz
956,2012-11-24 21:44:17,"This is an attempt to solve issue #910 (and hopefully #896 by association).

This solution is unlikely to cause a regression except in one specific case, where the form data _is_ urlencoded but doesn't have the correct Content-Type header set. If we think that's a likely scenario, I can go back and make the test for that case less restrictive. The flip side is that we will not mistakenly include data we shouldn't.

There aren't any tests for OAuth functionality in Requests, so I have no idea if this makes things worse or better. My local tests seem to suggest that everything is ok though, so let's assume it's all awesome. =D

NB: I'm aware that we've moved the OAuth stuff to requests/requests-oauth. I'll raise an equivalent PR over there if we decide to use this one, but there are a few open issues here that make raising this PR here useful as well.

@idan: Does this look right to you?
@michaelhelmick: Does my branch resolve your issues?
",Lukasa,idan
956,2012-11-26 08:31:02,"I'll merge once @idan approves :)
",kennethreitz,idan
953,2012-11-24 13:21:17,"I recall @kennethreitz first said he did not like the patch, then after a while he said he actually could accept it but then nothing more happened. If you compare our pull requests you notice that your change to `Session._send_request()` is not enough as this is not being called when `return_response` param is false (https://github.com/kennethreitz/requests/blob/v0.14.2/requests/sessions.py#L237) The change must be made on `Request.send()` level which always is called and that's what I did in my patch.
",piotr-dobrogost,kennethreitz
952,2012-11-23 10:54:15,"Currently refactoring the codebase for v1.0. Cleaning things up.

Plan:
- Replace the current `prefetch` parameter with `stream`. 
- Make all uploads stream.

Is there any downside to streaming all uploads? @mitsuhiko quick opinion?

---

Related: #895
",kennethreitz,mitsuhiko
951,2012-11-24 11:24:35,"@Lukasa @kennethreitz #939 solves these problems without any major disadvantage I can think of. Furthermore it has the slight advantage of letting the package maintainers easily add separate versions of any other package `requests` may depend on in the future without having to follow this same forking approach everytime. But this is just my humble opinion, of course :-)
",ghost,Lukasa
944,2012-11-18 12:15:12,"It's my understanding that the general intent is that `requests` will stop supporting Python 3.2 and move to supporting Python 3.3, in no small part because of this syntax.

@kennethreitz, do you want this to be fixed now, or should we just leave this until it fixes itself in 3.3 and so consider this a ""won't fix""?
",Lukasa,kennethreitz
944,2012-11-23 10:24:57,"Yeah, it'll be super simple. All the actual work is done by @idan and team :)
",kennethreitz,idan
929,2012-11-08 13:59:45,"Long short story, the associated commit aims at fixing #916 by refactoring the setup script and moving dependencies out of the package itself... These are now downloaded from PyPI whenever necessary before installation (via `install_requires`) while keeping compatibility with Python 2 and Python 3 in the same codebase (`chardet` is now resolved via two external requirements file).

The only downside of this is that one cannot import `requests` in `setup.py` (since that would require importing `charder`, which is not available at that time). Thus, the version is repeated both in `setup.py` and `requests/__init__.py`. I'll have a go at that once I find some more time, but for now this seems like acceptable for me.

**Note:** The package's version had been kept as `0.14.1` in the previous release. I decided to amend that to `0.14.2` instead of `0.14.3.dev0` or something like that, since maybe @kennethreitz wants to manage that himself. :-)
",ghost,kennethreitz
924,2012-11-25 02:00:46,"I'm unaware. @shazow?
",kennethreitz,shazow
910,2012-11-24 11:35:35,"I seem to remember this having come up before, though I can't find a good example of this issue.

@idan, you're way more familiar with OAuth than me. Is the principle supposed to be that OAuth doesn't include the body for signing purposes, or is it just specifically not supposed to sign the body for `multipart/form-data`?
",Lukasa,idan
907,2012-10-23 06:23:34,"See #889 for motivation.

My understanding is that this was added in order to fix tests, which were trying to POST `__file__` and failing on the second run, once `__file__` began to refer to a .pyc instead of a .py?

It looks like postbin/httpbin no longer choke on .pyc files, so we should be able to omit this.

cc @sigmavirus24
",slingamn,sigmavirus24
907,2012-10-23 13:55:15,"This wasn't exactly my decision/idea but yeah, I don't think the problem was solely with HTTPbin. I don't remember exactly since it was happening in July/early August, but I was seeing it with Amazon S3 if I remember correctly (I was using it via the GitHub Downloads API). But my memory is too hazy to be 100% certain. _shrug_ It's @kennethreitz's call anyway. I'm swamped with other stuff at the moment (I'm not even working on my own projects).
",sigmavirus24,kennethreitz
905,2012-11-26 18:05:26,"@machinae two things:

First, @kennethreitz is working on a rewrite using transport adapters which should alleviate or fix this issue.

Second, if you want the patch at shadow/urllib3 to get through so it can be used here, go and work on that issue. It's already known to be am issue that affects plenty of people but complaining won't help anyone. I'm sure @shazow would love help on that pull request but until he gets some, it isn't going anywhere fast.
",sigmavirus24,kennethreitz
905,2012-11-26 18:05:26,"@machinae two things:

First, @kennethreitz is working on a rewrite using transport adapters which should alleviate or fix this issue.

Second, if you want the patch at shadow/urllib3 to get through so it can be used here, go and work on that issue. It's already known to be am issue that affects plenty of people but complaining won't help anyone. I'm sure @shazow would love help on that pull request but until he gets some, it isn't going anywhere fast.
",sigmavirus24,shazow
900,2012-10-22 12:46:44,"I made one last change to correct an issue where you could have a crash if Python was built without SSL support.

Also, I've submitted all of these changes to the urllib3 project, and it looks like @shazow is interested in merging, so you should be good there.
",dandrzejewski,shazow
896,2013-01-25 16:11:27,"@slingamn I've been super busy, but I guess the fixes are currently in requests but we're waiting on requests_oauthlib 0.3.0 to be released? I think..
",michaelhelmick,slingamn
875,2012-11-27 01:04:51,"@Lukasa @shazow @kennethreitz opinions on this? It seems reasonable to me, but I'm not as familiar with urllib3 honestly.
",sigmavirus24,kennethreitz
875,2012-11-27 01:04:51,"@Lukasa @shazow @kennethreitz opinions on this? It seems reasonable to me, but I'm not as familiar with urllib3 honestly.
",sigmavirus24,shazow
875,2012-11-27 01:04:51,"@Lukasa @shazow @kennethreitz opinions on this? It seems reasonable to me, but I'm not as familiar with urllib3 honestly.
",sigmavirus24,Lukasa
868,2012-09-26 19:18:24,"https://github.com/sigmavirus24/requests/commit/4dd3d1a1a2534f2996a368ebe26114bf974e15f9#commitcomment-1904600

For reference, I'm going to copy the comments into here.

---

@piotr-dobrogost

Explicit type checking is code smell. We should be checking for iterability not types. See http://stackoverflow.com/questions/1952464/

---

@sigmavirus24

I was following the style of the rest of requests, if wanted I'll clean up this and the other places I saw it.

---

@piotr-dobrogost

@kennethreitz What do you think?

---

@kennethreitz 

iterability is always preferred, but not always possible in requests because of the way certain types work (strings, dicts)

---

@sigmavirus24 

@kennethreitz correct me if I'm wrong, but I believe you're against this. And if I remember correctly I had submitted an earlier PR that tried to do something similar to what @piotr-dobrogost is suggesting in `_encode_data` which failed when passed a dictionary. For reference in python 2 and 3, the following works



Which may not be entirely obvious to some and certainly might seem like unintended behavior. Likewise,



We could always do the complement though (which is far less obvious):



---

@piotr-dobrogost 

All we should care here is that we want iterable. It's up to the user to pass iterable he thinks is appropriate. If someone wants to pass ""random"" iterable then it's not our business to know better and reject it. Yes, there is more type checking in the Requests and I think it should be cleaned as well.

---

@piotr-dobrogost 

@shazow What's your opinion?

---

@shazow 

I would vote for making an is_foo helper somewhere nearby which contains the ""is iter and not str/dict"" logic or whatever we want it to be. Then re-use that where appropriate. Also name foo something descriptive. is_plural?

As for the logic, I'd be +0 on {iter and not str/dict}. I imagine people may want to pass in arbitrary generators and whatnot.

---

@sigmavirus24 

I think @piotr-dobrogost would just rather the logic `hasattr(foo, '__iter__')` rather than excluding `str`s and `dict`s.

---

@shazow 

In the context of hooks, shouldn't the check just be if it's callable? If not, then extend?

(My previous suggestion was more towards the general style of Requests, rather than this hook-specific example.)

---

@sigmavirus24 

Right, I was following the general style of requests when writing this. But yeah, you're right, if it's callable then it should be append otherwise we should try to extend it.

---

@shazow 

Further: Shouldn't the logic for register_hook and deregister_hook live in the same place as dispatch_hook? If that was the case, then the {check for `__call__` or extend} decision would be very evident. :)

---

@piotr-dobrogost 

Just in case I asked this http://stackoverflow.com/questions/12590494/.

> In the context of hooks, shouldn't the check just be if it's callable? If not, then extend?

Either we check all for callability or none. Why should the first one be better than the rest? :)

---

@sigmavirus24 

So you just want something like:



?

How about a compromise on:



Because calling it with None would cause some issues. If you would rather check that each element of the iterable is callable then simply:



That will take care of checking for callability and allow for lists, tuples, sets, etc to be passed.

---

@sigmavirus24 

@piotr-dobrogost, @shazow, @kennethreitz, does anyone mind if we take this to an issue instead of continuing to discuss everything here? An issue would make this more visible to future contributors/users/etc. of requests than it is here.

---

@piotr-dobrogost 

+1

> That will take care of checking for callability and allow for lists, tuples, sets, etc to be passed.

Not only this, this will allow for nested iterables (trees) to be passed as well! :)

---

Well that was painful, I'm all up for fixing this in particular and going through the rest of requests to see if we can get rid of some of these explicit checks unless anyone disagrees.
",sigmavirus24,shazow
868,2012-09-26 19:18:24,"https://github.com/sigmavirus24/requests/commit/4dd3d1a1a2534f2996a368ebe26114bf974e15f9#commitcomment-1904600

For reference, I'm going to copy the comments into here.

---

@piotr-dobrogost

Explicit type checking is code smell. We should be checking for iterability not types. See http://stackoverflow.com/questions/1952464/

---

@sigmavirus24

I was following the style of the rest of requests, if wanted I'll clean up this and the other places I saw it.

---

@piotr-dobrogost

@kennethreitz What do you think?

---

@kennethreitz 

iterability is always preferred, but not always possible in requests because of the way certain types work (strings, dicts)

---

@sigmavirus24 

@kennethreitz correct me if I'm wrong, but I believe you're against this. And if I remember correctly I had submitted an earlier PR that tried to do something similar to what @piotr-dobrogost is suggesting in `_encode_data` which failed when passed a dictionary. For reference in python 2 and 3, the following works



Which may not be entirely obvious to some and certainly might seem like unintended behavior. Likewise,



We could always do the complement though (which is far less obvious):



---

@piotr-dobrogost 

All we should care here is that we want iterable. It's up to the user to pass iterable he thinks is appropriate. If someone wants to pass ""random"" iterable then it's not our business to know better and reject it. Yes, there is more type checking in the Requests and I think it should be cleaned as well.

---

@piotr-dobrogost 

@shazow What's your opinion?

---

@shazow 

I would vote for making an is_foo helper somewhere nearby which contains the ""is iter and not str/dict"" logic or whatever we want it to be. Then re-use that where appropriate. Also name foo something descriptive. is_plural?

As for the logic, I'd be +0 on {iter and not str/dict}. I imagine people may want to pass in arbitrary generators and whatnot.

---

@sigmavirus24 

I think @piotr-dobrogost would just rather the logic `hasattr(foo, '__iter__')` rather than excluding `str`s and `dict`s.

---

@shazow 

In the context of hooks, shouldn't the check just be if it's callable? If not, then extend?

(My previous suggestion was more towards the general style of Requests, rather than this hook-specific example.)

---

@sigmavirus24 

Right, I was following the general style of requests when writing this. But yeah, you're right, if it's callable then it should be append otherwise we should try to extend it.

---

@shazow 

Further: Shouldn't the logic for register_hook and deregister_hook live in the same place as dispatch_hook? If that was the case, then the {check for `__call__` or extend} decision would be very evident. :)

---

@piotr-dobrogost 

Just in case I asked this http://stackoverflow.com/questions/12590494/.

> In the context of hooks, shouldn't the check just be if it's callable? If not, then extend?

Either we check all for callability or none. Why should the first one be better than the rest? :)

---

@sigmavirus24 

So you just want something like:



?

How about a compromise on:



Because calling it with None would cause some issues. If you would rather check that each element of the iterable is callable then simply:



That will take care of checking for callability and allow for lists, tuples, sets, etc to be passed.

---

@sigmavirus24 

@piotr-dobrogost, @shazow, @kennethreitz, does anyone mind if we take this to an issue instead of continuing to discuss everything here? An issue would make this more visible to future contributors/users/etc. of requests than it is here.

---

@piotr-dobrogost 

+1

> That will take care of checking for callability and allow for lists, tuples, sets, etc to be passed.

Not only this, this will allow for nested iterables (trees) to be passed as well! :)

---

Well that was painful, I'm all up for fixing this in particular and going through the rest of requests to see if we can get rid of some of these explicit checks unless anyone disagrees.
",sigmavirus24,kennethreitz
868,2012-09-26 19:18:24,"https://github.com/sigmavirus24/requests/commit/4dd3d1a1a2534f2996a368ebe26114bf974e15f9#commitcomment-1904600

For reference, I'm going to copy the comments into here.

---

@piotr-dobrogost

Explicit type checking is code smell. We should be checking for iterability not types. See http://stackoverflow.com/questions/1952464/

---

@sigmavirus24

I was following the style of the rest of requests, if wanted I'll clean up this and the other places I saw it.

---

@piotr-dobrogost

@kennethreitz What do you think?

---

@kennethreitz 

iterability is always preferred, but not always possible in requests because of the way certain types work (strings, dicts)

---

@sigmavirus24 

@kennethreitz correct me if I'm wrong, but I believe you're against this. And if I remember correctly I had submitted an earlier PR that tried to do something similar to what @piotr-dobrogost is suggesting in `_encode_data` which failed when passed a dictionary. For reference in python 2 and 3, the following works



Which may not be entirely obvious to some and certainly might seem like unintended behavior. Likewise,



We could always do the complement though (which is far less obvious):



---

@piotr-dobrogost 

All we should care here is that we want iterable. It's up to the user to pass iterable he thinks is appropriate. If someone wants to pass ""random"" iterable then it's not our business to know better and reject it. Yes, there is more type checking in the Requests and I think it should be cleaned as well.

---

@piotr-dobrogost 

@shazow What's your opinion?

---

@shazow 

I would vote for making an is_foo helper somewhere nearby which contains the ""is iter and not str/dict"" logic or whatever we want it to be. Then re-use that where appropriate. Also name foo something descriptive. is_plural?

As for the logic, I'd be +0 on {iter and not str/dict}. I imagine people may want to pass in arbitrary generators and whatnot.

---

@sigmavirus24 

I think @piotr-dobrogost would just rather the logic `hasattr(foo, '__iter__')` rather than excluding `str`s and `dict`s.

---

@shazow 

In the context of hooks, shouldn't the check just be if it's callable? If not, then extend?

(My previous suggestion was more towards the general style of Requests, rather than this hook-specific example.)

---

@sigmavirus24 

Right, I was following the general style of requests when writing this. But yeah, you're right, if it's callable then it should be append otherwise we should try to extend it.

---

@shazow 

Further: Shouldn't the logic for register_hook and deregister_hook live in the same place as dispatch_hook? If that was the case, then the {check for `__call__` or extend} decision would be very evident. :)

---

@piotr-dobrogost 

Just in case I asked this http://stackoverflow.com/questions/12590494/.

> In the context of hooks, shouldn't the check just be if it's callable? If not, then extend?

Either we check all for callability or none. Why should the first one be better than the rest? :)

---

@sigmavirus24 

So you just want something like:



?

How about a compromise on:



Because calling it with None would cause some issues. If you would rather check that each element of the iterable is callable then simply:



That will take care of checking for callability and allow for lists, tuples, sets, etc to be passed.

---

@sigmavirus24 

@piotr-dobrogost, @shazow, @kennethreitz, does anyone mind if we take this to an issue instead of continuing to discuss everything here? An issue would make this more visible to future contributors/users/etc. of requests than it is here.

---

@piotr-dobrogost 

+1

> That will take care of checking for callability and allow for lists, tuples, sets, etc to be passed.

Not only this, this will allow for nested iterables (trees) to be passed as well! :)

---

Well that was painful, I'm all up for fixing this in particular and going through the rest of requests to see if we can get rid of some of these explicit checks unless anyone disagrees.
",sigmavirus24,sigmavirus24
860,2012-09-18 22:00:20,"Pretty sure it is described in the docs but I'm glad to help. Care to close the issue for @kennethreitz? 
",sigmavirus24,kennethreitz
857,2013-01-26 19:54:24,"@jkl1337 we don't set the options on the sockets and I'm not sure if urllib3 does either (although if it does, it would probably be a good idea to submit the idea to @shazow). That aside, it would be ideal if we could use `select` but that would break Windows support.

We could definitely catch and re-throw the `socket.error`. That's easy. The problem it seems (to me) is that reading from a broken socket isn't raising any exception at all.
",sigmavirus24,shazow
851,2012-09-24 14:41:50,"Hmm, maybe this belongs in urllib3. @shazow ?
",kennethreitz,shazow
847,2012-09-10 13:49:51,"I'm pretty sure this support would have to be added to urllib3 (or if it's already there, the vendorized package would have to be updated). @shazow might be a better person to talk to about this. In the meantime, I'll see if it's part of urllib3.
",sigmavirus24,shazow
847,2012-09-10 15:48:13,"It could be done on the requests side since `file` scheme won't be doing any network-based work nor does it require any connection pooling. For those reasons I'm also uncomfortable adding it into urllib3. :/ What do you think, @kennethreitz?
",shazow,kennethreitz
844,2012-09-07 03:54:10,"This ticket is intended to aggregate previous discussion from #539, #589, and #597 about the default value of `chunk_size` used by `iter_content` and `iter_lines`.

cc @mponton @gwrtheyrn @shazow

Issues:
1. The default read size of `iter_content` is 1 byte; this is probably inefficient
2. Requests does not expose the ability to read chunked encoding streams in the ""correct"" way, i.e., using the provided octet counts to tell how much to read.
3. However, this would not be suitable as the default implementation of `iter_content` anyway; not all websites are standards-compliant and when this was tried it caused more problems than it solved.
4. The current default read size for `iter_lines` is 10kB. This is high enough that iteration over lines can be perceived as unresponsive --- no lines are returned until all 10kB have been read.
5. There is no ""correct"" way to implement `iter_lines` using blocking I/O, we just have to bite the bullet and take a guess as to how much data we should read.
6. There's apparently some nondeterminism in `iter_lines`, I think because of the edge case where a read ends between a `\r` and a `\n`.
7. `iter_lines` is backed by `iter_content`, which operates on raw byte strings and splits at byte boundaries. I think there may be edge cases where we could split the body in the middle of a multi-byte encoding of a Unicode character.

My guess at a solution:
1. Set the default `chunk_size` to 1024 bytes, for both `iter_content` and `iter_lines`.
2. Provide a separate interface (possibly `iter_chunks`) for iterating over chunks of pages that are known to correctly implement chunked encoding, e.g., Twitter's firehose APIs
3. We may need our own implementation of `splitlines` that is deterministic with respect to our chunking boundaries, i.e., remembers if the last-read character was `\r` and suppresses a subsequent `\n`. We may also need to build in Unicode awareness at this level, i.e., decode as much of the body as is valid, then save any leftover invalid bytes to be prepended to the next chunk.

Comments and thoughts are much appreciated. Thanks for your time!
",slingamn,shazow
837,2012-09-04 06:58:52,"Take #4.
Thanks @sigmavirus24
",alicebob,sigmavirus24
820,2012-08-29 19:31:04,"@idan?
",kennethreitz,idan
819,2012-08-28 05:39:39,"@idan @dgouldin @gulopine code review? :)
",kennethreitz,idan
819,2012-08-28 05:39:39,"@idan @dgouldin @gulopine code review? :)
",kennethreitz,gulopine
803,2012-08-21 14:44:19,"Part of this should actually be a PR on shazow/urllib3 and the other part would depend on @shazow accepting it unless I'm speaking out of turn.
",sigmavirus24,shazow
800,2012-08-24 15:06:28,"Anyone can handle this?
What the bot said is wrong. It is not merged. The bot gives a wrong commit.
cc @Lukasa @kennethreitz
",ayanamist,kennethreitz
800,2012-08-24 15:06:28,"Anyone can handle this?
What the bot said is wrong. It is not merged. The bot gives a wrong commit.
cc @Lukasa @kennethreitz
",ayanamist,Lukasa
799,2014-01-07 18:57:08,"I did not intend for that message to seem like I was chastising you, or to seem abrasive: I'm sorry.

It's worth noting that, as I mentioned, this comes up very frequently. This means that a lot of developer time is spent responding to duplicate issues and comments on previous issues. Any GitHub comment is responded to as if it is urgent, because they often are. This means a comment or GitHub issue is an instant cost of our time (as I'm sure you can appreciate from your own projects).

This is what I meant when I said there were ""lower-noise"" ways of raising this issue. For example, Stack Overflow would have been a suitable place to ask. Alternatively, emailing one of the triagers would have been just as appropriate: both @sigmavirus24 and I have our email addresses published very clearly.

My response was born in part from frustration at repeatedly having to follow up on issues opened or commented on that could have been resolved by more care. I'm sure you just made a mistake, and as I said, I'm sorry for not taking care with my language.
",Lukasa,sigmavirus24
799,2014-01-07 19:33:44,"I appreciate that, Cory. I was also very frustrated, as it seems like I've
been dealing with tiny, obstructive bugs from every dependency lately, and
I just spent a week rebuilding a new solution as an alternative to an
existing one that was broken and blocking me. Right when I was about to
check-off the last feature, I run into a mechanical issue with SSL, where
I'm a bit fuzzy when it comes to implementation on _ssl and urllib, and how
requests wraps one or both. I'd be more careful in the afternoon, but when
I just couldn't get it done before having to walk out, this morning, I got
hasty.

On Tue, Jan 7, 2014 at 1:57 PM, Cory Benfield notifications@github.comwrote:

> I did not intend for that message to seem like I was chastising you, or to
> seem abrasive: I'm sorry.
> 
> It's worth noting that, as I mentioned, this comes up very frequently.
> This means that a lot of developer time is spent responding to duplicate
> issues and comments on previous issues. Any GitHub comment is responded to
> as if it is urgent, because they often are. This means a comment or GitHub
> issue is an instant cost of our time (as I'm sure you can appreciate from
> your own projects).
> 
> This is what I meant when I said there were ""lower-noise"" ways of raising
> this issue. For example, Stack Overflow would have been a suitable place to
> ask. Alternatively, emailing one of the triagers would have been just as
> appropriate: both @sigmavirus24 https://github.com/sigmavirus24 and I
> have our email addresses published very clearly.
> 
> My response was born in part from frustration at repeatedly having to
> follow up on issues opened or commented on that could have been resolved by
> more care. I'm sure you just made a mistake, and as I said, I'm sorry for
> not taking care with my language.
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/kennethreitz/requests/pull/799#issuecomment-31767192
> .
",dsoprea,sigmavirus24
795,2012-08-19 19:11:20,"Ah ok, I'll just wait for word from @kennethreitz before issuing the pull request.
",sigmavirus24,kennethreitz
794,2012-08-19 10:31:15,"Ehh...standards are hard.

The comma (along with some other characters) is defined in [RFC 3986](http://pretty-rfc.herokuapp.com/RFC3986#reserved) as a reserved character. This means the comma has defined meaning at various parts in a URL, and if it is not being used in that context it needs to be percent-encoded.

That said, the [query parameter](http://pretty-rfc.herokuapp.com/RFC3986#query) doesn't give the comma any special syntax, so in query parameters we probably shouldn't be encoding it. That said, it's not entirely Requests' fault: the parameters are encoded using [`urllib.urlencode()`](http://docs.python.org/library/urllib#urllib.urlencode), which is what is percent-encoding the query parameters.

This isn't easy to fix though, because some web services use `,` and some use `%2C`, and neither is wrong. You might just have to handle this encoding yourself.

@kennethreitz, can you think of anyone who would have a better insight into this problem?
",Lukasa,kennethreitz
791,2012-08-18 20:31:57,"@kennethreitz @Lukasa should I start a new branch off of develop and cherry pick my commits on top of it to avoid the merges? Besides, one of the merges would not have been able to be automatic, so it would have required a merge commit anyway.
",sigmavirus24,kennethreitz
790,2012-08-18 20:17:20,"So, I know nothing about OAuth, but it looks like we just shouldn't be checking the Content Type header. The OAuth code appears to check for the presence of a file anyway, so it doesn't seem like it matters.

Thoughts @kennethreitz?
",Lukasa,kennethreitz
790,2012-08-20 06:33:45,"So nobody cares?
@kennethreitz has so many activities today.
",ayanamist,kennethreitz
790,2012-08-20 07:03:14,"I'm on the go, but very generally:

If you're passing in a dict, requests will render the dict as www-form-urlencoded in the HTTP body, and set the appropriate content type. These parameters are taken as input when building the base signing string (http://tools.ietf.org/html/rfc5849#section-3.4.1.3.1), but only if the content-type header is set to ""application/x-www-form-urlencoded"", otherwise the request body is ignored for the purpose of signing.

Since the act of passing a body dict payload in requests is a clear signal of your intention to have form-urlencoded body, the content-type is set automatically.

If the body is anything but a dict (or a manually-generated, valid x-www-form-urlencoded body), oauth should be ignoring the body for the purposes of signing and should not be setting a content-type (since you're doing something special).

That's how it should work. If it works any other way, then we need to look into it.  

On Monday, August 20, 2012 at 9:33 AM, 老A wrote:

> So nobody cares?
> @kennethreitz (https://github.com/kennethreitz) has so many activities today.  
> 
> —
> Reply to this email directly or view it on GitHub (https://github.com/kennethreitz/requests/issues/790#issuecomment-7861749).  
",idan,kennethreitz
783,2012-08-16 19:56:51,"Obviously, whether this gets pulled is up to @kennethreitz, but I would suspect that answer will be that it won't be.

Requests is not intended for formatting or displaying data, but for obtaining it. Even the `Response.json` method only really exists as a courtesy, with any fault-tolerant JSON handling being done by the end user (see #691). Requests generally does not deal with the data beyond decoding the response and handing it back to the programmer.

As @kennethreitz [has said before](https://github.com/kennethreitz/requests/issues/769#issuecomment-7622419), Requests is an HTTP library, not anything else. This is **not** my decision, but I think it's likely that the answer will be that this pull request is not accepted. Don't take my word for it though, wait for Kenneth to look at it. I might be talking total rubbish. =)
",Lukasa,kennethreitz
775,2012-08-14 11:39:53,"Thanks for understanding. I didn't anticipate this problem when I changed the default, unfortunately.

@kennethreitz maybe `iter_content` and `iter_lines` should still work after the content is fetched, for example, `iter_content` could return an iterator containing a single item, the entire response body?
",slingamn,kennethreitz
759,2012-08-06 14:14:21,"This _is_ a `urllib3` issue. It's also a stupid Python version problem.

In Python 2, the correct code would read:



In Python 3, that's a NameError because the `long` type was removed. @shazow, does this fix look useful?

Whack this function definition into `util.py`:



Then just replace the test with:


",Lukasa,shazow
749,2013-05-03 22:30:30,"I actually prefer the former because the people who need it are the ones who know they'll need it and they're few enough that it won't cause too many complaints. However, if we're going to be consistent with the existing API, the latter would be the way to implement it. Currently, urllib3 (and requests) will provide the API to send HTTPS requests without the ssl module present but will fail in the case where the ssl module is not available. In other words, the API is there and you can use it but it just won't work and that is indicated by an exception.

As to re-opening this, that's up to @kennethreitz. The issue of course (with all of this) is that we're currently under a feature freeze (#1165, #1168) so I'm not sure if the work is even worth doing because it might not be accepted.
",sigmavirus24,kennethreitz
749,2013-05-03 23:36:10,"Let me be a bit clearer: what I meant by the second alternative was ""Try to use SNI if the optional deps are available, but still use SSL as we always have it they aren't."". This shouldn't break anything existing up to now.

As for reopening (or not) the issue, IMHO, this issue shouldn't be taken lightly. requests can become useless without SNI support for many scenarios. In particular, multiple domains on a single IPv4 host without SNI become impossible. And additional IPv4 addresses are out of the questions for many users (due to their cost).

In any case, this is half feature, half bug, though I'll await @kennethreitz's reply regarding reopening the issue. 
",hobarrera,kennethreitz
746,2012-07-27 15:23:51,"Looks good to me. I'm uncomfortable merging pull requests on @kennethreitz 's behalf, so I'll let him do it.

That TravisCI failure is worrying me, because it seems to come and go, and not be related to the contents of a particular build. /pensiveface
",Lukasa,kennethreitz
746,2012-07-27 17:43:44,"Ah, good idea. =)

If they're 5xx's though, it's probably on httpbin.org's side (aka. it's @kennethreitz 's problem, not mine. =D ).
",Lukasa,kennethreitz
743,2012-07-27 06:02:49,"@idan @dgouldin @gulopine?
",kennethreitz,idan
743,2012-07-27 06:02:49,"@idan @dgouldin @gulopine?
",kennethreitz,gulopine
741,2012-07-26 23:47:05,"Way more than syntactic sugar, but yes, that's where the problem lies.

We have plans to fix this I believe, right @shazow?
",kennethreitz,shazow
738,2012-07-25 12:44:57,"These are all genuine Python 3 Syntax Errors that are in libraries included within `requests`. The errors in `chardet` don't affect functionality, as in Python 3 `chardet2` is imported instead, which does not have these problems. It would be nice if we could somehow exclude the module entirely in Python 3 though.

`oauthlib` is a separate problem. Looking at the code in `auth.py`, the Syntax Errors raised when importing `oauthlib` cause the import to fail, and that's handled by the except block [here](https://github.com/kennethreitz/requests/blob/develop/requests/auth.py#L22). In the Python 3 case, that except block allows you to use the other Auth methods in that file, but _will_ cause OAuth1 to fail. (For detail, it fails with a NameError because `Client` was not successfully imported.)

Interestingly, all of the Syntax Errors in `oauthlib` are caused by unicode literals, which means they will stop being errors in Python 3.3. It's @kennethreitz 's call, but I know that he's planning to drop Python 3.2 as a supported version sometime after the release of 3.3, when this problem will go away.
",Lukasa,kennethreitz
737,2012-07-24 22:21:30,"I've just started looking into this, and I can explain the behaviour of the second case. The list of tuples you provided to `data` has `dict()` called on it. Dictionaries (obviously) don't allow duplicate keys, but your list of tuples has duplicate keys, so the last item in the iterable takes the value for that key. Exactly the same behaviour is shown if you provide a list of tuples to `data` without the `files` argument, so I'd assume that this is intended behaviour.

I'm significantly less sure about your first case. The actual exception is occurring in urllib3, which does not expect to be passed a list. If I had to hazard a guess, I'd say that the fix should be applied to requests, not urllib3, because requests already has edge case code for lists in [`_encode_params()`](https://github.com/kennethreitz/requests/blob/develop/requests/models.py#L306). It's a fix I'm prepared to write, but I would want @kennethreitz to confirm that this is a bug, and not actually correct behaviour, first.
",Lukasa,kennethreitz
731,2012-07-18 13:40:00,"I checked it out on my machine and it looks good, to me, so you don't need to do anything else. @kennethreitz will pop by at some point when he has time and will handle merging into the development branch, but it'll merge nicely so it shouldn't be a big deal. If you want to, you could add your name to the Authors file, but that's really up to you. As for trunk, it should make it into the next minor version of requests.
",Lukasa,kennethreitz
729,2012-07-14 22:30:26,"Co-Authored By: Timnit Gebru tgebru@gmail.com @tgebru
Co-Authored By: Sarah Gonzalez smar.gonz@gmail.com @smargonz
Co-Authored By: Leila Muhtasib muhtasib@gmail.com @muhtasib

This is a fix for Issue #661.
",vickimo,muhtasib
728,2012-07-14 21:15:30,"This is a fix for issue #695. 

Additional co-authors:
@smargonz
@vickimo
@tgebru
",muhtasib,vickimo
727,2012-07-14 20:12:25,"Modified code to use the current fix versus the old fix, which was broken.

Co-Authored By: Timnit Gebru tgebru@gmail.com @tgebru
Co-Authored By: Sarah Gonzalez smar.gonz@gmail.com @smargonz
Co-Authored By: Leila Muhtasib muhtasib@gmail.com @muhtasib

This is a fix for issue #541 and #547.
",vickimo,muhtasib
722,2012-07-15 05:18:45,"After trying out a few alternatives, I talked with @kennethreitz briefly about this on IRC, and he came up with the obvious solution:

https://github.com/gulopine/requests/commit/cf24d37caf567c0e89122dc4943315c974f5b3b8

I think I screwed up my fork, so I don't know if I can submit a proper pull request, much less attach it to this ticket. That commit should illustrate the point, though.
",gulopine,kennethreitz
711,2012-07-11 11:25:55,"HAL is now an [Internet-Draft](http://tools.ietf.org/html/draft-kelly-json-hal-03). This is likely to become _the_ hypermedia guideline for expressing connections between resources (what the Link header does) with many more features. 

Also @mitsuhiko expressed his hate for embedding ""new things"" into headers. I think it might have something to do with cachability or intermediaries that mess with headers.

See more:
- [Backbone.HAL](https://github.com/mikekelly/backbone.hal)
- [frenetic](http://dlindahl.github.com/frenetic/)
",jokull,mitsuhiko
700,2012-06-29 04:16:38,"Going to implement conditional gets first, content will come at a later date.

/c @dstufft
",kennethreitz,dstufft
700,2012-06-29 04:16:39,"Going to implement conditional gets first, content will come at a later date.

/c @dstufft
",kennethreitz,dstufft
694,2012-06-22 12:27:55,"So, I have a theory. It's just a theory, and it'd be great if someone who knows more about HTTPS proxies could try to back me up here (@kennethreitz, any ideas?), but here goes.

[According to the Squid wiki](http://wiki.squid-cache.org/Features/HTTPS), SSL/TLS can be tunnelled through Squid using the HTTP CONNECT method or, in some tiny subset of cases, insert itself as a man-in-the-middle in your HTTPS connection. The issue with presenting itself as a man-in-the-middle is that it'll cause browsers to alert about mismatching certificates (and it's a little unethical if the proxy is controlled by someone who isn't you).

I'd be prepared to be £5 (maybe more) that your browser is using the CONNECT verb to handle the connection, and AFAIK `requests` doesn't do that. A quick reading of `urllib3.request.RequestMethods` shows that `urllib3` [doesn't handle the CONNECT verb](https://github.com/shazow/urllib3/blob/master/urllib3/request.py#L41), which suggests that if you want the functionality you'd need to submit a pull request to `urllib3`.

**tl;dr**: `requests` can't use the only HTTPS proxying method your setup of Squid allows. Probably.
",Lukasa,kennethreitz
692,2012-06-26 12:16:16,"Yes this would be very good, I'm currently bumping up against this, I'm wondering why git sub-modules are not used here

Note it looks like this was fixed in 8f0ac668da796b04df9901db58f5d4215be308dd 

But it's yet to be pushed to PyPi. @kennethreitz can you do this?
",graingert,kennethreitz
671,2012-09-22 10:11:24,"@kennethreitz Any comments why it was closed? I believe that failing tests have nothing to do with this pull request.
",piotr-dobrogost,kennethreitz
665,2012-06-07 23:54:19,"If this [comment is true](https://github.com/kennethreitz/requests/issues/239#issuecomment-6180202) and requests.async has been removed and is now GRequests, then we want to make sure that people are being pointed in the right place :). 

If this isn't the case then disregard this request.

Thank you for all your work @kennethreitz
",dalanmiller,kennethreitz
664,2012-06-10 20:14:25,"This is fixed in shazow/urllib3@1c22dec48bc57199d71b86476dbed8f027877824, feel free to pull @kennethreitz.

Also worth noting that this bug is partly due to the improperly encoded param. It does not manifest if we do this instead:


",shazow,kennethreitz
664,2012-06-19 13:20:52,"Thanks @shazow @kennethreitz!
",lsemel,kennethreitz
655,2012-06-06 06:35:08,"That's... interesting. @shazow ?
",kennethreitz,shazow
655,2012-06-15 18:31:38,"@shazow, any insight here?
",kennethreitz,shazow
632,2012-05-29 20:49:41,"Hi @kennethreitz,

Any update on this pull request? I understand if you are busy with other things.
Jus trying to ""bug the maintainer"" :) 

Also let me know if this requires any other change.

Thanks,
Arup  
",amalakar,kennethreitz
627,2012-11-26 22:11:01,"@vlcinsky in my impatience I installed cherrypy in a virtualenv and ran your script. If you instead return the response for the awspage function instead of calling assertions, I get 



What's interesting is that both the Signature query string and Authorization header can not both be specified. There-in lies your problem.

You can probably fix this with a hook that will remove the auth header on redirect. @kennethreitz @Lukasa @piotr-dobrogost would any of you be able to help him write the hook?
",sigmavirus24,kennethreitz
627,2012-11-26 22:11:01,"@vlcinsky in my impatience I installed cherrypy in a virtualenv and ran your script. If you instead return the response for the awspage function instead of calling assertions, I get 



What's interesting is that both the Signature query string and Authorization header can not both be specified. There-in lies your problem.

You can probably fix this with a hook that will remove the auth header on redirect. @kennethreitz @Lukasa @piotr-dobrogost would any of you be able to help him write the hook?
",sigmavirus24,Lukasa
627,2012-11-26 22:49:03,"Oh, this has been interesting. Turns out that hooks are _not_ passed to a new request on redirect, as you can see [here](https://github.com/kennethreitz/requests/blob/develop/requests/models.py#L294). In the meantime, I can hack around this. @kennethreitz, is this a bug?
",Lukasa,kennethreitz
627,2012-11-27 14:28:16,"I love it when people quote RFC's so thank you for doing your research and giving us the necessary information. The problem is your use case seems to be in the 10% and knowing what little I do about Kenneth's design philosophy means that your proposed solutions are likely to not be implemented. As you said, requests is technically correct in how it behaves since the specification is permissive in its language. 

Furthermore, I think treating this issue as the hooks issue is the better option to avoid creating more issues than needed. We're just waiting on @kennethreitz to affirm that the exclusion of hooks from the redirected requests is not intentional.
",sigmavirus24,kennethreitz
624,2015-06-14 16:43:13,"So now's the time when @Lukasa or @kennethreitz should add you (@csparpa) to the translators team. They also need to set up a repository for the translation and import it from your fork. I'll set up the stuff so that it appears on python-requests.org, once they've taken care of that. (I'm not an owner/admin on the @requests org so I can't do that stuff for you.)
",sigmavirus24,Lukasa
619,2012-05-19 20:36:43,"@kennethreitz: How do you feel about that last commit as a first draft of a solution to #482?
",Lukasa,kennethreitz
616,2012-05-15 23:59:16,"Please do not merge this! This is intended as a ""design review"" :-) It needs more comments and tests before it is production ready.

This implements explicit disposal of sockets in urllib3 and Requests that does not rely on instantaneous garbage collection via reference counts (e.g., for PyPy). It should fix #520 without removing the `Response.request` member, or otherwise breaking the API (or the reference cycle, depending on whether you're a glass-half-full or glass-half-empty person).

It's split into three commits: one is a test case, one adds explicit disposal, one defaults `prefetch=True` and changes the tests to match.

@shazow maybe you could take a look at the urllib3 changes?

Thanks everyone.
",slingamn,shazow
578,2012-05-18 04:15:19,"@kennethreitz Any opinion on the error handling that @dhagrow mentions? I'm working on a patch, but I don't really think I should make that decision.
",joshimhoff,kennethreitz
559,2012-05-06 23:31:58,"@kennethreitz want to merge this? Looks correct, is tested, Travis likes it, it closes #526.
",slingamn,kennethreitz
520,2012-03-31 05:31:36,"@shazow any thoughts?
",kennethreitz,shazow
503,2012-06-17 14:13:19,"@jups23 that's a good idea. @kennethreitz, what do you think about me removing all of the >>> prefixes from all code examples to allow easy copy/paste?
",kylerob,kennethreitz
478,2012-03-10 14:22:11,"Also @kennethreitz my apologies that this wasn't branched from the 'develop' branch, by the time I had realised it was too late. On the plus side, it seems master and develop were in sync at the time of forking.
",foxx,kennethreitz
478,2012-03-14 16:59:54,"@vly I can confirm that currently the code does not currently have proxy authentication support (however afaik, proxy authentication is not currently in the original code either).

It does still however work for remote authentication (i.e. using auth= to authenticate to the site).

@kennethreitz can you confirm if you would be happy to merge the code in its current state, with proxy authentication being done at a later date?
",foxx,kennethreitz
478,2012-03-15 13:28:08,"@vly Excellent stuff - it would be great to have proxy authentication support, however this patch was contributed as open source from work done for a clients specific requirement - which sadly doesn't require proxy authentication. I'm hoping @kennethreitz will accept the patch without, and either myself or someone else will add proxy authentication support at a later date.
",foxx,kennethreitz
478,2012-03-15 13:57:34,"@shazow any comments on this? I know there was another SOCKS implementation submitted recently, right?
",kennethreitz,shazow
478,2012-03-15 14:10:03,"@shazow Let me know if you would consider merging in my changes, and if so, then I will create a new fork of urllib3 with the necessary modifications (as the patch in its current state _might_ not cleanly apply to the urllib3 repo, or at the very least would only have the new features usable via undocumented attributes).
---edited---
",foxx,shazow
478,2012-03-15 14:24:19,"@shazow Or - you could accept the patch as 'undocumented features', then apply your own changes in the future to use them in a documented fashion (as they should apply cleanly - they just won't be accessibly without setting some undocumented attributes). Then once this has been done, Python Requests would be modified to use the documented approach.

This would mean the features will be accessible quicker, without being blocked on many other issues. I was thinking of just applying the patches myself to urllib3, but it would appear you have quite a few other issues that _might_ block this, as well as a design decision needed, which would probably take up too much time for me to do myself.
",foxx,shazow
475,2012-03-08 21:25:34,"Here's a potential fix for #436 - @RonnyPfannschmidt is this what you had in mind?
",umbrae,RonnyPfannschmidt
468,2012-03-14 15:07:29,"Other people actively involved in this are: @dgouldin, @pydanny, @ghickman, and @ib-lundgren. Dozens are passively involved. 

Here's a log of the stuff that went down at PyCon: https://github.com/pydanny/pycon2012-oauth-sprint/blob/master/index.rst

@ib-lundgren is a grad student that is hoping to do his thesis on OAuthlib, actually :)
",kennethreitz,ib-lundgren
429,2012-02-14 21:56:53,"See issue #369. Once changes made by @mgiuca are merged your problem should be fixed as well.
",piotr-dobrogost,mgiuca
429,2012-03-31 06:32:51,"@mgiuca we could really use your help with a new collaboration with @mitsuhiko to merge requests and werkzeug. Would you like to help?
",kennethreitz,mitsuhiko
429,2012-04-01 09:54:45,"> (...) a new collaboration with @mitsuhiko to merge requests and werkzeug. 

Was the intention of merging published somewhere?
",piotr-dobrogost,mitsuhiko
426,2012-02-15 00:15:15,"@kennethreitz Thanks for merging. I forgot to add my name to AUTHORS. Could you please do that (Matt Giuca)?
@foxx I don't think we are talking about the same issue. I'll respond on the talk page for Issue #429.
",mgiuca,kennethreitz
416,2012-02-08 16:50:10,"Because we're not dealing with multi-part uploads, if I understand correctly.

Also, @kennethreitz mentioned wanting to add generator support at the same time, so the 'data' kwarg doesn't care what you feed it, as long as you give it something. Maybe it's a file, maybe it's a huge string, it probably shouldn't care.
",gtaylor,kennethreitz
399,2012-01-30 17:13:52,"Perhaps sessions are leaving connections open when they should be killed. @shazow ?
",kennethreitz,shazow
386,2012-01-25 14:55:47,"@shazow have (_very_ lightly) been discussing the idea of merging our projects in the mid-future. If that comes to fruition, that will be the time to merge test suites.

In the meantime, I'm fine with requests ""trusting"" that `urllib3` is tested and reliable.
",kennethreitz,shazow
386,2012-01-25 15:25:36,"> @shazow have (very lightly) been discussing the idea of merging our projects in the mid-future

That's really interesting.

> In the meantime, I'm fine with requests ""trusting"" that urllib3 is tested and reliable.

I was not implying it's not the case. I think it would be easier to have one test platform used by both libs. Taking into account that requests already embeds urllib3 there's no reason not to use its test platform if it supports all features requests' tests need. This way we could start taking advantage of this right away and in addition there would be less work to do when merging projects :)
",piotr-dobrogost,shazow
378,2012-01-23 00:55:58,"@kennethreitz, what do you think? :)
",johtso,kennethreitz
362,2012-02-01 17:06:41,"@shazow, we need to send `CONNECT` first.
",kennethreitz,shazow
306,2011-12-13 16:19:18,"@piotr-dobrogost: urllib3 is, as always, not an issue. 

Probally worth noting to @shazow though.
",kennethreitz,shazow
295,2011-11-29 14:35:22,"Correct. This is currently a bit of a fundamental limitation of the current architecture, but is something that I'd like to revisit on the longterm roadmap. 

Requests no longer utilizes Poster internally. 

I believe @mitsuhiko has some thoughts on this.
",kennethreitz,mitsuhiko
295,2012-08-22 20:03:35,"The bulk of your changes are in `urllib3`, so we should probably see if @shazow is interested in your changes. (NB: the reason the tests pass is because requests doesn't test `urllib3`. =D )
",Lukasa,shazow
295,2012-08-22 21:08:59,"Yes and no. The encode_multipart_formdata() function is in urllib3, but the call to it is in requests in models.py (there's another call to it in urllib3, but that's unused by requests as far as i can see, as the body is already encoded by the time it gets that far). Requests is encoding the body before handing the request off to urllib3. It just happens to be reusing that function by importing it from urllib3. There's no reason they have to share the same copy of the function other than sanity. :)

Anyway, i'm just pointing out that, in the case that @shazow doesn't want these sorts of changes in urllib3, the patch can be changed to move that functionality local to requests. Btw, does anyone know the reason why requests doesn't just hand off the unencoded files to urllib3 but rather encodes them itself?

In any case, i'm sort of only posting this as a ""could we do something like this?"" suggestion rather than a pull request. There's a few flaws in my approach that even I noticed. I don't really have enough knowledge of the code and all the interactions to make this change. It's also almost certainly a performance regression for small files.
",zigmonty,shazow
295,2012-08-22 21:31:21,"I believe we'll have to do chunked encoding, yes.

As for httplib, @shazow would be a better person to ask ;)
",kennethreitz,shazow
293,2011-11-29 14:37:57,"This may be an unexpected side effect of @shazow's hated for the urlparse module :)
",kennethreitz,shazow
286,2011-11-24 20:53:49,"Format of cookies is defined in RFC 2109. In section 4.1 value part of cookie is defined as `token | quoted-string`. `token` is defined in section 2.2 of RFC 2068 as `1*<any CHAR except CTLs or tspecials>` and _tspecials_ are defined as `tspecials = ""("" | "")"" | ""<"" | "">"" | ""@"" | "","" | "";"" | "":"" | ""\"" | <""> | ""/"" | ""["" | ""]"" | ""?"" | ""="" | ""{"" | ""}"" | SP | HT`. According to this `:` is a special character and any cookie's value with this character should be quoted. The behavior of Requests which you observe is in line with the specification. It's worth noting that as every cookie's value can be quoted the server should accept any such cookie. Btw. `[` and `]` are special so they should be absent from `_LegalChars`'s value I guess but it's a question to @kennethreitz
",piotr-dobrogost,kennethreitz
276,2011-11-17 15:57:33,"@idangazit I can see you referenced this pull request in the main [OAUTH Feature request](https://github.com/kennethreitz/requests/issues/65).

After reading the full thread thoroughly I can see how @maraujop [already thought about doing a pull request and then decided better](https://github.com/kennethreitz/requests/issues/65#issuecomment-2257820) to move into a separate hook to avoid a potential raise in issues, bugs, etc, in requests until the OAUTH functionality gets stable enough to consider pulling it in.

I think your code is pretty cool as it brings the HEADER authentication into the mix (although less supported by third parties, it's definitely more standard-friendly and much compact to code!). But, like @maraujop and probably like @kennethreitz, I think your code would be better merged and tested within @maraujop's [requests-oauth](https://github.com/maraujop/requests-oauth) hook.

I'm sure he will be happy to add such an awesome feature to the hook!

Cheers,
",jjmaestro,kennethreitz
276,2011-11-18 15:48:30,"@idangazit sure, I can see how this is not OAuth but I read the main issue #275 (where you link your gist about OAuth) then read the Feature Request for OAuth in requests and saw your mention, clicked and kept reading...

Since everything ties together and since this is the place to discuss the potential foundations of OAuth I thought that it would be very interesting to discuss this with @maraujop and to also talk about your implementation of header-based Oauth.

@maraujop's hook is the de-facto OAuth support in requests (definitely the best available) and it's already pip-installable:

pip install requests-oauth

So what about merging this pull request and then putting your gist into a requests-oauth pull request? That way we get the foundations of a better auth support in requests and we get your nice header-based OAuth thus improving the best hook available. Later, you guys can test the shit out the hook and improve it to the point where @kennethreitz can consider if it makes sense to add it to the requests core :)

Win-Win for me!
",jjmaestro,kennethreitz
251,2011-11-11 22:50:37,"Includes [Verbose logging example does not work](https://github.com/dasevilla/requests/commit/2d8440ea98301131edd537c4f2010b749f005e3b#commitcomment-712251), from @dasevilla and should close [issue #250](https://github.com/kennethreitz/requests/pull/250)
",joequery,dasevilla
234,2011-11-01 01:52:18,"via @kennethreitz
",shanemcd,kennethreitz
202,2011-10-14 01:55:17,"@mitsuhiko
",kennethreitz,mitsuhiko
185,2011-10-08 04:35:06,"Oh, interesting. I didn't know about `requests.request`. That certainly mitigates my issue, and makes me agree with you even more.

I'm okay with closing this issue if @kennethreitz is.
",apetresc,kennethreitz
179,2012-07-31 15:41:36,"So I started with `_encode_files` but realized that I cannot accurately test those changes since we seem to be having problems with the tests failing as the repository is now (at least they fail on my machine and I know @Lukasa is aware of this). Once those stop failing, I'll get back to work on this.
",sigmavirus24,Lukasa
161,2011-11-15 23:34:59,"@kennethreitz I think this problem is still there, I'm getting a lot TooManyRedirects errors.

The fix by @jerem worked for me, but had to adapt it a little bit to the latest Requests version.
",michielgardner,jerem
153,2011-09-25 23:45:40,"Thanks, @mrtazz!
",kennethreitz,mrtazz
141,2011-09-19 19:44:02,"New proposal, as discussed with @robmadole:

All locally passed dictionaries will merge with the session-level dictionaries. If a value is set to None, that key will be removed before being passed to the request method.
",kennethreitz,robmadole
65,2011-12-28 09:22:38,"update: @idan is working on an oauth module that is library-agnostic. It'll be fully integrated into requests when it is complete.
",kennethreitz,idan
