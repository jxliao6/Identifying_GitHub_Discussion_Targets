issue_num,datetime,body,login,mention_login
2679,2017-03-23 14:27:44,"@redapple thanks for this, I was wondering the same. Is this documented somewhere?",eLRuLL,redapple
2679,2017-03-23 16:30:21,"@eLRuLL , not really. I found this mention on [Response.flags](https://doc.scrapy.org/en/latest/topics/request-response.html?#scrapy.http.Response.flags) but that's about it.",redapple,eLRuLL
2669,2017-03-20 15:02:56,@redapple relevant discussion: https://github.com/seomoz/reppy/pull/33,kmike,redapple
2668,2017-03-20 14:57:04,"@redapple Yes, I too thought of the same. :) 
But since there are just 2 requirements for building the docs, do you think adding a requirements.txt would be beneficial?",harshasrinivas,redapple
2668,2017-03-20 16:45:06,"Sure @redapple , adding right away.",harshasrinivas,redapple
2663,2017-03-22 14:49:36,"@kmike One of `parsel`'s member said:
>I don't see a clear generic solution that would fit Parsel, I think right now it's best to handle these cases in the caller code.

So I launched a pull request for `scrapy` project",karlind,kmike
2663,2017-03-23 00:51:58,@kmike I think so and that's exactly what I did before. But question was I couldn't pass through exist unit tests.,karlind,kmike
2661,2017-03-24 12:17:48,hey @redapple @harshasrinivas. Just tested it on Macos with Python 3.5 and confirmed that `make htmlview` doesn't open a browser window if you don't provide a URL scheme.,stummjr,redapple
2658,2017-03-23 20:30:02,"@redapple 
I didn't notice your last sentence until now. The download timeout doesn't kick in at all in my example, the spider seems to hang before that, usually within 3 minutes or so.",rjbks,redapple
2657,2017-03-16 23:01:30,@eLRuLL I really like your idea. I'll get started on it.,jorenham,eLRuLL
2656,2017-03-17 06:17:09,"Hey @kmike 
Thank you for your reply, I updated to scrapy master now, but why repeat the same link will always be downloaded, and the file name is very strange

the download link:
http://m.baidu.com/api?action=redirect&token=kpyysd&from=1014090y&type=app&dltype=new&refid=0605419344&tj=soft_10656291_1528757_root%E6%8E%88%E6%9D%83%E5%A4%A7%E5%B8%88&refp=action_search&blink=bd7d687474703a2f2f612e67646f776e2e62616964752e636f6d2f646174612f7769736567616d652f333930343133303861666538663265652f726f6f7473686f757175616e64617368695f393135302e61706b3f66726f6d3d6131313031cb58&crversion=1&apilevel=22&bdi_bear=WF&bdi_imei=lHqPpO57c84mSArMIpHYMg%253D%253D&bdi_loc=5YyX5Lqs5biC&bdi_uip=182.101.163.98&brand=Yulong&model=Coolpad+5367&os_version=5.1&pver=3&resolution=720_1280&sign=77036280D318066694AED0FAAFCDF6D5

![2](https://cloud.githubusercontent.com/assets/3350372/24031211/4a721f6e-0b1c-11e7-9a0b-4e6846de80c3.png)
![1](https://cloud.githubusercontent.com/assets/3350372/24031217/59656ddc-0b1c-11e7-8c83-bc2d6b2e438f.png)

",fengfangqian,kmike
2649,2017-03-20 18:09:10,"Thanks @pawelmhm, looks good to me!",kmike,pawelmhm
2647,2017-03-13 13:21:35,"what exactly do you mean @kmike 
",pawelmhm,kmike
2645,2017-03-13 09:41:14,"@kmike Sure, I totally see the benefits in the `open_spider` and `process_item` cases. I am just not sure what benefit we get from calling `B.close_spider` *before* `A.close_spider`. Isn't it more intuitive for all the functions to be serial? For example, a call order like,

    1. A.open_spider
    2. B.open_spider
    3. A.process_item
    4. B.process_item
    5. A.close_spider
    6. B.close_spider

I stumbled upon this in a usage where, my `*pipeline A*` posts to ES and `*pipeline B*` then works on the indices touched by `*pipeline A*`. Say, `*pipeline A*` stores a buffer of items, that is completely flushed on close spider, then if `*pipeline B*`'s close_spider is called before `*pipeline A*`'s close_spider, the datastore has not yet been updated with all the data and it appears like the overall spider pipeline is behaving like a bi-directional valve. Given that an ordering for pipelines exists, I think there should be a way to support both orderings at the least.

Can you mention a use-case where you would have to have pipelines structured such that B would *require* being closed before A?",debosmit,kmike
2644,2017-03-13 14:05:38,Looks good to me. Thanks @jorenham ,redapple,jorenham
2643,2017-03-14 10:58:56,@redapple I've added the test functions. Let me know whether it could be improvised as well. Thanks!,harshasrinivas,redapple
2643,2017-03-19 00:44:48,"@redapple , Just realized that the statement 

would cause trouble when `meta('max_retry_times')` is set to zero. In that case, Python considers it to be [`False`](https://docs.python.org/3/library/stdtypes.html#truth-value-testing) and sets `retry_times = self.max_retry_times` instead. So I've fixed it in the above commit.",harshasrinivas,redapple
2634,2017-03-09 11:48:08,Fixed by #2636. Thanks @jorenham!,kmike,jorenham
2633,2017-03-13 09:28:25,"Hey @jorenham,

There are no required methods in scrapy pipelines, middlewares and extensions, all methods are optional. So the tricky part is to figure out what should these base classes contain. We also need to make sure pipelines/middleware/extensions which are not subclasses of these base classes still work.

The current solution to documentation problem is to generate extension/middleware/pipeline templates, with all these methods - each new Scrapy project has them.

But I also find myself looking at docs each time I'm implementing a middleware or a pipeline, I feel the pain. Also, sometimes you have a middlewares.py file with a middleware and it is not immediately clear if it is a downloader or a spider middleware; inheriting from a certain class could help with readability.",kmike,jorenham
2633,2017-03-17 14:09:21,"Like @eLRuLL mentioned in #2657, it would be a nice feature to have the base classes of spider/downloader middleware and extensions self register to the settings with a priority defined in the subclass. 
I figured there are two ways of implementing:

1. Create a method in the base classes that registers itself to the default settings (e.g. `def register(self, priority)`. This method should in turn be called from the subclasses.
2. Metaclass hacking; register to the default settings once the base classes get subclassed with priority as an attribute of the subclasses.

What would be the best solution in your opion? @eLRuLL @kmike ",jorenham,kmike
2633,2017-03-17 21:46:33,"@jorenham , I would suggest to make this self registering in a separated issue/PR.",djunzu,jorenham
2632,2017-03-09 15:01:32,"@redapple The changes looks good to me, thanks Paul! cc @chekunkov",vshlapakov,redapple
2632,2017-03-09 18:01:39,"Thanks @redapple, @vshlapakov and @chekunkov!",kmike,redapple
2632,2017-03-09 18:01:39,"Thanks @redapple, @vshlapakov and @chekunkov!",kmike,chekunkov
2631,2017-03-10 02:12:45,@kmike What could be the reason behind the build failure?,harshasrinivas,kmike
2624,2017-03-08 07:48:19,"Hello @rolando , I tried it on oh-my-zsh and basic zsh as well. Opening double-quotes beforehand does not seem to help. (Tested it on both iTerm2 and Terminal)",harshasrinivas,rolando
2616,2017-03-16 10:31:36,"Looks good, thanks @redapple!",kmike,redapple
2616,2017-03-16 11:05:39,@redapple how does it fix #2449?,kmike,redapple
2616,2017-03-16 11:45:47,"@kmike , looks like I misread #2449. It has nothing to do with redirects.",redapple,kmike
2611,2017-03-03 12:49:52,@kmike So if I understand you correctly I should move the log in `DbmCacheStorage` and `LeveldbCacheStorage` from `__init__` to `open_spider` and include the `dbpath` in the message. Is that correct?,jorenham,kmike
2611,2017-03-03 14:24:41,"@jorenham yes, I think it makes sense. For consistency logging can be moved to open_spider method for FilesystemCacheStorage as well, so that they all log a message at the same time point.",kmike,jorenham
2611,2017-03-04 14:41:26,@kmike I've moved the logging and it now logs the entire paths when available,jorenham,kmike
2609,2017-03-19 11:55:37,"Hey @kmike! 👋

1. It works without defining and changes to `default_settings.py`. The underlaying `botocore` library has predefined default values,...
2. I've added documentation to `settings.rst` and `media-pipeline.rst`. Hope this is enough? 

Cheers!

- Oto",otobrglez,kmike
2605,2017-02-28 15:33:37,"Hi @redapple, ok if you wish. But would be nice to have a link here though, for someone who accesses this later. ",manisoftwartist,redapple
2604,2017-03-02 09:25:29,@jorenham sounds great!,kmike,jorenham
2602,2017-02-28 09:58:16,"hi @djunzu , indeed, ""Check SPIDER_MODULES setting"" is not always relevant (see https://github.com/scrapy/scrapy/pull/2433#issuecomment-279451719)",redapple,djunzu
2602,2017-02-28 20:15:06,"@redapple , [#2433 (comment)](https://github.com/scrapy/scrapy/pull/2433#issuecomment-279451719) is a slightly different problem. Removing message ""Check SPIDER_MODULES setting"" would not solve his problem. But I think removing it would still solve #2430 (one can clearly see that the problem is ""Could not load spiders from module 'crawler.spiders'"") and other errors (like a fail to import a loader in the spider itself) would not mislead to the wrong place. What do you think?

I mean, let everything as it is but remove just ""Check SPIDER_MODULES setting"" from the message.",djunzu,redapple
2598,2017-02-28 11:03:28,"I also cannot reproduce this:



[As @kmike commented earlier](https://github.com/scrapy/scrapy/pull/2598#issuecomment-282684243), there's something weird about this PR that is triggerring Travis CI builds for nothing (duplicate builds for the master branch).
So I'm closing this PR as it is.

@MalikRumi , if you can re-check the tutorial and clarify what is the issue you are facing, please open a new issue. We may be missing something here.",redapple,kmike
2598,2017-02-28 23:49:41,"Ok, sorry to be late getting back to you all. Yes, I will try again and see
what might be different or causing an issue for me and report back. BTW, I
am on Ubuntu 16.04, Python 3.5.2 and Scrapy 1.2.1 And I am not the only one
having similar issues.

http://stackoverflow.com/questions/42059084/scrapy-code-throws-typeerror-nonetype-object-is-not-iterable

On Tue, Feb 28, 2017 at 3:04 AM, Paul Tremberth <notifications@github.com>
wrote:

> I also cannot reproduce this:
>
> $ cat t.py
> import scrapy
>
>
> class QuotesSpider(scrapy.Spider):
>     name = ""quotes""
>     start_urls = [
>         'http://quotes.toscrape.com/page/1/',
>         'http://quotes.toscrape.com/page/2/',
>     ]
>
>     def parse(self, response):
>         for quote in response.css('div.quote'):
>             yield {
>                 'text': quote.css('span.text::text').extract_first(),
>                 'author': quote.css('small.author::text').extract_first(),
>                 'tags': quote.css('div.tags a.tag::text').extract(),
>             }
>
> $ scrapy runspider t.py
> 2017-02-28 12:00:46 [scrapy.utils.log] INFO: Scrapy 1.3.2 started (bot: scrapybot)
> 2017-02-28 12:00:46 [scrapy.utils.log] INFO: Overridden settings: {}
> 2017-02-28 12:00:46 [scrapy.middleware] INFO: Enabled extensions:
> ['scrapy.extensions.logstats.LogStats',
>  'scrapy.extensions.telnet.TelnetConsole',
>  'scrapy.extensions.corestats.CoreStats']
> 2017-02-28 12:00:46 [scrapy.middleware] INFO: Enabled downloader middlewares:
> ['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
>  'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
>  'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
>  'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
>  'scrapy.downloadermiddlewares.retry.RetryMiddleware',
>  'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
>  'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
>  'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
>  'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
>  'scrapy.downloadermiddlewares.stats.DownloaderStats']
> 2017-02-28 12:00:46 [scrapy.middleware] INFO: Enabled spider middlewares:
> ['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
>  'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
>  'scrapy.spidermiddlewares.referer.RefererMiddleware',
>  'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
>  'scrapy.spidermiddlewares.depth.DepthMiddleware']
> 2017-02-28 12:00:46 [scrapy.middleware] INFO: Enabled item pipelines:
> []
> 2017-02-28 12:00:46 [scrapy.core.engine] INFO: Spider opened
> 2017-02-28 12:00:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
> 2017-02-28 12:00:46 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
> 2017-02-28 12:00:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/2/> (referer: None)
> 2017-02-28 12:00:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/1/> (referer: None)
> 2017-02-28 12:00:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/2/>
> {'text': u""\u201cThis life is what you make it. No matter what, you're going to mess up sometimes, it's a universal truth. But the good part is you get to decide how you're going to mess it up. Girls will be your friends - they'll act like it anyway. But just remember, some come, some go. The ones that stay with you through everything - they're your true best friends. Don't let go of them. Also remember, sisters make the best friends in the world. As for lovers, well, they'll come and go too. And baby, I hate to say it, most of them - actually pretty much all of them are going to break your heart, but you can't give up because if you give up, you'll never find your soulmate. You'll never find that half who makes you whole and that goes for everything. Just because you fail once, doesn't mean you're gonna fail at everything. Keep trying, hold on, and always, always, always believe in yourself, because if you don't, then who will, sweetie? So keep your head high, keep your chin up, and most importantly, keep smiling, because life's a beautiful thing and there's so much to smile about.\u201d"", 'tags': [u'friends', u'heartbreak', u'inspirational', u'life', u'love', u'sisters'], 'author': u'Marilyn Monroe'}
> 2017-02-28 12:00:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/2/>
> {'text': u'\u201cIt takes a great deal of bravery to stand up to our enemies, but just as much to stand up to our friends.\u201d', 'tags': [u'courage', u'friends'], 'author': u'J.K. Rowling'}
> 2017-02-28 12:00:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/2/>
> {'text': u""\u201cIf you can't explain it to a six year old, you don't understand it yourself.\u201d"", 'tags': [u'simplicity', u'understand'], 'author': u'Albert Einstein'}
> 2017-02-28 12:00:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/2/>
> {'text': u""\u201cYou may not be her first, her last, or her only. She loved before she may love again. But if she loves you now, what else matters? She's not perfect\u2014you aren't either, and the two of you may never be perfect together but if she can make you laugh, cause you to think twice, and admit to being human and making mistakes, hold onto her and give her the most you can. She may not be thinking about you every second of the day, but she will give you a part of her that she knows you can break\u2014her heart. So don't hurt her, don't change her, don't analyze and don't expect more than she can give. Smile when she makes you happy, let her know when she makes you mad, and miss her when she's not there.\u201d"", 'tags': [u'love'], 'author': u'Bob Marley'}
> 2017-02-28 12:00:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/2/>
> {'text': u'\u201cI like nonsense, it wakes up the brain cells. Fantasy is a necessary ingredient in living.\u201d', 'tags': [u'fantasy'], 'author': u'Dr. Seuss'}
> 2017-02-28 12:00:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>
> {'text': u'\u201cThe world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.\u201d', 'tags': [u'change', u'deep-thoughts', u'thinking', u'world'], 'author': u'Albert Einstein'}
> 2017-02-28 12:00:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>
> {'text': u'\u201cIt is our choices, Harry, that show what we truly are, far more than our abilities.\u201d', 'tags': [u'abilities', u'choices'], 'author': u'J.K. Rowling'}
> 2017-02-28 12:00:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>
> {'text': u'\u201cThere are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.\u201d', 'tags': [u'inspirational', u'life', u'live', u'miracle', u'miracles'], 'author': u'Albert Einstein'}
> 2017-02-28 12:00:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/2/>
> {'text': u'\u201cI may not have gone where I intended to go, but I think I have ended up where I needed to be.\u201d', 'tags': [u'life', u'navigation'], 'author': u'Douglas Adams'}
> 2017-02-28 12:00:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/2/>
> {'text': u""\u201cThe opposite of love is not hate, it's indifference. The opposite of art is not ugliness, it's indifference. The opposite of faith is not heresy, it's indifference. And the opposite of life is not death, it's indifference.\u201d"", 'tags': [u'activism', u'apathy', u'hate', u'indifference', u'inspirational', u'love', u'opposite', u'philosophy'], 'author': u'Elie Wiesel'}
> 2017-02-28 12:00:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/2/>
> {'text': u'\u201cIt is not a lack of love, but a lack of friendship that makes unhappy marriages.\u201d', 'tags': [u'friendship', u'lack-of-friendship', u'lack-of-love', u'love', u'marriage', u'unhappy-marriage'], 'author': u'Friedrich Nietzsche'}
> 2017-02-28 12:00:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>
> {'text': u'\u201cThe person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.\u201d', 'tags': [u'aliteracy', u'books', u'classic', u'humor'], 'author': u'Jane Austen'}
> 2017-02-28 12:00:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>
> {'text': u""\u201cImperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.\u201d"", 'tags': [u'be-yourself', u'inspirational'], 'author': u'Marilyn Monroe'}
> 2017-02-28 12:00:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>
> {'text': u'\u201cTry not to become a man of success. Rather become a man of value.\u201d', 'tags': [u'adulthood', u'success', u'value'], 'author': u'Albert Einstein'}
> 2017-02-28 12:00:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>
> {'text': u'\u201cIt is better to be hated for what you are than to be loved for what you are not.\u201d', 'tags': [u'life', u'love'], 'author': u'Andr\xe9 Gide'}
> 2017-02-28 12:00:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>
> {'text': u""\u201cI have not failed. I've just found 10,000 ways that won't work.\u201d"", 'tags': [u'edison', u'failure', u'inspirational', u'paraphrased'], 'author': u'Thomas A. Edison'}
> 2017-02-28 12:00:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/2/>
> {'text': u'\u201cGood friends, good books, and a sleepy conscience: this is the ideal life.\u201d', 'tags': [u'books', u'contentment', u'friends', u'friendship', u'life'], 'author': u'Mark Twain'}
> 2017-02-28 12:00:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/2/>
> {'text': u'\u201cLife is what happens to us while we are making other plans.\u201d', 'tags': [u'fate', u'life', u'misattributed-john-lennon', u'planning', u'plans'], 'author': u'Allen Saunders'}
> 2017-02-28 12:00:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>
> {'text': u""\u201cA woman is like a tea bag; you never know how strong it is until it's in hot water.\u201d"", 'tags': [u'misattributed-eleanor-roosevelt'], 'author': u'Eleanor Roosevelt'}
> 2017-02-28 12:00:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>
> {'text': u'\u201cA day without sunshine is like, you know, night.\u201d', 'tags': [u'humor', u'obvious', u'simile'], 'author': u'Steve Martin'}
> 2017-02-28 12:00:46 [scrapy.core.engine] INFO: Closing spider (finished)
> 2017-02-28 12:00:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
> {'downloader/request_bytes': 448,
>  'downloader/request_count': 2,
>  'downloader/request_method_count/GET': 2,
>  'downloader/response_bytes': 5607,
>  'downloader/response_count': 2,
>  'downloader/response_status_count/200': 2,
>  'finish_reason': 'finished',
>  'finish_time': datetime.datetime(2017, 2, 28, 11, 0, 46, 743834),
>  'item_scraped_count': 20,
>  'log_count/DEBUG': 23,
>  'log_count/INFO': 7,
>  'response_received_count': 2,
>  'scheduler/dequeued': 2,
>  'scheduler/dequeued/memory': 2,
>  'scheduler/enqueued': 2,
>  'scheduler/enqueued/memory': 2,
>  'start_time': datetime.datetime(2017, 2, 28, 11, 0, 46, 260111)}
> 2017-02-28 12:00:46 [scrapy.core.engine] INFO: Spider closed (finished)
>
> As @kmike commented earlier
> <https://github.com/scrapy/scrapy/pull/2598#issuecomment-282684243>,
> there's something weird about this PR that is triggerring Travis CI builds
> for nothing (duplicate builds for the master branch).
> So I'm closing this PR as it is.
>
> @MalikRumi <https://github.com/MalikRumi> , if you can re-check the
> tutorial and clarify what is the issue you are facing, please open a new
> issue. We may be missing something here.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/scrapy/scrapy/pull/2598#issuecomment-283010161>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AFOIeg8lhFHNH1_ZOCpvjEg78VVqrlOmks5rg_83gaJpZM4MM5s4>
> .
>
",MalikRumi,kmike
2597,2017-03-08 18:03:54,@kmike Thank you,hanjihun,kmike
2596,2017-02-26 10:15:16,Thanks @arvindch !,lopuhin,arvindch
2594,2017-02-24 23:24:45,@rolando Thanks!  I'm experimenting with scrapy vs some custom Elixir code.  Will try yours out.,liveresume,rolando
2594,2017-02-27 20:20:19,"@kmike I don't think that Scrapy should require us to jump through such a big hoop just to get it working in Jupyter.  Which gives reason to why no work has been done on it.  I realize the blame might be more on Twisted, but the problem is solvable modifying only Scrapy.    

It's not a Jupyter problem, it's a Scrapy/Twisted one.

It seems too tightly integrated with Twisted.  But then again, it seems Scrapy was made more in a plug and play manner (just run a script), rather than with the goal of composing it with any program's pipeline and threading model.",liveresume,kmike
2593,2017-03-09 17:37:19,"@kmike found this related changes in webkit's changelog:

> Ver 2.7.3
>  - Use latin1 instead of UTF-8 for HTTP header values.
> ...
>
> Ver 1.11.1
> - Fix a crash in network backend with non-UTF8 HTTP header names.

I'm in favor of using `latin1` as default, but that may be a backwards incompatible change.",rolando,kmike
2593,2017-03-09 17:39:50,@kmike the fallback idea came from requests: https://github.com/scrapy/scrapy/issues/2592#issue-209880064,rolando,kmike
2590,2017-02-24 15:08:05,"Looks good @rolando , thanks.
The new setting needs to be documented.",redapple,rolando
2590,2017-02-28 16:49:41,Looks good. Thanks @rolando !,redapple,rolando
2590,2017-02-28 23:01:02,LGTM. Thanks @rolando!,kmike,rolando
2589,2017-02-23 14:16:56,"@redapple yes, as far I can tell. Here is the raw body: https://gist.github.com/rolando/45de1e7985cf78c753e362b6f1ece281",rolando,redapple
2588,2017-02-23 19:53:08,"@redapple I found out there is a [RFC](https://tools.ietf.org/html/rfc7230#section-3.2.4) that says:

>    Historically, HTTP has allowed field content with text in the
   ISO-8859-1 charset [ISO-8859-1], supporting other charsets only
   through use of [RFC2047] encoding.  In practice, most HTTP header
   field values use only a subset of the US-ASCII charset [USASCII].
   Newly defined header fields SHOULD limit their field values to
   US-ASCII octets.  A recipient SHOULD treat other octets in field
   content (obs-text) as opaque data.

So, by default headers should be decoded with `latin1` charset. But that's a backwards incompatible change (only in cases where headers are non-ascii / non-latin1 compatible).

The example of the issue decodes fine with `latin1`:


I've updated the PR to include a `encoding` parameter.",rolando,redapple
2588,2017-02-23 20:31:38,"@rolando yeah, latin1 is a better default, but as I recall e.g. Cookie header should be decoded from utf-8. I'm not wild about adding another argument which still doesn't solve the issue with encodings, but having this argument can be a bit more convenient indeed. Maybe encoding=None should have a meaning ""do as good as you can - e.g. utf-8 for a fixed set of known headers (with latin1 fallback), latin1 for other headers"". It is also not clear what is `encoding` constructor argument for.",kmike,rolando
2588,2017-02-23 20:49:48,@kmike I created this issue https://github.com/scrapy/scrapy/issues/2592 to follow the default encoding discussion.,rolando,kmike
2586,2017-02-23 18:22:05,"@redapple nice approach without requiring twisted patches. I made a PR #2590 based on your suggestion and it works well, although the tests were a bit tricky.",rolando,redapple
2582,2017-02-27 20:33:50,"@immerrr do you have an idea on how this RequestSet API could look like? 
It is also somewhat related to https://github.com/scrapy/scrapy/issues/1226, though in an indirect way.",kmike,immerrr
2582,2017-02-27 21:01:48,"@kmike, only a rough one.

I think of RequestSet as something that:
- has a `DeferredList`-like API (`asyncio.gather` has the drawback of being a function rather than an object)
- knows that it contains Requests as deferreds
- ~~has its own `callback` to be run when the request set is dealt with, probably an `errback` too, for symmetry~~ has a Deferred that would fire when the requestset is being cleaned up
- has its own `'meta'` dictionary, much like the one shared between Request & Response objects
- since it knows that it contains Requests, it can piggyback on the first received value and do `response.meta['request_set'] = self` so that the callbacks can access the shared data
- (maybe) it should silently copy fields from `request_set.meta` to `response.meta` if they are unset in `request.meta`, or maybe even make `request.meta` a ChainDict with fallback to `request_set.meta`
- it should wrap requests coming from its respective response callbacks unless specifically asked not to do that, e.g. with `request.meta['request_set'] = None`
- (maybe) it should be possible to return other RequestSet from response callbacks 
- (maybe) returned RequestSets should be made nestable, i.e. to keep the parent RequestSet alive during their lifetime if not explicitly asked not to with `request_set.meta['request_set'] = None` (if nesting is considered, the `request_set` metadata key seems redundant and we might consider `parent_set` instead.
- not sure if it's worth it to make them nestable, i.e. if a certain response callback produces a different RequestSet, should it be owned by the parent request set?

One more thing to consider is cross-referencing RequestSets, i.e. when two requests that should belong to one RequestSet are produced by different callbacks and thus have different scopes. Maybe a simple `WeakValueDictionary` would suffice to lookup the sets and ensure the references are cleaned up as necessary. But then you'd have the usual get-or-create operation, that might be worth creating an etalon implementation for.",immerrr,kmike
2582,2017-02-27 21:26:57,@immerrr could you please coopy-paste this to a new issue?,kmike,immerrr
2582,2017-02-27 21:35:23,@kmike done: #2600,immerrr,kmike
2581,2017-02-23 14:26:19,"@djunzu I'd expect it to still follow log level for this spider, at least if ``spider`` is in context. This is not always possible though because some messages are global (without ``spider`` in context). But if we document current behavior then maybe we'll documet a bug; I'm not sure. Anyways, it looks like a different issue.",kmike,djunzu
2581,2017-02-23 23:37:19,"@kmike I decided to test it just to know what happens. I tried

and

and

None of them works. It does not solve the problem and it is still not possible to set log level using `custom_settings` attribute. (This or I did something very wrong!)

tested with scrapy 1.3.2

@lopuhin Did you do any other test than the one you wrote?",djunzu,lopuhin
2581,2017-02-23 23:37:19,"@kmike I decided to test it just to know what happens. I tried

and

and

None of them works. It does not solve the problem and it is still not possible to set log level using `custom_settings` attribute. (This or I did something very wrong!)

tested with scrapy 1.3.2

@lopuhin Did you do any other test than the one you wrote?",djunzu,kmike
2581,2017-03-01 17:54:03,"@djunzu yes, I did check that this pull request solves the issue for me (specifically, I was using ``scrapy runspider`` command), and I checked that the test I wrote fails without this PR. Let me also check the ``scrapy crawl`` command.",lopuhin,djunzu
2581,2017-03-01 18:55:23,"@djunzu you are right, this is my mistake. Honestly, I'm not sure how did I test it and get what I expected. Thanks for checking! I'll see if I can fix it.
The test also needs updating: it just check log stats (which do respect log level in custom_settings), but not the actual log output.",lopuhin,djunzu
2581,2017-03-02 11:53:07,"The tests are green finally, yay! @kmike @djunzu @redapple do you mind having another look at this? I updated the PR description with summary of changes and also checked that ``scrapy crawl`` and ``scrapy runspider`` commands both respect custom log settings.",lopuhin,djunzu
2581,2017-03-02 11:53:07,"The tests are green finally, yay! @kmike @djunzu @redapple do you mind having another look at this? I updated the PR description with summary of changes and also checked that ``scrapy crawl`` and ``scrapy runspider`` commands both respect custom log settings.",lopuhin,kmike
2581,2017-03-07 13:09:59,"Looks good to me, thanks @lopuhin for the fix and @djunzu for the review. ",kmike,djunzu
2581,2017-03-07 13:09:59,"Looks good to me, thanks @lopuhin for the fix and @djunzu for the review. ",kmike,lopuhin
2581,2017-03-07 20:15:59,":+1: 


Back to my initial comment:
I think that docs should have this note: ""in case of several spiders, logging settings from the last spider take precedence"".

@kmike, regarding
> But if we document current behavior then maybe we'll documet a bug

I think documented bugs are good: everyone knows what to expect. Undocumented bugs are bad: when you are not expecting it, something blows up in front of you!

Nonetheless, +1 for merge.",djunzu,kmike
2580,2017-02-21 10:32:09,"> Could you try installing Scrapy in virtualenv (see https://doc.scrapy.org/en/latest/intro/install.html#using-a-virtual-environment-recommended)?

@kmike I have try to create a venv with `virtualenv -p /usr/bin/python3 even` then active it, then I install scrapy with pip, it remind me to install Twisted, so i did it.
This is my pip freeze output with the venv, as follow:

And I want to ensure it on python command-line in venv:

Nothing works.",Keleir,kmike
2580,2017-02-22 07:05:35,"@kmike Today I'm try to reinstall Twisted as the version `16.3.0`, and it works. I did it as following:

Add then the `scrapy -v` is enable:

I didn't know what is the differences between `Twisted-16.3.0` and `Twisted-15.2.1` :question: ",Keleir,kmike
2580,2017-02-22 10:38:46,"@kmike When I use pip to install `scrapy`, the output is:

So I install `twisted` with pip, the output is:

Due to this, I install `twisted` through a source tar package.",Keleir,kmike
2580,2017-03-01 02:22:15,"@rolando Thanks a lot ! `Conda` is useful, my pip works again after installing conda. And it's easy to install scrapy just by `conda install scrapy`. :+1: ",Keleir,rolando
2578,2017-02-21 10:02:12,"Thank you @kmike! I read that sentence, but I misinterpreted the information that it is *only* usable from spiders.

I was now able to close the spider from the `DownloaderMiddleware` using the mechanism from [` scrapy.extensions.closespider.CloseSpider`](https://github.com/scrapy/scrapy/blob/master/scrapy/extensions/closespider.py).

It still raises an error as no `Response` is returned, see [`scrapy.core.downloader.DownloaderMiddlewareManager`]( https://github.com/scrapy/scrapy/blob/master/scrapy/core/downloader/middleware.py#L54), but this doesn't matter:
![image](https://cloud.githubusercontent.com/assets/5816039/23159833/93f19206-f824-11e6-962e-6ca3c30b3f7e.png)

Would be nice if raising `CloseSpider` from middlewares would be supported in future versions though.
",philonor,kmike
2572,2017-02-17 14:46:49,"A good catch, thanks @ashkulz!",kmike,ashkulz
2568,2017-02-22 14:31:04,"@kmike I'm new to this, but in my opinion , a user would get confused if autocomplete shows both options. Then he'd have to look up in the docs.
Hence, my main purpose is that the user should have a smooth experience.",Parth-Vader,kmike
2568,2017-02-22 15:30:51,"@kmike Yeah, that's true. It's always hard to remove things once the project becomes so complex. :stuck_out_tongue: ",Parth-Vader,kmike
2568,2017-02-28 10:49:29,"@djunzu it makes sense, though we tend to keep deprecations for a long time if they don't hurt. From the developer point of view some of the deprecations affect productivity, and some don't. 

I think currently we're following this approach: it is OK to deprecate something in major release X and remove it in major release X+1 (given that they separated by at least half year or a year), but if deprecated path doesn't hurt it can be kept for much longer. ""Deprecated methods will be dropped after three minor releases"" can be more aggressive than the current policy, though more predicatable. I agree that it is good to have policy written down explicitly.",kmike,djunzu
2564,2017-02-14 16:11:17,"Looks good, thanks @elacuesta!",kmike,elacuesta
2563,2017-02-14 14:50:04,"@kmike I think I don't disallowed network access, when I install twisted independence, the error
",driftluo,kmike
2563,2017-02-15 08:51:06,"@Handsome2734 'Py version' displayed in pypi has nothing to do with supported Python versions; source 'Twisted-17.1.0.tar.bz2' file should work fine in Python 3.6. ""Categories"" are also optional - but yeah, it'd be nice for Twisted to update them if they support Python 3.6 (which they are likely support).

As for bz2 support - a good find @rolando! If this is the reason then it is easy to fix for Twisted - just provide .tar.gz source distributions, not only .tar.bz2.",kmike,rolando
2563,2017-02-15 12:37:57,"@rolando @Handsome2734 @kmike I'm sorry to thank you so late. I'm complie Python3.6 by myself , I will try to download bzip2-dev, and then re-compile Python3.6, thanks again ",driftluo,rolando
2563,2017-02-15 12:37:57,"@rolando @Handsome2734 @kmike I'm sorry to thank you so late. I'm complie Python3.6 by myself , I will try to download bzip2-dev, and then re-compile Python3.6, thanks again ",driftluo,kmike
2558,2017-02-13 17:29:59,"Looks good to me. Thanks @kmike!

Le 13 févr. 2017 4:08 PM, ""Mikhail Korobov"" <notifications@github.com> a
écrit :

> I've replaced try/except with explicit version checks and ported code to
> be compatible with Twisted 17+. This should fix #2555
> <https://github.com/scrapy/scrapy/issues/2555>.
>
> Also, we no longer support Ubuntu 12.04, so its tox environment is
> replaced with Ubuntu 14.04 environment (we still don't run it on Travis
> though).
> ------------------------------
> You can view, comment on, or merge this pull request online at:
>
>   https://github.com/scrapy/scrapy/pull/2558
> Commit Summary
>
>    - TST replace Ubuntu 12.04 tox environment with 14.04
>    - fixed tls in Twisted 17+
>
> File Changes
>
>    - *M* scrapy/core/downloader/tls.py
>    <https://github.com/scrapy/scrapy/pull/2558/files#diff-0> (28)
>    - *M* tox.ini <https://github.com/scrapy/scrapy/pull/2558/files#diff-1>
>    (12)
>
> Patch Links:
>
>    - https://github.com/scrapy/scrapy/pull/2558.patch
>    - https://github.com/scrapy/scrapy/pull/2558.diff
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/scrapy/scrapy/pull/2558>, or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AA2GGFZTOL41u-UgT5pBeK6n5APpmZGmks5rcHH1gaJpZM4L_UFR>
> .
>
",redapple,kmike
2556,2017-02-13 18:15:15,Superseded by https://github.com/scrapy/scrapy/pull/2558. Thanks @pawelmhm for the heads up!,kmike,pawelmhm
2555,2017-02-15 00:14:17,"@kmike Glad to help!

@redapple - Looking forward to hearing more about the use-case :).",glyph,kmike
2552,2017-02-09 11:37:28,"aha but it seems like there's something in this specific invalid url I've got there @redapple , because others are raising exception, e.g. 



raises `ValueError: Missing scheme in request url: /aaaa/`
",pawelmhm,redapple
2552,2017-02-16 00:16:52,"@elacuesta urlsplit uses cache, it is likely that's the reason urlsplit is fast in your example.",kmike,elacuesta
2552,2017-02-21 17:08:16,"@elacuesta , `urlsplit` in `Request._set_url` and `urlsplit` in `safe_url_string` may not operate on the same input since `safe_url_string` [converts the URL to Unicode](https://github.com/scrapy/w3lib/blob/f46b4c4140dfd0081d34327d91f496f7a221bed7/w3lib/url.py#L95) beforehand if it's not already.
So you can also try with Unicode input and with bytes input if it makes a difference.",redapple,elacuesta
2549,2017-02-09 10:26:00,"@redapple thanks.
it works for me.",podolskyi,redapple
2546,2017-02-09 16:34:38,Hi @redapple I'd like to work on this issue. I'm new to scrapy so I don't know the workflow i.e. whether we need to be assigned first or we can submit the PR straightaway :),satwikkansal,redapple
2545,2017-02-18 14:30:23,"@kmike I ran the whole test for both python2 and python3.
For `python2`, only `test_proxy_connect.py` ran (among the proxy ones) while no proxy tests for `python3`.

But I, currently, am using a proxy network.Does it affect the test?",Parth-Vader,kmike
2545,2017-02-21 04:18:33,"ping @kmike 
What has to be done next?",Parth-Vader,kmike
2545,2017-02-21 10:04:19,"@kmike Alright, I will take it. 
What is the workflow? Should I make a new branch and then send a PR?",Parth-Vader,kmike
2545,2017-02-23 16:40:05,"Ping @kmike 

Progress till now :- 

- Python 3 support from `mitmproxy` starts from version 0.18.1.

- The library `libmproxy` has changed to `mitproxy` from versions 0.16 and so have `libmproxy.controller` and `libmproxy.proxy`.

- I am successful in running the `test_proxy_connect.py` for python3, although all the 6 tests fail for it.

The tests fail since `libmproxy` has been changed and `controller` and `proxy` have to be replaced.

Now, I'll try to make the tests pass.

",Parth-Vader,kmike
2544,2017-02-08 11:46:51,Great @kmike !,redapple,kmike
2543,2017-02-27 16:03:02,"Merging it, given that https://github.com/scrapy/scrapy/pull/2566 implementation is the same and I had @redapple's +1 for the previous backwards incompatible version.",kmike,redapple
2537,2017-02-07 13:23:01,"@redapple I don't know.. This is a a bug fix, but it is also a big change (e.g. fragments are preserved by default now). Waiting until 1.4 sounds fine to me.",kmike,redapple
2537,2017-02-20 14:38:16,"@redapple right, e.g. fragments are no longer stripped.",kmike,redapple
2535,2017-02-06 11:15:12,"> Does the library require anything special/additional to get installed properly?

I didnt have to install anything, but it seems like some environments are failing to install, so most likely some systems are missing required libraries. Testing for support at runtime may be much safer yes, I'll add that @redapple ",pawelmhm,redapple
2535,2017-02-06 14:24:22,"@pawelmhm , I do not see brotlipy being installed for Python 3.x tests on Travis.",redapple,pawelmhm
2535,2017-02-06 14:45:56,@redapple sorry missed that updated in af802bad14f833178bc4e,pawelmhm,redapple
2535,2017-02-07 09:25:30,@kmike re performance there are good benchmarks and charts here: https://quixdb.github.io/squash-benchmark/#ratio-vs-compression ,pawelmhm,kmike
2535,2017-02-07 11:27:05,@pawelmhm I was more worried about Python wrappers speed,kmike,pawelmhm
2530,2017-02-06 14:17:29,"Hi @redapple, I included some tests. I still have one doubt though. Should scrapy skip proxies based on `no_proxy` env var even for proxies set on `request.meta`? I think users would expect that explicitly setting a proxy in `meta` would override `no_proxy`, but I would like your opinion on this. Thank you!",elacuesta,redapple
2530,2017-02-06 15:43:03,"~What about a setting (like `HTTPPROXY_ENABLED`), allowing to completely disable the middleware? Not sure what should be the default value, though.~

`HTTPPROXY_ENABLED` setting added, please review again @redapple",elacuesta,redapple
2530,2017-02-14 14:58:46,Thanks @elacuesta!,kmike,elacuesta
2528,2017-02-02 21:43:39,"Thanks @redapple, @dangra and @glyph!",kmike,redapple
2527,2017-02-02 14:40:29,Thank you @redapple for the explanation! :D,starrify,redapple
2524,2017-02-01 18:28:39,@redapple thanks for your reply. Something must be wrong with my environment because if I launch it in Docker container (with mounted project root directory) command is visible.,nihn,redapple
2519,2017-02-02 16:15:21,Merging it as per https://github.com/scrapy/scrapy/pull/743#issuecomment-276078468 :) Thanks @redapple!,kmike,redapple
2518,2017-01-30 13:08:09,"@kmike I noticed that too but I don't know where it comes from.

I was able to circumvent the problem by generating a new project. I don't know why but scrapy tends to become buggy when a project holds mutliple spiders. For instance in one project one spider had an `import` problem (library not found) and I wasn't able to run any other spider beacuse of it.
 One also have to use `scrapy genspider`after generating a project because when I copy a spider from another project without using `genspider` it can't find the spider.

Scrapy is a terrific engine but sometimes it's hard to use for soeone who isn't familiar with the innerworkings of it.",fausterjames,kmike
2515,2017-02-01 13:29:10,Would be great @Digenis. Would it make sense?,redapple,Digenis
2513,2017-01-25 10:42:39,"@redapple 
1、There is the only bug in log, and it repeat 268 times.
2、The error happen  when my spider have ran 2 days.
3、if you need I can proive my spider code.",whwq2012,redapple
2513,2017-01-25 10:48:42,"@redapple This is my error log. https://drive.google.com/open?id=0B7iXisqaWzlENHFjT2ZHQzlOY0k
The log contain two type of error, one is the twisted error, another is redis auth error. ",whwq2012,redapple
2513,2017-01-26 11:00:03,"@redapple OK, I try to use LOG_LEVEL=DEBUG. But please give me some time. Because my server have no transfer until next month. I will run spider in 2.1. About 2.3~2.6. I will you send you log",whwq2012,redapple
2513,2017-01-26 11:10:50,"@redapple it means February 1st, February 3rd, February 6th. Sorry My English is poor",whwq2012,redapple
2513,2017-02-10 14:22:20,@redapple here is log. https://drive.google.com/file/d/0B7iXisqaWzlEZEd6dTlSSlNqS1U/view?usp=sharing,whwq2012,redapple
2512,2017-01-25 13:40:58,Thanks @fladi ,redapple,fladi
2510,2017-02-06 19:57:49,Looks good! Thanks @elacuesta and @voith.,kmike,elacuesta
2510,2017-02-08 17:30:29,Thanks @elacuesta ,redapple,elacuesta
2509,2017-02-08 07:13:49,Thanks @rolando!,kmike,rolando
2509,2017-02-08 11:15:04,"Thanks @rolando !
I tested with scrapy-djangoitem as described in https://github.com/scrapy-plugins/scrapy-djangoitem/issues/18#issue-202632911 and it worked fine.",redapple,rolando
2507,2017-01-23 12:04:04,Thanks @eLRuLL!,kmike,eLRuLL
2505,2017-01-20 16:45:04,"thanks @Digenis , I'll make the proposed change sometime today",bernardotorres,Digenis
2505,2017-01-21 11:36:25,"@Digenis thanks, just to be sure, can you comment on which line you'd like me to document? ",bernardotorres,Digenis
2505,2017-02-15 23:55:18,"@Digenis - you wrote

> I tried using it for Microsoft's FTP server and it didn't work.

How did it manifest? Was `.files` empty or incorrect, or was the whole response failed with an exception? How did it work before this PR? Sorry for stupid questions :)",kmike,Digenis
2504,2017-02-28 14:58:14,"@redapple Thanks a bunch for your help. I did manage to solve the problem although with a slightly different approach. I am posting the code here for completeness and so others might benefit.

",manisoftwartist,redapple
2503,2017-02-02 16:24:51,Thanks @redapple!,kmike,redapple
2500,2017-03-01 03:15:03,"@kmike yes I will in the next few days, sorry for the delay in replying ",jlong49,kmike
2499,2017-03-08 13:38:19,"Yes celery uses multiprocess but also have an eventlet implementation... 
Using celery is still the most straight forward solution for us right now because its comes with its own scheduler (celery beat) and so it's really easy to schedule a task and ensure that no more than x crawls run at the same time. Since we have a lot of spiders (more than 50), i doubt that we could all run them in the background waiting to receive new orders but maybe i am wrong ? 
On the twisted bug, i am really not sure what are the implications because we are using celery + scrapy for more than one year now and this bug doesn't seem to impact any of our crawl... Do any of you have any idea on what lead to this bug and what happen next (as i say, the spider doesn't seem to stop afterward) ?

I may try eventlet, @kmike do you know if scrapy/twisted can be interfaced with eventlet ?",cp2587,kmike
2498,2017-01-15 16:58:14,"Thanks, @kmike .
If I understand correctly, it's exactly the same problem (and the same fix incidentally:)) But it's not merged for 2.5 years?",medse,kmike
2496,2017-02-02 11:58:39,"@sibiryakov , I do not understand why accepting tuple helps with broad crawls (honest question).
This patch is about forcing the DNS resolution timeout to the value set in scrapy settings (prior, only Twisted's default 60s was being used).
Other changes to DNS resolution will probably be based on the new pluggable resolver interface in Twisted 17.",redapple,sibiryakov
2496,2017-02-02 13:07:34,"@sibiryakov , are you ok with opening a seperate new issue to discuss DNS resolution implementation  and configuration options that would play nicer for broad crawls?",redapple,sibiryakov
2496,2017-02-02 15:05:56,"sure @redapple 
",sibiryakov,redapple
2493,2017-03-01 17:57:17,"@kmike yes, #2590 address this case in particular.",rolando,kmike
2493,2017-03-01 17:58:54,I'm closing this pull request then. Thanks @dan-blanchard and @redapple for debugging it and @rolando for the fix!,kmike,rolando
2493,2017-03-01 17:58:54,I'm closing this pull request then. Thanks @dan-blanchard and @redapple for debugging it and @rolando for the fix!,kmike,redapple
2489,2017-01-10 09:26:14,"Indeed! Good catch, @aPaulius , thanks!",redapple,aPaulius
2484,2017-01-11 03:55:38,"@eLRuLL I have renamed all module with new name in project. But still same error. From that traceback , it said that i can not change anything in project (all traceback is from scrapy library )",astrung,eLRuLL
2483,2017-01-10 09:28:59,Thank you @Digenis (and @voith ),redapple,Digenis
2481,2017-01-05 10:02:23,"I would say this kind of HTML-fixing should be handled by Parsel, but I know of (at least) https://github.com/alecxe/scrapy-beautifulsoup downloader middleware that ""pipes"" responses through BeautifulSoup (with html5lib parser if requested)

From the README:
> BeautifulSoup itself with the help of an underlying parser of choice does a pretty good job of handling non-well-formed or broken HTML. In some cases, it makes sense to pipe the HTML through BeautifulSoup to ""fix"" it.

@eliasdorneles , have you tested these broken pages with html5lib?  If so, there's also the option of finalizing https://github.com/scrapy/scrapy/pull/1043 in Parsel.",redapple,eliasdorneles
2475,2017-01-03 13:21:39,"Hey @redapple -- yeah, I wanted to try this out before doing the change, just to be sure.

I've installed Windows on a VM to verify the installation process, and I've found trouble already just trying to install CPython directly from python.org downloads. The installation would never end, even after leaving it running for hours.

I've successfully installed Anaconda and am now testing the Scrapy installation process with it.
Sorry for the delay on reporting, I'll keep this issue updated as I go along.",eliasdorneles,redapple
2474,2017-01-05 14:44:45,"@kmike Interesting, thanks for the links.

In lieu of a fancy new algorithm, I think my suggestion would be for a random de-queuing feature.

So rather than FIFO or LIFO, it randomly picks an item from the queue and crawls that instead assuming it's allowed (i.e. autothrottle etc taken into account). 
For bonus points, if it can't do that url (i.e. autothrottle), it could pop it back into the queue and try another one up to n times before just waiting for autothrottle or whatever to allow it to pass.

I'd imagine at least the first part of that would be relatively simple to implement (although my attempt to grok that portion of the code didn't get me far), much more so than a new featureful algorithm, and for use-cases like mine (~3-10 requests to each of thousands of domains, it would probably work almost as well as a fancy algorithm).",mohmad-null,kmike
2473,2017-02-14 13:30:12,"@kmike The latest from pip install: 1.3.2. I am running on an old machine which is not upgraded for a while: Ubuntu 12.04 LTS - 32 bit
So may be that's why I needed to downgrade Twisted.
",pembeci,kmike
2473,2017-03-02 03:50:07,"@kmike awesome! 👍 
My pyOpenSSL version is 0.13.1. After upgrading it to 16.2.0, scrapy works like a charm!",wzpan,kmike
2473,2017-03-02 12:42:21,"@rolando 
Cool! Thanks a lot.😄",noprom,rolando
2473,2017-03-27 12:42:59,"@redapple I haven't realized it is just a warning, not an error. If adding `[tls]` still allows to install Twisted then +1 to add it.",kmike,redapple
2472,2017-01-02 19:11:07,"Hi @kmike
Thanks for the thoughts; I've not encountered your two cases as yet. I'll have a look at the request_fingerprint method; it looks like it may be useful.",mohmad-null,kmike
2471,2017-01-11 11:01:29,"@barraponto , I would not mind having no-conversion by default. But as far as I understand the code, it does [currently convert](https://github.com/scrapy/scrapy/blob/d8672689761f0bb6c0550a841f35534265e87fee/scrapy/pipelines/images.py#L122) [to JPEG](https://github.com/scrapy/scrapy/blob/d8672689761f0bb6c0550a841f35534265e87fee/scrapy/pipelines/images.py#L143) by default, so it would not be backward compatible. (am I missing something?)",redapple,barraponto
2471,2017-01-13 09:41:30,"@redapple my bad, I thought we were **adding** the feature.
I can't imagine how to keep backwards compatibility and have no conversion by default...",barraponto,redapple
2469,2017-01-03 11:37:26,Looks good to me. Thanks @eliasdorneles !,redapple,eliasdorneles
2466,2017-01-10 09:22:57,"I think the project template needs to be consistent in naming, and the generated settings need to be coherent with the generated classes.
Currently, in `settings.py`, there's:
- `$project_name.middlewares.${ProjectName}SpiderMiddleware`, which matches the generated `class ${ProjectName}SpiderMiddleware(object)` in `middlewares.py`
- `$project_name.middlewares.MyCustomDownloaderMiddleware`, which is not in the template,
- `$project_name.pipelines.SomePipeline` **which does not match** the generated `class ${ProjectName}Pipeline(object)` in `pipelines.py`

So it's far from using descriptive component names (I agree with @kmike that well-named single-purpose components is good advice, but it may be something for the docs or via comments in the generated files?), nor is it coherent.
And there could also be a `class ${ProjectName}DownloaderMiddleware(object)` in `middlewares.py` even.",redapple,kmike
2466,2017-01-12 23:22:33,I agree with you @kmike but I think that isn't the case here. I am just changing the `SomePipeline` name that was already in the project without any real purpose. I would say it actually could confuse a beginner.,eLRuLL,kmike
2464,2017-01-09 12:30:53,"They worked, and now that I took a closer look the documentation doesn't actually say that the numbers should be integers. I'll change it to allow floats too, thanks for noticing @kmike!",elacuesta,kmike
2464,2017-01-09 12:39:22,"@redapple, @kmike The branch is still named `component_order_integer`, would you guys like me to close this PR and open a new one with a more suitable branch name?",elacuesta,kmike
2464,2017-01-09 12:54:14,"@elacuesta , that branch name is not an issue, don't worry",redapple,elacuesta
2464,2017-01-31 15:48:01,"Still looks good to me.
@dangra , @kmike , @vshlapakov, what do you think?",redapple,kmike
2464,2017-02-03 11:56:51,"@kmike Updated, I went with `_validate_numeric_values` :-)",elacuesta,kmike
2464,2017-02-05 10:47:08,"Looks good, thanks @elacuesta! ",vshlapakov,elacuesta
2464,2017-02-06 20:05:12,"Hey @elacuesta! Sorry for being so picky. I'm not a native speaker, but for me `_validate_numeric_values` reads as ""validate values if they are numeric"", i.e. ""check only numeric values, but not other values - e.g. ensure that all numeric values are positive"", not ""check that all values are numeric"" :)",kmike,elacuesta
2464,2017-02-07 12:17:41,"Hahaha no problem @kmike :-)
Sure, the name wasn't really the best, I changed it to `_validate_values`",elacuesta,kmike
2464,2017-02-08 12:01:14,Thank you @elacuesta !,redapple,elacuesta
2457,2017-01-11 18:46:00,"@kmike , I updated the docs on Selectors with XPath variables.
Do you want to mention ad-hoc namespaces too?",redapple,kmike
2456,2017-01-06 15:40:33,"@elacuesta , I believe it also needs tests for non-default (None, 2, etc.) widths.",redapple,elacuesta
2456,2017-01-06 18:34:05,"Hey @redapple, thanks for your comments. I added a few tests with distinct width values.
Regarding the default, I set it to `4` following @kmike's [comment](https://github.com/scrapy/scrapy/issues/1327#issuecomment-118660643) and my own desire, but I think it would be just fine to leave it optional.",elacuesta,redapple
2456,2017-02-08 17:03:30,"Alright, I set `FEED_EXPORT_INDENT_WIDTH=None` to maintain backwards compatibility.

Ping @redapple, @kmike: I'm sorry to bother you guys, but do you have any further comments on this? Thanks!",elacuesta,redapple
2456,2017-02-08 17:24:45,Looks good to me @elacuesta ,redapple,elacuesta
2451,2016-12-15 13:29:37,"I ran into the same issue a year ago. In my case the most CPU intensive task was the `lxml` extraction logic. Try to use profiler as @kmike said (or like in [this](http://stackoverflow.com/questions/582336/how-can-you-profile-a-python-script#582337) answer).

And if possible, share the output result here - it will be really interesting information.
Also, it will be great if you can provide OS and Python version. Maybe there is something wrong with the default Twisted reactor, because some VPS providers use patched Linux kernels.",jacob1237,kmike
2451,2016-12-18 08:41:49,"Nevermind, could we close this now. I feel really dumb. After looking at the profiler that @kmike sent me too. I literally saw that retailer was returning me empty responses, and since my `parse_product_page` would never raise any exceptions , it actually was an anti scraping measurement from the retailer site. I just never noticed. Issue solved, I'm going to have to scrape more politely.",IAlwaysBeCoding,kmike
2451,2016-12-19 23:20:00,"Thanks for closing it, and sorry for the inconvenience. However, I have to say that I learned a new tool to use in my arsenal of debugging, snakeviz!!! Thanks for the help @kmike I appreciate it a ton, you rock keep up the good work! ",IAlwaysBeCoding,kmike
2444,2016-12-12 11:27:30,"@kmike oops, only `.text` is raising AttributeError: https://github.com/scrapy/scrapy/blob/master/scrapy/http/response/__init__.py#L86-L103 -- should I change that for the case people do `hasattr(response, 'css')` as well?",eliasdorneles,kmike
2439,2016-12-14 11:30:14,"@kmike  tnx I want to pass in a variable then use it to concatenate the `full url` and use it as the start_url.

My code is like this :  and get the error `self is not defined` why is that ?

",woshichuanqilz,kmike
2439,2016-12-14 12:16:54,"@redapple  already post ! 
http://stackoverflow.com/questions/41142375/how-to-use-the-a-option-to-pass-a-parameter-to-scrapy",woshichuanqilz,redapple
2438,2016-12-12 04:25:37,"@redapple 

Interesting that home page is working fine. And rss is responding with 500.

Closing this. Sorry for bothering..",yunmanger1,redapple
2437,2016-12-21 14:05:38,"@kmike , good idea. Would you mind writing it up? This branch is under ""scrapy"" org",redapple,kmike
2437,2016-12-21 14:57:06,"@kmike , what do you think of 
",redapple,kmike
2437,2016-12-21 14:58:18,@redapple you've beaten me :) Looks perfect.,kmike,redapple
2436,2016-12-14 10:37:13,"@pawelmhm 

> Sorry just wanted to clarify one thing. Why do you need response in pipeline if you say you want to compare items?

I need only the `meta` field where I store old checksums.

Also, hypothetically (not my case, just example) I may want to store some response-specific parameters such as URL, status or some meta fields, together with the Item (because pipelines can be used for item storing).

> if checksum is tied to Scrapy.Item why are you putting it in request meta instead of putting it in item as a field? Just add field 'checksum' to your item, keep some old item somewhere (e.g. in some database that can be accessed from pipeline) and compare checksums in process_item pipeline method.

Because I have no elegant way to access old checksum (from pipeline) except the database retrieval request. So if the page contains 100 items (in our case it can be up to 500), I will need to make 100 retrieval requests to the database. It is OK for simple cases when you can use local Berkeley DB files (like **DeltaFetch** do), but it is not good for distributed systems with centralized network-access storages (our case).

So in our case it is better to retrieve checksums once and put them into the `meta` field than produce more than 100 additional retrieval requests to the database for every URL.

> There is Scrapy extension called Delta-Fetch that seems to be something that might be useful for you, maybe you can give it a try https://github.com/scrapy-plugins/scrapy-deltafetch

Thanks for advice! Actually, I've already solved my problem and DeltaFetch wasn't the better solution for our case (because it doesn't detect any real changes in Item fields).

> If you actually really need response in pipeline (very rare but happens), you can just do following: store response on disk or in database in spider middleware process_spider_output method - generate 'filename' key add it to item. In pipeline when processing item, fetch 'filename' key from item and use it to load response from filesystem.

Of course it's possible, but it looks like a dirty hack for such case. That's why I started this discussion.",jacob1237,pawelmhm
2436,2016-12-14 14:28:19,"@pawelmhm

> hmm why not store them in item too? Keeping whole response body in memory (which may be huge) just to get status code or other parameter seems excessive.

This is not a problem because the response body is not destroyed during the pipeline processing (if I understand correctly). The last reference to the response (and its body) MAY be destroyed only after `item_scraped` signal call (check [this answer](https://github.com/scrapy/scrapy/issues/2436#issuecomment-266983206) about the response lifecycle).

So the response body is keeping anyway, even if you don't use it in your pipeline.

> You can use spider middleware process_spider_output, it has access to response and all output from spider callback and is ideal for this kind of use case, you can fetch something from response and append to item as a field.

Well, this is my current solution. The problem occurs when I want to drop some items (which weren't modified). The `DropItem` exception can be raised only from the pipeline.

So when I want to drop an item from the middleware, I can't do that in a direct way. Right now I manually call the `item_dropped` signal to be able to catch it from extensions (`crawler.signals.send_catch_log_deferred()`).

This looks like a duct tape programming.

> yeah you're right, making database query for each item is not efficient. You can bypass this easily by downloading all checksums in pipeline on spider_opened signal.

Not my case. We have long-running Scrapy workers that are feeding from the one distributed queue in a real-time. This is also the [Frontera](https://github.com/scrapinghub/frontera) case (distributed mode).

So as I said before, we don't have any ability to do that from the pipeline.

> The fact that you managed to solve your problem without this change seems to confirm that it's not very urgent to add it, am I right?

It is not very urgent, you're right. I've started this discussion both to show and to better understand the problem.

And if we'll come to any decision, I'm ready to make a pull request to fix that problem.",jacob1237,pawelmhm
2435,2016-12-08 21:48:07,"> The most common problem with inefficient fetching is a queue filled with a single domain

Yeah, it is a very common issue; I'd love to have it solved in Scrapy. @sibiryakov could you please clarify what changes do you think we should make? Scheduler can already get information from self.crawler.engine.downloader - what do you propose?

> and polite crawling requirement.

Hm, one can send URLs of the same hostname/ip to the same worker, and use existing scrapy limit features. Do you have examples of when this method doesn't work, or when it is better to solve this in scheduler? Is it to support a case where a single worker is not enough to scrape from a single ip, but there still should be limits?
",kmike,sibiryakov
2434,2016-12-12 16:25:39,"@kmike 

> Because of these questions I'd start without pygments; it should make PR easier to merge and less controversial.

yeah good idea. I removed pygments.

> It'd be also nice to detect cases where colored log doesn't make any sense (e.g. when redirecting scrapy output to a file using either shell redirect 2> or using scrapy option)

yes. this is already detected, when you pass `logfile` `configure_logging` in utils jumps to if filename block and never moves to if log_enabled and log_colors. Logging in color to file is not good, because ansii codes are printed to file and it looks unreadable and noisy if logfile reader doesn't support it.

> A nice feature would be to highlight different parts of log messages - e.g. dates could be more dim, as well as logger names; maybe they will look better with fixed colors (i.e. not red/green depending on log level). 

This is somewhat tricky part. There is question what exactly deserves its own colors (dates? HTTP status codes? different types of messages? item_scraped, item_dropped etc?). There is also difficult in finding color combinations that will look good. It may require some good designing skills and some form of visual sensitivity to colors that allows you to choose right color palette and it seems somewhat difficult to me. Maybe we could just provide some easy way to load different colorings via log_format string, so that users can conrtribute their own formattings?

> How does highlighting look like on a white terminal background? Are colors still ok? 

Need to check with white background, I'm not sure at the moment. 

> If not, how to solve that - provide several themes, at least for white and dark backgrounds? What about other themes with fancy colors, i.e. not just red/green/cyan from 80s?

Yeah ideally we could provide different colors, and maybe even allow users to contribute their own colors? Perhaps setting COLOR_LOG_FORMAT could control that, but i need to test that. 

",pawelmhm,kmike
2434,2017-03-17 15:35:19,"@kmike @redapple I removed colorama dependency, it is apparently only needed for windows logging. I followed approach from [this commit](https://github.com/qutebrowser/qutebrowser/commit/c64e5c9bd595ef310825d07bf29e1d3eb7674714) and it works fine. Now only need to:


- [ ] test on windows - add some warning on windows saying what to do (probably saying to add colorama)
- [ ] add unit tests 

for tests I'll add one test to test_log_utils - testing logformatter object format() method, and probably also need some sort of integration test. If you have any suggestions that's nice ",pawelmhm,redapple
2434,2017-03-17 15:35:19,"@kmike @redapple I removed colorama dependency, it is apparently only needed for windows logging. I followed approach from [this commit](https://github.com/qutebrowser/qutebrowser/commit/c64e5c9bd595ef310825d07bf29e1d3eb7674714) and it works fine. Now only need to:


- [ ] test on windows - add some warning on windows saying what to do (probably saying to add colorama)
- [ ] add unit tests 

for tests I'll add one test to test_log_utils - testing logformatter object format() method, and probably also need some sort of integration test. If you have any suggestions that's nice ",pawelmhm,kmike
2433,2017-02-13 16:55:28,"@redapple @kmike I recently found this PR and realised that now Scrapy swallows import errors not only for `scrapy version` but for other commands like `scrapy list`. The problem is that we rely on `scrapy list` results as a part of deploy process: earlier if someone forgot to add some necessary dependency - the deploy was failed at once. Now the exception is swallowed and `scrapy list` returns 0 code with some stderr, as a result platform thinks that everything is fine and there're just 0 spiders.

Of course it's possible to catch `RuntimeWarning` when running `scrapy-list`, but it's more about guessing as client code can also raise same exceptions and parsing stderr is not a reliable way. For now we started to always pass stderr logs if any, but I wonder if there's an option to add some switch between exception/warning behaviours (via Scrapy setting for example). 

Please share your thoughts, I can start a separate issue to track it.",vshlapakov,redapple
2433,2017-02-21 15:29:44,"@redapple Agree. We have a similar setting in [sh-scrapy](https://github.com/scrapinghub/scrapinghub-entrypoint-scrapy/blob/master/sh_scrapy/settings.py#L130) to decide if we should fail on import errors, or just print the error/warning, something like this should here as well. So I'd propose to limit this feature to the commands that really need to load spiders code + use a setting to change its behaviour. In other words I don't mind against current behaviour but we need to be able to configure it.",vshlapakov,redapple
2431,2016-12-08 13:08:24,"@redapple I updated the code to force scrapy and requests to use the same headers (including the User-Agent), and the responses are still different (see the attached zip file for the two html files, and you can use diff or vimdiff to see the differences).  You can also run the code to get the two html files.",fjdu,redapple
2431,2016-12-08 15:27:47,@redapple Upgrading to scrapy 1.2.2 solved the problem!  Thanks a lot!,fjdu,redapple
2424,2016-12-05 21:12:20,@redapple I'm not using any proxies and it's been happening consistently for a while. I'll see if I can reproduce it in an isolated environment.,briehanlombaard,redapple
2420,2016-12-01 15:24:24,"@redapple Yeah, that's a not too frequent case to worry about, but I found it in our own internal project (it's linked with this issue on above). It's fixed there already, but someone can do the same mistake.",vshlapakov,redapple
2420,2016-12-21 06:37:58,"@elacuesta Makes sense to me, that would be nice, thanks. In my case I encountered with a string with an integer inside as order, so it should be covered by the similar changes. Btw there's no need to strip value before converting to int, but that's nitpicking :)",vshlapakov,elacuesta
2419,2016-12-01 17:11:42,"Thanks @redapple for your feedback. I have allowed tox env ""isort"" to fail on travis.
I would like to get the opinion of other maintainers if they'd think this PR would be useful for scrapy.
ping @kmike @eliasdorneles @dangra ",voith,redapple
2419,2016-12-12 05:37:14,I am going by @dangra's comment. Closing this,voith,dangra
2418,2016-11-30 12:49:34,"Looks good to me, thanks @ahlinc @redapple @Tarliton !",eliasdorneles,redapple
2418,2016-11-30 12:49:34,"Looks good to me, thanks @ahlinc @redapple @Tarliton !",eliasdorneles,ahlinc
2417,2016-11-29 17:27:38,Thanks @pawelmhm !,redapple,pawelmhm
2413,2017-03-01 17:37:01,"@rolando it should be fixed by https://github.com/scrapy/scrapy/pull/2590, right?",kmike,rolando
2413,2017-03-01 17:53:54,"@kmike yes, this is the case of the broken chunked response.",rolando,kmike
2411,2016-12-06 18:04:11,@redapple what do you think about putting this to 1.3?,kmike,redapple
2411,2016-12-06 18:06:03,"@kmike , +1",redapple,kmike
2403,2016-11-24 10:37:05,"Thanks @nyov . I'm ok with waiting for your commits to #1887.
A good point on backporting this to 1.0.x, 1.1.x (we haven't branched out 1.2.x yet)",redapple,nyov
2403,2016-12-02 09:49:38,"@nyov , with
> Okay I'm good with merging this (With the version at 13.1.0)

do you mean starting with this?:
",redapple,nyov
2403,2016-12-05 16:33:35,"@dangra , what do you think of @nyov comment about using `if twisted_version > (13, 1, 0):` ?",redapple,nyov
2403,2016-12-05 16:33:35,"@dangra , what do you think of @nyov comment about using `if twisted_version > (13, 1, 0):` ?",redapple,dangra
2403,2016-12-05 17:05:15,"@redapple : it makes sense, I bumped the version to 13.1.0 ",dangra,redapple
2403,2016-12-05 17:25:17,"Right @kmike , touching Twisted integration is never simple.
Though, I'm for including this in 1.2.2, we have quite some tests around this and if there's backlash, we can revert and be better prepared for 1.3 and upcoming #1887 (btw, @dangra , I don't see any new commit)",redapple,dangra
2403,2016-12-05 17:25:17,"Right @kmike , touching Twisted integration is never simple.
Though, I'm for including this in 1.2.2, we have quite some tests around this and if there's backlash, we can revert and be better prepared for 1.3 and upcoming #1887 (btw, @dangra , I don't see any new commit)",redapple,kmike
2403,2016-12-05 17:51:03,"@kmike , another option is to 1) rename current 1.3 milestone to 1.4, 2) release 1.2.2 without this #2403 (which would be current master branch I believe), and 3) release a 1.3 only with this #2403 fix difference with 1.2.2 right after",redapple,kmike
2403,2016-12-05 21:28:54,"@redapple I think if we're releasing 1.3 right away we can incude #1887. 
+0 from me to release this PR as a part of 1.2.2, but I wanted to start a discussion.",kmike,redapple
2403,2016-12-08 10:08:46,"@dangra , with #1887 merged, this is not needed for ""master"" branch anymore, but we may want to ""backport"" to 1.2 and 1.1 branches. Is it worth it?",redapple,dangra
2402,2016-11-22 09:56:47,"WeChat in China is a communication tool, similar to the mailbox @redapple ",Lampere1021,redapple
2402,2016-11-23 03:16:52,@redapple  I am so sorry for my rudeness.,theharveyz,redapple
2400,2017-02-16 17:31:24,"@elacuesta yeah, it can't handle them because the encoding is unknown. But it shouldn't raise an exception even if a header is not utf-8 - there are byte sequences which can raise UnicodeDecodeError when decoded from utf-8; the whole response shouldn't fail in this case.",kmike,elacuesta
2400,2017-03-01 17:28:21,"@kmike It seems to me that `http.cookiejar` works consistently in py2/py3 only with unicode strings, I added some code to decode cookies from the `Cookie` header (which is always `bytes` thanks to the `Headers` implementation) using `utf8` and falling back to `latin1` in case of `UnicodeDecodeError`.

With this implementation the `Cookie` header would always end up encoded with `utf8` even if the cookies were already passed as bytes using a different encoding. What do you think about that? ",elacuesta,kmike
2399,2017-02-23 05:13:38,"@redapple , I am interested in this issue, from what I've seen so far, Scrapy handles graceful shut-down by calling ""stop"" on  CrawlerProcess ,and than waiting for the crawlers themselves to stop. I couldn't find how jobs are being persisted - could you please direct me?
And also, what would you say is the desired behaviour if Ctrl+C is called twice? Assuming graceful shut down + persistence takes time , but the user requested a force shut down?

Thanks,
Omer",foromer4,redapple
2399,2017-02-23 08:31:52,"@foromer4 Scrapy has two request queues - in Scheduler and in Downloader. Downloader uses a small in-memory buffer for requests (its size is equal to CONCURRENT_REQUESTS); it is the component which ensures concurrency limits and download timeouts are respected. Scheduler stores all requests; it can store them either in memory or on disk (see https://doc.scrapy.org/en/latest/topics/jobs.html). So request is first retrieved from Scheduler queue, put to Downloader queue, and then Downloader downloads it.

""Graceful stop"" (a single Ctrl-C) means that a spider stops retrieving requests form Scheduler and waits for Downloader queue to become empty. Double Ctrl-C means spider is stopped immediately - currently it means that all requests in Downloader queue are dropped.",kmike,foromer4
2399,2017-02-23 09:22:52,"@kmike  -  So, If we support a configuration in which the downloader requests buffer is on disk (instead of in memory)
that would ensure persistency , right?",foromer4,kmike
2396,2016-11-16 12:13:00,"@pablohoffman no, but I guess we want old links to show the page (and the current caution note)
",redapple,pablohoffman
2396,2016-11-22 13:21:47,"Thanks @redapple, nice trick!",kmike,redapple
2395,2016-11-15 15:40:22,"Thanks @elacuesta !
What do you think of adding a `base_url` parameter or something to `sitemap_urls_from_robots()` and fix it there?
",redapple,elacuesta
2395,2016-11-15 16:05:24,"@redapple done in [9e7694](https://github.com/scrapy/scrapy/pull/2395/commits/9e7694c039c26a54d5608f0934e59c57dfcd87f5), we could probably squash those two commits before merging.
",elacuesta,redapple
2395,2016-11-16 15:27:51,"Thanks @elacuesta 
",redapple,elacuesta
2393,2017-03-06 21:57:29,"@kmike , what do you mean by the fix not being complete?
Is it because `responsetypes.from_args()` does not follow whatwg?
Or because other uses of `from_args()` do not use the body either?

The aim here was to fix an issue at http decompression where it determines something else than the default `Response` type if there are hints in the body.",redapple,kmike
2393,2017-03-06 22:04:42,"@redapple the patch looks good because it improves response handling in decompression middleware, so I'm fine with merging it after a rebase. 

The logic middleware uses to detect response type is still different from what browsers do, and Scrapy is inconsistent in mime sniffing it performs. I should have opened another ticket for that, but I found it while reviewing this PR, so I wrote it in a comment :) ",kmike,redapple
2391,2016-12-07 13:21:27,Should we make this behavior optional @redapple ?,sibiryakov,redapple
2391,2016-12-07 14:26:27,"@sibiryakov , I would prefer that we make this change not configurable, handing responses with the correct type sooner than later.
#2393 is a good companion to this",redapple,sibiryakov
2391,2017-03-07 09:10:03,"Other than gzip signature this PR looks good. Signature also looks fine - 2 bytes is what gzip format rfc's say, and what wikipedia shows, but 3 bytes with hardcoded compression method seems to be more robust and recommended in mime sniffing spec.

@sibiryakov it seems the only case HTTP decompression should be made optional is when response bodies are not processed at all (no link extraction, no peeking into response body), and it is fine to store them in a raw compressed form, as they are received from transport - it means e.g. removing an ability to search inside these bodies. For me it sounds like a rare use case (http cache without any processing? what to use these cache results for?) - if you're downloading a page then likely you want to use it somehow, and to do that you need to decompress it. 

This issue is not about decompressing all gzipped files automatically (scrapy is not doing this), it is about undoing http compression.",kmike,sibiryakov
2391,2017-03-07 11:57:02,Thanks @redapple!,kmike,redapple
2390,2016-11-15 13:22:09,"@redapple: maybe I'm overlooking something, but do we need something more complex than just adding



or even



right before https://github.com/scrapy/scrapy/blob/master/scrapy/spiders/sitemap.py#L36?
",elacuesta,redapple
2390,2016-11-15 14:26:03,"@elacuesta , it could be that simple, I haven't had a look :)
Patch welcome!
",redapple,elacuesta
2388,2016-11-15 10:22:39,"Very good point @kmike.
This patch introduces a regression for some robots.txt in the wild, eg. http://www.quikr.com/robots.txt


",redapple,kmike
2388,2016-11-16 16:44:03,"@kmike , it looks like http://www.huffpostarabi.com/robots.txt responses depend on User-Agent string. With ""scrapy"" or my Chrome's value, I can't reproduce `IOError: Not a gzipped file`
",redapple,kmike
2388,2016-11-16 18:35:48,"@kmike , MIME sniffing looks like an interesting addition for w3lib
",redapple,kmike
2386,2016-11-09 18:33:10,"@rolando could you please explain why are pyc files a problem?
",kmike,rolando
2386,2016-11-09 18:41:19,"@kmike IMO, it leaves the build dirty as only templates directory have `pyc` files. But, particularly this was breaking the conda-forge build on windows: https://github.com/conda-forge/staged-recipes/pull/1646#issuecomment-249061645

The conda build scripts cleaned up pyc files, but not in the templates directory. So when attempting to recreate pyc files it failed because templates already had pyc files around.
",rolando,kmike
2386,2016-12-01 19:19:09,"@kmike The Manifest.in is only for `sdist` command: https://docs.python.org/2/distutils/sourcedist.html#the-manifest-in-template

The `sdist` output already does not include any `pyc` file except those two listed above, which usually don't cause a problem because the installers  (or binary packaging tools) compile all `pyc` files for the given platform.

This PR only aims to have a clean source distribution without any `pyc` file (those two files listed above).

PD: The source distribution package does not include pyc files but default, but given templates are data directories those two `pyc` get included.",rolando,kmike
2385,2016-11-08 16:42:58,"> I'm not sure about the license though. Does GNU GPLv2 apply here?

@redapple I think users of this file are free to choose either GPL or PSF license; choosing PSF license is fine.

The code looks small enough 👍  How hard would it be to separate robotparser and our scrapy-specific fixes? It may be hard to keep packages in sync. I can imagine creating e.g. backports.robotparser package, which has your fixes for Python 2 compatibility, but not canonicalize_url bits. Or we can make robotparser2 or something like that, which uses w3lib. 
",kmike,redapple
2385,2016-11-08 17:36:57,"@kmike , thanks for the feedback .
This parser is not scrapy-specific but `canonicalize_url` works around the root problem of Python 2's `unquote` producing unicode strings with ""raw"" bytes that it cannot quote back (quoting sort of acts as the normalization/canonicalization)



but I think a w3lib-free implementation is doable. I'll work on this.
",redapple,kmike
2384,2016-11-14 08:02:47,"@redapple  Hi, Thank you for your reply, 

So to be sure I have in my settings.py: 
LOG_LEVEL = 'DEBUG'

If in my spidername.py I put nothing of logging in, I get the debug output when running scrapy, which is perfect, BUT as soon as I implement my own logging in this way all logging to the console from scrapy disappears. Is there a way I can have 'both' ? Cheers.

Things I have added in my spidername.py that makes the default scrapy output dissapear: 

`import logging`

`logger.error(""I LOG SOMETHING"")`

As soon as I remove that code the default logging output in scrapy appears again. 

Cheers
",yssvic,redapple
2384,2016-11-18 10:52:48,"@redapple Can you change the label of insufficient info please? Cheers
",yssoe,redapple
2384,2016-11-22 15:47:12,"@redapple Thank you for trying this. 

LOL, what you're getting is exactly what I would like to get, let me dig this a bit deeper, will be back. Cheers
",yssoe,redapple
2384,2016-11-23 14:31:58,"Hi @redapple,

I'm having the same problem. Here's my example:


And this is my spider.cfg config file:


If I try without to use the spider.cfg file, Scrapy debug works.

I tried removing this line:

`LOG_CONFIG = logging.config.fileConfig(""spider.cfg"")`

but I need to use the spider.cfg file to configure my logger.

Any suggestion?

Thanks a lot.",zar777,redapple
2382,2016-11-08 10:43:44,"@kmike , I did not manage to write a test on captured logs,
but https://github.com/scrapy/scrapy/pull/2382/commits/61efacdd1f6fb55cf1f694e56502a3d8a1d5a325 tests the expected exception,
and this test fails without the patch:

Python 3



Python 2:


",redapple,kmike
2382,2016-11-08 12:32:00,"Thanks @redapple, looks good!
",kmike,redapple
2378,2016-11-10 10:55:33,"@redapple 
https://doc.scrapy.org/en/latest/topics/settings.html#populating-the-settings - on that under the ""3. Project settings module"" bit, I'd probably add a line ""get_project_settings can be used to access these settings"" - which links to the documentation for get_project_settings

https://doc.scrapy.org/en/latest/topics/practices.html (which seems to be the documentation for ""get_project_settings"" - it's not documented on either the settings page or the api page) - I'd mention that settings loaded this way are not updated globally (so it can't be used in conjunction with os.environ['SCRAPY_SETTINGS_MODULE'] = 'myproject.settings' within the spider).
",mohmad-null,redapple
2377,2016-11-02 17:24:08,"@kmike , are you thinking of fixing this in `w3lib.url.safe_url_string()` perhaps?
",redapple,kmike
2377,2016-11-02 17:43:09,"@redapple yep
",kmike,redapple
2376,2016-11-02 19:27:16,"I had the same thoughts as @djunzu. What is your use case @mohmad-null?
",kmike,djunzu
2376,2016-11-10 10:48:56,"@redapple ; Sure. I have a global variable BLOCKED_DOMAINS set elsewhere like this:
`BLOCKED_DOMAINS = open('\\path\to\file\blocked_domains.txt')).read().splitlines()`

Which reads from a file that basically is just a list of URL's:



And then the middleware class looks like this:



There are probably better (and almost certainly more optimised) ways to do this.
",mohmad-null,redapple
2375,2016-11-18 05:34:36,"I confirm having the same issue on the latest scrapy on python 3.5. @redapple , so, before twisted will fix that issue, what is the proper way of sending email reports from scrapy on python 3+ installations? Thank you.
",MrLokans,redapple
2375,2016-11-18 12:58:41,"@kmike Thanks a lot for the reply.
",MrLokans,kmike
2375,2017-02-26 22:23:09,"@redapple I've gone ahead and merged https://github.com/twisted/twisted/pull/509 .
It is not in any released version of Twisted yet.
If you are willing to try out Twisted directly from git, can you try it and and let me know how it goes?",rodrigc,redapple
2373,2016-11-02 15:53:56,"What do you suggest @redapple? Use a backport of Python 3.x robotparser, switch to reppy, create our own robots.txt parser, tell people to use Python 3, something else?
",kmike,redapple
2369,2016-10-29 06:46:41,"Thanks @jc0n!
",kmike,jc0n
2364,2016-10-27 09:22:45,"@redapple Here you go

Item definition:



Populated item from logs:


",birla,redapple
2364,2016-10-28 06:21:54,"@redapple Thanks for the guidance and sorry for the clutter of an addition bug
",birla,redapple
2363,2016-11-02 16:53:51,"@redapple but browsers support this redirect somehow, right?
",kmike,redapple
2363,2016-11-02 16:55:03,"@kmike , this needs to be checked.
",redapple,kmike
2363,2016-11-02 17:01:35,"@kmike The redirect works in Google Chrome, but fails in Firefox.
",Tarliton,kmike
2362,2016-11-02 13:19:32,"@redapple  - its taken me several weeks just to get a basic scraper working; I wouldn't have the faintest notion where to begin trying to resolve this.
",mohmad-null,redapple
2362,2016-11-04 18:14:52,"@redapple out of curiosity, does it print the actual exception (maybe not as the last one) if executed in Python 3? 
",kmike,redapple
2362,2016-11-07 13:55:43,"@kmike , in Python 3; you get a bit more info but not the actual problematic line it seems:


",redapple,kmike
2360,2016-11-30 13:24:48,"@vionemc , I'm closing this issue. I assume you found @Digenis link useful.",redapple,Digenis
2359,2016-11-10 10:45:19,"@kmike , so you would go for https://github.com/scrapy/scrapy/issues/1877 ?
",redapple,kmike
2359,2016-11-13 12:46:22,"@redapple Hm, yeah, I think it can be a solution. 
",kmike,redapple
2355,2016-10-24 14:18:35,"@redapple will write up some docs.

Is your concern about renaming `DEFAULT_LOGGING` because we modify it later in `configure_logging` and as such `LOGGING` would be a misnomer? In this case I would agree.
",iserko,redapple
2355,2016-10-25 12:20:31,"@redapple updated my PR. Logging tests will pass now.

I've tried to add a Deprecation Class around DEFAULT_LOGGING in scrapy.utils.log as follows:



But logging.config.dictConfig overrides the dictionary 😂  
See: https://github.com/python/cpython/blob/master/Lib/logging/config.py#L314
used in:
https://github.com/python/cpython/blob/master/Lib/logging/config.py#L367
",iserko,redapple
2355,2016-11-15 09:17:10,"@redapple any word on this? If there's anything you'd want me to change I will.
Have been running this on our system for the past 3 weeks now without a problem.
",iserko,redapple
2355,2017-01-17 09:13:40,@eliasdorneles sorry for taking so long with a fix ... I've changed the PR to match ... still working on tests,iserko,eliasdorneles
2355,2017-01-17 09:50:18,Tests fixed and passing @eliasdorneles. @redapple you may want to review again as well,iserko,redapple
2355,2017-01-17 09:50:18,Tests fixed and passing @eliasdorneles. @redapple you may want to review again as well,iserko,eliasdorneles
2355,2017-01-17 16:05:37,"@kmike so do I, but scrapy already has existing custom logging setup which makes the transition much harder",iserko,kmike
2355,2017-02-20 17:08:48,"@kmike @redapple @eliasdorneles any updates here?

I'm in favor of doing it the same was as Django where if you specify a `LOGGING` dict in your settings, it uses that ... otherwise it uses the default.",iserko,redapple
2355,2017-02-20 17:08:48,"@kmike @redapple @eliasdorneles any updates here?

I'm in favor of doing it the same was as Django where if you specify a `LOGGING` dict in your settings, it uses that ... otherwise it uses the default.",iserko,eliasdorneles
2355,2017-02-20 17:08:48,"@kmike @redapple @eliasdorneles any updates here?

I'm in favor of doing it the same was as Django where if you specify a `LOGGING` dict in your settings, it uses that ... otherwise it uses the default.",iserko,kmike
2353,2016-11-23 10:55:24,"hi @MrMenezes , @kmike ,
I had a go at the testcase: https://github.com/MrMenezes/scrapy/pull/1
how does it look?",redapple,MrMenezes
2353,2016-11-23 10:55:24,"hi @MrMenezes , @kmike ,
I had a go at the testcase: https://github.com/MrMenezes/scrapy/pull/1
how does it look?",redapple,kmike
2353,2017-03-02 16:15:34,"@MrMenezes , I took the liberty to submit my proposed changes on top of yours in #2612.
Hence I'm closing this PR. Thanks for bootstrapping this!",redapple,MrMenezes
2351,2016-12-19 20:59:11,"What you suggest makes sense to me @dangra. In this case, the lack of foundation can be replaced by Scrapinghub as the legal representative. Once Scrapy grows its own foundation (some day, some day :), the rights will be transferred to that.",pablohoffman,dangra
2350,2016-11-02 13:24:56,"@djunzu  - for your example of the Duplicates Filter, my thoughts would be:
- How do I integrate it with the rest of the spider?
- What is ""item"" and how do I define it? (I get that there's the entire ""items"" page of the help, but the documentation could really use better integration of the various concepts).
- Is ""DropItem"" a built in exception or something custom created? If the later, what's the definition/handling look like?
",mohmad-null,djunzu
2344,2016-10-20 20:24:30,"Thanks @jaympatel !
",redapple,jaympatel
2343,2016-10-24 12:18:19,"thanks @nyov .
Right, `FTP_DEFAULT_USER` feels more reasonable.
With a password of `None` I was wanting to handle your first ""Already authed"" example.
But I'm not familiar with FTP ""in the wild"", so I trust you in that requiring password is saner.
",redapple,nyov
2343,2017-02-20 14:03:40,"Looks good @redapple! 

I opened an issue about ftp_user and ftp_password meta keys; it can be fixed as a prt of this PR while we're on it, but a separate PR is also fine.

What do you think about quoting rfc in ftp_password docs? Users may wonder why their default password doesn't work with some of the anonymous FTP servers.",kmike,redapple
2343,2017-02-20 16:16:09,"@kmike , I added a note in https://github.com/scrapy/scrapy/pull/2343/commits/f3a7567443d9e52add6224291e98999740d4c33d",redapple,kmike
2343,2017-02-20 17:19:45,Thanks @redapple!,kmike,redapple
2340,2016-10-20 08:54:07,"@redapple thanks for reply.



All my code shows above.
The system info is:



The ""totally blocked"" means the program stopped at this step and no matter how long I wait, nothing happened and I cannot use double `Ctrl+c` to force quit the program.
Actually, if I use a small pageLen such as 20, It works well.
",Sraw,redapple
2340,2016-10-20 09:33:44,"@redapple YES, that's real what confuses me. If there are some problems with the network(surely including proxies), it will shows timeout and go on.
Still thanks for answer. I will keep looking for solution on the website you recommend.
",Sraw,redapple
2340,2016-10-23 08:39:03,"@redapple HI, finally, I have located the problem.
if a string extracted from html contains `<style[^>]+</style>` , it will be blocked. But if I use module ""re"" to remove the part, it works well.
I have no idea about it, but I hope it can provide some information for the development group.
",Sraw,redapple
2338,2016-10-19 09:36:59,">  Files changed: 3,675

Your PR seems incorrect, too many changed files for a documentation issue #1704.
What is this PR about @gustavodeandrade ?
Are you having trouble with git?
",redapple,gustavodeandrade
2337,2016-10-19 10:26:51,"Thanks @muriloviana , it's a great addition to the project templates!
",redapple,muriloviana
2337,2016-10-19 16:13:48,"Thanks again @muriloviana ! 
",redapple,muriloviana
2337,2016-11-15 14:58:51,"Oops, sorry for taking long -- thanks @muriloviana !
",eliasdorneles,muriloviana
2334,2017-02-08 16:35:44,"@ArturGaspar , w3lib 1.17 will be released soon (see https://github.com/scrapy/w3lib/pull/87)",redapple,ArturGaspar
2334,2017-02-08 21:08:35,"@ArturGaspar , I relaunched the Travis builds with w3lib 1.17. All green :)",redapple,ArturGaspar
2334,2017-02-24 15:11:30,"Looks good to me.
@dangra , @kmike , what do you think?",redapple,dangra
2334,2017-02-24 15:11:30,"Looks good to me.
@dangra , @kmike , what do you think?",redapple,kmike
2334,2017-02-27 18:24:32,"@ArturGaspar looks good! 

Could you please add a test (or modify an existing test) to check other Response attributes: url, headers?",kmike,ArturGaspar
2334,2017-03-02 21:45:00,"Looks good, thanks @ArturGaspar!",kmike,ArturGaspar
2333,2016-10-19 15:48:47,"Thanks @ArturGaspar ! A long-needed improvement.
Have you considered using 3rd party libraries for this? e.g. [cachetools.TTLCache](http://pythonhosted.org/cachetools/#cachetools.TTLCache)
",redapple,ArturGaspar
2333,2016-10-19 16:33:43,"@redapple the implementation is simple enough, my vote is not to add a dependency for it
",kmike,redapple
2333,2016-11-14 11:15:54,"@kmike , what do you think of the PR?
If we merge it, I believe it means we start working towards a 1.3.x series
",redapple,kmike
2331,2016-11-30 10:56:11,"@moisesguimaraes , can you update the docs as @kmike commented?
Other than that, looks good. Thanks.",redapple,moisesguimaraes
2331,2016-11-30 12:19:45,"Hi @redapple , I'll try to do it until Friday. I was competing in an orienteering race last Sunday and I had an accident falling over my left arm. But I'm almost 100% right now ;)",moisesguimaraes,redapple
2331,2016-12-04 18:28:20,"Hey @moisesguimaraes,

Thanks for the change! I prefer ""(str or list or str)"" because 

1. str is also an iterable;
2. even if any iterable is supported right now it doesn't mean we should expose that; documenting it as a list looks more future-proof.",kmike,moisesguimaraes
2331,2016-12-05 09:20:39,"@moisesguimaraes yep, ""or"" was a typo :)",kmike,moisesguimaraes
2331,2016-12-05 19:33:30,"Thanks @moisesguimaraes!

There is also a note in docs (scroll above your changes) which should be removed:

> As shown in the example above, ``to`` and ``cc`` need to be lists
of email addresses, not single addresses, and even for one recipient,
i.e. ``to=""someone@example.com""`` will not work.",kmike,moisesguimaraes
2331,2016-12-07 15:07:20,LGTM. @kmike ?,redapple,kmike
2331,2016-12-07 15:08:24,Thanks @moisesguimaraes!,kmike,moisesguimaraes
2330,2016-10-18 14:35:32,"Thanks @lfmattossch !
",redapple,lfmattossch
2328,2016-11-14 14:59:25,"@stummjr I think it makes sense to remove ""While other meta keys"", or re-phrase it to something like ""While most other meta keys"" because currently it reads like ""While all other meta keys"".
",kmike,stummjr
2328,2016-11-14 16:04:41,"@kmike just updated the text there.
",stummjr,kmike
2328,2016-11-14 16:24:19,"Thanks @stummjr!
",kmike,stummjr
2327,2016-10-18 14:39:01,"Thanks @bopace ! I nitpicked a little.
",redapple,bopace
2327,2016-10-18 16:28:05,"Looks good, thanks @bopace !
",eliasdorneles,bopace
2325,2016-10-14 08:52:43,"so what's your suggestion @kmike ?
",redapple,kmike
2325,2016-10-14 09:02:34,"@redapple it seems quoting rules are different in cmd.exe (http://stackoverflow.com/questions/24173825/what-does-single-quote-do-in-windows-batch-files) - only double quotes are supported. It means that on Linux and OS X the right way is to use single quotes, and on Windows the right way is to use double quotes. My vote is to add a note about Windows and keep single quotes in examples.
",kmike,redapple
2323,2016-10-12 18:01:01,"Thanks for reporting @starrify
Is it the same as https://github.com/scrapy/scrapy/issues/2304 ?
Does https://github.com/scrapy/w3lib/pull/75 fix it?

Le 12 oct. 2016 7:11 PM, ""Pengyu CHEN"" notifications@github.com a écrit :

> _Affected version:_
> dc1f9ad
> https://github.com/scrapy/scrapy/commit/dc1f9ad211669e6a89774c69fed01567960b9a3f
> 
> _Affected Python version:_
> Python 2 only
> 
> _Steps to reproduce:_
> 
> > > > import scrapy.http>>> response = scrapy.http.TextResponse(url='http://foo.com', body=u'<a href=""http://foo\u263a"">', encoding='utf8')>>> response.css('a::attr(href)').extract()
> > > > [u'http://foo\u263a']>>> import scrapy.linkextractors>>> extractor = scrapy.linkextractors.LinkExtractor()>>> extractor.extract_links(response)
> > > > Traceback (most recent call last):
> > > >   File ""<stdin>"", line 1, in <module>
> > > >   File ""/tmp/virtualenv/src/scrapy/scrapy/linkextractors/lxmlhtml.py"", line 111, in extract_links
> > > >     all_links.extend(self._process_links(links))
> > > >   File ""/tmp/virtualenv/src/scrapy/scrapy/linkextractors/__init__.py"", line 104, in _process_links
> > > >     link.url = canonicalize_url(urlparse(link.url))
> > > >   File ""/tmp/virtualenv/lib/python2.7/site-packages/w3lib/url.py"", line 354, in canonicalize_url
> > > >     parse_url(url), encoding=encoding)
> > > >   File ""/tmp/virtualenv/lib/python2.7/site-packages/w3lib/url.py"", line 298, in _safe_ParseResult
> > > >     netloc = parts.netloc.encode('idna')
> > > >   File ""/tmp/virtualenv/lib/python2.7/encodings/idna.py"", line 164, in encode
> > > >     result.append(ToASCII(label))
> > > >   File ""/tmp/virtualenv/lib/python2.7/encodings/idna.py"", line 76, in ToASCII
> > > >     label = nameprep(label)
> > > >   File ""/tmp/virtualenv/lib/python2.7/encodings/idna.py"", line 21, in nameprep
> > > >     newlabel.append(stringprep.map_table_b2(c))
> > > >   File ""/usr/lib64/python2.7/stringprep.py"", line 197, in map_table_b2
> > > >     b = unicodedata.normalize(""NFKC"", al)TypeError: normalize() argument 2 must be unicode, not str
> 
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> https://github.com/scrapy/scrapy/issues/2323, or mute the thread
> https://github.com/notifications/unsubscribe-auth/AA2GGD4my6g33KGKtIgbtoRKnZChO4ipks5qzRSmgaJpZM4KU_lv
> .
",redapple,starrify
2320,2016-10-18 11:14:19,"@moisesguimaraes , there's already an open PR for this: https://github.com/scrapy/scrapy/pull/2327
",redapple,moisesguimaraes
2320,2016-10-18 11:16:58,"ok @redapple, thanks for the clarification =)
",moisesguimaraes,redapple
2318,2016-10-11 11:02:10,"I think this feature makes sense; there is a pull request with a different implementation and unit tests: https://github.com/scrapy/scrapy/pull/1548. What do you think @redapple?
",kmike,redapple
2318,2016-10-11 12:50:25,"Oh thanks, @kmike , I had not realized this was fixing an open issue #1287
(I was thinking it was only related to cleaning URLs from params for the GUID)
Makes sense to have it fixed indeed. (Although the extension detection in #1548 still feels hacky for me)
",redapple,kmike
2316,2016-12-01 04:19:14,"thanks @redapple , I am more and more familiar with scrapy and thanks to your help, I see exactly how I can do this.
Thanks",rolele,redapple
2315,2016-10-15 08:29:11,"@bmgraff what are you using for downloading? As @nyov said, fragment is not passed to remote website as a part of HTTP request. 

scrapy.Request also escapes ajax crawlable URLs by default (`example.com/!#foo=bar`), so fragment can be removed when Request is instantiated, you can't rely on Request(url) to preserve fragment. In [scrapy-splash](https://github.com/scrapy-plugins/scrapy-splash) there is a custom request dupefilter which takes this in account, and a workaround for passing URLs with fragments to Request; just taking fragment in account is not enough.
",kmike,nyov
2309,2016-10-29 20:03:35,"Nop.
`'IMAGES_EXPIRES': 0` means ""never cache it"" or ""always download it"".
Let's say someone downloads two or more different images from the same link in less than 24 hours... He needs to set `'IMAGES_EXPIRES': 0`.

@redapple , your previous 2nd option is out for the given reason. Agree?
",djunzu,redapple
2308,2016-10-06 08:15:23,"Hi @DharmeshPandav ,
[`ROBOTSTXT_OBEY` is still `False` by default](https://github.com/scrapy/scrapy/blob/129421c7e31b89b9b0f9c5f7d8ae59e47df36091/scrapy/settings/default_settings.py#L236). See also @kmike 's comment [here](https://github.com/scrapy/scrapy/pull/1867#issuecomment-200922527).
The note about it being `True` when using `scrapy startproject` is also [there](https://doc.scrapy.org/en/latest/topics/settings.html?#robotstxt-obey).
",redapple,DharmeshPandav
2308,2016-10-06 12:41:15,"Hi @redapple  I missed that comment, makes sens though,
In docs for 1.x channel, it says it is false by default with a note that `startproject` command will set  `ROBOTSTXT_OBEY` to  `True`

I am closing this
",DharmeshPandav,redapple
2306,2017-02-22 17:42:03,@redapple I think current code structure is fine; policies are not Scrapy extensions,kmike,redapple
2306,2017-03-01 16:57:43,"@kmike , I think I have addressed all your comments. (and rebased)",redapple,kmike
2306,2017-03-01 17:31:35,@redapple tests are failing - could you please check it?,kmike,redapple
2306,2017-03-02 15:34:54,"@kmike , done. Sorry about that.",redapple,kmike
2306,2017-03-02 23:05:23,"Looks great, thanks @redapple!",kmike,redapple
2305,2016-10-12 10:28:01,"@redapple I work around this by creating a custom pipeline for validation.

`get_unified_currency_name` is not called on a `scrapy.Field` currency.  Is like one input_processor overrides the other one.  Maybe this is a desired behaviour, but I was thinking that it could execute both of them:



but if you remove the `currency_in = MapCompose(lambda x: x[0])`, `utils.get_unified_currency_name)` will be called.
",dolohow,redapple
2303,2016-10-04 10:32:06,"@djunzu , `deny` is in essence an argument of the link extractor, to filter links from pages and only that, not an instruction on the `RedirectMiddleware`.
Having the filtering directives ""leak"" way down to `RedirectMiddleware` feels like a stretch to me.

On the other had, having more control over what `RedirectMiddleware` does with redirected requests can be improved quite a bit, I agree (cf. your comment https://github.com/scrapy/scrapy/issues/2241#issuecomment-251213143)

Therefore, regarding using `deny/allow` arguments to prevent redirected requests being parsed with `CrawlSpider`, I think this could be done with new `Rule` arguments, say, something like `redirect_allow/_deny`, that `CrawlSpider` would translate in some `meta` info for the generated `Request`, for `RedirectMiddleware` to abide by. (this would need changes to `RedirectMiddleware` of course since it only ""understands"" `dont_redirect`)

Regarding redirections being parsed by the ""wrong"" rule, it seems hard to get right for all use-cases: should it switch rule and send it to that other callback? or should you not send it to any callback?
I believe this can be implemented with a custom `CrawlSpider` subclass.
IMHO, following a link, expecting some entity A, and in the end receiving an entity B, it's more about building robust extraction than relying on too much magic from `CrawlSpider`.

All this is arguable, I'm not a heavy `CrawlSpider` myself.
",redapple,djunzu
2302,2016-10-05 12:41:02,"@redapple I'm talking about this note:

> Previous example (JsonWriterPipeline) doesn't clean up resources properly.
> Fixing it is left as an exercise for the reader.
",kmike,redapple
2300,2016-10-03 10:34:47,"Well spotted @mineo ! Thanks
",redapple,mineo
2299,2016-10-05 15:39:39,"Totally agree with @stummjr , www.domain.com is a different site that domain.com
",eLRuLL,stummjr
2293,2016-09-29 08:27:43,"Thanks @eliasdorneles !
",redapple,eliasdorneles
2292,2016-09-29 12:11:42,"Merging, thanks @stummjr !
",eliasdorneles,stummjr
2290,2016-11-24 12:45:19,"Hey @elacuesta !
Do you have that patch ready? because I worked on this this morning actually. And was about to submit a WIP PR
I opted for an ""handle statuses"" argument and not a ""redirect on or off"" one.",redapple,elacuesta
2290,2016-11-24 12:49:02,"@elacuesta , this is the patch I was referring to https://github.com/scrapy/scrapy/pull/2410",redapple,elacuesta
2290,2016-12-08 16:33:33,"@elacuesta , I updated my PR for this one at #2410. If you want to have a look and discuss",redapple,elacuesta
2289,2016-09-27 15:03:58,"@eLRuLL , `DEPTH_PRIORITY` is a setting [read by  `DepthMiddleware`](https://github.com/scrapy/scrapy/blob/ebef6d7c6dd8922210db8a4a44f48fe27ee0cd16/scrapy/spidermiddlewares/depth.py#L27), not specifically for `CrawlSpider`.
In `DepthMiddleware`, `DEPTH_PRIORITY` affects Requests priority [but with a _minus_ sign](https://github.com/scrapy/scrapy/blob/ebef6d7c6dd8922210db8a4a44f48fe27ee0cd16/scrapy/spidermiddlewares/depth.py#L36), so the documentation is correct. But I admit it can be confusing.
",redapple,eLRuLL
2287,2016-09-29 12:15:42,"Thanks @pawelmhm !
",eliasdorneles,pawelmhm
2282,2016-09-23 09:03:26,"Thanks @pawelmhm !
A handy example too
",redapple,pawelmhm
2277,2016-09-22 10:42:39,"@kmike  Right, I've seen a lack of `twisted.protocols.ftp`, is there any feedback from their end in this case?  or any hack for that, created by community? 
",mateuszdargacz,kmike
2277,2016-09-22 10:49:19,"@kmike no trace of that on their roadmap as well. I'll investigate that and try to get to the source of the issue. Thanks for your time.
",mateuszdargacz,kmike
2277,2016-10-27 14:58:14,"@mateuszdargacz , @kmike , or anyone interested in helping out,
I started porting Twisted's FTP module to Python 3.
https://github.com/twisted/twisted/compare/trunk...redapple:ftp-py3?expand=1

Kind of a slow effort because there are a lot of tests and some a tricky (for me at least) to unfail.
",redapple,kmike
2276,2016-09-23 02:25:59,"i can get links from response in `parse` method, bug how can i send it to spider and crawl regularly
@kmike
",zhengbomo,kmike
2275,2016-11-10 10:38:34,"Had missed that one. Thanks @eliasdorneles !
",redapple,eliasdorneles
2273,2016-09-20 17:16:19,"@redapple merged, I was waiting for the build, just to be sure. :)
",eliasdorneles,redapple
2273,2016-09-20 17:31:59,"Thanks @eliasdorneles 
",redapple,eliasdorneles
2272,2016-10-04 19:01:12,"Hey @redapple, I would like to work on this. Can you tell me if checking for `isinstance`  of `basestring` a good idea so I can cast the `to` argument into a `list` ?
",kirankoduru,redapple
2272,2016-10-04 21:30:32,"@redapple that's exactly what I was looking for! I will create a PR soon. Thanks :)
",kirankoduru,redapple
2272,2016-10-18 13:45:56,"hey @lfmattossch I was working last weekend. Some reasons my tests weren't passing. I was under the impression that `tox` was the command to run them. Is there any other way to run them? Also if you want to work on it please go ahead and start.
",kirankoduru,lfmattossch
2272,2016-10-18 13:58:12,"hi @kirankoduru, I'm with @lfmattossch. I'm already running the unit tests on a fix. Thanks for updating us.
",moisesguimaraes,lfmattossch
2272,2016-10-18 14:01:26,"@moisesguimaraes @lfmattossch great guys. Has the fix been pushed yet? I wonder how it was merged without passing tests.
",kirankoduru,moisesguimaraes
2272,2016-10-18 14:01:26,"@moisesguimaraes @lfmattossch great guys. Has the fix been pushed yet? I wonder how it was merged without passing tests.
",kirankoduru,lfmattossch
2271,2016-09-20 15:45:53,"@kmike , why not.
Can we log the issue as feature request and implement it after 1.2 though?
",redapple,kmike
2271,2016-09-20 15:47:36,"@redapple yep, makes sense
",kmike,redapple
2270,2016-12-09 09:42:30,"@kmike , I think so yes",redapple,kmike
2268,2016-11-23 11:18:47,"Thanks @ahlinc . Sorry for the late feedback.
Looks suspiciously harmless ""à-priori"" but as this touches the heart of the orchestration, I would ask @curita if she can comment, being the one who last touched (and refactored) this part.
@jdemaeyer may also want to check how this affects the Add-Ons project (if it does).

@curita , @jdemaeyer , would you have time to check this? Thanks",redapple,ahlinc
2260,2016-09-17 12:19:30,"Thanks @waynelovely !
",redapple,waynelovely
2258,2016-09-19 08:40:42,"Thanks @redapple!
",kmike,redapple
2253,2017-02-04 22:36:55,"@redapple I have the same thoughts as @dolohow , but I would love some enlightenment on how to do it in another way! If you know how to do it, please let me learn! ",djunzu,redapple
2252,2016-09-19 22:29:09,"@eliasdorneles I did an attempt to complete the section on data extraction. Would you mind having a look?
",stummjr,eliasdorneles
2252,2016-09-20 16:20:24,"I think this is ready for review, I've updated the description with the main changes and some explanation of what @stummjr and I had in mind while writing. :)

For better reviewing, comment in this PR and read the text here: https://github.com/eliasdorneles/scrapy/blob/tutorial-upgrades/docs/intro/tutorial.rst
",eliasdorneles,stummjr
2252,2016-09-20 18:05:00,"@redapple @stummjr I've address your comments, plus some other minor things. What do you think?
",eliasdorneles,stummjr
2252,2016-09-20 18:11:21,"@eliasdorneles LGTM 👍
",stummjr,eliasdorneles
2252,2016-09-21 22:40:04,"Besides a few minor remaining comments the PR looks good, great work @eliasdorneles and @stummjr!
",kmike,eliasdorneles
2252,2016-09-21 22:40:04,"Besides a few minor remaining comments the PR looks good, great work @eliasdorneles and @stummjr!
",kmike,stummjr
2252,2016-09-22 14:08:06,"Thanks for reviewing @kmike, I've addressed your comments and made other small changes in the latest commit.
@redapple @kmike Does this look good to merge?
",eliasdorneles,kmike
2252,2016-09-22 15:04:56,"@eliasdorneles @stummjr , I believe the tutorial now is also compatible with 1.1 branch, correct?
Worth a backport maybe?
",redapple,eliasdorneles
2252,2016-09-22 15:04:56,"@eliasdorneles @stummjr , I believe the tutorial now is also compatible with 1.1 branch, correct?
Worth a backport maybe?
",redapple,stummjr
2252,2016-09-22 17:07:20,"@eliasdorneles , could you check https://github.com/scrapy/scrapy/pull/2281 ?
I basically overwrote tutorial.rst with the content of this PR
",redapple,eliasdorneles
2246,2016-09-20 14:11:24,"@redapple probably you meant #2085? I think that the general approach outlined there is good, it's nice to have SSL on by default and be able to turn it off.
",lopuhin,redapple
2246,2016-09-20 14:16:06,"@lopuhin , yes, I meant #2085. I'm mixing PR numbers these days, I dunno why.
",redapple,lopuhin
2243,2016-09-15 11:47:40,"@redapple That fixes the problem. Thanks!
",briehanlombaard,redapple
2241,2016-09-15 09:30:29,"@pawelmhm , I also think it makes sense to have `OffsiteMiddleware` (or it's functionality) as a downloader middleware, if only to handle offsite-redirects (#15, #184)
[@pablohoffman 's comment there](https://github.com/scrapy/scrapy/issues/15#issuecomment-2048222) recommends to not change `OffsiteMiddleware` (_""avoid modifying the behavior of a well established and known middleware such as the offsite one""_), but that was 5 years ago, it may be time to revisit the position.
I believe the main reason for offsite middleware being a spider middleware is that it is conceptually spider dependent (through its  `.allowed_domains`) and not really standalone or spider-agnostic.
But of course, downloader middlewares do have access to the spiders in their methods.
Other attempts at fixing #15 have patched `RedirectMiddleware` (a downloader middleware) to merge with the offsite feature  (#1002, #1028).
Such a change on `OffsiteMiddleware` would be backwards incompatible if we provide it as downloader mdw, as those who patched it may have 2 competing mdw or a broken one  (e.g. #1640) -- to be checked.
",redapple,pawelmhm
2241,2016-09-15 12:07:26,"> Such a change on OffsiteMiddleware would be backwards incompatible if we provide it as downloader mdw, as those who patched it may have 2 competing mdw or a broken one (e.g. #1640) -- to be checked.

Yeah I do agree this is backward incompatible. Aside from things noted by @redapple it is also worth considering that users code may depend on current behavior of offsite middleware, e.g. user schedules urls to offsite urls  without having netloc in allowed_domains. This works now and maybe this is ok for some use cases (e.g. spider makes requests to example.com and also example_api.com, developer forgot to add example_api.com to allowed_domains, but since he scheduled on spider_idle it never mattered, after update suddenly only example.com urls will work and example_api.com will be filtered). 

Also worth seeing how this might affect scheduling requests from pipelines. It is possible to schedule request from pipeline, now they dont go via offsite middleware. Will they go via middleware after this change? If someone makes some requests to some common api from pipeline (e.g. translates strings via some HTTP API from pipelines or does splash rendering ) spider allowed_domains may not work properly and API url will be filtered, maybe then pipeline would have to update allowed domains? Or user would define some project wide allowed domains?
",pawelmhm,redapple
2241,2016-10-03 20:18:55,"@pawelmhm 

> e.g. spider makes requests to example.com and also example_api.com, developer forgot to add example_api.com to allowed_domains, but since he scheduled on spider_idle it never mattered, after update suddenly only example.com urls will work and example_api.com will be filtered.

I must say that this particular example sounds to me like ""relying on a 'bug' to work"".
",djunzu,pawelmhm
2238,2016-09-14 11:05:43,"@redapple 
C:\Users\Ibrahim>scrapy version -v
Scrapy    : 1.1.0
lxml      : 3.6.0.0
libxml2   : 2.9.3
Twisted   : 16.4.0
Python    : 3.5.2 |Anaconda 4.1.1 (64-bit)| (default, Jul  5 2016, 11:41:13) [MSC v.1900 64 bit (AMD64)]
pyOpenSSL : 16.0.0 (OpenSSL 1.0.2h  3 May 2016)
Platform  : Windows-10-10.0.10586-SP0
",Ibrahim2311,redapple
2236,2016-09-15 10:06:02,"Many thanks @stummjr and @eliasdorneles !
",redapple,stummjr
2233,2016-09-13 16:49:41,"@djunzu we are going to use http://quotes.toscrape.com for the tutorial and its source code is available at: https://github.com/scrapinghub/spidyquotes
",stummjr,djunzu
2232,2016-09-13 13:52:50,"oh @kmike , I missed that one.
",redapple,kmike
2229,2016-09-22 14:15:26,"It fixes it for me too, thanks @ahlinc, merging!
Could you please open another PR for those other changes, preferably describing a way to reproduce the problem it solves?
",eliasdorneles,ahlinc
2229,2016-11-29 14:54:03,"@ahlinc , this change may have re-triggered #396 
Would you mind having a look there? thanks.",redapple,ahlinc
2229,2016-11-30 08:00:50,"@redapple yes, I've tested it and confirm that the #396 bug is re-triggered by this change and the cause of the issue was that IPython Shell is a singleton. Fixed in #2418.",ahlinc,redapple
2226,2016-09-09 12:40:56,"@kmike I was thinking mostly about current scrapy versions: say we fix a bug in canonicalize_url in w3lib - at the moment we will not get the fix without fixing it in scrapy too, because a function from scrapy will be used.
Now that I wrote that, I realized that even after this change those who still use old w3lib will not get the fix. Do you think we'll want to update it in scrapy as well anyway?
",lopuhin,kmike
2225,2016-09-12 11:40:30,"Thanks @Tethik !
Can I suggest that we add tests for the different cases?
My take on this: https://github.com/Tethik/scrapy/pull/1
(room for factorization)
",redapple,Tethik
2225,2016-09-12 12:57:31,"@redapple Sounds good, I'll take a look at the your pull with the tests when I next get the time.
",Tethik,redapple
2225,2016-09-19 08:23:05,"@kmike , do you (still) agree with the change?
",redapple,kmike
2225,2016-09-19 08:39:03,"@redapple yep!
@Tethik hm, I'm not sure coverage problem is caused by generated code (but maybe it is); it looks like our testing suite has problems with subprocesses again.
",kmike,Tethik
2225,2016-09-19 08:39:03,"@redapple yep!
@Tethik hm, I'm not sure coverage problem is caused by generated code (but maybe it is); it looks like our testing suite has problems with subprocesses again.
",kmike,redapple
2225,2016-09-19 08:39:31,"Thanks @Tethik!
",kmike,Tethik
2220,2016-09-07 08:15:18,"Thanks @kmike !
",redapple,kmike
2215,2016-09-01 14:00:19,"@redapple the build has the same number of failures as my local build. The only issue is that it took 18 minutes, compared to 8 minutes for the second longest build. We could set fast-finishing https://docs.travis-ci.com/user/customizing-the-build#Fast-Finishing but it's only temporary, until we make that build required to pass.
",lopuhin,redapple
2215,2016-11-24 16:36:07,"No idea @kmike . Probably not.
I don't think we'll know unless we give it a try",redapple,kmike
2215,2017-02-01 14:52:58,"Thanks for the input @lopuhin .
I rebased the PR, and configured tox to not run code coverage for pypy.
It's now taking just under 11 minutes: https://travis-ci.org/scrapy/scrapy/builds/197310281",redapple,lopuhin
2215,2017-02-01 16:23:47,"@redapple oh, this is great, so it's even not the slowest build now! Thanks for fixing that! 👍 ",lopuhin,redapple
2215,2017-02-02 21:39:52,Thanks @redapple and @lopuhin! Let's enable these tests :),kmike,redapple
2215,2017-02-02 21:39:52,Thanks @redapple and @lopuhin! Let's enable these tests :),kmike,lopuhin
2213,2016-09-01 10:32:50,"@lopuhin , what do you think of adding PyPy build to Travis and adding it to allowed failures?
",redapple,lopuhin
2213,2016-09-01 13:15:30,"@redapple it's possible to do it now, but I'd rather do it after merge of #2030, unless we want to fix pypy issues in it before the merge. I think turning it on after #2030 is less work overall, because although that PR brings some failing tests, it still reduces overall number of pypy failures, including many crawler and spider tests. 
",lopuhin,redapple
2213,2016-09-01 13:17:36,"@redapple I think I misunderstood you - ""adding it to allowed failures"" means we'll know the status of pypy tests, but the build will not fail if there are pypy failures? If that is correct then yes, I'm +1!
",lopuhin,redapple
2213,2016-09-01 13:19:11,"@lopuhin , correct, this will allow running the tests, monitor progress with PyPy, while still having the greenlight from Travis (assuming other Py2.7..Py3.5 tests pass)
",redapple,lopuhin
2213,2016-09-05 11:29:51,"@lopuhin , what's the input for the non-ASCII domain test?
",redapple,lopuhin
2213,2016-09-09 12:50:30,"@redapple sorry for the delay: it turns out it's not limited to just PyPy, CPython 2.7 also has this issue:



And it looks that #1949 covers that, thanks for the pointer!
",lopuhin,redapple
2212,2016-09-19 18:17:56,"@redapple sorry for delay.
",dangra,redapple
2212,2016-09-20 08:53:30,"@dangra , where do you see this mention? release notes? installation guide?
",redapple,dangra
2212,2016-09-21 13:42:45,"Release notes for sure. It has to be clear in installation guide what
distribution versions it is supported out of the box and what ""may work"".

On Tue, Sep 20, 2016 at 5:53 AM Paul Tremberth notifications@github.com
wrote:

> @dangra https://github.com/dangra , where do you see this mention?
> release notes? installation guide?
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/scrapy/scrapy/pull/2212#issuecomment-248243002, or mute
> the thread
> https://github.com/notifications/unsubscribe-auth/AACR-W__kGsMpHRs3DXbXoTEZp4Kl22bks5qr58LgaJpZM4Jycof
> .
",dangra,dangra
2203,2016-08-30 05:22:57,"I install scrapy using pip command.
When I import lxml.html in python  shell, it works. @redapple 
",shahidkarimi,redapple
2202,2016-08-31 11:53:13,"@redapple a good idea, +1
",kmike,redapple
2202,2016-09-19 10:31:04,"@kmike , I opened https://github.com/scrapy/scrapy/issues/2262 to track the improvement discussed above.
",redapple,kmike
2199,2016-08-25 08:56:59,"@kmike version 1.1.1   today i get the same in version 1.1.2
",Canidy,kmike
2199,2016-08-30 07:35:12,"@redapple  the urls have no record while errors occur, so  i just comment the lines to ignore the retry logging ,



now it feel better.
",Canidy,redapple
2198,2016-08-25 09:27:31,"@redapple I actually didn't try that version until now, but it does seem to work as well. So the problem must be somewhere between 1.1.0 and 1.1.1 
",briehanlombaard,redapple
2198,2016-09-09 22:08:44,"@redapple , I am not sure it is a bug. It was coded to work just like that. See [this test](https://github.com/scrapy/scrapy/blob/master/tests/test_pipeline_images.py#L324). I don't remember details, but if I am not wrong it was coded in this way to accommodate class attributes in custom ImagesPipelines.

I think that in order to do this given example work it would be necessary some backward incompatibility with class attributes. And, as far as we know, there is no backward incompatibility with class attributes now.

Maybe the way to go now should be just improve the docs and better explain how to create/use custom pipelines. (And accept this little backward incompatibility as it is.)

@briehanlombaard , you should precede `IMAGES_THUMBS` with your custom ImagesPipeline class name:


",djunzu,redapple
2197,2016-08-24 16:54:44,"Nice catch, thanks @thomdixon!
",kmike,thomdixon
2192,2016-08-23 14:40:10,"@pawelmhm , I like the idea of that option. it could go into scrapy-plugins, but it could be a nice-to-have, default-off built-in middleware
",redapple,pawelmhm
2190,2016-08-31 09:51:18,"@stummjr , let's make it another PR for bot name.
",redapple,stummjr
2187,2016-08-19 14:24:22,"@kmike , as long as we keep `MEMUSAGE_LIMIT_MB` default to 0, I like the idea.
I'm sensing this is related to https://github.com/scrapy/scrapy/issues/2173 , or?
By the way, `MEMUSAGE_REPORT` [doesn't seem to do anything](https://github.com/scrapy/scrapy/blob/ebef6d7c6dd8922210db8a4a44f48fe27ee0cd16/scrapy/extensions/memusage.py#L38).

As a side note, we need to do a better job at reporting memory usage when limit is reached, it seems the extension only prints the engine status when email notificaations are on.
",redapple,kmike
2187,2016-09-19 09:29:30,"@redapple yep, it is related to #2173 - it helps with ad-hoc monitoring of the crawls by looking at the logs while the spider is still crawling.
",kmike,redapple
2177,2016-08-13 02:05:51,"Hi @redapple ,

Yes I understand this case right now, so we can get more control in scrapy shell, redirections still works in scraping.

Thanks a lot for your help.
",lhuaizhong,redapple
2177,2016-09-28 12:31:06,"Hm, since most expectations for Scrapy are based on what a browser would do, I would think that _NOT_ following a redirect should be the exceptional behavior here (when you'd have to create a custom Request object).

I agree with @kmike that it makes sense to handle redirections in scrapy shell by default.
",eliasdorneles,kmike
2177,2016-09-28 13:32:30,"I also agree that it makes sense to handle redirections in the general and simple case of `fetch(url)`.
It's technically a new feature (as there is explicit -- legacy -- code to handle all HTTP status codes) and backward incompatible though.
@eliasdorneles , do you mind opening an issue for this new `fetch(url)` behavior?
",redapple,eliasdorneles
2176,2016-08-16 16:18:57,"@podolskyi  Thanks for that info and i have tried using the Crawler,CrawlerProcess,CrawlerRunner methods to run the Spiders but i end up with the ReactorNotStartable Error all the time which i'm really confused on how to handle the reactor when the Spider runs on repeated basis like

Assume i have the following



@redapple Thanks for your reply and here are the files info

scrapy.cfg



settings.py --- Nothing changed in this file.

@redapple I'm more interested in doing the API Crawler way of doing but as said i'm more confused on the reactor handling and which Class i need to use to run the Spiders on regular intervals.Any Preferred Code Sample or gist on how to do it will be really helpful.
",kgrvamsi,redapple
2176,2016-08-17 19:48:49,"@podolskyi @redapple Thanks for your prompt responses and i regret the decision of running scrapy in a microservice model with Nameko framework and now needs to check how i can make my own microservice implementation with the thin Frameworks like Flask.
",kgrvamsi,redapple
2176,2016-08-17 20:26:36,"@kmike  Thanks for the Info and does the code out there in those url works with the Scrapy 1.0.5 version or they are any version specific code out there?
",kgrvamsi,kmike
2175,2016-08-12 04:33:55,"Hey @ArturGaspar! What do you think about moving `data:` URIs parsing to w3lib and making it independent of scrapy Response / Request?
",kmike,ArturGaspar
2175,2016-08-12 05:26:57,"Agreed, that makes more sense, @kmike.
",ArturGaspar,kmike
2168,2016-08-09 06:17:24,"@ashkulz: it is good to have a readable commits history, thanks for taking time to do the rebase! 

Currently we are not too strict on rebase vs extra commits - sometimes we ask for a rebase when a commit history gets messy, but not always. Probably that's why it is not in contributing.rst - we haven't agreed on a policy, it is just a common sense now. 
",kmike,ashkulz
2168,2016-08-16 12:10:42,"@kmike: sorry for the delay. Is there a necessity to keep `_safe_chars` and `_unquotepath`, as those are supposed to be internal interfaces for which [backwards compatibility may not apply](https://www.python.org/dev/peps/pep-0008/#public-and-internal-interfaces)?
",ashkulz,kmike
2168,2016-08-16 12:40:43,"@ashkulz , we sometimes break this guideline to be on the safe side with what users import.
We may have to live with this for some time.
Another thing to do (in another PR) could be to add a warning in the release notes that a later version may not expose `_safe_chars` and `_unquotepath`.
",redapple,ashkulz
2168,2016-08-16 13:38:49,"@redapple: any other feedback? I've already pushed a rebased commit which includes this change, unless you want me to include an entry in `docs/news.rst` in this PR itself.
",ashkulz,redapple
2168,2016-08-16 14:57:50,"@ashkulz , no other feedback. Looks good to me!
Thanks again.
",redapple,ashkulz
2168,2016-08-16 14:59:52,"Thanks @ashkulz!
",kmike,ashkulz
2167,2016-08-08 05:17:33,"Hi @kmike ,

I'd like to record origin requests so that:
- Make it possible to analyse request info, sometimes url is needed and sometimes referer is.
- Make it possible to recrawl the page if needed.

For this use case, `request_to_dict` comes to my mind.

While I'm used to checking fields scraped by `scrapy check` command and found it failed in the function.
",cockcrow,kmike
2166,2016-08-11 10:17:23,"Thanks @ashkulz !
I made a few comments.
",redapple,ashkulz
2166,2016-08-16 11:59:55,"@redapple: I've used the syntax for referring to issues and have reworded the installation instructions so that the specific system packages are mentioned explicitly. Let me know if you have any further feedback.
",ashkulz,redapple
2166,2016-08-16 12:43:23,"LGTM. Thanks @ashkulz 
",redapple,ashkulz
2166,2016-09-20 09:20:08,"@kmike , @dangra , @eliasdorneles ,
I've created https://github.com/scrapy/scrapy/pull/2267 with @ashkulz 's change to add installation guidelines (cf. https://github.com/scrapy/scrapy/pull/2166#r79451738).
It's using a new branch on the scrapy repo, `deprecate-ubuntu-packages`, so that you can commit any changes directly.
",redapple,ashkulz
2165,2016-08-04 18:44:42,"Looks great, thank you very much @loreguerra !
",eliasdorneles,loreguerra
2165,2016-08-04 18:59:03,"@loreguerra btw, could you include the vector file too?
Just in case we need to update it someday. =)
",eliasdorneles,loreguerra
2165,2016-08-06 01:46:23,"Awesome, thanks @loreguerra!
",kmike,loreguerra
2162,2016-09-02 10:12:21,"@redapple
Why Scrapy does not decompress responses with `Content-Encoding: gzip`?
I use `scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware`

> This middleware allows compressed (gzip, deflate) traffic to be sent/received from web sites
",tonal,redapple
2159,2017-02-02 16:37:24,"@redapple @eliasdorneles this is all voodoo for me, feel free to merge :)",kmike,redapple
2159,2017-02-02 16:37:24,"@redapple @eliasdorneles this is all voodoo for me, feel free to merge :)",kmike,eliasdorneles
2156,2016-08-02 17:03:42,"Nice project, @ArturGaspar !
Yeah, that download handler looks useful, I think it would be valuable in Scrapy too.

I don't quite follow all those tricky regexes, wouldn't it be clearer if they were spelled out?
",eliasdorneles,ArturGaspar
2156,2016-08-11 08:56:53,"Thanks @ArturGaspar!
I second @eliasdorneles on this, I'd also like to see this in scrapy core.
Waiting for the PR :)
",redapple,ArturGaspar
2156,2016-08-11 08:56:53,"Thanks @ArturGaspar!
I second @eliasdorneles on this, I'd also like to see this in scrapy core.
Waiting for the PR :)
",redapple,eliasdorneles
2155,2016-08-11 08:49:36,"@lucab0ni , @pawelmhm , as a side note,
while at it, this telnet extension could be moved to scrapy-plugins.
what do you think? cc @kmike , @dangra , @eliasdorneles 
",redapple,pawelmhm
2155,2016-08-11 13:03:46,"@redapple would the idea be to remove the feature from Scrapy or would Scrapy depend on it?

If the idea is to remove the feature from Scrapy, +1.

If the idea is to make Scrapy depend on it, it would make only sense if the code can be made completely independent of Scrapy (to avoid a cyclic dependency case, where Scrapy depends on it and it depends on Scrapy), and I'm not sure that is possible.
",eliasdorneles,redapple
2155,2016-08-18 06:59:14,"@eliasdorneles it just worked fine. That fixed the problem.
",lucab0ni,eliasdorneles
2155,2016-08-22 18:34:45,"@redapple, @kmike, et al: What about moving telnet to scrapy-plugins anyway, and make it a ""soft-hard"" dependency? I'm still in favor of decoupling it.
It could still be added as a requirement for installation, or in case it isn't available a big User Warning could be printed at startup.
Something like ""TELNET_ENABLED defaults to True, but scrapy-telnet could not be loaded. You will not be able to debug your Spider in this run, if you continue."" ?
",nyov,redapple
2155,2016-08-22 18:34:45,"@redapple, @kmike, et al: What about moving telnet to scrapy-plugins anyway, and make it a ""soft-hard"" dependency? I'm still in favor of decoupling it.
It could still be added as a requirement for installation, or in case it isn't available a big User Warning could be printed at startup.
Something like ""TELNET_ENABLED defaults to True, but scrapy-telnet could not be loaded. You will not be able to debug your Spider in this run, if you continue."" ?
",nyov,kmike
2154,2016-08-22 19:00:02,"I am not sure this is a bug?
Usually in HTML/XML, `<` can not occur unescaped, it should be `&laquo;` or entity-encoded, so perhaps the parser considers it an invalid start tag in the code and eats it.
Maybe @redapple has some version or workaround of `lxml` to relax the parsing there?

Perhaps there is some way to configure `lxml.html.HTMLParser` to handle such issues, or perhaps it's something with debian's packaging of libxml2.
I don't think that the difference of behavior would be in a minor version of libxml (2 to 3), but I could be wrong.

FWIW, I'm also on debian with libxml 2.9.2



and I get the same results:


",nyov,redapple
2154,2016-08-23 11:53:07,"@mayouf, @nyov , I don't know what's different with my setup.

I'm pasting console logs from a fresh Python 2.7 virtualenv creation, pip install of scrapy and scrapy shell session:




",redapple,nyov
2154,2016-08-23 18:55:10,"@redapple, the virtualenv version works as you posted it. So I would assume at this point that debian's libxml2 version is responsible.
",nyov,redapple
2153,2016-08-01 00:33:46,"Looks good, thanks @Digenis !
",eliasdorneles,Digenis
2152,2016-07-29 17:32:56,"@eliasdorneles , thanks, it's a good start :)
Let's see how it goes for master branch after 1.2 release.
",redapple,eliasdorneles
2149,2017-01-13 14:34:03,"@redapple I apologize for the confusion, this was entirely our fault. What happened was that we're running scrapyd via supervisord and we had logging enable via supervisord that I was not aware of.",dan-blanchard,redapple
2148,2016-09-15 10:46:59,"@sbarratt , as @Digenis points out, the docs do mention ""to"" and ""cc"" should be lists.
I've opened https://github.com/scrapy/scrapy/issues/2244 to improve the docs.
",redapple,Digenis
2146,2016-07-27 01:21:59,"@eLRuLL this usage of `text` argument was never intended AFAIK. You can pass url in `base_url` argument: scrapy.Selector wraps [parsel.Selector](https://github.com/scrapy/parsel/blob/2060660dd3f331e410b740bc55a3879eaf984eef/parsel/selector.py#L141) to 1) provide backwards compatibility and 2) an ability to pass response instead of text.
",kmike,eLRuLL
2146,2016-07-29 14:46:11,"@eLRuLL, use `response.replace(body=...`.
I can only think of 3 reasons one would use separate selectors for different segments:
The document size is impacting performance and you can more efficiently parse only a fragment.
You want to modify parts of the document.
Most of the mark-up is broken but the interesting segments can be parsed.
Does yours fall into one of them?

@kmike's concern is valid. Here's a patch: https://github.com/Digenis/scrapy/commit/b57de97a22911c1c77fbefef45450f0cfadaf120
I find it a bit verbose, I typically just do an assertion,
let the traceback lead to some head scratching
and blame python.
Wouldn't it be better?
",Digenis,eLRuLL
2146,2016-07-29 14:46:11,"@eLRuLL, use `response.replace(body=...`.
I can only think of 3 reasons one would use separate selectors for different segments:
The document size is impacting performance and you can more efficiently parse only a fragment.
You want to modify parts of the document.
Most of the mark-up is broken but the interesting segments can be parsed.
Does yours fall into one of them?

@kmike's concern is valid. Here's a patch: https://github.com/Digenis/scrapy/commit/b57de97a22911c1c77fbefef45450f0cfadaf120
I find it a bit verbose, I typically just do an assertion,
let the traceback lead to some head scratching
and blame python.
Wouldn't it be better?
",Digenis,kmike
2146,2016-07-29 16:30:37,"@Digenis your patch looks fine. 
",kmike,Digenis
2146,2016-08-01 01:29:40,"Fixed by #2153 - thanks @Digenis!
",kmike,Digenis
2146,2016-08-03 14:26:41,"thanks @Digenis , yeah mine was the first:

> The document size is impacting performance and you can more efficiently parse only a fragment.

I haven't tried it but I think using separate responses could do it
",eLRuLL,Digenis
2145,2016-07-26 15:47:29,"I just tried it, @kmike. The error persists.
From what I examined, I guess it stems from `HttpCompressionMiddleware` trying to decode a response with no body. The decoding itself goes fine, but there's something in the logic for guessing the response type that breaks. I added a `if response.status == 303: return response` in there and it worked...
",barraponto,kmike
2144,2016-07-29 07:51:39,"hey @darshanime,

I used to be (and mostly still am) a big PEP8 nitpicker, but I really enjoyed this talk by Raymond Hettinger: https://www.youtube.com/watch?v=wf-BqAjZb8M. Style guides are meant to increase code readability and not maintenance overhead. In particular, there is a reason that we explicitly allow lines to be longer than 79 characters.

For the other points, I think they are ""fix-worthy"" though not high priority. If you feel like making a patch, go for it! Putting PEP8 checks (with the requirement on line length dropped/adjusted) is also a good move in my opinion.
",jdemaeyer,darshanime
2140,2016-08-01 14:29:27,"@jesuslosada , I agree that `IMAGES_EXPIRES` should be `90`, and if it's not, the tests are missing one case.
But I admit I'm having a hard time re-reading https://github.com/scrapy/scrapy/pull/1989 and especially https://github.com/scrapy/scrapy/pull/1989/files#diff-0441c35e69fd9bb28942a9b2c47dc565R43 where not only `IMAGES_EXPIRES` but `MIN_WIDTH`, `MIN_HEIGHT` and others set to `0`
Maybe @pawelmhm can comment on this one.
",redapple,jesuslosada
2140,2016-08-01 15:59:06,"Thanks for the feedback @pawelmhm !
",redapple,pawelmhm
2138,2016-07-26 10:50:36,"Thanks a bunch @jesuslosada !
",redapple,jesuslosada
2136,2016-07-21 15:18:04,"@redapple Thank you for the very prompt reply. `pip install scrapy scrapyd` works fine indeed. 

I just had the impression from the document (http://scrapyd.readthedocs.io/en/stable/install.html#installing-scrapyd-in-ubuntu) that the official repo is better suited for deployment on a server than `pip`. I did not know the repo was left dead. 

Should I close this issue?
",hieu-n,redapple
2131,2016-07-19 00:06:23,"@redapple I do post the question to the [ In scrapy-users mailing list ](https://groups.google.com/forum/#!topic/scrapy-users/ZQv8aa6diZc) and [Stackoverflow](http://stackoverflow.com/questions/38432786/scrapy-a-weird-bug-code-that-cant-call-pipeline)
Maybe it is a hard question, nobody can answer.
",fusae,redapple
2126,2016-07-15 14:39:44,"@redapple Thank you for your suggestion, I just recheck and apparently my mistake when implement the parse.
",hugo53,redapple
2124,2017-03-02 11:54:49,"@elacuesta , it looks like I missed your notification.
I'm not familiar with cookies middleware so I need to read your debugging very carefully.",redapple,elacuesta
2120,2016-07-12 20:05:45,"Thanks @kas ! :)
",eliasdorneles,kas
2116,2016-07-20 09:50:19,"@redapple  I think I found it. I'm calling a LUA script as follows: 
request = SplashRequest(url, callback=self.parser_somename, endpoint='execute',args={'lua_source': SPLASH_SCRIPT})

and the lua script:

function main(splash)
    splash.resource_timeout = 200.0
    assert(splash:go(splash.args.url))
    splash:wait(15)
    splash:runjs('SOMEJAVASCRIPT')
    splash:wait(1)
    return splash:html()
end

@redapple  could it be I need to add something extra parameter back and forth so the links don't get messed up? 

Thanks for your help, please can you send an account manager's email address from scrapinghub so I can get a quote for a few 1000 scrapers? 

Cheers
",yssvic,redapple
2116,2016-07-26 06:51:02,"@redapple @kmike Hello, I fixed it by grabbing the url data from 

item['debugsplash'] = response.meta['splash']['args']['url']

This was 100% always the correct url. I haven't figured it out why response.url is giving me the wrong one sometimes because I also have this problem with scrapers who don't use scrapy-splash and are just scraping pure html without any splash or lua scripting. 

Thanks for the help, I've submitted quote request. 

Cheers, 
",yssvic,redapple
2116,2016-07-26 06:51:02,"@redapple @kmike Hello, I fixed it by grabbing the url data from 

item['debugsplash'] = response.meta['splash']['args']['url']

This was 100% always the correct url. I haven't figured it out why response.url is giving me the wrong one sometimes because I also have this problem with scrapers who don't use scrapy-splash and are just scraping pure html without any splash or lua scripting. 

Thanks for the help, I've submitted quote request. 

Cheers, 
",yssvic,kmike
2116,2016-07-26 07:02:43,"@kmike  It's on my list to do to allocate time to see what's exactly going on. I need to finish some spiders that needs to go in production first :-) 

Thanks for your help kmike, I've read so much posts from you over the years and that helped me a lot. 

Cheers
",yssvic,kmike
2105,2016-07-09 07:30:22,"thanks Sir @redapple  it worked .... how to install it for Python2
",euler16,redapple
2095,2016-07-06 13:41:04,"+1 @kmike , I've also forced `pytest-cov<2.3` locally and got rid of `Coverage.py warning: No data was collected.` message I was seeing
",redapple,kmike
2095,2016-07-06 14:07:11,"@redapple can we merge it?
",kmike,redapple
2091,2016-07-05 20:40:57,"@eLRuLL , I suspect this is related to https://github.com/scrapy/scrapy/issues/2088, right?
",redapple,eLRuLL
2089,2016-07-06 06:45:50,"@redapple Sure, i have :)
Simplest sample is to define `FEED_URI` with some setting (let's say `undefined_property`) and not define it within spider itself. For instance, `FEED_URU = s3://%(undefined_bucket)s/file.json`. In case spider (or `FEED_URI_PARAMS` setting) lacks such settings _all_ signal handlers would fail at runtime.

Proper (in my opinion) implementation is to check it during init and do not turn it on if misconfigured.

I just don't have any idea (for now) how to resolve it as `open_spider` isn't wrapped with try-catch block and it doesn't make any sense to throw it (otherwise, i'd submit PR and not an issue, it's just a reminder for myself to do it later). 
",rampage644,redapple
2085,2016-07-05 16:50:13,"@nyov , @wearpants, I agree with you here, i.e. enable SSL by default and have a new setting to turn it off for those wanting this (and wanting a backward compatible setup).

We've done something similar with ACL policy for 1.1, writing this in the [release notes](http://doc.scrapy.org/en/latest/news.html):

> When uploading files or images to S3 (with FilesPipeline or ImagesPipeline), the default ACL policy is now “private” instead of “public” Warning: backwards incompatible!. You can use FILES_STORE_S3_ACL to change it.

With proper warning note in the changelog, I think it's fine.
",redapple,nyov
2085,2016-09-15 11:13:21,"@maksimbormot , can you implement @kmike 's suggestion of having `AWS_SECURE_CONNECTION` as a setting instead?
",redapple,kmike
2085,2016-10-28 12:03:22,"@redapple lets give someone a chance to do it lol
",maksimbormot,redapple
2085,2016-10-28 20:14:37,"@redapple I’m up for it and will make the change needed.
",korsunowk,redapple
2082,2016-08-09 06:46:44,"Hey @foromer4,

It looks better. I think docs are still needed: this feature should be mentioned here: https://github.com/scrapy/scrapy/blob/master/docs/topics/request-response.rst
",kmike,foromer4
2082,2017-02-15 22:45:50,"Looks good 👍 
@foromer4 could you please sqash the commits?",kmike,foromer4
2082,2017-02-16 10:42:05,@foromer4 something went wrong - there is a lot of unrelated commits in this PR. Could you please rebase it on master?,kmike,foromer4
2082,2017-02-20 14:42:59,@redapple let's try it :),kmike,redapple
2082,2017-02-20 14:43:20,Thanks @foromer4 for the persistence!,kmike,foromer4
2078,2016-06-28 08:26:06,"@redapple sry for my mistake. 
",woshichuanqilz,redapple
2077,2016-06-24 12:00:13,"@lopuhin, I'm using 100 `CONCURRENT_REQUESTS` setting and able to get 1200 rpm with 1 unit.
On startup `top` shows 60M rss size, in 30 minutes it grows up to 300M
",rampage644,lopuhin
2077,2016-06-24 12:02:59,"@lopuhin It will continue to grow until it gets killed :)
",rampage644,lopuhin
2077,2016-06-25 05:20:38,"@kmike Thanks for digging in!

This is the exact version of spider i'm using (forgot to prune it back after some changes).
I've measured `Dupefilter` object memory usage and it was ~250 MB per 2M links. My understanding is that it has impact on memory usage but it's not a major one (at least)
In my experiments I'm seeing `len(engine.slot.scheduler.dqs) == 0` and `len(engine.slot.scheduler.mqs) == 0` (possibly because there is and inherent _queue_ -> generator). So queue should't have memory impact also.
",rampage644,kmike
2077,2016-06-27 12:46:28,"@lopuhin @kmike 
200k urls is my error - i've removed initial seed file for 2M links. Spider uses 200k seed file. That's why you're all seeing 172-180k links.

I have another idea in my mind: could it be possible stacktraces are stored somewhere? There are lot's of errors due to bad links: dns resolution, timeouts, etc.
",rampage644,lopuhin
2077,2016-06-27 12:46:28,"@lopuhin @kmike 
200k urls is my error - i've removed initial seed file for 2M links. Spider uses 200k seed file. That's why you're all seeing 172-180k links.

I have another idea in my mind: could it be possible stacktraces are stored somewhere? There are lot's of errors due to bad links: dns resolution, timeouts, etc.
",rampage644,kmike
2077,2016-06-30 09:24:48,"I've done some more experiments t pin down what is causing a leak:
1. First I removed all links that caused error messages in log (because of downloader errors). Good news are that memory footprint was reduced from 440MB to 300MB at peak (according to stats). Bad news are it's still there. (Error entries in log count reduced from 20k to 2k).
2. Second. Long in the past i noticed that sometimes `Requests` objects are stuck somewhere (according to `prefs()` output and `live_refs` info). There is a pattern here. `pprint.pprint(map(lambda x: (x[0], time.time()-x[1]), sorted(rqs.items(), key=operator.itemgetter(1))))`   prints requests objects sorted by their creation time. Once `Requests` object start staying alive a group of them with pretty same time (>60s) appear in a tracking dict. That could happen multiple time, i.e. multiple groups.
3. Third finding: While working on some focused crawl spider and trying to use guppy it shows nothing interesting: str, tuples, dict. But here i get bunch of `twisted` objects staying in memory (maybe they are present in other spiders, but here there are much more of them):



I'm going to try @kmike advice  regarding `tracemalloc` module.
",rampage644,kmike
2077,2016-06-30 09:59:50,"@lopuhin: What info do you want to get about requests? I', getting request object itself with `live_refs[scrapy.http.request.Request]` as well as creation time.
",rampage644,lopuhin
2073,2016-06-27 13:44:02,"Thanks @vinayan3 .
@rgtk , are you ok with closing this issue?
",redapple,rgtk
2070,2016-06-22 12:52:56,"@redapple Thanks for the response! Here's what I'm seeing:


",kiral26,redapple
2069,2016-06-21 14:35:37,"@redapple do you know why are tests failng in Python 3.5?
",kmike,redapple
2069,2016-06-21 14:50:38,"@kmike , hm, I don't know, the tests pass for me locally.
Maybe something to do with that ""no-such-domain.nosuch:443"" sent as response body
",redapple,kmike
2069,2016-06-21 15:27:34,"@kmike , test is passing but we should really strengthen these proxy tests
",redapple,kmike
2067,2016-07-04 22:06:43,"@nyov so do you propose to remove .gz, .bz2, .xz, but add tar.(gz|bz2|xz) ?
",anatolykazantsev,nyov
2063,2016-06-20 11:25:24,"Thanks @DharmeshPandav  for reminding us of this issue.
It's the same problem as @juraseg commented on in https://github.com/scrapy/scrapy/issues/951#issuecomment-180249747
For http://www.amazon.fr/sitemaps.f3053414d236e84.SitemapIndex_0.xml.gz for example you get



from the web server, with `HttpCompressionMiddleware` decompressing the response based on Content-Encoding being ""gzip"".

When reading IANA's page on [`application/octet-stream`](https://www.iana.org/assignments/media-types/application/octet-stream)

> The recommended action for an implementation that receives an
> ""application/octet-stream"" entity is to simply offer to put the data
> in a file, with any Content-Transfer-Encoding undone, or perhaps to
> use it as input to a user-specified process.

I'd go for considering `Content-Type: binary/octet-stream` the same way, taking precedence over `Content-Encoding` (for the same reasons as in https://github.com/scrapy/scrapy/issues/193#issuecomment-12845302), i.e. not try to decompress it at `HttpCompressionMiddleware` level as a gzipped HTML/XML/JSON... response, but like a (compressed) file for another layer to interpret (here, `SitemapSpider`)
",redapple,DharmeshPandav
2060,2016-06-17 10:11:49,"thanks @pawelmhm ,
yes, I think we need to update installation guide and also update [the FAQ entry](http://doc.scrapy.org/en/latest/faq.html#what-python-versions-does-scrapy-support) on Python 3. we never know on which page users land from a Google search
",redapple,pawelmhm
2060,2016-06-20 09:14:26,"Thanks @pawelmhm , added a few comments.
",redapple,pawelmhm
2060,2016-07-05 09:17:44,"Thanks @pawelmhm !
",redapple,pawelmhm
2059,2016-06-17 06:57:14,"Hi, @kmike . As you can see from your link it states 

> Don’t use the python-scrapy package provided by Ubuntu, they are typically too old and slow to catch up with latest Scrapy.

And from same link I used link in initial post http://doc.scrapy.org/en/master/topics/ubuntu.html#topics-ubuntu

> Instead, use the official Ubuntu Packages, which already solve all dependencies for you and are continuously updated with the latest bug fixes.

From Scrapy faq http://doc.scrapy.org/en/latest/faq.html you can find python versions supported

> What Python versions does Scrapy support?
> Scrapy is supported under Python 2.7 and Python 3.3+. Python 2.6 support was dropped starting at Scrapy 0.20. Python 3 support was added in Scrapy 1.1.

I know this not true for Windows though. But this topic about ubuntu 16.04. Here you can find previous discussion http://stackoverflow.com/questions/37834330/scrapy-setup-ubuntu-16-04-or-any-other/37835006?noredirect=1#comment63167008_37835006

paul trmbrth asked there to report about steps I did. So here I am.
",pavelmorozov,kmike
2059,2016-06-19 04:10:01,"@redapple 
Thanks for your help. Though cannot check fast. My time limit for this task ended, but I think will give it a try next month. I work with scrapers often, and looking for frameworks for faster development. Scrapy probably something interesting, as I read basic concept.
",pavelmorozov,redapple
2059,2016-06-28 13:42:00,"@redapple , sorry for the late responding.
 I could fix it by fully unninstaling the package and installing it again.
Thanks !
",Draidel,redapple
2058,2016-07-22 12:19:11,"hi @dalleng , have you have time to check @kmike 's comment?
",redapple,dalleng
2058,2016-07-22 12:19:11,"hi @dalleng , have you have time to check @kmike 's comment?
",redapple,kmike
2058,2016-07-22 14:37:23,"@redapple Yes, just haven't had much spare time recently.

@kmike On a more general sense there's no way to serialize to JSON and get a set back when deserializing, so that's probably why this feature is not already in the json module. I added this PR because using a set to eliminate duplicates is a common python pattern and thinking of items export as the intended use case.
",dalleng,redapple
2058,2016-07-22 14:37:23,"@redapple Yes, just haven't had much spare time recently.

@kmike On a more general sense there's no way to serialize to JSON and get a set back when deserializing, so that's probably why this feature is not already in the json module. I added this PR because using a set to eliminate duplicates is a common python pattern and thinking of items export as the intended use case.
",dalleng,kmike
2058,2016-08-12 16:29:23,"Thanks @dalleng!
",kmike,dalleng
2052,2016-07-05 13:42:42,"@stummjr looks good!
Can you update the docs?
Will be ready to merge after that. :)
",eliasdorneles,stummjr
2052,2016-07-06 21:01:28,"Thanks @stummjr !
",eliasdorneles,stummjr
2051,2016-09-19 09:15:42,"@redapple ok,thanks
In fact , my code is the definition of download middleware, when httpcode=403 , the router automatically reconnect short net
[mycode](https://github.com/kimg1234/zhihuCollec/blob/master/zhihu/middlewares.py)
",kimg1234,redapple
2050,2016-06-14 12:21:01,"@redapple Alright, I'll look into it. Good catch on the gzipppp
",Tethik,redapple
2050,2016-06-14 14:01:21,"Looks good to me.
Thanks @Tethik!
",redapple,Tethik
2049,2016-07-08 11:51:49,"Fixed in #2050!
@Tethik @robsonpeixoto @redapple thanks fellows!
",eliasdorneles,Tethik
2048,2016-06-14 17:45:19,"Looks good, thanks @redapple!
",kmike,redapple
2046,2016-06-09 17:47:32,"@kmike yeah, that's a great tool, but I'm talking about spider's level. or cookiecutter can be used in this direction too?
",DrJackilD,kmike
2046,2016-06-09 18:24:17,"@kmike Ok, I'll make PR with some explanation about .tmpl files and necessary arguments, which need to be in templates. Thanks for the discussion :)
",DrJackilD,kmike
2044,2016-06-09 10:24:04,"@kmike, thanks for more information.

I personally don't like the idea implementing `from_crawler` factory for all classes but most people seem to be OK with it. 
However, I'd like to get this done. :) PR you mentioned wasn't merged and discussion has stopped. Is it possible to get this merged if I switch to `from_crawler` methods and not regular ctors?
",rampage644,kmike
2036,2016-06-07 10:11:50,"@lopuhin: yes, you're right. Will update description soon.
",rampage644,lopuhin
2034,2016-06-07 14:20:13,"@dracony , thanks for this.
how will people use this? use their own JSON exporter subclass?
if yes, how about introducing a setting for this? it could be simpler.
Also, [@mgachhui was suggesting](https://github.com/scrapy/scrapy/issues/1965#issuecomment-218077754) having an encoding parameter, not only default UTF-8
(and applying this to XML exports too)
",redapple,dracony
2034,2016-06-08 15:32:43,"@redapple Yay, I made it work. 
Like it more now?
",dracony,redapple
2034,2016-06-15 09:58:57,"@redapple True, it seems I overcomplicated it a bit when I was trying to make the test pass. I removed those lines and the tests still pass 👍 
",dracony,redapple
2034,2016-06-20 17:06:50,"@redapple @robsonpeixoto Ping =) 
Can this get merged now?
",dracony,redapple
2034,2016-07-04 17:57:31,"Hey @dracony -- sorry for the delay in reviewing, I've made some comments above.

The main thing not clear to me are the changes in the CSV exporter -- is line buffering really needed?
It would be better to have it in a second PR (less decisions to be done to merge this one :) ), it might deserve its own setting, since flushing at every line can be slower.
",eliasdorneles,dracony
2034,2016-07-05 08:57:27,"@eliasdorneles 
The change https://github.com/scrapy/scrapy/pull/2034#discussion-diff-69482186L203 was indeed irrelevant and changes nothing. It was a sideeffect with me fighting with the CSV writer.  

I gave it one more try and made the CSV writer work without changing encoding line by line (although in my defence I got that approach from the csv manual page)

As for the `line_buffering` being on it does seem to be required since turning it off broke the test entirely. I would be happy to skip the TextIOWrapper altogether, but the only way to do that would be reopening the file again with the encoding parameter specified. And considering it's the file object that is being passed to the exporter, closing and reopening the file seems more like a hack
",dracony,eliasdorneles
2034,2016-07-06 20:23:29,"@dracony 
Using `TextIOWrapper` is totally fine, the current PY3 code is already using it.
I still think it's weird why `line_buffering` is being required for PY3, it looks like we'll have different behavior in PY2. Which test failed?

Please see my other comment about passing `ensure_ascii` parameter -- it's best to use the `kwargs.setdefault` approach to maintain compatibility with subclasses.
Aside that little nitpicking, the PR looks good to me, nice work! :+1: 
",eliasdorneles,dracony
2034,2016-07-11 14:48:39,"@eliasdorneles I changed the ensure_ascii to kwargs as you suggested. Also I found a solution for why `line_buffering` `False` was failing tests. The solution was to also enable `write_through`, otherwise the buffer was not flushed to disk
",dracony,eliasdorneles
2034,2016-07-11 15:03:38,"@dracony hey, thanks for digging! :+1: 
Hm, that kinda makes me suspicious of the tests, like they may be trying to read without having guarantee the file is flushed.
I don't think we should need to be forcing flush (we don't seem to need it for the PY 2 code)... I'll have a look to confirm, thank you!
",eliasdorneles,dracony
2034,2016-07-11 15:06:40,"@eliasdorneles It's not the file that is nt being flushed, it's that TextIOBuffer
",dracony,eliasdorneles
2034,2016-07-11 15:24:09,"Yup, you're right. :)
Alright, this looks good now!
Thank you, @dracony !
",eliasdorneles,dracony
2034,2016-07-11 15:28:06,"Yay =)

On Mon, Jul 11, 2016 at 5:24 PM, Elias Dorneles notifications@github.com
wrote:

> Yup, you're right. :)
> Alright, this looks good now!
> Thank you, @dracony https://github.com/dracony !
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/scrapy/scrapy/pull/2034#issuecomment-231768335, or mute
> the thread
> https://github.com/notifications/unsubscribe/AC-3KI7FN-GaAVBW08_ZCtXABujFYKFdks5qUmBFgaJpZM4Iu3Yz
> .
",dracony,dracony
2034,2016-07-12 15:02:31,"@dracony , thanks a lot!
It took a while to review it properly, sorry about that.
",redapple,dracony
2033,2016-06-06 14:29:30,"@kmike , ok, do you agree it should be mentioned in the docs?
",redapple,kmike
2032,2016-09-19 08:52:53,"Thanks for your help @matt-oconnell! We've rewritten the tutorial to use http://toscrape.com website instead of DMOZ in order to prevent it from breaking again in future. Sorry for the broken tutorial; this must have caused hours of wasted time :(
",kmike,matt-oconnell
2031,2016-06-13 17:01:31,"Looks good to me.
@matt-oconnell Could you make this PR to master branch instead?
",eliasdorneles,matt-oconnell
2030,2016-06-08 17:46:22,"Hey @rootAvish,

thanks for the WIP. You need to replace your imports to use either the full path or a relative one (`from dispatch import Signal` -> `from scrapy.dispatch import Signal` or `from .dispatch import Signal`, and a few others in the `dispatch` subpackage, preferably the absolute path version), you can easily see what I mean when you open a Python interpreter in the repositories base directory and simply try `import scrapy`.

Did you take a look at the tests?
",jdemaeyer,rootAvish
2030,2016-08-08 15:15:45,"Hey @rootAvish :) I've added my usual nitpicks here and there but I am quite happy with how this PR looks. Great work!

It would be cool if you could squash the commits into a couple of ""meaningful"" ones (tip: make a tag or write down the current commit hash before calling `rebase`, it's an easy way out if you accidentally enter rebase hell ;)). Then when we have settled the remaining smaller issues (the line comments) and the benchmarks are up somewhere I think we're ready to have a second reviewer look over it and add some docs (or the other way around).
",jdemaeyer,rootAvish
2030,2016-08-15 08:10:44,"Hey @jdemaeyer , thanks for the code review. In accordance with the email I sent you, the patched receiver sending was abysmally slow when compared to the current API, and in the spirit of always coming up with overly complicated solutions, this too was due to over engineering on my part. I introduced a large amount of method call overhead as well, which was taking a lot of time too. I reworked that and completely got rid of `_patched_receivers` and instead worked using the `Signal` class itself, IMO the new approach is much simpler and cleaner, and the benchmark too is fixed(in case you didn't check the email yet, here's a link to the benchmark suite: https://github.com/rootAvish/scrapysignalbench).

Would appreciate it if you had a look over that part again, do you see any problems with the new implementation?
",rootAvish,jdemaeyer
2030,2016-08-19 18:53:17,"@jdemaeyer thanks for responding on such short notice, always appreciate your insights.

>  there's still three that are open from earlier commits. (Btw these are never commands or anything, feel free to answer with why you disagree.)

Sorry for missing out on those, I don't disagree, I thought you wanted to deliberate further on deprecating `utils.signal` further so I didn't move on it at the time, I thought it would be too much of a change but then I considered that the utils pass-through methods are not directly usable after the change anyway, plus it also gets rid of the method call overhead.

> So this leaves us with the following stuff still to do, right?
> smaller issues (line comments)  

I think I took care of all of them all (I think, I'll make sure to go over all of them again to make sure I didn't miss any like before).

> squash commits  

Right, I wanted to do that once I was done with the development.

> finish docs

I'm almost done with those, I'll upload them sometime later today. I did the docstrings for `dispatcher.py` as a starting point, I tried to adhere to the formatting style used in Scrapy.

> If you find time for it, maybe you can put the output for the benchmark suite somewhere (just a raw paste of the command line output should be fine)?

Hey sure, I thought you would like to run the same on your end for verification so I didn't include my run at first, here's a sample: https://github.com/rootAvish/scrapysignalbench/blob/master/scrapysignalbench/benchmarks/README.md

As you can see, in theory connection is slower than before but most of that time is used up in checking for kwargs and raising the deprecation warning(line_profiler output that I went by in the past suggest so), in time further down the road we can always look to migrate away from non-kwargs receivers completely. Do you think slower connection times an issue?
",rootAvish,jdemaeyer
2030,2016-09-07 06:06:30,"Hey @jdemaeyer , did you guys change the openSSL version or something? I don't think that break with Python 3.3 has anything to do with the code. Also, please review the docs when you get a chance. I hope the commits are as desired now.
",rootAvish,jdemaeyer
2030,2016-09-13 14:26:30,"Hi @rootAvish , we did not change anything explicitly, but newer packages were used:
If I compare python packages from the [last PASSING Travis build](https://travis-ci.org/scrapy/scrapy/jobs/154405979)
and [this FAILING one](https://travis-ci.org/scrapy/scrapy/jobs/157872588), this is what you get:


",redapple,rootAvish
2030,2016-09-13 15:16:25,"hey @rootAvish , I don't know why, but a new PR of mine with _only_ your commits, rebased on current ""master"" branch, built fine, including on Python 3.3
https://github.com/scrapy/scrapy/issues/2235
https://travis-ci.org/scrapy/scrapy/builds/159620850
",redapple,rootAvish
2030,2016-09-13 16:21:06,"@redapple I'm unable to make out what the problem is at first glance, but using your build I'm sure I can figure that part out. I'll report back as soon I have something. Thanks for the pointers.
",rootAvish,redapple
2030,2016-09-24 15:03:28,"Hey @redapple , sorry for not replying all this time but just wanted to give you an update. I haven't been able to solve this issue yet since the only place I was able to even reproduce the problem was on a Ubuntu 14.04 installation and on there it seems like this is an error with OpenSSL itself (scrapy-master breaks too).


",rootAvish,redapple
2030,2016-10-26 13:12:15,"@rootAvish , sorry for the late feedback.
I just tried again submitting a master-branch-rebased version of your commits, and it built ok: https://github.com/scrapy/scrapy/pull/2361

Not sure what issue Travis is having to be honest.

But perhaps you can try doing the same thing: open a new PR, on top of master-branch, and continue/finish-up the work there (if there's anything missing)
",redapple,rootAvish
2030,2016-11-30 05:13:28,"@redapple Right, I'm anyway out of ideas on what else I can try here.",rootAvish,redapple
2030,2016-12-29 12:37:09,"@redapple, whatever issue Travis was having seems to be fixed now, waiting for @kmike 's comments on the usage of the weakrefmethod package, all the other stuff is taken care of. Sorry for the delay, was in a crunch at work.",rootAvish,redapple
2024,2016-10-20 01:45:06,"@eliasdorneles 

I found no references to [sample projects](https://github.com/scrapinghub/sample-projects/tree/master/quotes_crawler) in docs. Is there any? It should!
",djunzu,eliasdorneles
2024,2016-10-22 05:53:02,"Those sample are more about Scrapinghub stack... in which way?
For me they are good examples on how to use Scrapy in different ways!

@eliasdorneles , maybe #2350 would answer your previous question.
",djunzu,eliasdorneles
2024,2016-10-24 17:09:39,"@eliasdorneles @djunzu [the repository as a whole](https://github.com/scrapinghub/sample-projects) has some examples that are specific to Scrapinghub platform. The [quotes-crawler](https://github.com/scrapinghub/sample-projects/tree/master/quotes_crawler), though, is not and I agree that it has good examples that could be useful for users in general.

So, we could add a pointer to that specific folder in the documentation.
",stummjr,djunzu
2024,2016-10-24 17:09:39,"@eliasdorneles @djunzu [the repository as a whole](https://github.com/scrapinghub/sample-projects) has some examples that are specific to Scrapinghub platform. The [quotes-crawler](https://github.com/scrapinghub/sample-projects/tree/master/quotes_crawler), though, is not and I agree that it has good examples that could be useful for users in general.

So, we could add a pointer to that specific folder in the documentation.
",stummjr,eliasdorneles
2023,2016-07-11 14:44:44,"hi @carlosp420 , sorry for the very late review. I left some comments in-line.
",redapple,carlosp420
2023,2016-08-16 12:49:15,"LGTM, thanks @carlosp420 !
what do you think @kmike , @eliasdorneles , @stummjr ?
",redapple,carlosp420
2023,2016-08-16 12:49:15,"LGTM, thanks @carlosp420 !
what do you think @kmike , @eliasdorneles , @stummjr ?
",redapple,kmike
2023,2016-09-19 08:49:38,"Thanks @carlosp420! Your changes were good 👍, but we've rewritten the tutorial to use http://toscrape.com instead of dmoz (#2236), in order to prevent it from breaking again in future.
",kmike,carlosp420
2020,2016-06-06 14:22:29,"@Karla-Isabel-Sandoval ,
it's really hard to help you without the context nor the spider code.
As @djunzu suggest, you'll find more help for these ""why is my spider not working?"" questions on the [mailing list](https://groups.google.com/forum/#!forum/scrapy-users) or [stackoverflow](https://stackoverflow.com/questions/tagged/scrapy).
Please note that GitHub issues should be [complete, reproducible, specific bug reports](http://doc.scrapy.org/en/latest/contributing.html#reporting-bugs) (except if it's a design discussion)
",redapple,djunzu
2015,2016-06-06 14:11:35,"Thanks @lopuhin 
",redapple,lopuhin
2014,2016-05-27 10:12:48,"Hey @starrify, 

I think the reason is that you're using Scrapy 1.0.x; it was a known limitation of robots.txt middleware - see Scrapy 1.0 [docs](http://doc.scrapy.org/en/1.0/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.robotstxt). The issue is fixed in Scrapy 1.1 by https://github.com/scrapy/scrapy/pull/1473, so there is no warning in 1.1 [docs](http://doc.scrapy.org/en/1.1/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.robotstxt).
",kmike,starrify
2014,2016-05-27 10:19:31,":+1: Thanks @kmike! :D
",starrify,kmike
2008,2016-06-06 14:14:42,"Thanks @foromer4 
",redapple,foromer4
2007,2016-06-06 14:05:16,"@pawelmhm , thanks for opening the discussion.
I'm really not familiar with websockets so what I say below is surely missing a lot of important stuff.
I believe the notion of callbacks and errbacks does fit websockets well, including server push. whether it'd still be `Response` or something else that are passed to spider methods needs careful design obviously.
WebSockets being low-level, I'm not sure there's a need for a downloader middleware pass, so bypassing them altogether seems reasonable.

As for the need, I do think scrapy will have to support HTTP/2 at some point, and HTTP/2 has server push option too, so it makes total sense to work on a design (API and/or Twisted-compatible implementation) that accomodates both HTTP/2 and web sockets.

EDIT: there could be a need for caching pushed resources in HTTP/2, so a middleware chain would make sense
",redapple,pawelmhm
2007,2016-06-06 15:51:53,"@pawelmhm @redapple Thank you for your work. Successfully subscribing to a websocket server and getting data pushed from it is a much better way to scrape data than getting the whole page every time, for us, scrapers, and for the site owners because of the selective variable data that's been send via the websocket without the whole html/static shebang. What really would be cool, and what the whole goal is about websockets, is to keep the connection alive (sendalives are used in websockets and needed since most proxy servers disconnect after 65 seconds idle (default with nginx)) with the websocket server so updated data gets pushed and eventually populated into the item fields of scrapy, every time there's an updated data field, the websocket server will push the update to your subscription. Think about how cool this would be :-) 
",yssoe,pawelmhm
2007,2016-06-06 15:51:53,"@pawelmhm @redapple Thank you for your work. Successfully subscribing to a websocket server and getting data pushed from it is a much better way to scrape data than getting the whole page every time, for us, scrapers, and for the site owners because of the selective variable data that's been send via the websocket without the whole html/static shebang. What really would be cool, and what the whole goal is about websockets, is to keep the connection alive (sendalives are used in websockets and needed since most proxy servers disconnect after 65 seconds idle (default with nginx)) with the websocket server so updated data gets pushed and eventually populated into the item fields of scrapy, every time there's an updated data field, the websocket server will push the update to your subscription. Think about how cool this would be :-) 
",yssoe,redapple
2005,2016-05-24 13:51:48,"Hello, @feliperuhland.

I like the feature idea, it will be nice to be able to do `scrapy startproject myproj .` inside a directory.

The build is failing for Python 3, can you have a look?
",eliasdorneles,feliperuhland
2005,2016-05-24 13:54:22,"Thanks, @eliasdorneles . I am working on it.
",feliperuhland,eliasdorneles
2005,2016-05-24 15:34:11,"@eliasdorneles you're right.
I will fix this.
",feliperuhland,eliasdorneles
2005,2016-05-24 17:00:25,"Looks great, thanks @feliperuhland !
",eliasdorneles,feliperuhland
2005,2016-07-19 03:23:23,"@eliasdorneles I done what you suggested and I think it's good. Can you read the docstring to see if it's good enough?

Thanks again!
",feliperuhland,eliasdorneles
2005,2016-07-19 10:13:01,"Yup, looks great now. Thank you, @feliperuhland !

@redapple this looks ready. How do you feel about merging it for 1.2?
",eliasdorneles,feliperuhland
2005,2016-07-19 10:31:19,"Thanks @feliperuhland !
",redapple,feliperuhland
2003,2016-05-21 09:19:40,"@redapple do you know if restarting Travis job will help? We should have a tarball on pypi.
",kmike,redapple
2003,2016-05-21 09:31:21,"@kmike , I did try rebuild on Travis.
Got an error on the wheel upload (already there) so src upload did not happen

https://s3.amazonaws.com/archive.travis-ci.org/jobs/129470985/log.txt


",redapple,kmike
2003,2016-05-21 15:15:28,"@kmike, @daniel, can one of you do it? I don't have permission on pypi for
scrapy (+ I'm away for 2 weeks)
Le 21 mai 2016 15:53, ""Robson Roberto Souza Peixoto"" <
notifications@github.com> a écrit :

> Why not upload manually the tarball?
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/scrapy/scrapy/issues/2003#issuecomment-220778962
",redapple,kmike
2003,2016-05-23 10:35:43,"Works for me, thanks @robsonpeixoto for the report and @dangra for the fix!
",kmike,dangra
2002,2016-05-20 12:44:55,"Thanks @kmike. I'll study more about WARC and scrapy Response.
Thanks the the tip about warcprox.
",robsonpeixoto,kmike
2001,2016-06-06 14:37:14,"@matveinazaruk thanks for this,
could you add a test for this change?
",redapple,matveinazaruk
2001,2016-06-13 20:36:51,"@redapple Thanks for remark. Done!
Also, As I've investigated it was setting simple `Response` in case of all types of request and all expected types of bodies. 
Now it fixed for http 1.1.

Can you check please?
",matveinazaruk,redapple
2001,2016-06-14 13:51:37,"Looks good to me.
Thanks @matveinazaruk 
",redapple,matveinazaruk
2000,2016-06-06 14:36:42,"@matveinazaruk , do you have a reproducible example for this behavior?
",redapple,matveinazaruk
2000,2016-07-13 15:53:11,"Fixed in https://github.com/scrapy/scrapy/issues/2001
Thanks again @matveinazaruk !
",redapple,matveinazaruk
1998,2016-12-22 17:38:05,@rolando `twisted-win` has been removed from pypi because it is no longer required. scrapy now works by default with `pip install scrapy` because the `twisted` package has been updated.,xoviat,rolando
1997,2016-05-19 11:45:21,"@kmike - ah, ok. I'm working on an inherited codebase that doesn't use the `LinkExtractors` directly, so this was obviously just a misunderstanding of the API/using something non-public. I'll close this issue - agree with backwards incompatible change reasons.
",knaveofdiamonds,kmike
1992,2016-11-21 07:20:16,"@elacuesta yeah, I agree that using value set in `cookies` argument makes more sense in this case.",kmike,elacuesta
1991,2016-05-18 16:22:25,"Hi, thank you the feedback :) 

@rolando for both duplicate requests and non-200, I think that they should be dispatched in the `exception` message, because they are caught in the spider runtime and it was not expected by the spider. 

@kmike I'll talk with my mentors about it. But talking about implementation, it's not too dependent on scrapy source code, so I think that if necessary, I can implement it as an extension with a few modifications in the original proposal
",aron-bordin,rolando
1991,2016-05-18 16:22:25,"Hi, thank you the feedback :) 

@rolando for both duplicate requests and non-200, I think that they should be dispatched in the `exception` message, because they are caught in the spider runtime and it was not expected by the spider. 

@kmike I'll talk with my mentors about it. But talking about implementation, it's not too dependent on scrapy source code, so I think that if necessary, I can implement it as an extension with a few modifications in the original proposal
",aron-bordin,kmike
1991,2016-05-18 16:43:36,"@kmike I agree with starting a new repository on scrapy-plugins
",eLRuLL,kmike
1989,2016-05-17 10:11:32,"About this PR, I believe it's the best that can be done to handle both the ease-of-use of settings and also legacy customization using class attributes, even if it could be confusing that settings take precedence over class attributes (note/warning needed in docs at the very least).

Maybe we should promote the use of settings and show with code example how to customize the values at init.

About https://github.com/scrapy/scrapy/pull/1989#issuecomment-219368547 , I agree with @eliasdorneles on not using dicts settings for image pipelines.
I see settings as global knobs, effective for all subclasses, except if explicitly changed in the subclass contructor in some way (via a custom setting for example) by the developer.
",redapple,eliasdorneles
1989,2016-05-17 11:02:41,"ok so I'll add tests for overriding class attributes with settings (as @eliasdorneles suggests) and also perhaps test for keeping uppercase attributes if there's pipeline inheriting from base image pipeline. 

In docs I'll add following things:
- warning that values from settings will override class attributes 
- code sample showing how to take values from settings in init
",pawelmhm,eliasdorneles
1989,2016-05-30 14:51:26,"@djunzu 

> docs could explain how to write a custom init for custom ImagesPipelines. But, as a developer, I think it would expose a layer of complexity that the end user doesn't need to know about. 

I also think that adding this layer of complexity in docs may NOT be necessary. It will require long code sample that will be used by 1-2 % of users. So that would suggest current PR is good as it is (of course if there are no other objections).  @redapple @eliasdorneles is it okay if we dont add code sample illustrating how to take settings on pipeline init?

@djunzu 

> Then, it makes sense to me provide some out of the box feature in that settings for custom ImagesPipelines can be set easily in the settings.py.

There is no obvious way to provide such feature. One way I was thinking about was using some form of dictionaries in settings see this comment here https://github.com/scrapy/scrapy/pull/1989#issuecomment-219368547, but others suggested that it would add complexity and break our established notions about settings keys. Plus if you have multiple image pipelines you have to create object instance anyway, so you can just as well add your own keys in settings or as class attributes whatever works best for you. It's actually not that complicated so I think most users should manage.
",pawelmhm,djunzu
1989,2016-05-30 14:51:26,"@djunzu 

> docs could explain how to write a custom init for custom ImagesPipelines. But, as a developer, I think it would expose a layer of complexity that the end user doesn't need to know about. 

I also think that adding this layer of complexity in docs may NOT be necessary. It will require long code sample that will be used by 1-2 % of users. So that would suggest current PR is good as it is (of course if there are no other objections).  @redapple @eliasdorneles is it okay if we dont add code sample illustrating how to take settings on pipeline init?

@djunzu 

> Then, it makes sense to me provide some out of the box feature in that settings for custom ImagesPipelines can be set easily in the settings.py.

There is no obvious way to provide such feature. One way I was thinking about was using some form of dictionaries in settings see this comment here https://github.com/scrapy/scrapy/pull/1989#issuecomment-219368547, but others suggested that it would add complexity and break our established notions about settings keys. Plus if you have multiple image pipelines you have to create object instance anyway, so you can just as well add your own keys in settings or as class attributes whatever works best for you. It's actually not that complicated so I think most users should manage.
",pawelmhm,redapple
1989,2016-05-30 14:51:26,"@djunzu 

> docs could explain how to write a custom init for custom ImagesPipelines. But, as a developer, I think it would expose a layer of complexity that the end user doesn't need to know about. 

I also think that adding this layer of complexity in docs may NOT be necessary. It will require long code sample that will be used by 1-2 % of users. So that would suggest current PR is good as it is (of course if there are no other objections).  @redapple @eliasdorneles is it okay if we dont add code sample illustrating how to take settings on pipeline init?

@djunzu 

> Then, it makes sense to me provide some out of the box feature in that settings for custom ImagesPipelines can be set easily in the settings.py.

There is no obvious way to provide such feature. One way I was thinking about was using some form of dictionaries in settings see this comment here https://github.com/scrapy/scrapy/pull/1989#issuecomment-219368547, but others suggested that it would add complexity and break our established notions about settings keys. Plus if you have multiple image pipelines you have to create object instance anyway, so you can just as well add your own keys in settings or as class attributes whatever works best for you. It's actually not that complicated so I think most users should manage.
",pawelmhm,eliasdorneles
1989,2016-05-31 20:56:30,"@pawelmhm 

> It's actually not that complicated so I think most users should manage.

Here I must ask: how much most users know or should know about Scrapy?

Imagine you know absolutely nothing about Scrapy, you just started. How you would know it is possible to use a class attribute in the init? Two options: docs or dig into the code. We have agreed it should not go in the docs. Thinking as a end user of  the framework, I do not want dig in the code.

Ok,, there is the Settings API. But... All we want is to set a custom value in a custom pipeline for a existing setting parameter. I think it should not be required to know about the Settings API for a simple task like that.

---

> So that would suggest current PR is good as it is

I agree with you 100 percent!

---

> There is no obvious way to provide such feature.

What about something like this?



It allows users to have:


",djunzu,pawelmhm
1989,2016-06-10 11:09:40,"@redapple @eliasdorneles @djunzu updated a62d4b081c8eef1e54c6f7399e7a
",pawelmhm,djunzu
1989,2016-06-10 11:09:40,"@redapple @eliasdorneles @djunzu updated a62d4b081c8eef1e54c6f7399e7a
",pawelmhm,redapple
1989,2016-06-10 11:09:40,"@redapple @eliasdorneles @djunzu updated a62d4b081c8eef1e54c6f7399e7a
",pawelmhm,eliasdorneles
1989,2016-06-10 15:10:30,"@pawelmhm 

In `default_settings.py`, there are still three values for FilesPipeline that will override the respective class attributes.

I think FilesPipeline should also get its `key_for_pipe` and some tests.

And I think you can add one more test asserting a custom Files/ImagesPipeline inherit custom settings from parent. (So, if someone's code relies on this behavior, future updates will not break it.)
",djunzu,pawelmhm
1989,2016-06-15 13:53:39,"hey @djunzu @redapple @eliasdorneles I updated PR. Majority of new changes is just adding more tests and unifying existing tests. Aside from that I added option to set custom settings for files pipeline (previously it was just images pipeline) and properly fixed uppercase attributes for file pipeline.
",pawelmhm,djunzu
1989,2016-06-15 13:53:39,"hey @djunzu @redapple @eliasdorneles I updated PR. Majority of new changes is just adding more tests and unifying existing tests. Aside from that I added option to set custom settings for files pipeline (previously it was just images pipeline) and properly fixed uppercase attributes for file pipeline.
",pawelmhm,redapple
1989,2016-06-15 13:53:39,"hey @djunzu @redapple @eliasdorneles I updated PR. Majority of new changes is just adding more tests and unifying existing tests. Aside from that I added option to set custom settings for files pipeline (previously it was just images pipeline) and properly fixed uppercase attributes for file pipeline.
",pawelmhm,eliasdorneles
1989,2016-06-29 21:05:59,"so at this point it is probably ready. Are there any other comments or feedback I should include @redapple 
",pawelmhm,redapple
1989,2016-07-08 10:57:13,"@kmike , @eliasdorneles ,
are you ok with merging this? (and backporting to 1.1 of course)
",redapple,eliasdorneles
1989,2016-07-12 15:46:46,"LGTM!
Thanks again @pawelmhm 
",redapple,pawelmhm
1986,2016-05-12 23:50:33,"@pawelmhm can you check again please? :)
",eliasdorneles,pawelmhm
1986,2016-05-13 07:50:12,"hey @eliasdorneles I rerun all of my custom image pipelines unit tests with your branch and it still fails.

Old pipeline had attribute MIN_WIDTH which is now renamed to IMAGES_MIN_WIDTH. So it's still not fully backward compatible.

On my side I'm updating all my code to use new interface (get values from settings and not from uppercase class attributes) and I'm going to have code working with new pipeline on master, but if we care about not breaking things for people using undocumented interfaces we should keep MIN_WIDTH.

EDIT: not only MAX_WIDTH, same for MIN_HEIGHT, EXPIRES and THUMBS keys please check all attributes that were removed here: https://github.com/scrapy/scrapy/pull/1891/files#diff-0c52d5edf5deca8bf1993b707225b409R243
",pawelmhm,eliasdorneles
1985,2016-05-12 08:49:39,"@djunzu I think your approach is right, but if a change breaks users code and there is an easy workaround then we usually prefer to add a backwards compatibility shim with deprecation warnings. It doesn't mean users should write code like that (relying on undocumented attributes), but the userbase is large, and we don't want to break user code without a good reason.

Also, we should have named attribute `self._IMAGES_RESULT_FIELD` if we wanted to communicate it is private.
",kmike,djunzu
1976,2016-05-11 09:18:03,"@nyov , I'm not sure I understand _""(at least on rc3 as well)""_
you're seeing this with 1.1 RC3 too? or only from 1.1 RC4?
A [`logger.warning()` call was indeed added in RC4](https://github.com/scrapy/scrapy/blob/1.1.0rc4/scrapy/core/downloader/tls.py#L47) for hostname verification failures, but it was not in RC3
",redapple,nyov
1976,2016-05-11 11:15:15,"@nyov , what do you think of https://github.com/scrapy/scrapy/issues/1979 ?
",redapple,nyov
1975,2016-05-09 14:43:41,"Thanks @lipis !
",redapple,lipis
1974,2016-05-09 13:47:50,"hi @starrify , what kind of error triggered `TunnelError` in your tests?
did you see retrying them succeed eventually?
",redapple,starrify
1974,2016-05-09 13:57:51,"Hi @redapple , here is the error message:



For details, please check https://staging.scrapinghub.com/p/6327/job/9/17618/#/log?filterAndHigher&filterType=error (private access only). From the job stats we know there's no retrying on such failures.

Here's another sample job after applying the same fix in this PR: https://staging.scrapinghub.com/p/6327/job/9/17622/#details (private access only) where the same error has almost disapeared. Also the job stats suggest that there are many retries.
",starrify,redapple
1974,2016-05-17 14:16:23,"@starrify , could this be configured via a new downloader setting?
",redapple,starrify
1974,2016-05-17 14:38:36,"@starrify , I'm not sure I understand your comment.
one could still update an instance attribute with exceptions to retry, defaulting to `EXCEPTIONS_TO_RETRY`, adding `TunnelError` to the tuple if some new `RETRY_TUNNEL_ERROR` is `True` or something.
maybe I'm missing something?
",redapple,starrify
1974,2016-05-18 11:39:30,"@starrify , after some thought, I think your initial commit (adding `TunnelError` to `EXCEPTIONS_TO_RETRY`) makes more sense in the end. Sorry for making you update it.
",redapple,starrify
1974,2016-06-06 10:19:07,"Thanks @starrify 
",redapple,starrify
1973,2016-05-11 18:41:33,"thanks @mgachhui , I cherry-picked it into master in https://github.com/scrapy/scrapy/commit/aecc23d24a801f8b7c50497bc37a0c0a2e37e173
",redapple,mgachhui
1969,2016-05-11 12:16:53,"Thanks for your replying!
I guess I use scrapy with cralwera, and the config of download timeout, see [crawlera](http://doc.scrapinghub.com/crawlera.html)
`CRAWLERA_DOWNLOAD_TIMEOUT  timeout for requests (default: 1800)`
it maybe the cause of my problem.
@nyov 
",night1008,nyov
1969,2016-05-16 17:03:32,"Thanks @nyov for the help!
",kmike,nyov
1968,2016-05-06 06:06:29,"@eLRuLL Thank you, but I'm getting errors when executing your code: https://paste.ee/r/8YSJG

I'm running in the scrapy1.1.0rc4 and Python 3.5.1

I change `from BeautifulSoup import BeautifulSoup` to `from bs4 import BeautifulSoup`.

Any Ideas? Thank you
",rafaelcapucho,eLRuLL
1961,2016-05-11 08:58:19,"Similarly, we received feedback that the current (latest) FAQ (1.0 branch) should also say that Python 3 is supported from 1.1+, albeit with beta status and requiring `pip install` with specific version.
What do you think @kmike @nyov ?
",redapple,nyov
1961,2016-05-11 08:58:19,"Similarly, we received feedback that the current (latest) FAQ (1.0 branch) should also say that Python 3 is supported from 1.1+, albeit with beta status and requiring `pip install` with specific version.
What do you think @kmike @nyov ?
",redapple,kmike
1961,2016-05-11 10:40:39,"@redapple @mgachhui I think we should release 1.1 ASAP; it will have Python 3 support and all such problems will fade away.
",kmike,redapple
1961,2016-05-11 10:40:39,"@redapple @mgachhui I think we should release 1.1 ASAP; it will have Python 3 support and all such problems will fade away.
",kmike,mgachhui
1961,2016-05-11 12:58:08,"@kmike , I assume you are ok with releasing current 1.1 branch (perhaps with #1979) as v1.1.0. Correct?
",redapple,kmike
1961,2016-05-11 14:08:49,"@redapple yep!
",kmike,redapple
1960,2016-04-28 15:57:02,"@kmike thx. Updated.
",redapple,kmike
1956,2016-04-28 19:01:20,"@Digenis a good point: I think this issue is relevant: https://github.com/rtfd/readthedocs.org/issues/1881
",kmike,Digenis
1954,2016-04-26 14:24:20,"@aron-bordin ,
I'm not seeing where the data is actually encoded as [`multipart/form-data`](https://tools.ietf.org/html/rfc7578). Added tests don't test it.

Current `FormRequest` parameters require `formdata` to be a dict, to be [URL-encoded internally](https://github.com/scrapy/scrapy/blob/ebef6d7c6dd8922210db8a4a44f48fe27ee0cd16/scrapy/http/request/form.py#L28).
How does this transpose for multipart POSTs? Does `formdata` already need to be encoded?

Multipart support could mean changes in the API (which is not bad per se, if it's backward compatible)
",redapple,aron-bordin
1954,2016-04-27 05:04:18,"Hi @redapple 

Sorry, this pr was just handling the header. I read the multipart's spec [[1]](https://www.w3.org/TR/html401/interact/forms.html#h-17.13.4) [[2]](https://tools.ietf.org/html/rfc7578) and I'll try to add the data encoding.  I'm thinking about two possibilities for `formdata`:
1. Use `formdata` as usual, representing `field: value` from the form and encode it as specified. Add a new field `formfiles`, or something like that, that is a dict that contains `field: file_path`. So these files can be read and added to the request body as explained in the multipart spec.
2. Use html fields and binary data (files) in `formdata`. In this case, `formdata` would be a dict that contains `field: [text value or a binary object representing the file]`

I prefer the first solution because it provides a simpler API and it's backward compatible. Please, let me know what do you think about it.
",aron-bordin,redapple
1954,2016-05-02 04:09:37,"Hi @redapple and @eLRuLL 

I updated the PR as follows:
1. Form fields and form files are present in the `formdata` parameter. To add a file in the `formdata`, it's necessary to use the new `FormRequestFile` object (this is not yet documented, because I'd like to confirm if this api design is good first.)
2. Now, the `from_response` method analyzes the `enctype` field in the form. If present, uses its value as default. If not, keep the `application/x-www-form-urlencoded` as default.
3. A sample Spider using  this feature:
   
   

<s>This PR is working based on the new `multipart` parameter. However, this parameter can be removed, and we can just check if the `header['Content-Type']` is multipart/form-data. I like to have this parameter because it's user friendly, however this adds some unnecessary code to handle it. Do you have some opinions about it ?  </s>

**Edited**: The current implementation checks the header content type to set the form encoding.
",aron-bordin,eLRuLL
1954,2016-05-02 04:09:37,"Hi @redapple and @eLRuLL 

I updated the PR as follows:
1. Form fields and form files are present in the `formdata` parameter. To add a file in the `formdata`, it's necessary to use the new `FormRequestFile` object (this is not yet documented, because I'd like to confirm if this api design is good first.)
2. Now, the `from_response` method analyzes the `enctype` field in the form. If present, uses its value as default. If not, keep the `application/x-www-form-urlencoded` as default.
3. A sample Spider using  this feature:
   
   

<s>This PR is working based on the new `multipart` parameter. However, this parameter can be removed, and we can just check if the `header['Content-Type']` is multipart/form-data. I like to have this parameter because it's user friendly, however this adds some unnecessary code to handle it. Do you have some opinions about it ?  </s>

**Edited**: The current implementation checks the header content type to set the form encoding.
",aron-bordin,redapple
1954,2016-05-02 14:10:24,"great job @aron-bordin . I have some observations:
1. Now, I think the `multipart` argument makes everything look too forced. What happens if we get a different `enctype` in the future, then we'll have to add a new argument and setup all the `if`s necessary on the entire code, just like it is happening now.
2. A dictionary seem like a good way to specify the the parameters, but also it looks to forced and not natural, I would prefer to just pass a `File` object, and set defaults on the other fields for that case, and to create a `class` that could be passed on those cases, which should take care also of all the encoding. the `File` should also be transformed to this new class for encoding.
3. Passing the `enctype` directly on the headers seems more natural to me, maybe we can discuss this some more, but an argument to determine the whole functionality of the data handling seems too much.
",eLRuLL,aron-bordin
1954,2016-05-06 22:00:05,"Hi @redapple 

For me, the current implementation looks simpler to maintain and more user friendly than creating a new class `MultipartFormRequest`. I removed the new parameters in current implementation, so the encoding type is now determined by the `header[Content-Type]`. For me, this looks more intuitive than having a new object to handle it. I updated the sample above to use it.

Also, I don't see where a new `MultipartPart` object could help, because the in the multipart form we'll have the same html inputs (but now it's accepted non ascii values by default) plus binary data (files). So having an API similar to the example above looks more intuitive and it makes less modification to the current scrapy API.

Do you see some advantages in separating the `MultipartFormRequest` instead of checking the header's content-type value ?
",aron-bordin,redapple
1954,2016-10-13 19:47:51,"Hi @redapple 
I updated this PR, and separated the multipart feature to the `MultipartFormRequest`. I updated the PR description with more details about this PR, please, let me know what do you think about it :)

Thx
",aron-bordin,redapple
1953,2016-04-26 15:32:27,"@redapple This is the relevant part of my spider: https://paste.ee/p/0Amww

I made the changes you suggested and It is working properly now:



Thank you for your help!
",rafaelcapucho,redapple
1947,2016-04-26 16:32:32,"Looks good, thanks @redapple for the hard work!

By the way, do you know if this is true? I think it can be a desirable property of canonicalize_url function.


",kmike,redapple
1947,2016-04-26 17:06:59,"@kmike , [`safe_url_string` is tested for this](https://github.com/scrapy/w3lib/blob/fddf599cda53adecc27da0b9b246186b3444be72/tests/test_url.py#L163) so worth a try with canonicalize_url too... and a test
",redapple,kmike
1947,2016-04-26 17:15:57,"@redapple do you want to make these changes in this PR, or should we just merge it as-is?
",kmike,redapple
1941,2016-04-20 11:24:01,"@Digenis +1 to not using canonicalize_url by default in link extractors. It causes other issues - see e.g. https://github.com/scrapy/scrapy/issues/1202. 

See also: https://github.com/scrapy/w3lib/pull/25#issuecomment-205354098.
",kmike,Digenis
1941,2016-04-21 11:12:13,"@Digenis just to be clear, are we taking about changing the default for LinkExtractor, or maybe even removing canoincalize support form them, to provide a better experience? One can already work around the issue by turning canonicalize off (`canonicalize=False` LinkExtractor argument).
",kmike,Digenis
1940,2017-01-30 20:17:42,"@dangra yep! It is Python 3.6+ only, but I'm fine with that.",kmike,dangra
1936,2016-04-18 11:53:07,"@redapple, have sent you in email, thanks.
",lhuaizhong,redapple
1936,2016-04-19 03:36:52,"@redapple , I upgraded scrapy to 1.1.0rc3, using your patch in file http11.py, it works well right now.

Thanks Paul!
",lhuaizhong,redapple
1933,2016-04-19 23:22:41,"@redapple how hard would it be to create a test case for this change?
",kmike,redapple
1933,2016-04-20 12:47:49,"@kmike done in https://github.com/scrapy/scrapy/pull/1933/commits/cd979ace40f26e8d921aaa4cf2d603b434fa2064

Applying the tests on top of current ""master"" branch, you get these failures:


",redapple,kmike
1933,2016-04-20 12:57:40,"Thanks @redapple!
",kmike,redapple
1930,2016-04-14 13:07:19,"@redapple Thank you for the good and fast fix!
After changing contextfactory.py, the crawler now opens perfectly! 
I just wonder: As it does in curl or in browsers like chrome, should scrapy send some kind of warnings, as it does in Scrapy 1.0.5 (but maybe it's because I'm using it in a virtualenv with an older Twisted and it does not make all the verifications) when there is an invalid certificate?
",natoinet,redapple
1925,2016-04-12 10:35:48,"@redapple  yeah , I made two `post_login` that's the point. 
Tnx, I should use a ide to code... 
",woshichuanqilz,redapple
1918,2016-04-19 23:46:03,"Hi @pawelmhm,

Thanks for writing this; it looks like a good reference for anyone who want to give alternative contracts library a try or who wants to fix scrapy contracts.

The general sentiment is that we should extract contracts to a separate library and move it to scrapy-plugins organization. YAML-based contracts can be either added to this library, or one can create a separate library for YAML-based contracts.
",kmike,pawelmhm
1916,2016-04-11 13:12:34,"Thanks @nblock !
",redapple,nblock
1915,2016-05-10 03:35:22,"@mgachhui 

This is just my personal opinion, but I think the intent behind per-item processing is that it happens on the fly and ""in parallel"", in the sense that we don't wait for other items to process the current one. 

It makes sense to do this if the pipeline does not require information from other items (e.g. dropping an item based on one field's value), but sometimes it's just a bit restricting.
",dxue2012,mgachhui
1915,2016-05-12 17:17:30,"Edit: Added some more questions.
In addition to what @eLRuLL said, I have a couple of questions. 
1. Currently the scraper utilizes a slot system. So if an item is being split, does it register as an extra item in the slot?
2. If an iterable of dicts/Items is used, how is the event of an item being dropped supposed to be handled? It may be that an iterable has items removed until it's empty. In any case, raising an exception would invalidate the complete iterable, won't it?
3. One idea I had was raising a SplitItem exception, after packing the new items into it. Then the new items can be processed again. In scenarios where a large number of items get split, that would be a problem though.
",mgachhui,eLRuLL
1915,2016-05-16 11:52:26,"Good questions @eLRuLL @mgachhui!
My take on this:
1. Items shouldn't be processed by the whole pipeline again.
2. Extra items should use more slots.
3. DropItem is not necessary if several items can be returned - one can return an empty list. It means signal won't be fired, but I think that's OK.
",kmike,eLRuLL
1915,2016-05-16 11:52:26,"Good questions @eLRuLL @mgachhui!
My take on this:
1. Items shouldn't be processed by the whole pipeline again.
2. Extra items should use more slots.
3. DropItem is not necessary if several items can be returned - one can return an empty list. It means signal won't be fired, but I think that's OK.
",kmike,mgachhui
1915,2016-05-16 21:08:59,"@kmike

> What's your use case? When do you need to drop an item based on other item's field value?

Sorry my previous comment was worded poorly. ""dropping an item based on one field's value"" was meant to be an example of a pipeline that _doesn't_ require information from other items.

> In general, to merge items you may need to have them all at once - not only items from a single response. That's why I think merging task is better suited for post-processing.

I agree that the merging task--and in general, tasks that require all items at once--should not happen in pipelines. What are your thoughts on Scrapy providing built-in post-processing capabilities that handle such tasks? This could be an extension to pipelines that begins after all pipelines have finished, and we could use a chain of `process_items` functions (many-in, many-out).

edit: wording.
",dxue2012,kmike
1915,2016-06-14 18:38:07,"Guys and girls, this is kind of large architectural change that might have various implications. You have to think of all the ways people use and abuse pipelines and make sure that any change is well thought, enough to not break tons of stuff.

To the extend that this feature is worth the effort, I would personally prefer to see two API calls that do explicitly `pipeline.push_item_to_front()` and `pipeline.push_item_to_next_stage()` and let them prove their usefulness for a while, before any of them becomes the default behaviour.

Just a random example of the things that could go bad - if a pipeline stage forks an `Item` to 200 `Item`s and the next pipeline stage is MySQL write stage which relies on [`CONCURRENT_ITEMS`](http://doc.scrapy.org/en/latest/topics/settings.html#concurrent-items) in order to prevent MySQL from crashing, then, suddenly, as long as I can see in #2020, MySQL will receive 20000 write requests. (I might be getting the `parallel` API wrong - but it might be a good point to have some tests in #2020 that prove that [`CONCURRENT_ITEMS`](http://doc.scrapy.org/en/latest/topics/settings.html#concurrent-items) is still respected).

If new `Item`s go to the beginning of the pipeline (instead of just subsequent stages), then you can end-up with the recursive situation of always forking as soon as you hit a pipeline stage i.e. from a single `Item` you might end up with `Inf`. Note that this seems unlikely and ""definitely a bug"" but as soon as you write somewhat generic pipeline code, I guess, that the number of forked `Item`s actually depends on the content of the original `Item` you crawled... which comes from the Web! Not good. It's ok if the cards have two sides, as @jeremysmitherman says - but what happens if suddenly they have 10k or 0? As soon as you allow your pipeline to loop... a web document becomes your program.

Let's see an example in the case you forward your `Item`s just to subsequent stages. If you have stages `A->B->C->D` and stage `C` forks a few `Item`s, then the new `Item` might be missing fields that `A->B` populated. Now `D` will have to handle both ""plain"" and ""forked"" `Item`s which makes it less reusable, maintainable and of course more complex. For example, if on stage `A` I was calculating the price from the Zip code, then `D` should have both that logic and the ""passthrough"" logic. It doesn't sound that good.

Not supporting forking `Item`s seems like a restriction but it's a reasonable restriction to keep the complexity to an acceptable level and guide people to write simple and reasonably reusable pipelines.

As @pawelmhm and @kmike mention, a spider middleware is the right ""instrument"" for this type of functionality if you have a use case where it really makes sense. I would guess you have multiple Spiders creating the same type of `Item`s (otherwise - obviously the code belongs to the Spider) and you have a business case where e.g. you crawl orders and orders might have multiple products. For some reason you want to denormalize early and issue multiple `Product` `Item`s where potentially many of them have the same `order_id`. This is a case where one might think it's good to have a pipeline that returns iterable. But that's exactly what Spider Middlewares are there for - application specific logic that applies across many spiders. [By default it expects an iterable](https://github.com/scrapy/scrapy/blob/ebef6d7c6dd8922210db8a4a44f48fe27ee0cd16/scrapy/core/spidermw.py#L62) and it's easy to use (example [1](https://github.com/scrapy/scrapy/blob/ebef6d7c6dd8922210db8a4a44f48fe27ee0cd16/scrapy/spidermiddlewares/depth.py#L30), [2](https://github.com/scrapy/scrapy/blob/ebef6d7c6dd8922210db8a4a44f48fe27ee0cd16/scrapy/spidermiddlewares/urllength.py#L27)). If you need to make split logic even more Spider-specific, you can write a middleware that delegates splitting to a Spider method (if it exists). This should work like charm.

Finally, Scrapy is supposed to be fast and relatively simple crawling engine. The fact that pipelines are called ""pipeline""s doesn't mean that it's the right place to put computationally intensive DAGs. [Storm](http://storm.apache.org/), [Flink](https://flink.apache.org/features.html) and [Spark](http://spark.apache.org/docs/latest/streaming-programming-guide.html) are awesome for this job and will make you think about fault tolerance, redundancy and all those distributed aspects that one needs and are outside Scrapy's scope.

Just summarizing, if #2029 was 5 lines long and had 500 lines of tests, it would be fantastic to merge... It would also show that Scrapy - as an architecture - is open to this extension but it just hasn't been implemented yet. #2029 shows unbelievable skill and persistence - it's really admirable. I think that if it's about to be merged, someone should spend time to write sufficient tests and to make sure the community adopts it without it causing any serious problems.
",lookfwd,pawelmhm
1915,2016-06-14 18:38:07,"Guys and girls, this is kind of large architectural change that might have various implications. You have to think of all the ways people use and abuse pipelines and make sure that any change is well thought, enough to not break tons of stuff.

To the extend that this feature is worth the effort, I would personally prefer to see two API calls that do explicitly `pipeline.push_item_to_front()` and `pipeline.push_item_to_next_stage()` and let them prove their usefulness for a while, before any of them becomes the default behaviour.

Just a random example of the things that could go bad - if a pipeline stage forks an `Item` to 200 `Item`s and the next pipeline stage is MySQL write stage which relies on [`CONCURRENT_ITEMS`](http://doc.scrapy.org/en/latest/topics/settings.html#concurrent-items) in order to prevent MySQL from crashing, then, suddenly, as long as I can see in #2020, MySQL will receive 20000 write requests. (I might be getting the `parallel` API wrong - but it might be a good point to have some tests in #2020 that prove that [`CONCURRENT_ITEMS`](http://doc.scrapy.org/en/latest/topics/settings.html#concurrent-items) is still respected).

If new `Item`s go to the beginning of the pipeline (instead of just subsequent stages), then you can end-up with the recursive situation of always forking as soon as you hit a pipeline stage i.e. from a single `Item` you might end up with `Inf`. Note that this seems unlikely and ""definitely a bug"" but as soon as you write somewhat generic pipeline code, I guess, that the number of forked `Item`s actually depends on the content of the original `Item` you crawled... which comes from the Web! Not good. It's ok if the cards have two sides, as @jeremysmitherman says - but what happens if suddenly they have 10k or 0? As soon as you allow your pipeline to loop... a web document becomes your program.

Let's see an example in the case you forward your `Item`s just to subsequent stages. If you have stages `A->B->C->D` and stage `C` forks a few `Item`s, then the new `Item` might be missing fields that `A->B` populated. Now `D` will have to handle both ""plain"" and ""forked"" `Item`s which makes it less reusable, maintainable and of course more complex. For example, if on stage `A` I was calculating the price from the Zip code, then `D` should have both that logic and the ""passthrough"" logic. It doesn't sound that good.

Not supporting forking `Item`s seems like a restriction but it's a reasonable restriction to keep the complexity to an acceptable level and guide people to write simple and reasonably reusable pipelines.

As @pawelmhm and @kmike mention, a spider middleware is the right ""instrument"" for this type of functionality if you have a use case where it really makes sense. I would guess you have multiple Spiders creating the same type of `Item`s (otherwise - obviously the code belongs to the Spider) and you have a business case where e.g. you crawl orders and orders might have multiple products. For some reason you want to denormalize early and issue multiple `Product` `Item`s where potentially many of them have the same `order_id`. This is a case where one might think it's good to have a pipeline that returns iterable. But that's exactly what Spider Middlewares are there for - application specific logic that applies across many spiders. [By default it expects an iterable](https://github.com/scrapy/scrapy/blob/ebef6d7c6dd8922210db8a4a44f48fe27ee0cd16/scrapy/core/spidermw.py#L62) and it's easy to use (example [1](https://github.com/scrapy/scrapy/blob/ebef6d7c6dd8922210db8a4a44f48fe27ee0cd16/scrapy/spidermiddlewares/depth.py#L30), [2](https://github.com/scrapy/scrapy/blob/ebef6d7c6dd8922210db8a4a44f48fe27ee0cd16/scrapy/spidermiddlewares/urllength.py#L27)). If you need to make split logic even more Spider-specific, you can write a middleware that delegates splitting to a Spider method (if it exists). This should work like charm.

Finally, Scrapy is supposed to be fast and relatively simple crawling engine. The fact that pipelines are called ""pipeline""s doesn't mean that it's the right place to put computationally intensive DAGs. [Storm](http://storm.apache.org/), [Flink](https://flink.apache.org/features.html) and [Spark](http://spark.apache.org/docs/latest/streaming-programming-guide.html) are awesome for this job and will make you think about fault tolerance, redundancy and all those distributed aspects that one needs and are outside Scrapy's scope.

Just summarizing, if #2029 was 5 lines long and had 500 lines of tests, it would be fantastic to merge... It would also show that Scrapy - as an architecture - is open to this extension but it just hasn't been implemented yet. #2029 shows unbelievable skill and persistence - it's really admirable. I think that if it's about to be merged, someone should spend time to write sufficient tests and to make sure the community adopts it without it causing any serious problems.
",lookfwd,kmike
1915,2016-09-14 13:25:14,"Thank you @Digenis 
",nengine,Digenis
1914,2016-04-11 02:17:35,"@redapple it solved my problem, thanks
",liuyunclouder,redapple
1911,2016-04-09 10:43:10,"@redapple  yeah, it worked
",woshichuanqilz,redapple
1908,2016-04-07 12:33:14,"nice @kmike !
For reference: https://github.com/scrapinghub/adblockparser/blob/master/adblockparser/parser.py#L438
",redapple,kmike
1908,2016-04-11 03:13:51,"@redapple https://stackoverflow.com/questions/36440681/how-to-work-with-a-very-large-allowed-domains-attribute-in-scrapy/36454033 the problem is my question, but the method efficienty is too low
",15310944349,redapple
1908,2016-04-11 03:21:19,"@kmike How do i use re2 modified the `get_host_regex` method, could you give a complete sample, i don't have the ability to solve, thank you.
",15310944349,kmike
1908,2016-04-11 10:43:08,"@kmike  my mistake . think you very much, re2 is very strong, i successed just import re2 as re.everything is ok. think you
hahahah
",15310944349,kmike
1908,2016-04-11 10:57:44,"@kmike  oh, no, when i run my scrapy project, it mention out of mmemory

> re2/dfa.cc:457: DFA out of memory: prog size 517126 mem 533686
",15310944349,kmike
1908,2016-04-12 05:30:16,"@kmike I find the related information, but i don't know how to get more memory for regex, can you give a link under reference.think you.
",15310944349,kmike
1908,2016-04-18 10:14:57,"@kmike oh, think you very much
",15310944349,kmike
1907,2016-04-07 16:01:10,"Thank you guys. I agree with @kmike it is an expected behavior I think. It also makes more sense in my real use case which is something like this:



I found a workaround using `.re()`, but not very clean
",VinGarcia,kmike
1907,2016-08-08 10:17:04,"@Tethik yeah, parsel is a right repo now 👍 
",kmike,Tethik
1905,2016-04-07 11:09:03,"Hey @rootAvish,
thanks for the PR!

> Oh.. I did not notice initially we have a test here(https://travis-ci.org/scrapy/scrapy/jobs/121056809#L319) to always discard partial archives.

The problem with that error is not the test itself but the fact that `GzipFile` doesn't have a `_read_eof()` method in Python 3. If you look at https://travis-ci.org/scrapy/scrapy/builds/121056805 you'll see that the tests passed under Python 2.

Your patch looks like you always use the content manager, maybe it's possible to only use it if we detect an incomplete archive (that would be somewhere in the `except` clause I guess)? Also, could you add a test? :)
",jdemaeyer,rootAvish
1905,2016-04-12 09:34:30,"@kmike  I incorporated the changes you and @jdemaeyer suggested, and also added a test. Any feedback for code quality enhancements etc. is appreciated, also would you like to me to apply these changes to a fresh clone and open a different pull request with just one-two commit containing the entire patch, since merging this one would add useless commit history to the log?
",rootAvish,jdemaeyer
1905,2016-04-12 09:34:30,"@kmike  I incorporated the changes you and @jdemaeyer suggested, and also added a test. Any feedback for code quality enhancements etc. is appreciated, also would you like to me to apply these changes to a fresh clone and open a different pull request with just one-two commit containing the entire patch, since merging this one would add useless commit history to the log?
",rootAvish,kmike
1905,2016-04-29 16:27:07,"Hey @rootAvish,

I've added a few more (small!) line comments, but overall I think the approach is good now. :)

> would you like to me to apply these changes to a fresh clone and open a different pull request with just one-two commit containing the entire patch, since merging this one would add useless commit history to the log?

You can prune the commit history by [squashing your commits](https://davidwalsh.name/squash-commits-git), then force-pushing, no need to create a separate PR

Cheers,
-Jakob
",jdemaeyer,rootAvish
1905,2016-05-30 17:18:52,"Hey @rootAvish,

thanks for all the changes and again for investigating this. I've spent a little time playing around with the test you wrote, and I found that the wrap-around read comes from our existing [read failure recovery mechanism](https://github.com/scrapy/scrapy/blob/master/scrapy/utils/gz.py#L46). It looks like what happens is this:
- The first chunk is read by `read1()`, and we add it to `output`
- On the second call to `read1()`, we get an `IOError`, and go into our `except` clause
- We add all the contents of `f.extrabuf` to our `output`. _However_, for some reason `GzipFile` has not yet updated `extrabuf` and it still contains the chunk that we already read. Hence, the first 8196 are replicated in our output (the ""wrap-around read"").

I found that, while `f.extrabuf` was not yet updated, `f.extrasize` (a variable presumably holding the size of the extra buffer) was already updated, and the tests you created pass when I replace the line



with



(reading only the end of the extra buffer).

Maybe this could be a simpler fix?
",jdemaeyer,rootAvish
1905,2016-06-06 19:05:10,"@jdemaeyer, sorry for responding so late, but yeah this fix is much better, I wasn't aware of the existence of `f.extrasize`. I didn't comment thinking perhaps you'd like to make the fix since you're the one who solved the problem. But I've made the change here too should you want to contain both the test and the fix in the same PR.
",rootAvish,jdemaeyer
1905,2016-08-08 10:25:26,"Sorry, must've missed your last comment here.

LGTM, could you rebase? @redapple you recently merged changes to `scrapy.utils.gz`, but it seems that they're not related to this 'properly recover from illegal EOF' issue, right? /cc @kmike 
",jdemaeyer,kmike
1905,2016-08-09 18:32:40,"@jdemaeyer I did the rebase, I think we can merge this now.
",rootAvish,jdemaeyer
1905,2016-08-12 16:22:27,"LGTM. Thanks @rootAvish !
The only thing I'm puzzled with is the difference between the short fix (which is always good)



and the description of the PR about a previous fix implementation.
Maybe it could be updated to reflect the last implementation choice.
",redapple,rootAvish
1905,2016-08-17 11:42:26,"@redapple Hi, sorry I missed that comment earlier. The fix is detailed in @jdemaeyer 's comment above, and the title of the PR has also been changed, do you want me to update the description in the commit message?
",rootAvish,redapple
1905,2016-08-17 11:42:26,"@redapple Hi, sorry I missed that comment earlier. The fix is detailed in @jdemaeyer 's comment above, and the title of the PR has also been changed, do you want me to update the description in the commit message?
",rootAvish,jdemaeyer
1905,2016-08-17 12:01:34,"@rootAvish , I may very well be bikesheeding here, so apologies :)
but I'd prefer a PR title that reflects the change a little better.
The current _Modification to the existing read failure recovery mechanism, fixes #1606: response.body is duplicate_ could be more explicit as to what was changed in gzip read.
Also, _fixes #1606: response.body is duplicate_ can be left in the PR description (where it is already), I believe GitHub will pick it up when this is merged.
",redapple,rootAvish
1905,2016-08-17 12:21:45,"@redapple I see, is it fine now?
",rootAvish,redapple
1905,2016-08-17 12:51:44,"Thanks @rootAvish 
",redapple,rootAvish
1897,2016-04-13 13:45:59,"@aron-bordin , sure!
Volunteers always welcome :)
Why in `from_response` and not the constructor?
",redapple,aron-bordin
1896,2016-03-31 10:39:57,"nice improvement @kmike !
worth contributing back to Twisted?
",redapple,kmike
1896,2016-03-31 13:50:49,"@redapple Twisted provides `_ChunkedTransferDecoder`; `decode_chunked_transfer_twisted` is not a Twisted function, it is an example on how to implement our `decode_chunked_transfer` using `_ChunkedTransferDecoder`. As last example shows, it can be pretty fast if body is passed in parts, and I think  (not 100% sure) Twisted does that already. It should be possible to rewrite `_ChunkedTransferDecoder` to not use `text.split` (a main cause of O(N^2) complexity) though.
",kmike,redapple
1896,2016-12-08 09:50:42,"@kmike , I believe we can close this now that #2411 is merged?",redapple,kmike
1896,2016-12-08 10:52:31,"@redapple yeah, let's close it.",kmike,redapple
1895,2016-04-04 17:32:53,"@kmike , did you consider moving `scrapy.downloadermiddlewares.cookies.CookiesMiddleware` upwards towards the engine?
e.g.


",redapple,kmike
1895,2016-04-26 19:45:36,"@redapple it turns out this is not really related to CookiesMiddleware; scrapy-splash doesn't use it to handle cookies in the end, and handles cookies using a custom . 

scrapy-splash uses original JSON response.body to set response.status, response.headers, response.url and a new response.body. Any other downloader middleware which wants to return a new response based on current response body must be put closer to Engine side than HttpCompressionMiddleware. Currently it means that RedirectMiddleware and CookiesMiddleware won't process such adjusted responses.
",kmike,redapple
1893,2016-03-30 13:16:38,"@kmike , which change you mean?
I tried with Sphinx 1.3.6 (forcing <1.4 in tox.ini) and it worked for me.
i'm ok with `Sphinx>=1.3.6`
",redapple,kmike
1893,2016-03-30 14:55:37,"Thanks, @redapple !
",nyov,redapple
1893,2017-03-19 14:56:48,"@redapple @kmike  , since `sphinx_rtd_theme` has to be installed manually for building the docs, right now, it has only been added to the `tox.ini` file. So wouldn't it be better to add it to the `requirements.txt` as well? Or it could be added to [docs/README.rst](https://github.com/scrapy/scrapy/blob/master/docs/README.rst), something like

What do you think?",harshasrinivas,redapple
1893,2017-03-19 14:56:48,"@redapple @kmike  , since `sphinx_rtd_theme` has to be installed manually for building the docs, right now, it has only been added to the `tox.ini` file. So wouldn't it be better to add it to the `requirements.txt` as well? Or it could be added to [docs/README.rst](https://github.com/scrapy/scrapy/blob/master/docs/README.rst), something like

What do you think?",harshasrinivas,kmike
1891,2016-03-30 16:53:25,"@djunzu , would you mind rebasing against current ""master"" branch?
The Travis CI build failure for docs is now [fixed](https://github.com/scrapy/scrapy/pull/1893).
",redapple,djunzu
1891,2016-03-30 20:04:35,"@djunzu awesome, thanks for the PR! 
Could you please add docs for new options?
",kmike,djunzu
1891,2016-03-30 20:35:36,"@kmike , there are no new options. They were all available before!
But indeed some of them are not documented. Is it enough to document them [here](http://doc.scrapy.org/en/master/topics/media-pipeline.html)?
",djunzu,kmike
1891,2016-03-30 21:10:13,"@djunzu ouch, a good call! Some options are still not documented (see http://doc.scrapy.org/en/master/topics/settings.html#settings-documented-elsewhere - not all IMAGE/FILES options have links). But that's not directly related to this PR. If you can fix that it'd be great, but a PR looks good as-is :+1: 
",kmike,djunzu
1891,2016-04-08 08:19:29,"Awesome, thanks @djunzu!
",kmike,djunzu
1890,2017-01-24 15:02:53,"Hello @kmike! I think the problem is in [this line](https://github.com/scrapy/scrapy/blob/master/scrapy/utils/reqser.py#L50), the `request_from_dict` function always returns a `Request` object regardless of the type of the request passed to `request_to_dict`. Please check the above PR which tries to solve the issue. Thanks!",elacuesta,kmike
1887,2016-11-11 20:29:01,"@nyov I like this approach. 
And wow, it seems you've found a pretty serious issue in scrapy/xlib/tx.**init**.py
",kmike,nyov
1887,2016-11-22 12:59:14,"Hi guys, please take a look to https://github.com/scrapy/scrapy/pull/2403. It fix the issue described by @nyov in https://github.com/scrapy/scrapy/pull/1887#issuecomment-260047541",dangra,nyov
1887,2016-12-02 04:28:10,"Well, this looked easy in concept...
There seem to be quite a few ways to skin this cat, however.
I've been conservative I think, we could remove yet more code, which should not really be used by anyone directly but only from inside `xlib.tx` (`interfaces.py`, `iweb.py`, `_newclient.py`).

Now this may be a nightmare to review. I'm not sure after all this work whether it's actually comprehensible or useful.
So in theory there are three branches here:



Findings:
Twisted 11.1: all Exceptions can be imported from twisted.
Differences between Twisted 11.1 and 12.0.0 (debian wheezy/oldstable) for this code are  nonexisting. Bumping to 11.1 would have the same result, and be enough.
Twisted 12.1: `TCP4ClientEndpoint` can be imported from twisted.
Twisted 12.3: `Agent` and `ProxyAgent` can be imported from twisted.
Twisted 13.1: `HTTPConnectionPool` can be imported from twisted. Which is everything.

As @dangra said, wheezy-backports includes Twisted 13.2 however, so the next obvious choice would be to go for 13.1 directly I guess, and should be safe enough, if we even care about debian here.
Current debian stable (""jessie"") is at Twisted 14.0 and has been around for some time. (With next stable ""stretch"" freezing in Feb. 17, perhaps releasing around June.)


(Aside: I had to leave this for a while, it took me 5 rebases at least through the patch stack to bring the patches in line; but every other time I edited one commit and rebased the rest back on top, git would wrongly consider some things newer and merge it into the later commits -- breaking the rebased commits. So lots of re-checking against twisted codebase and re-re-fixing. Infuriating.
Finally just had to reaquaint myself with quilt and get to know StGit, to finish this. Both seem worth knowing.

And I felt like a bomb expert while doing this, tracing all the imports that wanted to trip me up. Good thing I'm not, and have unlimited tries.)
",nyov,dangra
1887,2016-12-02 10:54:03,"@nyov thanks for your work! 

Since Scrapy 1.2 the baseline is Debian Jessie, so it is fine to bump Twisted to 14.0. 
jessie-backports contain Twisted 16.2.0, but I'm not sure what's our policy regarding backports.",kmike,nyov
1887,2016-12-07 15:10:37,"LGTM.
@dangra , @kmike , @eliasdorneles ?
This supercedes #2403 I believe, for master branch at least",redapple,dangra
1887,2016-12-07 15:10:37,"LGTM.
@dangra , @kmike , @eliasdorneles ?
This supercedes #2403 I believe, for master branch at least",redapple,kmike
1887,2016-12-08 09:49:03,Thanks again @nyov !,redapple,nyov
1885,2016-03-30 08:52:45,"Hi I tried the virtualenv, still I'm hitting the wall, below are the steps I followed:



I did the above again without using python3, my default python version is 2.7 and scrapy worked this time...I don't understand what is the issue with python3, but this is enough for me to get started....Thanks @kmike for the pointers
",nikhilgeo,kmike
1885,2016-03-30 10:22:06,"Oh yeah it worked for Python3..!!
Thanks @redapple 
",nikhilgeo,redapple
1881,2016-03-30 12:53:01,"@nyov , CI error on docs should be fixed with https://github.com/scrapy/scrapy/pull/1893
",redapple,nyov
1880,2016-03-25 20:01:19,"@stav I think that's temporarily, to make tests working until w3lib with fixes is released.
",kmike,stav
1880,2016-03-29 13:08:48,"@stav , @kmike is correct. I copied the code from https://github.com/scrapy/w3lib/pull/45 (now merged).
I'll update the PR once a new w3lib is released
",redapple,stav
1880,2016-03-29 13:08:48,"@stav , @kmike is correct. I copied the code from https://github.com/scrapy/w3lib/pull/45 (now merged).
I'll update the PR once a new w3lib is released
",redapple,kmike
1879,2016-03-25 12:06:18,"@redapple there is a picture for this data flow in docs (http://doc.scrapy.org/en/latest/topics/architecture.html).
",kmike,redapple
1879,2016-03-25 12:30:00,"I know @kmike :) but I still think it could be clearer. I'll suggest something concrete
",redapple,kmike
1878,2016-04-20 09:33:46,"how should this cookiejar api be designed @kmike ? Should it be part of Scrapy or perhaps some external library? I imagine that some external library could simply subclass cookie middleware and add some useful functions and utilities - e.g. for setting/getting cookies or maybe even persisting cookies across spider runs (something that is currently not supported but could be very useful). Reading about some bot detection systems, e.g. [here](https://www.blackhat.com/docs/asia-16/materials/asia-16-Sivakorn-Im-Not-a-Human-Breaking-the-Google-reCAPTCHA-wp.pdf) they seem to appreciate clients that have long living cookies, so perhaps persisting some cookies could be useful in dealing with them.

One problem here is communicating between cookie middleware and spider. Cookiejars are stored as attribute of middleware, so if we want to expose cookiejars they would have to be attribute of spider probably. Are there any problems with linking middleware ""jars"" to spider, e.g. add spider opened to middleware and set middleware ""jars"" on spider instance, then add some methods for getting and setting cookies in middleware and make them available from spider.
",pawelmhm,kmike
1878,2016-12-12 14:56:25,"+1 to exposing cookiejars.

I'm needing it now for a new project, and intend to do the same as @pawelmhm mentioned (custom middleware adding a spider attribute referencing the jars object).",eliasdorneles,pawelmhm
1875,2016-03-26 13:23:34,"+1, looks like a safe change in any case.

This should be merged into master, though (and then maybe backported). But alas, changing the target branch in an open PR isn't possible - hopefully someone does manually merge it.

@nanolab, you can change the commit title by doing a `git checkout patch-1; git commit --amend -m ""FS cache storage misuses mtime for expiration""` or the equivalent in whatever GUI, then `git push --force origin patch-1`
",nyov,nanolab
1875,2016-03-27 20:57:53,"Thanks @nanolab for the fix and @nyov and @Digenis for the review! I think that's not a big deal to leave commit message as-is; changing it would be nice, but not required. A good point about 1.0 branch; we should cherry-pick it to 1.1 and to master after merging.
",kmike,nanolab
1875,2016-03-27 20:57:53,"Thanks @nanolab for the fix and @nyov and @Digenis for the review! I think that's not a big deal to leave commit message as-is; changing it would be nice, but not required. A good point about 1.0 branch; we should cherry-pick it to 1.1 and to master after merging.
",kmike,nyov
1875,2016-03-27 20:57:53,"Thanks @nanolab for the fix and @nyov and @Digenis for the review! I think that's not a big deal to leave commit message as-is; changing it would be nice, but not required. A good point about 1.0 branch; we should cherry-pick it to 1.1 and to master after merging.
",kmike,Digenis
1871,2016-03-21 14:31:42,"@redapple Oh! I'm sorry, thank you. :)
",NiranjanaD,redapple
1870,2017-02-08 12:12:41,Closing it as per @moisesguimaraes's comment.,kmike,moisesguimaraes
1869,2016-03-18 13:38:15,"Thanks @eldios !
",eliasdorneles,eldios
1868,2016-03-28 10:17:57,"@kmike yes, indeed. The key thing here is exactly in Splash: sometimes pages comes with nested iframes and I believe it is best to process them as separate response objects. Such approach allows to apply middleware & other neat things from Scrapy frameworks.
",dizlv,kmike
1868,2016-03-29 14:45:12,"@kmike in this particular case I had middleware which built absolute links for responses. But as long as atm it can not process multiple responses (f.e. iframes) I had to move the code in spider.
",dizlv,kmike
1864,2017-03-14 07:56:54,"@kmike json.dumps({'records': [{'en_kicker': u'Sinosphere'}]}) is ok, but if {'records': [{'en_kicker': u'Sinosphere'}]} as a item(for example json.dumps(dict(RecordItem))),then get the error",peter-wang-wsl,kmike
1864,2017-03-14 09:57:52,"@kmike 
**example**

_file: item.py_
class QAMainItem(scrapy.Item):
    title = scrapy.Field()
   type = scrapy.Field()
   ask_time = scrapy.Field()
   asker = scrapy.Field()
   question_content = scrapy.Field()
   answer_num = scrapy.Field()
   answers = scrapy.Field()
class QAAnswerItem(scrapy.Item):
    answer = scrapy.Field()
    answer_type = scrapy.Field()
    answer_time = scrapy.Field()
    answer_content = scrapy.Field()

_file:spider.py_
    def store_item(self, question, title, type):
        ask_time = question.xpath(""div[1]/h1/b/text()"").re(r'\d+/\d+/\d+ \d+:\d+:\d+')[0]
        asker = question.xpath(""div[1]/h1/span/text()"").extract_first()
        question_content = question.xpath(""div[1]/p/text()"").extract_first().strip()
        answer_num = question.xpath(""div[1]/p/span/text()"").re(r'\d+')[0]
        question_item = QAMainItem()
        question_item['ask_time'] = ask_time
        question_item['asker'] = asker
        question_item['question_content'] = question_content
        question_item['answer_num'] = answer_num
        question_item['title'] = title
        question_item['type'] = type
        answer_list = []
        for index, qa in enumerate(question.xpath(""div"")):
            if 0 == index:
                pass
            else:
                answer_item = QAAnswerItem()
                answer_item['answer'] = qa.xpath(""h2/text()"").re(r'\w+')[0]
                answer_item['answer_type'] = qa.xpath(""h2/strong/text()"").extract_first()
                answer_item['answer_time'] = qa.xpath(""h2/b/text()"").extract_first()
                answer_item['answer_content'] = qa.xpath(""p/text()"").extract_first().strip()
                answer_list.append(answer_item)
        question_item['answers'] = answer_list
        yield question_item

_file:pipeline.py  class:JsonWriterPipeline_
    def process_item(self, item, spider):
        line = json.dumps(dict(item), indent=4) 
        self.file.write(line.decode(""unicode_escape"") + ',' + ""\n"")
        return item
",peter-wang-wsl,kmike
1864,2017-03-14 12:32:44,"@kmike @danbao i get the solution! it is use defaut in json.dumps().
example
1.item.py
change QAAnswerItem to class QAAnswer(object)
2.pipeline.py class:JsonWriterPipeline
use json.dumps(dict(item), indent=4, default=self.answer2dict)

    def answer2dict(self, std):
        return {
                    'answer':std.answer,
                    'answer_type':std.answer_type,
                    'answer_time':std.answer_time,
                    'answer_content':std.answer_content
               }

",peter-wang-wsl,kmike
1864,2017-03-15 00:17:16,@kmike thanks your answer and suggestion,peter-wang-wsl,kmike
1864,2017-03-15 00:42:08,"@kmike I try it.And your sample need to be changed like below.

from scrapy.utils.serialize import ScrapyJSONEncoder
json.dumps(..., cls=ScrapyJSONEncoder)
",peter-wang-wsl,kmike
1862,2016-03-15 10:12:43,"@Digenis ah, oops! Thanks for pointing out that mistake.

Do you have any idea what might be going on with the `Spider.closed` approach?
",johtso,Digenis
1860,2016-03-24 16:57:50,"I think this note is fine; it may save some time for someone using our example in production. 
But +1 to @Digenis that a comment should be about serialization in general. There are other issues - e.g. mongo has troubles with dots in key names.
",kmike,Digenis
1857,2016-03-16 15:38:38,"@pawelmhm , I was suggesting to use 200, 300, 400, 500 reasons for their respective classes and ""Unknown Reason"" for invalid classes.
But it's probably more cosmetic than anything as I doubt many people look at reason message
",redapple,pawelmhm
1855,2016-03-09 16:49:27,"Thanks for answering @redapple.

The solution was changing `base64.encodestring` to `base64.b64encode` in my ProxyMiddleware.
Did `scrapy shell 'https://www.base.net'` a few times and printed `request.meta`. The value for `meta['proxy']`changes each time and corresponds to those in my proxy list.
",Cesped,redapple
1854,2017-01-19 15:10:02,"it seems like work on Twisted client is not making much progress now, but this example you linked @kmike doesn't look terribly complicated so maybe we could just add h2 as Scrapy requirement and write our own HTTP2 Twisted-Scrapy client? ",pawelmhm,kmike
1854,2017-03-06 19:37:37,"Hi, 
I am a student from India, planning to participate in GSoC this year. There was a project regarding ""New HTTP/1.1 download handler"", this was required because some issues with twisted API, but they have recently resolved those issues (@redapple told me). 
Then I realized that scrapy doesn't have support for HTTP/2 so why not choose this as GSoC project.

I have also done a bit of research about twisted implementing HTTP/2, python-hyper which is nice implementation of HTTP/2, twisted also uses h2 for supportng for HTTP2.
Currently, Twisted only have support for HTTP2 on server side ( @kmike   )

As @pawelmhm mentioned, one way is to use h2 and start the work. Since we don't know when Twisted will add support for HTTP2 client, so we should write our client.

I am seeking your reviews for this idea, If I should do it, what things should I keep in mind, where can I find more info about HTTP2. And most important, is this idea worth? (Twisted might add support for HTTP2 in future)",AnshulMalik,pawelmhm
1854,2017-03-06 19:37:37,"Hi, 
I am a student from India, planning to participate in GSoC this year. There was a project regarding ""New HTTP/1.1 download handler"", this was required because some issues with twisted API, but they have recently resolved those issues (@redapple told me). 
Then I realized that scrapy doesn't have support for HTTP/2 so why not choose this as GSoC project.

I have also done a bit of research about twisted implementing HTTP/2, python-hyper which is nice implementation of HTTP/2, twisted also uses h2 for supportng for HTTP2.
Currently, Twisted only have support for HTTP2 on server side ( @kmike   )

As @pawelmhm mentioned, one way is to use h2 and start the work. Since we don't know when Twisted will add support for HTTP2 client, so we should write our client.

I am seeking your reviews for this idea, If I should do it, what things should I keep in mind, where can I find more info about HTTP2. And most important, is this idea worth? (Twisted might add support for HTTP2 in future)",AnshulMalik,redapple
1854,2017-03-06 19:37:37,"Hi, 
I am a student from India, planning to participate in GSoC this year. There was a project regarding ""New HTTP/1.1 download handler"", this was required because some issues with twisted API, but they have recently resolved those issues (@redapple told me). 
Then I realized that scrapy doesn't have support for HTTP/2 so why not choose this as GSoC project.

I have also done a bit of research about twisted implementing HTTP/2, python-hyper which is nice implementation of HTTP/2, twisted also uses h2 for supportng for HTTP2.
Currently, Twisted only have support for HTTP2 on server side ( @kmike   )

As @pawelmhm mentioned, one way is to use h2 and start the work. Since we don't know when Twisted will add support for HTTP2 client, so we should write our client.

I am seeking your reviews for this idea, If I should do it, what things should I keep in mind, where can I find more info about HTTP2. And most important, is this idea worth? (Twisted might add support for HTTP2 in future)",AnshulMalik,kmike
1847,2016-03-17 16:46:22,"@aron-bordin , could you add tests for this feature and setting to `scrapy.tests.feedexport`?
",redapple,aron-bordin
1847,2016-03-17 22:49:33,"Hi @redapple , I updated the PR. Please, take a look when possible.
",aron-bordin,redapple
1847,2016-03-18 18:09:42,"@aron-bordin , could you add a test for the default case to make Codecov happy? :)
",redapple,aron-bordin
1847,2016-03-18 21:56:29,"@redapple updated
",aron-bordin,redapple
1847,2016-03-31 09:50:40,"looks good to me. Thanks @aron-bordin !
",redapple,aron-bordin
1847,2016-04-01 03:25:31,"HI @kmike I updated the name to `FEED_TEMPDIR`, it sounds easier. Let me know if there is something else that I can change.

Thx
",aron-bordin,kmike
1847,2016-04-01 13:38:20,"@redapple are you OK with the updated option name?
",kmike,redapple
1845,2016-03-04 17:50:04,"@kmike What about this:
 If i am in project path can i pass spider name rather then URL to get inside shell and get response for the particular spider eg: if i have 2 spiders in my project can i do this: 
    `scrapy shell spider1`
",rosnk,kmike
1845,2016-03-04 18:17:30,"Normally we pass URL when we want to run shell eg: scrapy shell {url} and we get response back where we can play with response using css or xpath selector. 

Similarly, if i am in scrapy project path (in the same path where scrapy.cfg file is in), and if i type scrapy shell with no parameter i will get ""crawler"" object right. So what i intend is i need to run particular spider from many spider in my project and some how get response to play with it using scrapy selector(xpath or css).  OR is there a way to pass spider name while typing scrapy shell from the project path(where scrapy.cfg is in). 

As per @kmike, scrapy shell --spider={spider_name} does not provide me with ""response"" object. Rather it gives me ""crawler"" object.  
",rosnk,kmike
1845,2017-02-27 15:00:37,"@rosnk
have you solved the issue?

@kmike 
Can you comment please?  In my understanding, by calling spider from shell we should get response object that we can play with. And this response is recieved directly from spider we called. With all the settings that apply.",archfch2,kmike
1842,2016-03-04 10:50:40,"Many thanks @nyov !
",redapple,nyov
1840,2016-03-04 13:10:22,"Thanks for comments.

As @Digenis mentioned, I've chosen to use meta instead of settings because 'empty' could be a valid response in some cases. We can have the best of both worlds if I test for `request.meta.get('retry_empty', settings.getbool('RETRY_EMPTY_RESPONSE'))`

The `retry?` and `RejectResponse` alternatives have also crossed my mind. I like those, but I opted to keep it simple on my first PR at the project.

I prefer `RejectResponse`, but as you mentioned, that sounds more difficult to implement, and I'm afraid I'm not familiar with the details.

I can gladly ditch this patch in favor of a `retry?` approach.
",paulo-raca,Digenis
1837,2016-03-06 10:58:12,"@kmike: Is cancelling a response/download the right way though? It galls me that there is no better way. Sending two requests for every resource ending in `.gz` is ugly (HEAD first, then possibly GET).

But `RST`ing the connection seems like it shouldn't be a ""default action"", either.
The server will be sending the response body already at that time. A RST on the connection might impact things like HTTP pipelining (Without research that's just a guess, though, not sure if twisted even supports it.)
Depending on the number of such aborted requests, that might be considered a DoS attack, since it asks the server for the _full_ response first, then aborts when the server did work to fetch or compute the ressource (possibly on-the-fly compression of something big).
Since that's a special case only for `.gz` extensions, it might not happen that often, but we should be aware of that.
",nyov,kmike
1835,2016-03-02 11:02:41,"Makes sense. Thanks @djunzu !
",redapple,djunzu
1832,2016-04-04 17:43:27,"@nyov , FYI, I tried with https://github.com/scrapy/scrapy/pull/1874 and contacting www.google.com with IPv6 and got the host to be passed correctly to Twisted,
but it seems that the default [`Agent` only handles IPv4](https://twistedmatrix.com/trac/browser/tags/releases/twisted-16.0.0/twisted/web/client.py#L1451), and missed the correct `Host` header value with brackets.

What wireshark sniffed (it did contact the correct endpoint at `2a00:1450:4007:80d::2004`)



Scrapy shell showing HTTP 400 (for bad host header presumably)


",redapple,nyov
1819,2016-03-02 07:56:46,"@redapple  thank you very much for your advice, i will seriously think about it.
",aimer12,redapple
1819,2016-03-02 08:02:01,"@lopuhin  thank you very much for your advice, it works!
",aimer12,lopuhin
1819,2016-03-02 08:16:45,"@redapple ^ ^ i didn't notice the !note , i could have read doc more  carefully. 
",aimer12,redapple
1819,2016-07-30 08:17:37,"And, I use Python3.5.2, 
@redapple @aimer12 @lopuhin 
Any way can solve this? I think it maybe lxml's problem....
thx.
",nolanjian,redapple
1819,2016-07-30 08:17:37,"And, I use Python3.5.2, 
@redapple @aimer12 @lopuhin 
Any way can solve this? I think it maybe lxml's problem....
thx.
",nolanjian,lopuhin
1818,2016-02-26 12:15:53,"@redapple yes, why not
",kmike,redapple
1817,2016-02-25 22:05:56,"@nyov Spider.from_crawler attaches settings after `__init__` is called.
",kmike,nyov
1817,2016-02-25 22:42:58,"@kmike: Actually can you say why that is? What's the reason for not passing `crawler` as optional kwarg to spider on instancing?
",nyov,kmike
1807,2016-02-24 14:03:39,"@redapple Hello, and thank you for your attention. This is a full dump from my last scrapy run. As I understand, when I'm use a concurrent > 1, some proxies are working good, but only in a first iteration. Next iterations using a previous proxies and don't change it. Look at it:



As you see, the proxies in the second batch (real proxy, from which request sent to httpbin) are the same, as in the first batch (include its order). Little later I'll give you Wireshark dump, but as I see, roxies really didn't changed. Thank you.

UPD.
Just now I've try to do same thing with a http protocol (use http://httpbin.org instead of https) and didn't have that issue! Proxy was changed correctly. So, I think @rverbitsky was right, this is really some kind of https issue.
",DrJackilD,redapple
1807,2016-05-02 22:24:57,"@redapple Thank you so much! I've test Scrapy 1.1.0rc4 and can confirm, that this issue is solved! Also, thank you for @rverbitsky!
",DrJackilD,redapple
1806,2016-02-23 10:17:43,"thanks @darshanime for starting work on this.

From the example website in https://github.com/scrapy/scrapy/issues/1615#issuecomment-186237075 , I don't think ""Location"" header is mandatory to consider the response valid.

See response headers from my Chrome browser while inspecting http://www.carroya.com/web/vehiculo/nuevoficha/renault/kangoo/c.a./2016/no-disponible/1616709.do



Maybe a logic like this is simpler:



Also, this needs unit tests.
",redapple,darshanime
1806,2016-02-24 07:47:33,"@redapple, I believe a true `201` has to include the Location header. Taken from [here](https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html)

> 201 Created
> The request has been fulfilled and resulted in a new resource being created. The newly created resource can be referenced by the URI(s) returned in the entity of the response, with the most specific URI for the resource given by a Location header field. The response SHOULD include an entity containing a list of resource characteristics and location(s) from which the user or user agent can choose the one most appropriate. 

Yes, will add the tests!
",darshanime,redapple
1806,2016-02-26 10:28:19,"@redapple, review please!
Why is appveyor complaining?
",darshanime,redapple
1806,2016-03-02 13:37:12,"@darshanime , I've no idea why appveyor kicked in here...

About the PR,
I believe it's easier to keep successful response in the current ""else"" of `if response.status != 200:`, and 201 is a successful response in most cases.

Also, ""Location"" header in 201 response is a _should_ not a must.
So scrapy should accept it.
https://github.com/scrapy/scrapy/issues/1615#issuecomment-186237075 shows valid response, with data, and without ""Location"" header.

So, following _""Be conservative in what you send, be liberal in what you accept""_ saying, I suggest you just test response.status == 201 and ""location"" header to perform that extra request when there's no data in body.
Otherwise, the response can continue with current instructions (I believe)

What I imagined is something like (very ugly pseudo-code):



what do you think?
",redapple,darshanime
1806,2016-11-04 17:01:33,"@darshanime Please add changes that @redapple propose
",kurkop,redapple
1806,2016-11-04 17:01:33,"@darshanime Please add changes that @redapple propose
",kurkop,darshanime
1805,2016-02-23 14:24:22,"Hi @kmike, yes looks like implementing the _load_all_spiders method is the most straightforward solution :) that would allow to subclass without touching the constructor.
",lagenar,kmike
1801,2016-02-21 06:18:58,"Thanks @redapple ! Looks good.
",lopuhin,redapple
1799,2016-03-24 11:13:27,"Thanks @Digenis , sorry for the delay in reviewing.

I do understand the need to customize what's extracted as text for the link from the elements.
However, I would find it difficult to grasp (say, as a new scrapy user) the new ""text"" argument as either:
- _string argument for XPath string() function applied to the element, defaulting to """"_
- or, _a callable returning a string when given a link-matching element as input_

(this would need to be documented I believe. Shame that LxmlParserLinkExtractor is not already)

I would prefer if `LxmlParserLinkExtractor` had a method taking the matching element and returning a string. it would default to `return lxml.etree.XPath('string()')(element)`. And you could subclass it to customize the behavior.
Would that work for you?

@eliasdorneles , @dangra , @kmike, any thoughts ?
",redapple,Digenis
1799,2016-03-24 11:33:49,"@Digenis , about #1403 , I started fixing them on the top of https://github.com/scrapy/w3lib/pull/45 (and https://github.com/scrapy/scrapy/pull/1874)
I hope I can push a PR soon enough, and make it part of 1.1
",redapple,Digenis
1796,2016-02-24 17:29:21,"@dangra , I would go with private by default, proper warning in release notes, and taking advantage of RC2 so users can tell us if it's a good move or not (sure, some users would discover only when 1.1 is officially baked)

@kmike , @eliasdorneles , @curita , @jdemaeyer , any comments ?
",redapple,dangra
1795,2016-02-19 15:44:24,"Thanks @AmbientLighter !
",redapple,AmbientLighter
1794,2016-02-23 17:53:26,"@dangra , I wasn't planning on another 1.0 release.
Do you think it's needed?
Also, in any case, it'd be good to test it out with users and 1.1rc2, so they can tell us if it's indeed better with this fix
",redapple,dangra
1794,2016-02-23 18:02:55,"@redapple: testing on 1.1rc2 sounds good to me, let's see later if we need to backport it to 1.0 then. thankx 
",dangra,redapple
1792,2016-02-19 05:55:43,"Yes, everything is here, @redapple , thank you!
",lopuhin,redapple
1790,2016-02-19 17:47:41,"Hey @lopuhin!
I would like to attempt to set `fake-s3` up. I installed `fake-s3` on my local machine and successfully tried a few `put`, `rb` requests. Now, for our tests, I would need to install ruby and flask-s3 in `tox` and Travis environments by editing the `tox.ini` and `.travis.yml`. In the `TestS3FilesStore` test itself, what about this:



(We can start the flask-s3 server at port `4567` in the `tox.ini` and `.travis.yml`.)

Am I on the correct path?  
",darshanime,lopuhin
1790,2016-02-19 20:29:02,"Hey @darshanime , I think your approach is correct, please go ahead! Maybe it would also be nice to check that `fake-s3` is running at the start of the test, and skip the test if it's not.
",lopuhin,darshanime
1790,2016-02-20 17:25:49,"Hey @kmike, @lopuhin!
I tried to use `moto` today to simulate S3 with mixed success.
I used pytest fixtures to setup the S3 session as suggested.



The tests **passed** when I ignored the `is_botocore()` checks everywhere(by setting it to false). However, with `botocore`, I am getting `SSLError: [Errno bad handshake] (32, 'EPIPE')` error. 
",darshanime,lopuhin
1790,2016-02-20 17:25:49,"Hey @kmike, @lopuhin!
I tried to use `moto` today to simulate S3 with mixed success.
I used pytest fixtures to setup the S3 session as suggested.



The tests **passed** when I ignored the `is_botocore()` checks everywhere(by setting it to false). However, with `botocore`, I am getting `SSLError: [Errno bad handshake] (32, 'EPIPE')` error. 
",darshanime,kmike
1790,2016-02-20 17:27:55,"Hey @darshanime - with mock_s3 we won't be able to test botocore, so it is worse than fake-s3; but maybe we can use moto in server mode for tests if it provides a server which emulates s3 like fake-s3.
",kmike,darshanime
1790,2016-02-20 19:20:40,"@darshanime subprocess / Popen is fine
",kmike,darshanime
1788,2016-02-18 10:34:55,"I'm sorry, that was a typo. We do pass with `-s`, I've updated my issue accordingly.

@kmike Thanks, I'll take a look at that
",GilJ,kmike
1787,2016-02-18 10:22:35,"@kmike , [Travis says](https://travis-ci.org/scrapy/scrapy/requests)
`f766dd0 .. branch not included or excluded`
",redapple,kmike
1787,2016-02-18 10:24:47,"@redapple the branch should be excluded, but the PR shouldn't - previously tests were run twice for all such PRs, so we excluded non-release branches from Travis in [.travis.yml](https://github.com/scrapy/scrapy/blob/master/.travis.yml)
",kmike,redapple
1786,2016-02-17 16:33:56,"Thanks @redapple!
",kmike,redapple
1784,2016-02-18 15:28:09,"@kmike that's correct, that is what pretty much what I had to do to process my html results using Zorba, but I would like it to be included by default in Scrapy's Selector for it to be used the same way in current Lxml selectors, for example:

`Selector(response=response).xquery('...').extract()`

or 

`response.selector.xquery('...').extract()`

I guess I can achieve the same result by writing my own Downloader to return something like a ZorbaResponse and writing my own ZorbaSelectors but, I said it would be great to be included by default in Scrapy
",gerosalesc,kmike
1784,2016-02-18 15:36:23,"@redapple Oh shame on me to be outdated with the architecture of the project... I totally agree I should move this, thanks.
",gerosalesc,redapple
1784,2016-02-18 15:47:22,"@redapple  ok, then They are actually good news. Decoupling is most of the time a good idea.  
",gerosalesc,redapple
1781,2016-02-16 19:21:39,"@kmike if you try what do you get?

`scrapy shell ""https://olx.pt/anuncio/escritrio-em-lea-da-palmeira-IDv0opO.html#c49d3d94cf""` 
`response.css('.brkword.lheight28::text').extract()[0].encode('utf8')`

I get `Escrit\xc3\xb3rio em le\xc3\xa7a da palmeira` and it should be `Escritório em leça da palmeira`
",psychok7,kmike
1781,2016-02-16 19:26:52,"@kmike oh i see.. but how do i force it to output it the ""proper"" way to the json file when i `-o items.json` ?
",psychok7,kmike
1780,2016-02-16 15:29:33,"Thanks @kmike 

The version is v1.0.5.  I installed from pip today.

I faced another error before this, I changed 'itervalues()' to 'values()', then this error hadn't appeared any more.

File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/scrapy/cmdline.py"", line 21, in _iter_command_classes
    for obj in vars(module).itervalues():
AttributeError: 'dict' object has no attribute 'itervalues'
",vincent6987,kmike
1780,2016-02-28 11:11:39,"Thanks @kmike 

It works like a charm!! :100: 
",taktikal17,kmike
1779,2016-02-26 15:59:17,"Hi @redapple 

I'm learning about scrapy and I'd like to take this issue.

How can I name this new setting ? And how should it behave ? I'm thinking about a setting that defaults to 'auto' (use the temp file), or you can set the path name (the BlockingFeedStorage raise some exception if this folder does not exitst). What do you think about it?
",aron-bordin,redapple
1777,2016-02-15 09:26:18,"Hey @nyov,

libscrapy is an interesting idea; what do you think should be in this package? 

Scrapy is different from many packages because most of its core features (e.g. redirects or cookies) are implemented as extensions/middlewares using the same extension points as users. If we bundle none of them to 'libscrapy' then libscrapy won't be useful; if we bundle all of them it'll be almost the same as scrapy. 

We were going to another direction recently, extracting parts of scrapy to separate packages (e.g. scrapy-jsonrpc, scrapy-djangoitem, parsel); in the end it may look somewhat similar to 'libscrapy'. I haven't thought about moving project support out of scrapy though. I thought it'd be good to replace our project generation with [cookiecutter](https://github.com/audreyr/cookiecutter), but cookiecutter has lots of dependencies - yaml library, jinja2, python-future, etc.; I don't want to impose all these deps on Scrapy users. `<rant>` _Once upon a time line-profiler package was broken in cases python-future library was in use (e.g. as a dependency of a dependency) because python-future is doing some monkey-patching, and it broke Python 3 compat code in line-profiler. Breaking third-party packages to make it a tiny bit easier to port to Python 3 is an awful tradeoff. YAML libraries are also a problem - they often require libyaml and won't build without a compiler. People are using someone's fork to make things easier._ `</rant>`.

In addition to project generation there are also:
- tutorial which uses scrapy project;
- default SpiderLoader which loads spiders by name from a project.

---

As for load_object, there were two proposals: make it accept classes and make it accept instances. See discussion at https://github.com/scrapy/scrapy/pull/1215. This feature is needed.

---

Another idea for scrapy-as-a-library is to provide a better way to retrieve items when spider is started using CrawlerRunner/CrawlerProcess, see https://github.com/scrapy/scrapy/issues/1395. The example at https://github.com/scrapy/scrapy/issues/1395#issuecomment-145920958 works both in Twisted and Tornado.
",kmike,nyov
1777,2016-02-26 18:43:33,"@kmike, I can appreciate the rant about using cookiecutter, and having libraries that embed too much stuff themselves in general.
Which is somewhat my point here: to be a bit more sensitive to the use-case of scrapy for embedding itself.

> - contracts;
> - item loaders;
> - all base spider classes other than Spider (i.e. CrawlSpider, InitSpider, SitemapSpider, etc);
> - link extractors;

That looks essentially like contrib, but IIRC moving `contrib` out of scrapy was scrapped at the time. I can't disagree with any of those, they would make sense to me.
But in the same style I might like Exporters to be moved out (as ""pipeline implementations"" -- somehow I always see some competition/feature overlap with Pipelines there).
...I can already hear the outcry over that, though. So removing some extensions over others is subjective here, mostly guided by popularity vote.

---

With the ""Framework vs Library"" -glasses on, I'd instead modularize or strip the ""user interface"":
- cli: `scrapy/commands/`, `scrapy/cmdline.py`
- ipython magic: `scrapy/shell`,
- debugger (can't find anymore, mostly already gone I think)

and, ""project support"":
- `scrapy/templates/`, `scrapy/commands/[genspider, startproject]`
- scrapy.cfg support: `scrapy/utils/project.py`, `scrapy/utils/conf.py`,
- settings-from-environment magic (`os.environ.get()`)

Unlike the extensions listed before, these features are pretty clear-cut to me.
- They make sense as a group, but not on their own (cli needs cfg or project env).
- They configure a spider and fire up twisted, but are not interacting with the running process
- They have no use in the embedded/'as-a-library' case, which I feel should be side-effect free (i.e. not side-loading ENV vars but being configured from the embedding code), and only add complexity and extra code there.

Broken down: The **scrapy shell** (`scrapy/commands/`) uses the **scrapy project environment**  (`scrapy.cfg` and env) to configure the **scrapy system (library)**.
It then uses this `CrawlerProcess` interface to launch and ends by executing into the twisted reactor.
(The same thing in a different skin would be: starting a dedicated scrapy app on a twisted reactor, break and idle, then attach a shell for configuration and 'cruise control' from a second process -- debugger style).
As my conclusion this makes the shell and environment the _embedding side_ here, doing very much the same as a 'scripted scrapy'.

So if the interface is the same, making this a clear distinction even in scrapy itself (embedded/library code vs embedding/framework code)
might ultimately help to find potential issues with this in-between interface, and possibly improve it.

And if the interface here was, say, a json-RPC call instead of an internal API call (crawlerprocess),
such a decision might be more obvious. But I would say it's the same in principle.

Lastly, if the UI or ""cruise-control"" part was separate, that would improve ""hackability"" of it.
It's hard to get more features into scrapy-proper (remembering the bpython shell support here).
But it'd be easier to ""spin your own"" if it was a smaller, separate, thing.
One might even have a basic setup which only understands project environments as far as reading them (server deployed), and one with added template-support, cookiecutter and all the whistles, for creating projects on the developer machine.
",nyov,kmike
1776,2016-02-18 11:37:36,"@Guoozz , as @nyov suggests, `dont_filter=True` for start requests is set by design.
Changing this would break a lot of spiders in the wild.
If you really get into redirection loops in your use cases when the crawl starts, override `start_requests` method in your spider. Indeed, you don't need to set `dont_filter=False`, it's the default value for `Request` instances.
",redapple,nyov
1774,2016-02-15 10:19:22,"@redapple is it easy to include #1765 here too?
",eliasdorneles,redapple
1774,2016-02-15 10:44:27,"@eliasdorneles , done.
Release notes still need update:

> The previously bundled scrapy.xlib.pydispatch library was deprecated and replaced by pydispatcher.
",redapple,eliasdorneles
1774,2016-02-15 10:53:57,"@eliasdorneles , nevermind. reads good enough
",redapple,eliasdorneles
1772,2016-02-17 12:07:48,"@kmike @sibiryakov I took into account what you said about signal dispatching and I came up with [this](https://github.com/scrapy/scrapy/compare/master...Djayb6:cancel-request-downloaderhandler)

This approach does not use signal dispatching. Instead, an optional callback `on_headers_received` has been added to `scrapy.http.Request` that is called in the dowloader handler if the callback was set. 
A `scrapy.http.Response` is created with the url and the headers in order to hide Twisted's API, and is passed to the callback along with the original `request`. Then as before if the callback returns `True` the request is cancelled.  
I think this approach is way less expensive and more user-friendly. What do you think ?  

JB
",Djayb6,sibiryakov
1772,2016-02-17 12:07:48,"@kmike @sibiryakov I took into account what you said about signal dispatching and I came up with [this](https://github.com/scrapy/scrapy/compare/master...Djayb6:cancel-request-downloaderhandler)

This approach does not use signal dispatching. Instead, an optional callback `on_headers_received` has been added to `scrapy.http.Request` that is called in the dowloader handler if the callback was set. 
A `scrapy.http.Response` is created with the url and the headers in order to hide Twisted's API, and is passed to the callback along with the original `request`. Then as before if the callback returns `True` the request is cancelled.  
I think this approach is way less expensive and more user-friendly. What do you think ?  

JB
",Djayb6,kmike
1772,2016-03-19 01:58:24,"@kmike see my new implementation [here](https://github.com/scrapy/scrapy/compare/master...Djayb6:headers-received-event) . It is based on signals (one signal per download), hides Twisted API, and it is possible to disable the signal based on a setting (for @sibiryakov).  
You can find an extension that cancels downloads based on pre-defined conditions [here](https://github.com/Djayb6/received_headers_spider/blob/master/received_headers_spider/headersreceived.py)

FYI, regarding the signal dispatching implementation, the celery project extracted django pydispatcher's fork and [modified it](https://github.com/celery/celery/tree/master/celery/utils/dispatch) for its needs.  
JB
",Djayb6,sibiryakov
1772,2016-03-19 01:58:24,"@kmike see my new implementation [here](https://github.com/scrapy/scrapy/compare/master...Djayb6:headers-received-event) . It is based on signals (one signal per download), hides Twisted API, and it is possible to disable the signal based on a setting (for @sibiryakov).  
You can find an extension that cancels downloads based on pre-defined conditions [here](https://github.com/Djayb6/received_headers_spider/blob/master/received_headers_spider/headersreceived.py)

FYI, regarding the signal dispatching implementation, the celery project extracted django pydispatcher's fork and [modified it](https://github.com/celery/celery/tree/master/celery/utils/dispatch) for its needs.  
JB
",Djayb6,kmike
1771,2016-02-08 10:52:22,"Thanks @orangain !
",redapple,orangain
1768,2016-02-07 02:41:28,"@nyov Thanks for the comments. I think there is no strong reason to use Text I/O instead of Binary I/O. If  an encoding you want to get and system's encoding is different, using Text I/O may break the file.

For example, the xml exporter always writes `<?xml version=""1.0"" encoding=""utf-8""?>` at the top of file, but a file created by  `scrapy runspider myspider.py -o - -t xml > items.xml` will be encoded by system encoding if Text I/O is used. Note that there are some systems not using utf-8 as a system encoding, e.g. system encoding of Japanese version of Windows is cp932 not utf-8.

I've proposed to use `sys.stdout.buffer` in Python 3. See #1769 
",orangain,nyov
1767,2016-02-06 18:33:11,"Thanks for the patch @orangain, looks good! :+1: 
",eliasdorneles,orangain
1767,2016-02-08 05:11:55,"Thanks @orangain!
",kmike,orangain
1765,2016-02-08 18:41:01,"@eliasdorneles , I tried a few different things but I did not manage to make the test pass on Travis CI
",redapple,eliasdorneles
1765,2016-02-08 19:54:51,"hey @redapple -- I don't understand what's going on here, it passes using tox locally. =/
thanks for trying anyway!
",eliasdorneles,redapple
1765,2016-02-09 09:06:10,"@kmike , yes that's the problem. Local tests pass for me (and @eliasdorneles ). On Travis CI, how we configured it at least, it doesn't pass. 
I was under the assumption that adding `warnings.simplefilter('always')` would show the warning in any case, but  it fails as well.
",redapple,eliasdorneles
1765,2016-02-09 09:06:10,"@kmike , yes that's the problem. Local tests pass for me (and @eliasdorneles ). On Travis CI, how we configured it at least, it doesn't pass. 
I was under the assumption that adding `warnings.simplefilter('always')` would show the warning in any case, but  it fails as well.
",redapple,kmike
1765,2016-02-09 15:05:42,"@kmike , @eliasdorneles , is it ok to allow the test to fail?
",redapple,eliasdorneles
1765,2016-02-09 15:05:42,"@kmike , @eliasdorneles , is it ok to allow the test to fail?
",redapple,kmike
1765,2016-02-10 09:29:10,"@eliasdorneles , nice :)
you can probably remove the changes in requirements.txt
although it may be good to use a newer pytest in the future
",redapple,eliasdorneles
1765,2016-02-11 18:17:34,"squashed.
@redapple since squashing deletes your commits, I gave you credits in the commit message -- merci pour ton aide! :)
",eliasdorneles,redapple
1765,2016-02-15 10:45:20,"@nyov , indeed. Added in #1774 
",redapple,nyov
1764,2016-02-09 16:45:10,"@lagenar , I can confirm the failure with scrapy 1.0.5 (latest) and also scrapy 1.1.0rc1

Current SSL/TLS connections use TLSv1 method:

> TLSv1_method(), TLSv1_server_method(), TLSv1_client_method()
> A TLS connection established with these methods will only understand the TLS 1.0 protocol.

The trick from https://github.com/scrapy/scrapy/issues/1429#issuecomment-131782133 worked for me. `SSLv23_METHOD` really means negotiation. On the wire, I see TLSv1.2 being negotiated.

Define this somewhere in your project (e.g. `myproject/contextfactory.py`, next to `myproject/settings.py`)



and change the HTTP client context factory in your `settings.py` to something like
`DOWNLOADER_CLIENTCONTEXTFACTORY = 'myproject.contextfactory.TLSFlexibleContextFactory'`
",redapple,lagenar
1764,2016-02-10 13:46:38,"Hi @redapple , here's another site that fails despite working fine with the browser.
https://www.buyagift.co.uk

Do you plan to fix this on the scrapy trunk or should we use the alternative fix? I haven't tested the alternative fix yet but I'll give it a try in a while.
",lagenar,redapple
1764,2016-02-10 13:51:45,"@lagenar , we added #1629 to 1.1 milestone, so hopefully the core devs will agree on a good solution for most cases.
By the way, the custom context factory works for https://www.buyagift.co.uk/, so one more point for #1629
",redapple,lagenar
1764,2016-02-10 17:55:37,"@redapple , please see my log and my settings. If you have some hints for me, this will be great. If I ran this from Scrapy shell it worked, but does not work from a Scrapy application.

My settings: 



and the log trace:


",PoulTur,redapple
1764,2016-02-11 11:53:03,"@redapple I did some updates. I actually get now the error from the shell as well, although it worked for me few times. I use ProxyMesh, I believe it shouldn't be hooked up if using scrapy shell. Attached is my scrapy shell log and the updated specs.



Trace:


",PoulTur,redapple
1764,2016-02-11 12:49:17,"@redapple this is everything I got in the trace. I just did not paste the command `scrapy shell https://shop.clares.co.uk` on top. I'm not sure about Wireshark, I checked this tool, but I would not know where to start looking for a solution with it.
",PoulTur,redapple
1764,2016-02-11 20:50:34,"@redapple thank you for all your help.

I was not seeing the initial Scrapy messages due to custom logging settings. I think the Wireshark and the in-depths of networking protocols might be too much for poor me. The good news is your comments led me to try to switch off the Proxy Mesh and without it I got the spider working now.

I'm not sure if you have some hint for me for my ProxyMesh issue, in the longer run I will probably need this service. I attach the ProxyMesh note below with their statements towards https, which I'm not fully following, how to adjust the connect settings with Scrapy. You were very helpful to me already, I should probably reach out to their support, per their docs it looks like they know Scrapy.

_Does the proxy server support HTTPS/SSL sites?
Yes. The proxy server itself is still HTTP, but it can securely proxy HTTPS/SSL connections between you and a HTTPS server (using the CONNECT method). All communication between your client/browser and the secure site is encrypted; the proxy server is only moving the data back and forth. The only caveat is that since the proxy server cannot inspect HTTPS requests, all proxy authorization headers or custom ProxyMesh headers must be sent with the initial CONNECT method. IP based authentication is recommended. End-to-end HTTPS support will be added in the future._
",PoulTur,redapple
1764,2016-03-27 01:51:22,"@redapple Thanks for your reply: I tried it with scrapy 1.1.0rc3 over python 2.7 & python 3.4. 
It works without TLSFlexibleContextFactory for https://www.buyagift.co.uk/ and https://shop.clares.co.uk/
However it does not work for https://revonsunpeu.net/, with or without TLSFlexibleContextFactory. 
Any idea?




",natoinet,redapple
1764,2016-03-27 11:32:33,"I tried without changing context factory. Scrapy 1.1rc3 uses better
defaults, so you shouldn't need to tweak anything. Can you try with
out-of-the-box scrapy 1.1rc3? (I used `scrapy shell https://...`)
Le 27 mars 2016 03:51, ""@ntoinet"" notifications@github.com a écrit :

> @redapple https://github.com/redapple Thanks for your reply: I tried it
> with scrapy 1.1.0rc3 over python 2.7 & python 3.4.
> It works without TLSFlexibleContextFactory for https://www.buyagift.co.uk/
> and https://shop.clares.co.uk/
> However it does not work for https://revonsunpeu.net/, with or without
> TLSFlexibleContextFactory.
> 
> (scrapy1.1rc3p3)HeyHeyHey:scrapy_googleindex thatsme $ scrapy version -v
> Scrapy    : 1.1.0rc3
> lxml      : 3.6.0.0
> libxml2   : 2.9.0
> Twisted   : 16.0.0
> Python    : 3.4.1 (v3.4.1:c0e311e010fc, May 18 2014, 00:54:21) - [GCC 4.2.1 (Apple Inc. build 5666) (dot 3)]
> pyOpenSSL : 16.0.0 (OpenSSL 0.9.8zg 14 July 2015)
> Platform  : Darwin-14.5.0-x86_64-i386-64bit
> 
> (scrapy1.1rc3p3)HeyHeyHey:scrapy_googleindex thatsme$ scrapy shell https://revonsunpeu.net
> 2016-03-27 03:44:24 [scrapy] INFO: Scrapy 1.1.0rc3 started (bot: scrapy_googleindex)
> 2016-03-27 03:44:24 [scrapy] INFO: Overridden settings: {'LOGSTATS_INTERVAL': 0, 'BOT_NAME': 'scrapy_googleindex', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'USER_AGENT': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10; rv:44.0) Gecko/20100101 Firefox/44.0', 'DOWNLOAD_DELAY': 0.5, 'NEWSPIDER_MODULE': 'scrapy_googleindex.spiders', 'SPIDER_MODULES': ['scrapy_googleindex.spiders']}
> 2016-03-27 03:44:24 [scrapy] INFO: Enabled extensions:
> ['scrapy.extensions.corestats.CoreStats']
> 2016-03-27 03:44:24 [scrapy] INFO: Enabled downloader middlewares:
> ['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
>  'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
>  'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
>  'scrapy.downloadermiddlewares.retry.RetryMiddleware',
>  'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
>  'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
>  'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
>  'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
>  'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
>  'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
>  'scrapy.downloadermiddlewares.stats.DownloaderStats']
> 2016-03-27 03:44:24 [scrapy] INFO: Enabled spider middlewares:
> ['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
>  'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
>  'scrapy.spidermiddlewares.referer.RefererMiddleware',
>  'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
>  'scrapy.spidermiddlewares.depth.DepthMiddleware']
> 2016-03-27 03:44:24 [scrapy] INFO: Enabled item pipelines:
> []
> 2016-03-27 03:44:24 [scrapy] INFO: Spider opened
> 2016-03-27 03:44:24 [scrapy] DEBUG: Retrying <GET https://revonsunpeu.net> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'SSL23_GET_SERVER_HELLO', 'sslv3 alert handshake failure')]>]
> 2016-03-27 03:44:25 [scrapy] DEBUG: Retrying <GET https://revonsunpeu.net> (failed 2 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'SSL23_GET_SERVER_HELLO', 'sslv3 alert handshake failure')]>]
> 2016-03-27 03:44:26 [scrapy] DEBUG: Gave up retrying <GET https://revonsunpeu.net> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'SSL23_GET_SERVER_HELLO', 'sslv3 alert handshake failure')]>]
> Traceback (most recent call last):
>   File ""/Users/thatsme/.virtualenvs/scrapy1.1rc3p3/bin/scrapy"", line 11, in <module>
>     sys.exit(execute())
>   File ""/Users/thatsme/.virtualenvs/scrapy1.1rc3p3/lib/python3.4/site-packages/scrapy/cmdline.py"", line 142, in execute
>     _run_print_help(parser, _run_command, cmd, args, opts)
>   File ""/Users/thatsme/.virtualenvs/scrapy1.1rc3p3/lib/python3.4/site-packages/scrapy/cmdline.py"", line 88, in _run_print_help
>     func(_a, *_kw)
>   File ""/Users/thatsme/.virtualenvs/scrapy1.1rc3p3/lib/python3.4/site-packages/scrapy/cmdline.py"", line 149, in _run_command
>     cmd.run(args, opts)
>   File ""/Users/thatsme/.virtualenvs/scrapy1.1rc3p3/lib/python3.4/site-packages/scrapy/commands/shell.py"", line 71, in run
>     shell.start(url=url)
>   File ""/Users/thatsme/.virtualenvs/scrapy1.1rc3p3/lib/python3.4/site-packages/scrapy/shell.py"", line 47, in start
>     self.fetch(url, spider)
>   File ""/Users/thatsme/.virtualenvs/scrapy1.1rc3p3/lib/python3.4/site-packages/scrapy/shell.py"", line 112, in fetch
>     reactor, self._schedule, request, spider)
>   File ""/Users/thatsme/.virtualenvs/scrapy1.1rc3p3/lib/python3.4/site-packages/twisted/internet/threads.py"", line 122, in blockingCallFromThread
>     result.raiseException()
>   File ""/Users/thatsme/.virtualenvs/scrapy1.1rc3p3/lib/python3.4/site-packages/twisted/python/failure.py"", line 368, in raiseException
>     raise self.value.with_traceback(self.tb)
> twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'SSL23_GET_SERVER_HELLO', 'sslv3 alert handshake failure')]>]
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/scrapy/scrapy/issues/1764#issuecomment-201971036
",redapple,redapple
1764,2016-03-27 11:35:03,"Sorry, that's what you did, I misread. Maybe someone on Mac OS can test
this. @kmike maybe?
Le 27 mars 2016 13:32, ""Paul Tremberth"" paul.tremberth@gmail.com a écrit :

> I tried without changing context factory. Scrapy 1.1rc3 uses better
> defaults, so you shouldn't need to tweak anything. Can you try with
> out-of-the-box scrapy 1.1rc3? (I used `scrapy shell https://...`)
> Le 27 mars 2016 03:51, ""@ntoinet"" notifications@github.com a écrit :
> 
> > @redapple https://github.com/redapple Thanks for your reply: I tried
> > it with scrapy 1.1.0rc3 over python 2.7 & python 3.4.
> > It works without TLSFlexibleContextFactory for
> > https://www.buyagift.co.uk/ and https://shop.clares.co.uk/
> > However it does not work for https://revonsunpeu.net/, with or without
> > TLSFlexibleContextFactory.
> > 
> > (scrapy1.1rc3p3)HeyHeyHey:scrapy_googleindex thatsme $ scrapy version -v
> > Scrapy    : 1.1.0rc3
> > lxml      : 3.6.0.0
> > libxml2   : 2.9.0
> > Twisted   : 16.0.0
> > Python    : 3.4.1 (v3.4.1:c0e311e010fc, May 18 2014, 00:54:21) - [GCC 4.2.1 (Apple Inc. build 5666) (dot 3)]
> > pyOpenSSL : 16.0.0 (OpenSSL 0.9.8zg 14 July 2015)
> > Platform  : Darwin-14.5.0-x86_64-i386-64bit
> > 
> > (scrapy1.1rc3p3)HeyHeyHey:scrapy_googleindex thatsme$ scrapy shell https://revonsunpeu.net
> > 2016-03-27 03:44:24 [scrapy] INFO: Scrapy 1.1.0rc3 started (bot: scrapy_googleindex)
> > 2016-03-27 03:44:24 [scrapy] INFO: Overridden settings: {'LOGSTATS_INTERVAL': 0, 'BOT_NAME': 'scrapy_googleindex', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'USER_AGENT': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10; rv:44.0) Gecko/20100101 Firefox/44.0', 'DOWNLOAD_DELAY': 0.5, 'NEWSPIDER_MODULE': 'scrapy_googleindex.spiders', 'SPIDER_MODULES': ['scrapy_googleindex.spiders']}
> > 2016-03-27 03:44:24 [scrapy] INFO: Enabled extensions:
> > ['scrapy.extensions.corestats.CoreStats']
> > 2016-03-27 03:44:24 [scrapy] INFO: Enabled downloader middlewares:
> > ['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
> >  'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
> >  'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
> >  'scrapy.downloadermiddlewares.retry.RetryMiddleware',
> >  'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
> >  'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
> >  'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
> >  'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
> >  'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
> >  'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
> >  'scrapy.downloadermiddlewares.stats.DownloaderStats']
> > 2016-03-27 03:44:24 [scrapy] INFO: Enabled spider middlewares:
> > ['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
> >  'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
> >  'scrapy.spidermiddlewares.referer.RefererMiddleware',
> >  'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
> >  'scrapy.spidermiddlewares.depth.DepthMiddleware']
> > 2016-03-27 03:44:24 [scrapy] INFO: Enabled item pipelines:
> > []
> > 2016-03-27 03:44:24 [scrapy] INFO: Spider opened
> > 2016-03-27 03:44:24 [scrapy] DEBUG: Retrying <GET https://revonsunpeu.net> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'SSL23_GET_SERVER_HELLO', 'sslv3 alert handshake failure')]>]
> > 2016-03-27 03:44:25 [scrapy] DEBUG: Retrying <GET https://revonsunpeu.net> (failed 2 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'SSL23_GET_SERVER_HELLO', 'sslv3 alert handshake failure')]>]
> > 2016-03-27 03:44:26 [scrapy] DEBUG: Gave up retrying <GET https://revonsunpeu.net> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'SSL23_GET_SERVER_HELLO', 'sslv3 alert handshake failure')]>]
> > Traceback (most recent call last):
> >   File ""/Users/thatsme/.virtualenvs/scrapy1.1rc3p3/bin/scrapy"", line 11, in <module>
> >     sys.exit(execute())
> >   File ""/Users/thatsme/.virtualenvs/scrapy1.1rc3p3/lib/python3.4/site-packages/scrapy/cmdline.py"", line 142, in execute
> >     _run_print_help(parser, _run_command, cmd, args, opts)
> >   File ""/Users/thatsme/.virtualenvs/scrapy1.1rc3p3/lib/python3.4/site-packages/scrapy/cmdline.py"", line 88, in _run_print_help
> >     func(_a, *_kw)
> >   File ""/Users/thatsme/.virtualenvs/scrapy1.1rc3p3/lib/python3.4/site-packages/scrapy/cmdline.py"", line 149, in _run_command
> >     cmd.run(args, opts)
> >   File ""/Users/thatsme/.virtualenvs/scrapy1.1rc3p3/lib/python3.4/site-packages/scrapy/commands/shell.py"", line 71, in run
> >     shell.start(url=url)
> >   File ""/Users/thatsme/.virtualenvs/scrapy1.1rc3p3/lib/python3.4/site-packages/scrapy/shell.py"", line 47, in start
> >     self.fetch(url, spider)
> >   File ""/Users/thatsme/.virtualenvs/scrapy1.1rc3p3/lib/python3.4/site-packages/scrapy/shell.py"", line 112, in fetch
> >     reactor, self._schedule, request, spider)
> >   File ""/Users/thatsme/.virtualenvs/scrapy1.1rc3p3/lib/python3.4/site-packages/twisted/internet/threads.py"", line 122, in blockingCallFromThread
> >     result.raiseException()
> >   File ""/Users/thatsme/.virtualenvs/scrapy1.1rc3p3/lib/python3.4/site-packages/twisted/python/failure.py"", line 368, in raiseException
> >     raise self.value.with_traceback(self.tb)
> > twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'SSL23_GET_SERVER_HELLO', 'sslv3 alert handshake failure')]>]
> > 
> > —
> > You are receiving this because you were mentioned.
> > Reply to this email directly or view it on GitHub
> > https://github.com/scrapy/scrapy/issues/1764#issuecomment-201971036
",redapple,redapple
1764,2016-03-28 19:42:38,"@redapple @kmike I just updated to the latest version of osx and the result is identical.
Also, when, I check openssl version, the result is:


",natoinet,redapple
1764,2016-03-30 21:49:54,"@redapple You are right, I checked on Debian Wheezy with python 2.7 & Scrapy 1.1.0rc3 and it works fine. It's only on OSX that it does not work with https://revonsunpeu.net. I don't know if anyone else can confirm this?
",natoinet,redapple
1763,2016-02-06 01:35:17,"Thanks for the report, @orangain !
I went with updating the regular expression to scrape the links from the categories. :+1: 
https://github.com/scrapy/scrapy.org/pull/58
",eliasdorneles,orangain
1762,2016-02-05 14:44:52,"Hey, @lagenar, thanks for the report, it seems we missed this in the release notes.

The code was previously using a bundled pydispatch library, but now it uses [PyDispatcher](https://pypi.python.org/pypi/PyDispatcher) which is Python 3 compatible.

In case your project was using pydispatch imports to connect signals, you can add a `from_crawler` method to your spider to do that:


",eliasdorneles,lagenar
1762,2016-02-05 15:34:44,"@eliasdorneles I'm afraid yes, we should add a fallback with a big deprecation warning.
",dangra,eliasdorneles
1761,2016-02-18 14:13:06,"thanks @lopuhin !
",redapple,lopuhin
1760,2016-02-17 18:09:34,"@Digenis thanks for the reference, it was useful.
",kmike,Digenis
1755,2016-02-03 21:34:39,"Looks pretty good! well done @eliasdorneles and @stummjr 

I'm not sure `Extracted CrawlerRunner._crawl method` is worth mentioning now that I read it with everything.
",redapple,eliasdorneles
1755,2016-02-03 21:34:39,"Looks pretty good! well done @eliasdorneles and @stummjr 

I'm not sure `Extracted CrawlerRunner._crawl method` is worth mentioning now that I read it with everything.
",redapple,stummjr
1754,2016-02-03 17:29:20,"ah @stummjr , sorry, I committed some changes to ""New Features and Enhancements"" since.
can you try rebasing?
",redapple,stummjr
1754,2016-02-03 17:32:50,"sure, @redapple .
",stummjr,redapple
1752,2016-02-03 15:18:03,"Thanks @stummjr 
",redapple,stummjr
1746,2016-01-29 21:09:01,"@Digenis , no, what do you suggest?
",redapple,Digenis
1746,2016-02-01 11:46:30,"@Digenis ,
this is the output I get: https://gist.github.com/redapple/ba015fab046310b49cdc
simplistic implementation here but works fine IMO, thanks for the tip!
",redapple,Digenis
1746,2016-02-02 18:57:41,"@nramirezuy , that's the best I've got to without changing too much
",redapple,nramirezuy
1746,2016-02-02 18:59:59,"@nramirezuy , `_repr_pretty_` was added after https://github.com/scrapy/scrapy/pull/1746#issuecomment-177164854
it can be removed.
",redapple,nramirezuy
1746,2016-02-02 19:17:43,"@redapple Is not a problem, just feels weird;  like I would expect `__repr__` to be implemented too :smile: 
",nramirezuy,redapple
1745,2016-01-29 15:31:00,"@kmike Now I'm starting with CrawlerProcess.
Thanks! Will try CrawlerRunner
",konstruilo,kmike
1744,2016-01-29 15:26:04,"Thanks for your help, @nramirezuy & @kmike.

I'm not sure I get your hint, though: Using `process_request`, I can block the spider from following redirects. But at that point, I still don't know whether the n-th redirect URL will match my deny pattern. 

As far as I understand scrapy's architecture (please correct me here!), the earliest place I can block redirect requests is in the redirect chain.

Maybe I can illustrate this with an example. Say I have a site mysite.com of which I don't want to crawl mysite.com/bogus. Now, if the site has one or more redirects in place, I could run into this request chain:

mysite.com/?redirect=1 -->
mysite.com/?redirect=2 -->
mysite.com/bogus

I assume that these requests pass through the middlewares, but do not reach the spider code until the final response. If that is true, then I only have the option to use a middleware OR block after receiving the final response, right?
",trifle,nramirezuy
1744,2016-01-29 15:26:04,"Thanks for your help, @nramirezuy & @kmike.

I'm not sure I get your hint, though: Using `process_request`, I can block the spider from following redirects. But at that point, I still don't know whether the n-th redirect URL will match my deny pattern. 

As far as I understand scrapy's architecture (please correct me here!), the earliest place I can block redirect requests is in the redirect chain.

Maybe I can illustrate this with an example. Say I have a site mysite.com of which I don't want to crawl mysite.com/bogus. Now, if the site has one or more redirects in place, I could run into this request chain:

mysite.com/?redirect=1 -->
mysite.com/?redirect=2 -->
mysite.com/bogus

I assume that these requests pass through the middlewares, but do not reach the spider code until the final response. If that is true, then I only have the option to use a middleware OR block after receiving the final response, right?
",trifle,kmike
1744,2016-01-29 15:58:58,"Thanks @nramirezuy, that sounds very appealing.
I didn't know this came up so often! Obviously there is no need to keep this issue open when there are several related others, so feel free to close. I guess I'll have a stab at creating a DownloaderMiddleware, hopefully with a PR.

Thanks again for scrapy and your hard work in maintaining and expanding it!
",trifle,nramirezuy
1743,2016-02-01 09:05:55,"@kmike I just follow the link you gave. And it is OK now. Thanks
",onmyway133,kmike
1737,2016-01-27 19:44:33,"@kmike for this case, yes, XmlItemExporter also has a bug for this case but it's there also in Scrapy 1.0.4 (i.e., not a regression), @stummjr filed a issue: https://github.com/scrapy/scrapy/issues/1738
",eliasdorneles,stummjr
1737,2016-01-27 19:44:33,"@kmike for this case, yes, XmlItemExporter also has a bug for this case but it's there also in Scrapy 1.0.4 (i.e., not a regression), @stummjr filed a issue: https://github.com/scrapy/scrapy/issues/1738
",eliasdorneles,kmike
1737,2016-01-27 19:46:55,"Thanks @stummjr!
",kmike,stummjr
1737,2016-01-27 19:53:48,"hey, @kmike ! I can add the tests for the other exporters too.
(edit: working on it)
",stummjr,kmike
1737,2016-01-28 16:36:06,"hey, @kmike!
I just included tests for the other exporters: https://github.com/scrapy/scrapy/pull/1742
",stummjr,kmike
1735,2016-01-27 20:46:10,"@nramirezuy can refactoring this middleware be a separate issue? It has a few other problems: it doesn't handle Crawl-Delay directives (https://github.com/scrapy/scrapy/issues/892), and it breaks on robots.txt files found in the wild (https://github.com/scrapy/scrapy/issues/754).
",kmike,nramirezuy
1735,2016-01-27 20:49:22,"@kmike Yes, the second point should fix for the issue. Just avoid every request to a domain which `robots.txt` failed to download. For example what if the domain just blocked the UA you using or the Crawler IP.

Storing the failure might be overboard, just storing None should be enough. Also you can collect stats and see the different failure you are getting on robots requests.
",nramirezuy,kmike
1735,2016-01-27 23:34:00,"@nramirezuy I'm not sure why I was removing the netloc in the first place. All tests still pass when leaving it as none, and it is also what the middleware already did, so I'm making it this way again.

I don't understand your first suggestion, and I'm not using callback and errback because the previous code didn't use it.
I think blocking every request if robots.txt fails downloading is inconsistent with the behaviour on other errors (and incompatible with the previous behaviour on download errors), which is to consider it as allowing all.
",ArturGaspar,nramirezuy
1735,2016-01-28 14:56:34,"@ArturGaspar I just looked at the [previous version before the defer](https://github.com/ArturGaspar/scrapy/blob/ca83a0b02880aad8c34f48ac81c7005880f5140e/scrapy/downloadermiddlewares/robotstxt.py). 
And looks like the robots.txt requests was done only once and if it failed it let you download the from the site, it also let you download while the robots request wasn't handled.
",nramirezuy,ArturGaspar
1735,2016-01-28 14:59:00,"@nramirezuy I think that not downloading when robots.txt request fails is a bug, but waiting for robots.txt to be downloaded before proceeding with the main website is a bug fix.
",kmike,nramirezuy
1735,2016-01-28 15:03:33,"@kmike I agree, is how I suppose to be implemented. But we should also documented, because it will inform users of the middleware and will help us to review on the future.
",nramirezuy,kmike
1735,2016-01-28 15:05:13,"@nramirezuy The documentation mentioned it as a limitation, and #1473 removed that warning from the documentation.
",ArturGaspar,nramirezuy
1735,2016-01-28 15:12:46,"@ArturGaspar  But we should add a warning about when robots request fails (when it reach the errback), also tell that it respect the `Downloader Middlewares` but doesn't use the `Spider Middlewares`.
Sorry, I'm just dumping everything while I'm on it. Otherwise it get lost on the void.
",nramirezuy,ArturGaspar
1735,2016-01-28 15:57:14,"@nramirezuy Isn't it better to make a separate issue for improving documentation on this middleware?
",ArturGaspar,nramirezuy
1735,2016-01-29 13:32:01,"@kmike 

> Is it retried? 

By the default retry middleware. (Without this PR, it would be retried on every new request to a domain, and only remembered for that domain if succeeded.)

> If robots.txt request fails, is the main request sent? If so, does it happen before or after retries?

Yes. After the retries.

> Is an error with robots.txt logged?

Yes.

> Is the behaviour different in Scrapy 1.0.x?

With this PR, no, except for the previously documented limitation (that in 1.0 the first request is made before robots.txt, and some requests can still go through before it was downloaded).
",ArturGaspar,kmike
1735,2016-01-29 15:01:26,"> > Is it retried? 
> 
> By the default retry middleware. (Without this PR, it would be retried on every new request to a domain, and only remembered for that domain if succeeded.)

Depends on the position of `RobotsTxtMiddleware` on the chain, by default answer is valid.

> > If robots.txt request fails, is the main request sent? If so, does it happen before or after retries?
> 
> Yes. After the retries.

Depends on the position of `RobotsTxtMiddleware` on the chain, by default answer is valid.

> > Is the behaviour different in Scrapy 1.0.x?
> 
> With this PR, no, except for the previously documented limitation (that in 1.0 the first request is made before robots.txt, and some requests can still go through before it was downloaded).

1.0.x behavior was bogus, so it have to change. Maybe in another PR.

@ArturGaspar can you create PR changing behavior https://github.com/scrapy/scrapy/pull/1735#issuecomment-176225527 with it documentation?
",nramirezuy,ArturGaspar
1735,2016-01-29 15:17:54,"@nramirezuy I think position of the middleware doesn't matter, because it calls engine.download() for a new request, which goes through the entire middleware chain. Isn't that so?

> @ArturGaspar can you create PR changing behavior #1735 (comment) with it documentation?

Sorry, I'm confused. You mean documenting only what was improved in #1473 and this PR, or the behaviour that was already present (what happens when robots.txt fails etc.)?
",ArturGaspar,ArturGaspar
1735,2016-01-29 15:17:54,"@nramirezuy I think position of the middleware doesn't matter, because it calls engine.download() for a new request, which goes through the entire middleware chain. Isn't that so?

> @ArturGaspar can you create PR changing behavior #1735 (comment) with it documentation?

Sorry, I'm confused. You mean documenting only what was improved in #1473 and this PR, or the behaviour that was already present (what happens when robots.txt fails etc.)?
",ArturGaspar,nramirezuy
1735,2016-01-29 15:36:25,"@ArturGaspar Yea you are right it will work regardless. What I'm not sure is if it gets propagated to the `SpiderMiddlewares` and if is the same for error and normal.

We should first fix this https://github.com/scrapy/scrapy/pull/1735#issuecomment-176225527, then document the change. And also will be useful to document that it indeed passes through the `DownloaderMiddlewares`, and if it gets propagated to `SpiderMiddlewares` too. So we don't have to science it every time we want to use it.
",nramirezuy,ArturGaspar
1735,2016-01-29 15:38:25,"@nramirezuy sorry, what exactly should we fix?
",kmike,nramirezuy
1735,2016-02-03 14:48:51,"Hey @kmike @nramirezuy @ArturGaspar, I'll merge this one now in order to move forward with the 1.1 RC release.
@nramirezuy can you please open a ticket for the follow-up?
",eliasdorneles,ArturGaspar
1735,2016-02-03 14:48:51,"Hey @kmike @nramirezuy @ArturGaspar, I'll merge this one now in order to move forward with the 1.1 RC release.
@nramirezuy can you please open a ticket for the follow-up?
",eliasdorneles,nramirezuy
1735,2016-02-03 14:48:51,"Hey @kmike @nramirezuy @ArturGaspar, I'll merge this one now in order to move forward with the 1.1 RC release.
@nramirezuy can you please open a ticket for the follow-up?
",eliasdorneles,kmike
1732,2016-01-27 11:03:36,"@kmike , you may want to edit the shell output in Github (e.g. replacing `<` with `&lt;`). it looks really scarier even :)
",redapple,kmike
1732,2016-01-27 17:46:40,"@kmike , how would you prefer it?
",redapple,kmike
1732,2016-01-28 18:27:21,"@redapple I'm fine the `to_dict` method. I kinda wanted to have it on the past, but never really had a need, now that we have a more complicated `Settings class`  I suppose is useful. Just be sure to document is a copy of the `Settings object` and changes on the `dict` wont affect the `Settings`.
",nramirezuy,redapple
1731,2016-12-09 10:53:33,"@kmike , can you share (in a comment or in the description of the PR) some sample console output before and after the change?",redapple,kmike
1731,2016-12-09 16:04:14,"Right @kmike , I think you've convinced me to enable long names by default. Experienced users could switch it off easily.",redapple,kmike
1731,2016-12-16 17:16:39,@redapple what do you think now? //cc @eliasdorneles @dangra @nyov @Digenis @curita and everyone else,kmike,redapple
1731,2016-12-16 17:42:48,"@redapple, yes that seems common in the Java world. But urgh, what an ugly thing to do. You still don't know what component it came from without knowing some internals first.",nyov,redapple
1731,2016-12-19 13:56:01,"Alright, so we need another ""official"" +1. @eliasdorneles , @dangra or @curita perhaps?",redapple,eliasdorneles
1731,2016-12-19 14:55:03,@eliasdorneles gave me his +1 offline. So we're good to merge.,redapple,eliasdorneles
1730,2016-01-27 19:18:14,"@nramirezuy yeah, you're right, it is better to call `text` from `body_as_unicode`: we'll save a function call this way.

As for deprecations - I don't have a strong opinion here, just tried to play safe in this PR to sneak it to 1.1 :)
",kmike,nramirezuy
1729,2016-01-26 20:44:46,"@eliasdorneles I don't know, maybe we need this method only in JsonResponse (which we don't have now). JsonResponse may also provide .jmes method to query json data.
",kmike,eliasdorneles
1727,2016-01-26 18:48:12,"@kmike , do you mean also changing the description for http://doc.scrapy.org/en/latest/topics/settings.html#depth-priority ?
",redapple,kmike
1727,2016-01-27 10:56:52,"@kmike , I'd rather we do not change the setting name to `DEPTH_PRIORITY_ADJUST` as it behave differently to the 2 others.
(we could rename to `DEPTH_PRIORITY_NEGATIVE_ADJUST` to be explicit but I wonder if it would not confuse more than anything)
",redapple,kmike
1727,2016-01-27 10:58:29,"@redapple a good point. 
We can make `DEPTH_PRIORITY_ADJUST` option an opposite of `DEPTH_PRIORITY`.
",kmike,redapple
1727,2016-01-27 11:01:58,"@kmike , is renaming ok in a seperate PR?
",redapple,kmike
1727,2016-01-27 11:03:14,"@redapple sure, improved docs is already a big step forward
",kmike,redapple
1727,2016-01-27 11:58:05,"ok @kmike , I also added a link to the FAQ entry about BFO/DFO.
",redapple,kmike
1724,2016-01-26 13:09:30,"@eliasdorneles do you mean a deprecation warning with ROBOTSTXT_OBEY is not set? It could be inconvenient for CrawlerProcess users because `CrawlerProcess()` will be raising warnings by default.

Changing `scrapy runspider` is backwards incompatible; to do a deprecation cycle we will have to require users to pass `-s ROBOTSTXT_OBEY=1` at each runspider call, it makes API much worse.

What about changing default values in future Scrapy 2.0?
",kmike,eliasdorneles
1724,2016-01-26 13:49:43,"@Digenis do you suggest to enable or disable robots.txt handling when user agent is overridden? 

We encourage users to override user agent in generated setting.py (near the top), and provide an example: `'$project_name (+http://www.yourdomain.com)'`. Users who follow this suggestion will likely want to have robots.txt enabled by default as well. On the other hand, some user override user-agent to browser-like values, they are more likely to want robots.txt to be disabled. I'm not sure it is a good idea to tie robots.txt handling with user agent overrides - it may be surprising and unwelcome regardless of what default we choose.
",kmike,Digenis
1722,2016-01-26 14:30:54,"@kmike , @eliasdorneles , sorry, it seems I pushed the wrong version:
with this patch in master, you currently get the short names:



instead of `'scrapy.extensions.corestats.CoreStats'` and the likes

I will submit a new PR on top of this
",redapple,eliasdorneles
1721,2016-01-26 11:37:26,"Thanks @Digenis 
",eliasdorneles,Digenis
1710,2016-01-22 16:21:12,"@kmike , I added a few different tests, including localhost,
hopefully catching the most common cases.
",redapple,kmike
1710,2016-01-22 18:29:55,"@kmike , importing `pytest` seems to mess up the subsequent tests.
any idea what might be happening?
",redapple,kmike
1710,2016-01-22 19:01:34,"Thanks @kmike , I will give it a try.
",redapple,kmike
1710,2016-01-23 11:58:34,"@kmike, I didn't manage to import pytest without failing subsequent tests, so I left the parameterized tests at the previous commit for the moment, in order to discuss tests inputs & outputs at least
",redapple,kmike
1710,2016-01-23 20:49:37,"@redapple I think the implementation, inputs and outputs are all good.
",kmike,redapple
1710,2016-01-26 09:48:44,"@alecxe , @nramirezuy , @kmike ,
are you ok with the updated docs?
",redapple,kmike
1710,2016-01-26 16:41:50,"@nramirezuy , maybe just include the link in the release notes. what do you think?
",redapple,nramirezuy
1707,2016-01-21 20:47:48,"Super, thanks @kmike !
",lopuhin,kmike
1707,2016-02-05 17:21:42,"A follow up to what @kmike was pointing out about performance,  cached runs of my spiders show python 3 to be ~10% slower than python 2. What's more surprising and unfortunate is that python 3 memory usage is only slightly better and in fact sometimes even higher.  I was expecting an improvement since python 3 uses less memory for ascii only unicode objects, and i've seen this in other text intensive scripts.
",jschilling1,kmike
1707,2017-02-06 23:18:27,+1 to @redapple.,kmike,redapple
1701,2016-02-23 13:38:31,"In fact, this does not work with hostname as bytes.
I just got the error while testing scrapy 1.1 with Crawlera, the string ending up as `b""CONNECT b'httpbin.org':443 HTTP/1.1\r\n""`
I'll submit a PR for this.

@kmike , @eliasdorneles , @lopuhin , which has your preference?

the original 



or a fix on bytes param with


",redapple,lopuhin
1701,2016-02-23 19:42:06,"Oh, sorry @redapple - did not check properly. I have no particular preference, but for me second still looks slightly better.
",lopuhin,redapple
1701,2016-02-23 19:43:13,"Ah, I see you already fixed this - thanks @redapple ! :)
",lopuhin,redapple
1699,2016-01-19 18:44:51,"@eliasdorneles go ahead and merge then :)
",dangra,eliasdorneles
1698,2016-01-19 16:18:49,"Thanks @carlosp420 
",eliasdorneles,carlosp420
1695,2016-01-19 14:45:04,"@nramirezuy could you please fire an issue here: https://github.com/codecov/support/issues?
",kmike,nramirezuy
1692,2016-01-21 10:19:08,"@kmike @lopuhin ,
I also had to rerun Python 3.3 build for https://travis-ci.org/scrapy/scrapy/builds/103806098
It passed that 2nd time.
",redapple,lopuhin
1692,2016-01-21 10:19:08,"@kmike @lopuhin ,
I also had to rerun Python 3.3 build for https://travis-ci.org/scrapy/scrapy/builds/103806098
It passed that 2nd time.
",redapple,kmike
1692,2016-01-21 14:53:51,"@dangra done)
",lopuhin,dangra
1691,2016-01-20 19:59:51,"I hope I rebased correctly, @kmike 
",lopuhin,kmike
1689,2016-01-20 19:11:03,"@kmike I've never used this option but might get someone mad xD
",nramirezuy,kmike
1689,2016-01-20 19:18:39,"@nramirezuy it looks trivial to work around using e.g. https://pypi.python.org/pypi/pyprof2calltree
",kmike,nramirezuy
1687,2016-01-18 11:48:57,"Thanks @palego!
",kmike,palego
1686,2016-01-18 06:57:03,"Thanks @cclauss!
",kmike,cclauss
1683,2016-01-15 18:30:50,"Yup, sounds good, thanks @rgtk !
",eliasdorneles,rgtk
1682,2016-01-17 22:12:01,"@kmike , I can't see mention of paid account restriction on https://docs.travis-ci.com/user/osx-ci-environment/
I wanted to try after seeing https://www.youtube.com/watch?v=d-p6lJJObLU (https://speakerdeck.com/pycon2015/olivier-grisel-build-and-test-wheel-packages-on-linux-osx-and-windows) which suggest it's possible
",redapple,kmike
1680,2016-01-20 19:05:49,"Another great job, thanks @lopuhin !
",eliasdorneles,lopuhin
1680,2016-01-20 19:49:11,"Scrapy is a pleasure to work on, @eliasdorneles !
",lopuhin,eliasdorneles
1678,2016-01-19 16:10:30,"Thanks so much @lopuhin, awesome work! :bow: :bow: 
",eliasdorneles,lopuhin
1678,2016-01-19 16:13:41,"Merci @lopuhin !
",redapple,lopuhin
1676,2016-01-16 09:06:40,"Hey @lopuhin, nice to see you here! 

> scrapy.http.Request has unicode attributes (url, method, etc),

I don't 100% recall why, but after lengthy discussions with @dangra we settled on URLs as 'native strings' - i.e. bytes in Python 2 and unicode in Python 3. Likely the two main reasons were (1) backwards compatibility and (2) spotty/incorrect support for unicode URLs in Python 2.x stdlib. 

> response from twisted (body) is in unicode

hm, that's surprising - what if response is not text?
",kmike,lopuhin
1676,2016-01-18 14:06:36,"@dangra hmm, you're right, webclient is unused when twisted >= 11.1.0. 
Does it mean we'll drop Ubuntu 10.04 support if we drop webclient? 
I think that's fine because of https://lists.ubuntu.com/archives/ubuntu-announce/2015-April/000196.html.
",kmike,dangra
1676,2016-01-19 13:53:41,"@dangra ok, feel free to merge this PR then
",kmike,dangra
1676,2016-01-19 14:08:25,"thanks @lopuhin !
",dangra,lopuhin
1676,2016-01-19 14:10:45,"yay, thanks @dangra and @kmike !
",lopuhin,dangra
1676,2016-01-19 14:10:45,"yay, thanks @dangra and @kmike !
",lopuhin,kmike
1674,2016-01-14 09:58:43,"@kmike : thanks, the trick for Travis + tox and Python 3.5 worked.
Pythong 3.5 tests currently fail though
",redapple,kmike
1664,2016-01-11 18:45:37,"thanks @kmike for the feedback.

the failing tests are around the unquoting of the ""path"" part of the URL, that was not UTF-8 encoded before percent-encoding.
and I was wondering if the following original test for example is actually valid if the encoding is not known:



what do you suggest in order to fix these tests?
- work on an improved `canonicalize_url` function, handling those tricky things you mention, perhaps guessing encoding for this kind of URLs above?
- or rewrite the tests to re-encode paths, using UTF-8 + then ASCII percent-encoding?
",redapple,kmike
1664,2016-01-18 13:22:31,"ok @kmike , I'm closing this one for now. Waiting for discussion like https://github.com/scrapy/scrapy/issues/1306 to progress
",redapple,kmike
1662,2016-02-04 19:09:34,"Looks good, thanks @NicolasP!
",kmike,NicolasP
1660,2016-01-06 12:43:06,"Thanks @stummjr !
",eliasdorneles,stummjr
1659,2016-01-05 14:19:05,"@orangain domains are used as slots key by default but you can [change this behaviour](http://doc.scrapy.org/en/1.0/topics/settings.html#concurrent-requests-per-ip).   
",nramirezuy,orangain
1659,2016-01-05 14:50:25,"@nramirezuy Thank you for your reply, but I did set `CONCURRENT_REQUESTS_PER_IP = 1` by `custom_settings` of the spider. 



What I mean is download delay per IP does not work as documented:

> This setting also affects DOWNLOAD_DELAY: if CONCURRENT_REQUESTS_PER_IP is non-zero, download delay is enforced per IP, not per domain.

http://doc.scrapy.org/en/1.0/topics/settings.html#concurrent-requests-per-ip
",orangain,nramirezuy
1654,2016-01-19 14:23:41,"@kmike can't verify what's going on on codecov, I get a 404.
",nramirezuy,kmike
1654,2016-01-19 14:24:41,"@nramirezuy yeah, mee too. 
Does it matter?
",kmike,nramirezuy
1650,2015-12-29 10:32:45,"Ok,Thank you very much,I'll try js2xml instead @redapple 
",yuiopt,redapple
1649,2016-02-02 17:20:34,"Hey @aron-bordin,

Sorry, it took a long time to review this PR. The problem is that after this change base classes (including  scrapy.Spider?) will appear in `scrapy list` command; this is confusing and not backwards compatible. We may add a special attribute which can be set explicitly to mark spider as a base class (e.g. show=False or base=True), but as by default there won't be such attribute this change will require users to add it to all base spiders. So it is backwards-incompatible again. This all is obvious in hindsight, but only your PR helped me to realize that.

I think that unless someone come up with a clever idea this change have to wait until Scrapy 2.0.
",kmike,aron-bordin
1645,2016-01-06 11:51:40,"@kmike yeap, that's reasonable, +1 from me.
",eliasdorneles,kmike
1645,2016-01-07 06:40:45,"@kmike you have my vote for it.
",dangra,kmike
1645,2016-12-20 20:15:14,"This document insinuates that potential contributors are unethical, unprofessional, and immoral, and that they must be told to suppress their antisocial behavior before contributing to this project. This is now the first thing people are greeted with when they first roll up their sleeves to get started with this project, and it translates to a fairly negative experience. I'll probably still fork and make the changes I need, but I'm much less inclined to engage further. And while I actually agree with everything in this document, it states nothing that is not already expected of every individual.  I'm left to think that its purpose is purely political, and I'm disappointed to see politics creep its way into a technical project in guise of a contribution.
@kmike @dangra @eliasdorneles ",victor9000,dangra
1645,2016-12-20 20:15:14,"This document insinuates that potential contributors are unethical, unprofessional, and immoral, and that they must be told to suppress their antisocial behavior before contributing to this project. This is now the first thing people are greeted with when they first roll up their sleeves to get started with this project, and it translates to a fairly negative experience. I'll probably still fork and make the changes I need, but I'm much less inclined to engage further. And while I actually agree with everything in this document, it states nothing that is not already expected of every individual.  I'm left to think that its purpose is purely political, and I'm disappointed to see politics creep its way into a technical project in guise of a contribution.
@kmike @dangra @eliasdorneles ",victor9000,eliasdorneles
1645,2016-12-20 20:15:14,"This document insinuates that potential contributors are unethical, unprofessional, and immoral, and that they must be told to suppress their antisocial behavior before contributing to this project. This is now the first thing people are greeted with when they first roll up their sleeves to get started with this project, and it translates to a fairly negative experience. I'll probably still fork and make the changes I need, but I'm much less inclined to engage further. And while I actually agree with everything in this document, it states nothing that is not already expected of every individual.  I'm left to think that its purpose is purely political, and I'm disappointed to see politics creep its way into a technical project in guise of a contribution.
@kmike @dangra @eliasdorneles ",victor9000,kmike
1645,2016-12-20 20:30:27,"Sorry @victor9000, we didn't mean to offend anyone with this document! We certainly didn't mean that potential contributors are unethical, unprofessional and immoral. It seems anyone agree that the content of the CoC is reasonable, so it was a no-brainer to add it. 

I'd also prefer not to use formal licenses like BSD. When we're writing

> Neither the name of Scrapy nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.

in a license we certainly don't mean that Scrapy users are scammers who are going to tell ""hey, this is an awesome uber-scraper which is actually built by the awesome @dangra"", while the uber-scraper is in fact a virus built into Scrapy fork, which takes control of a computer and plays Yankee Doodle. But we're still using a BSD license, because it is common to do that, it could help us in a 1-in-1000000 event, and because it is very easy to comply with this license for ~100% of Scrapy users. ",kmike,dangra
1645,2016-12-20 21:23:20,"Hi @victor9000 , as @kmike said, we're sorry if the content of the Code of Conduct document put you on the defensive and makes you reconsider contributing further. It's definitely not the intent.

To me, and I'm speaking here for myself, not in the name of scrapy or its maintainers, it serves as a written reference of rules when something inapropriate comes up. And although I'm really thankful that I only witnessed very few examples of such wrong behavior, it does happen, trust me.

Not so long ago I had to remove a comment on GitHub from a 3rd person in a thread I was participating in with a scrapy user reporting some issue (it was a sexual-orientation oriented comment -- to put it mildly, ironic/""humorous"" or not, I judged it very misplaced in any case)
Another time, last summer, there was conversation on IRC between a regular contributor and a (new) user who progressively starting insulting either the scrapy contributor or the project maintainers (I'm still not sure which one it was, if not both) because there was no obvious or immediate ""solution"" to his use-case.

The Code of Conduct is also for cases where maintainers do not see the bad mouthing or harassement that could happen privately between members of the scrapy community. People can refer to the code of conduct and they have an email address to report to (although it's a `@scrapinghub.com` one a the moment, we want to have an independant `@scrapy.org` one)

I've re-read the Code of Coduct following your message and I don't feel the insinuation. Could you elaborate of some improvements to it if any? (By the way, the text originally comes from http://contributor-covenant.org/)",redapple,kmike
1645,2016-12-21 11:42:33,"+1 to everything @kmike and @redapple said.

",eliasdorneles,redapple
1645,2016-12-21 11:42:33,"+1 to everything @kmike and @redapple said.

",eliasdorneles,kmike
1645,2016-12-23 02:17:44,"Hi Everyone, first of all sorry for creating this distraction, I'm sure you all have plenty of real work to do.  Also @kmike and @redapple, thank you for your productive approach on this, I'll do my best to match your tone.  My main point is to simply make you aware of my experience when I tried to on-board as a contributor.  I ran into a roadblock due to the language of this document, and I wanted to make you aware that using it as a greeting may be having some unintended consequences.  And as I mentioned above, I agree with the high-level goal of encouraging contributors to treat each other with respect, but I'm finding this particular implementation to be a bit buggy.

The first problem I encountered is that it unnecessarily focuses on lots of negative behavior.  The document provides a list of antisocial activities and a list of classes which should be protected from these activities.  In doing so the document splits the pool of potential contributors into aggressors and victims, and it's up to the reader to figure out where they fit in.  The document also implicitly paints the reader as someone who needs to be warned against engaging in this type of behavior, otherwise it would not be required reading.  This is a fairly pessimistic perspective, and it's somewhat demeaning if you happen to believe that people are innately good.  A more productive approach would be to assume benevolence on the part of all participants, and instead focus on the qualities which make up the ideal contributor.  You will find that the tone of this document becomes much more productive if you take every negative activity that it describes and replace it with its positive counterpart.

The second problem is that all of this negativity is placed at the beginning of the on-boarding pipeline.  Imagine for a second that you're invited to a friend's house to bake a pie.  And you love pie, so you're really looking forward to helping them improve their recipes.  But when you show up at their house, they greet you with a lecture on politics and morality instead of anything regarding pies.  Then they take you around the house and say things like ""do not kick the cat, do not break the windows, do not make advances toward my grandmother, do not relieve yourself on the floor"".  This language is not appropriate for the situation, it's not a pleasant way to greet someone, and pretty soon you're going to change your mind about the pie.

The third problem is how it handles conflict resolution between the reporter and the accused.  Many of the problems described in this document will depend on how the reporter perceives the intentions of the accused.  The difference between perception and intent may be due to a misunderstanding, cultural differences, or some other external factor.  Productive conflict resolution would bring both parties to the table and go over the experience of each participant in an effort to bridge their differences.  However, this document prescribes that reporter of an incident should remain anonymous, which is counter to how these things are handled in the real world.

@kmike - You make a fair point regarding the BSD license.  Every project has some amount of boilerplate which serves its purpose in 1-in-1000000 scenarios.  However, the language of the CoC is much less user-friendly than the text you quoted.  If we had to transform one text into the other, then the CoC would read something along the lines of ""We expect all contributors to behave professionally, and treat each other with respect"".  While the BSD text would first describe a list of undesirable products, then a host of unethical activities, and then finally make the point that the contributors do not endorse any of those things. 

@redapple - It sounds like you handled those situations appropriately.  I can certainly see how it's useful to have a document that explicitly states the course of action that will be taken in those cases.  Again, my experience is only from the perspective of on-boarding.  Also, the bulk of my response was meant to answer the questions you asked, let me know if you think I'm misinterpreting anything.",victor9000,redapple
1645,2016-12-23 02:17:44,"Hi Everyone, first of all sorry for creating this distraction, I'm sure you all have plenty of real work to do.  Also @kmike and @redapple, thank you for your productive approach on this, I'll do my best to match your tone.  My main point is to simply make you aware of my experience when I tried to on-board as a contributor.  I ran into a roadblock due to the language of this document, and I wanted to make you aware that using it as a greeting may be having some unintended consequences.  And as I mentioned above, I agree with the high-level goal of encouraging contributors to treat each other with respect, but I'm finding this particular implementation to be a bit buggy.

The first problem I encountered is that it unnecessarily focuses on lots of negative behavior.  The document provides a list of antisocial activities and a list of classes which should be protected from these activities.  In doing so the document splits the pool of potential contributors into aggressors and victims, and it's up to the reader to figure out where they fit in.  The document also implicitly paints the reader as someone who needs to be warned against engaging in this type of behavior, otherwise it would not be required reading.  This is a fairly pessimistic perspective, and it's somewhat demeaning if you happen to believe that people are innately good.  A more productive approach would be to assume benevolence on the part of all participants, and instead focus on the qualities which make up the ideal contributor.  You will find that the tone of this document becomes much more productive if you take every negative activity that it describes and replace it with its positive counterpart.

The second problem is that all of this negativity is placed at the beginning of the on-boarding pipeline.  Imagine for a second that you're invited to a friend's house to bake a pie.  And you love pie, so you're really looking forward to helping them improve their recipes.  But when you show up at their house, they greet you with a lecture on politics and morality instead of anything regarding pies.  Then they take you around the house and say things like ""do not kick the cat, do not break the windows, do not make advances toward my grandmother, do not relieve yourself on the floor"".  This language is not appropriate for the situation, it's not a pleasant way to greet someone, and pretty soon you're going to change your mind about the pie.

The third problem is how it handles conflict resolution between the reporter and the accused.  Many of the problems described in this document will depend on how the reporter perceives the intentions of the accused.  The difference between perception and intent may be due to a misunderstanding, cultural differences, or some other external factor.  Productive conflict resolution would bring both parties to the table and go over the experience of each participant in an effort to bridge their differences.  However, this document prescribes that reporter of an incident should remain anonymous, which is counter to how these things are handled in the real world.

@kmike - You make a fair point regarding the BSD license.  Every project has some amount of boilerplate which serves its purpose in 1-in-1000000 scenarios.  However, the language of the CoC is much less user-friendly than the text you quoted.  If we had to transform one text into the other, then the CoC would read something along the lines of ""We expect all contributors to behave professionally, and treat each other with respect"".  While the BSD text would first describe a list of undesirable products, then a host of unethical activities, and then finally make the point that the contributors do not endorse any of those things. 

@redapple - It sounds like you handled those situations appropriately.  I can certainly see how it's useful to have a document that explicitly states the course of action that will be taken in those cases.  Again, my experience is only from the perspective of on-boarding.  Also, the bulk of my response was meant to answer the questions you asked, let me know if you think I'm misinterpreting anything.",victor9000,kmike
1645,2016-12-27 12:01:43,"@victor9000 as @kmike has already said, having a Code of Conduct in place doesn't mean we expect negative behavior from most users. It's just making explicit a commitment to keep the community harassment-free and that's a tiny tiny thing we can do to make minorities feel more welcome (sometimes ""Explicit is better than implicit"", as the Python zen goes).

I think I get one point of your complaint, that you find it weird that the CoC is the first thing mentioned under ""Contributing"" on the README. I can see how it can be weird, because really the most important thing someone just wanting to contribute should be reading is the [contributing](http://doc.scrapy.org/en/master/contributing.html) document (which is just under the mention to the Code of Conduct).

We can change that, I'll send a PR.

But please don't make this about having a CoC in place or not (a decision that has already been taken and that is _a good thing_) and don't make this a bigger distraction than it already is.

If you want to understand why having a CoC is good, please don't put the burden on us of explaining it to you. There is plenty of documentation and pointers at: http://contributor-covenant.org which includes a list of other projects that adopt it.",eliasdorneles,kmike
1645,2016-12-30 03:20:09,"@eliasdorneles Sorry for the misunderstanding if it seemed like I was advocating against the use of a CoC.    As mentioned above, I agree with the intent of this document.  I was just advocating for language which was more user friendly than what's included in version 1.3.  However, given that this document gets pulled down from upstream as-is, then perhaps this is not the right forum for discussing the specific contents of the document.  Nonetheless, I agree that upgrading to version 1.4 which does include some positive language seems like an objective improvement.  I also agree that it's more useful to focus on the technical contributing document.  Thanks!",victor9000,eliasdorneles
1644,2016-01-13 10:59:45,"Thanks @yarikoptic !
",eliasdorneles,yarikoptic
1640,2016-02-10 16:11:12,"@redapple Python's re module caches compiled regexes, no need to worry about it. :)
",eliasdorneles,redapple
1640,2016-02-12 13:50:26,"@nhuray , @mkcor , can you explain your use-case for @kmike ?
",redapple,kmike
1639,2015-12-14 18:33:10,"Thanks @seales!
",kmike,seales
1638,2015-12-14 18:32:15,"Thanks @orangain!
",kmike,orangain
1637,2016-01-18 17:16:37,"Hey @eLRuLL,

I think that's a good idea to make encoding configurable, but a global setting doesn't look right - what if there are different proxies (e.g. rotating) which use different encodings? 
",kmike,eLRuLL
1637,2016-01-18 21:26:11,"> Hey @eLRuLL,
> 
> I think that's a good idea to make encoding configurable, but a global setting doesn't look right - what if there are different proxies (e.g. rotating) which use different encodings?

The middleware isn't accepting that kind of behaviour I think, as it is not processing the request if `proxy` already on `meta`: https://github.com/scrapy/scrapy/blob/master/scrapy/downloadermiddlewares/httpproxy.py#L38

> What about allowing users to pass bytes as username and password? This way users will be able to use any encoding. Latin1 can be kept default for unicode input.

for this I think I should change the `_parse_proxy` method (because of the bytes patterns).
",eLRuLL,eLRuLL
1637,2016-01-19 06:35:57,"@eLRuLL you're right, sorry! The PR looks good; I only have a couple of minor comments.
",kmike,eLRuLL
1637,2016-01-21 11:57:27,"@eLRuLL :+1: 
Could you please rebase on master?
",kmike,eLRuLL
1637,2016-01-21 14:36:29,"@kmike rebased!
",eLRuLL,kmike
1635,2016-01-11 20:57:20,"@palego I get this failure:


",rolando,palego
1635,2016-01-26 17:36:59,"@rolando , are you ok with closing the issue now that https://github.com/scrapy/scrapy/pull/1657 is merged?
",redapple,rolando
1635,2016-01-26 17:42:44,"@redapple yes! This issue is fixed.
",rolando,redapple
1635,2016-01-26 17:43:09,"Thanks for checking @rolando 
",redapple,rolando
1633,2015-12-10 16:00:17,"ok thanks @nramirezuy so I'm closing this as duplicate issue no need to multiply bug reports.
",pawelmhm,nramirezuy
1632,2015-12-09 22:41:56,"@nramirezuy I apologize but I don't think I could offer up the code to reliably reproduce it. Its not even reliable within the spider. That stack trace was all I got out of the logs and we have an error callback on the request that handles all exceptions.

I couldn't even tell which URL caused it. 

I'm going to assume its within the Request object since its cookie lib and try catching exceptions there to see if I'm able to find where its occurring.
",gmeans,nramirezuy
1632,2015-12-10 12:46:28,"@Digenis I don't believe so. This particular spider is running on an Elastic Beanstalk instance using Amazon Linux. It's running within a docker container using this image:

https://hub.docker.com/r/ceroic/scrapyd/~/dockerfile/
",gmeans,Digenis
1631,2015-12-09 17:25:24,"Yes.

I want to keep writing any other warning, but have a way to avoid showing anything on ItemDrop.

I try to filter the scrapped items like shown in [the example at the docs | http://doc.scrapy.org/en/0.20/topics/item-pipeline.html#item-pipeline-example], and I have several different external loggers attached to my output. In my case, I expect to receive the items even if they are to be discarded, so in my mental model, this is an INFO, and not a WARNING. I cannot change the command used to launch scrapy, nor the config file

@nramirezuy Thanks for your quick response, by the way :+1: 
",sieira,nramirezuy
1631,2015-12-09 20:30:33,"@redapple's response was true for older logging, but I can't confirm right now if is still active after the migration (should be because is still there).

@sieira Now if you want to contribute to Scrapy. I would follow something like [this](https://github.com/scrapy/scrapy/blob/master/scrapy/dupefilters.py#L62). Where you have a setting to allow `DEBUG` mode, I know is not backwards compatible, but it follows some sort of ""standardization"".

Another thing I would wait for @kmike or @curita on the matter before start re factoring the PR, they might have something to comment :smile:
",nramirezuy,redapple
1629,2016-02-09 17:20:46,"@starrify , I agree,
`SSLv23_METHOD` seems to be working much better in my tests on current open issues with tag ""https"" (https://github.com/scrapy/scrapy/issues/1764#issuecomment-181950638 , https://github.com/scrapy/scrapy/issues/1486#issuecomment-181960139 , https://github.com/scrapy/scrapy/issues/1435#issuecomment-181955341)

Another option to make the change technically more backward compatible is to define another context factory in `scrapy/core/downloader/contextfactory.py`, say `TLSFlexibleContextFactory`, and documenting that users can change the `DOWNLOADER_CLIENTCONTEXTFACTORY` setting to this other builtin class.

But I'm fine with making this the default, with a minimum openssl version in requirements.txt perhaps?
",redapple,starrify
1629,2016-02-16 15:56:50,"@kmike , it seems to me that the initial problem was less a bug than issue with faulty servers, and OpenSSL implemented a few workarounds along the way to deal with them.

I've listed changelog changes related to TLS versions handling for OpenSSL and Ubuntu openssl:
https://docs.google.com/spreadsheets/d/1TLQAhAt2JsF9o-JfcH8D3ttCXHH5s0OczteXTg7OMK4/edit?usp=sharing

[As far as I can read](https://launchpad.net/ubuntu/precise/+source/openssl/+changelog), Ubuntu Precise's `openssl (1.0.1-4ubuntu5.27)` was the version re-enabling TLS 1.2 by default, suggesting state of servers in the wild is good enough.

[`OpenSSL 1.0.1d`](https://www.openssl.org/news/changelog.txt) looks like the last release with fixes to protocol version negotiation, before the rewrite in 1.1.0 (not released yet, Alpha 3 on 15-Feb-2016)

>   *) Version negotiation has been rewritten. In particular SSLv23_method(),
>      SSLv23_client_method() and SSLv23_server_method() have been deprecated,
>      and turned into macros which simply call the new preferred function names
>      TLS_method(), TLS_client_method() and TLS_server_method(). All new code
>      should use the new names instead. Also as part of this change the ssl23.h
>      header file has been removed.
>      [Matt Caswell]
",redapple,kmike
1629,2016-02-17 18:36:59,"@redapple a nice table! 

Older OpenSSL versions are still in use. E.g. on OS X 10.11.3 (most recent OS X) I get the following:



(though Scrapy uses OpenSSL 1.0.2e 3 Dec 2015, likely via homebrew Python)
",kmike,redapple
1629,2016-02-23 17:21:30,"@kmike , @starrify ,
I'd go for https://github.com/scrapy/scrapy/pull/1794
What do you think?
",redapple,starrify
1629,2016-02-23 17:21:30,"@kmike , @starrify ,
I'd go for https://github.com/scrapy/scrapy/pull/1794
What do you think?
",redapple,kmike
1622,2015-12-02 16:39:03,"@pawelmhm the purpose of xlib.tx.client is to support old Twisted versions; it is not used in modern Twisteds where these imports are deprecated, so I think not collecting these files is the way to go.
",kmike,pawelmhm
1618,2015-11-26 20:55:42,"@sibiryakov what is this custom `Scheduler` returning ? 

@dangra  Isn't better allow the `Scheduler` return a defer and chain it with the download? Or there is something I'm missing?
",nramirezuy,dangra
1618,2015-11-26 20:55:42,"@sibiryakov what is this custom `Scheduler` returning ? 

@dangra  Isn't better allow the `Scheduler` return a defer and chain it with the download? Or there is something I'm missing?
",nramirezuy,sibiryakov
1618,2015-11-27 14:20:54,"Hey @nramirezuy it returns `http.Request` objects, deserializing them from disk. https://github.com/scrapy/scrapy/blob/75cd056223a5a8da87a361aee42a541afcf27553/scrapy/core/scheduler.py#L63
I think returning defereds is too radical change.
",sibiryakov,nramirezuy
1618,2015-11-30 17:06:12,"Guys, @dangra, @nramirezuy, @kmike any more thoughts on this?
",sibiryakov,nramirezuy
1618,2015-11-30 17:06:12,"Guys, @dangra, @nramirezuy, @kmike any more thoughts on this?
",sibiryakov,dangra
1618,2015-12-01 18:11:20,"> Isn't better allow the Scheduler return a defer and chain it with the download? Or there is something I'm missing?

@nramirezuy the Scheduler API is settled on returning Request or None, supporting deferreds (although logical at first) require a bigger change to engine. 
",dangra,nramirezuy
1618,2015-12-01 18:17:37,"@dangra I'm not sure I've got what you mean by this message

> I am evaluating the differences on your approach compared to adding a slot.nextcall.schedule(5) on https://github.com/scrapy/scrapy/blob/b26241168314812cf30c4f93edf4e8ec45d2fe6e/scrapy/core/engine.py#L105-L106
> 
> have you considered it?

could you describe it in a more detailed way?
",sibiryakov,dangra
1618,2015-12-01 21:30:06,"@sibiryakov do you mind removing the other `nextcall.schedule(5)` calls? 
",dangra,sibiryakov
1618,2015-12-01 21:36:15,"@dangra and @kmike I think it makes sense to remove other `nextcall.schedule(5)` calls. I'm going to update PR during next 12 hours.
",sibiryakov,dangra
1618,2015-12-01 21:36:15,"@dangra and @kmike I think it makes sense to remove other `nextcall.schedule(5)` calls. I'm going to update PR during next 12 hours.
",sibiryakov,kmike
1615,2016-02-18 20:14:31,"Hey @redapple!
I would like to take a stab at the bug. 
I went thru the source and noticed that for each item, we are creating a `DeferredList` and adding callbacks to it. What I think we can do is, in [`media_downloaded`](https://github.com/scrapy/scrapy/blob/75cd056223a5a8da87a361aee42a541afcf27553/scrapy/pipelines/files.py#L243) we can check if the status code is 201. If it is, we can create a new `deferred` object there using the URL in the `Location` attribute of the 201 response's headers. We can attach the exact same callbacks as for the `DeferredList`.

This approach has one downside though, we will cache the wrong result for this request and our stats will not be accurate as we will log this particular item as `failed` when in fact, we may get the file downloaded in the redirect link.

What do you think of these ideas? I can issue a [WIP] PR once you approve this approach. 
",darshanime,redapple
1615,2016-02-19 10:32:17,"@darshanime , thanks for looking into it!
I've never seen HTTP 201 responses myself so I'm not sure how this all works.
But if the response headers have a `Location`, one might want to mimic what `RedirectMiddleware` does for 30x's
@kurkop , @Slepice1 , @darshanime ,
do you know of any public website behaving like this so it could be tested live?
",redapple,darshanime
1615,2016-02-19 14:33:52,"Yes @redapple ,

You can testing in the next url: http://www.carroya.com/web/vehiculo/nuevoficha/renault/kangoo/c.a./2016/no-disponible/1616709.do

![Firebug output](https://cloud.githubusercontent.com/assets/2367831/13178243/c1c43d6a-d6eb-11e5-9066-83520d97348e.png)
",kurkop,redapple
1614,2016-10-27 23:14:17,"Hi @eliasdorneles,

Yes, according to RFC 1808, white spaces are not part of the URL, so it is safe to strip them.
",moisesguimaraes,eliasdorneles
1614,2017-01-12 00:09:23,"@moisesguimaraes RFC 1808 is quite old. Accordingly to **RFC 3986 - 3.1 Scheme**, the scheme should begin with a letter. 

>  Scheme names consist of a sequence of characters beginning with a
   letter and followed by any combination of letters, digits, plus
   (""+""), period ("".""), or hyphen (""-""). ",abmxer,moisesguimaraes
1612,2015-11-24 01:45:56,"@nramirezuy Could you elaborate on your answer? I'm new to scrapy. Why it does not make sense to set the logging level on a Spider? 
Where do you set it? On a project? What if I have different Spider?

Thanks
",YAmikep,nramirezuy
1612,2015-11-25 01:09:55,"Thanks @nramirezuy . I see what you meant now.
I agree and actually, only the command line option makes sense. Changing it at the project setting does not make sense either.
",YAmikep,nramirezuy
1612,2016-01-03 14:58:12,"@nramirezuy I set LOG_FILE setting in custom_settings for every Spider to get a different log, but it not work, what should i do? Thanks.
",jackie2013,nramirezuy
1612,2016-09-16 21:20:18,"@nramirezuy , I agree with you: development (`DEBUG`) is different from production (`INFO`). But I am facing a particular case...

I run multiple spiders using `CrawlerProcess`, each spider for one website. One of them is very good example of poorly coded html we find in the wild! It is a really f\* mess. Plus, it is always changing. So, almost every time a run my crawler (once every 45 days) I find out this particular spider is not running correctly because of the recent changes. Then I need to run it alone in development to be able to fix it.

If I could set just this spider to debug mode, my production would be in info mode and I would know how to fix this particular spider without doing a complete run in debug mode.

My other option is to always run this spider alone. But my workflow would be much more consistent if this bug was fixed.
",djunzu,nramirezuy
1612,2016-10-12 15:47:37,"@djunzu 
hi i have the log level setting in settings.py
![image](https://cloud.githubusercontent.com/assets/5271457/19317020/3aefc6a0-9069-11e6-8a84-8e51ec60234d.png)

but the info, debug also wrote in log file
",shyandsy,djunzu
1610,2016-07-29 10:30:25,"@redapple, can we merge this?
",darshanime,redapple
1610,2016-07-29 13:12:50,"Looks good, merging -- thanks @darshanime !
",eliasdorneles,darshanime
1609,2016-06-28 09:39:01,"@Digenis   You mean if we use self.logger  and  use  errcallback there will not  have those  traceback print again?

@nyov  I think you can refer http://doc.scrapy.org/en/latest/topics/request-response.html#using-errbacks-to-catch-exceptions-in-request-processing
",westwind027,nyov
1609,2016-06-28 09:39:01,"@Digenis   You mean if we use self.logger  and  use  errcallback there will not  have those  traceback print again?

@nyov  I think you can refer http://doc.scrapy.org/en/latest/topics/request-response.html#using-errbacks-to-catch-exceptions-in-request-processing
",westwind027,Digenis
1606,2015-11-17 06:21:26,"Thank you very mutch! @jdemaeyer ! Your detail is very useful!.
",dontcontactme,jdemaeyer
1604,2015-11-16 18:37:21,"@nramirezuy I'm sorry I didn't realise that. Thanks a lot for directing me!
",praveshjain,nramirezuy
1601,2015-11-15 21:44:31,"Thanks @mvj3 for these corrections!

I think one of them (the one I commented above) isn't correct, but I'm not 100% sure, I'll need someone to verify it.

If you want you can delete that change from this pull request so we can merge the other changes sooner, and submit that particular correction in a separate pull request to discuss about it.
",curita,mvj3
1601,2015-11-15 23:35:45,"Thanks @curita for the careful review! I correct it in a new commit.
",mvj3,curita
1599,2015-11-13 20:58:48,"Hey @kmike no changes at all to the boto library.. I might try a reinstall and if that doesn't work, update to the master branch
",bnussey,kmike
1599,2015-11-13 22:05:26,"Hey @kmike sorry yeah no effect at all. Tried a reinstall but still the same issue..
",bnussey,kmike
1599,2015-11-13 22:18:40,"Hey @kmike I just noticed scrapy bench works fine in the root directory but soon as I got into the project's directory, thats where I have issues.. So going to mess with the settings and see what I can do
",bnussey,kmike
1599,2015-11-17 04:30:18,"Hey @kmike got it, thanks for your help. I know this is out of scope, but I am new to Scrapy, so if you could assist, I would be hugely appreciative: In terms of the structure of scrapy scripts, is there anyway to wrap this up in a function so the request is not executed every time I do anything else in my project?


",bnussey,kmike
1597,2015-11-23 20:53:21,"@Digenis , @kmike ,
looks alright to me,
although the `../@checked` part feels a bit odd.
But more correct now than before (I mean before the PR)
so LGTM

FWIW, I was thinking of something like


",redapple,Digenis
1597,2015-11-23 20:53:21,"@Digenis , @kmike ,
looks alright to me,
although the `../@checked` part feels a bit odd.
But more correct now than before (I mean before the PR)
so LGTM

FWIW, I was thinking of something like


",redapple,kmike
1594,2015-11-17 09:04:57,"@nramirezuy @kmike updated PR to address your concerns. 
",pawelmhm,nramirezuy
1594,2015-11-17 09:04:57,"@nramirezuy @kmike updated PR to address your concerns. 
",pawelmhm,kmike
1594,2015-11-17 16:32:35,"@pawelmhm Looks better to me, but make it stop! (the looping call task)
",nramirezuy,pawelmhm
1594,2015-12-01 15:02:25,"@nramirezuy good point, I made it stop
",pawelmhm,nramirezuy
1593,2015-11-12 13:19:12,"@curita Is a negative priority adjust of `+2` :sheep: (the text is wrong)

I guess this one is missing on the docs [`RETRY_PRIORITY_ADJUST`](https://github.com/scrapy/scrapy/blob/master/scrapy/settings/default_settings.py#L222).
",nramirezuy,curita
1593,2016-01-26 18:27:38,"@curita , @nramirezuy , would #1727 work?
",redapple,nramirezuy
1593,2016-01-26 18:27:38,"@curita , @nramirezuy , would #1727 work?
",redapple,curita
1592,2015-11-16 22:43:53,"@darshanime both suggestions sound good to me :) I'd add that we should mention this setting in the builtin settings list here: http://scrapy.readthedocs.org/en/latest/topics/settings.html#built-in-settings-reference as well.

Could you help us with a pull request with those changes? I'll gladly review it when you submit it.
",curita,darshanime
1591,2015-11-11 20:41:03,"@jdemaeyer If you can provide some simple example with `Python` `dicts` it would be amazing. It will make it a lot easier to understand.

There are some cases, for example if you want to change the scheduler for instance ([Frontera](https://github.com/scrapinghub/frontera)). It would also make possible to use Scrapy Cloud to update and not just override.
",nramirezuy,jdemaeyer
1591,2015-11-12 14:07:26,"@nramirezuy more like this? (didn't use the default `SPIDER_MIDDLEWARES` since it's empty, and we don't want users updating the `_BASE` settings)
",jdemaeyer,nramirezuy
1591,2016-08-22 10:09:50,"@pawelmhm nobody addressed @nyov's concerns so far :)
",kmike,nyov
1591,2016-08-22 10:09:50,"@pawelmhm nobody addressed @nyov's concerns so far :)
",kmike,pawelmhm
1589,2016-01-26 18:32:38,"@LinkZhang , are you still having this issue?
If yes, can you provide a full traceback as @kmike requested?
Thanks.
",redapple,kmike
1589,2016-01-27 01:41:06,"@kmike @redapple sorry, I used requests+lxml to rewrite this crawl. 
",LinkZhang,redapple
1589,2016-01-27 01:41:06,"@kmike @redapple sorry, I used requests+lxml to rewrite this crawl. 
",LinkZhang,kmike
1589,2016-11-29 09:30:52,"@Digenis  I have same issue  of scrapy+pyinstaller,my environment is  python3.5 +windows10 +scarpy1.2.1

This is my code of run my spiders:



Please you help me 
",WuQianyong,Digenis
1588,2016-01-19 09:12:18,"@Digenis, thank for you snippet, i will use it in my project, but when i talked about solution i meant a solution for scrapy. I think it is wrong when scrapy just throws an error in output and can log nothing about the error.
",kalombos,Digenis
1588,2016-02-18 12:08:19,"@pilgrim2go , did you get a chance to try @Digenis 's patch?
",redapple,Digenis
1588,2016-09-19 10:57:29,"As I understand this, it's a Django bug,
and there's a patch for it from @Digenis https://github.com/scrapy/scrapy/issues/1588#issuecomment-172779831
Thus, I'm closing it.
",redapple,Digenis
1587,2015-11-09 13:55:29,"@jdemaeyer pretty sure is legacy code. We should remove those tests to avoid confusion.
",nramirezuy,jdemaeyer
1586,2015-11-06 15:49:34,"@jdemaeyer there is a failure in docs check, could you please fix it?
",kmike,jdemaeyer
1586,2015-11-06 21:02:44,"@jdemaeyer yes, our code works as expected on this PR. Thanks.
",rolando,jdemaeyer
1586,2015-11-11 17:08:36,"Perfect! Thank you both @jdemaeyer and @kmike :) I'll be merging it now.
",curita,jdemaeyer
1586,2015-11-11 17:08:36,"Perfect! Thank you both @jdemaeyer and @kmike :) I'll be merging it now.
",curita,kmike
1583,2015-11-03 23:33:34,"Hey @akhillb, thanks for the patch!

What is a use case in which users need to see the full path of an internal Scrapy component?
",jdemaeyer,akhillb
1583,2015-11-03 23:53:35,"@jdemaeyer see https://github.com/scrapy/scrapy/issues/1309 for some motivation. I wonder if it should be default or not.
",kmike,jdemaeyer
1583,2015-11-06 16:01:02,"Sorry for the delay. @kmike i think LOG_SHORT_NAMES is good. @jdemaeyer what do you think about it?
",akhillb,jdemaeyer
1583,2015-11-06 16:01:02,"Sorry for the delay. @kmike i think LOG_SHORT_NAMES is good. @jdemaeyer what do you think about it?
",akhillb,kmike
1583,2015-11-22 12:59:40,"@akhillb A test would be good
",jdemaeyer,akhillb
1583,2015-12-04 11:32:20,"After going back and forth I think it is fine to merge it without tests. @akhillb could you please squash the commits?
",kmike,akhillb
1583,2016-10-18 15:30:22,"This PR looks good to me, but it appears that the original branch/repo doesn't exist anymore (Github says `unknown_repository` for the source branch).
@akhillb are you still interested in wrapping this up? Thanks!
",eliasdorneles,akhillb
1582,2016-09-09 22:40:17,"Thank you @kmike and @Digenis :)
Please have a look at this updated PR.
",starrify,kmike
1581,2015-11-06 18:25:05,"Hm, I'm not sure I can follow you @Digenis 

> If this change is applied then such projects can suddenly start caching.

This PR will have no impact on http caching in projects. Scrapy will _always_ cache as soon as `HTTPCACHE_ENABLED` is set to `True`, independent of whether `HTTPCACHE_DIR` is a relative or an absolute path (and independent of whether that paths exists before running the spider). When people don't want the http cache in their production environment, they need to set `HTTPCACHE_ENABLED = False` in that environment's `local_settings.py`.

This PR will only affect the http cache behaviour for _stand-alone spiders_ (those that you run through `scrapy runspider` instead of `scrapy crawl`), in that they can now also use relative paths. That someone explicitly sets `HTTPCACHE_ENABLED = True` in their `Spider.custom_settings` and then finds the http cache disabled (because they're not in a project) is definitely a bug and not part of an API in my eyes.
",jdemaeyer,Digenis
1581,2016-02-03 00:59:45,"@Digenis so the backwards-incompatible change users can face in practice is that with `HTTPCACHE_ENABLED=True` cache will be enabled when using scrapyd? Did I get this right? 
",kmike,Digenis
1581,2016-09-09 20:43:15,"Right, I still think that this is a bug fix.
It's not impossible to use http cache with them, there is an ugly workaround I mentioned above (creating a `scrapy.cfg` file in the current/root directory).

I agree with @jdemaeyer, I think it's reasonable to expect that if `HTTPCACHE_ENABLED=True` Scrapy should try to use the cache and I'm not keen on doing a cycle of deprecation just for this change.
Is there another way we could alleviate the problem for that case, like logging a message or something?
",eliasdorneles,jdemaeyer
1581,2016-09-22 14:36:46,"@redapple @kmike do you think if we document the backward incompatibility, we could get this in 1.2 as well?
",eliasdorneles,kmike
1581,2016-09-22 14:48:03,"@eliasdorneles , I'm ok with including this for 1.2
I don't know if I can explain the backwards incompatiblity myself, can you give it a shot for https://github.com/scrapy/scrapy/pull/2216 ?
Also, can this feature have a test for it?
",redapple,eliasdorneles
1581,2016-09-22 16:48:14,"Sure, I'll see if I can add a test and describe in a comment on #2216 (@redapple the branch is on your repo, I'm unable to commit on it).
",eliasdorneles,redapple
1581,2016-09-29 21:38:04,"So, @redapple and I wrote tests for this, and while we were at it we found that Deltafetch is also uding that data_path function -- so this fix will apply to that as well (PR title updated to reflect it).

Also, this will fix cache for `scrapy shell` besides `scrapy runspider` (consistent with the current behavior `scrapy shell` working inside a project if the setting is enabled).
Thanks @redapple for the help :+1: 
",eliasdorneles,redapple
1580,2015-11-04 09:59:52,"> In other words, why does crawler need a logger and stats as soon as it is initialized?

@kmike One thing which looks important to me - setup logging before `Spider.__init__` - I still remember times when it wasn't possible to log messages during spider initialization and that wasn't good - some spiders are doing quite complex configuration setup there and logging is essential.

> On the other hand, we cannot move the spider initialisation out of `crawl()` because we don't have access to the spider args before.

@jdemaeyer Maybe it makes sense to pass spider args to `Crawler.__init__` - so that spider can be instantiated, settings filled and frozen - and `Crawler.crawl` will be responsible only for creating engine and starting spider with `start_requests`? `crawl()` accepts spider args because of historical reasons - it was possible to run several spiders using the same `Crawler` instance. Now it's not supported.
",chekunkov,jdemaeyer
1580,2015-11-04 09:59:52,"> In other words, why does crawler need a logger and stats as soon as it is initialized?

@kmike One thing which looks important to me - setup logging before `Spider.__init__` - I still remember times when it wasn't possible to log messages during spider initialization and that wasn't good - some spiders are doing quite complex configuration setup there and logging is essential.

> On the other hand, we cannot move the spider initialisation out of `crawl()` because we don't have access to the spider args before.

@jdemaeyer Maybe it makes sense to pass spider args to `Crawler.__init__` - so that spider can be instantiated, settings filled and frozen - and `Crawler.crawl` will be responsible only for creating engine and starting spider with `start_requests`? `crawl()` accepts spider args because of historical reasons - it was possible to run several spiders using the same `Crawler` instance. Now it's not supported.
",chekunkov,kmike
1580,2015-11-04 11:16:42,"Yes, but it we keep logging setup before `Spider.__init__` - we cannot change LOG_LEVEL and LOG_FORMATTER from spider. If we move logging setup after `Spider.__init__` - we can change those settings but logging won't work from `Spider.__init__`. I think first option is better, but I don't like the idea of having special-case settings which cannot be changed from `Spider.__init__` because they are used to setup some object before spider creation. On the other hand @jdemaeyer is right and we already have such special cases (`SPIDER_LOADER_CLASS`) so maybe that's not a big problem.

I might regret telling that, but now I'm not sure it's a good idea to do settings configuration during/after `Spider.__init__`. Telling that I have an idea to share. If we have access to spider arguments in `Cralwer.__init__` - we can pass them to classmethod `update_settings(settings, spider_args)`. In this case developer can override `Spider.update_settings` and change settings based on spider arguments - that's basically what I wanted in #1305. Does it work for addons?
",chekunkov,jdemaeyer
1580,2015-11-09 21:24:43,"@jdemaeyer That last change would totally fix using custom_settings when sub classing.
",nramirezuy,jdemaeyer
1578,2016-01-25 13:31:21,"@kmike , as per our dicsussion last week, I believe we can close this issue,
as long as not retrying HTTP 400 is properly announced in the 1.1 release notes.
",redapple,kmike
1575,2015-11-16 23:52:00,"@kmike should I squash it all again? or is it too messed up at this point?
",palego,kmike
1575,2015-11-29 10:57:23,"Hey @palego,

Sorry for the delay. Yeah, codecov thinks you've touched many files because scrapy master changed since the PR was opened. Rebasing on master should fix it. I think there is no need to squash, but it doesn't hurt either :) 

The PR looks fine to me, +1 to merge it.

My main worry is that we probably shouldn't go too far improving scrapy template generating features; if it is hard to support IMHO it is better to switch to http://cookiecutter.readthedocs.org/en/latest/.
",kmike,palego
1575,2015-11-30 09:43:01,"Hi @kmike,

Nevermind the delay, I understand this is low priority. 

Thanks for the explanation (sorry, I should have realized that by myself instead of wasting your time...). I will rebase/squash this evening.

Regarding template generation, I won't go any further. The purpose here is just to have the current mechanism working as documented, with no harm to a future cookiecutter implementation.
",palego,kmike
1574,2015-11-02 08:22:04,"Hi @nhuray and @matveinazaruk! A nice feature. The implementation looks good, apart from a couple of minor issues; +1 to merge after they are fixed. 

Test coverage looks good, I'm not sure how to make coverage 100% either (it doesn't seem to worth an effort).
",kmike,matveinazaruk
1574,2015-11-02 15:13:48,"@kmike I added a commit with the recommendations you made
",nhuray,kmike
1574,2015-11-02 15:55:57,"@kmike do you want I squash the commit before merging the PR ?
",nhuray,kmike
1574,2015-12-08 17:48:28,"I would like to see this move forward.  Approaching this with a `ignored_errors` attribute would make sense to me, @kmike . I guess its value could be checked early on, so the contract would either be actually run or not, @nhuray .
",mkcor,kmike
1574,2015-12-08 18:12:00,"@kmike +1 the feature deserves it's own package.
",nramirezuy,kmike
1572,2015-11-05 21:34:06,"@nramirezuy disabling telnet in generated settings template solves nothing because these settings are used in a project, and in a project you likely want telnet enabled. You're more likely to want telnet disabled in a custom script which uses CrawlerProcess, but this script likely doesn't use settings.py. 

There is already commented out code to disable telnet in project settings template (a bit convoluted though, it doesn't use TELNET_ENABLED option).
",kmike,nramirezuy
1572,2015-11-06 15:06:31,"@kmike I would say to use `CrawlerProcess` you have `Scrapy` understatement, basically you provably already made some spider and you are trying something more advanced. Given this point I would say there is nothing to solve.

We should simply give more awareness for this settings on the documentation, and not change defaults because they are a better fit for an advanced user.
",nramirezuy,kmike
1572,2015-11-06 15:42:28,"@nramirezuy I think it is more a question of providing a first-class library interface, not about being beginner or advanced user. @nyov is well aware of telnet extension, but he doesn't want to start a new telnet server for each spider executed by CrawlerProcess, it doesn't make much sense for a library to do so.
",kmike,nyov
1572,2015-11-06 15:42:28,"@nramirezuy I think it is more a question of providing a first-class library interface, not about being beginner or advanced user. @nyov is well aware of telnet extension, but he doesn't want to start a new telnet server for each spider executed by CrawlerProcess, it doesn't make much sense for a library to do so.
",kmike,nramirezuy
1572,2015-11-11 20:45:13,"@curita I like that solution a lot more. It just has to be be clear on the documentation.
",nramirezuy,curita
1571,2016-02-18 14:32:30,"@redapple  Yes. Thanks a lot.
",rylanchiu,redapple
1568,2015-11-04 18:47:41,"@kmike arachnado isn't already some sort of extension? Or you looking to run the webserver with the same reactor as the spider?

Can you like explain a little bit more what do you want to accomplish with this new project?
",nramirezuy,kmike
1568,2015-11-04 18:51:59,"@nramirezuy AFAIU, arachnado doesn't use Scrapy at all, it uses Tornado.

I believe the idea here is something like `scrapy runspider myspider.py -s ENABLE_WEBUI=1`

This would run the spider and also start a small little http server which you could use to inspect the state of the crawl (pending requests, items already scraped, perhaps memory usage, etc).
",eliasdorneles,nramirezuy
1568,2015-11-05 21:26:15,"@eliasdorneles @nramirezuy  Arachnado is an UI for Scrapy spiders; backend is implemented as a Tornado app (running in the same event loop as Scrapy / Twisted) + several custom components (CrawlerProcess, ExecutionEngine, StatsCollector, ...). It provides an HTTP API to start/stop/resume spiders and get information about running jobs; there is also websocket support for real-time notifications. UI uses this API to present information to users. 

Arachnado is quite similar to https://github.com/scrapy-plugins/scrapy-jsonrpc, but with a more advanced UI. The fact Tornado is used for the API backend instead of Twisted doesn't matter much because they can share the same event loop; I picked tornado because it has websockets builtin and because I have more experience with it. 

So Arachnado is already kind-of a Scrapy extension, and it runs in the same event loop as Scrapy spiders. But there are several problems with it:
1. Currently spiders are started from Arachnado, not the other way around. You can plug a custom spider to Arachnado, but you can't (easily, in a documented way) plug Arachnado to a Scrapy project. It should be an opposite.
2. It contains some features not suitable for general-purpose UI, e.g. it exports items to MongoDB; in develop branch there is Formasaurus integration (when crawler finds a login form it shows it to user; user can fill it and start a new crawl logged in); @shirk3y works on several other similar features, like using specialized forum spiders automatically when a forum is detected.
3. It overrides way too many Scrapy components; the implementation is hack-ish: see https://github.com/TeamHG-Memex/arachnado/blob/master/arachnado/crawler_process.py. I think it is better to make Scrapy flexible enough to accomodate these use cases, not to keep all this stuff.
4. Arachnado is an UI for CrawlerRunner, i.e. it can display stats for multiple spiders executed in the same process and start/stop/pause them. But Scrapy doesn't have 'CrawlerRunner extension', it only has Spider extensions which are started for each spider. Maybe the page with UI for a single spider job (the one which displays requests, etc.) can work as a Spider extension.

So IMHO a new extension should take some parts from Arachnado, but many other parts should be changed or removed. Arachnado should become a project which uses this extension.
",kmike,nramirezuy
1568,2015-11-05 21:26:15,"@eliasdorneles @nramirezuy  Arachnado is an UI for Scrapy spiders; backend is implemented as a Tornado app (running in the same event loop as Scrapy / Twisted) + several custom components (CrawlerProcess, ExecutionEngine, StatsCollector, ...). It provides an HTTP API to start/stop/resume spiders and get information about running jobs; there is also websocket support for real-time notifications. UI uses this API to present information to users. 

Arachnado is quite similar to https://github.com/scrapy-plugins/scrapy-jsonrpc, but with a more advanced UI. The fact Tornado is used for the API backend instead of Twisted doesn't matter much because they can share the same event loop; I picked tornado because it has websockets builtin and because I have more experience with it. 

So Arachnado is already kind-of a Scrapy extension, and it runs in the same event loop as Scrapy spiders. But there are several problems with it:
1. Currently spiders are started from Arachnado, not the other way around. You can plug a custom spider to Arachnado, but you can't (easily, in a documented way) plug Arachnado to a Scrapy project. It should be an opposite.
2. It contains some features not suitable for general-purpose UI, e.g. it exports items to MongoDB; in develop branch there is Formasaurus integration (when crawler finds a login form it shows it to user; user can fill it and start a new crawl logged in); @shirk3y works on several other similar features, like using specialized forum spiders automatically when a forum is detected.
3. It overrides way too many Scrapy components; the implementation is hack-ish: see https://github.com/TeamHG-Memex/arachnado/blob/master/arachnado/crawler_process.py. I think it is better to make Scrapy flexible enough to accomodate these use cases, not to keep all this stuff.
4. Arachnado is an UI for CrawlerRunner, i.e. it can display stats for multiple spiders executed in the same process and start/stop/pause them. But Scrapy doesn't have 'CrawlerRunner extension', it only has Spider extensions which are started for each spider. Maybe the page with UI for a single spider job (the one which displays requests, etc.) can work as a Spider extension.

So IMHO a new extension should take some parts from Arachnado, but many other parts should be changed or removed. Arachnado should become a project which uses this extension.
",kmike,eliasdorneles
1567,2015-10-30 08:34:59,"I haven't thought about extensions @kmike. It sounds like a better idea since it would provide additional flexibility i.e. connecting to signals. For instance, one would want to flush the file and push it as a part to S3 once the spider idle.
",jersub,kmike
1566,2015-10-29 10:42:03,"Hey @darshanime, thanks for taking the time to help with the docs.

This is a very long example with quite a bit of boilerplate (setting up the Crawler and CrawlerProcess), maybe too much for just showing how to connect to a signal. Perhaps it could be sufficient to assume that the `crawler` object already exists and then connect a single callback? Just my two cents though ;)
",jdemaeyer,darshanime
1566,2015-11-02 08:39:34,"Hey @darshanime. Thanks for working on it!

I think the example is not perfect because it won't work if a spider is started using `scrapy crawl` or `scrapy runspider`. Check http://doc.scrapy.org/en/latest/intro/overview.html - this is how a spider is usually defined & executed. To connect a signal without writing a custom start script you have to override spider's [from_crawler](https://github.com/scrapy/scrapy/blob/57f87b95d4d705f8afdd8fb9f7551033a7d88ee2/scrapy/spiders/__init__.py#L49) method. Another way is to connect a signal in start_requests method, but some signals will be already fired when start_request is called. Heh, this shows how bad our signal docs are - it is hard even for a dedicated person to figure this out :)
",kmike,darshanime
1566,2015-11-02 10:43:33,"Hi @kmike !

I cooked this up but the function `item_scraped` is not firing. Can you help me figure this out ? I am learning Python still :/
(This is the `dmoz` spider) 


",darshanime,kmike
1566,2015-11-02 12:01:20,"I like the examples, good work @darshanime!

One small thing ;) We try to comply to [PEP8](https://www.python.org/dev/peps/pep-0008/) (which is the standard style guide for Python): The indentation level (e.g. after `def:` or `if:`) should always be four spaces. And there should be no spaces around the `=` in keyword arguments.

You can see at the bottom of this PR that there are failing checks. Ignore the codecov one (it makes sure that code you put into Scrapy's source is tested through a test in `tests/`, but since you only worked on the docs it got confused). The travis-ci one however should always pass. Here it failed because there was ""/home/travis/build/scrapy/scrapy/docs/topics/signals.rst:32: ERROR: Unexpected indentation."" For Sphinx (the tool building the docs), you need to announce code blocks with a double colon, e.g. `catch signals and perform some action::`

You can check whether the test passes at home by running `tox docs` in the scrapy repository.
",jdemaeyer,darshanime
1566,2016-05-10 10:37:39,"@mgachhui , looks like this conversation stalled after @darshanime 's question.

I would comment on the question by agreeing with @kmike that `spider_closed` is a better suited and more common example for using signals in spider classes. So I would remove `item_scraped` signal handler from the example.

I had to cook up a similar example the other day on IRC. Having it in the docs, how to properly and idiomatically hook a spider_closed handler makes a lot of sense.
",redapple,kmike
1566,2016-05-10 10:37:39,"@mgachhui , looks like this conversation stalled after @darshanime 's question.

I would comment on the question by agreeing with @kmike that `spider_closed` is a better suited and more common example for using signals in spider classes. So I would remove `item_scraped` signal handler from the example.

I had to cook up a similar example the other day on IRC. Having it in the docs, how to properly and idiomatically hook a spider_closed handler makes a lot of sense.
",redapple,darshanime
1566,2016-05-10 10:37:39,"@mgachhui , looks like this conversation stalled after @darshanime 's question.

I would comment on the question by agreeing with @kmike that `spider_closed` is a better suited and more common example for using signals in spider classes. So I would remove `item_scraped` signal handler from the example.

I had to cook up a similar example the other day on IRC. Having it in the docs, how to properly and idiomatically hook a spider_closed handler makes a lot of sense.
",redapple,mgachhui
1566,2016-05-10 13:47:14,"I will submit an update commit soon, thanks for the review!

On Tue, May 10, 2016 at 4:08 PM, Paul Tremberth notifications@github.com
wrote:

> @mgachhui https://github.com/mgachhui , looks like this conversation
> stalled after @darshanime https://github.com/darshanime 's question.
> 
> I would comment on the question by agreeing with @kmike
> https://github.com/kmike that spider_closed is a better suited and more
> common example for using signals in spider classes. So I would remove
> item_scraped signal handler from the example.
> 
> I had to cook up a similar example the other day on IRC. Having it in the
> docs, how to properly and idiomatically hook a spider_closed handler makes
> a lot of sense.
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/scrapy/scrapy/pull/1566#issuecomment-218120821
",darshanime,kmike
1566,2016-05-10 13:47:14,"I will submit an update commit soon, thanks for the review!

On Tue, May 10, 2016 at 4:08 PM, Paul Tremberth notifications@github.com
wrote:

> @mgachhui https://github.com/mgachhui , looks like this conversation
> stalled after @darshanime https://github.com/darshanime 's question.
> 
> I would comment on the question by agreeing with @kmike
> https://github.com/kmike that spider_closed is a better suited and more
> common example for using signals in spider classes. So I would remove
> item_scraped signal handler from the example.
> 
> I had to cook up a similar example the other day on IRC. Having it in the
> docs, how to properly and idiomatically hook a spider_closed handler makes
> a lot of sense.
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/scrapy/scrapy/pull/1566#issuecomment-218120821
",darshanime,darshanime
1566,2016-05-10 13:47:14,"I will submit an update commit soon, thanks for the review!

On Tue, May 10, 2016 at 4:08 PM, Paul Tremberth notifications@github.com
wrote:

> @mgachhui https://github.com/mgachhui , looks like this conversation
> stalled after @darshanime https://github.com/darshanime 's question.
> 
> I would comment on the question by agreeing with @kmike
> https://github.com/kmike that spider_closed is a better suited and more
> common example for using signals in spider classes. So I would remove
> item_scraped signal handler from the example.
> 
> I had to cook up a similar example the other day on IRC. Having it in the
> docs, how to properly and idiomatically hook a spider_closed handler makes
> a lot of sense.
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/scrapy/scrapy/pull/1566#issuecomment-218120821
",darshanime,mgachhui
1566,2016-07-24 14:30:20,"@redapple can we merge this?
",darshanime,redapple
1566,2016-07-25 09:53:05,"@redapple, kindly check now!
",darshanime,redapple
1566,2016-07-25 13:49:27,"Thanks @darshanime !
",eliasdorneles,darshanime
1565,2015-11-02 15:03:24,"Thanks for the kind notice @kmike :) A test case has now been added.
",starrify,kmike
1563,2015-11-02 15:03:34,"Thanks for the kind notice @kmike :) A test case has now been added.
",starrify,kmike
1561,2015-10-29 08:42:44,"Thanks for the response @jdemaeyer . I was going for those options you listed before, but I found a shorter way. I ended up keeping a self.pages_crawled property that gets incremented for each `parse` and call `CloseSpider('max pages reached')` when it reaches the maximum. It's not precise, but so was `CLOSESPIDER_PAGECOUNT`. At least it shuts off close to the number provided.
",deanq,jdemaeyer
1561,2015-10-29 09:29:58,"hi @pawelmhm. It is already possible to configure the the CloseSpider extension through the command line tool, because all settings can be overridden from there. Like this:
`scrapy crawl myspider -s CLOSESPIDER_PAGECOUNT=100`
",jdemaeyer,pawelmhm
1561,2015-10-29 17:25:09,"I'm using scrapyd to run my crawlers, and I need to be able to pickup parameters passed through the POST. Command line doesn't apply in my case.

On Thu, Oct 29, 2015 at 5:30 AM, Jakob de Maeyer notifications@github.com
wrote:

> hi @pawelmhm. It is already possible to configure the the CloseSpider extension through the command line tool, because all settings can be overridden from there. Like this:
> 
> ## `scrapy crawl -s CLOSESPIDER_PAGECOUNT=100 myspider`
> 
> Reply to this email directly or view it on GitHub:
> https://github.com/scrapy/scrapy/issues/1561#issuecomment-152126365
",deanq,pawelmhm
1560,2015-11-06 15:47:59,"> I've wrote a pseudo-cookie middleware which I configure to handle some query arguments as cookies.

@Digenis could you please give an example of how are you using it? Is it something similar to https://github.com/scrapy/scrapy/issues/900?
",kmike,Digenis
1560,2015-11-06 22:24:04,"The way it prevents ""cache busting"" should also prevent ""fingerprint cache busting"".
So it's just related.

> @Digenis could you please give an example of how are you using it? 

I just configure it from the settings
to always set some query argument to something
or always drop some query argument from the url
(~~or always merge some query argument in a cookiejar fashion~~ -> not implemented)
",Digenis,Digenis
1560,2016-02-09 11:22:01,"hey @Digenis this middleware you talk about sounds really useful, would you mind sharing it? Doesn't have to be PR to Scrapy, maybe just some repo or even github gist with middleware
",pawelmhm,Digenis
1559,2016-09-16 10:18:49,"@jersub , @kmike , @eliasdorneles , how can we make progress on this?
Add tests for S3 first, and then work on multipart uploads?
Move to boto3 first? (cf. #1866)
(Recent WIP on using boto3 for file uploads: #2036)
",redapple,eliasdorneles
1559,2016-09-16 10:18:49,"@jersub , @kmike , @eliasdorneles , how can we make progress on this?
Add tests for S3 first, and then work on multipart uploads?
Move to boto3 first? (cf. #1866)
(Recent WIP on using boto3 for file uploads: #2036)
",redapple,kmike
1558,2015-11-02 12:42:24,"As @jdemaeyer noted, the provided code example is not valid, so I'm closing this issue.
",kmike,jdemaeyer
1557,2015-10-27 17:57:28,"A good catch @curita, that's definitely one of the issues. 
@k287 packages required for Ubuntu/Debian are documented here: http://doc.scrapy.org/en/master/intro/install.html#ubuntu-9-10-or-above - do you think there is something missing? Or maybe we can improve this chapter to make it more clear how to install Scrapy - do you have any suggestions?
",kmike,curita
1556,2015-10-26 10:31:39,"Hey @darshanime,

you need to install the header files of the `libffi` library (which is not a Python but a C package). Depending on your OS, there should be a `libffi-dev` or similar package available in your package manager. See also [here](https://github.com/SimonSapin/cairocffi/issues/14).
",jdemaeyer,darshanime
1556,2015-10-26 10:57:02,"Thanks for the tip @jdemaeyer, that almost solved my problem. 
I am able to run the tests, but all fail. 
`ImportError: cannot import name _monkeypatches`.

I am running `Python 2.7.10 :: Anaconda 2.1.0 (64-bit)` on Ubuntu 14.04
",darshanime,jdemaeyer
1556,2015-10-26 11:28:18,"hey @darshanime - could you please paste the updated log, after fixing the libffi issue?
",kmike,darshanime
1556,2015-10-26 11:34:44,"@jdemaeyer thanks for your help !

@kmike Here is the new [log](https://gist.github.com/darshanime/2c40da1408265c260b41)
",darshanime,jdemaeyer
1556,2015-10-26 11:34:44,"@jdemaeyer thanks for your help !

@kmike Here is the new [log](https://gist.github.com/darshanime/2c40da1408265c260b41)
",darshanime,kmike
1556,2015-10-28 06:32:43,"@kmike, @jdemaeyer; got it working finally. 

I think this could be an error in the conda package management system. I created a fresh `conda` virtual environment, installed scrapy and ran the tests; same error. 
Then I created another virtual environment with my native python installation (not the Anaconda install) and used `virtualenv` to do it instead of `conda`. 

`virtualenv -p /usr/bin/python2.7 venv`

Installed scrapy in this, ran the tests; no error. 

Closing. 
",darshanime,jdemaeyer
1556,2015-10-28 06:32:43,"@kmike, @jdemaeyer; got it working finally. 

I think this could be an error in the conda package management system. I created a fresh `conda` virtual environment, installed scrapy and ran the tests; same error. 
Then I created another virtual environment with my native python installation (not the Anaconda install) and used `virtualenv` to do it instead of `conda`. 

`virtualenv -p /usr/bin/python2.7 venv`

Installed scrapy in this, ran the tests; no error. 

Closing. 
",darshanime,kmike
1556,2015-10-28 18:50:49,"@darshanime Could you share the output of `conda info`? Did you try updating conda and python? If not, try running: `conda update conda` and `conda update python`. Then you could try creating a new conda env.
",rolando,darshanime
1556,2015-10-28 19:12:27,"@rolando 
I ran `conda update conda` and `conda update python`, then created a virtualenv and ran `python setup.py install`.
I can now import scrapy successfully but cannot run the command line operations :
`$ python -c 'import scrapy` works, but :
`$ scrapy` gives me :



Here is `conda info` :


",darshanime,rolando
1556,2015-10-29 00:14:00,"@darshanime ~~The `OpenSSL` lines in your traceback are a bit suspicious.~~ ~~Did you used `pip` to install scrapy and its dependencies?~~ Installing packages that require compilation in `conda` via `pip` can cause problems sometimes.

Try: `conda create -n scrapy-env -c scrapinghub scrapy`

It will create a new conda environment `scrapy-env` (change the name if already exists) and install `scrapy` from the `scrapinghub` channel.

**Update:** If you require to run the developing version of `scrapy`, then you can run `pip develop .` inside the `scrapy` repository after installing most of its dependencies via `conda`, in particular `openssl` and related packags. (Running `python setup.py install` and `python setup.py develop` should work as well)


",rolando,darshanime
1556,2015-11-02 11:27:45,"@rolando, I did what you asked. Scrapy is installing fine but the tests are failing. Same error as earlier. 
",darshanime,rolando
1555,2015-10-26 07:30:59,"Thanks @stummjr!
",kmike,stummjr
1550,2015-10-19 21:36:19,"@kmike thanks for looking into this.

Yeah, I think the desired behavior is for the `scrapy shell`, if it does not find the ""protocol"" there or may be there is an error during url parsing, try to interpret the argument as a path to a file - it should also handle both absolute and relative to the current directory paths..I think this is actually how it worked before 1.0..

Here are the samples:



I'm sure you can think about other test/use cases. Thanks again!
",alecxe,kmike
1550,2015-11-02 12:12:53,"@kmike , I would also allow it for `scrapy parse`. what do you think? I have used that to test callback code with local files
",redapple,kmike
1550,2015-11-02 12:19:18,"@redapple yeah, this makes sense.
",kmike,redapple
1550,2016-01-25 17:41:50,"@redapple almost, `scrapy shell ./index.html` works, but `scrapy shell index.html` produces `twisted.internet.error.DNSLookupError: DNS lookup failed`. Let me know if you need more details. Thanks for looking into this.
",alecxe,redapple
1550,2016-01-25 17:47:47,"@kmike I am okay with it and it makes sense. Please though mention it in docs additionally. Thanks.
",alecxe,kmike
1550,2016-01-25 17:50:49,"@kmike @alecxe I would suggest adding both examples and based on that explain why `DNSLookupError` is raised.
",nramirezuy,kmike
1550,2016-01-26 13:45:51,"@alecxe , @nramirezuy , docs updated. Plz have a look at #1710
",redapple,nramirezuy
1548,2015-10-19 15:15:40,"@nramirezuy Can you suggest a failing test?  I'm not sure I understand your concern.
",jschnurr,nramirezuy
1548,2015-10-19 23:34:08,"@nramirezuy Here's what your examples look like before & after this PR:



I must be missing something - '.jpg' seems like a much more sensible file extension than '0&q=60&w=800'.  Is there some other side effect that hasn't been accounted for?

Thanks for clarifying.  Happy to update once I understand the problem.
",jschnurr,nramirezuy
1548,2016-10-11 16:54:32,"@redapple what logic do you propose for multiple GET arguments (if we are to improve it, maybe after merging this PR)?
",kmike,redapple
1548,2016-10-11 17:31:15,"@kmike , I haven't thought much about this obviously, but at least make it not dependent on query parameters order. Testing common extensions may be too naive.
Not directly related, but do we currently handle Content-Disposition?
",redapple,kmike
1546,2016-02-18 11:57:11,"@datakid , are you still having the issue?
If so, please provide a traceback as @kmike requested.
Thanks.
",redapple,kmike
1544,2015-10-18 03:59:12,"@kmike this PR restores `service_identity` as a dependency https://github.com/scrapinghub/scrapinghub-conda-recipes/pull/4 
",rolando,kmike
1544,2015-11-26 12:01:15,"@rolando is this PR ready to merge?
",kmike,rolando
1544,2015-11-26 16:56:17,"@kmike yes, it's good to merge.
",rolando,kmike
1536,2016-01-25 22:37:10,"@kmike , I believe tests for `parse` do now pass in Python (cf. https://github.com/scrapy/scrapy/pull/1678)
",redapple,kmike
1534,2015-10-12 12:12:56,"Yeah, that's a wrong use of logging API; the way to go is to fix it in Airbake. Thanks @curita for the investigation and for a workaround. I'm closing this ticket now; @rrrazdan please reopen if it didn't help.
",kmike,curita
1532,2015-10-07 11:29:44,"Thanks @hoatle!
",kmike,hoatle
1529,2015-10-09 19:40:08,"@Digenis I think there is no need for aggressive removals of deprecated code. IMHO we definitely shouldn't remove code and modules deprecated in 0.24 during 1.1 release.
",kmike,Digenis
1529,2015-10-30 17:33:24,"@Digenis that's a good question. Our options:
1. Release Scrapy 1.1 without these backwards incompatible changes, follow semver (create a branch, cherry-pick commits);
2. Declare 1.1 'unstable' (but I don't see a point of doing such release, compared to just using master);
3. Decide that these incompatibilities are in fact bug fixes and release 1.1 as-is (maybe without #1149).

What do you think?

""scrapy hasn't reached 1.0"" is outdated (see https://github.com/scrapy/scrapy/pull/1541)
",kmike,Digenis
1529,2015-10-30 17:42:08,"i'm with @redapple on that
",kmike,redapple
1529,2015-10-30 19:05:02,"@rolando , I think this is for Scrapy Cloud to worry, not to upgrade automatically. And that you have choice to upgrade or not. I'll raise that loud enough.
",redapple,rolando
1524,2015-10-05 11:12:59,"Thanks @Digenis! 

We need to think about backwards compatiility: say, people disabled Telnet extension using EXTENSIONS option (as shown in the [example](https://github.com/scrapy/scrapy/blob/master/scrapy/templates/project/module/settings.py.tmpl#L58) in generated settings.py) - it shouldn't become enabled after the relocation. It is better to have a test for it. Adding it [here](https://github.com/scrapy/scrapy/blob/91cbf974150309e8b0401e7d0f5b0c4757bf260f/scrapy/utils/deprecate.py#L126) may be enough, but I'm not sure. //cc @curita  
",kmike,Digenis
1524,2015-10-05 17:11:16,"Adding it to `DEPRECATION_RULES` in `scrapy/scrapy/utils/deprecate.py` should be enough. It's already tested, so there's no need to add a separate test for telnet's deprecation. I've been meaning to write a wiki entry for this kind of relocations/deprecations, probably I should get started :smile: 

@Digenis: Grepping 'telnet' in the repo shows that there are pending `scrapy.telnet` references in `docs/topics/settings.rst`, `docs/topics/telnetconsole.rst` and `scrapy/templates/project/module/settings.py.tmpl`, could you fix those too?
",curita,Digenis
1524,2015-10-09 22:34:01,"@nyov yeah, I'm also disabling telnet console with CrawlerProcess. Btw, Settings is not necessary because CrawlerProcess accepts dicts, and s3 handler shouldn't cause problems now (it used to cause 1s startup delay).
",kmike,nyov
1522,2015-10-03 08:23:19,"Thanks @smirecki!
",kmike,smirecki
1518,2015-10-05 05:20:06,"@dangra @kmike I see your points, looking into refactoring and the idea for helper functions.
",theresia,dangra
1518,2015-10-05 05:20:06,"@dangra @kmike I see your points, looking into refactoring and the idea for helper functions.
",theresia,kmike
1518,2015-10-05 05:30:42,"@dangra By the way, how is it backward incompatible? It passes all the existing tests.
",theresia,dangra
1516,2015-09-30 03:45:33,"@rolando please rebase on top of recent commits from master to fix travis-ci build.

btw, what happened to @darkrho? 
",dangra,rolando
1508,2015-09-24 23:35:29,"@dangra sounds good to me. Please go ahead and create the repository.
",rolando,dangra
1504,2015-09-21 01:42:54,"Ahh yes I see that now. Thanks @kmike
",JerPScott,kmike
1499,2016-01-21 23:56:41,"So, today I got together with @stummjr for a day sprint wrapping up the work for this port.

We followed the advice from @kmike on the above comments and in #1080 and we were finally able to run scrapy with Python 3 exporting items to JSON, XML and CSV -- so nice to see this working! =)

We weren't sure of some of the decisions, looking forward to your comments.

Thank you!
",eliasdorneles,kmike
1499,2016-01-23 19:20:31,"@kmike I've addressed your comments, and added a test for feed export XML output.
I tried adding tests for pickle and marshal output, but I think they're broken, hence I created issue #1713
",eliasdorneles,kmike
1499,2016-01-26 00:33:15,"@dangra so, I improved the coverage a tiny little bit, but 100% of coverage will be awkward because of reasons I cover below.

While improving coverage, I realized that the [except TypeError: pass in CsvItemExporter](https://codecov.io/github/scrapy/scrapy/scrapy/exporters.py?ref=d515212209051f48ad7cb634d8a02b2ed5efc50d#l-180) had kinda undefined behavior -- it's for the case when transforming lists to CSV which aren't made of strings/bytes. The current behavior (code in master, traced back to [this commit](https://github.com/scrapy/scrapy/commit/8a5c08a6bcde96938ed92ee4996aafb6655c3688)) is to just pass lists of non-strings to [csv.writer.writerow](https://docs.python.org/2/library/csv.html#csv.csvwriter.writerow), which would put in the CSV the Python representation of the object.
So I updated the new code (which was crashing trying to pass lists of non-strings to `to_native_str`), making explicit the fallback to `repr()` and added a test for it.

So, the reasons why getting to 100% coverage would be just awkward are:

1) `MarshalItemExporter` is already covered by an integration test in `test_feedexport.py` that runs a spider and exports to an actual file (btw, why isn't that showing up in the coverage?). The marshal module is awkward to test isolatedly in PY 2 because it doesn't work with file-like objects, only with actual files:



2) The [workaround in XmlItemExporter](https://codecov.io/github/scrapy/scrapy/scrapy/exporters.py?ref=d515212209051f48ad7cb634d8a02b2ed5efc50d#l-151) is unreachable unless it runs in Python <= 2.7.3 -- which we're not running it in Travis, so I don't really see the point.

So, does this look good?
",eliasdorneles,dangra
1499,2016-01-26 03:34:37,"@eliasdorneles: I added a test case for MarshalItemExporter and a coverage exclusion for XmlItemExporter workaround.

~~Interestingly it is not 100% due to partial coverage at https://codecov.io/github/scrapy/scrapy/scrapy/exporters.py?ref=6fc286753faadb22db89ea66e0ef934c0840ec29#l-275~~ Now it is 100%
",dangra,eliasdorneles
1498,2015-09-21 14:38:19,"Thanks @Preetwinder! Your PR is very good (and it was good from the beginning); +1 to merge it after a few minor changes.

> Since the master branch has changed, should I merge or rebase before pushing the new commits?

I think it is better to rebase.

> While writing more extensive tests, I discovered another failure point due to urlparse incorrectly handling invalid URLs. If the scheme is not present and both the port and the path are specified, an incorrect URL is generated. This happens because urlparse uses the first occurrence of : to parse the scheme. Correcting this would require some hackey code, if we intend to stay within the feature set provided by the urlparse library.

Yeah, urlparse doesn't handle invalid URLs. There is no need to stay within urlparse feature set; I'm [fine](https://github.com/scrapy/scrapy/issues/1306) even with replacing urplarse entirely.

In the end we should provide a function to ""normalize"" a URL like browsers normalize them when users enter a URL to the address bar. Adding schema is a step this function should be doing.
",kmike,Preetwinder
1494,2015-09-14 14:50:28,"@mlyundin It is nice, but I don't see how it helps. The values that go out from there are going to be loaded into an item.
",nramirezuy,mlyundin
1493,2015-09-13 14:46:36,"Yeah, I agree with @Digenis: we document Items as providing dictionary-like API.
But the example at http://doc.scrapy.org/en/latest/topics/items.html#other-common-tasks can be improved. So I think it is a documentation issue.
",kmike,Digenis
1490,2016-02-27 11:02:59,"@kmike I deleted the project and then rebuild the scrapy, followed the Scrapy Tutorial V1.0...and now everything is OK.
The project was built a long time ago, maybe that's the problem.
thx for your reply.
",chenjian158978,kmike
1485,2015-09-13 14:19:54,"@kmike Hey, I am actually trying to now port to python 3.4 and I did pip install scrapy, and the issue is bogus compiler found. Full error log here:-> http://pastie.org/10416881
",d3prof3t,kmike
1481,2015-09-08 19:18:30,"@mlyundin can you rebase to master? 
",nramirezuy,mlyundin
1481,2015-09-14 19:20:20,"@nramirezuy I don't think so. +1 to merge
",dangra,nramirezuy
1481,2015-09-14 19:25:11,"@kmike comments are fair but I think it is more clean code for same functionality when performance is not measured or taken into consideration at all.
",dangra,kmike
1481,2015-09-14 19:35:56,"@nramirezuy I don't think that is needed, the untested looks fine. 
",dangra,nramirezuy
1481,2015-09-14 19:38:52,"Ok, let's just merge this :) Thanks @mlyundin for the patience!
",kmike,mlyundin
1480,2015-09-04 17:07:28,"@dangra better?
",kmike,dangra
1479,2015-09-04 17:11:19,"@kmike it needs a backport to 1.0 and a new release
",dangra,kmike
1479,2015-09-04 17:11:59,"@dangra +1 to backport; not sure about a release because it is only a problem for Python 3.
",kmike,dangra
1474,2015-09-01 21:39:08,"@nramirezuy  sorry about that, I thought it might be a bug.
Just for the records: No, I have not changed anything and no, reinstalling it does not corrects.

Anyway, I am moving this thread to support. (https://groups.google.com/forum/#!topic/scrapy-users/F-bPk9PkdhE)

*PS: I was running version 1.0.1, now the same happens for version 1.0.3
",thalesfc,nramirezuy
1473,2015-09-02 13:09:14,"@ArturGaspar thanks for increasing coverage of robotstxt mw, looks like it didn't reach 100% because of a missing branch check at https://codecov.io/github/scrapy/scrapy/scrapy/downloadermiddlewares/robotstxt.py?ref=af2acd5046c9349b4f1082b401bf8a8a9f9d93b9#l-76.

I saw you added deferred support to spidermiddlewares but please, keep this PR focused on adding support to downloader middleware, It is acceptable to port robotstxt becaues it is a good example on how to use this feature.

Spider Middleware have subtles differences like been able to return iterables so I prefer if its support is added in another PR.
",dangra,ArturGaspar
1471,2015-09-01 14:59:08,"@ArturGaspar if you can do a PR for that it would be amazing!
",nramirezuy,ArturGaspar
1468,2015-09-09 13:40:17,"Hey @rhoekman,

There are some more steps necessary in Debian 8 (see and http://doc.scrapy.org/en/master/intro/install.html#ubuntu-9-10-or-above). I'm closing this PR because it is covered by https://github.com/scrapy/scrapy/pull/1350. Do you think the updated instructions are OK? If you have improvement ideas, PRs are welcome! 
",kmike,rhoekman
1467,2015-08-31 11:26:49,"@dacjames looks good!
I don't use contexts either, sorry.
",kmike,dacjames
1467,2015-09-04 17:31:24,"@dangra Well there are two things I can think of; I don't like right now:
- Sharing the item.
- Single method for xpath and css.

For some reason I would like having indentation; kinda makes the blocks more clear and define a start and end to that child.

I would also like to remove response and selector from the context, and provide a way to move the loader in the meta. Sometimes makes it hard to implement processors; when you have the data in two different pages.

EDIT: Also why creating a new ItemLoader instance when we are just looking to substitute the Selector instance inside it.
",nramirezuy,dangra
1467,2015-09-04 19:27:09,"@nramirezuy I split the function into two calls, hopefully eliminating concern number 2.  I agree on that front, this is better.

I'm very confused by the objection to sharing the item.  That's kind of the entire purpose of this feature, to be able to populate the same item with a loader specialized for a particular subset of the page.

Creating a new ItemLoader instance is desirable because modifying the existing ItemLoader leads to unexpected behavior and I like being able to use both loaders at the same time. 

... will fix the coverage issue shortly.
",dacjames,nramirezuy
1467,2015-09-04 21:19:05,"@nramirezuy do you propose to change syntax to this?


",kmike,nramirezuy
1467,2015-09-04 21:35:47,"@kmike That syntax would be worse for my use case.  I create two nested loaders, one for each section of the page that have partially redundant information, then go through each data point of interest and try to pull from both locations (and from the original loader in one case).

With the proposed syntax, there does not appear to be any way to reference the top-level, un-nested loader.
",dacjames,kmike
1467,2015-09-04 22:08:33,"@dacjames could you show a code example? 
",kmike,dacjames
1467,2015-09-04 22:15:14,"Sure, I need to filter out some private info, but I'll post an example
later this evening.

On Fri, Sep 4, 2015 at 3:09 PM, Mikhail Korobov notifications@github.com
wrote:

> @dacjames https://github.com/dacjames could you show a code example?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/scrapy/scrapy/pull/1467#issuecomment-137865400.
",dacjames,dacjames
1467,2015-09-07 15:06:23,"@dacjames having multiple instances of the same loader is an issue; first of all you gotta call `load_item` for each loader. Then the order matters, because it will override the field on the item.
",nramirezuy,dacjames
1467,2015-09-13 16:53:38,"@nramirezuy in @dacjames's implementation there is no need to call `load_item()` for each loader (it is a point of this PR), and an order matters just like it matters for `loader.add_xpath` calls with a single loader. Or did I get your comment wrong?
",kmike,nramirezuy
1467,2015-09-13 16:53:38,"@nramirezuy in @dacjames's implementation there is no need to call `load_item()` for each loader (it is a point of this PR), and an order matters just like it matters for `loader.add_xpath` calls with a single loader. Or did I get your comment wrong?
",kmike,dacjames
1467,2015-09-14 09:51:33,"@nramirezuy a nested loader is a distinct instance of ItemLoader that shares some state with its parent.  Specificaly, the [`_values`](https://github.com/dacjames/scrapy/blob/master/scrapy/loader/__init__.py#L39) and [`items`](https://github.com/dacjames/scrapy/blob/master/scrapy/loader/__init__.py#L46) properties are shared.  I landed on this implementation because it naturally preserves the expected ordering of `add_*` calls.
",dacjames,nramirezuy
1467,2015-09-14 15:49:03,"@kmike @dacjames Sorry I got that wrong. 

But I still think this is the way to go:



If you want you can:


",nramirezuy,dacjames
1467,2015-09-14 15:49:03,"@kmike @dacjames Sorry I got that wrong. 

But I still think this is the way to go:



If you want you can:


",nramirezuy,kmike
1467,2015-09-14 17:27:24,"@nramirezuy what's an adavntage of writing `selector=selector` in all these `.add_xpath` calls, why do you prefer it?
",kmike,nramirezuy
1467,2015-09-14 20:29:57,"@nramirezuy this PR doesn't preclude setting the selector per your example.  It just provides a nice shortcut to avoid having to write the all the duplicate `selector=loader.selector.xpath('/some/subquery')` arguments.

I personally don't like the context manager approach because using it incorrectly is too easy.


",dacjames,nramirezuy
1467,2015-09-14 21:02:04,"@dacjames what am I missing?


",nramirezuy,dacjames
1466,2016-04-27 21:43:54,"@redapple sorry for late reply, I just saw your comment. Looks like I'll be able to work on this.
",umrashrf,redapple
1466,2016-05-21 09:24:44,"@dangra @redapple do you like it?
",umrashrf,redapple
1466,2016-05-21 09:24:44,"@dangra @redapple do you like it?
",umrashrf,dangra
1466,2016-10-18 10:56:17,"@umrashrf , @nyov , @dangra , @eliasdorneles 
here are my suggestions: https://github.com/umrashrf/scrapy/pull/1
",redapple,nyov
1466,2016-10-18 10:56:17,"@umrashrf , @nyov , @dangra , @eliasdorneles 
here are my suggestions: https://github.com/umrashrf/scrapy/pull/1
",redapple,dangra
1466,2016-10-18 13:40:16,"@nyov , I also updated the docs in https://github.com/umrashrf/scrapy/pull/1
",redapple,nyov
1466,2016-10-20 10:41:30,"@redapple, That almost looks like something completely different to the previous patch ;D
You're amazing.
Now that I finally got my local testsuite working again, I can hopefully leave a review on this in a bit.
",nyov,redapple
1466,2016-10-20 14:07:49,"@nyov @redapple I am aware of the new developments :) I will go through your comments and work on it over this weekend.
",umrashrf,nyov
1466,2016-10-20 14:07:49,"@nyov @redapple I am aware of the new developments :) I will go through your comments and work on it over this weekend.
",umrashrf,redapple
1466,2016-12-27 20:58:27,"Okay, @redapple 's changes got merged into this PR.

Does anyone have a chance to test this ""in the wild""?",nyov,redapple
1466,2017-03-08 11:38:16,"@kmike I don't see a reason why FormRequest won't work. Can you please give an example where you think it fails to work?

Also if you can give an example of the autologin-middleware and OAuth as how you would expect it work with AuthMiddleware, that'd be great.",umrashrf,kmike
1466,2017-03-08 14:28:37,"So @kmike, do you recommend keep HttpAuth middleware with the changes for
credentials in uri, maybe rename it to HttpBasicAuth, and have a seperate
one for FtpAuth perhaps?

Le 8 mars 2017 15:03, ""Mikhail Korobov"" <notifications@github.com> a écrit :

> @umrashrf <https://github.com/umrashrf> my comment was only about names.
> autologin-middleware will work with AuthMiddleware.
>
> AuthMiddleware handles auth, but it doesn't handle all kinds of auth,
> only a couple of specific auth cases. AuthMiddleware name is too broad
> and generic to my taste. autologin-middleware is an example of middleware
> which handles a different kind of auth, which is even more common than HTTP
> Basic Auth - cookie-based auth (submit login form, keep cookies, login
> again in case of logouts).
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/scrapy/scrapy/pull/1466#issuecomment-285048082>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AA2GGJLQ3bh4CUgFTcR-oF9EdFp_zeohks5rjrT8gaJpZM4F0iZU>
> .
>
",redapple,kmike
1466,2017-03-08 14:58:55,"@redapple I haven't though about this option (use two middlewares), but now as you wrote it this sounds like a good idea, I like it.",kmike,redapple
1466,2017-03-09 14:02:51,"@umrashrf this name makes sense, but I also like @redapple's suggestion to create two middlewares: they have almost nothing in common.",kmike,redapple
1466,2017-03-14 09:55:36,"For the record, the single middleware approach comes from [@dangra 's comment here](https://github.com/scrapy/scrapy/pull/670#issuecomment-38921769).",redapple,dangra
1466,2017-03-16 15:15:19,"@umrashrf , @kmike,
what do think of these options:

1. create an UrlAuth middleware that 1) extracts user/pass from request urls, and 2) populate request.meta keys appropriately depending on scheme, 3) strip credentials from the urls
2. leave HttpAuth middleware as-is (current master branch state)
3. at HTTP downloader level, add Authorization header for HTTP (if missing), just like FTP downloader does

or,
1. create an UrlAuth middleware (make it run before HttpAuth for outgoing requests)
2. change HttpAuth to also handle http_user/pass from request meta

(/cc @dangra )",redapple,kmike
1466,2017-03-16 15:15:19,"@umrashrf , @kmike,
what do think of these options:

1. create an UrlAuth middleware that 1) extracts user/pass from request urls, and 2) populate request.meta keys appropriately depending on scheme, 3) strip credentials from the urls
2. leave HttpAuth middleware as-is (current master branch state)
3. at HTTP downloader level, add Authorization header for HTTP (if missing), just like FTP downloader does

or,
1. create an UrlAuth middleware (make it run before HttpAuth for outgoing requests)
2. change HttpAuth to also handle http_user/pass from request meta

(/cc @dangra )",redapple,dangra
1465,2015-09-22 03:32:07,"@kmike no making no proposals.  Just confirming that  Python 3 porting is an ongoing effort so it doesn't declare support for python 3 so I've left scrapy as supported by only python2 in our ebuilds.
",idella,kmike
1464,2015-08-31 11:51:15,"@kmike ok;
 this testsuite is not being run with  multiple Scrapy test suites in parallel. Definitely one at a time.  No other packages are being run, Just one thing at a time.  What makes me question this is that we have;
tests/test_proxy_connect.py FF..FF
so unless I am missing something the first test run is failing and is therefore not competing with any other test at least from test_proxy_connect.py unless it is tied up from a previous run of the suite. To me they should be cleared on completion of a run whether clean or unclean.  The first test failing then blockades those that follow except that the middle two actually pass. 

It appears the tests are not getting 'clear wind' and so are not actually being run to test what it is they were to test.  I'd say I can exclude these tests for now but they are currently broken or unreliable ( not robust) in the running of the whole suite.

Thx for reply
",idella,kmike
1455,2015-08-26 21:15:17,"@chekunkov we deprecate it  :sheep: 
",nramirezuy,chekunkov
1455,2015-08-26 21:29:33,"> we deprecate it

@nramirezuy jajaja

@chekunkov I guess asyncio integration will be experimental for a while and only available for py3.
",dangra,nramirezuy
1455,2015-08-26 21:29:33,"> we deprecate it

@nramirezuy jajaja

@chekunkov I guess asyncio integration will be experimental for a while and only available for py3.
",dangra,chekunkov
1454,2015-09-01 13:54:21,"thanks @kmike !
",pablohoffman,kmike
1454,2016-11-21 17:11:33,Hey @kmike !  Can you take a look at this issue? Seems like the README still doesn't render it the progress PY3 badge.,kirankoduru,kmike
1454,2016-11-21 17:13:35,@pablohoffman what do you think about removing the badge for now?,kmike,pablohoffman
1453,2015-08-24 18:20:52,"@nramirezuy I'd like to automatically have cookies set in response in the same way as [request constructor  accepts them](http://doc.scrapy.org/en/1.0/topics/request-response.html#scrapy.http.Request). In fact, in the [naïve implementation linked above](https://gist.github.com/vincent-ferotin/f7aa88b24d2b364a98e1), this is [how I set value of `response.meta['response_cookies']`](https://gist.github.com/vincent-ferotin/f7aa88b24d2b364a98e1#file-cutom_cookies-py-L81), parsing `Set-Cookie` header. But all present ticket is about, if Scrapy's developers think this could be a desirable enhancement, allowing such values to be given through arbitrary attributes set at runtime, e.g. `response.cookies` (for my concern, in the same way as in the aforementioned trial passed cookies to request are accessible through `request.cookies`), or for an other example `response.whatever_attribute_name`.

Is it more clear ?-)
",vincent-ferotin,nramirezuy
1453,2015-08-24 21:50:04,"@nramirezuy Perhaps I've previously not clearly split, in the narrative of the ticket,  (a)my main use case, from (b) the purpose of the ticket.

So (a):
You're right, my main concern was to easily get cookies as dicts from both request and response, and not let these pieces of information hidden inside the `CookiesMiddleware`. The short snippet linked above actually suffices to this need (modulo its bugs). But this is just a use case / application of the ticket.

Then: from (a) to (b):
I opened the ticket because, before using for response cookies `request.meta['response_cookies']`, I tried to store them directly in `response.cookies`. This trial failed because, somewhere (I don't know where (failing to find it), but presume in other location of middlewares' stack?), response object passed in `CookiesMiddleware.process_response`, is replaced by new instance of a more suitable class, i.e. `TextResponse`, `XmlResponse` or `HtmlResponse`. When this replacement occurs, only shared attributes between those classes are copied, by parameters of constructor I suppose. Additional attributes added at runtime in middlewares, such as my naïve `response.cookies` attribute, are then lost in translation.

Last (b):
This is where I found interested in opening this ticket. Could Scrapy relax this constraint of ""no other added attributes (than those set in `Response` and its subclasses) are preserved"" on response journey through middleware? Such constraint, that _now_ I fully understand, is _not obvious a priori_, especially for new comers to Scrapy, possibly accustomed to the very dynamic nature of Python. 

As a new comer to Scrapy, I found interesting to simply add my own attributes directly to request and response in my own middleware, and accessing them through spider parse methods. However, Scrapy developers could perfectly have an other point of view. So this ticket, serving for both discussion about it, a possible application to cookies, and a reference through code snippet to a possible bypass for other future users with similar goals...

(Please, as it clearly appears that I'm not succint nor clear, let reformulate ticket subject (as you previously tried!), for future reference. By the way, thank you very much for your patience!)
",vincent-ferotin,nramirezuy
1443,2015-08-21 11:39:39,"@kmike, the reason for the  shortcut was to do the plumbing for the user (https://github.com/scrapy/scrapy/issues/740#issue-34860974), so I think forcing the user to raise `DontCloseSpider` by default is removing half the added value of the shortcut.

Raising `CloseSpider` to bail out of the loop we started by 'recursively' calling `idled()` feels kinda dirty, like goto.

> I think yes, it should be called infinitely, with some delay in between.

Then I think the first one is actually the best: Not raising `DontCloseSpider` when we encounter an empty return value, will gracefully quit the loop.
_Unless_ the user raised `DontCloseSpider` in their `idle()` method (instead of  returning `None`).
That should give them better control flow I think. With the current code:


",nyov,kmike
1439,2015-08-28 02:34:52,"I like the current naming convention, I think is pretty straightforward to deduce each component usage by its name. I give you that it is boring and common but what it lacks in originality makes up for quick understanding. Just my 2 cents.  
BTW, how did you come up with that canary name @dangra :)
",agusc,dangra
1439,2015-08-28 13:46:25,"I'm OK to keep using current names, we only have to make it clear what is stable/testing/unstable in this context.

> BTW, how did you come up with that canary name @dangra :)

Chrome has a Canary channel https://www.google.es/chrome/browser/canary.html, but it is common in:
- http://whatis.techtarget.com/definition/canary-canary-testing
- http://martinfowler.com/bliki/CanaryRelease.html
",dangra,dangra
1438,2015-08-18 20:40:36,"@cyberplant can you update the Scrapy ubuntu package documentation to reflect this?
",pablohoffman,cyberplant
1435,2016-02-10 10:48:40,"as for the original discussion, @dangra , @nyov , @gmeans ,
I would rather that we leave the SSL/TLS layer agree with the remote host, negotiating with sane defaults (as SSLv23_method does), by default out of the box,
and offer pluggable context factories for special cases or needs.
I don't like the idea of retrying with different settings under-the-hood, on behalf of the user.
and I don't think configuring settings via requests meta would be easier than different available built-in context factories. (though I may be missing cases where one needs input from the user)
",redapple,nyov
1435,2016-02-10 10:48:40,"as for the original discussion, @dangra , @nyov , @gmeans ,
I would rather that we leave the SSL/TLS layer agree with the remote host, negotiating with sane defaults (as SSLv23_method does), by default out of the box,
and offer pluggable context factories for special cases or needs.
I don't like the idea of retrying with different settings under-the-hood, on behalf of the user.
and I don't think configuring settings via requests meta would be easier than different available built-in context factories. (though I may be missing cases where one needs input from the user)
",redapple,dangra
1435,2016-02-10 14:18:56,"Revisiting this, I think it might be good to have a user-friendly default, that simply works, for most users.
So long as behavior is somehow configurable (different context factories, etc.) and there would be some notification somehow if the project-chosen defaults are potentially insecure (such as an automated retrying might be), I'm fine with that.

But I'll excuse myself from this discussion, as I don't seem to have a good idea or strong opinion which way this might best be implemented. Both @dangra and @redapple make sense to me.

(Aside: It'd be cool to have the option to log the actual TLS connection options/Cipher Suite on first connection to a particular site, as a verification method. Doesn't have to be active by default.)
",nyov,redapple
1435,2016-02-10 14:18:56,"Revisiting this, I think it might be good to have a user-friendly default, that simply works, for most users.
So long as behavior is somehow configurable (different context factories, etc.) and there would be some notification somehow if the project-chosen defaults are potentially insecure (such as an automated retrying might be), I'm fine with that.

But I'll excuse myself from this discussion, as I don't seem to have a good idea or strong opinion which way this might best be implemented. Both @dangra and @redapple make sense to me.

(Aside: It'd be cool to have the option to log the actual TLS connection options/Cipher Suite on first connection to a particular site, as a verification method. Doesn't have to be active by default.)
",nyov,dangra
1435,2016-02-20 10:22:08,"@redapple works for me as well
",juanriaza,redapple
1435,2016-07-11 07:29:10,"I'm closing this as #1794  is now merged.
I opened https://github.com/scrapy/scrapy/issues/2111 about @nyov idea of logging actual cypher suites.
",redapple,nyov
1434,2016-06-02 18:20:43,"@kmike 



my settings looks like





There is no problem for http only website with proxy settings.
For instance, http://www.google.com is Ok. But, https://www.google.com, reporting ""Could not open CONNECT tunnel with proxy"" .
If disable ""misc.middleware.CustomHttpProxyMiddleware"", both http and https work well.

And  curl works well as well 


",yssource,kmike
1434,2016-06-17 22:23:55,"@kmike 

We are having a similar problem in Scrapy when using proxies to target https urls. ""Could not open connect tunnel with proxy.""  This includes proxies that are supposed to support https.  Curious if there are  any insights into this.  We are using Scrapy 1.1RC4.
",Lachoneous,kmike
1434,2016-06-20 23:21:22,"@redapple , I have sent an email to you describing the problem, sample python code and proxies to reproduce the problem.
",kalyanp,redapple
1434,2016-06-20 23:22:22,"@redapple , Forgot to mention. I am replying on behalf of @Lachoneous 
",kalyanp,redapple
1434,2016-06-21 19:42:03,"@redapple , I pulled the changes you made and I was doing some testing. So far I haven't seen any issues and that change actually fixed the issue. Thanks for the quick response and the fix. Now I see clear messages in the logs too which is very helpful.
",kalyanp,redapple
1434,2016-06-21 19:46:44,"Thanks @redapple !  Your quick response to this issue is nothing but awesome.  We'll keep testing and let you know if we find any other issues, but so far this fix appears to resolve the CONNECT issue in a remarkable way.
",Lachoneous,redapple
1431,2015-08-12 17:29:18,"@dangra oops, forgot to do this!
LGTM!
",eliasdorneles,dangra
1429,2015-08-14 17:20:42,"ok so after a lengthy pow-wow over on the PyOpenSSL dev channel we got this figured out.

First, the reason for the error is CloudFlare only supports TLS 1.2 and not TLS 1.0. How this worked on @dangra's machine I'm not sure. It was necessary for me to change the method in the context factory to SSLv23_METHOD. Per the PyOpenSSL guys this is a poorly named option that allows protocol negotiation.

You have to be sure the insecure protocols are disabled, however. Looking at Twisted's code these seem to be covered, and honestly I'm not sure how critical that is in the domain of a crawler.

I ran into another interesting issue that may explain the sporadic HTTPS issues @dangra initially mentioned. On OSX PyOpenSSL actually ends up bound to the original system OpenSSL (0.9.8z). This version of OpenSSL doesn't support TLS 1.1 or TLS 1.2 so even after switching the protocol methods I wasn't able to connect initially.

To fix that and bind PyOpenSSL to my homebrew installed OpenSSL I had to do the following:



Hope this can help someone else dealing with the same issues.
",gmeans,dangra
1427,2015-08-12 14:15:04,"@dangra updated.
",kmike,dangra
1423,2015-10-26 15:32:08,"@jdemaeyer it may be a codecov issue similar to https://github.com/codecov/support/issues/100
",kmike,jdemaeyer
1423,2015-10-29 14:10:53,"Thanks @jdemaeyer, @curita and @nramirezuy.
",kmike,curita
1423,2015-10-29 14:10:53,"Thanks @jdemaeyer, @curita and @nramirezuy.
",kmike,jdemaeyer
1418,2015-08-10 19:17:43,"> what about adding service_identity to Scrapy install_requires?

@kmike It's about time to do so and deal with the debian packaging problems somewhere else.
",dangra,kmike
1417,2015-10-06 20:34:55,"Hey @Preetwinder,

I don't know; maybe it is better even to remove `--lsprof` option. Sorry for a confusing ticket. 

If we decide to keep it then even a single test would be an improvement. But to test it properly we'll have to write some tests for https://github.com/scrapy/scrapy/blob/master/scrapy/xlib/lsprofcalltree.py: unlike `--profile` this option doesn't just call stdlib profiler.
",kmike,Preetwinder
1409,2015-08-05 20:05:41,"@eliasdorneles yes, I think so if it pass tests with that simple change.
",dangra,eliasdorneles
1409,2015-08-05 20:09:55,"@dangra talking about tests, any idea why I'm getting failures on `test_squeues.py` there? :point_down: : 
",eliasdorneles,dangra
1409,2015-08-05 20:40:56,"@dangra it's definitely weird, but I tested locally fixing Twisted version to 15.2.1 and it passed.
no idea why, though.
",eliasdorneles,dangra
1409,2015-08-05 21:03:23,"@kmike it fails if you run the whole testsuite but pass if you run just that test file
",dangra,kmike
1409,2015-08-05 21:25:46,"@kmike that's for sure. 
",dangra,kmike
1409,2015-08-06 00:42:33,"@dangra @kmike so, I've made the changes to use `response.selector` in link extractors.

The only place still using LxmlDocument is here: https://github.com/scrapy/scrapy/blob/master/scrapy/http/request/form.py#L60

I get a bunch of test failures if I try to replace that for `response.selector`, so I suppose it's not safe to make the assumption all responses there will have a selector for it -- should we keep it there?
",eliasdorneles,dangra
1409,2015-08-06 00:42:33,"@dangra @kmike so, I've made the changes to use `response.selector` in link extractors.

The only place still using LxmlDocument is here: https://github.com/scrapy/scrapy/blob/master/scrapy/http/request/form.py#L60

I get a bunch of test failures if I try to replace that for `response.selector`, so I suppose it's not safe to make the assumption all responses there will have a selector for it -- should we keep it there?
",eliasdorneles,kmike
1409,2015-08-07 01:39:25,"@dangra okay, finally got rid of it. :) :tada: 
should I rebase and/or squash the commits?
",eliasdorneles,dangra
1409,2015-08-09 04:33:17,"Updated addressing @kmike's comments and rebased on top of current master.
",eliasdorneles,kmike
1409,2015-08-09 18:31:49,"@dangra done!
I also left the first simple test for selector to serve as base case.
",eliasdorneles,dangra
1409,2015-08-11 17:10:02,"@dangra updated and rebased on top of current master :)
",eliasdorneles,dangra
1409,2015-08-11 17:23:35,"great work @eliasdorneles !
",dangra,eliasdorneles
1409,2015-08-11 17:40:32,"@kmike feel free to merge when you are happy  
",dangra,kmike
1409,2015-08-11 20:59:47,"Great job @eliasdorneles! Also, thanks @umrashrf for getting the ball rolling.
",kmike,eliasdorneles
1400,2015-08-05 12:14:01,"Hey @nyov! 
- [x] _split requirements into files_ - looks good;
- [ ] _fix test cmdline_ - changes look good. It seems we don't have tests for `--profile` (see `_run_command_profiled` and `scrapy.xlib.lsprofcalltree`). To port cmdline we should add a test for this option and make sure it works.
- [ ] _renames (six types)_ - just some renames using six library - it looks good, but it seems there is a conflict with @dangra's merged cookies PR;
- [x] _fix test loader_ - LGTM;
- [ ] _fix tests pipelines files_ - Referrer header should be decoded for nicer log messages. There is also a tricky part when headers are passed to boto, but it looks like we're fine here. I also wonder if we should use canonicalize_url to create a hash (as we do in fingerprints), but this is out of scope.
- [x] _fix tests pipelines images_ - looks good, depends on files pipeline;
- [x] _response bodies as bytes_ - looks good.
",kmike,nyov
1400,2015-08-05 14:50:45,"@nyov I can either cherry-pick some of the commits or wait - what do you prefer?
",kmike,nyov
1400,2015-08-05 16:21:43,"> @nyov I can either cherry-pick some of the commits or wait - what do you prefer?
>  Show all checks

+1 to cherry pick :)
",dangra,nyov
1400,2015-08-10 07:39:15,"Merged in #1415. Thanks @nyov!
",kmike,nyov
1398,2015-08-02 20:31:14,"Hey @dangra,

As I understand it, in Python 3 stdlib header names and values are unicode (see examples in [docs](https://docs.python.org/3/library/urllib.request.html#urllib.request.Request)). But our WrappedRequest and WrappedResponse objects work with scrapy.http.Headers class which returns values as bytes; I wonder if these values are used by stdlib's CookieJar and if it is a problem. 
",kmike,dangra
1398,2015-08-03 01:29:50,"@kmike you're right, I updated the code to use native string within cookiejars.
",dangra,kmike
1395,2015-08-20 19:22:13,"@eltermann is there a reason to do this instead of reading an output file?
",nramirezuy,eltermann
1394,2015-07-31 18:51:14,"Thanks @GregoryVigoTorres! I've merged your changes to master.
",kmike,GregoryVigoTorres
1393,2015-07-29 21:32:54,"Thanks for all the useful comments.
So I only fixed testcases for the most part here, @kmike is of course right that there is probably more to do in many of those modules. I was on the mindset that it doesn't make much sense to keep tests commented once they pass. But if they are the porting guideline that makes sense.
I will comment out the tests again where I didn't thouroughly look through all the code.
",nyov,kmike
1393,2015-07-29 21:42:06,"@nyov - yep, unfortunately we don't have a better porting guideline now. Maybe it worths a comment in py3-ignores.txt file.
",kmike,nyov
1393,2015-07-29 22:06:19,"@curita nice! +1 to depending on twisted trunk for Python 3
",kmike,curita
1393,2015-07-31 18:54:14,"@nyov the changes look good, but there is a failing test. Could you please also send a PR to master branch, not to tmp-py3?
",kmike,nyov
1393,2015-08-01 05:06:32,"Oh crap now I did it.
New PR against master: #1400 

forget this one.

@dangra, yes, I'm sorry for mixing this. I wasn't feeling like throwing up several dependent PRs for this.
If splitting the requirements.txt meets with approval, I'll put the other changes into a seperate PR (or otherwise I'll drop that change from this queue). Or just cherry-pick what you can use.
",nyov,dangra
1392,2015-08-21 01:35:18,"@dangra, that implies knowledge of the fact that they are considered private. I mean they are not excluded from the modules export list, have no docs that would say so, or a name that would imply such, to me. I wasn't truly aware of that fact.
Aside from that, I tend to do a lot of `git grep` to have an overview of and get through a code base, as I don't use an IDE often enough.
Seeing a lot of `Slot` definitions stuck out since I couldn't easily determine which would be which, from a context-less grep result.
OTOH I can see the aesthetic in having the short names (my engineer side likes). Maybe it just needs more pointing out that these are private, and only ever used inside another proxy class (""belongs to"")?
",nyov,dangra
1392,2016-03-01 14:04:18,"Thanks @redapple. I think I'll close this though.
Unless there is merit in what @nramirezuy said about removing some of them?
Feel free to reopen, in that case.
",nyov,nramirezuy
1389,2015-07-29 07:48:32,"I like @nyov's suggestion.
",kmike,nyov
1389,2015-07-29 11:39:14,"Could `isinstance(text, six.binary_type)` be used instead?

Could `isbinarytext`be renamed `isbinary` or `isbinarytype`?

I also think @nyov's doc string is OK.
",GregoryVigoTorres,nyov
1389,2015-07-29 13:30:24,"If you feed it the correct input, it doesn't raise any exception as @kmike already noted.
You can't feed a complex datatype to a function which expects an integer argument, either, without getting an exception.
",nyov,kmike
1388,2015-08-20 22:00:43,"@kmike are we going to fix this issue for OS X? 

The fix is like opening the file twice; first on reading and then on append.
",nramirezuy,kmike
1388,2015-08-20 22:02:26,"@nramirezuy AFAIK this PR fixes the issue for OS X.
",kmike,nramirezuy
1388,2015-08-21 16:17:48,"Sorry, I kept the last comment from @nyov. Then it means you can always seek to 0; it makes sense otherwise it would dog danm strange :tongue: 
",nramirezuy,nyov
1386,2015-07-29 14:26:45,"Thanks @nyov!
",kmike,nyov
1385,2015-08-01 05:09:10,"@nyov I think creating a document based on every thing which has been deprecated is a good idea. Something like what Django has provided:
https://docs.djangoproject.com/en/1.8/internals/deprecation/
This would help search the deprecated parts in your codes and fix them with the recommended part.
",SirbitoX,nyov
1385,2015-08-02 09:42:06,"@nyov I said something like what Django has provided not the exact one.

> But a simple list of all deprecations is perhaps good for scrapy, that could help keeping track of them and later remove deprecated code in a timely manner.

It is a good idea :+1: 

@kmike The developers, who do not implement appropriate tests for all part of their project, should run all possible parts of their project to catch all the deprecated notices. So I think this is not good enough to wait a deprecation notice jump out of the code then we fix it. 
",SirbitoX,nyov
1385,2015-08-02 09:42:06,"@nyov I said something like what Django has provided not the exact one.

> But a simple list of all deprecations is perhaps good for scrapy, that could help keeping track of them and later remove deprecated code in a timely manner.

It is a good idea :+1: 

@kmike The developers, who do not implement appropriate tests for all part of their project, should run all possible parts of their project to catch all the deprecated notices. So I think this is not good enough to wait a deprecation notice jump out of the code then we fix it. 
",SirbitoX,kmike
1385,2015-08-20 20:26:08,"What is tests? :sheep: 

I'm on @kmike side here; warnings are very informative and should be enough. If you aren't using the function/method why are you worrying? also the documentation gets updated along with the change.

About the file with the list of deprecations; I'm cool with it, will help track of deprecated code for future removal. So we don't keep backwards compatible changes for 2 or 3 years :dog: 
",nramirezuy,kmike
1384,2015-07-28 09:37:23,"@nyov I think encoding should be guessed in RobotsTxtMiddleware.
",kmike,nyov
1384,2015-07-29 21:12:26,"@nyov Response vs BytesResponse renames is kind of orthogonal to Python 3 porting. I think there is something in your ideas, and it worths its own ticket.
",kmike,nyov
1384,2015-07-30 15:34:27,"@kmike yes, we are!
",dangra,kmike
1382,2015-12-04 13:06:51,"hey @barraponto, this looks good!
Could you solve the conflicts, so we can merge this?
Thanks, man!
",eliasdorneles,barraponto
1382,2016-01-04 12:50:45,"Very nice, thanks, @barraponto !
Can you change the test to use bytes (e.g. `self.assertEqual(fs[b'one'], [b'1'])`), so that it passes for PY33 too? Will be ready to merge after that. :)
",eliasdorneles,barraponto
1382,2016-01-21 03:17:59,"Thanks a bunch @barraponto ! :)
",eliasdorneles,barraponto
1377,2015-07-24 01:31:40,"> You could use the Response.replace method to replace the body of an existing Response instance

...or build a custom new response



Agree with @starrify, please don't abuse the issue tracker for support questions.
",nyov,starrify
1376,2015-07-23 05:58:15,"@starrify

Your solution does solve the problem partially, However, Scrapy would continue to follow the pages (based on the regex rule). what if there are 1000+ pages to follow?  (scrapy would not scrape links in each page however)

The efficient solution would tell scrapy to stop following any pages as well (perhaps by modifying the regex rule in LinkExtractor)

Do you have any implementation code suggestions for this?
",stejesh,starrify
1373,2015-07-22 13:33:08,"@jdemaeyer Thanks! I will do that. 
",systemovich,jdemaeyer
1372,2015-07-22 16:31:43,"> and if Socket.IO has had such tremendous success with a public Slack community

Let's not make decisions based on assumptions

> It is indeed hard (if they haven't used IRC yet), or at least annoying (if they don't regularly use IRC), for most users to seek help in the IRC channel

If by users you mean just any computer user, I may agree.
But in our case, they are coders at least.
The python community uses IRC and mailing lists extensively.
Such users are pushed back by this transition.
Maybe it will manage to provide support to new users, but different users.
I am afraid it will actually repel coders in favour of another audience,
users who are more likely to ask for solutions to their problems
before they RTFM. (Excuse me for the term,
but I think it's an necessary rite going extinct)

Regarding the IRC gateway, I doubt it's going to be functional.
Once non-IRC features sneak-in, it will become unusable for IRC users.
And what exactly are those features?
Does the scrapy community need them?
Does the **scrapy** community consist of non-IT people?
I am sure scrapy users have their clients or managers who are interested in web scraping
but clients/managers are not IT.
They are not concerned what the scrapy community uses to communicate
they are concerned with their contractors/employees getting their job done
and they will use the communication means that **they** prefer.
For many users/programmers out of scrapinghub,
slack is going to be yet another system for communication.

Let's assume a non-IT person wanted to ask ""I am looking for a contractor...""
and didn't because he/she couldn't use IRC.
Would the scrapy community be the right place to ask?
(Isn't the commercial support link on the site obvious?)

Let's assume someone wants to upload a **screenshot of a stacktrace** (?!!)
and doesn't because he/she can't be bothered to upload somewhere else
and post the link to IRC.
Would he/she be the right person to answer to?

I don't want to be rude and I hope my email doesn't read on an aggressive tone :-) .
I just want to respond to the call for feedback made by @curita
and argue that the community has no problem, especially none that would be solved by Slack.
We are mostly programmers doing our work
and I find it very nice to only occupy a tmux window for IRC,
where I can attend various python (& other FOSS) communities
instead of running another tab on the browser,
next to some web applications a client/manager needs me to use. 

P.S. sorry for the long email
",Digenis,curita
1372,2015-07-22 17:14:27,"> > and if Socket.IO has had such tremendous success with a public Slack community
> 
> Let's not make decisions based on assumptions

From the [link](http://rauchg.com/slackin/) @curita posted:

> In less than two weeks, we already had a community of more than 400 users. Not only did it _dramatically_ outpace the growth rate of our IRC server, but it fostered a much stronger sense of community. I’ve already seen a great exchange of technical knowledge, new ideas and even business opportunities for those involved.

Sounds like a tremendous success to me ;). I'd also say the average Socket.IO user is at least as (probably even more) tech-savvy as the average Scrapy user, and apparently switching to Slack has not at all repelled coders or resulted in a degradation of the community to a free tech support team for them.

For your other questions: Yes, I think the Scrapy community is a great place to ask for contractors. And yes, someone who posts screenshots of a stacktrace is definitely the right person to answer to: we've all started with no clue whatsoever at some point. But I guess this boils down us having different ideas of what a healthy community looks like, and that's alright. :)
",jdemaeyer,curita
1372,2015-07-23 01:34:39,"I think any slack vs irc discussion isn't really relevant. It's a different communication platform. The more presence the better, usually. It's not really one versus the other, though some small fracturing of supporter base may result. People will continue to use IRC.
But there are people on Slack which are not on IRC, already, and the question is only whether to provide a ""public channel"" to them.
I'd say go for it, using slackin or building a custom integration looks reasonably easy, though I am unaware what constitutes ""integrate[ing] it to our current work env"".

My intent wasn't to sidetrack this to a discussion pro/contra IRC, just a general expression of sadness that not everyone can like IRC. But I grew up with it, nowadays people grow up with the notion ""Internet==Web"" and having profile pages to manage their account settings, so messaging some bot on IRC may be alien to them. (Though knowing irc has been a skill for life. It's been around since I got my first modem and while other services come and go, irc is likely still around for the rest of mine.)

Assuming those people, who would choose this medium over IRC, are computer-illiterate is bogus. (I'd sure work with @curita anytime, or any of the scrapinghubbers not in `#scrapy` anymore.) And clicking on a [webchat button](http://scrapy.org/community/) is dead simple, being on IRC doesn't confirm a persons tech-savvyness.
Besides, lowering the entry-level communications barrier with curious management-level types is exactly what they should be looking for as a company and a no-obligations hangout is probably a good thing for that.

All that aside there seem to be some [drawbacks](http://kukuruku.co/hub/pm/how-to-share-a-channel-between-slack-teams) to being an invited guest to a slack team -- e.g. losing all the chat history when getting booted off, which wouldn't happen with irc, or even email. Though maybe there is a way around that, similar to an IRC log bot.

But I would give it a try for some casual hanging out. So let me know when I can get an invite or beta-test this integration. You have my mail ;)
",nyov,curita
1372,2015-07-24 12:12:03,"@jdemaeyer 

>  And yes, someone who posts screenshots of a stacktrace is definitely the right person to answer to: we've all started with no clue whatsoever at some point.

I said the opposite

> Let's assume someone wants to upload a screenshot of a stacktrace (?!!)
> **and doesn't** because he/she **can't be bothered** to upload somewhere else

I don't argue that we shouldn't answer. I will.
And also suggest to upload as text to some pastebin.
My point is that **helping** upload images paves paths to laziness
(not ""laziness"" as a programmer's quality).

---

Personally I believe the ""user support boost"" is all about marketing,
using a communication platform with a more fancy/captivating experience.
The rest, ""fondness"", ""fashion"" and ""fear"" are not my kind of arguments.
More valid bug reports, more acceptable patches. Are these an expectation?
Just think of pypy, twisted, django or other successful python projects
who all manage very well with mailing lists and IRC.
",Digenis,jdemaeyer
1371,2015-08-18 22:13:37,"@kmike well if the concurrency is 10 then probably those 9 requests are already active. What can be problematic is the buffer stored on the downloader.
",nramirezuy,kmike
1371,2015-08-19 02:42:28,"@kmike  +1 to use `queuelib.pqueue`.  Sorry, I completely forgot to comment the ticket when we talked about it in Europython.

@nramirezuy The ticket is about the ""buffer"" you mention, precisely to prioritize the [last queue](https://github.com/scrapy/scrapy/blob/280eab241680c93a763a3ef3a9ccd0c257259ca0/scrapy/core/downloader/__init__.py#L28) used to store requests before they become active. 
",dangra,nramirezuy
1371,2015-08-19 02:42:28,"@kmike  +1 to use `queuelib.pqueue`.  Sorry, I completely forgot to comment the ticket when we talked about it in Europython.

@nramirezuy The ticket is about the ""buffer"" you mention, precisely to prioritize the [last queue](https://github.com/scrapy/scrapy/blob/280eab241680c93a763a3ef3a9ccd0c257259ca0/scrapy/core/downloader/__init__.py#L28) used to store requests before they become active. 
",dangra,kmike
1371,2015-08-21 15:54:08,"@dangra I know but how big is this buffer; does it really make that much of a difference ?
",nramirezuy,dangra
1371,2015-08-21 16:44:05,"@nramirezuy yes, it affects e.g. robots.txt middleware or any other middleware which needs to do extra high-priority requests before processing other requests. We (me and @shirk3y) had this issue with a middleware which re-logins if a cookie expires. It gets worse with broad crawls, when a higher concurrency is used (note that a global concurrency matters, not per-slot concurrency limits).
",kmike,nramirezuy
1371,2015-08-21 18:24:08,"@kmike but cookies are set even before we reach the buffer; I'm confused now :confused: 
",nramirezuy,kmike
1371,2015-08-21 18:32:51,"@nramirezuy hm, yes, it seems you're right about cookies. At least it is something worth checking, a good point. But I don't think the question about cookies is relevant here - we should respect priority, and stronger guarantees enable more features (like more robust robots.txt processing).
",kmike,nramirezuy
1365,2015-07-16 13:15:26,"@Granitas @barraponto please check http://doc.scrapy.org/en/latest/topics/commands.html#parse - it does almost everything you need
",chekunkov,barraponto
1360,2015-07-24 02:57:16,"Thanks @nyov 
",nautilus28,nyov
1359,2015-12-04 16:09:55,"Thanks @nyov!
This looks great actually, don't worry too much about that bot -- it's just a (noisy) metric. :)
",eliasdorneles,nyov
1354,2015-08-18 02:50:14,"@olafdietsche do you mind rebasing on top of master branch?  I think the change is good to be merged just want to confirm it is passing tests for py3.
",dangra,olafdietsche
1354,2015-08-21 16:04:40,"I will see, if I can split this into a patch, which allows `<button>` controls as submit buttons and another second patch, which considers disabled submit buttons.

@nramirezuy If you're concerned about disabled buttons, which might be enabled by Javascript or Javascript in general, you will have to consider a lot more than just that. And the HTML standard is clear about disabled elements.
",olafdietsche,nramirezuy
1354,2015-08-31 21:05:47,"@dangra I created a new pull request #1469 with only the first two commits, which contain the test case and patch for button controls. I ran both `tox -e py27` and `tox -e py34` successfully.

When this pull request is merged, I will look at the second part concerning disabled input and button controls.
",olafdietsche,dangra
1352,2015-08-16 07:16:31,"But what about what @barraponto said, should it log a warning for skipped links?
",nyov,barraponto
1345,2015-07-30 19:21:43,"@kmike if we no longer need the jmespath package to support jmespath, then we can declare an empty set of requirements. if we drop the jmespath feature at all, then it's up to the package depending on it to update its dependencies.
",barraponto,kmike
1345,2015-07-30 19:24:06,"@barraponto you're right there are ways to deal with it. But what's an advantage of writing `pip install scrapy[jmespath]` instead of `pip install scrapy jmespath`? 
",kmike,barraponto
1345,2015-08-20 18:43:34,"@kmike We have control over the version installed :)
",nramirezuy,kmike
1344,2015-07-08 00:43:02,"It works, thanks for you help @kmike 
",nautilus28,kmike
1343,2015-07-17 15:52:07,"@curita  I solved this by just using an [`Extension`](https://gist.github.com/nramirezuy/5aba864e0c36986f548f), maybe it helps :smile: 
",nramirezuy,curita
1340,2015-10-16 19:14:28,"@kmike  I created another PR : https://github.com/scrapy/scrapy/pull/1545
",nhuray,kmike
1336,2015-07-28 00:34:55,"@curita Just asking because, traditionally (at least in most GNU programs), command-line options don't need to be in a specific order to work. It's mostly a matter of style :)
",lufte,curita
1336,2015-07-28 20:49:16,"@kmike: Yes I understand that, but I could still use them and pass them in a weird order like `scrapy crawl -t csv spidername -o output1 -o output2 -o output3.xml -t json`. Scrapy would have to check if there aren't more `-t` args than `-o` and if the order makes sense (I think it shouldn't because they are not positional arguments, but otherwise how do I match them?). Removing the `-t` option makes it look a lot cleaner and simpler to check: `scrapy crawl spidername -o output1.csv -o output2.json -o output3.xml`, but it doesn't allow me to use other extensions or no extensions at all.
",lufte,kmike
1330,2015-07-03 12:27:54,"@curita I think the reason it doesn't work for @movingheart is 

> when I run it in scripts, output:
> ('ascii', None, 'cp936')
",kmike,curita
1330,2015-07-03 16:17:33,"I am moved by your(@kmike,@curita) help.I will list detailed environment:
system os: Windows 旗舰版(Ultimate) 32bit
python version: 2.7.9
scrapy: 1.0.1

My tries:
1.Change retry.py source code according your(@kmike) commit can't resolve this question.
2.Setting LOG_ENCODING to cp936 still can't resolve
3.@curita Running (scrapy fetch http://google.com:81) in cmd ,show same info with you.But it can not explain this question.Can you check the code with my follows (you will receive my code) ?

When I run my code by command(scrapy crawl stockuc -a ctime=20150703) in cmd,all shows as follows:

F:\BaiduYunDownload\STOCK_UC>scrapy crawl stockuc -a ctime=20150703
now use ip: 122.224.169.90:8081
now use ip: 163.177.79.5:81
now use ip: 122.224.169.90:8081
now use ip: 163.177.79.5:81
Traceback (most recent call last):
  File ""D:\Python27\lib\logging__init__.py"", line 882, in emit
    stream.write(fs % msg.encode(""UTF-8""))
UnicodeDecodeError: 'utf8' codec can't decode byte 0xd3 in position 279: invalid
 continuation byte
Logged from file retry.py, line 73
now use ip: 163.177.79.5:81
Traceback (most recent call last):
  File ""D:\Python27\lib\logging__init__.py"", line 882, in emit
    stream.write(fs % msg.encode(""UTF-8""))
UnicodeDecodeError: 'utf8' codec can't decode byte 0xd3 in position 279: invalid
 continuation byte
Logged from file retry.py, line 73
now use ip: 122.224.169.90:8081
Traceback (most recent call last):
  File ""D:\Python27\lib\logging__init__.py"", line 882, in emit
    stream.write(fs % msg.encode(""UTF-8""))
UnicodeDecodeError: 'utf8' codec can't decode byte 0xd3 in position 279: invalid
 continuation byte
Logged from file retry.py, line 73
now use ip: 122.224.169.90:8081

Finally,I will send my source code to you(@kmike,@curita). And please keep a secret and delete it after resolved this question.
",movingheart,curita
1330,2015-07-03 16:17:33,"I am moved by your(@kmike,@curita) help.I will list detailed environment:
system os: Windows 旗舰版(Ultimate) 32bit
python version: 2.7.9
scrapy: 1.0.1

My tries:
1.Change retry.py source code according your(@kmike) commit can't resolve this question.
2.Setting LOG_ENCODING to cp936 still can't resolve
3.@curita Running (scrapy fetch http://google.com:81) in cmd ,show same info with you.But it can not explain this question.Can you check the code with my follows (you will receive my code) ?

When I run my code by command(scrapy crawl stockuc -a ctime=20150703) in cmd,all shows as follows:

F:\BaiduYunDownload\STOCK_UC>scrapy crawl stockuc -a ctime=20150703
now use ip: 122.224.169.90:8081
now use ip: 163.177.79.5:81
now use ip: 122.224.169.90:8081
now use ip: 163.177.79.5:81
Traceback (most recent call last):
  File ""D:\Python27\lib\logging__init__.py"", line 882, in emit
    stream.write(fs % msg.encode(""UTF-8""))
UnicodeDecodeError: 'utf8' codec can't decode byte 0xd3 in position 279: invalid
 continuation byte
Logged from file retry.py, line 73
now use ip: 163.177.79.5:81
Traceback (most recent call last):
  File ""D:\Python27\lib\logging__init__.py"", line 882, in emit
    stream.write(fs % msg.encode(""UTF-8""))
UnicodeDecodeError: 'utf8' codec can't decode byte 0xd3 in position 279: invalid
 continuation byte
Logged from file retry.py, line 73
now use ip: 122.224.169.90:8081
Traceback (most recent call last):
  File ""D:\Python27\lib\logging__init__.py"", line 882, in emit
    stream.write(fs % msg.encode(""UTF-8""))
UnicodeDecodeError: 'utf8' codec can't decode byte 0xd3 in position 279: invalid
 continuation byte
Logged from file retry.py, line 73
now use ip: 122.224.169.90:8081

Finally,I will send my source code to you(@kmike,@curita). And please keep a secret and delete it after resolved this question.
",movingheart,kmike
1330,2015-07-04 03:33:15,"Thanks for sending us your project! Don't worry, I just deleted it, I can reproduce the issue now without it but it certainly helped to figure up what was going on.

To reproduce the issue, you have to log the output to a file:



Or, discarding all the env setup, log something with non-ascii chars in an encoding not compatible with utf-8.

Problem here is that the log messages are mixing encodings, changing LOG_ENCODING to either utf-8 or cp936 won't help because both are present.

@movingheart an immediate solution would be to set LOG_ENCODING to `None` in your settings.py file. That way log messages will be written as bytes (unicode seems to be encoded as utf-8 by default) without any prior decoding process. Some symbols may be unreadable because of the mixed encodings, but the content of the file would be the same as appending `2>log.txt` to the command.

@kmike a solution from our side could be to decode the exception message to unicode using the system locale but I'm not sure this is the correct approach, there could be a lot of cases we're missing and users could log things with different encodings in other places. For example, running something like `scrapy fetch http://google.ch --logfile log.txt -s LOG_STDOUT=1` breaks too in a similar manner.

I think something we could do is to set up LOG_ENCODING to `None` by default, I think twisted logging never really used it seeing how it mixes encodings in these test cases :/
",curita,kmike
1330,2015-07-07 15:30:33,"@curita I think exception messages should be ascii-only in Python 2.x; unicode is not really supported for exception messages in Python 2.x. If an error message is not ascii we can't know which encoding does it use. So https://github.com/scrapy/scrapy/pull/1331 starts to look good to me.
",kmike,curita
1329,2015-07-03 03:33:38,"> > What about creating BackendLike objects to control the 2 different behaviours.
> > It may be difficult if implementing the thread logic deviates too much from memusage code, but +1 if possible.

I prefer the BackendLike approach too, @nramirezuy has a good point about reusing middleware code for logging, mail and closing spider.
",dangra,nramirezuy
1328,2015-07-06 06:58:25,"@eliasdorneles  I have a question about the file you gave me:

where is ""bench.Command.crawler_process.crawl"" is defined? It seems that the parent implementation protocol ""ScrapyCommand"" in **init**.py just set it as none.

I cannot understand it.
",yiakwy,eliasdorneles
1327,2015-07-10 05:37:27,"@nramirezuy  Another method is to create a javascript runtime and clicent codes to send task to js runtime to process it. Phantom or v8 engine will help on this topic.
",yiakwy,nramirezuy
1327,2015-09-01 11:08:48,"@barraponto it'd be nice to have a solution which doesn't require building the whole DOM tree, as @nramirezuy suggested. 
",kmike,nramirezuy
1327,2015-09-01 11:08:48,"@barraponto it'd be nice to have a solution which doesn't require building the whole DOM tree, as @nramirezuy suggested. 
",kmike,barraponto
1326,2015-07-01 10:20:48,"Thanks for your comments **@dangra** :+1:  !!!
I know for a complete version of codes, it need to consider too much things. Just like **""Travis CI""** alerts:
 ""Travis CI"" reminds me of ""selenium is not installed in the testing server""

Yes _@intead_ really does the trick, however I beg ur attention on my second **unsuccessful but more twisted like approach** (where I want to add some benchmark notations afterwards). But it seems to be not ""theading safe"" :
everytime the program will hang after having added all callbacks...

As for **Splash** and **ScrapyJS** I will learn their efforts and ideas soon. Thank you ! ^ ^
",yiakwy,dangra
1326,2015-07-29 21:30:44,"@yiakwy: I agree with @dangra - your code is interesting, but it is better to create a separate project for now. If there will be some solution which is obviously good, stable, widely used and better than other solutions we may consider adding it to Scrapy in future, but now is not the right time.
",kmike,dangra
1325,2015-06-30 02:56:21,"@eliasdorneles I beat you by seconds but your answer is much more elaborated than mine :)
",dangra,eliasdorneles
1325,2015-06-30 12:06:11,"@dangra haha, the effects of spending too much time in StackOverflow :D
",eliasdorneles,dangra
1319,2015-06-25 23:18:20,"I added it so the `linkcheck` build doesn't break if there are invalid links, and then `linkfix` can fix those invalid links from `linkcheck` output.

I didn't find a way to ignore the exit code in build dependencies at the time (it ignores it in a per-command basis prefixing that `-` sign), but it wasn't the best way to deal with it. This pull request breaks `linkfix` because of that, @yarikoptic maybe be you could copy the commands needed to run `linkfix` inside its declaration so it doesn't rely on the other builds so we can fix it?
",curita,yarikoptic
1319,2015-06-26 00:36:58,"In none of my environments (sweep of Debian and ubuntu releases, now available from NeuroDebian) I believe build failed due to that. In which environment it failed for you before due to broken links? 

On June 25, 2015 7:18:53 PM EDT, Julia Medina notifications@github.com wrote:

> I added it so the `linkcheck` build doesn't break if there are invalid
> links, and then `linkfix` can fix those invalid links from `linkcheck`
> output.
> 
> I didn't find a way to ignore the exit code in build dependencies at
> the time (it ignores it in a per-command basis prefixing that `-`
> sign), but it wasn't the best way to deal with it. This pull request
> breaks `linkfix` because of that, @yarikoptic maybe be you could copy
> the commands needed to run `linkfix` inside its declaration so it
> doesn't rely on the other builds so we can fix it?
> 
> ---
> 
> Reply to this email directly or view it on GitHub:
> https://github.com/scrapy/scrapy/pull/1319#issuecomment-115427932
",yarikoptic,yarikoptic
1319,2015-06-26 01:32:19,"Gotcha @curita -- what about smth like 5f3e1e9 ?
",yarikoptic,curita
1306,2015-06-26 14:49:43,"Hey @kmike there is Yandex internal URL parsing routine. https://github.com/yandex/balancer/tree/master/util/uri Used and tested in their content system infrastructure. It's dependent on Ragel (https://en.wikipedia.org/wiki/Ragel) for generation of URL parser and encoder. Which is under GNU, but Ragel is only needed during build phase.
",sibiryakov,kmike
1306,2015-11-11 20:24:35,"@Preetwinder nice review! Do you have the benchmarking code? Note that Python urlparse uses [cache](https://hg.python.org/cpython/file/2.7/Lib/urlparse.py#l176) by default, so if you parse the same URL multiple times it won't be actually parsed again and again. It'd be nice to benchmark parsing of different URLs with paths, query arguments and fragments; `http://www.example.com` is too easy.
",kmike,Preetwinder
1306,2015-11-12 13:07:09,"@Digenis I don't know, there is no a PR yet; I opened this issue for discussion, to get more feedback and highlight a problem.
",kmike,Digenis
1306,2015-11-12 13:17:27,"@Preetwinder This list can be useful for your test https://github.com/sibiryakov/general-spider/blob/master/seeds_es_dmoz.txt. The problem with your test is code you're running will benefit from using CPU cache much more than production spider.
",sibiryakov,Preetwinder
1306,2015-11-16 13:59:01,"@Digenis, can you please provide the issue page links to the bugs you mention. Thank you.
",Preetwinder,Digenis
1306,2015-11-16 17:02:58,"@Preetwinder,
#1405, #1403, #998 
See https://github.com/scrapy/scrapy/issues/1403#issuecomment-149194449

> ... domain should be encoded using IDNA, path should be encoded to UTF8 and then escaped to ASCII, query should be encoded to response encoding and then escaped to ASCII - this is how browsers work. 

I asked here because I think the whole solution is not meant to be put in the link extractor.
",Digenis,Preetwinder
1306,2016-01-13 10:10:44,"@Preetwinder nice job, really! I really do value your effort! 
Here are the problems in your testing code:
- use clock(), it's meant for benchmarking, and test it on linux (not windows),
- prevent influence from file system access delays, it's more reliable to read the file contents in memory, and parse them from memory,
- collect time for every operation and calculate arithmetic mean, median (https://en.wikipedia.org/wiki/Average#Median) and 90% percentile,
- try if disabling GC affects running times.

Is it possible to use GURL class directly? Memory allocations for wrapper class could cost something.

Thanks for your work. 
",sibiryakov,Preetwinder
1306,2016-02-01 11:03:58,"@Preetwinder Awesome! So we have about 5x times of difference between native urlparse and Chrome URL parser wrapped with Cython? 5 vs 1.1 mcs per URL, which isn't bad I think. I can't find code for `getAll()` function, could you post the link?

I would like to get back to my question 

> Is it possible to use GURL class directly? Memory allocations for wrapper class could cost something.

Here https://github.com/Preetwinder/gurl-cython/blob/master/pygurl.pyx#L11 you allocate memory for every new URL wrapping object. Can we use GURL directly to avoid that? It probably requires ctypes/manual extension? 

What API, implementation differences GURL and urlparse have? I'm trying to imagine possible problems of adoption of GURL-based URL parsing in Scrapy.

Could you also try Rust/Yandex libraries?

Thanks for your time again.
",sibiryakov,Preetwinder
1306,2016-04-01 09:38:21,"Hey @Preetwinder here is what I've got using Yandex URL parser:
`count 82257, avg 580.3806971 ns, median 507 ns, 90% 682 ns`. 

It's collected using Chromium 80K test set (chosen by you). Major difference is I wrote a C++ application to get a feeling on what we loose in bindings.

So, as you see it's about 3x times faster. Though it's not correct to compare it to your number because they're collected using bindings. Now, I'm going to get your GURL library and test it with the same app.
",sibiryakov,Preetwinder
1306,2016-04-01 11:17:48,"@sibiryakov , what do you mean by ""batch operations on URL parsing""?
",redapple,sibiryakov
1305,2015-11-04 19:15:04,"@chekunkov Scrapy Cloud already implemented settings on [schedule](http://doc.scrapinghub.com/api/jobs.html#jobs-schedule-json). No idea about ScrapyRT. 
I think this is more an issue of the platform than Scrapy, but I agree it has some uses.

If we decide on implementing this:

`Crawler.crawl`:
1. All pipelines `from_crawler`
2. All downloader middlewares `from_crawler`
3. All spider middlewares `from_crawler`
4. All extensions `from_crawler`
5. `spider.update_settings(self.settings)` - notice that in this case it isn't required for `update_settings` to be a `@classmethod`
6. `Spider.from_crawler`
7. `self.settings.freeze()`

I would rather see it implemented on `Spider.from_crawler`, it isn't a lot of change but kinda helps sorting, you can remove the arguments from kwargs before reaching `__init__` if you want to. I also think every component should be allowed to change settings, I would like to be sure that `CookiesMware` is disabled when my `CustomCookiesMware` is enabled.

I don't think the init order really matters, we can supply different settings scopes and sort it from there.
",nramirezuy,chekunkov
1305,2015-11-09 14:38:44,"@chekunkov Besides you can always use standards to define where you want to set settings
",nramirezuy,chekunkov
1305,2016-03-04 16:07:53,"@eLRuLL You're right, it seems that all extensions features can be emulated with middleware if we make a change suggested in this ticket. Hm, but deprecating extension would mean that if you only need to connect signals you have to put it in a middleware or a spider; in spider it can't be enabled via an option; in middleware you need to figure out where to put this middleware (to downloader middlewares? to spider middlewares? what priority to use?). 
",kmike,eLRuLL
1304,2016-11-02 18:47:12,"@kmike great, we will post updates as they come!
",jlbren,kmike
1304,2016-11-02 18:56:23,"@kmike thanks, that is a helpful starting point. We will definitely take you up on posting questions. I am working with a team of 2 other CS grad students so our aim is to try and make significant progress as a semester project. I imagine they will post any questions they encounter as well. 
",jlbren,kmike
1303,2015-06-15 16:19:53,"@kmike once per run or per request ?
",nramirezuy,kmike
1303,2015-06-15 16:21:35,"@nramirezuy once per request. 

In this case it probably doesn't make sense to show how much more data is received because it will be only slightly larger than a limit.
",kmike,nramirezuy
1302,2015-06-15 16:08:39,"@kmike updated and rebased
",eliasdorneles,kmike
1300,2015-06-14 12:42:42,"@eliasdorneles the ""populating the settings"" gives 4 ways to set settings, yet an example for the most generic way imo isn't shown in an example.

Adding key value pairs to `settings.py`

A method for settings for different environments eg. staging and production is not given.

@kmike the `from scrapy.conf import settings` is the only example that worked that I could find. What is the best practise for accessing settings in a spider?

Do we have to do this? Why isn't it just automatically accessible when you import `scrapy`?:


",surfer190,eliasdorneles
1300,2015-06-14 12:42:42,"@eliasdorneles the ""populating the settings"" gives 4 ways to set settings, yet an example for the most generic way imo isn't shown in an example.

Adding key value pairs to `settings.py`

A method for settings for different environments eg. staging and production is not given.

@kmike the `from scrapy.conf import settings` is the only example that worked that I could find. What is the best practise for accessing settings in a spider?

Do we have to do this? Why isn't it just automatically accessible when you import `scrapy`?:


",surfer190,kmike
1300,2015-06-16 07:20:36,"@eliasdorneles It seems that it is not as easy as you say.

I'm getting `AssertionError: Spider not bounded to any crawler`

So you have to assign a `crawler` to the `spider`
",surfer190,eliasdorneles
1300,2015-07-03 04:06:55,"I think @eliasdorneles handled in #1302 all mentioned issues with that doc section - @surfer190, could you review the new documentation in http://scrapy.readthedocs.org/en/master/topics/settings.html and tell us if you find anything missing?
",curita,eliasdorneles
1291,2015-06-10 19:23:40,"@kmike you're right. 
",dangra,kmike
1287,2016-09-21 13:28:27,"Hey @kmike! 👋 Is there a way I can help to speed this up?
",otobrglez,kmike
1281,2015-06-04 00:15:25,"oh wow haha @kmike thanks, what an oversight of mine
",rrshaban,kmike
1276,2015-06-10 09:40:12,"@kmike @Curita I want to ask you a question. Why extensions, middlewares and pipelines are initialized during `Crawler.__init__`, while spider and engine objects are initialized only during `Crawler.crawl`? Does it work like that because we want to preserve an ability to run multiple spiders via the same set of middlewares?

I'm asking this because sometimes I feel like I want to change some crawl settings _after_ spider initialization and initialize middlewares only after that. For example I got request from customer to make it possible to set `CLOSESPIDER_TIMEOUT` based on passed spider argument. Due to `CloseSpider` implementation to support this I need to override it, disable default and set custom extension in settings. If initialization order was 



that task would be as easy as set CLOSESPIDER_TIMEOUT in `custom_settings`
",chekunkov,kmike
1276,2015-06-10 14:14:27,"@chekunkov `-s` doesn't works for you?
ex: `scrapy crawl <spider> -s CLOSESPIDER_TIMEOUT=900`
",nramirezuy,chekunkov
1276,2015-06-10 14:17:49,"@nramirezuy `-s` does work in command line, but doesn't work on dash, API doesn't provide a way to set custom settings during schedule and customer wants to be able to set this setting per crawl.
",chekunkov,nramirezuy
1276,2015-06-12 18:07:28,"@chekunkov: I think you have a point, the order of components initialization is not clear but  it is better to propose changing it in a new ticket. Scrapy 1.0 is pretty much ready, so we can work on your problem for 1.1.
",dangra,chekunkov
1276,2015-06-15 13:56:21,"@chekunkov :+1: 
",kmike,chekunkov
1272,2015-06-03 00:29:24,"Hey @jdemaeyer,

I've read your proposal and the SEP for the first time, so please excuse me if I'm missing some context :)
1. Is it a coincidence that scrapy.Spider implements update_settings method which addons will implement?
2. Addons are utilities to auto-update and check Settings objects? Why do they need Crawler?
3. How can user activate an addon when crawling is started by CrawlerRunner or CrawlerProcess? 
4. How can user enable an addon if there is no Scrapy project?
5. How to enable an addon if `scrapy runspider` is used to start a spider?
6. Why is scrapy.cfg needed? It is confusing to have two config files, and it is an unnecessary boilerplate if you don't have a Scrapy project. 
7. In the SEP it is said that `MiddlewareManager.from_settings()` will be changed to allow python objects instead of class paths. Why should it be specific to MiddlewareManager? (see discussion at https://github.com/scrapy/scrapy/pull/1215).
8. Do you think it is possible to make Scrapy addons feature less framework-ish? In the current proposal  Scrapy calls various AddonManager methods during the standard `scrapy crawl` init sequence, and AddonManager calls various addon methods itself. Can we made it so that user can also use the AddonManger (or individual addon) to update or check settings, without invoking all the `scrapy crawl` machinery? 
9. It looks like the proposed AddonManager does two unrelated things: 
   - it loads addon classes from scrapy.cfg;
   - it uses loaded addons to update settings and check crawler.
   
   IMHO it is better to separate those. Scrapy users who don't use `scrapy crawl` command may prefer to import addon classes themselves, or these classes can be in the same module as CrawlerProcess call and a spider, all in the same self-contained Python script.

// a rant: I think it is usually a bad idea to name somathing ..Manager :) ""Manager"" tells us that a class ""manages"" something. It is quite broad, so 1) often such classes end up doing several unrelated things and 2) it is not clear what class is doing just from the class name - it can do anything. In Scrapy we renamed SpiderManager to SpiderLoader because of that. It turns out this is not only mine [opinion](http://www.slideshare.net/pirhilton/how-to-name-things-the-hardest-problem-in-programming). 
",kmike,jdemaeyer
1272,2015-06-03 13:02:41,"Hey @kmike, thanks for the feedback!

> 1 . Is it a coincidence that scrapy.Spider implements update_settings method which addons will implement?

That is on purpose. `Spider.update_settings()` and the add-on `update_settings()` both do roughly the same, i.e. update settings outside of `settings.py`, albeit with a different level of ""generality""

> 2 . Addons are utilities to auto-update and check Settings objects? Why do they need Crawler?

Add-ons should create a plug-and-play experience for the user. For checking that everything ""Works As Advertised™"", and provide help when it doesn't, they need to know the actual state of the crawler, and not just how its state is supposed to be from the configuration. E.g. an extension listed in `settings['EXTENSIONS']` could have thrown `NotConfigured`. Add-ons can only detect this if they have access to the crawler.

> 3 . How can user activate an addon when crawling is started by CrawlerRunner or CrawlerProcess?

Hm, don't both of these instantiate `Crawler` objects where the add-on manager would then be instantiated and load add-ons? Then again I guess it again comes down to whether a `scrapy.cfg` exists in these use cases (also see answer to 6).

> 4 . How can user enable an addon if there is no Scrapy project?
> 5 . How to enable an addon if scrapy runspider is used to start a spider?

That would not be possible with the current suggested implementation. Just like you cannot have a `settings.py` file or use `scrapyd-deploy` in these cases. It could be made possible if add-on configuration were to live in `settings` using per-spider settings. I'll put the (current) rationale for `scrapy.cfg` below.

> 6 . Why is scrapy.cfg needed? It is confusing to have two config files, and it is an unnecessary boilerplate if you don't have a Scrapy project.

I opted for `scrapy.cfg` for two reasons:
1. Its syntax is more user-friendly for uninitiated users
2. It makes it easy to keep settings local through the ini sections. E.g. something like this:
   
   
   
   is much easier in `scrapy.cfg` than in `settings.py`, and would not have to expose anything into the `settings` object, avoiding possible nameclashes.

I think if done properly, users won't have to touch `settings.py` for any of the shipped add-ons at all (instead they could set, say `HTTPCACHE_DIR`, in the `scrapy.cfg`. We could even tidy up the settings namespace this way). Those users that then need to touch `settings.py` will most probably know what they're doing anyways.

> 7 . In the SEP it is said that MiddlewareManager.from_settings() will be changed to allow python objects instead of class paths. Why should it be specific to MiddlewareManager? (see discussion at #1215).

Uh, I didn't see #1215 before, that looks good. The SEP goes a little further than the PR though: It suggests to allow not only classes, but _instances_. This will allow add-ons to instantiate components and keep settings as local as possible. Objects retrieved from `X = load_object()` are (so far) always assumed to be classes and then instantiated with `y = X()`. That should probably stay that way, and it is a good question where the ""allow instances"" code could live so it is as generic as possible.

> 8 . Do you think it is possible to make Scrapy addons feature less framework-ish? In the current proposal Scrapy calls various AddonManager methods during the standard scrapy crawl init sequence, and AddonManager calls various addon methods itself. Can we made it so that user can also use the AddonManger (or individual addon) to update or check settings, without invoking all the scrapy crawl machinery?

Hm, I'd say it is already quite modular. The user could in principle instantiate their own `AddonManager` and hand it the `.cfg` file. Or they could load an add-on object and call its `update_settings()` method directly.

> 9 . It looks like the proposed AddonManager does two unrelated things:
> - it loads addon classes from scrapy.cfg;
> - it uses loaded addons to update settings and check crawler.
>   IMHO it is better to separate those. Scrapy users who don't use scrapy crawl command may prefer to import addon classes themselves, or these classes can be in the same module as CrawlerProcess call and a spider, all in the same self-contained Python script.

That is a good point. The Manager idea arised from the #591 discussion, but now that I come to think of it splitting it up seems to make more sense. Also, it would avoid having a ""...Manager"" named class :). The loaded add-ons need to live somewhere between loading <-> updating <-> checking, though. With the current SEP, that could be in an attribute of `Crawler` (much like extensions).

Maybe like this?
- An `AddonLoader` class, or even only a function, is handed the `scrapy.cfg` file and provides/returns an `{ 'addon name': (<addon object>, <addon config>) }` dict. The `check_dependency_clashes()` functionality also lives within this class/function.
- The `addons` dict is given to the `Crawler` object upon instantiation, much like `settings`.
- `Crawler` calls the add-ons `update_settings()` and `check_configuration()` functions at appropriate times.
- If the user does not want the `Crawler` to call the callbacks (e.g. because she does it somewhere else), she hands over an empty addons dict on instantiation. In this case, it might be tricky that the `check_configuration()` callback requires the crawler object.

I'm still quite unhappy about the `scrapy.cfg`/`settings.py` issue. It is indeed annoying overhead to have two configuration files. But then, I think it makes sense that add-ons and their configuration are not present in any `Settings` instance, and the `scrapy.cfg` syntax is very much suited for user-friendliness and local settings.
",jdemaeyer,kmike
1272,2015-06-03 18:20:41,"Hi @jdemaeyer,

> That is on purpose. Spider.update_settings() and the add-on update_settings() both do roughly the same, i.e. update settings outside of settings.py, albeit with a different level of ""generality""

Hm, do you think we can use exactly the same code path for Spider then? I.e. maybe spider can implement addon interface itself? I'm not sure when can it be useful, but why not :) This way a spider will be able not only to update settings, but also to check them.

> It makes it easy to keep settings local through the ini sections. E.g. something like this: (... snip ...) is much easier in scrapy.cfg than in settings.py, and would not have to expose anything into the settings object, avoiding possible nameclashes.

I don't think it is a problem to separate sections in settings.py. Django-like approach works:



> Maybe like this? ...

Yeah, your approach looks good to me.

Some questions:

Why are `{ 'addon name': (<addon object>, <addon config>) }` dicts needed, what are keys ('addon names') for? Can it be a list of `(<addon class>, <addon config>)` pairs, or just a list of instantiated addon objects?

> I'm still quite unhappy about the scrapy.cfg/settings.py issue. It is indeed annoying overhead to have two configuration files. But then, I think it makes sense that add-ons and their configuration are not present in any Settings instance, and the scrapy.cfg syntax is very much suited for user-friendliness and local settings.

What's the problem with having ADDONS list in settings.py? It means one less config file and more flexibility - addons will be able to disable or enable or check other addons. This way you also avoid changing Crawler API and other realted APIs - no need to add 'addons' argument. I think that two config files is less user-friendly than one config file. By default settings.py may contain only this ADDONS option.
",kmike,jdemaeyer
1272,2015-06-10 13:14:22,"I've written the add-on interface spec and two add-on loaders, one loading from `scrapy.cfg` and one loading from `settings.py`.

Is there a way I can retrieve the project module name? I could use the first part of `os.env['SCRAPY_SETTINGS_MODULE']` but it seems to me that there should be a cleaner way.

Again, thanks for the feedback @kmike!

> Hm, do you think we can use exactly the same code path for Spider then?

I think that should be possible, but I'm not sure how enforce that the `update_settings()` method is not called twice.

> I don't think it is a problem to separate sections in settings.py. Django-like approach works:

One problem with the current SEP is that the add-on name (= variable name) is allowed to be a python path to the add-on, but python does not allow dots in variable names. I have therefore allowed a `_name` variable in the `settings.py` dicts for now.

> Why are { 'addon name': (<addon object>, <addon config>) } dicts needed, what are keys ('addon names') for? Can it be a list of (<addon class>, <addon config>) pairs, or just a list of instantiated addon objects?

The keys are simply whatever the addon was called in `scrapy.cfg`/`settings.py`. I'm not yet fully sure if we can drop it this early so I'd prefer to leave it in. It cannot be a list of instantiated add-on objects with the current SEP b/c we also allow add-ons to be modules right now (i.e. cannot be instantiated)

I don't really like the idea of exposing add-ons in the global settings, because they are essentially ""one level above it"": Add-ons fiddle with settings and extensions, but never the other way around. That's why I've designed the settings module add-on loader to only load variables starting with `addon_` (in lower case, so they're not picked up by `Settings.setmodule()`) so far. It would be cool to allow add-ons to load/configure other add-ons, though. In that case an add-on manager could become useful again...
",jdemaeyer,kmike
1272,2015-07-14 03:28:23,"I think there are some things to discuss about the design, I took note on some questions and suggestions I had so we can talk them through.

There are some concerns about the usage of the `scrapy.cfg` file to set the addons configuration. I agree here with @kmike that we need to support using the current settings for a couple of reasons:
- I'm not sure if we can maintain the locality of the addons configuration. This could be backward incompatible when scrapy components get ported, there could be addons that depend in other addons config. Not sure if I'm stretching the issue, but at least it was mentioned in #1334 (sharing a `retry` setting with `redirect` middlewares).
- As mentioned earlier in the pr, `scrapy.cfg` can't be the only way of configuring addons because having a project is not required for running scrapy, setting core components is something that should be possible with single spiders.
- We can't remove the already present flexibly of configuring components dynamically via command line or `custom_settings`. 

Both options, using the current settings and the `scrapy.cfg` files could coexist, but if there is any complication keeping both we should favor the first one.

On a related note, I'm not sure it's a good idea to discard `_ENABLED` settings (read about discarding them in the SEP), specially if we're keeping the possibility of dynamic configuration. Users may want to disable a component temporary without deleting or commenting out the whole `.cfg` section.

Some miscellaneous comments in the addons interface:
- Related with what I mentioned before, if we loose settings locality, is it required to instantiate the addons so early in the loading process? Could `update_settings()` and `check_configuration()` be class methods?
- An idea :) There could be more non-required attributes in the zope.interface with general information about the addons, for example, some field with a brief description, a dictionary detailing the settings that the addon reads, maybe a list with some non-required dependencies. This sort of copies some utilities from python packages, I wonder if we can make a specific interface for python packages that grabs these settings (name, version, dependencies, etc.) using setuptools.
- I think we should add a field in the addons (maybe not required?) where the devs could set a fixed number denoting the loading order position for that addon. I know that we have REQUIRES/MODIFIES/PROVIDES to try to address that, but I doubt this is going to be enough to define all cases.
- A detail, is it ok to enforce having a `check_configuration()` method in the addons? I understand `update_settings()` being required, addons' only connection with the crawling engine is by updating the settings, there is no point in not having this method, but enforcing `check_configuration()` seems a little restrictive. We could check in `AddonManager.check_configuration()` if the method exists or not and call it in the first case.
",curita,kmike
1272,2015-07-15 22:53:59,"Hey everyone, thanks again for the feedback (especially for the full review @curita :)), I really appreciate it. Here's my two cents on some of the design issues (once again, caution, wall of text):

##### Add-on configuration entry point

After I had to discuss the add-on management in a blog post a couple of days ago, I have backpedaled a little on my preference. `scrapy.cfg` is a place for those settings that are not necessarily bound to any particular project (if I have multiple projects I probably still want to upload them to the same `scrapyd` server). Add-on configuration _is_ (most commonly) bound to a project, much like other crawling settings, and I agree that it should therefore live inside the project. If we don't want to introduce a new settings file (which I think would be a bad idea), that leaves us with the project settings module.

I see two possible syntaxes for this:
1. Each add-on has its own variable, like it is implemented right now, and is _not_ exposed into the global settings:
   
   
2. There is an `ADDONS` dictionary setting that lives among all other global settings:
   
   

The first approach does not mix settings and add-ons, following the 'add-ons are one level above settings' philosophy I had in mind earlier. However the syntax is not very user-friendly, particularly having to use the `_name` key. The second approach puts add-ons onto the same level as the settings they are intended to fiddle with. I don't see an immediate drawback of this, but it strucks me as odd design. I also find the nested dictionaries, like these used for `logging.config.dictConfig()`, a little tedious. The user experience would be much more cumbersome than what we had initially hoped for. The big big plus is that it would integrate really well with custom Spider settings.

Now the user _might_ want to enable an add-on system-wide, and I think we could keep the support for `scrapy.cfg` add-on configuration for this use case only. However, the docs should emphasise that this configuration does not belong to a project, and is therefore not deployed.

##### Add-ons without a project

I agree that there should be a way to enable and configure add-ons for stand-alone spiders. That would be no problem at all when using syntax 2 from above, so I guess that is what it will boil down to. We could keep syntax 1 if we introduced some new mechanism to enable/configure add-ons from within a spider, but that seems unnecessarily confusing. And frankly, I think syntax 1 is just as ugly, if not a little more, as syntax 2. ;D

##### Local configuration

First I want to point out that the (local) add-on configuration is not inaccessible for other add-ons or components. All add-on configurations are available to all components via the `crawler.addons.configs` dictionary. They just no longer clog up the global settings namespace. That being said, for backwards compatibility we should definitely keep the global settings for all builtin components. The idea I have for the builtin add-ons right now is that they simply take their configuration and expose it into the global settings namespace, much like [in `scrapy.addons.Addon.update_settings()`](https://github.com/scrapy/scrapy/pull/1272/files#diff-3e1ec9ad5dd8cd07504c3d6763322dbdR24). The components themselves won't be touched at all. This would allow users to configure built-in add-ons/component through both the new (add-on config) and old (settings module) way, and keep overwriting component configuration ad-hoc via Spider settings or the command line possible.

##### Add-on callback placement

> Related with what I mentioned before, if we loose settings locality, is it required to instantiate the addons so early in the loading process? Could update_settings() and check_configuration() be class methods?

I'm not quite sure what you mean, aren't the callbacks already class methods in the common use case (i.e. when the add-on interface is provided by a class instance, such as in the `scrapy.addons.Addon` class)? I have used the call to `Spider.update_settings()` as orientation where the add-on callbacks should be placed. An add-on might want to replace the stats class, so I guess one line below where it is right now is where we should call them at latest. Then again, I just realised there's already some settings that cannot be changed in add-ons right now, like the spider loader class. I'm curious what you had in mind?

##### Add-on interface

+1 for dropping the requirement of `check_configuration()`, it is indeed very conceivable that many add-ons won't want to use this callback.

I like the additional attributes for further describing the add-on, and reusing `setuptools`/`distutils`/`packaging` etc., in particular for replacing the clumpy version check function I coded (I wasn't finished, I swear ;), but of course I could've figured that there's got to be a much better implementation ready out there).

As to the additional field for load order, i'm not really sure if that really helps us. Opposed to the current implementation of component orders, the user would have no (easy) influence on the add-on load order. That means there needs to be some kind of communication between devs, or a guide with recommended load order numbers for different use cases. While that might be doable, it seems to me that this will become hard to maintain if full-blown dependency management is introduced at some point.

##### Add-ons configuring other add-ons

I'm just keeping this in here as a reminder to myself to think about it. As I outlined earlier, I think as soon as the first `update_settings()` callback is touched, depending on whether we want to introduce the load order attribute, add-on configuration should be fixed. This only leaves the option of introducing a third (optional) callback, `configure_addons()`, that is called before `update_settings()`.
",jdemaeyer,curita
1272,2015-08-01 18:08:59,"A few small updates/thoughts:

I opted for a new `ExecutionEngine.close()` method instead of updating the `stop()` method. The reason is that I didn't want to remove the `assert` statement made in `ExecutionEngine.stop()` (after all the whole idea was to clean up even if the engine wasn't running yet).
Code: [`close()` method](https://github.com/scrapy/scrapy/pull/1272/files#diff-960cf0590f97e813c9ec6b2b0f197a33R87) and [usage in `Crawler.crawl()`](https://github.com/scrapy/scrapy/pull/1272/files#diff-017ca5ab6671590721d197e95de3cea3L75)
I've also cleaned up the [graceful shutdown test](https://github.com/scrapy/scrapy/pull/1272/files#diff-ad82c3d0394ef19ca24ea9d022470bf7R232).

I've enhanced [base `Addon` class](https://github.com/scrapy/scrapy/pull/1272/files#diff-3e1ec9ad5dd8cd07504c3d6763322dbdR33): It is now much easier to configure single components through the `component`, `component_type`, and `component_key` attributes and the [`export_component` method](https://github.com/scrapy/scrapy/pull/1272/files#diff-3e1ec9ad5dd8cd07504c3d6763322dbdR75). Thank you for the idea @curita, does that look somewhat like what you had in mind? I've also updated the [built-in add-ons](https://github.com/scrapy/scrapy/pull/1272/files#diff-8d70f795b45293b4516d41d5aaa66897R42) to make use of that new feature, and they look much tidier now.

@curita makes a very valid point with how the `ADDONS` dictionary is a usability stepback. I like the Django-like `INSTALLED_ADDONS` approach and have added a corresponding [method](https://github.com/scrapy/scrapy/pull/1272/files#diff-3e1ec9ad5dd8cd07504c3d6763322dbdR330) and [test](https://github.com/scrapy/scrapy/pull/1272/files#diff-6473f345dba5b625efc61b379f1d80ddR214) alongside the old methods. A few thoughts:
- A stumbling block with this approach is that it's not possible to _append_, not overwrite, to the `INSTALLED_ADDONS` setting from the command line; not quite sure how to overcome that...
- Similarly, we somehow need to communicate to users that they should use `append` in their Spider's `update_settings`
- A way out of this could be to allow using both `ADDONS` and `INSTALLED_ADDONS`, however that would result in **three** entry points to the add-on framework. Not very streamlined :/

@kmike mentioned in one of his first posts if it would be possible that Spiders also implement the add-on interface and have their methods called through the same code path. I very much like that idea but so far couldn't wrap my head around a couple of questions: Should we support `config` for spiders and what would it be? Should the Spider be added to the add-on manager? Or should the add-on manager `update_settings()` etc. method have an optional keyword argument where the Spider is passed to call its callbacks?

Now that I've coded around the add-on manager quite a bit, I think @kmike was indeed right in that it's not very useful to save the `used_keys`, I think I'll drop them soon.

I have refactored the [`check_dependency_clashes()` method](https://github.com/scrapy/scrapy/pull/1272/files#diff-3e1ec9ad5dd8cd07504c3d6763322dbdR392) so that it now uses `pkg_resources`. But I wonder if we can make even more use of that package?

Last, I realise that this PR is getting very large (+2,000 lines, phew) and therefore hard to review. However I don't see how it could be split up into smaller chunks. Any ideas how I could make it more clearly laid out?
",jdemaeyer,curita
1272,2015-08-01 18:08:59,"A few small updates/thoughts:

I opted for a new `ExecutionEngine.close()` method instead of updating the `stop()` method. The reason is that I didn't want to remove the `assert` statement made in `ExecutionEngine.stop()` (after all the whole idea was to clean up even if the engine wasn't running yet).
Code: [`close()` method](https://github.com/scrapy/scrapy/pull/1272/files#diff-960cf0590f97e813c9ec6b2b0f197a33R87) and [usage in `Crawler.crawl()`](https://github.com/scrapy/scrapy/pull/1272/files#diff-017ca5ab6671590721d197e95de3cea3L75)
I've also cleaned up the [graceful shutdown test](https://github.com/scrapy/scrapy/pull/1272/files#diff-ad82c3d0394ef19ca24ea9d022470bf7R232).

I've enhanced [base `Addon` class](https://github.com/scrapy/scrapy/pull/1272/files#diff-3e1ec9ad5dd8cd07504c3d6763322dbdR33): It is now much easier to configure single components through the `component`, `component_type`, and `component_key` attributes and the [`export_component` method](https://github.com/scrapy/scrapy/pull/1272/files#diff-3e1ec9ad5dd8cd07504c3d6763322dbdR75). Thank you for the idea @curita, does that look somewhat like what you had in mind? I've also updated the [built-in add-ons](https://github.com/scrapy/scrapy/pull/1272/files#diff-8d70f795b45293b4516d41d5aaa66897R42) to make use of that new feature, and they look much tidier now.

@curita makes a very valid point with how the `ADDONS` dictionary is a usability stepback. I like the Django-like `INSTALLED_ADDONS` approach and have added a corresponding [method](https://github.com/scrapy/scrapy/pull/1272/files#diff-3e1ec9ad5dd8cd07504c3d6763322dbdR330) and [test](https://github.com/scrapy/scrapy/pull/1272/files#diff-6473f345dba5b625efc61b379f1d80ddR214) alongside the old methods. A few thoughts:
- A stumbling block with this approach is that it's not possible to _append_, not overwrite, to the `INSTALLED_ADDONS` setting from the command line; not quite sure how to overcome that...
- Similarly, we somehow need to communicate to users that they should use `append` in their Spider's `update_settings`
- A way out of this could be to allow using both `ADDONS` and `INSTALLED_ADDONS`, however that would result in **three** entry points to the add-on framework. Not very streamlined :/

@kmike mentioned in one of his first posts if it would be possible that Spiders also implement the add-on interface and have their methods called through the same code path. I very much like that idea but so far couldn't wrap my head around a couple of questions: Should we support `config` for spiders and what would it be? Should the Spider be added to the add-on manager? Or should the add-on manager `update_settings()` etc. method have an optional keyword argument where the Spider is passed to call its callbacks?

Now that I've coded around the add-on manager quite a bit, I think @kmike was indeed right in that it's not very useful to save the `used_keys`, I think I'll drop them soon.

I have refactored the [`check_dependency_clashes()` method](https://github.com/scrapy/scrapy/pull/1272/files#diff-3e1ec9ad5dd8cd07504c3d6763322dbdR392) so that it now uses `pkg_resources`. But I wonder if we can make even more use of that package?

Last, I realise that this PR is getting very large (+2,000 lines, phew) and therefore hard to review. However I don't see how it could be split up into smaller chunks. Any ideas how I could make it more clearly laid out?
",jdemaeyer,kmike
1272,2015-08-10 08:50:49,"> I opted for a new ExecutionEngine.close() method instead of updating the stop() method. [...]

Love how clean that solution resulted, if you want you can open a separate pull request with it and its test and we can merge it right away.

> I've enhanced base Addon class: It is now much easier to configure single components through the component, component_type, and component_key attributes and the export_component method. Thank you for the idea @curita, does that look somewhat like what you had in mind?

That was pretty much what I had in mind :) Not sure if I'd have added the abbreviated versions of the `component_type`s though, they seem rather cryptic for the sake of saving keystrokes, I'd prefer users to stick with the fullnames. Maybe I'd have added the component `order` as a class attr to match both `component` and `component_key` placements (in addition of being a config option), but this isn't necessary, and it could be taken the wrong way by thinking the order can't be changed.

> A stumbling block with this approach [INSTALLED_ADDONS] is that it's not possible to append, not overwrite, to the INSTALLED_ADDONS setting from the command line; not quite sure how to overcome that...

That's a prevailing problem with any list valued setting :( I think as long as we can enable or disable add-ons using their configs (not sure if this is true for all add-ons though, maybe we could support a general `enabled` config in AddonManager?) it isn't a big drawback to have a fixed INSTALLED_ADDONS in settings.py listing all add-ons used across the spiders in a project. This is far from ideal (dynamically adding/removing add-ons to the list will be hard) but I think it's a reasonable compromise.

> @kmike mentioned in one of his first posts if it would be possible that Spiders also implement the add-on interface and have their methods called through the same code path.

I don't recall right now any constrain on spiders being add-ons (hm, wait, calling the spider update_settings() at the same time as the other add-ons update_settings() couldn't conflict with the add-ons config? could a spider still disable another add-on this way?), but if there aren't any issues (f8cdf1a?) let's implement it :+1:

> Should we support config for spiders and what would it be?

If possible, I'd like to see spider arguments mapped into the config. A neat side effect of this would be that spider arguments could be configured like any other regular setting.

> Should the Spider be added to the add-on manager? Or should the add-on manager update_settings() etc. method have an optional keyword argument where the Spider is passed to call its callbacks?

The second option is the one you implemented in f8cdf1a right? I like it, I think this distinction should be helpful at some point.

> I have refactored the check_dependency_clashes() method so that it now uses pkg_resources. But I wonder if we can make even more use of that package?

Not sure myself, I thought we could have a custom Addon class for add-ons defined inside packages and get `version` and `requires` from that package using pkg_resources but I'm not sure if that's even something that should be done with it.

> Last, I realise that this PR is getting very large (+2,000 lines, phew) and therefore hard to review. However I don't see how it could be split up into smaller chunks. Any ideas how I could make it more clearly laid out?

Cleaning up the commit history should be a must to get the pr fully reviewed once it's done, but I don't see how its readability could be improved any further. Your code is really well documented so don't mind its extension too much :)
",curita,curita
1272,2015-08-10 08:50:49,"> I opted for a new ExecutionEngine.close() method instead of updating the stop() method. [...]

Love how clean that solution resulted, if you want you can open a separate pull request with it and its test and we can merge it right away.

> I've enhanced base Addon class: It is now much easier to configure single components through the component, component_type, and component_key attributes and the export_component method. Thank you for the idea @curita, does that look somewhat like what you had in mind?

That was pretty much what I had in mind :) Not sure if I'd have added the abbreviated versions of the `component_type`s though, they seem rather cryptic for the sake of saving keystrokes, I'd prefer users to stick with the fullnames. Maybe I'd have added the component `order` as a class attr to match both `component` and `component_key` placements (in addition of being a config option), but this isn't necessary, and it could be taken the wrong way by thinking the order can't be changed.

> A stumbling block with this approach [INSTALLED_ADDONS] is that it's not possible to append, not overwrite, to the INSTALLED_ADDONS setting from the command line; not quite sure how to overcome that...

That's a prevailing problem with any list valued setting :( I think as long as we can enable or disable add-ons using their configs (not sure if this is true for all add-ons though, maybe we could support a general `enabled` config in AddonManager?) it isn't a big drawback to have a fixed INSTALLED_ADDONS in settings.py listing all add-ons used across the spiders in a project. This is far from ideal (dynamically adding/removing add-ons to the list will be hard) but I think it's a reasonable compromise.

> @kmike mentioned in one of his first posts if it would be possible that Spiders also implement the add-on interface and have their methods called through the same code path.

I don't recall right now any constrain on spiders being add-ons (hm, wait, calling the spider update_settings() at the same time as the other add-ons update_settings() couldn't conflict with the add-ons config? could a spider still disable another add-on this way?), but if there aren't any issues (f8cdf1a?) let's implement it :+1:

> Should we support config for spiders and what would it be?

If possible, I'd like to see spider arguments mapped into the config. A neat side effect of this would be that spider arguments could be configured like any other regular setting.

> Should the Spider be added to the add-on manager? Or should the add-on manager update_settings() etc. method have an optional keyword argument where the Spider is passed to call its callbacks?

The second option is the one you implemented in f8cdf1a right? I like it, I think this distinction should be helpful at some point.

> I have refactored the check_dependency_clashes() method so that it now uses pkg_resources. But I wonder if we can make even more use of that package?

Not sure myself, I thought we could have a custom Addon class for add-ons defined inside packages and get `version` and `requires` from that package using pkg_resources but I'm not sure if that's even something that should be done with it.

> Last, I realise that this PR is getting very large (+2,000 lines, phew) and therefore hard to review. However I don't see how it could be split up into smaller chunks. Any ideas how I could make it more clearly laid out?

Cleaning up the commit history should be a must to get the pr fully reviewed once it's done, but I don't see how its readability could be improved any further. Your code is really well documented so don't mind its extension too much :)
",curita,kmike
1272,2015-10-29 14:08:37,"Rebased.

While this is a 'ready' implementation, there are some design questions that should be reconsidered.

**Bold alternatives** mark design changes I have implemented in later commits.

**Major**:

| What | Current Implementation | Alternatives |
| --- | --- | --- |
| Add-on configuration entry point | Preferredly, add-ons are [enabled through a _tuple_ setting, `INSTALLED_ADDONS` (inspired by Django), and configured through an `ADDONNAME` setting](https://github.com/jdemaeyer/scrapy/blob/enhancement/addons/scrapy/addons/__init__.py#L309). Additionally (but not preferred), add-ons can also be [enabled and configured with one section per add-on in `scrapy.cfg`](https://github.com/jdemaeyer/scrapy/blob/enhancement/addons/scrapy/addons/__init__.py#L327). | <ul><li>**Make `INSTALLED_ADDONS` (maybe just `ADDONS`?) a dictionary, so it is consistent with the middleware/extension settings**</li><li>Make configuration through `scrapy.cfg` the preferred way but keep configuration through settings to allow using add-ons outside of projects and ad-hoc configuration in the command line (emphasises the ""add-ons are above settings"" idea)<li>**Completely drop support for `scrapy.cfg` to avoid splitting up the configuration entry point (somewhat opposes the ""supersimple to configure for users"" goal)**</li><li>Drop support for configuration through settings in favour of `scrapy.cfg` (emphasises the 'add-ons are above settings' idea but makes using add-ons outside of projects, and configuring add-ons from the command line, impossible)</li></ul> |

Inbetween:

| What | Current Implementation | Alternatives |
| --- | --- | --- |
| Add-ons enabling/configuring other add-ons | In a [`update_addons()` callback](https://github.com/jdemaeyer/scrapy/blob/enhancement/addons/scrapy/addons/__init__.py#L467) (and only there), add-ons can load/enable and configure other add-ons. They cannot use the `INSTALLED_ADDONS` setting but must go through the add-on manager methods. | <ul><li>Drop support for add-ons configuring other add-ons (renders creating ""umbrella add-ons"" impossible, and makes add-ons slightly less user-friendly because add-ons cannot automatically enable required other add-ons)</li></ul> |
| Callback signatures | The callback signature are quite restrictive to avoid add-on authors misusing them, i.e. the [`update_addons()` callback](https://github.com/jdemaeyer/scrapy/blob/enhancement/addons/scrapy/addons/__init__.py#L467) only receives the add-on manager but not the settings, the [`update_settings()` callback](https://github.com/jdemaeyer/scrapy/blob/enhancement/addons/scrapy/addons/__init__.py#L479) gets only the settings but not the crawler or the add-on manager. | <ul><li>Hand over the crawler to all callbacks to make them more versatile and rely on developers to know what they're doing</li></ul> |
| Add-ons for builtin extensions/middlewares | For every builtin extension/middleware, there is a [corresponding add-on](https://github.com/jdemaeyer/scrapy/blob/enhancement/addons/scrapy/addons/builtins.py). This add-on simply [proxies the configuration into the global settings](https://github.com/jdemaeyer/scrapy/blob/enhancement/addons/scrapy/addons/__init__.py#L103). Users can now do most of the configuration in `scrapy.cfg` through these proxy add-ons if they wish | <ul><li>Drop built-in addons to avoid creating a new configuration entry point for the built-in components</li></ul> |
| Dependency management | Add-ons can provide information about their dependencies through `provides`, `modifies` and `requires` attributes. The [`check_dependency_clashes()` function](https://github.com/jdemaeyer/scrapy/blob/enhancement/addons/scrapy/addons/__init__.py#L352) checks these for clashes with the help of `pkg_resources`. | <ul><li>Make sure that these really are the three attributes we want to use</li><li>Outsource dependency management into a new PR to allow dedicated development/discussion there while moving forward with the add-on framework</li></ul> |

Minor:

| What | Current Implementation | Alternatives |
| --- | --- | --- |
| Add-on manager name | `AddonManager` | I totally see [your point](https://github.com/scrapy/scrapy/pull/1272#issuecomment-108139931) @kmike, I just can't think of a better name ;) the add-on manager is very multi-purpose (enable addons, find addons, provide configuration entry point, check dependencies, call callbacks, ...). Maybe it is too monolithic? |
| Finding add-ons | When passing `httpcache` to the [`AddonManager.get_addon()` method](https://github.com/jdemaeyer/scrapy/blob/enhancement/addons/scrapy/addons/__init__.py#L242), it will first look for a module/object `httpcache` on the Python path, then for `projectname.addons.httpcache`, then for `scrapy.addons.httpcache`. This allows slightly more user-friendly addressing of add-ons. | <ul><li>Drop looking for `projectname.addons.` because the project name can [not always be detected reliably](https://github.com/jdemaeyer/scrapy/blob/enhancement/addons/scrapy/utils/project.py#L75), possibly leading to inconsistent behaviour</li><li>**Drop looking for `projectname.addons.` as well as for `scrapy.addons.`, forcing the user to always write out the full path, consistent with the middlewares and extensions**</li></ul> |
| Interface declaration | The [`scrapy.interfaces.IAddon` zope interface](https://github.com/jdemaeyer/scrapy/blob/enhancement/addons/scrapy/interfaces.py#L25) only requires the existence of a `name` and `version` attribute. | <ul><li>Find out how to enforce the signature of a method without enforcing its existence (add-ons should be able to choose to implement only one or two callbacks)</li></ul> |
",jdemaeyer,kmike
1272,2015-10-31 01:07:35,"A nice table, makes it much easier to understand the PR :+1: 
- _Add-on configuration entry point_ - I think we should go API-first; I don't see a point of having two files with options. INI syntax is a tiny bit nicer, but that's all - the syntax doesn't transfer to Spider.custom_settings or Crawler/CrawlerRunner/CrawlerProcess settings, and it is not clear how to define and access your own settings in scrapy.cfg. So my vote is to drop addons support in scrapy.cfg - as @jdemaeyer said, Scrapy is a framework, not an application. This way we'll have one less concept to learn for users - there are already too many of them in Scrapy.
- _Add-ons enabling/configuring other add-ons_ - an option for addons to configure other addons looks nice, +1 to keep it.
- _Callback signatures_ - I like that addons don't know about a crawler until check_configuration is called. It makes it possible to build an almost final Settings object without a Crawler (?). Are there use cases for passing a Crawler earlier?
- _Add-ons for builtin extensions/middlewares_ - the implementation doesn't look DRY - priorities and default options are currently duplicated (?). I think it makes sense to convert extensions to addons only if they have a use of addon features. Maybe short addon names is enough to justify creating addons for built-in components though. But it should be DRY.
- _Dependency management_ - it may be nice to have, but my vote is to do it later, in another PR. Even better if this feature would be justified by real addons, i.e. we may release without it, then check what's going on in users' `check_configuration` methods (?) and then add helpers for common cases.
- _Add-on manager name_ - What do you think about the following?
  1. rename the class to Addons - it represents a set of addons, not something wich just manages a set of externally maintained addons;
  2. rename `load_dict` to `from_dict` and make it a classmethod (an alternative constructor);
  3. rename `load_settings` to `from_settings` and make it a classmethod (an alternative constructor);
  4. rename `load_cfg` to `from_cfg` and make it a classmethod (one more alternative constructor)
  5. make it possible to construct a merged Addons instance from other Addons instances, to allow populating addons from multiple sources.
  
  (2-5) is very minor, to decrease usage of mutable state.
- _Finding add-ons_ - My vote is to support 1. setuptools entrypoints and 2. python paths. 3. nothing more - no magic prefixes, no local/relative file paths. `myproject.addons.Foo` is better than `Foo` because it clearly shows that addon is a custom one defined in this project. With setuptools entrypoints there is no need for scrapy.addons, and pypi packages can provide shortened addon names. But maybe we don't even need entry points, just Python paths can be fine.
- _Interface declaration_ - sounds fine; I don't care about formal interfaces much though.
- +1 to use ADDONS name instead of INSTALLED_ADDONS. Regarding priorities - hm, why don't they matter?
",kmike,jdemaeyer
1272,2015-11-10 22:32:38,"### Add-on configuration entry point

> Completely dropped support for configuring add-ons in scrapy.cfg

+1

> Replaced INSTALLED_ADDONS tuple with ADDONS dictionary as entry point for add-on configuration

hm, -1 if the only reason of this change is ""so it is consistent with the middleware/extension settings"" 

### Add-ons enabling/configuring other add-ons

Can you point me to the part of doc which describes how this cross-configuration is going to work?

### Callback signatures

generally I like the idea of being more strict, but it should be justified - what could go wrong if we give 

> The callback signature are quite restrictive to avoid add-on authors misusing them

what is understood by misuse? example?

### Add-ons for builtin extensions/middlewares

agree with @kmike https://github.com/scrapy/scrapy/pull/1272#issuecomment-152682302

also one of the _current implementation_ point is irrelevant now as `scrapy.cfg` support is dropped - ""Users can now do most of the configuration in scrapy.cfg through these proxy add-ons if they wish""

### Dependency management

+1 to separate PR, agree with @kmike https://github.com/scrapy/scrapy/pull/1272#issuecomment-152682302

### Add-on manager name

Honestly don't see a big conceptual difference between `AddonsManager` and `Addons` as far as it doesn't change how it works :)

> (2-5) is very minor, to decrease usage of mutable state.

`load_cfg` is already removed, so there are only `load_settings` and `load_dict` to decide. `load_settings` is used once during `Crawler.__init__`. what's expected usage of `load_dict`? I see implementation, what is expected usecase?

### Finding add-ons

> Enabling the http cache now requires ADDONS = {'scrapy.addons.httpcache': 0} instead of ADDONS = {'httpcache': 0}

+1 to remove damn magic

### Interface declaration

@jdemaeyer can you explain what's the problem with current implementation?
",chekunkov,jdemaeyer
1272,2015-11-10 22:32:38,"### Add-on configuration entry point

> Completely dropped support for configuring add-ons in scrapy.cfg

+1

> Replaced INSTALLED_ADDONS tuple with ADDONS dictionary as entry point for add-on configuration

hm, -1 if the only reason of this change is ""so it is consistent with the middleware/extension settings"" 

### Add-ons enabling/configuring other add-ons

Can you point me to the part of doc which describes how this cross-configuration is going to work?

### Callback signatures

generally I like the idea of being more strict, but it should be justified - what could go wrong if we give 

> The callback signature are quite restrictive to avoid add-on authors misusing them

what is understood by misuse? example?

### Add-ons for builtin extensions/middlewares

agree with @kmike https://github.com/scrapy/scrapy/pull/1272#issuecomment-152682302

also one of the _current implementation_ point is irrelevant now as `scrapy.cfg` support is dropped - ""Users can now do most of the configuration in scrapy.cfg through these proxy add-ons if they wish""

### Dependency management

+1 to separate PR, agree with @kmike https://github.com/scrapy/scrapy/pull/1272#issuecomment-152682302

### Add-on manager name

Honestly don't see a big conceptual difference between `AddonsManager` and `Addons` as far as it doesn't change how it works :)

> (2-5) is very minor, to decrease usage of mutable state.

`load_cfg` is already removed, so there are only `load_settings` and `load_dict` to decide. `load_settings` is used once during `Crawler.__init__`. what's expected usage of `load_dict`? I see implementation, what is expected usecase?

### Finding add-ons

> Enabling the http cache now requires ADDONS = {'scrapy.addons.httpcache': 0} instead of ADDONS = {'httpcache': 0}

+1 to remove damn magic

### Interface declaration

@jdemaeyer can you explain what's the problem with current implementation?
",chekunkov,kmike
1271,2015-06-01 15:45:16,"Thanks @allyjweir!
",kmike,allyjweir
1267,2015-06-01 13:49:44,"I agree with @jdemaeyer, this bug came from Scrapy Cloud because were using old paths. They have to simply start using the new ones.
",nramirezuy,jdemaeyer
1267,2015-06-01 15:31:04,"Hate to see a backward incompatible change out of all the module relocations, but it surely makes sense to raise an error if we can't guarantee the same behavior from last stable release, specially if we could have avoided an actual related issue. Mixing paths in the same setting dict is still something that should be uncommon, so users shouldn't get this error too often.

I have updated the pr, what do you think? /cc @jdemaeyer @nramirezuy @dangra
",curita,nramirezuy
1267,2015-06-01 15:31:04,"Hate to see a backward incompatible change out of all the module relocations, but it surely makes sense to raise an error if we can't guarantee the same behavior from last stable release, specially if we could have avoided an actual related issue. Mixing paths in the same setting dict is still something that should be uncommon, so users shouldn't get this error too often.

I have updated the pr, what do you think? /cc @jdemaeyer @nramirezuy @dangra
",curita,jdemaeyer
1267,2015-06-01 20:25:00,"@kmike Sure thing, I've added it and squashed the commits so it's easier to cherry-pick.
",curita,kmike
1265,2015-05-29 15:00:28,"@nramirezuy Yes, it's the same class, but with those settings is going to be loaded twice because of how MiddlewareManager and build_component_list work, none of them check for duplicates.
",curita,nramirezuy
1263,2015-05-29 12:06:19,"@Digenis for me main goal of this PR - make list of enabled middlewares easier to read. I'm not against logging list of middlewares as a json if this will found support from core Scrapy developers:



But on the other side Scrapy logging is for humans, not for robots, that's why I tried to make output more or less pretty, intuitive and easy to read + you can copy-paste such output in a markdown editor and it will be converted to unordered list, it's quite convenient.
",chekunkov,Digenis
1263,2015-05-29 13:32:39,"@chekunkov If you are going to make it so large, why don't you log the whole dict returned by settings, with full path to the classes?

Instead of using json you can use `pprint.pformat`.
",nramirezuy,chekunkov
1263,2015-09-13 15:32:25,"@nramirezuy this information is useful if you're using custom middlewares and trying to debug why aren't they applied and what's applied. If you don't need this (and very often you don't) extra verbosity makes debugging harder because it makes other logs harder to find - you need to skip all this stuff when scrolling back terminal window.

In some cases it is better to always have this verbose information (e.g. in hosted Scrapy environments where it is easy to search logs), but I'd prefer for this extra information to be opt-in, to make Scrapy less scary for people who use it locally for small tasks. :bike: :circus_tent: 
",kmike,nramirezuy
1263,2015-09-14 16:12:30,"so options we have: 

1) make if DEBUG. pros: if you don't need extra verbosity - use appropriate logging level. cons: it doesn't make shelf command less verbose, it prints a lot of info before starting doing something valuable. is that a big problem? It shows configuration only once... And I didn't know that saving number of lines written in terminal before launching interactive shell or starting crawl that usually dumps much much more than a screen of text has ever been an issue :) 

2) we can add some option (`--logconfig`?)to enable this whole block of log messages



and keep it disabled by default. but wouldn't it harm the purpose of such logging? it's there to provide user full information on how crawler is configured

> If you don't need this (and very often you don't) extra verbosity makes debugging harder because it makes other logs harder to find - you need to skip all this stuff when scrolling back terminal window.

well, sometimes user **think** he don't need this logging, but having this logging actually helps to find problems. mail from @dangra I referred in the PR description was about two copies of Autothrottle middleware that were running in the same crawler process and significantly slowed down crawl. problem with old logging was that it wasn't readable at all, just a mix of words that doesn't provide any hint on what middlewares are enabled unless you specifically search some middleware - I think that's why it took so long to detect that problem. I hope this PR made logging more readable and next time finding issues like that will be easier. I you don't see this logging by default and if you don't know you have problems with crawl configuration - it will take more time to detect and resolve issue.

I you still think that should be optional - maybe we can also discuss this option:

3) remove these message from logs and add command which dumps configuration and exits - say `scrapy showconfig`. It would:
- make things consistent - you don't expect this info to appear in your logs, if you need it you call this command and check you config
- in can output nicely formatted json
- hosted Scrapy environment can show output of this command in some tab where user can navigate to check scrapy version and see other configurations - I think it's even more convenient that searching this in logs
- this will reduce number of logs which are fired by Scrapy without spider object in `extra` - and this sounds good and somehow related to this old discussion https://github.com/scrapy/scrapy/pull/1060#issuecomment-88877124 I hope once it would be possible to completely separate logs coming from different spiders in the same process...
",chekunkov,dangra
1255,2015-05-26 15:45:30,"I fully second @kmike opinion 
",dangra,kmike
1255,2015-05-28 12:39:41,"Hey @kmike, @dangra thanks for your comments. Looking at the big picture I see your point about having parsing libraries in separate modules as they are outside the scope of scrapy and need to be interchangeable.

I will study scrapy-selectors a bit more and try to propose ideas to refactor so that external dependencies are easily expanded and bridged from a central location.

I think right after scrapy-selectors are refactored, it would be easier to provide more optional shortcuts like WebTest in scrapy.

It would be great if you put this module under scrapy or separate organization if it makes more sense, with the name “markupquery” maybe? I like the syntax change you proposed it makes the code more readable. I’ve also seen some ideas in pymongo api that I intend to add to make queries more flexible.
",stphivos,dangra
1255,2015-05-28 12:39:41,"Hey @kmike, @dangra thanks for your comments. Looking at the big picture I see your point about having parsing libraries in separate modules as they are outside the scope of scrapy and need to be interchangeable.

I will study scrapy-selectors a bit more and try to propose ideas to refactor so that external dependencies are easily expanded and bridged from a central location.

I think right after scrapy-selectors are refactored, it would be easier to provide more optional shortcuts like WebTest in scrapy.

It would be great if you put this module under scrapy or separate organization if it makes more sense, with the name “markupquery” maybe? I like the syntax change you proposed it makes the code more readable. I’ve also seen some ideas in pymongo api that I intend to add to make queries more flexible.
",stphivos,kmike
1251,2015-05-28 09:53:33,"@nramirezuy Do you mean http requests? Server: 154 packets per second (tops). On my mac more than 400 packages per second if I'm right.
",ErikvdVen,nramirezuy
1240,2015-05-18 15:37:39,"thanks, @kmike :)
",eliasdorneles,kmike
1238,2015-05-15 15:23:12,"Hi @kmike ,
I tried to execute the command ""scrapy runspider stackoverflow_spider.py -o top-stackoverflow-questions.json"" , directly, without creating any project to store the python script for spider.

It gave an error ofcourse, and since being new to Scrapy, it took me alot of time to realise that it wasn't working because we first need to create a project using ""startproject"" command and define the spider there.
",preetis19,kmike
1238,2015-05-15 16:45:45,"@kmike  @dangra  I'm sorry, the link for Scrapy at Glance is http://doc.scrapy.org/en/0.24/intro/overview.html , I gave the wrong link with wrong command.

the command is ""scrapy crawl mininova -o scraped_data.json"" .

Now the crawl commands needs the project to be created which is not mentioned on the page.
",preetis19,dangra
1238,2015-05-15 16:45:45,"@kmike  @dangra  I'm sorry, the link for Scrapy at Glance is http://doc.scrapy.org/en/0.24/intro/overview.html , I gave the wrong link with wrong command.

the command is ""scrapy crawl mininova -o scraped_data.json"" .

Now the crawl commands needs the project to be created which is not mentioned on the page.
",preetis19,kmike
1238,2015-05-15 16:59:11,"@dangra ok,
but even the latest document ""http://doc.scrapy.org/en/latest/intro/overview.html"" , has the same content as that of version 0.24.
",preetis19,dangra
1229,2015-05-14 14:32:48,"@dangra I added a log without boto installed and have no pipelines enabled.
",nramirezuy,dangra
1228,2015-05-13 18:14:47,"@dangra That works?
",nramirezuy,dangra
1225,2016-12-07 19:37:26,@kmike you beat me to it. again :),immerrr,kmike
1220,2015-05-12 01:13:12,"@pablohoffman done!
",eliasdorneles,pablohoffman
1220,2015-05-13 17:43:34,"good job @eliasdorneles !
",pablohoffman,eliasdorneles
1217,2015-06-23 09:07:17,"The fix was merged already. Closing this issue. @nramirezuy 
",MojoJolo,nramirezuy
1215,2015-07-16 06:47:04,"@barraponto: It's not about from where we update `DOWNLOADER_MIDDLEWARES` (and similar), but about what what we can put in there. `Spider.custom_settings` allows us to set some middlewares paths (!) on a per-spider basis, but it doesn't allow us to pass a class/object instead of a string.
",jdemaeyer,barraponto
1215,2015-07-16 13:05:14,"@jdemaeyer oh. can we have a better issue title, then?
",barraponto,jdemaeyer
1215,2015-08-17 17:02:57,"@barraponto better now?
",nramirezuy,barraponto
1215,2015-08-21 15:39:09,"@jdemaeyer What are we supposed to look at? Passing instances instead of classes?
",nramirezuy,jdemaeyer
1215,2015-08-24 16:35:04,"@kmike I guess the use case can be that one; a component not tied to a crawler. But this is usable for ""`lib` `mode`"".
",nramirezuy,kmike
1215,2015-08-24 16:48:23,"@jdemaeyer sorry, I don't quite get it. So is passing an instance a workaround for having classes/class paths as dict keys in various Scrapy settings? 
",kmike,jdemaeyer
1215,2015-08-26 16:10:55,"@jdemaeyer  I don't know how much do I like that feature; since those components will have limited access. Those wont be able to connect to `signals` or write `stats` for example.

You can probably hack around it using spider as a bridge; but still isn't clear.
",nramirezuy,jdemaeyer
1214,2015-08-24 10:48:41,"@kmike I will take care of it today.
",rgtk,kmike
1214,2015-08-24 10:52:38,"@rgtk thanks! Sorry for not providing an answer for your question about where to put this function; creating a new module just for it doesn't seem to worth it, but I'm fine with any location.
",kmike,rgtk
1214,2015-08-27 22:58:03,"@rgtk sorry for the delay in approving this fix, we could have addressed our code reviews after merging :( thanks for your work!
",curita,rgtk
1212,2015-05-06 21:14:30,"@Digenis - a good catch about rstrip, +1 to fix it.

I'm not a fan of mock tests though - they are unreadable, and they may require a rewrite if the internal implementation changes. But some tests may be better than no tests :)

Maybe we should just switch to https://github.com/audreyr/cookiecutter to reduce maintenance burden. It is well-supported, provide more features and already works in Python 3.
",kmike,Digenis
1202,2015-05-05 13:53:57,"@nramirezuy `Link` supports `fragment` but the `LinkExtractor`does not extract the possible fragment and thus does not instantiate `Link`objects with fragment. What's more, even though `Link` had not `fragment`property, it would still support fragment since it would appear in the url. Here is why fragments are dropped: https://github.com/scrapy/scrapy/blob/master/scrapy/linkextractor.py#L95

Calling `canonicalize_url` without `keep_fragment=True` drops the fragment
",Djayb6,nramirezuy
1202,2015-05-14 11:40:48,"@nramirezuy any update ?
",Djayb6,nramirezuy
1200,2015-05-05 16:24:50,"I agree with @eliasdorneles - with modules relocation we're simplifying import paths which users use while not breaking anything; with doc files relocation we don't simplify anything for the user, but break existing links to the docs.
",kmike,eliasdorneles
1192,2015-05-01 04:20:44,"@nramirezuy Hello, here is one real world case containing `<a href=""http://gostariadefazerinscriçãoposcursos,obrigada."">A link</a>` :
http://medicatriz.com.br/manual-tecnico-profissional-medicatriz/

I found out what's happening: I use `LxmlLinkExtractor` like so: 



It happens that in my case all domains in allowed_domains are `unicode` strings, so when the link extractor checks whether the url belongs to any domain with the `url_is_from_any_domain` function, the call to `endswith` on `host` (<a href=""https://github.com/scrapy/scrapy/blob/master/scrapy/utils/url.py#L23"">utils.url line 23</a>) implicitly decodes `http://gostariadefazerinscriçãoposcursos,obrigada.` with the ASCII codec, thus producing the UnicodeDecodeError.
",Djayb6,nramirezuy
1192,2015-05-01 16:56:07,"@nramirezuy Do you want me to produce a Pull Request ? Just applying `unicode_to_str` to each domain when iterating on domains and when it is compared to the host should be enough.
",Djayb6,nramirezuy
1192,2015-05-04 16:52:55,"@nramirezuy I tried with both 0.24.6 and 0.25.1. It raises an exception when the domain is an unicode string. You can reproduce it in scrapy shell with the following code:


",Djayb6,nramirezuy
1183,2016-01-27 13:01:14,"@nashuiliang , I am closing the issue as you have not provided sample input requested by @dangra .

If you are still experiencing the issue, please provide enough information to reproduce the bug.
Please refer to http://doc.scrapy.org/en/stable/contributing.html#reporting-bugs
Thanks.
",redapple,dangra
1181,2015-04-28 18:15:29,"@nyov Thanks for the review! :)

I fully agree with your comments, I didn't mind the shortened filenames except for statscols.py, when I renamed it to plural it sounded weirder. I'm going to start implementing those changes in separated commits so we can revert them if we decide otherwise.
",curita,nyov
1180,2015-04-21 14:02:35,"@kmike +1, I added some examples and attended your other comments.

Let me know if you think of another thing. =)
",eliasdorneles,kmike
1178,2015-04-19 18:18:38,"@kmike Yup it's same issue
",jbinfo,kmike
1178,2015-04-19 18:30:47,"I will check it and back to you, thank you @kmike :)
",jbinfo,kmike
1177,2015-04-20 23:21:58,"@kmike I agree, I removed it in latest commit. Probably we should remove `topics/webservice` from the index as well.
",curita,kmike
1172,2015-04-19 16:41:35,"Thanks @bagratte!
",kmike,bagratte
1169,2015-04-17 22:52:23,"thanks @bagratte!  /cc @eliasdorneles 
",pablohoffman,bagratte
1160,2015-04-15 06:51:55,"@pablohoffman @Curita sphinx_rtd_theme is installed automaticaly when you install recent sphinx (1.3 works).
",kmike,pablohoffman
1160,2015-04-15 16:58:45,"@kmike great, didn't know that!
",pablohoffman,kmike
1157,2015-12-28 14:46:42,"Hi @kmike 
I'd like to start to contribute with scrapy and I'm trying to work on this issue.
I've some questions about it:
- in class names that ends with `Spider`, should the default name contains the Spider? eg. `class StackOverflowSpider(scrapy.Spider):` -> should I name it as `StackOverflowSpider` or `StackOverflow` ?
- if we use a property or add some code on `__init__` to create a default name, we need to instantiate this object to get the default value. Some classes (https://github.com/scrapy/scrapy/blob/master/scrapy/spiderloader.py#L25, https://github.com/scrapy/scrapy/blob/master/scrapy%2Fcommands%2Fcheck.py#L78) checks the spider name. So to get the default value working, it's necessary to instantiate the class in these files. What do you think about it? Can I create a `spcls_instance = spcls()` to get the default name?
",aron-bordin,kmike
1157,2015-12-28 15:19:21,"Hey @aron-bordin, thanks for offering help! 

> in class names that ends with Spider, should the default name contains the Spider? eg. class StackOverflowSpider(scrapy.Spider): -> should I name it as StackOverflowSpider or StackOverflow?

I see what you're heading to, but I think we should preserve the name as-is.

> if we use a property or add some code on **init** to create a default name, we need to instantiate this object to get the default value. Some classes (https://github.com/scrapy/scrapy/blob/master/scrapy/spiderloader.py#L25, https://github.com/scrapy/scrapy/blob/master/scrapy%2Fcommands%2Fcheck.py#L78) checks the spider name. So to get the default value working, it's necessary to instantiate the class in these files. What do you think about it? Can I create a spcls_instance = spcls() to get the default name?

We shouldn't call `__init__` and create Spider instances just to get their names; there are many problems with calling `__init__` - some spiders may have required attributes, other spiders may do heavy work in `__init__` or create/delete some resources. `__init__` is called after Crawler instance is created, and spiders may rely on it. 

This means `@property` on object methods won't work; to make magic `name` work on class you can define a metaclass. Another option (which I prefer more - it is less magical and easier to support) is to create and use a `Spider.get_spider_name` classmethod which returns spider name either from .name attribute or from class name.
",kmike,aron-bordin
1157,2016-02-14 16:43:31,"Is there actually any greater value in this Spider-class `name` attribute?
Does it make sense to put this emphasis on it, instead of simply using the Spider-class name itself?
If, as @kmike says, there is only `SpiderLoader` using it, that code looks straightforward enough to substitute looking up the Spider-class name instead (or simply pulling in all `[Base]Spider` subclasses).

The only interesting case here seems to be having different _classes_ with the _same_ name attribute? What's the defined behavior there, and could it be replicated by a user using python class inheritance rules instead?
In which case I would downgrade this into an extension/addon feature, for whom may care, instead of trading in _more_ magic for _less_ importance ;)
",nyov,kmike
1151,2015-04-13 13:33:40,"Thanks for the tests @marven!

As discussed in #994, we should add `scrapy/contrib/downloadermiddleware/httpcache.py` to `tests/py3-ignores.txt` (and probably `tests/test_downloadermiddleware_httpcache.py` too if not already there), that's why the travis build is failing.

 A nitpick, can we rebase @jameysharp commits in this PR instead of merging them, so we can avoid the merge commit ce38129? Merge commits in Scrapy right now are only done when merging PRs to `scrapy:master`.
",curita,marven
1151,2015-06-01 10:34:10,"@dangra, I've rebased the commits
",marven,dangra
1150,2015-04-15 14:31:26,"Thank you for the review, @dangra -- I will update this soon.

@Curita +1, I thought about it too but decided to do it as the last thing to make it easier to see the diff in the review. :)
",eliasdorneles,dangra
1148,2015-04-17 07:24:42,"@kmike pointed me into this issue. Looks like my problem is related.

I'm having an error and getting `The find_by_request attribute was not provided.` message when executing `scrapy crawl` or even `scrapy version`.
",MojoJolo,kmike
1146,2015-04-23 15:42:37,"@kmike 

>  For me it looks like for script usage CrawlerProcess is a better fit, and CrawlerRunner is better if you have some Twisted-based service and want to run Scrapy in the same reactor

ScrapyRT uses CrawlerProcess to run multiple crawlers in the same reactor with Twisted server - it reuses code from `__init__` and `_stop_logging`. But probably you are right and CrawlerRunner is a better fit - I should review the code as Scrapy core changed a bit since ScrapyRT core was developed.
",chekunkov,kmike
1145,2015-04-13 14:04:32,"@bosnj Simple test plz :+1: 
",nramirezuy,bosnj
1141,2015-04-14 18:53:30,"I have the same question as @nramirezuy.
",kmike,nramirezuy
1140,2015-04-06 18:23:05,"@nyov have you tried to use a property for `self._parser`, that way rewriting `self.request()` is not required and lot of imports can be removed.

I have contradicted feelings about this issue and its possible fixes, monkeypatching is never recommended although we have used it before with Twisted classes that are deep  buried; in the other side going trough the classes is more clean except because we end up relying on patching a private attribute to do the magic.
",dangra,nyov
1140,2015-04-07 00:38:04,"@nyov 
1. I added a test case in https://github.com/dangra/scrapy/commit/4e5d61a72629e74bbcd7913c1ddd961c8684e1df, please pull.
2. As for _property_ solution I was thinking in the lines of https://github.com/dangra/scrapy/commit/c12496edc308cb0d6c152c83c42bedb89f21efd8 but the bad news is that _HTTPClientProtocol_ is an old style class and properties doesn't work there. :disappointed: 



Not sure what other maintainers think but Instead of adding all this lines of duplicated code now I tend to prefer monkeypatching HTTPClientParser directly.
",dangra,nyov
1139,2015-07-30 13:18:12,"oh....   thx. for my friends. @dangra  @tycho01 
 i had the simillar question with u .   i baidu  two days ,no hints to follow .. 
outside spider project , that worked fine.     
",liihu1n,dangra
1138,2015-04-08 10:47:43,"@kmike Right now all the callbacks have just `(self, response)`; when you approach a `Spider` from another dev you can do a quick pane recognizing those ones, but is minimal and not relevant.

What do you think about:


",nramirezuy,kmike
1138,2015-04-08 11:01:56,"> @kmike Right now all the callbacks have just (self, response); when you approach a Spider from another dev you can do a quick pane recognizing those ones, but is minimal and not relevant.

All the callbacks are `def foo(self, response)`, but not all `def foo(self, response)` are callbacks, so I don't think this matters. `parse_foo` names could be a stronger indicator, but it is also only a convention.

As for syntax, I like `kwargs={}` more (less nested braces + consistent argument types - callback is always a callable), but that's bike-shedding; `callback=(self.parse_page2, {'item': item})` could also work. Not sure it worths it to care about *args.

`callback=(self.parse_page2, {'item': item})` looks similar to `callback=partial(self.parse_page2, item=item)`.
",kmike,kmike
1138,2015-04-16 19:45:14,"@kmike functools.partial can be pickled, but instance methods cannot. Pickling e.g. `Request(callback=partial(self.parse_page, extra_arg=25))` will fail with ""can't pickle instancemethod objects"".
This is why `scrapy.utils.reqser` exists, not?
",ArturGaspar,kmike
1138,2015-04-16 20:07:08,"@ArturGaspar you're right! 
I should check my sources better :)
So functools.partial is not an option.
",kmike,ArturGaspar
1138,2016-12-12 07:11:21,"I came across this issue while searching for passing arguments to callbacks. @kmike proposal seems interesting as it leads to more readable code.

I see that the thread basically dried, almost 2 years, ago, receiving only short hydration 210 days ago (in engineering world, exactly 7 months ago).

Hence, my question, is there any progress/traction on this? Or has this thread became a zombie haunting the issue page? Would be good for either the status page to be updated or feature implemented 🎱 ;).",jhirniak,kmike
1138,2016-12-12 09:19:16,"@redapple, thanks, I'll give it a go, if I find some time later.

> what do you mean by ?
> > Would be good for either the status page to be updated

What I meant was: ""the status was open, so long that it could be decided not just to be implemented, hence it would be good to close it"". From what you wrote, I understand why it was left open.",jhirniak,redapple
1137,2015-04-09 21:11:39,"Hi @DharmeshPandav, can you add a test?. Also, this seems to have broken the build - can you check Travis?
",pablohoffman,DharmeshPandav
1137,2015-04-12 05:48:28,"HI @pablohoffman  ..yes there was some issue with travis ..I have missed one variable in signature of function _get_form ...adding it and pushing the changes...travis is in progress

building test is not required..as test for this object is already defined
",DharmeshPandav,pablohoffman
1137,2015-04-15 03:52:54,"As per [Mozilla docs](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/form#attr-name) `id` in HTML5 is the successor of `name` attribute from HTML4, so +1 to some way of shortcut for it.

@DharmeshPandav where is the test you mention? the function argument is new so it need a test case IMO.
",dangra,DharmeshPandav
1137,2015-04-15 05:58:17,"@dangra  , @pablohoffman  :  Yes i agree :+1:  , added four unit tests for this new form attribute support- formid
1. test_from_response_formid_exists
2. ~~test_from_response_formid_notexists_fallback_formname~~
3. test_from_response_formname_notexists_fallback_formid
4. test_from_response_formid_notexist
5. test_from_response_formid_errors_formnumber
",DharmeshPandav,dangra
1137,2015-04-15 05:58:17,"@dangra  , @pablohoffman  :  Yes i agree :+1:  , added four unit tests for this new form attribute support- formid
1. test_from_response_formid_exists
2. ~~test_from_response_formid_notexists_fallback_formname~~
3. test_from_response_formname_notexists_fallback_formid
4. test_from_response_formid_notexist
5. test_from_response_formid_errors_formnumber
",DharmeshPandav,pablohoffman
1136,2015-05-28 19:10:51,"@DharmeshPandav  You can also write the XPath as `id(""formid"")` to be shorter.
",ArturGaspar,DharmeshPandav
1136,2016-01-27 12:38:07,"Hi @DharmeshPandav ,
are you ok with closing this issue now that #1382 is merged?
",redapple,DharmeshPandav
1136,2016-09-14 14:43:48,"Hi @DharmeshPandav , are you ok with closing this issue?
",redapple,DharmeshPandav
1136,2016-10-28 11:24:51,"@DharmeshPandav , I'm closing the issue now that [`FormRequest.from_response` has `formcss` argument](https://docs.scrapy.org/en/latest/topics/request-response.html#scrapy.http.FormRequest.from_response).
",redapple,DharmeshPandav
1134,2015-04-07 17:34:45,"Thanks @nyov, the PR looks good to me. 
+1 to merge it after https://github.com/scrapy/scrapy/pull/963.
",kmike,nyov
1132,2015-04-02 19:09:30,"@nyov sorry, we discussed a list vs a single value in a private chat. The problem with list is that from Scrapy user point of view it is meaningless - ""provide a list and we'll sum up the values"" is a strange API. As @sibiryakov said, we can always add another setting in future, or just allow a list of values as well as a single value for this option. 

This PR (like many others) is not marked as [Scrapy 1.0](https://github.com/scrapy/scrapy/milestones/Scrapy%201.0), so merging it is not a requirement for 1.0 release. It just happens that when you start paying more attention to PRs more PRs begin to appear :)
",kmike,nyov
1132,2015-04-02 19:09:30,"@nyov sorry, we discussed a list vs a single value in a private chat. The problem with list is that from Scrapy user point of view it is meaningless - ""provide a list and we'll sum up the values"" is a strange API. As @sibiryakov said, we can always add another setting in future, or just allow a list of values as well as a single value for this option. 

This PR (like many others) is not marked as [Scrapy 1.0](https://github.com/scrapy/scrapy/milestones/Scrapy%201.0), so merging it is not a requirement for 1.0 release. It just happens that when you start paying more attention to PRs more PRs begin to appear :)
",kmike,sibiryakov
1131,2015-04-08 15:53:26,"I'm happy to merge this as it currently stands, but I want to give a chance to @torymur to add tests until tomorrow. (robots middleware has tests in `tests/test_downloadermiddleware_robotstxt.py`)
",pablohoffman,torymur
1126,2015-07-13 14:13:30,"@curita I tried installing Scrapy in a virtualenv and all works well without a problem. All the dependencies are installed properly.

We haven't heard from @linus-young lately. I wonder if he successfully installed Scrapy.
",MojoJolo,curita
1126,2015-07-27 05:18:00,"Looks like it. Check out @curita's reply.

@curita, is it possible to close this already?
",MojoJolo,curita
1124,2015-03-31 20:38:10,"+1 to @nyov suggestions.

I'd link http://doc.scrapinghub.com/scrapy-cloud.html#deploying-a-scrapy-spider and maybe mention that you are deploying to ""Scrapy Cloud"" specifically, though I'm not sure if this could be confusing. 

Maybe we could briefly introduce (or just link) this other tool for deploying: https://github.com/TeamHG-Memex/scrapy-dockerhub, that is not related to scrapyd. Maybe @shirk3y can help us with that.

/cc @eliasdorneles 
",curita,nyov
1120,2015-04-02 13:27:11,"@kmike It's implemented by means of IReactorTime.callLater which has float support.
",sibiryakov,kmike
1118,2015-03-30 17:02:18,"@nyov , was a [WIP] when my first commit failed miserably...

Oh sorry @nramirezuy , I totally missed yours.
I'm fine with any, as long as I can revive https://github.com/scrapy/scrapy/pull/303

I'm not particularly fond of what's being discussed in https://github.com/scrapy/scrapy/issues/356 and https://github.com/scrapy/scrapy/pull/1048
",redapple,nyov
1118,2015-03-30 17:02:18,"@nyov , was a [WIP] when my first commit failed miserably...

Oh sorry @nramirezuy , I totally missed yours.
I'm fine with any, as long as I can revive https://github.com/scrapy/scrapy/pull/303

I'm not particularly fond of what's being discussed in https://github.com/scrapy/scrapy/issues/356 and https://github.com/scrapy/scrapy/pull/1048
",redapple,nramirezuy
1112,2015-03-28 00:04:18,"@nyov thank you!
I've applied all but the change in `its`, which I think it's the correct usage there. ;)
",eliasdorneles,nyov
1111,2015-03-27 16:35:50,"@nramirezuy the note about dynamic items creation is removed in https://github.com/scrapy/scrapy/pull/1081
",kmike,nramirezuy
1106,2015-03-26 15:31:13,"@eliasdorneles a good overview, I like it :+1: 

I'm trying to attack it from a position of a person who can hack together a spider using requests + concurrent.futures + pyquery + json. Please don't take it as a criticism :) Why should such person bother with Scrapy?
",kmike,eliasdorneles
1106,2015-03-26 16:11:52,"Don't worry @kmike, I appreciate your feedback a good deal, always good points! :)
",eliasdorneles,kmike
1106,2015-03-26 17:27:40,"Hey @kmike -- I've just updated addressing your concerns and did some more editing.
Can you please have a look again?
Thank you!
",eliasdorneles,kmike
1106,2015-03-27 20:11:31,"Hey @nyov -- since this is already merged, please feel free to send a PR to fix those commas or whatever. =)
",eliasdorneles,nyov
1104,2015-03-25 04:49:35,"@sibiryakov, I didn't mean to say scrapy should roll out with a resolver that circumvents host's facilities, sorry if it came across like that. That was just some personal pride I guess.
The `twisted.names` approach could allow passing nameserver addresses directly to be used by the resolver, and I think supporting it would be nice, but yes it should be an optional feature.

Not sure what you mean with working with ""IPv4 out of the box"", I think it also should work with IPv6-only stacks out of the box - which is why I wrote this code.
It's working exactly like the currently used [ThreadedResolver](https://github.com/twisted/twisted/blob/trunk/twisted/internet/base.py), only replacing it with a `getaddrinfo` call.
So it's useable as a drop-in replacement and the risk that it behaves differently from the current implementation is minimal, where as a `twisted.names` approach would be different and require more testing.
Using `getaddrinfo` is also superior for IPv4-only setups, as it can return multiple IPs in a single call and with some code-changes we could load-balance requests across different IPs for a given domain or service.
",nyov,sibiryakov
1104,2015-03-31 15:20:16,"Given the problems with twisted.names @sibiryakov found in https://github.com/scrapy/scrapy/pull/1092#issuecomment-86466566 this PR started to look appealing. It is not a wasted effort!
",kmike,sibiryakov
1104,2016-01-07 11:50:05,"Thanks @nyov !
@sibiryakov , @kmike ,
how does the PR look for you?
",redapple,nyov
1104,2016-01-07 11:50:05,"Thanks @nyov !
@sibiryakov , @kmike ,
how does the PR look for you?
",redapple,kmike
1104,2016-01-07 11:50:05,"Thanks @nyov !
@sibiryakov , @kmike ,
how does the PR look for you?
",redapple,sibiryakov
1104,2016-01-18 19:34:43,"@redapple the code looks good, but I haven't tried it yet. 
",kmike,redapple
1104,2016-03-14 23:18:17,"Thank you for the info, @glyph.
You are probably right that this is a problem which shouldn't necessarily be solved in Scrapy itself.
From what @kmike mentioned, it seemed a solution using `twisted.names` was being investigated? `HostnameEndpoint` features look interesting, but `endpointForURI` seems rather new itself, from twisted 15.0.

I'm not sure what's the best way to implement this here. My understanding of the twisted library is, admittedly, limited, and not having the time to investigate properly. I hope someone else here will look into your suggestions!

This code was written from ""necessity"", as usual. It seemed good enough, replicating things exactly like twisted did with the `gethostbyname` resolution, just replacing the function lookup to use `getaddrinfo`. From that perspective it wasn't doing things any _worse_ than the code before. Though that was before adding the IPv6 configurable stuff.

This code was for myself initially, after becoming aware of the ""GHOST"" vulnerability ([CVE-2015-0235](https://security-tracker.debian.org/tracker/CVE-2015-0235)) and not quite being able to figure out if a crawler might be exploited by a target website. A bit later I added the IPv6 lookup features and shared it, in response to someone else looking for some IPv6 support.
I'm still using it until some other solution comes around.

As an aside, `gethostbyname` is so outdated, it really shouldn't be used anywhere anymore. E.g. the [Postfix homepage](http://www.postfix.org/) says about it:

> GHOST Attack: Postfix does not call gethostbyname() since 2005. There is no Postfix code that invokes this function unless Postfix is specifically built for operating systems from more than 10 years ago.
",nyov,kmike
1104,2016-03-14 23:34:13,"@glyph's opinion has serious weight to us though, I think.

The others will probably agree. @kmike, @redapple, @sibiryakov, what's your take?

If this is unlikely to see a merge then it seems I should maybe invest some time to make it available as a scrapy-plugin for other users in the interim?
",nyov,redapple
1104,2016-03-14 23:34:13,"@glyph's opinion has serious weight to us though, I think.

The others will probably agree. @kmike, @redapple, @sibiryakov, what's your take?

If this is unlikely to see a merge then it seems I should maybe invest some time to make it available as a scrapy-plugin for other users in the interim?
",nyov,sibiryakov
1104,2016-03-14 23:34:13,"@glyph's opinion has serious weight to us though, I think.

The others will probably agree. @kmike, @redapple, @sibiryakov, what's your take?

If this is unlikely to see a merge then it seems I should maybe invest some time to make it available as a scrapy-plugin for other users in the interim?
",nyov,kmike
1103,2015-04-02 19:19:08,"@berkerpeksag did you have a chance to figure out why are tests failing?
",kmike,berkerpeksag
1100,2015-03-24 13:30:44,"@nyov we need to be sure it is respecting the default order and if you implement the env var it is actually overriding that. I guess you can use [this](https://github.com/scrapy/scrapy/blob/master/tests/test_command_shell.py), run some dummy or version command and validate the output.
",nramirezuy,nyov
1100,2015-04-02 19:24:15,"@nramirezuy likes clever tricks :)
",kmike,nramirezuy
1100,2015-07-17 14:45:48,"Naming the kwarg and the constant the same feels so dangerous :tongue: 

Maybe something like this?



I don't know about environment variables; thoughts @kmike ?
",nramirezuy,kmike
1100,2015-07-20 21:22:02,"@nyov we usually avoid mutable default arguments due to this: http://docs.python-guide.org/en/latest/writing/gotchas/#mutable-default-arguments
",barraponto,nyov
1100,2015-08-21 01:14:21,"@nramirezuy , yeah I knew that would come once I saw the bot :(
And another blocker. Wellll, I'll put it on the todo list. Soon(TM)

@cyberplant, thanks that looks nice! Want me to pull the commit in here, or do you want to keep going with the whole thing in your PR?
",nyov,nramirezuy
1100,2015-08-21 01:14:21,"@nramirezuy , yeah I knew that would come once I saw the bot :(
And another blocker. Wellll, I'll put it on the todo list. Soon(TM)

@cyberplant, thanks that looks nice! Want me to pull the commit in here, or do you want to keep going with the whole thing in your PR?
",nyov,cyberplant
1098,2015-03-24 11:27:18,"@nyov making a link from the project doesn't work ? or we can use an environment variable.
",nramirezuy,nyov
1098,2015-03-25 05:32:51,"@nyov I think we can't use pyxdg because it is GPL and doesn't support Python 3.
",kmike,nyov
1098,2015-03-26 14:49:19,"Thanks @nyov! 
- What about mentioning that the settings from all available scrapy.cfg files are merged?
- Regarding env variables - ""Scrapy also understands, and can be configured through, a number of environment variables."" doesn't help users because it is not documented which variables Scrapy understands. I see the code which allows to override settings with SCRAPY_... env variables is marked for deprecation/removal. The only relevant env variables seems to be SCRAPY_SETTINGS_MODULE and SCRAPY_PROJECT. It could be better to document them explicitly. 

@nyov @nramirezuy what do you think? I can merge the changes and tweak the docs if this sounds ok (updated PR is also welcome, of course :).
",kmike,nyov
1098,2015-03-26 14:49:19,"Thanks @nyov! 
- What about mentioning that the settings from all available scrapy.cfg files are merged?
- Regarding env variables - ""Scrapy also understands, and can be configured through, a number of environment variables."" doesn't help users because it is not documented which variables Scrapy understands. I see the code which allows to override settings with SCRAPY_... env variables is marked for deprecation/removal. The only relevant env variables seems to be SCRAPY_SETTINGS_MODULE and SCRAPY_PROJECT. It could be better to document them explicitly. 

@nyov @nramirezuy what do you think? I can merge the changes and tweak the docs if this sounds ok (updated PR is also welcome, of course :).
",kmike,nramirezuy
1098,2015-03-26 20:18:46,"@kmike, I was afraid of that when I rwrote the sentence ;) Okay I will list the ENV vars you mentioned, as I'm not sure what all `SCRAPY_...` would include.
",nyov,kmike
1096,2015-03-23 20:59:30,"Hey @nyov,

I agree with all your statements :) Scrapy 1.0 is very close to your Scrapy 0.26. There are no plans to break backwards compatibility or do a v2-style rewrite; spiders written for Scrapy 0.24 should work in Scrapy 1.0. The idea is to cleanup what we have now - improve the documentation and address some pain points in user API. 

It is not a far future - check the tickets for 1.0 milestone: https://github.com/scrapy/scrapy/milestones/Scrapy%201.0. Most planned features are either minor or implemented or already have pull requests in a good shape (switch to Python logging, items-as-dicts). We want to make a release in next few weeks.
",kmike,nyov
1096,2015-03-23 21:27:47,"Thanks @nyov - your feedback is welcome!
",kmike,nyov
1094,2015-03-25 00:55:53,"@nyov I agree there's enough there for scrapyd, but we also wanted to mention `shub` for deploying to Scrapinghub, and perhaps condense what's in the scrapyd deployment docs to give a brief overview with a link to the main docs if the reader wants to learn more.
",rdowinton,nyov
1094,2015-03-25 12:51:09,"@rdowinton 
Scrapy, a fast high-level web crawling and screen scraping framework for Python.
Scrapyd, a service daemon to run Scrapy spiders.

Scrapy isn't responsible to be deployed anywhere, we can add it to [here](https://github.com/scrapy/scrapy/wiki#projects-tools-and-libraries-using-scrapy) and give this more visibility on the documentation.
",nramirezuy,rdowinton
1094,2015-03-31 11:28:44,"@nramirezuy while it isn't responsible to be deployed anywhere, it would be good to point readers in the right direction towards the docs. As previously mentioned, we don't want to maintain duplicate documentation, so the page is more to tell them where to look. I think people are less likely to look at the projects/tools/library section than the main documentation, so I would imagine having something in the docs will provide more visibility. I've created a PR which will better demonstrate what I mean: https://github.com/scrapy/scrapy/pull/1124
",rdowinton,nramirezuy
1094,2015-03-31 15:57:34,"I don't have an opinion about FAQ vs a dedicated docs section, but I like @nramirezuy's list. It makes it more clear how are scrapyd and scrapy cloud related.

There are some other projects to deploy scrapy, e.g. https://github.com/dmclain/scrapy-heroku or https://github.com/TeamHG-Memex/scrapy-dockerhub. I have zero experience with them though. We shouldn't mention them in docs directly, but maybe it worths noting scrapyd is not the only way to run spiders.

https://github.com/scrapinghub/scrapyrt can be also seen as a deployment solution.

""Production"" and ""deployment"" terms are unclear, by the way. E.g. I created a spider, executed it locally and got the data I need - what does ""production"" and ""deployment"" means here? Is it starting spiders on a schedule, or some kind of monitoring, or creating an HTTP API for a spider, or storing data in a shared DB, or crawling in parallel using many virtual servers, or what? I think it should be explained more precisely. Just ""production"" or ""deployment"" are meaningless. It is clear what ""deploy a web site"" means - make it reliably available to many other people, that's a point of building a web site. For spiders the end result is data, and you don't need anything besides Scrapy itself to get it.
",kmike,nramirezuy
1094,2015-04-08 14:50:33,"@nramirezuy I think having the FAQ is a good idea, but perhaps we could have both? The FAQ would cover deploying to Scrapinghub or a remote server, and the page could cover different ways to run Scrapy as @kmike suggested, such as Scrapyd, ScrapyRT, scrapy-jsonrpc, cron jobs etc. The FAQ would explain how you can deploy, and it would also be worth mentioning why for example you would want to use Scrapyd vs other generic deployment/automation tools such as fabric, Ansible and so on. Thoughts?
",rdowinton,nramirezuy
1094,2015-04-08 14:50:33,"@nramirezuy I think having the FAQ is a good idea, but perhaps we could have both? The FAQ would cover deploying to Scrapinghub or a remote server, and the page could cover different ways to run Scrapy as @kmike suggested, such as Scrapyd, ScrapyRT, scrapy-jsonrpc, cron jobs etc. The FAQ would explain how you can deploy, and it would also be worth mentioning why for example you would want to use Scrapyd vs other generic deployment/automation tools such as fabric, Ansible and so on. Thoughts?
",rdowinton,kmike
1093,2015-03-24 22:08:14,"@pablohoffman created here: https://github.com/scrapy/scrapyd/issues/85
",rdowinton,pablohoffman
1092,2015-03-30 10:54:34,"Guys, I have an idea. Let's implement thread pool adjustment somewhere else, outside of DNS resolver. E.g. setting can be REACTOR_THREADPOOL_MINSIZE. And pass the rest of changes to DNS resolver as they are now. What do you think? @kmike @nyov  
",sibiryakov,nyov
1092,2015-03-30 10:54:34,"Guys, I have an idea. Let's implement thread pool adjustment somewhere else, outside of DNS resolver. E.g. setting can be REACTOR_THREADPOOL_MINSIZE. And pass the rest of changes to DNS resolver as they are now. What do you think? @kmike @nyov  
",sibiryakov,kmike
1092,2015-03-30 13:19:57,"@nyov by default ThreadedResolver uses a common shared thread pool, and other Scrapy components also use shared threadpool. It is possible to create separate thread pools for each component; this could be a bit easier for Scrapy components and harder for builtin Twisted components.
",kmike,nyov
1092,2015-03-30 13:58:46,"Ah, that is good to know. And trying to micromanage threadpool would probably be digging too deep, and just having a global adjustment setting as @sibiryakov proposes is better, if we consider other technologies such as asyncio in the future and the 'other languages' support for spiders or even other scrapy components.

I don't know what this shared threadpool's size is and never had any issues with it, but if it's not currently adjustable that looks like a sensible option to have.
",nyov,sibiryakov
1089,2015-03-22 13:44:21,"@kmike I have added few lines to `ScrapyHTTPClientFactory` implementation and now it passes the test. Could you check this?
",persiyanov,kmike
1089,2015-03-25 14:44:09,"@nyov fixed.
",persiyanov,nyov
1087,2015-03-20 16:10:52,"Hi @nramirezuy - could you please summarize the changes? Sorry, I can't wrap my head around your charts :)
",kmike,nramirezuy
1087,2015-03-20 16:19:35,"@kmike that works?
",nramirezuy,kmike
1087,2016-01-19 09:34:15,"As I recall, this change breaks a lot (all?) of custom user middlewares, right? 
@nramirezuy @dangra you've discussed it further, what was the resolution?
",kmike,nramirezuy
1087,2016-01-19 13:06:14,"@kmike I recall it break a lot as you stated and our discussion comes up to an alternative flow that was respectful with a very little variation for returning requests at process_request hook. I think we can close this ticket and reopen once @nramirezuy come up with the new flow.
",dangra,nramirezuy
1087,2016-01-19 13:06:14,"@kmike I recall it break a lot as you stated and our discussion comes up to an alternative flow that was respectful with a very little variation for returning requests at process_request hook. I think we can close this ticket and reopen once @nramirezuy come up with the new flow.
",dangra,kmike
1086,2015-03-25 16:42:39,"@nyov looks goo, thanks! A couple of questions:
1. Why is this helper added to TextResponse and not to HtmlResponse?
2. What do you think about caching get_base_url result? 
",kmike,nyov
1086,2015-03-25 22:03:07,"@kmike, I did this because it seemed to me that `TextResponse` is actually the class that handles html (because of possible MIME or content-type mis-detection IIRC), while `HtmlResponse` is just an empty subclass. (Also the selector property is added to the `TextResponse` class, not `HtmlResponse`).

edit: note: I had to change `scrapy/utils/response.py` to lazy-load responses in `open_in_browser`, because otherwise it seems there is a circular import loop with `scrapy/http/response/text.py` now and tests would fail.
",nyov,kmike
1086,2015-03-26 03:50:33,"Thanks @nyov for the tests!
I agree that `TextResponse` seems the right place to add `urljoin()` using `get_base_url()`. Right now it's where all html specific functions are declared and `get_base_url()` works fine if response.body is not html (it fallbacks to response.url).

I'd change the name of `test_urljoin` in TextResponseTest to something else (`test_urljoin_with_base_url`?) instead of replacing the one in BaseResponseTest since both cases should work in TextResponse, but otherwise I think I can rebase those tests and the PR should be complete.
",curita,nyov
1086,2015-03-27 17:51:39,"Just rebased @nyov tests.
",curita,nyov
1085,2015-03-19 18:12:47,"@pablohoffman I think it's best all these parameters match those from `default_settings.py`, log level is already `DEBUG` by default.
",curita,pablohoffman
1085,2015-03-26 02:49:00,"@kmike Thought that maybe they were dependency issues and that's why it wasn't imported before but that doesn't seem to be the case. I just changed it.
",curita,kmike
1085,2015-03-30 22:47:42,"@kmike Yes, `log.start` is (hopefully) going to be deleted by next release. I can resubmit this change to `scrapy:master` and rebase the python logging changes instead.
",curita,kmike
1083,2015-03-18 20:49:18,"@nramirezuy, `fetch(url)` doesn't get dupefiltered, try `fetch(Request(url))`
",marven,nramirezuy
1083,2015-03-18 20:58:26,"@marven You are right!

In theory this can be solved by doing [this](https://gist.github.com/nramirezuy/81e0875079e5889d2a86).
",nramirezuy,marven
1083,2015-06-16 14:54:46,"What is the status on this PR?

I was about to open a PR doing exactly what @nramirezuy suggested
when I noticed there already this PR right here.

My opinions on your two points @marven :
- using `fetch` to test for duplicate urls is an overkill, `scrapy.utils.request.request_fingerprint` would do this very well without the side effect of downloading
- while testing a custom dupefilter, again, the shell is an overkill. Most tests can be adapted from the builtin tests and be run efficiently from a test script/suite

Because of this, I'd prefer the second, simpler solution: overriding the setting just for the shell.
",Digenis,nramirezuy
1083,2015-06-16 14:54:46,"What is the status on this PR?

I was about to open a PR doing exactly what @nramirezuy suggested
when I noticed there already this PR right here.

My opinions on your two points @marven :
- using `fetch` to test for duplicate urls is an overkill, `scrapy.utils.request.request_fingerprint` would do this very well without the side effect of downloading
- while testing a custom dupefilter, again, the shell is an overkill. Most tests can be adapted from the builtin tests and be run efficiently from a test script/suite

Because of this, I'd prefer the second, simpler solution: overriding the setting just for the shell.
",Digenis,marven
1083,2015-12-09 08:14:00,"I think #1598 resolves this unless someone can demonstrate the usefulness of a dupefilter in the shell.
So far @marven hypothesized that someone would use the scrapy shell to filter duplicate urls or test a custom dupefilter. Are there any practical use-cases?
",Digenis,marven
1081,2015-03-18 03:09:16,"good job @kmike !
",pablohoffman,kmike
1080,2016-02-02 13:11:50,"@kmike I think we can close this now, right?
",eliasdorneles,kmike
1064,2015-03-10 10:22:43,"@shirk3y interesting idea, but it has only a limited utility - unicode/str/Link as a Request only works if you use a default callback, so it doesn't look that useful for scrapy.Spider because usually there are several callbacks. Actually I don't like that there is a default callback in scrapy.Request - using a default calback makes code less clear, it ties Request to a Spider in a non-obvious way, and typing savings are small.

I'm not an user of CrawlSpider myself, so can't comment on it; your example looks quite clean. If this feature is useful only for CrawlSpider then maybe it makes more sense not to add itto the core - it can be handled by a crawl spider itself.

@nramirezuy the problem with replacing dicts with Items in a middleware is that some components will get Items, and some will get dicts even if user sends only dicts. That's why I think it is good to support dicts directly, without dict->item middleware hacks. To disable dicts one can write another middleware. I think it is good to have dicts as a well-defined and supported Scrapy feature, not just as a shortcut to define dynamic Item classes.
",kmike,nramirezuy
1064,2015-03-11 03:26:33,":+1:  It will make it easier for newcomers and make Scrapy more suitable for small projects. This also means anyting that acts like a dict would be suitable as an item.

@nramirezuy - For well defined projects maybe it would make sense to add a pipeline that requires items to be of a certain type, in addition to any other validation you have.
",shaneaevans,nramirezuy
1063,2015-03-13 21:33:09,"In a meeting with @pablohoffman, @dangra, @redapple and @Curita we agreed on the following:
1. relocate scrapy.contrib;
2. don't do the scrapy.core / scrapy.http relocation;
3. remove contrib_exp;
4. move djangoitem to a separate project named scrapy-djangoitem;
5. keep backwards compatibility (what to do with contrib_exp?);
6. use plural names consistently (of course, when it makes sense).

As @nramirezuy said, it would be nice to have this in Scrapy 1.0 release.
",kmike,nramirezuy
1063,2015-03-23 22:55:14,"After some more thought, I wouldn't rule out having a single ""contrib"" repository. Even git ships with some good stuff in a contrib folder.

And it might make sense to think about splitting scrapylib in this context:
Some of the code in it is scrapinghub-API only, which should stay in there or be renamed to shublib or similar; the more generic rest (magicfields, etc) would be better in a contrib repo under the /scrapy/ group namespace.

So despite @kmike's reluctance for this, I could envision going with a(nother) single repo for contrib/exp/ext here. No-one would necessarily need to maintain it; sure there's bound to be some old or broken code over time. But it could also be a place to direct new contributors (or gsoc entrents) for easy cleanup tasks, learning the ropes by bringing code up to date with current spec, showing their abilities.

If the decision would be for such a single repo to follow in `contrib_exp`'s footsteps, I'd volunteer the name I used for my codedump: `scrapyext` (kind of a misnomer for extensions) for scrapy as `scrapylib` is for scrapinghub now -- or put `scrapylib` under scrapy and `shublib` for shub.
The repo layout then should follow scrapylib style: every folder in it being a self-standing module (and I would convert lone files to `folder/__init__.py` style to make that obvious).

p.s. Thanks for not doing a scrapy.core or scrapy.http. And sorry for :wall-of-text:
",nyov,kmike
1063,2015-03-25 12:58:16,"@nyov AFAIK nobody is working on it. It seems an outstanding question is what to do with contrib_exp.iterators.xmliter_lxml - the options are
1. add it to scrapy.utils.iterators;
2. add it to scrapy.utils.iterators and remove the other xml iterator;
3. remove contrib_exp.iterators.xmliter_lxml.

> Does that have to be backwards-compatible (keeping stubs in contrib?)

Yes.
",kmike,nyov
1063,2015-04-03 07:14:13,"Just checking, is there anyone working on this ticket right now? I can help if there's no one interested. /cc @nyov, @nramirezuy
",curita,nyov
1063,2015-04-03 07:14:13,"Just checking, is there anyone working on this ticket right now? I can help if there's no one interested. /cc @nyov, @nramirezuy
",curita,nramirezuy
1063,2015-04-15 18:44:17,"@SudShekhar @Curita was going to do it so I leave her to comment on that. The repo is created already: https://github.com/scrapy/scrapy-djangoitem
",pablohoffman,SudShekhar
1063,2015-04-15 18:56:01,"Sorry I forgot to reply to this issue, I've already started working on it. Thanks @SudShekhar for offering your help! I'll let you know if I need someone else to jump in.
",curita,SudShekhar
1060,2015-03-30 23:48:42,"@nyov:

> So is there any chance to keep or change the general naming convention self.log.level() instead of self.logger.level() in spider classes? And log.level() instead of logger.level() for others?

I prefer self.logger since the actual object is a Python logger, and you have a log method in there, calling self.log.log(""level"", ""msg"") is kind of confusing. Name ""logger"" at the top of the modules, like:



is merely a convention, but it's a well established one. 

> And I liked the brevity of having the loglevel as an optional function argument instead of a requirement in the name. But oh well.

There's a logger.log(""level"", ""msg"") function available in Python loggers, and `self.log` in the Spider is still there (it calls to self.logger.log now).
",curita,nyov
1060,2015-03-31 08:44:55,"@kmike, @chekunkov:
Oh, I think this is going to raise some doubts, routing all spider/crawler messages into a single logger (thus not supporting directly the redirection to different files in a per-spider basis) was a design decision, I'll try to explain why I think this is a better approach.

First of all, Scrapy needs to log things before crawlers are instantiated. It also needs to log messages independently entirely from crawlers, for example, if `LOG_STDOUT` is set print messages will be redirected to the logging system, and those clearly won’t have a Crawler attached to them. The same goes for messages logged outside Scrapy by other libraries, like Twisted.

On top of that, there are messages logged inside the crawler when you haven’t a spider instance yet (all extension initialization happens before the spider is instantiated) so there are messages outside spiders too. 

Because of that there are always going to be messages outside crawlers. By itself I think this is a deal breaker, we can’t link every messages to a crawler or spider, and it’s not clear what to do with those unlinked (which crawler is going to configure them? they are going to be present in crawler outputs, facing duplication?) and I think any decision here will be somehow arbitrary.

Also, we’re switching to python logging, so discouraging the users to use it directly, forcing them to call Scrapy wrappers over it (like with a `crawler.log()` call) goes against one of the main ideas for this change.

Another problem is that there’s no easy way around passing Crawler or Spider instances to the `extra` parameter in log messages, and then filtering them with custom logging.Filters per crawler/spider to get those messages issued by a specific crawler. This makes it hard to use the available tools for configuring logging with the standard lib, so we would discard a lot of benefits for this port. For example, logging.Handlers (which take care of redirecting log messages to different sources) can’t be attached to filters, only loggers, logging.Formatters only attach to handlers, we can’t attach filters to other filters (we’re using filters in [scrapy.utils.log](https://github.com/scrapy/scrapy/pull/1060/files#diff-7386607e889af488be474e8932dc8febR17)), etc.

Those issues could be bypassed by using different loggers per Crawler/Spider, but there isn't a convenient way to implement this. Loggers are referenced by their name, so we should have a unique string representation for every instance. I know that the Spider.name is the first thing that comes to mind, but this is just not robust enough. It could change at any time during the Spider execution. It could be defined in its __init__ call, not as class attribute, so we won’t be able to log anything until then. And more importantly, we can actually run two spiders at the same time with the same name, so we won’t be able to differentiate their loggers. I think we could use a hash or id of the instance as logger name, but this will make the log output non-deterministic, and loggers will be hard to reference manually.

Messages attached to crawlers are still given an instance of them in their `extra` parameters so implementing something that redirects messages in a per-crawler basis is still possible, but I rather keep it outside Scrapy to maintain simplicity and consistency.
",curita,chekunkov
1060,2015-03-31 08:44:55,"@kmike, @chekunkov:
Oh, I think this is going to raise some doubts, routing all spider/crawler messages into a single logger (thus not supporting directly the redirection to different files in a per-spider basis) was a design decision, I'll try to explain why I think this is a better approach.

First of all, Scrapy needs to log things before crawlers are instantiated. It also needs to log messages independently entirely from crawlers, for example, if `LOG_STDOUT` is set print messages will be redirected to the logging system, and those clearly won’t have a Crawler attached to them. The same goes for messages logged outside Scrapy by other libraries, like Twisted.

On top of that, there are messages logged inside the crawler when you haven’t a spider instance yet (all extension initialization happens before the spider is instantiated) so there are messages outside spiders too. 

Because of that there are always going to be messages outside crawlers. By itself I think this is a deal breaker, we can’t link every messages to a crawler or spider, and it’s not clear what to do with those unlinked (which crawler is going to configure them? they are going to be present in crawler outputs, facing duplication?) and I think any decision here will be somehow arbitrary.

Also, we’re switching to python logging, so discouraging the users to use it directly, forcing them to call Scrapy wrappers over it (like with a `crawler.log()` call) goes against one of the main ideas for this change.

Another problem is that there’s no easy way around passing Crawler or Spider instances to the `extra` parameter in log messages, and then filtering them with custom logging.Filters per crawler/spider to get those messages issued by a specific crawler. This makes it hard to use the available tools for configuring logging with the standard lib, so we would discard a lot of benefits for this port. For example, logging.Handlers (which take care of redirecting log messages to different sources) can’t be attached to filters, only loggers, logging.Formatters only attach to handlers, we can’t attach filters to other filters (we’re using filters in [scrapy.utils.log](https://github.com/scrapy/scrapy/pull/1060/files#diff-7386607e889af488be474e8932dc8febR17)), etc.

Those issues could be bypassed by using different loggers per Crawler/Spider, but there isn't a convenient way to implement this. Loggers are referenced by their name, so we should have a unique string representation for every instance. I know that the Spider.name is the first thing that comes to mind, but this is just not robust enough. It could change at any time during the Spider execution. It could be defined in its __init__ call, not as class attribute, so we won’t be able to log anything until then. And more importantly, we can actually run two spiders at the same time with the same name, so we won’t be able to differentiate their loggers. I think we could use a hash or id of the instance as logger name, but this will make the log output non-deterministic, and loggers will be hard to reference manually.

Messages attached to crawlers are still given an instance of them in their `extra` parameters so implementing something that redirects messages in a per-crawler basis is still possible, but I rather keep it outside Scrapy to maintain simplicity and consistency.
",curita,kmike
1060,2015-04-10 10:10:03,"Thanks @chekunkov for reviewing this!

I think that keeping a single Scrapy root logger, and using filters to separate messages based in their crawler is the way to go (that’s why I kept the ‘crawler’ key in all log messages’ meta parameter actually), but I’m hesitant of implementing those filters in Scrapy, since their usage is going to be really dependant of what the user or application wants to do with them. They can be implemented quite easily with the standard logging library outside Scrapy, and by doing that the users would hold entire control of what they want to filter and what they want to do with those filtered messages.

By any case, if we decide that we want to make that feature available in Scrapy I think we can implement it after merging this PR, since it should be something complementary that shouldn’t change the current approach.

Some notes:

> In case of ScrapyRT one crawler creates exactly one spider like it expected by latest Scrapy version - crawler doesn't support having multiple Spider objects any more.

It can’t have multiple spiders running at the same time, and it’s attached to a single Spider class, but it can have different instances of that Spider class after each call of Crawler.crawl (though these calls should be sequential, can’t be done in parallel).

> You say we want users to use it directly - so why Spider.logger was added?

Spider.logger was added as a convenient already created logger to avoid users the need of creating one themselves, and this attribute allows us to provide backward support for the old Spider.log method. It’s documented that this is not the only way to log things, I rewrote the logging topic in the docs to show how to use the standard logging directly.
",curita,chekunkov
1054,2015-03-09 13:58:59,"@kmike You can raise `NotConfigured` if [these](https://github.com/scrapy/scrapy/blob/master/scrapy/core/downloader/handlers/s3.py#L39) aren't present or are empty; that will fix the one second delay for most of the cases.
",nramirezuy,kmike
1054,2015-03-09 14:31:32,"@nramirezuy a good idea
",kmike,nramirezuy
1054,2015-03-09 14:51:00,"@nramirezuy the problem with raising NotConfigured it is that when these options are None boto tries to get them from environment variables or from config. I don't quite like an idea of replicating boto logic just to raise a NotConfigured error earlier. We can create boto.provider.Provider and use its get_credentials method to check if the credentials are set, but to me it also looks like a hack.
",kmike,nramirezuy
1054,2015-03-09 22:40:06,"@kmike Yeah I forgot about that one. The provider seems fine to me it has been there since [2012](https://github.com/boto/boto/commit/3328d99106cfc7b8773985eab086046bfdde20dd), It isn't documented but boto doesn't seems to document all the features it has, not sure whats the policy there.
",nramirezuy,kmike
1054,2015-03-16 17:39:22,"@nramirezuy it turns out checking the credentials manually won't help because it is the credentials check which causes the delay. See https://github.com/scrapy/scrapy/pull/1055#issuecomment-81831270.
",kmike,nramirezuy
1051,2015-04-25 16:25:01,"Sorry for my disappearance.

@kmike  for you opinions:

##### what happens if idled returned nothing?

If users implement the idled() and return nothing, or something other than a generator, nothing will happen.

##### it should be documented how to stop the spider from idled signal; usage example in docs would be good;

I added a `try..except....` block. Now if users can raise `CloseSpider` in idled() method if they want to close spider. Do you think this is ok?

##### there should be more tests, at least for request handling.

I add some tests in the test case. But the test failed because the engine of the crawler did not exist. How can I fix this?

##### Another API

I am not sure which one is better. I think this should let core developers to decide.  
",ChienliMa,kmike
1048,2015-03-23 02:21:33,"I've updated spiders.rst, but can't see where to place info about `--arg-json` and `--set-json` in commands.rst. @pablohoffman @kmike Any ideas?
",shirk3y,kmike
1048,2015-03-23 02:21:33,"I've updated spiders.rst, but can't see where to place info about `--arg-json` and `--set-json` in commands.rst. @pablohoffman @kmike Any ideas?
",shirk3y,pablohoffman
1048,2015-04-22 22:01:14,"@pablohoffman After rebasing and re-run Travis, tests are passing now. @kmike Where to place `--set-json` docs and `--arg-json` tests?
",shirk3y,kmike
1048,2015-04-22 22:01:14,"@pablohoffman After rebasing and re-run Travis, tests are passing now. @kmike Where to place `--set-json` docs and `--arg-json` tests?
",shirk3y,pablohoffman
1048,2015-04-30 07:13:34,"Thanks @eliasdorneles! If you mind, you could make docs and I will implement tests. Makes sense?
",shirk3y,eliasdorneles
1048,2015-05-07 15:04:47,"Added tests for `runspider` and `crawl` command, and docs for `--set-json` - by @eliasdorneles.
",shirk3y,eliasdorneles
1047,2015-02-12 11:52:43,"@klangner sorry for being picky :) Could you please check the docstrings of the functions in this module? For example, `unicode_to_str` docstring says ""Return the str representation"" which is incorrect in Python 3. 

I think it is a good to review a module in more details while fixing the tests (and maybe add more tests) because we may think its porting is done when tests are passing while it is not.
",kmike,klangner
1047,2015-07-28 19:00:33,"Thanks @klangner! Me, @eliasdorneles and @dangra went forward and merged https://github.com/scrapy/scrapy/pull/1379, copy-pasting some of your changes. 
",kmike,klangner
1041,2015-03-10 08:37:28,"@eliasdorneles it failed because of https://github.com/scrapy/scrapy/issues/1034
",kmike,eliasdorneles
1041,2015-03-10 20:41:50,"LGTM too, but I'd also add the script because we'll surely need it again. @dufferzafar can you add the `linkfix.py` script to the repo? I think we could create a folder under `docs` (let's say `docs/utils`) and put it there.
",curita,dufferzafar
1041,2015-03-12 22:51:41,"@pablohoffman: @dufferzafar's script uses `linkcheck` output to fix those links. Maybe we could add a command in the [Makefile](https://github.com/scrapy/scrapy/blob/master/docs/Makefile) that runs `linkcheck` and then runs this script?

Could be something like:


",curita,dufferzafar
1041,2015-03-12 22:51:41,"@pablohoffman: @dufferzafar's script uses `linkcheck` output to fix those links. Maybe we could add a command in the [Makefile](https://github.com/scrapy/scrapy/blob/master/docs/Makefile) that runs `linkcheck` and then runs this script?

Could be something like:


",curita,pablohoffman
1035,2015-02-13 16:29:53,"How do you feel about supporting multiple folders? Our function returning existent folders and the command being able to look into these folders?

/cc @SudShekhar @kmike 
",nramirezuy,SudShekhar
1030,2015-02-02 20:18:15,"Thanks! I'll leave it to someone else to close; maybe other developers have a different opinion  (@nramirezuy?)
",kmike,nramirezuy
1029,2015-01-29 17:17:14,"@nramirezuy good one :dancers: 
",dangra,nramirezuy
1029,2015-01-29 17:36:30,"@kmike you comment is missing an icon :sheep: 
",nramirezuy,kmike
1029,2015-02-06 12:38:33,"@barraponto no, the point of this PR was to change ""latest"" to ""master"" :) In RTFD ""latest"" points to docs for the latest Scrapy released version, ""master"" points to docs for the latest commit in master branch. When contributing, users should use latest master; instructions in latest stable release could be already  outdated (this is what happened in https://github.com/scrapy/scrapy/issues/975).

Here is some food for your pet snail: :leaves:.
",kmike,barraponto
1028,2015-06-03 18:57:09,"@stav Are there any plans to merge this in? We had the issue from https://github.com/scrapy/scrapy/issues/15 and it looks like this is the right fix. Thanks for sharing it!
",gwintrob,stav
1027,2015-03-25 00:51:49,"@nyov good point :) I think the case for removing from `scrapy` is so we have the deploy functionality in one place (scrapyd-client) rather than maintaining the same functionality in two separate tools.
",rdowinton,nyov
1023,2015-01-26 15:29:34,"Hi @pawelmhm,

I like your examples a lot more than the existing Overview pages. But the issue is that it overlaps with the tutorial. If we merge it there will be two doc pages which explains the same thing in a slightly different way. `runspider` could be confusing - we introduce it here, but then, in a tutorial, it is not even mentioned.

As for github, I'm not sure it is the best website to scrape - it has an API, and the website changes are frequent.
",kmike,pawelmhm
1023,2015-03-11 01:51:23,"Btw, @pawelmhm -- pls lemme know if you're still working on this and/or if you need help. Maybe we can do it in four hands/eyes. ;)
",eliasdorneles,pawelmhm
1023,2015-03-27 20:06:57,"Okay, the new overview page addresses the issues here, so I'm closing this.
@pawelmhm thanks for the inspiration and trusting this to me :)
",eliasdorneles,pawelmhm
1020,2015-03-13 21:44:55,"Looks good, thanks @jojje! 
+1 to merge.

> Whether or not to add a versionadded number or not. After all I don't control which version the feature would be added to if any, as it depends on when you'd merge.

A good question :) I think the way you've done it is fine.
",kmike,jojje
1020,2015-03-17 17:32:13,"Nice contribution, thanks @jojje !. No need to updates news.rst, the release notes are semi-automatically collected before each release these days.
",pablohoffman,jojje
1016,2015-02-03 15:06:30,"@kmike but sending non compatible ""json strings"", doesn't make too much sense either. Also it isn't wrong just nobody implemented it: https://docs.python.org/2.7/library/json.html#top-level-non-object-non-array-values

If you manage to get a ""non-json string"" as input for your `Processor` is because the chaining is wrong and deserve the `ValueError`. 
",nramirezuy,kmike
1016,2015-02-03 16:05:01,"@nramirezuy: If we choose to load all strings as json objects (and return an error if this isn't possible), what should be the behaviour in case of empty strings? Should we return None or raise a `ValueError`?
",SudShekhar,nramirezuy
1016,2015-02-03 16:19:47,"@nramirezuy check this example:



Correct behaviour would be to return None, but with special-casing strings it'll try to decode JSON one more time and raise an exception.
",kmike,nramirezuy
1016,2015-02-03 21:13:41,"@kmike I kinda disagree on that because because if you are applying a path is because you are expecting something different than a string.

@SudShekhar I think empty strings have to return `raise ValueError` because aren't valid.
",nramirezuy,SudShekhar
1016,2015-02-03 21:13:41,"@kmike I kinda disagree on that because because if you are applying a path is because you are expecting something different than a string.

@SudShekhar I think empty strings have to return `raise ValueError` because aren't valid.
",nramirezuy,kmike
1016,2015-02-03 21:29:40,"@nramirezuy still, the input was a valid JSON data which happen not to contain the value we requested. It is similar to asking for `foo.bar` when there is no 'foo' key in a JSON object - you expect 'foo' to be a dictionary with 'bar' key, when it is not the case. But the first query will raise a JSON parsing error, and the second will return None. And if a decoded string happen to contain valid JSON then the answer in first case would be just wrong.

Actually, I'm fine with leaving it as-is if this gotcha is documented. I agree that it is not a common case, and I don't see a way to handle it without making the API worse. Maybe we can have an option in JmesProcessor `__init__` (not in `__call__` method) to turn off string auto-decoding.
",kmike,nramirezuy
1016,2015-02-03 22:32:50,"

But I guess doing the `json.loads` outside is not that hard.



I like more the second option. 
@kmike  @SudShekhar  What do you think?

EDIT: We should also add examples to the doc.
",nramirezuy,SudShekhar
1016,2015-02-03 22:32:50,"

But I guess doing the `json.loads` outside is not that hard.



I like more the second option. 
@kmike  @SudShekhar  What do you think?

EDIT: We should also add examples to the doc.
",nramirezuy,kmike
1016,2015-02-03 22:45:02,"I like not doing json.loads in the JmesProcessor, and the Compose / MapCompose examples are good. 

@nramirezuy you should really create a few nice images/diagrams which show how Compose and MapCompose work :) What are the inputs, what are the outputs, how they can be used together. And maybe we should rename the processors to verbs - instead of JmesProcessor write SelectJmes - `Compose(json.loads, SelectJmes('foo'))` reads very well.
",kmike,nramirezuy
1016,2015-02-03 22:50:49,"@nramirezuy a realted question: is it possible to use the processors without the item loaders? 



is rather nice. If it works then I think it worths documenting. An maybe splitting this micro-framework from the item loaders, if it is not only about item loaders.
",kmike,nramirezuy
1016,2015-02-04 16:42:31,"@nramirezuy : The second options looks much better and it gives users more control. So, should I remove the json.loads from inside the processor? Regarding the documentation, the examples you have given look pretty comprehensive to me :smile: .I will add them and some others to the list. 
+1 to the renaming too.

EDIT: In the documentation, I didn't add the chaining example because I felt that it fit better in the Compose examples list (same can be said for the processor handling a list of json strings I guess). Do let me know your views on this.
",SudShekhar,nramirezuy
1016,2015-02-17 14:54:07,"There is something with the markup but I don't know what it is. 

/cc @kmike 
",nramirezuy,kmike
1016,2015-03-08 07:10:48,"Hi,
Is there anything else that needs to be changed in this commit? Thanks for all your feedback.
/cc @nramirezuy @kmike 
",SudShekhar,nramirezuy
1016,2015-03-08 07:10:48,"Hi,
Is there anything else that needs to be changed in this commit? Thanks for all your feedback.
/cc @nramirezuy @kmike 
",SudShekhar,kmike
1016,2015-03-18 17:59:35,"@pablohoffman why does it matter? jmespath is not added to `install_requires`, it is optional. 

Imports are very fast after the first successful import (a lookup in a dict) - by moving import to module level we won't get any speed benefits, but the exception may become less clear is jmespath is absent. 
",kmike,pablohoffman
1016,2015-03-20 15:07:57,"@pablohoffman @kmike `jmespath` was added to `tests/requirements.txt`, not Scrapy ones.
",nramirezuy,kmike
1016,2015-03-20 15:07:57,"@pablohoffman @kmike `jmespath` was added to `tests/requirements.txt`, not Scrapy ones.
",nramirezuy,pablohoffman
1014,2015-01-23 15:50:02,"Hi @gatufo,

This feature looks useful, but what is the reason it should be a part of Scrapy and not a separate package? I share @nramirezuy worries about extra code added to Scrapy. 

My vote is to move in an opposite direction - move some of the Scrapy components to separate packages, not add more to the bundle. 
",kmike,nramirezuy
1014,2015-01-26 09:47:58,"@nramirezuy @kmike  - a separate repo, used by default in Scrapy like queuelib?
",shaneaevans,nramirezuy
1014,2015-01-26 09:47:58,"@nramirezuy @kmike  - a separate repo, used by default in Scrapy like queuelib?
",shaneaevans,kmike
1014,2015-01-26 14:59:51,"@shane42  yes and w3lib. I think that a library for collecting stats compatible with std logging, might be useful to use on external scripts.

@kmike I should just move everything to a separate library and work from there. We currently don't have a recurring tasks API, so let that new library do it bit it self. Whenever this API gets merged to Scrapy we can work on a better integration, on Scrapy side.
",nramirezuy,kmike
1012,2015-02-10 20:24:23,"@nramirezuy my take on this: https://github.com/kmike/scrapy/blob/processors-overhaul/sep/sep-022.rst. I think that more processors is good, but I'm worried that we may be creating a worse/incomplete version of https://github.com/Suor/funcy or https://github.com/pytoolz/toolz.

See https://github.com/scrapy/scrapy/pull/1045
",kmike,nramirezuy
1012,2015-02-10 21:03:23,"I agree with @kmike on this.
I have been using these instead of scrapy's processors.
I wish I could say more but I don't have the time right now.
I am however interested and will follow/comment on  discussion #1045.
",Digenis,kmike
1012,2015-06-24 14:30:27,"I'm also with @kmike on this. Perhaps we should close this ticket to concentrate efforts on his approach.
",pablohoffman,kmike
1011,2015-01-23 17:37:42,"@SudShekhar Careful you are using the same branch for both issues and now are messed up. Can you fix?
",nramirezuy,SudShekhar
1011,2015-01-23 18:27:05,"@nramirezuy : Done. I had intially forgotten to create a new branch for the JsonProcessor but later created one. Had forgotten to remove these old commits from here. 
",SudShekhar,nramirezuy
1010,2015-01-19 12:47:22,"Thanks for the feedback @aufziehvogel . I've implemented your suggested changes and a bit more (see https://github.com/scrapy/scrapy/pull/1012)
",Granitosaurus,aufziehvogel
1009,2015-01-13 20:18:13,"@SudShekhar Do you want to submit a PR? I can merge it
",nramirezuy,SudShekhar
1007,2015-03-18 12:18:11,"@nyov @kmike sorry for late reply, this is weird I didn't get notified.

@nyov you're right. I was going to drop it.

@kmike Script delivery is pending https://github.com/scrapy/scrapy/issues/906#issuecomment-71879184.
",umrashrf,nyov
1007,2015-03-18 12:18:11,"@nyov @kmike sorry for late reply, this is weird I didn't get notified.

@nyov you're right. I was going to drop it.

@kmike Script delivery is pending https://github.com/scrapy/scrapy/issues/906#issuecomment-71879184.
",umrashrf,kmike
1007,2015-04-13 17:08:41,"@nyov @kmike updated selectors repo and this PR based on changes in master.

Should I drop `scrapy/docs/topics/selectors.rst` in favor of `selectors/docs/topics/selectors.rst`?
",umrashrf,nyov
1007,2015-04-13 17:08:41,"@nyov @kmike updated selectors repo and this PR based on changes in master.

Should I drop `scrapy/docs/topics/selectors.rst` in favor of `selectors/docs/topics/selectors.rst`?
",umrashrf,kmike
1007,2015-08-07 15:02:27,"@Digenis you're right, thanks!
",kmike,Digenis
1005,2015-01-21 15:21:45,"@pawelmhm There is other guy working on a processor using jmespath. https://github.com/scrapy/scrapy/pull/1016

I motivated this processor thing because I believe that to have a `add_json` on `ItemLoader`; we need to first have a json method on Selector, and to accomplish this we will probably need a new `Response` type too.
",nramirezuy,pawelmhm
1003,2015-01-07 01:11:26,"@nramirezuy Great. Thanks a lot.
",wlnirvana,nramirezuy
1003,2015-11-02 11:16:27,"@nramirezuy Thanks, save my day!
",ayonliu,nramirezuy
1000,2015-01-08 12:43:55,"@nramirezuy Maybe you've missed that but description says ""it should work for any possible/existing spider"". Also I should add - it should work without providing any additional arguments to the spider, without writing any custom handlers/callbacks/overriding init or start_requests, it should be done from Crawler API by design and we need this for project we are going to opensource soon.
",chekunkov,nramirezuy
1000,2015-01-08 16:34:13,"@nramirezuy spider_opened signal is emitted from here:https://github.com/scrapy/scrapy/blob/master/scrapy/core/engine.py#L228  at the moment when it is sent to the world start_requests were already processed by many other things. Extension reacting to spider_opened will not stop start_requests from being executed, perhaps using some middleware handling start_requests would be better.

We do subclass Crawler in our use case, but what we do is just dumb copy-paste with if - else around start_requests: https://bitbucket.org/scrapinghub/scrapyrt/src/feafc46c760002b68f87a86a045e675377bf26ef/scrapyrt/core.py?at=master#cl-20 

I think using or nor using start_requests makes crawler more flexible, it gives you more options when you inherit from it. 
",pawelmhm,nramirezuy
1000,2015-01-08 16:46:33,"@nramirezuy Both `spider_opened` and `engine_started` signals are fired after start_requests are processed and added to the slot, there's no way to guarantee that `spider_opened` or `engine_started` handler from 'extra extension' will be executed before any other handler. Thus it's possible to have undesired sideeffects, so creating an `Extension` has no sense. Disabling `start_requests` in place where it is originally added before any processing happens looks like better idea to me if I want this to work with many different Scrapy projects.
And actually we are subclassing Crawler to achieve that, but there's no good place to override default behavior and disable `start_requests` so we ended up overriding entire `Crawler.crawl` method without reusing original code - which is bad because any change there in Scrapy should be ported manually then. That's why we decided to propagate this tiny change into Scrapy.
Can you explain why you are resisting this change? Default stays the same, nothing changes for any existing project, change is tested. We are just adding convenient toggle to provide more control over crawl in core API.
",chekunkov,nramirezuy
1000,2015-01-12 20:06:56,"@pawelmhm  @chekunkov You can also mimic [`parse`command](https://github.com/scrapy/scrapy/blob/master/scrapy/commands/parse.py#L126)
",nramirezuy,pawelmhm
1000,2015-01-12 20:06:56,"@pawelmhm  @chekunkov You can also mimic [`parse`command](https://github.com/scrapy/scrapy/blob/master/scrapy/commands/parse.py#L126)
",nramirezuy,chekunkov
1000,2015-01-27 12:41:25,"> I think this change isn't useful for Scrapy, at least you are using it as a library.

It's not clear to me what do you mean here. Do we have any restrictions on using Scrapy components and overriding them?

> However having a signal that guarantees to be executed before start_requests is by far more useful
1. I can't imagine any application for such kind of signal
2. There're no guaranties on handlers execution order - possible side effects when spider.start_requests is changed in such signal callback and other signal callbacks are not expecting this change.

> Another option is to use a SpiderMiddleware and yield your single requests there. As we are doing on HCF middlewares.

Yes, that was an option - to create middleware which will suppress defined start_requests and will return single request as you mentioned. This approach has one major drawback: start_requests are already called. There's no way to check and/or guarantee that start_requests don't have any code that has impact on further spider behaviour, like setting/changing some variables/flags, writing to file/db. So if we don't want to use default start_requests - I think the only safe way to do that - do not execute start_requests at all.

> You can also mimic parse command

Problem with this approach - start_requests are changed before spider initialisation, it's easy to 'trick' parse command and change self.start_requests in `__init__.py` which is not how it expected to work in our case. We want to have reliable toggle - if it's 'on' - enable start_requests, if it's 'off' - disable them not giving any chance for them to be executed. With no other options.

BTW, talking about ScrapyRT and `parse` command. We are going to add optional default start_requests execution in future - for example some sites need authentication/login/chaning location, which is usually done using start_requests - that's why we need this toggle. But it may be useful to have such option for `parse` command as well - that's the place where proposed change can be applied to enable/disable start_requests.

So far I still think that the way the problem was solved in this PR is:

a) clear 
b) easy to test 
c) doesn't break anything 
d) has no side effects
e) safe to use
f) can be found useful for further Scrapy development or for instance while making custom commands

For the reference - [here](https://github.com/scrapinghub/scrapyrt/blob/master/scrapyrt/core.py#L35) is the link to code we are using in ScrapyRT, which was motivation to push this change to Scrapy.

I'm not sure if my long speech can convince @nramirezuy :smile: but also it provides detailed motivation behind this trivial change for others, I would like to hear more opinions here.
",chekunkov,nramirezuy
994,2015-01-05 22:52:26,"@dangra Having read Python's docs on porting (https://docs.python.org/3/howto/pyporting.html), I think we could add a static py3 linter to partially ensure support on all files, regardless there were dependency issues for actually running them with python3. Since the twisted's imports aren't easily mocked I think we'll still need to blacklist files using them, but the linter should help a little with the regression problem.

For stating why the files are blacklisted, maybe we could add a note on that file, but I'm not sure how maintainable that would be. Twisted has a `twisted/python/dist3.py` file where they list supported py3 modules (thought I'm not sure if it's a full support since I haven't seen yet how they consider internal dependencies), maybe we could check there and construct the list of blacklisted files for twisted dependencies automatically.

@jameysharp I like the idea, maybe we could add that list to an existing utils file or a new one?
",curita,jameysharp
994,2015-01-05 22:52:26,"@dangra Having read Python's docs on porting (https://docs.python.org/3/howto/pyporting.html), I think we could add a static py3 linter to partially ensure support on all files, regardless there were dependency issues for actually running them with python3. Since the twisted's imports aren't easily mocked I think we'll still need to blacklist files using them, but the linter should help a little with the regression problem.

For stating why the files are blacklisted, maybe we could add a note on that file, but I'm not sure how maintainable that would be. Twisted has a `twisted/python/dist3.py` file where they list supported py3 modules (thought I'm not sure if it's a full support since I haven't seen yet how they consider internal dependencies), maybe we could check there and construct the list of blacklisted files for twisted dependencies automatically.

@jameysharp I like the idea, maybe we could add that list to an existing utils file or a new one?
",curita,dangra
994,2015-01-14 03:02:53,"@jameysharp I don't mind if you add the file to py3-ignores.txt if that is blocking you from finishing this PR.
",dangra,jameysharp
992,2015-03-16 12:46:18,"@chekunkov Already wasted time doing it, and merging it won't harm anyone. I will just suggest to sqash and rename commit to make allusion to `pydispatch`.
",nramirezuy,chekunkov
992,2015-03-16 20:25:50,"@kmike some plans are too long-term :) I was reading that code and fix costed me almost nothing - 2-5 clicks and 2 minutes of my time

@nramirezuy yep, I can squash and rename commit
",chekunkov,nramirezuy
992,2015-03-16 20:25:50,"@kmike some plans are too long-term :) I was reading that code and fix costed me almost nothing - 2-5 clicks and 2 minutes of my time

@nramirezuy yep, I can squash and rename commit
",chekunkov,kmike
992,2015-03-16 21:48:09,"@nramirezuy please tell if commit message looks better now
",chekunkov,nramirezuy
991,2015-01-06 15:33:32,"The design of `make_requests_from_url` is outside the scope of this PR.
- We should open a new thread for further discussion, if any. For @nramirezuy's quick info, regarding reasoning of the use of `dont_dedup=True` for first set of requests, consider a developer yielding multiple requests with `make_requests_from_url` inside `start_requests`, one'd expect any URLs he put there are intended. So setting `dont_dedup=True` makes sense.
",adon-at-work,nramirezuy
990,2015-12-28 11:38:56,"@kmike Thanks! I will give it a try when i get home.

The docs are note very clear in these sections. Regarding the pipeline, am I doing something wrong? I've also tried connecting to the signals instead of depending on the open_ and close_ methods, like in the web example, but nothing happened. I'm actually going to use Redis instead of a simple json, but wanted to try a simple test.

Thanks for your help!
",sebastiandev,kmike
990,2015-12-29 21:29:39,"@kmike I have just tried it and is working :) and of course the same fix applies for the pipeline since the config now is been set on the Runner, now the pipeline classes are executed. Perfect, thanks!
",sebastiandev,kmike
985,2014-12-16 13:37:52,"@dangra #708
",nramirezuy,dangra
985,2014-12-16 13:39:50,"@nramirezuy but in this case start_requests output should be an standard iterable based on start_urls. how is #708 supposed to fix the issue?
",dangra,nramirezuy
985,2014-12-16 15:02:56,"@dangra You are right, this is different.

[I found this is not being used anywhere.](https://github.com/scrapy/scrapy/blob/master/scrapy/crawler.py#L125)
",nramirezuy,dangra
984,2015-03-16 16:26:57,"@aufziehvogel can you add a test with unicode inside list and dict objects? 
",nramirezuy,aufziehvogel
984,2015-03-16 17:29:30,"@nramirezuy @kmike Sure I can, just give me some time :) Somewhen this week, I'd say.
",aufziehvogel,nramirezuy
984,2015-03-16 17:29:30,"@nramirezuy @kmike Sure I can, just give me some time :) Somewhen this week, I'd say.
",aufziehvogel,kmike
984,2015-03-16 17:41:19,"Thanks @aufziehvogel!
",kmike,aufziehvogel
984,2015-03-20 17:20:34,"@nramirezuy @kmike 

There are one version I would think is correct and one version that passes all tests.

My problem is: We have this function `_to_str_if_unicode` sometimes being called within `serialize_field`, which in turn is called in `_get_serialized_fields`.

In the `PythonItemExporter` the unicode is transformed to string, as expected by the function name. But in `JsonLinesItemExporter` the unicode remains.

What is the expected output? I guess it should always be string, not unicode? Otherwise we probably would not have this function.
Or do we really expect unicode for `JsonLinesItemExporter`, and string for `PythonItemExporter`?

In the first case, we need to find out, why in `JsonLinesItemExporter` it is unicode coming out. I guess it has to do with the line being `serializer = field.get('serializer', self._to_str_if_unicode)`, so we only get `_to_str_if_unicode` if there is no other serializer? And is there some other default serializer for JSON?
",aufziehvogel,nramirezuy
984,2015-03-20 17:20:34,"@nramirezuy @kmike 

There are one version I would think is correct and one version that passes all tests.

My problem is: We have this function `_to_str_if_unicode` sometimes being called within `serialize_field`, which in turn is called in `_get_serialized_fields`.

In the `PythonItemExporter` the unicode is transformed to string, as expected by the function name. But in `JsonLinesItemExporter` the unicode remains.

What is the expected output? I guess it should always be string, not unicode? Otherwise we probably would not have this function.
Or do we really expect unicode for `JsonLinesItemExporter`, and string for `PythonItemExporter`?

In the first case, we need to find out, why in `JsonLinesItemExporter` it is unicode coming out. I guess it has to do with the line being `serializer = field.get('serializer', self._to_str_if_unicode)`, so we only get `_to_str_if_unicode` if there is no other serializer? And is there some other default serializer for JSON?
",aufziehvogel,kmike
984,2015-03-20 18:05:29,"@aufziehvogel see https://github.com/scrapy/scrapy/issues/1080 - I think we should kill _to_str_if_unicode.
",kmike,aufziehvogel
984,2015-03-20 18:26:13,"@kmike This sounds reasonable, but I'm thinking about the split between both pull requests. I mean, I can do both, I'm a bit into the code now (not much, but a bit).

Then I just have to get this PR ""right"" (right meaning = right for current situation, even if the other PR is rejected) and the question is, can we see it as correct, that atm we get unicode for JSON exporters and string for Python exporter?...
",aufziehvogel,kmike
984,2015-03-23 13:26:22,"@aufziehvogel maybe you can make this PR test-only, without `serialized_value = str(serialized_value)` line? Fixing unicode issues will be left for #1080. 
",kmike,aufziehvogel
984,2015-03-23 17:40:43,"@kmike of course I can, but I still need to know which output to expect :) Because it's different if I set expected value to `u""井上""` or to `""井上""`. Both are possible, but both get different representations in the test case (and the output of the test case). One of them works with the current code, the other doesn't.

One is `u'\u4e95\u4e0a'` and one is `'\xe4\xba\x95\xe4\xb8\x8a'` (am not 100% sure which is which, but I guess the first one is the unicode then)

Also: without the `str(serialized_value)`, another test will fail, but I think this is intended and ok? We just create a failing test to fix it in another PR (like test driven development)?

From #1080 ""JsonLinesItemExporter and JsonItemExporter shouldn't encode values to bytes because JSON is a text format"", I'd say that the currently expected format is bytes and I should write my test to expect bytes (as your issue has not been approved yet, so we're still at bytes). However, the interesting thing now is, that the `JsonLinesItemExporterTest` fails if I _do expect bytes_. So this is now the difficult decision for me: Should I write a failing test for a currently expected behaviour or a working test for a proposed (and according to you the only really working) behaviour? If we really go stable software development, I'd say I should write the failing test for the currently expected behaviour. But I am pretty young and new in-job, we often tend to exaggerate in all directions before we learn better :)
",aufziehvogel,kmike
981,2014-12-12 13:10:46,"@aufziehvogel yes that was my first thought.
flixbus.de doesn't support SSLv2 nor SSLv3, only TLSv1+.
Additional a question, does scrapy/twisted-webclient support SNI?
The Flixbus site is on amazon cloudfront and expected SNI-support to present the correct certificate.
",toolking,aufziehvogel
981,2015-03-24 11:36:17,"@nyov whats this? This is an issue not a PR. :smile: 
",nramirezuy,nyov
979,2014-12-11 18:08:34,"@kmike Besides the memory leaks problems; this change also makes public the lxml API which is also important.
",nramirezuy,kmike
979,2014-12-11 18:38:35,"@jameysharp I don't know, it needs some thought; ideas are welcome.
What is your use case?
",kmike,jameysharp
975,2014-12-10 21:35:51,"Hi @aufziehvogel,

""latest"" means 0.24.x branch; in 0.24.x branch there is `bin/runtests.sh` file, and `scrapy/tests` folder is present - the docs are correct. In master branch test running method was changed, and this is also documented - see http://doc.scrapy.org/en/master/contributing.html (a link by @nramirezuy is correct). 

I understand the confusion since ""contributing"" docs for 0.24.x are displayed by default, but developers should contribute to Scrapy master which has a different way of running tests. Also, ""latest"" means ""latest stable"", not ""latest"" which is also confusing (see https://github.com/scrapy/scrapy/issues/589). But I'm not sure how to fix it. Anyways, the next release is not far away, so let's hope this won't waste too much time of the people who contribute to Scrapy. 

Sorry for the inconvenience!
",kmike,nramirezuy
975,2014-12-10 21:35:51,"Hi @aufziehvogel,

""latest"" means 0.24.x branch; in 0.24.x branch there is `bin/runtests.sh` file, and `scrapy/tests` folder is present - the docs are correct. In master branch test running method was changed, and this is also documented - see http://doc.scrapy.org/en/master/contributing.html (a link by @nramirezuy is correct). 

I understand the confusion since ""contributing"" docs for 0.24.x are displayed by default, but developers should contribute to Scrapy master which has a different way of running tests. Also, ""latest"" means ""latest stable"", not ""latest"" which is also confusing (see https://github.com/scrapy/scrapy/issues/589). But I'm not sure how to fix it. Anyways, the next release is not far away, so let's hope this won't waste too much time of the people who contribute to Scrapy. 

Sorry for the inconvenience!
",kmike,aufziehvogel
975,2014-12-11 04:15:31,"@kmike how do you mean?. The link is already there: https://github.com/scrapy/scrapy/blob/master/CONTRIBUTING.md
",pablohoffman,kmike
975,2014-12-11 04:16:43,"@kmike got it (I read too quickly). ""master"" instead of ""latest"", makes sense!
",pablohoffman,kmike
975,2015-01-29 16:56:01,"> are we going to make the change?

@pablohoffman see #1029 
",dangra,pablohoffman
971,2016-01-13 13:24:50,"@kmike Thanks! :+1: 
",umpirsky,kmike
969,2014-12-08 14:25:57,"@aufziehvogel referer is a header already handled by `RefererMiddleware`; we aren't adding a new `__init__` parameter for it by any means. 
I understand what you want to do, but I must tell that is not the way of solving it. Missing scheme on a sitemap is not something valid in the real world.
",nramirezuy,aufziehvogel
965,2014-12-10 19:04:40,"@aufziehvogel I would modify `process_links` to receive response as an argument. So if you need to join relative urls you can do it there when its needed. As implementation goes it isn't backwards compatible at least you do some hacking; how i would manage that is by checking at one point if the function expect the argument using [`get_func_args`](https://github.com/scrapy/scrapy/blob/master/scrapy/utils/python.py#L134) if the function doesn't expect response print a warning. So after couple of versions we can remove that check without breaking people spiders.
",nramirezuy,aufziehvogel
965,2014-12-10 19:19:17,"@nramirezuy Is it then okay to move `Rule` to a higher level and also use it in `SitemapSpider`? because currently `Rule` is defined in `crawl.py` only for the `CrawlSpider`. And I think you mean the `process_links` of `Rule`?

And am I getting you right, that you prefer a solution where the link correction itself is not part of scrapy core, but we only adjust the API so users can implement the correction themselves if they want to?
",aufziehvogel,nramirezuy
965,2014-12-10 19:58:05,"@aufziehvogel Sorry I got confused with `Rule`, `LinkExtractor` and `SitemapSpider` not using either. Promoting `Rule`might be too much work, if you want to do it go ahead. A much simpler to implement approach would be adding another attribute to define link manipulation. 

Example:



You are right.
",nramirezuy,aufziehvogel
964,2015-03-11 14:23:37,"@nramirezuy I didn't realise that behaviour - why is that?
",demelziraptor,nramirezuy
963,2015-03-13 21:49:00,"@tpeng I wonder why are you using this code. We have some evil plans to remove it from Scrapy (see https://github.com/scrapy/scrapy/issues/1063), or to move it to some other place. 

But I'm not familiar enough with xmliter_lxml - is it really better than scrapy.utils.iterators.xmliter? If so, does it make sense to move it to scrapy.utils.iterators? Or maybe even use it by default?
",kmike,tpeng
963,2015-03-17 04:11:35,"@tpeng bump to @kmike question
",pablohoffman,tpeng
963,2015-03-17 04:11:35,"@tpeng bump to @kmike question
",pablohoffman,kmike
963,2015-03-18 15:29:03,"@kmike sorry for the delay. the original reason was i found the `xmliter_lxml` is memory efficient and can work with the xml i used. `scrapy.utils.iterators.xmliter` didn't work if the namespace is not empty. 
",tpeng,kmike
961,2014-12-02 16:16:49,"@nramirezuy he meant #957 
",pablohoffman,nramirezuy
961,2014-12-03 08:36:05,"Yep, you right @ldmberman.
It will be great to have a possibility to know what requests was scheduled, and what requests was dropped.

So, if authors wants to leave **request_scheduled** signal as a main signal, I would suggest to create a third signal: **request_accepted** or **request_passed**, for example.

And the code will be:


",jacob1237,ldmberman
958,2015-04-17 09:28:31,"@kmike 
<code>
sudo pip install Scrapy
</code>
",oleksandr-sumtsov,kmike
958,2015-09-02 01:53:55,"@kmike many thanks, mike ^_^
scrapy is awesome,Thank you for your effort!
",ii0,kmike
956,2014-11-24 07:04:40,"Hi @kmike,

No worries, I'm experimenting with autopep8 and only spent a couple of lazy hours on this. I'll exclude the changes to xlib and process your feedback (based on what you said I have a pretty good feeling about which changes are desired and which aren't). After that, feel free to apply or disregard the PR :)
",jtwaleson,kmike
956,2014-12-17 08:42:28,"@nyov : I completely understand your sentiment about drive-by commits. It would be my first reaction as well. It seems like a quick way to a earn a github reputation quickly without any real effort. This is lame.

Therefore it was with quite some hesitation that I tried to apply semi-automated PEP8 to a couple of repositories to see how the maintainers would respond. (If there was any way of doing that while not gaining a github rep, I would do it. For people who look closely PR's like this one might actually actually reflect badly.) Some maintainers, like you, are reluctant as they already have strong opinions about code formatting & PEP8. In other cases applying PEP8 has been on a to do list for some time, or code was just an organicly grown mess, and PEP8 is a huge improvement. It's hard to tell in advance which is the case, so I took this from the scrapy contributing guidelines: ""A good starting point is to send a pull request on Github"". If you don't mind, let me exlain my stance on PEP8 and commits like this. Just like your comment: this is my opinion, don't take it personally.

As someone who maintains a lot of Python code with multiple contributors at work, I see that there are as many opinions about how to do code formatting as there are people. This leads to uncertainty and chaos: If someone uses style A and touches an existing line in style B, it will feel unnatural to him and chances are this portion of the code will be changed to style A. If this person instead chooses to keep style B, it will be against the settings in his code editor or against the way he writes code naturally. So the result is commits that mix style and functional changes or more effort/frustration while writing code. For the contributors I think having an editor configured to automatically format the code the standard way is a huge win, which adheres to the Python philosophy of ""there should be one way to do it"".



In some cases, like this, I could not agree more with you. This is a very good reason to deviate from the standard guidelines, and in general a line length of > 80 characters is fine. For a lot of other changes in this PR (90%) I just don't have a strong opinion, so in my view it's better to adapt to the standard. Most statements that span multiple lines are ugly anyway, so it's better to adopt the standard way of ugliness, even if you think that it is slightly less ugly.

I see that you care about a proper git history: this is a very good reason against rewrites like this. Presupposing that everyone is on board for a specific code style you can either switch overnight or move gradually in the direction, reformatting whenever code is touched. Both have pros and cons. I prefer overnight, but that's personal.

Some of the changes I like to make are simply to make a better git history possible:



Adding/removing one import in this style leads to a clearer git history.



Same here, especially with the trailing comma after _each_ line.

The separate commits in the PR are because:
- I applied PEP8 in increments, using git diff to see the outcome of each step
- the maintainers can see which aspects of PEP8 they like and which they don't

On the changes to the 3rd party library: that was sloppy, I've reverted that.

In the end it boils down to this: if you don't like it (which is probably the case), feel absolutely, completely, totally free to close this pull request if only because writing this comment has been more effort than the entire change. If by any chance you think it's an improvement, look at it objectively: does it really matter that it's an automated drive-by commit that made the improvement? Code does not have feelings :)
",jtwaleson,nyov
952,2015-04-01 17:02:22,"@Digenis IMHO `.. if six.PY3 else ...` is fine as-is. The PR looks good, but I think it is better to split `test_invalid_xpath` into two tests, one for unicode message and one for ascii-safe message. You can add `assertInvalidXpathHandled` method and call it in two test methods with two different xpaths (unicode and ascii-safe).

For `scrapy.utils.python.unicode_to_str` see https://github.com/scrapy/scrapy/issues/778.
",kmike,Digenis
952,2015-04-02 12:54:43,"@Digenis I don't think we're still supporting 0.22.x. A backport to 0.24.x is more likely, but I'm not sure there will be another 0.24.x release. @dangra @pablohoffman - are there plans to do one more 0.24.x release?
",kmike,Digenis
952,2015-04-04 18:08:00,"> What about @umrashrf's work on #906?

@Digenis What do you mean? Please don't let my PR get in your way if it is! It's a small change so I can cherry-pick it.
",umrashrf,Digenis
952,2015-04-04 20:02:15,"yeah reference is helpful thanks @Digenis
",umrashrf,Digenis
952,2015-04-10 19:39:17,"Thanks @Digenis!
",kmike,Digenis
950,2015-11-09 08:36:03,"Hi @aufziehvogel , I am encountering the same problem. you mentioned that "" check the return value (of the method MailSender.send) to check for errors"" is necessary. But when I tried to get that return value, I got a ""deferred"" object. While I'm not quite familiar with Twisted, can you explain more detailed that how can I get the error information from this object?
Thanks!
",downtown12,aufziehvogel
950,2015-11-09 10:55:55,"@Lazar-T which scrapy version are you using?
I got MaildSender working out of nowhere,
I think when I upgraded to 0.24 or 1.0
",Digenis,Lazar-T
950,2015-11-10 06:11:23,"@Lazar-T Thanks! Great project. Maybe I would try the smtplib later (if the MailSender way failed again..).
And @aufziehvogel Thanks! Scrapy mentions little about it's MailSender object, so going deep into the Twisted maybe a better choice. Hope I can bring some feedback about this issue.
",downtown12,Lazar-T
950,2015-11-10 06:11:23,"@Lazar-T Thanks! Great project. Maybe I would try the smtplib later (if the MailSender way failed again..).
And @aufziehvogel Thanks! Scrapy mentions little about it's MailSender object, so going deep into the Twisted maybe a better choice. Hope I can bring some feedback about this issue.
",downtown12,aufziehvogel
946,2014-11-19 10:13:54,"@kmike this PR has the feature to discard the response earlier base on the Content-Length header. see https://github.com/scrapy/scrapy/pull/946/files#diff-18150b1d259c93bf10bf1d4e5028d753R210 and http://twistedmatrix.com/documents/10.1.0/api/twisted.web.client.Response.html : the txresponse.length will be set by twisted which is the content-length value
",tpeng,kmike
946,2014-11-19 10:17:14,"@tpeng that's great! But if there is such feature it should be tested, including edge cases when Content-Length is set for responses compressed with gzip. There is also an edge case when a large amount of headers is sent; if I'm not mistaken headers size is not included in Content-Length.
",kmike,tpeng
946,2014-11-19 10:39:32,"@kmike regarding the edge case like large amount of headers: this PR only limit the amount of data to transfer of the response body. so it will not count the data in the headers. for the case like the compressed response, since it limits the data to transfer so it the size is counted on the data after compression. i will add some test cases to make it clear. 
",tpeng,kmike
946,2014-11-25 12:30:22,"@tpeng check Travis tests - they fail, it seems because of an older Twisted version. Py33 tests should be also guarded.
",kmike,tpeng
946,2014-11-25 12:36:42,"@kmike according to @dangra the failures are not related to the change:


",tpeng,kmike
946,2014-11-25 12:45:33,"@kmike ok, now i see the failures are different:

for precise builder:
`ImportError: cannot import name GzipEncoderFactory`
this change introduced it, we should be able to fix it.

for the py33 builder:
`ImportError: No module named 'twisted.python.systemd`
no idea how to fix it.
",tpeng,kmike
942,2014-11-30 21:35:54,"@kmike thanks for that, I was facing similiar issue on OSX 10.10 but when I ran `from scrapy.spider import Spider` in python console the deps that were not installed were twisted, w3lib, lxml, cssselect. I installed all these and also did `xcode-select --install` before pip installing lxml, then I was able to import scrapy. 
",sindhus,kmike
942,2014-12-02 06:35:29,"@kmike Hi! I have the same issue. I installed scrapy using pip install scrapy (python27, Windows, 32 bit). When I tried from scrapy.spider import Spider it also shows the error
ImportError: cannot import name log

the file C:\Python27\Lib\site-packages\scrapy\log.py indeed exists and I do not have another module named scrapy.

Thanks!
",jia-11,kmike
942,2014-12-02 09:28:31,"@kmike I just tried to uninstall and re-install scrapy and now it works without error. Many thanks!
",jia-11,kmike
942,2015-03-20 14:38:42,"Hey @kmike I guess it's fine to close this issue for now. The problem is probably solved and was in a previous release. If it occurs again we can reopen this issue. :+1: 
",yasoob,kmike
941,2015-11-24 16:07:21,"Made changes according to feedback.
Tests fail on `precise` because FTP file listing for filenames with spaces was only fixed at a later Twisted version: https://github.com/twisted/twisted/commit/fa2bfda65cb17ab94690ec93647152dfbac4d078
I could subclass `FTPFileListProtocol` with the correct `fileLinePattern` as a fix (tested and works). But I'll wait for your approval, @kmike :


",bernardotorres,kmike
934,2015-04-02 21:50:00,"@Dineshs91 @nyov It works for me when copying the file to either `/usr/share/zsh/vendor-functions/_scrapy_zsh_completion` or `/usr/share/zsh/vendor-completions/_scrapy`. Notice that in the latter case the filename must be `_scrapy`.
",rolando,Dineshs91
934,2015-04-02 21:50:00,"@Dineshs91 @nyov It works for me when copying the file to either `/usr/share/zsh/vendor-functions/_scrapy_zsh_completion` or `/usr/share/zsh/vendor-completions/_scrapy`. Notice that in the latter case the filename must be `_scrapy`.
",rolando,nyov
934,2015-04-03 03:07:18,"@darkrho Thanks for your inputs !

@nyov I have updated the pull request. 
",Dineshs91,nyov
934,2015-04-03 18:39:53,"@Dineshs91 a good catch; `scrapy server` was removed a long time ago (it was a command for starting https://github.com/scrapy/scrapyd).
",kmike,Dineshs91
934,2015-04-03 18:46:34,"@kmike Thanks for the prompt response. 

These commands should be removed from scrapy_bash_completion also.
",Dineshs91,kmike
934,2015-04-03 19:27:24,"@Dineshs91 

> This really helps. I am using a mac and this path (usr/share/zsh/vendor-completions/) doesn't exist, but /usr/share/zsh/site-functions is available.

Don't forget you're adding this to the `debian/` package path here, though. So for this PR, it should follow debian rules. For oh-my-zsh it's a different matter.

And I think no-one would mind if you fix scrapy_bash_completion as part of this ticket.
",nyov,Dineshs91
924,2014-10-30 08:21:22,"Sure thing.  I've actually had these setup in a separate repo for a while.. was just thinking it might be best to keep everything in one place.  @kmike, you want me to just add a backlink in the readme?  I'll do that right now.  If you want to setup a side repo here, by all means; or I've already had [scrapy-spiders](https://github.com/dcondrey/scrapy-spiders) setup for a while.. if you just want to backlink to that.
",dcondrey,kmike
918,2016-03-24 08:25:25,"@kmike For example 1: if work with proxy list. Some proxy return status 502, 503, and other. It do`nt need save. Bat if return status 200 or 404 - save it. In middleware analize responce and Retray request without read in cache.
Example 2: Some site without cookie return simple JS code. It set cookie and refresh page. In callback I parse this JS, get this cookie, and return new Request with it. Response need save.
",tonal,kmike
918,2016-03-24 09:06:50,"For both examples this feature would only serve as a workaround.

However see #1840 and my comment there.
I'd reword part of my comment as follows:

> This is a typical problem where deciding whether
> - ~~to retry a request~~
> - ~~to cache a response~~
> - **a response was a failure**
> 
> is unavoidably tied to spider/parsing logic.

It's very easy to pollute response.meta with reserved keys
if we go on implementing something like this for every middleware
for whatever can go wrong.

I have some ideas on that PR
but for now, your fastest solution would be @nramirezuy's suggestion.
",Digenis,nramirezuy
916,2014-10-08 14:24:18,"@Digenis He is fixing #872 that says it is ignoring [this setting](http://doc.scrapy.org/en/master/topics/feed-exports.html#feed-store-empty)
",nramirezuy,Digenis
914,2014-10-07 12:56:20,"@nramirezuy What? It just removes the `bin` folder and #913 is an issue about removing the bin folder?

I don't think I understand what you are asking? Could you be more clear?
",brunsgaard,nramirezuy
914,2014-10-07 13:12:32,"@nramirezuy  You could achieve the same thing using `python setup.py develop`, right?  
Also, if you did. You would not have to customize `$PYTHONPATH`, which I guess you have modified to make your little setup thingy working. And manually copy over the script. Seems like unnecessary work, when `python setup.py develop` does it all for you.. (and for everybody else that have a similar need) In my opinion this would be the right way to do it these days.. ;)  

Take a look at the docs if you are not acquainted with the [setuptools development mode.](https://pythonhosted.org/setuptools/setuptools.html#development-mode)

@kmike What do you think?
",brunsgaard,nramirezuy
914,2014-10-07 13:53:45,"I second @brunsgaard, not sure I understand why it doesn't work for you @nramirezuy 
",dangra,nramirezuy
914,2014-10-07 13:53:45,"I second @brunsgaard, not sure I understand why it doesn't work for you @nramirezuy 
",dangra,brunsgaard
914,2014-10-09 03:58:59,"@dangra @nramirezuy What is needed to move forward?
",brunsgaard,nramirezuy
914,2014-10-09 03:58:59,"@dangra @nramirezuy What is needed to move forward?
",brunsgaard,dangra
914,2014-10-09 12:21:00,"waiting for @nramirez or other maintainers to share his views. I am not a
user of bin/scrapy myself and I am used to ""pip install -e"" for the
packages I use in develop mode. If time passes I will just merge.

On Thu, Oct 9, 2014 at 1:59 AM, Jonas Brunsgaard notifications@github.com
wrote:

> @dangra https://github.com/dangra @nramirezuy
> https://github.com/nramirezuy What is needed to move forward?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/scrapy/scrapy/pull/914#issuecomment-58460653.
",dangra,nramirezuy
914,2014-10-09 12:21:00,"waiting for @nramirez or other maintainers to share his views. I am not a
user of bin/scrapy myself and I am used to ""pip install -e"" for the
packages I use in develop mode. If time passes I will just merge.

On Thu, Oct 9, 2014 at 1:59 AM, Jonas Brunsgaard notifications@github.com
wrote:

> @dangra https://github.com/dangra @nramirezuy
> https://github.com/nramirezuy What is needed to move forward?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/scrapy/scrapy/pull/914#issuecomment-58460653.
",dangra,dangra
914,2014-10-09 13:35:26,"@dangra merge it if you like, I'll do copies instead of links for `backwards compatibility` (:
",nramirezuy,dangra
914,2014-10-09 16:25:30,"@nramirezuy I don't quite get it: do you symlink `scrapy` binary without installing scrapy in virtualenvs, and this works? Or do you install scrapy in each virtualenv by symlinking its source + the `scrapy` script manually?

We can add support for `python -m scrapy`, so there won't be a need to symlink binaries if Scrapy is installed. But as others said in you have Scrapy installed you already have `scrapy` in a proper place.
",kmike,nramirezuy
914,2014-10-09 17:01:43,"@kmike, My guess is that he has modified `PYTHONPATH` to include a version of scrapy. This way he can avoid installing scrapy in each virtualenv, he only needs to copy the binary in.
",brunsgaard,kmike
914,2014-10-09 17:02:03,"@kmike I make a egg link to the source with `add2virtualenv` from `virtualenvwrapper` and then I create a symbolic link in the virtualenv bin folder to the binary that is in the source.
",nramirezuy,kmike
914,2014-10-09 17:09:28,"@nramirezuy looks like if you use `python setup.py develop` instead of `add2virtualenv` it will just work.
",kmike,nramirezuy
914,2014-10-09 17:10:45,"@brunsgaard AFAIK virtualenv overwrites PYTHONPATH, so modifications to it are discarded when you activate the virtualenv.
",kmike,brunsgaard
914,2014-10-09 17:15:48,"@dangra The directory wasn't removed, just the file inside it (:
",nramirezuy,dangra
914,2014-10-09 17:22:15,"@nramirezuy git doesn't track folders, so you can't remove a directory by a commit. If you remove your local copy and clone the repo from scratch `bin` folder won't be there.
",kmike,nramirezuy
914,2014-10-09 18:36:32,"> @dangra The directory wasn't removed, just the file inside it (:
> @nramirezuy git doesn't track folders, so you can't remove a directory by a commit. If you remove your local copy and clone the repo from scratch bin folder won't be there.

or run `git clean -dfq` 
",dangra,nramirezuy
914,2014-10-09 18:36:32,"> @dangra The directory wasn't removed, just the file inside it (:
> @nramirezuy git doesn't track folders, so you can't remove a directory by a commit. If you remove your local copy and clone the repo from scratch bin folder won't be there.

or run `git clean -dfq` 
",dangra,dangra
913,2014-10-07 12:25:51,"@kmike made a trivial PR for you :)
",brunsgaard,kmike
912,2014-10-07 04:48:34,"-0 to add this feature; I agree with @dangra. While relative imports support in load_object could make copy-paste easier, and copy-paste is sometimes the best form of reuse, the gains are small (one sed call per each new project that uses copy-pasted code); I think the advantages don't outweight the disadvantages. Disadvantages are:
- more complex implementation (+test & docs are missing in this PR);
- scrapy project creeps into places it was not required before;
- this encourages copy-paste :)
",kmike,dangra
911,2014-10-03 21:55:43,"@kmike I found it while researching your answer in the scrapylib PR, so bonus goes to you ;)
",nyov,kmike
910,2014-10-07 12:17:48,"@brunsgaard I agree with you ;)
",daveoncode,brunsgaard
908,2014-10-07 12:49:10,"@brunsgaard as you wish... I hope that now it's clear
",daveoncode,brunsgaard
906,2015-01-07 10:18:01,"@dangra @kmike I'm interested to do it.

I think there are two steps to the PR.
- First step is to create a separate package with working selectors in its own repository at github.com/scrapy/selectors
  I have successfully untied Selector code from Scrapy and have created separate package. I made some tests and It is working fine locally. I completely removed response caching from LxmlDocument in hope to implement it at Scrapy side.
- Second part is replacing Scrapy selectors with new library
  Here I need to dig how to use new selectors with response object efficiently having same kind of caching.
",umrashrf,dangra
906,2015-01-07 10:18:01,"@dangra @kmike I'm interested to do it.

I think there are two steps to the PR.
- First step is to create a separate package with working selectors in its own repository at github.com/scrapy/selectors
  I have successfully untied Selector code from Scrapy and have created separate package. I made some tests and It is working fine locally. I completely removed response caching from LxmlDocument in hope to implement it at Scrapy side.
- Second part is replacing Scrapy selectors with new library
  Here I need to dig how to use new selectors with response object efficiently having same kind of caching.
",umrashrf,kmike
906,2015-01-27 14:32:25,"@Digenis good point. I will test this with temporary selectors repo and see how it goes. 
",umrashrf,Digenis
906,2015-01-28 16:03:36,"@Digenis I understand filter-branch will only bring in history to some extent. So the advantage of using your approach over filter-branch is that people get to see where did Selectors repo come from right? If I fork Scrapy and cut it short to Selectors then it answers that question.

> We should be able on an empty repo to add and fetch scrapy as a remote
> and after checking out some specific reference, run this list of commands
> to create the individual repo.

Why is that required? Why would someone like to do that if there already exists Selectors repo?
",umrashrf,Digenis
906,2015-01-28 16:59:10,"I second @Digenis on this, I splitted several repositories in the past and it never was in a single pass so I found the best way is to write a script (bash) to rerun on each debug iteration.

Once you wrote the script it is simple to make it public.
",dangra,Digenis
906,2015-02-12 15:31:05,"My pleasure @kmike 
",umrashrf,kmike
906,2015-03-18 14:51:54,"@nyov I think they are okay with history provided by git-filter-branch. They need the script for historical reasons. The script will be outdated for sure but at any point in future they can see how was it brought into existence.

After writing script, I found that it's indeed better approach to split the repo as you don't need to keep tabs on everything in your head :)
",umrashrf,nyov
906,2015-03-18 15:29:32,"So @Digenis are you happy with my current approach to split it? Can you leave your comments at https://github.com/scrapy/scrapy/pull/1007. I pushed script with patches so you can see https://github.com/umrashrf/scrapy/commit/dc6565d989a08756335dcbd514e369fd0a78c274.
",umrashrf,Digenis
906,2015-03-19 13:03:40,"I think you make good points @nyov. I will play with git-replace over the weekend.

@Digenis @dangra @kmike what do you think about this approach?
",umrashrf,nyov
906,2015-03-19 13:03:40,"I think you make good points @nyov. I will play with git-replace over the weekend.

@Digenis @dangra @kmike what do you think about this approach?
",umrashrf,dangra
906,2015-03-19 13:03:40,"I think you make good points @nyov. I will play with git-replace over the weekend.

@Digenis @dangra @kmike what do you think about this approach?
",umrashrf,Digenis
906,2015-03-19 13:03:40,"I think you make good points @nyov. I will play with git-replace over the weekend.

@Digenis @dangra @kmike what do you think about this approach?
",umrashrf,kmike
906,2015-03-29 18:00:13,"thanks @nyov, it makes sense. My concern was if git-replace will handle the moves like `scrapy/selector/ -> selectors/` and `scrapy/utils -> selectors/utils` without having parent commit. I need to test this and will update the repo shortly.
",umrashrf,nyov
906,2015-03-29 20:02:25,"@nyov please review it when you can https://github.com/umrashrf/selectors/commits/selectors
",umrashrf,nyov
906,2015-04-02 11:02:50,"@nyov done those changes. Now only docs are pending.
",umrashrf,nyov
906,2015-04-02 16:30:11,"@kmike, I don't really like to write this again, it already feels like I was pushing for this choice, which wasn't my intent. I wasn't thinking about github's blame not working with this choice.

My reason for suggesting git replace is that I think it keeps the history here clean and honest.
If we include a copy of a rewritten history here, it's a partial duplicate to scrapy's and potentially not quite right (from dropping files in the rewrite).

In summary, if scrapy.git would be rewritten afterwards to remove selectors history, moving it here would feel right, to me. Since it isn't and scrapy.git is sticking around for the foreseeable future, where you can look it up, duplicating some mangled history (other sha1 sums, commit messages not quite fitting since files are missing from commits) felt not quite right, to me.

That is my opinion, which I hoped to explain, nothing else. Feel free to disagree and whatever works best for everyone else is good enough for me.
",nyov,kmike
906,2015-04-04 17:51:14,"> Sorry, I don't understand all this git voodoo :) But in https://github.com/umrashrf/selectors/tree/master it is possible to check the history of each file using github interface - it shows all relevant commits, ""blame"" works. In ""selectors"" branch it doesn't work.

@kmike what about creating a separate branch at Selectors repo with full history? I mean the result of git-replace. Call that separate branch **history** or anything.
",umrashrf,kmike
906,2015-04-04 19:28:23,"> My reason for suggesting git replace is that I think it keeps the history here clean and honest.
> If we include a copy of a rewritten history here, it's a partial duplicate to scrapy's and potentially not quite right (from dropping files in the rewrite).
> 
> In summary, if scrapy.git would be rewritten afterwards to remove selectors history, moving it here would feel right, to me. Since it isn't and scrapy.git is sticking around for the foreseeable future, where you can look it up, duplicating some mangled history (other sha1 sums, commit messages not quite fitting since files are missing from commits) felt not quite right, to me.

What practical problems are caused by unclean dishonest duplicated history, changed sha1 sums and not-quite-fitting commit messages? Unfitting commit messages may cause some confusion, but IMHO they are better than nothing. Having full history can save some time on tracking the code changes, allow people to use github UI and local git without any setup, it also preserves contribution history.

> @kmike what about creating a separate branch at Selectors repo with full history? I mean the result of git-replace. Call that separate branch history or anything.

I think that a separate branch won't help much once we start making more changes to selectors; one can always use Scrapy repository to see old changes.
",kmike,kmike
906,2015-04-04 19:56:23,"> Yes we can write a script which can update that separate branch. The advantage is it gives full selectors history in github UI. 

This adds complexity and thus makes maintenance harder. It is better to agree on one of the options, be it ""drop history before relocation, but provide instructions on how to get it back"" or ""provide pseudo-history"".

Sorry, I still don't get what is clean about dropping the history and why do you like it :) As @Digenis said, ""git blame gives you what you couldn't find in the code comments"", and pseudo-history gives you a meaningful git blame - it is a practical advantage. There must be something, because it seems so far there is +2 or +3 ( @umrashrf, @nyov and @Digenis?) for dropping pseudo-history vs +1 (me) for providing it.
",kmike,nyov
906,2015-04-04 19:56:23,"> Yes we can write a script which can update that separate branch. The advantage is it gives full selectors history in github UI. 

This adds complexity and thus makes maintenance harder. It is better to agree on one of the options, be it ""drop history before relocation, but provide instructions on how to get it back"" or ""provide pseudo-history"".

Sorry, I still don't get what is clean about dropping the history and why do you like it :) As @Digenis said, ""git blame gives you what you couldn't find in the code comments"", and pseudo-history gives you a meaningful git blame - it is a practical advantage. There must be something, because it seems so far there is +2 or +3 ( @umrashrf, @nyov and @Digenis?) for dropping pseudo-history vs +1 (me) for providing it.
",kmike,Digenis
906,2015-04-04 20:20:46,"> Sorry, I still don't get what is clean about dropping the history and why do you like it :)

I don't like dropping the history and neither do they. git-filter-branch has a good chance to break git-blame usefulness by giving you partial history. I also wondered how and talked this through with @nyov on IRC so I'm just going to paste them here. I was going to paste it before but sorry I got lost in other stuff.



So based on these points what do you think @kmike?
",umrashrf,nyov
906,2015-04-04 20:20:46,"> Sorry, I still don't get what is clean about dropping the history and why do you like it :)

I don't like dropping the history and neither do they. git-filter-branch has a good chance to break git-blame usefulness by giving you partial history. I also wondered how and talked this through with @nyov on IRC so I'm just going to paste them here. I was going to paste it before but sorry I got lost in other stuff.



So based on these points what do you think @kmike?
",umrashrf,kmike
906,2015-04-04 20:32:58,"ah, that stupid irc log. :p 

@kmike, I disagree on ""it also preserves contribution history"", at least commit author and dates are overwritten. But count me as +/-0. Both approaches have dis/advantages, I'm not sure which to favor anymore in this case.
",nyov,kmike
906,2015-04-06 15:04:37,"> I disagree on ""it also preserves contribution history"", at least commit author and dates are overwritten.

@nyov https://github.com/umrashrf/selectors/commits/master shows right authors and original commit dates (""authored on""), while preserving information about the fact Umair made these commits; https://github.com/umrashrf/selectors/graphs/contributors displays correct contributors and graphs with original (""authored on"") commit dates. My git foo is weak, but author/commiter and ""authored on""/ ""commited on"" distinction sounds right, and github supports it well enough.
",kmike,nyov
906,2015-04-07 16:57:51,"Let me put the lid on this discussion. It seems I sowed dissent by making my viewpoint known here. For that I am sorry, I shall try to wrap it up.
If taken as a precedent case, this currently looks like a blocker for the `selectors`, `scrapyd-client` and upcoming `scrapy-djangoitem` repos and that is bad. @dangra put his foot down, @kmike agrees, I'll follow. I converted @Digenis and @umrashrf ;) I guess, but I assume they'll follow, too?

@dangra also said to keep commiter stuff (and merges too). That's currently not the case here as I showed, but is possible. That makes part of my argument invalid, so let's do this.
[`scrapy-jsonrpc`](https://github.com/scrapy/scrapy-jsonrpc) seems the actual precedent case with included history (I hadn't noticed before), and it looks like dangra [handled it then](https://github.com/scrapy/scrapy-jsonrpc/commit/d3d2abecd489968a7aba38410ac067a2d7307689), in a way that kept merges and committer information AFAICS.

It's possible using the index filter, my apologies for saying it wouldn't be that trivial -- I must have lost some gitfoo myself. It's only hard to figure out all the right files to keep.
@umrashrf, I feel bad for all the work you did already, if I should pitch in, let me know!
Here's a quick way to kill everything _but_ the files you want to keep, but it requires listing all the possible paths that those files are on, in history (renames and moves included).



The inverse, eradicating the listed files from history, can be done like this:



Using both in tandem, and maybe repeatedly, it's possible to whittle down the tree and commits to only the stuff to keep.
",nyov,dangra
906,2015-04-07 16:57:51,"Let me put the lid on this discussion. It seems I sowed dissent by making my viewpoint known here. For that I am sorry, I shall try to wrap it up.
If taken as a precedent case, this currently looks like a blocker for the `selectors`, `scrapyd-client` and upcoming `scrapy-djangoitem` repos and that is bad. @dangra put his foot down, @kmike agrees, I'll follow. I converted @Digenis and @umrashrf ;) I guess, but I assume they'll follow, too?

@dangra also said to keep commiter stuff (and merges too). That's currently not the case here as I showed, but is possible. That makes part of my argument invalid, so let's do this.
[`scrapy-jsonrpc`](https://github.com/scrapy/scrapy-jsonrpc) seems the actual precedent case with included history (I hadn't noticed before), and it looks like dangra [handled it then](https://github.com/scrapy/scrapy-jsonrpc/commit/d3d2abecd489968a7aba38410ac067a2d7307689), in a way that kept merges and committer information AFAICS.

It's possible using the index filter, my apologies for saying it wouldn't be that trivial -- I must have lost some gitfoo myself. It's only hard to figure out all the right files to keep.
@umrashrf, I feel bad for all the work you did already, if I should pitch in, let me know!
Here's a quick way to kill everything _but_ the files you want to keep, but it requires listing all the possible paths that those files are on, in history (renames and moves included).



The inverse, eradicating the listed files from history, can be done like this:



Using both in tandem, and maybe repeatedly, it's possible to whittle down the tree and commits to only the stuff to keep.
",nyov,Digenis
906,2015-04-07 16:57:51,"Let me put the lid on this discussion. It seems I sowed dissent by making my viewpoint known here. For that I am sorry, I shall try to wrap it up.
If taken as a precedent case, this currently looks like a blocker for the `selectors`, `scrapyd-client` and upcoming `scrapy-djangoitem` repos and that is bad. @dangra put his foot down, @kmike agrees, I'll follow. I converted @Digenis and @umrashrf ;) I guess, but I assume they'll follow, too?

@dangra also said to keep commiter stuff (and merges too). That's currently not the case here as I showed, but is possible. That makes part of my argument invalid, so let's do this.
[`scrapy-jsonrpc`](https://github.com/scrapy/scrapy-jsonrpc) seems the actual precedent case with included history (I hadn't noticed before), and it looks like dangra [handled it then](https://github.com/scrapy/scrapy-jsonrpc/commit/d3d2abecd489968a7aba38410ac067a2d7307689), in a way that kept merges and committer information AFAICS.

It's possible using the index filter, my apologies for saying it wouldn't be that trivial -- I must have lost some gitfoo myself. It's only hard to figure out all the right files to keep.
@umrashrf, I feel bad for all the work you did already, if I should pitch in, let me know!
Here's a quick way to kill everything _but_ the files you want to keep, but it requires listing all the possible paths that those files are on, in history (renames and moves included).



The inverse, eradicating the listed files from history, can be done like this:



Using both in tandem, and maybe repeatedly, it's possible to whittle down the tree and commits to only the stuff to keep.
",nyov,kmike
906,2015-04-07 17:33:24,"@nyov no no, i'm glad to do it. thanks for teaching me more git!
",umrashrf,nyov
906,2015-08-26 18:21:44,"@Digenis some of them should, but there is still scrapy.Selector which wraps parsel.Selector. Do you have specific tickets in mind?
",kmike,Digenis
905,2014-10-16 08:36:24,"@dangra +1
",birla,dangra
905,2015-08-20 19:14:22,"I wish all our customers could follow this your advice @nramirezuy :)
",qrilka,nramirezuy
900,2014-09-23 18:13:16,"@chekunkov Insensitive in the arguments or the path


",nramirezuy,chekunkov
900,2014-09-23 18:23:53,"@nramirezuy in one project I wanted fingerprint to be calculated for the url with lowercased path with query and fragments removed
",chekunkov,nramirezuy
900,2014-09-23 18:36:31,"> 4. add a settings.py option to override request fingerprint function globally.

@kmike +1
",chekunkov,kmike
900,2014-09-23 18:42:42,"@kmike In HCF fingerprint generation is different, I think we are in troubles already.
",nramirezuy,kmike
900,2014-09-23 18:52:16,"@chekunkov I also like (4) most. Probably we will have to undo/change some parts of https://github.com/scrapy/scrapy/pull/597. 

@nramirezuy you're right; HCF middleware in scrapylib should probably use the same fingerprint function as other Scrapy parts. This is not a Scrapy issue though.
",kmike,nramirezuy
900,2014-09-23 18:52:16,"@chekunkov I also like (4) most. Probably we will have to undo/change some parts of https://github.com/scrapy/scrapy/pull/597. 

@nramirezuy you're right; HCF middleware in scrapylib should probably use the same fingerprint function as other Scrapy parts. This is not a Scrapy issue though.
",kmike,chekunkov
900,2014-09-23 18:59:12,"@kmike @nramirezuy can you explain ""why we are in troubles already"" by example? For HCF and dupefilter+cache, because it isn't clear enough for me.
",chekunkov,nramirezuy
900,2014-09-23 18:59:12,"@kmike @nramirezuy can you explain ""why we are in troubles already"" by example? For HCF and dupefilter+cache, because it isn't clear enough for me.
",chekunkov,kmike
900,2014-09-23 19:03:20,"@chekunkov by default HCF middleware uses url as a fingerprint. If you want to vary on something else (e.g. take a header in account) some requests will be dropped by HCF - you put them to queue but never get them back.
",kmike,chekunkov
900,2014-11-20 00:11:42,"for the record: I agree with @nramirezuy and also think option (2) is better.
",kmike,nramirezuy
900,2014-12-01 14:44:06,"@kmike Few questions:
- When the hash is calculated? 
- Are we going to keep `func:request_fingerprint` or the hash will be calculated inside the `class:Request`?
- `Request.fingerprint` will allow us to change the hash calculation behavior? How?
",nramirezuy,kmike
900,2014-12-01 15:25:57,"@nramirezuy 

> When the hash is calculated?

A good question. I'd say on first access to `Request.fingerprint_hash` attribute / function / (whatever name will it use). Also, in order to prevent errors after the first access to `fingerprint_hash` `Request.fingerprint` should become immutable.

> Are we going to keep func:request_fingerprint or the hash will be calculated inside the class:Request?

The hash will be calculated inside Request class, but we'll keep `request_fingerprint` for backwards compatibility and for `include_headers` option.

> Request.fingerprint will allow us to change the hash calculation behavior? How?

It will allow us to change hash calculation algorithm only by subclassing. This limits what can be done, but I don't see use cases when a global change is needed, and if there are such use cases it can be fixed later.
",kmike,nramirezuy
900,2014-12-01 16:22:47,"@nramirezuy The advantage is that you can globally add/remove/change keys/values to be hashed. You can't globally swap sha1 to md5, that's what I don't see an use case for.
",kmike,nramirezuy
900,2014-12-01 17:50:30,"We can do the fix in 2 steps:
- Add a warning telling people to enable certain setting. ""Add FINGERPRINT_OLD =True in project your settings if you want to keep current fingerprint calculation approach, in next version it will be changed and will affect data caches and resumed runs"".
- Next version if you don't have that setting new way is used by default.

I think `Request().meta['fingerprint']` should be the way of using overriding fingerprint globally. As @redapple said before by forcing `request_fingerprint` to return that value if present. I just see the problem on moving from one to another and adding a bunch of stuff to request doesn't seem to be the best approach.
",nramirezuy,redapple
900,2014-12-01 18:21:23,"> Add a warning telling people to enable certain setting. ""Add FINGERPRINT_OLD =True in project your settings if you want to keep current fingerprint calculation approach, in next version it will be changed and will affect data caches and resumed runs"".

I'm OK with this option, but users can keep old behaviour by using Scrapy 0.24.4 - why is an extra setting needed? Scrapy resumed runs don't use request fingerprints, so they shouldn't be affected.

> I think Request().meta['fingerprint'] should be the way of using overriding fingerprint globally. As @redapple said before by forcing request_fingerprint to return that value if present. I just see the problem on moving from one to another and adding a bunch of stuff to request doesn't seem to be the best approach.

Downsides of meta[""fingerprint""]:
- ""ignore case in URLs"" middleware is impossible to implement because there is no way to change only ""url"" component (consider that fingerprint may be already modified by another middleware);
- users must be careful copying meta when creating new requests: if they change some Request parameters and keep (or copy) meta[""fingerprint""] the request will be filtered out. This is a problem for `Request.replace()` or for manually constructed requests which pass some data through meta. It was also discussed here: https://github.com/scrapy/scrapy/issues/126. But the data in a Request attribute has the same issue with `.replace()`..
- it is slightly less efficient because each component that modifies a fingerprint will have to run hashing algorithm, something like `fp = sha1(); fp.update(meta.get('fingerprint', """") + extra); meta['fingerprint'] = fp.hexdigest()` - in case of a dict sha1 will be run once;
- more code for users to write, and more user errors: an obvious and short `meta[""fingerprint""] = new_fingerprint` in user code is likely wrong - it doesn't take a previous fingerprint in account.

I like that `meta[""fingerprint""]` allows us to make less changes to Request, but request fingerprint is an important concept used by many Scrapy components, so I'm also ok with adding it as an attribute to the base Request class.
",kmike,redapple
898,2014-10-03 20:16:07,"LGTM but it doesn't merge cleanly anymore, could you rebase @kmike?
",pablohoffman,kmike
897,2014-09-22 21:31:54,"@kmike, i dont think there is unless another update. But to me, passing variable starts with '@' is not practical.

After having passed '@qwe=sda' and breaking at **init** of the spider.



@chekunkov `-A` could be used well, but i think its not intuitive and does not worth to add this feature as another parameter option, if using '@' is the really matter here, supporting that character as `-a '\@param=val'` makes better sense to me. Curl should have a reason of supporting both option (key=val and file) at the same parameter.
",sardok,chekunkov
897,2014-09-22 21:31:54,"@kmike, i dont think there is unless another update. But to me, passing variable starts with '@' is not practical.

After having passed '@qwe=sda' and breaking at **init** of the spider.



@chekunkov `-A` could be used well, but i think its not intuitive and does not worth to add this feature as another parameter option, if using '@' is the really matter here, supporting that character as `-a '\@param=val'` makes better sense to me. Curl should have a reason of supporting both option (key=val and file) at the same parameter.
",sardok,kmike
897,2014-09-23 09:53:02,"@chekunkov some more examples, [gcc](http://linux.die.net/man/1/gcc), [ntop](http://www.ntop.org/wp-content/uploads/2011/09/ntop-man.html) , [climate](https://pypi.python.org/pypi/climate/0.2.0), [ninja](https://github.com/martine/ninja/issues/53), also quick search revealed some more examples from windows platform; fractint, AppAsure, uniPaas, dtscli. These applications have different functionality but same motivation in regard to @filename syntax which is, long parameters.
Regarding letter choice, i think it is matter of personal taste.
",sardok,chekunkov
891,2014-09-22 13:08:51,"@kmike Because is the same, but shorter.
",nramirezuy,kmike
887,2014-09-23 17:45:38,"Whats the difference between 'infinite' and 1 hour when you are trying to fetch a page?
@nyov do you have any real life use case?
",nramirezuy,nyov
884,2014-10-03 13:54:14,"@nramirezuy this was my first reaction as well, but 
- this feature is quite easy to support, and I can't find an argument why is this feature bad. My only argument against it is that changing setting dynamically is not supported. But the thing is that changing `spider.download_delay` _is_ already supported - Downloader respects changes to this attribute.
- by changing `spider.download_delay` you change delay for all existing requests; I'd expect `Request.meta` to work for a single request only, so they are not the same.
",kmike,nramirezuy
884,2014-10-03 15:22:13,"@kmike Downloader doesn't work because Autothrottle overrides the slot on every response.
",nramirezuy,kmike
884,2014-10-03 16:33:02,"@nramirezuy exactly :) Without autothrottle it works, with authrottle it doesn't work; one way to fix it is to fix autothrottle.
",kmike,nramirezuy
884,2014-10-06 17:17:56,"> this feature is quite easy to support, and I can't find an argument why is this feature bad. My only argument against it is that changing setting dynamically is not supported. But the thing is that changing spider.download_delay is already supported - Downloader respects changes to this attribute.

@kmike: this is not true, Downloader reads `spider.download_delay` attribute only the first time the download slot is created, from there it always respects `slot.delay` which is different. 
",dangra,kmike
884,2014-10-07 04:35:17,"@dangra ah, right - changes to `spider.download_delay` are only applied for new slots. This is a good reason not to support this feature. Now I want a public API for download slots.
",kmike,dangra
884,2014-10-07 16:22:14,"> Now I want a public API for download slots.

@kmike This is an interesting idea, any draft ideas on what the API should provide? 
",dangra,kmike
884,2014-10-07 17:06:17,"I think @kmike is talking about a public API, one we can document and people use.
",nramirezuy,kmike
883,2014-09-14 11:18:25,"@kmike thanks for clearing my doubts and I also agree with your points
I think expiring cache is good idea, and maybe small tweaks to save number of files, as there are 6 files (meta  pickled_meta  request_body  request_headers  response_body  response_headers) now.

In the end, maybe it's better to use database and save/ease the trouble, if cache are expected to be large. If LevelDB is neither stable nor efficient, how about DBM?

Thank you!
",larryxiao,kmike
883,2014-09-17 00:41:35,"Thanks @nyov! It's important to understand this.
I'm now running with DbmCache, let's see how it goes.

@kmike, I like some ideas here: https://bugzilla.mozilla.org/show_bug.cgi?id=674210



I didn't find DB with expiration built-in, though I think there should be.
- update
  I see mongodb http://docs.mongodb.org/manual/tutorial/expire-data/, it needs manually set TTL.
",larryxiao,nyov
883,2014-09-17 00:41:35,"Thanks @nyov! It's important to understand this.
I'm now running with DbmCache, let's see how it goes.

@kmike, I like some ideas here: https://bugzilla.mozilla.org/show_bug.cgi?id=674210



I didn't find DB with expiration built-in, though I think there should be.
- update
  I see mongodb http://docs.mongodb.org/manual/tutorial/expire-data/, it needs manually set TTL.
",larryxiao,kmike
881,2014-09-11 17:33:06,"Thanks @dangra I will be checking at this bug, and making a pull request.
",rocioar,dangra
881,2014-09-11 17:33:37,"thank @rocioar.
",dangra,rocioar
881,2014-09-14 19:52:15,"Seems like the spider given to the `log.msg` call is a class instead of an instance (otherwise its crawler attribute should be present).

The condition `crawler and (not spider or spider.crawler is not crawler)` filters spider instances (that's why it checks for `spider.crawler`) that don't belong to the crawler in the log observer. I didn't check thoroughly but I think @dangra's version is interchangeable with the current one, but it won't trigger this issue's exception because it doesn't make assumptions on the spider argument, it just checks if it's the same as `crawler.spider` or not.

Since it's a more robust condition I'd rather replace the current one with @dangra's, but we should check why there is a spider class instead of an instance of it passed to the `log.msg` function in the first place. How is this bug reproduced?
",curita,dangra
881,2014-10-06 18:51:04,"@rocioar have you found the reason for calling log.msg with a spider class instead of an instance?
",dangra,rocioar
878,2014-09-20 23:24:39,"Thanks @andrewshir!
",kmike,andrewshir
873,2014-09-03 13:57:46,"@kmike I think CrawlerRunner is there to support multiples Crawlers some day. Is most of all to use it as a library, scrapy.cmd will always support a single crawler. 
1. Rename `spiders` to `spidermanager`. +1

2, 3, 4. I'm agree on this, it will allow a more easy usage from a script.
- Find a better name for CrawlerProcess and if possible do not extend CrawlerManager. -> `CrawlerManager` using an instance of the `CrawlerProcess`.  Isn't `Process` there because of a twisted interface?

Basically @dangra is proposing a mutation of `CrawlerRunner` to `CrawlerManager`.
",nramirezuy,dangra
873,2014-09-03 13:57:46,"@kmike I think CrawlerRunner is there to support multiples Crawlers some day. Is most of all to use it as a library, scrapy.cmd will always support a single crawler. 
1. Rename `spiders` to `spidermanager`. +1

2, 3, 4. I'm agree on this, it will allow a more easy usage from a script.
- Find a better name for CrawlerProcess and if possible do not extend CrawlerManager. -> `CrawlerManager` using an instance of the `CrawlerProcess`.  Isn't `Process` there because of a twisted interface?

Basically @dangra is proposing a mutation of `CrawlerRunner` to `CrawlerManager`.
",nramirezuy,kmike
873,2014-09-03 15:13:32,"@nramirezuy You're right that CrawlerRunner is a documented way to run Crawlers, I've missed that. 

It  is documented like this:



If I'm not mistaken, this is how it can be written without CrawlerRunner:


",kmike,nramirezuy
873,2014-09-03 15:31:06,"@kmike: because CrawlerRunner also

1- Creates the Crawler after merging spider settings 
2- It instantiate the spidermanager and holds a reference
3- It implements the correct logic to wait for all crawlers (`.join()`)
4- it implements the logic to stop all running crawlers.

I think above are common enough to be grouped into a single helper object.
",dangra,kmike
873,2014-09-07 22:28:35,"There are a lot of suggestions, I'll try to summarize them:

**CrawlerRunner:**
- _Rename references to SPIDER_MANAGER_CLASS instance from spiders to spidermanager_
  Yes, it's kind of misleading otherwise.
- _Rename CrawlerRunner to CrawlerManager for consistency with Spider->SpiderManager_
  Both are equally bad for different reasons :P I like CrawlerRunner slightly more but I don't have a strong preference.
- _Remove CrawlerRunner.spiders_
  It's true that loading spiders isn't necessary a task for CrawlerRunner, it could be somewhere else, but I think keeping it there simplifies the user API by saving an additional step. SpiderManager assumes there is a project where to look spiders from (at least if the user didn't provide a different manager, which is something possible), but if it's not there, it doesn't interfere in providing spider instances directly.
- _Correctly detect when all crawlers finished so reactor can be stopped (new public method CrawlerManager.join())_
  Agree, it should return a deferred list with the `crawl_deferreds` (Like it's done in the `_start_reactor` method in CrawlerProcess).
- _Allow passing Crawler instances directly to CrawlerManager.crawl()_
  +1, but crawlers created outside CrawlerRunners won't have their spider settings populated at initialization time if not done manually, it should be documented to avoid unexpected behavior maybe.
- _Move crawler logging initialization to crawler class_
  +1

**CrawlerProcess:**
- _Find a better name for CrawlerProcess and if possible do not extend CrawlerManager_
  +1, and I like @nramirezuy's idea of using an instance of CrawlerRunner in CrawlerProcess instead of inheriting it.
- _Move CrawlerProcess to another module, it is only used by the command line so far_
  If we change its inheritance of CrawlerRunner, I agree, since there will be no strong dependency and it's true that is more of a helper of the ScrapyCommands and command line.

**SpiderManager:**
- _Rename SpiderManager to SpiderLoader or SpiderFinder to make it clear_
  I'll go with SpiderLoader since it actually has a `load` method, but I think both options are more descriptive than the current name is.
- _Use SPIDERMANAGER_CLASS (missing one underscore) to point to implementer of the new interface_
  Not exactly sure about this, I prefer keeping its name and provide a meaningful error if the spider manager interface doesn't respect the new one.
- _Show a more explanatory error message when SPIDER_MANAGER_CLASS is set (there is not support for old interface)_
  Yes, it should be checked somewhere that the manager adapts the zope.interface. (Maybe when loading it in CrawlerRunner?)
",curita,nramirezuy
873,2014-09-14 20:26:39,"@nramirezuy I don't know how much granularity on configuration we want, but it's something possible. There's an issue about that on the spider settings although. If settings are overridden in the `spider.from_crawler` method, they won't be available when setting other components while creating crawlers, since spiders are instantiated later in `crawler.crawl`. Actually, we could avoid what I mentioned about spider settings not being populated when creating crawlers outside CrawlerRunners by moving this population to the `crawler.init` method. If we add another configuration level we should consider carefully when and for what components these new settings will be available.
",curita,nramirezuy
869,2014-08-30 16:33:00,"Hello @kmike,

Well, I don't know neither, I wasn't digging too much into it, cuz I didn't want to overload server. Maybe with cookiejar it would be possible also. Basically what I needed was to log in with around hundred of different accounts and then do scraping. I found this quite clean solution for this.
",madvas,kmike
869,2016-09-21 10:37:32,"@kmike 
I use cookiejar for this kind of situationm, but it seems that you cannot specify CONCURRENT_REQUESTS and DOWNLOAD_DELAY per cookiejar. 
",jmaynier,kmike
869,2016-09-21 12:07:30,"Thanks @kmike, it seems to be exactly what I was looking for ! I will try it right away :-)
It should be part of scrapy documentation.
",jmaynier,kmike
861,2014-10-29 21:31:00,"@pablohoffman Know if we keep this approach or we move to iterparse.
",nramirezuy,pablohoffman
858,2014-08-15 14:44:56,"@kmike I didn't work that much on implementation, and even less about threads, is more about the concept. I don't think it has unexpected consequences, because you can't have more than one open context, the only way is if you call `.__enter__()` manually and in that case, handle it by yourself.
",nramirezuy,kmike
858,2014-08-15 16:41:07,"@kmike more appropriate implementation, I would like to remove the global reference but I'm not quite sure yet.

https://gist.github.com/nramirezuy/3780bb83e87d4cf79227


",nramirezuy,kmike
858,2016-09-14 11:36:47,"I'm afraid I'm not getting the big picture here.
Is there still interest in this @nramirezuy , @Digenis , @kmike ?
If so, I believe this should be moved to scrapy/parsel
",redapple,nramirezuy
858,2016-09-14 11:36:47,"I'm afraid I'm not getting the big picture here.
Is there still interest in this @nramirezuy , @Digenis , @kmike ?
If so, I believe this should be moved to scrapy/parsel
",redapple,Digenis
858,2016-09-14 11:36:47,"I'm afraid I'm not getting the big picture here.
Is there still interest in this @nramirezuy , @Digenis , @kmike ?
If so, I believe this should be moved to scrapy/parsel
",redapple,kmike
856,2014-08-18 20:33:20,"@kmike Unfortunately the problem described in #396 seems to exist in all IPythons (I've confirmed it in 0.12.1, 1.0 and 2.2 too).
",eliasdorneles,kmike
856,2014-08-19 02:23:26,"LGTM.

I will leave pushing the merge button honors to @kmike just in case he wants to comment before merging  :)
",dangra,kmike
849,2014-08-11 14:52:53,"Hi @dangra , I managed to understand where the error came from.
I was trying to use the new Django 1.7 setup() function to import my settings from within scrapy settings.



It created some conflicts (like an infinite loop or something like this) and the solution was for me to use django.setup() outside scrapy's settings.py.
",tanzaho,dangra
845,2016-03-06 18:05:42,"@redapple I'm using v1.0.5. I'm not sure about the queuelib version. it came with the scrapy. I solved my problem by doing a fresh start and copying the previous `requests.seen` file to the new one.
",asuraphel,redapple
843,2014-08-11 13:47:35,"You are right @dangra , it should therefore for `:first`, `:last` and `:nth()` (`:nth-child()` is already defined http://www.w3.org/TR/selectors/#nth-child-pseudo)

http://www.w3.org/TR/selectors/#grammar
",redapple,dangra
843,2014-08-14 14:57:11,"@redapple what's your opinion on pyquery? It seems it already supports `:first`, `:last` and alike pseudo-classes.
",kmike,redapple
843,2014-08-18 13:15:22,"@kmike , I've looked at pyquery's implementation. The `post_condition` patch resembles my initial trials (https://github.com/redapple/scrapy/commit/cbf051249d3fd5081ae9c099c33a02aedb9ed64f)

I'm also working on a simpler implementation that uses cssselect's [`.add_star_prefix`](https://github.com/SimonSapin/cssselect/blob/master/cssselect/xpath.py#L72) which fixes my issues with positions (I don't know how I ended up with such a complicated solution...).
It's used in cssselect for `:nth-child()` et al.

Also, coming back to pyquery, their `:even`/`:odd` could be done with `:nth(2n)`/`:nth(2n+1)` (_an+b_ notation makes perfect sense for this extension).
And I think pyquery is missing the same `.add_start_prefix()` as I missed earlier:



Maybe these Scrapy extensions would need different names `:first-of`, `:last-of`, `:nth-of(an+b)`, `:nth-last-of(an+b)` (note that current _an+b_ support is wrong for negative _a_'s: https://github.com/SimonSapin/cssselect/pull/42)
",redapple,kmike
843,2014-09-24 07:04:11,"@redapple what is missing in this PR?
",kmike,redapple
843,2014-10-08 11:07:01,"sorry @kmike for not updating earlier. I just commited a minor optimization on brackets for modulo. This matches what I submitted for cssselect https://github.com/SimonSapin/cssselect/pull/42

I'm thinking we could name those `:nth-of`, `:first-of` and `:last-of`
",redapple,kmike
842,2014-08-11 12:13:15,"@barraponto , I pushed a commit with some documentation.

You're right, these extensions are pseudo-classes, much like [structural pseudo-classes](http://www.w3.org/TR/selectors/#structural-pseudos)
",redapple,barraponto
837,2014-08-01 20:27:53,"@kmike can you check this https://github.com/scrapy/scrapy/issues/508#issuecomment-50087286? while you are in that function
",nramirezuy,kmike
834,2014-08-29 02:41:42,"I think that @nramirezuy concern is valid, but let's not throw this right away - using `__str__` is an interesting idea. But it requires more justification: @Digenis - what functions do you use often which benefit from this change?

Returning unicode from `__str__` in 2.x is [wrong](http://kmike.ru/python-with-strings-attached/) (there is `__unicode__` for that in 2.x); you don't have any issues because of your system configuration and because you haven't tried all edge cases - e.g. some strings can be unexpectingly promoted to unicode, which could trigger unicode decoding in unrelated places. 

`__repr__` is correct in existing implementation - it returns an ascii-only bytestring in Python 2.x because repr() function and %r format specifier return escaped ascii-only bytestrings in 2.x, and it will work in 3.x (displaying non-ascii chars nicely) because it uses ""native"" strings.
",kmike,nramirezuy
834,2014-08-29 02:41:42,"I think that @nramirezuy concern is valid, but let's not throw this right away - using `__str__` is an interesting idea. But it requires more justification: @Digenis - what functions do you use often which benefit from this change?

Returning unicode from `__str__` in 2.x is [wrong](http://kmike.ru/python-with-strings-attached/) (there is `__unicode__` for that in 2.x); you don't have any issues because of your system configuration and because you haven't tried all edge cases - e.g. some strings can be unexpectingly promoted to unicode, which could trigger unicode decoding in unrelated places. 

`__repr__` is correct in existing implementation - it returns an ascii-only bytestring in Python 2.x because repr() function and %r format specifier return escaped ascii-only bytestrings in 2.x, and it will work in 3.x (displaying non-ascii chars nicely) because it uses ""native"" strings.
",kmike,Digenis
831,2014-08-03 10:41:10,"@dangra I ran the commands as recommended:



That allowed me to run `scrapy`, which now works as expected.

Just need to push that out to pypi now...
",jpswade,dangra
825,2014-07-29 15:25:59,"@kmike I think all the metas are documented as ""contains the XXX key, blabla"", so the check for existence is not wrong.

`dont_filter` isn't a meta value right now, so it doesn't count.
",nramirezuy,kmike
825,2014-07-29 17:45:38,"+1 to @kmike arguments and example
",chekunkov,kmike
825,2014-08-10 17:07:46,"@nramirezuy what are advantages of `meta={'dont': ('dont_cache', 'dont_retry', 'dont_custom')}` over existing approach? Disadvantages:
- either Scrapy must insert empty dictionary under 'dont' key, or users will have to ensure 'dont' is present in meta;
- there will be more gotchas with `meta.copy()`;
- `'dont'` is duplicated - you write it both as a dictionary key and as values;
- `meta={'dont_cache': self.dont_cache}` is still hard to implement.
",kmike,nramirezuy
825,2014-08-13 20:18:47,"@barraponto I think that the `dont_` standardization is to avoid conflict with user info.

@kmike 

> Disadvantages:
> - either Scrapy must insert empty dictionary under 'dont' key, or users will have to ensure 'dont' is present in meta; -- I prefer a tuple inside the 'dont' key.
> - there will be more gotchas with `meta.copy()`; -- I didn't think about it, but it is the same gotcha as with user info.
> - `'dont'` is duplicated - you write it both as a dictionary key and as values; -- True, I just typed a random name.
> - `meta={'dont_cache': self.dont_cache}` is still hard to implement. -- But it can be implemented as `meta={'dont': self.request_dont}`, `self.request_dont=('dont_cache', 'dont_merge_cookies')` 

Advantages:
- All the `dont_`keys located in the same place. Easier to pop undesired flags from the meta `Request(..., meta=response.meta)`. I saw this to many times.

I feel that the using True or False is overhead when existence is enough. 

I don't mind too much the current approach or this one. I just have to point that this change isn't backward compatible.
",nramirezuy,barraponto
825,2014-08-13 20:18:47,"@barraponto I think that the `dont_` standardization is to avoid conflict with user info.

@kmike 

> Disadvantages:
> - either Scrapy must insert empty dictionary under 'dont' key, or users will have to ensure 'dont' is present in meta; -- I prefer a tuple inside the 'dont' key.
> - there will be more gotchas with `meta.copy()`; -- I didn't think about it, but it is the same gotcha as with user info.
> - `'dont'` is duplicated - you write it both as a dictionary key and as values; -- True, I just typed a random name.
> - `meta={'dont_cache': self.dont_cache}` is still hard to implement. -- But it can be implemented as `meta={'dont': self.request_dont}`, `self.request_dont=('dont_cache', 'dont_merge_cookies')` 

Advantages:
- All the `dont_`keys located in the same place. Easier to pop undesired flags from the meta `Request(..., meta=response.meta)`. I saw this to many times.

I feel that the using True or False is overhead when existence is enough. 

I don't mind too much the current approach or this one. I just have to point that this change isn't backward compatible.
",nramirezuy,kmike
825,2014-08-14 17:14:41,"@nramirezuy You're right that this change is backards incompatible, but I agree with @dangra that it is unlikely people are using False values (unless they make a mistake like `{'dont_cache': self.dont_cache}`). Separating ""user data"" meta from ""control meta"" may be worth investigating, but moving ""dont"" flags alone doesn't solve an issue with copying meta, and this new ""dont"" requires deprecation of old meta options, auto-inserting ""dont"" item to meta and changes to user code, so I'd prefer just to allow False values.

> but this must happen in a single change including docs and tests

@rocioar said she can work on it :)
",kmike,nramirezuy
825,2014-08-14 17:14:41,"@nramirezuy You're right that this change is backards incompatible, but I agree with @dangra that it is unlikely people are using False values (unless they make a mistake like `{'dont_cache': self.dont_cache}`). Separating ""user data"" meta from ""control meta"" may be worth investigating, but moving ""dont"" flags alone doesn't solve an issue with copying meta, and this new ""dont"" requires deprecation of old meta options, auto-inserting ""dont"" item to meta and changes to user code, so I'd prefer just to allow False values.

> but this must happen in a single change including docs and tests

@rocioar said she can work on it :)
",kmike,dangra
825,2014-08-29 03:08:41,"@rocioar's PR was merged.
",kmike,rocioar
823,2014-08-01 15:16:11,"@juanriaza you pointed to the download handler implementation, that is an internal detail on how to handle bodyless requests. The Request instance will have an empty string as body as pointed out by @nramirezuy 
",dangra,nramirezuy
823,2014-08-01 15:41:02,"@dangra @nramirezuy you are right, request instance save an empty string. My problem is with how `ScrapyHTTPClientFactory` deals with empty bodies. Given an empty body `Content-Length` header should always be set to zero.
",juanriaza,nramirezuy
823,2014-08-01 15:41:02,"@dangra @nramirezuy you are right, request instance save an empty string. My problem is with how `ScrapyHTTPClientFactory` deals with empty bodies. Given an empty body `Content-Length` header should always be set to zero.
",juanriaza,dangra
823,2014-08-01 16:05:48,"I'd not trust http://requestb.in because Scrapy don't send all these headers - it seems there is a proxy in use.

Server code:



Results for @dangra's spider:


",kmike,dangra
823,2014-08-01 22:11:26,"@dangra I've been fooled by httpbin.org as well. See the actual headers "">"" sent by curl vs the response:


",rolando,dangra
823,2015-03-18 22:06:23,"@kmike test that was provided above (with selfmade server and simple crawler) still does not pass.



I will try to find out the problem.
",persiyanov,kmike
821,2014-07-29 14:47:52,"@kmike it doesn't work on any in any [middleaware](https://github.com/scrapy/scrapy/blob/master/scrapy/contrib/downloadermiddleware/cookies.py#L25). 
",nramirezuy,kmike
821,2015-03-13 22:03:41,"Ping @nramirezuy :) It is an useful feature, and it is almost ready.
",kmike,nramirezuy
821,2015-03-16 13:40:06,"@kmike is that doc enough? or there are other places to change?
",nramirezuy,kmike
821,2015-03-16 14:00:42,"@nramirezuy this flag should be also added here: https://github.com/scrapy/scrapy/blob/master/docs/topics/request-response.rst#requestmeta-special-keys
",kmike,nramirezuy
821,2015-03-16 14:52:15,"@kmike done
",nramirezuy,kmike
818,2014-07-29 00:45:42,"@nramirezuy indeed, the implementation is kinda hacky but that's the idea. It needs a try/finally around the yield to avoid your bug.

I would like to hear more thoughts on this. @redapple @dangra 
",rolando,nramirezuy
818,2014-08-07 15:00:10,"Another related use case I had with `ItemLoaders` is when you want to build lists of sub-items. (e.g. list of staff members with phone, list of prices with currencies or discounts, lists of available sizes with stock status...)

I used `Selectors` directly, building a list for `dicts` and then assigned this to a field name with `.add_value()`.

The example by @darkrho above (https://github.com/scrapy/scrapy/pull/818#issuecomment-51404351) assumes an item that has no nesting of items.

What's the best practice for my use-case with current `ItemLoaders` and with the proposed changes? @darkrho , @nramirezuy , @dangra?
Probably define another `Item` for the nested/sub-items?
",redapple,nramirezuy
818,2014-08-07 15:00:10,"Another related use case I had with `ItemLoaders` is when you want to build lists of sub-items. (e.g. list of staff members with phone, list of prices with currencies or discounts, lists of available sizes with stock status...)

I used `Selectors` directly, building a list for `dicts` and then assigned this to a field name with `.add_value()`.

The example by @darkrho above (https://github.com/scrapy/scrapy/pull/818#issuecomment-51404351) assumes an item that has no nesting of items.

What's the best practice for my use-case with current `ItemLoaders` and with the proposed changes? @darkrho , @nramirezuy , @dangra?
Probably define another `Item` for the nested/sub-items?
",redapple,dangra
818,2014-08-07 15:18:00,"The API change looks nice, but still has some issues.



@dangra 
1. I like to have different methods it keep the things sorted.
2. Idem 1
3. Are you talking about `.replace`?
",nramirezuy,dangra
818,2014-08-07 15:41:42,"@redapple If you have a predefined format and you want the control use a `Item class` for your sub-items. Just instead of using `Selector` use a different instance of `ItemLoader` and then `.add_value(second_il.load_item())`.
",nramirezuy,redapple
818,2014-08-11 13:33:04,"@barraponto in your specific case, you can use the `selector` argument in the item loader and don't need the push methods. The push methods' usefulness shines when you have many fields in a common container. Here is an example that mixes xml and html selectors into the same item loader:


",rolando,barraponto
818,2014-08-11 13:47:29,"@redapple I'm not sure how nested items conflicts with the push methods. Here is an example where I (ab)use a subitem loader to load a table of values `[{""col1"": ""value1"", ""col2"": ""vallue2"", ...}]`. Although it doesn't have too much to do with the push method except to combine css and xpath selectors to retrieve the table (which could be done by a single xpath selector).



As pointed out by @nramirezuy, this implementation of the push methods causes an inconsistency in the selector value in the context loader, where input processors get the overrided value and the output processors might get the original selector (depending on where is called `load_item`).
",rolando,nramirezuy
818,2014-08-11 13:47:29,"@redapple I'm not sure how nested items conflicts with the push methods. Here is an example where I (ab)use a subitem loader to load a table of values `[{""col1"": ""value1"", ""col2"": ""vallue2"", ...}]`. Although it doesn't have too much to do with the push method except to combine css and xpath selectors to retrieve the table (which could be done by a single xpath selector).



As pointed out by @nramirezuy, this implementation of the push methods causes an inconsistency in the selector value in the context loader, where input processors get the overrided value and the output processors might get the original selector (depending on where is called `load_item`).
",rolando,redapple
818,2015-10-18 23:43:05,"@kmike oh, I was not aware of that. Thanks! I'm closing this PR.
",rolando,kmike
817,2015-06-16 18:59:21,"@dangra: gotcha. Thanks.
",timschwab,dangra
816,2014-08-14 17:23:23,"@kmike @dangra I've implemented your suggestions.
",curita,dangra
816,2014-10-02 04:17:56,"@nyov damn right - 993b543e1b895772ec8ac1aeb8db5a87d2b5f784
",pablohoffman,nyov
815,2014-07-23 20:58:27,"@dangra there are missing exceptions, should I add them?
",nramirezuy,dangra
814,2014-07-23 23:10:41,"@kmike hey man, I tried to address the issues you pointed out, can you please take a look at it again, see if it's OK?
Thanks!
",eliasdorneles,kmike
810,2014-07-24 17:06:42,"@kmike, it is needed because of [scrapy.utils.trackref#L30](https://github.com/scrapy/scrapy/blob/d7d9f2f96d6b190bc7e73981ee66f8aca29c6df8/scrapy/utils/trackref.py#L30)

~~In our case `DictItem.__hash__`  should return `hash(self._values)`.~~
",dangra,kmike
810,2014-07-24 17:39:23,"@kmike alternative to `__hash__` method is OK for me too.
",dangra,kmike
810,2014-07-24 17:51:57,"@felixonmars : what do you think if we use [`six.add_metaclass()`](http://pythonhosted.org/six/#six.add_metaclass) instead of `six.with_metaclass()`. The former doesn't create an intermediate class.
",dangra,felixonmars
810,2014-07-24 17:59:30,"@dangra i thought it was fixed in six 1.7 (https://bitbucket.org/gutworth/six/issue/66/replace-the-implementation-of). Not sure why aren't docs on pythonhosted.org up to date (people, use RTFD instead).
",kmike,dangra
803,2014-07-15 12:41:47,"@felixonmars +1 to your suggestion. 

We may want to add something copy-less for Python 3.x in files/images pipelines in future, but that's another issue.
",kmike,felixonmars
803,2014-07-21 17:17:39,"good work @felixonmars ! thanks.
",dangra,felixonmars
803,2014-07-21 17:22:00,"@kmike you rocked too (as usual) :-P
",dangra,kmike
802,2014-07-14 16:11:37,"@chekunkov I think the main reason is because is hard to implement and maintain. 
",nramirezuy,chekunkov
802,2015-01-23 15:31:41,"@chekunkov What do you think of adding this feature as an idea for this year's GSoC? Don't know if you've already started implementing it, but seems like a really fun and challenging project for a student, and provided of good mentoring (I'd love to have you as a mentor if you're interested) seems feasible on GSoC's timeframe.
",curita,chekunkov
802,2015-03-10 15:16:58,"hey @chekunkov @Curita I have this implemented for epubdirect project, will generalize it and make a PR, if that sounds good.
",rocioar,chekunkov
802,2015-03-10 16:58:02,"@rocioar yay, sounds great I think
",chekunkov,rocioar
802,2015-03-10 20:38:44,"@chekunkov Something like this makes sense to you?


",nramirezuy,chekunkov
802,2015-03-13 12:56:36,"Well, my idea of it was:

Creating another extension, call it CustomDelayThrottle... or whatever, that will inherit almost all funcionality from Autothrottle, only different thing is that will setup delays specified in settings for the different domains. Discussing with @kmike he mentioned it would be better to make slots api better and make it public, make some docs and add this CustomDelayThrottle as an example there, which I thought was reasonable.
",rocioar,kmike
802,2015-03-13 14:32:02,"@rocioar AutoThrottle handles slots delays not Request delays.
",nramirezuy,rocioar
802,2015-03-13 17:09:57,"@rocioar Autothrottle logic doesn't cover `to add exponential backoff for the retry request`. It will slow down the whole slot, not just that single request. 
Your proposal is valid and I like it, but doesn't solve the current issue.
",nramirezuy,rocioar
802,2015-03-17 19:24:07,"I drew 2 schemes - I hope they will help to explain what I mean

As far as remember when I was creating this issue I had following problem - for some specific URL (some API call?) site returned error (or empty body?) from time to time (non predictable) and only way to workaround this was retry with some delay. Problem is - it was 9 months ago and I don't remember details and also I don't remember why we haven't just changed slot delay. Possible reasons:

1) slot API isn't public, it's not clear how to change slot delay in callback or create and use different slot for retries - so I had not idea I can use that

2) for some reason I didn't want to affect other requests running in background and wanted to apply delay only to given retry request (see fig. 1)

fig. 1
![photo_3-17-15__20_11_03](https://cloud.githubusercontent.com/assets/744331/6694955/4d607728-cce6-11e4-8353-ac6423f48bf5.jpg)

And what I probably wanted in this case is to be able to apply custom delays only to requests where they are set (see fig. 2)

fig. 2
![photo_3-17-15__20_37_12](https://cloud.githubusercontent.com/assets/744331/6695031/d280b49a-cce6-11e4-9249-f28924f29166.jpg)

i.e. use `slot.download_delay()` by default and `Request.download_delay` when it's set.

> Delay is a time between two events, right? What does ""per-request"" delay mean? Which two events are separated by this delay?

@kmike two events - fired requests from single slot. ""per-request"" - because here I meant delay that's different from default and is set in Request object (see fig. 2). Ideally - as a `Request` object attribute, by analogy with `dont_filter`.

> Something like this makes sense to you?

@nramirezuy theoretically this solution fits problem I described. Nothing like this came to mind 9 months ago, so maybe not very obvious one. Wouldn't it be nice to have something like this handled out of the box by scheduler?

> using the functionality that Autothrottle already has we could set up custom delays per domain.

@rocioar you mean not only set it during crawl initialisation but also to be able to change it from spider or using some request.meta key - right?
",chekunkov,rocioar
802,2015-03-17 19:24:07,"I drew 2 schemes - I hope they will help to explain what I mean

As far as remember when I was creating this issue I had following problem - for some specific URL (some API call?) site returned error (or empty body?) from time to time (non predictable) and only way to workaround this was retry with some delay. Problem is - it was 9 months ago and I don't remember details and also I don't remember why we haven't just changed slot delay. Possible reasons:

1) slot API isn't public, it's not clear how to change slot delay in callback or create and use different slot for retries - so I had not idea I can use that

2) for some reason I didn't want to affect other requests running in background and wanted to apply delay only to given retry request (see fig. 1)

fig. 1
![photo_3-17-15__20_11_03](https://cloud.githubusercontent.com/assets/744331/6694955/4d607728-cce6-11e4-8353-ac6423f48bf5.jpg)

And what I probably wanted in this case is to be able to apply custom delays only to requests where they are set (see fig. 2)

fig. 2
![photo_3-17-15__20_37_12](https://cloud.githubusercontent.com/assets/744331/6695031/d280b49a-cce6-11e4-9249-f28924f29166.jpg)

i.e. use `slot.download_delay()` by default and `Request.download_delay` when it's set.

> Delay is a time between two events, right? What does ""per-request"" delay mean? Which two events are separated by this delay?

@kmike two events - fired requests from single slot. ""per-request"" - because here I meant delay that's different from default and is set in Request object (see fig. 2). Ideally - as a `Request` object attribute, by analogy with `dont_filter`.

> Something like this makes sense to you?

@nramirezuy theoretically this solution fits problem I described. Nothing like this came to mind 9 months ago, so maybe not very obvious one. Wouldn't it be nice to have something like this handled out of the box by scheduler?

> using the functionality that Autothrottle already has we could set up custom delays per domain.

@rocioar you mean not only set it during crawl initialisation but also to be able to change it from spider or using some request.meta key - right?
",chekunkov,nramirezuy
802,2015-03-17 19:24:07,"I drew 2 schemes - I hope they will help to explain what I mean

As far as remember when I was creating this issue I had following problem - for some specific URL (some API call?) site returned error (or empty body?) from time to time (non predictable) and only way to workaround this was retry with some delay. Problem is - it was 9 months ago and I don't remember details and also I don't remember why we haven't just changed slot delay. Possible reasons:

1) slot API isn't public, it's not clear how to change slot delay in callback or create and use different slot for retries - so I had not idea I can use that

2) for some reason I didn't want to affect other requests running in background and wanted to apply delay only to given retry request (see fig. 1)

fig. 1
![photo_3-17-15__20_11_03](https://cloud.githubusercontent.com/assets/744331/6694955/4d607728-cce6-11e4-8353-ac6423f48bf5.jpg)

And what I probably wanted in this case is to be able to apply custom delays only to requests where they are set (see fig. 2)

fig. 2
![photo_3-17-15__20_37_12](https://cloud.githubusercontent.com/assets/744331/6695031/d280b49a-cce6-11e4-9249-f28924f29166.jpg)

i.e. use `slot.download_delay()` by default and `Request.download_delay` when it's set.

> Delay is a time between two events, right? What does ""per-request"" delay mean? Which two events are separated by this delay?

@kmike two events - fired requests from single slot. ""per-request"" - because here I meant delay that's different from default and is set in Request object (see fig. 2). Ideally - as a `Request` object attribute, by analogy with `dont_filter`.

> Something like this makes sense to you?

@nramirezuy theoretically this solution fits problem I described. Nothing like this came to mind 9 months ago, so maybe not very obvious one. Wouldn't it be nice to have something like this handled out of the box by scheduler?

> using the functionality that Autothrottle already has we could set up custom delays per domain.

@rocioar you mean not only set it during crawl initialisation but also to be able to change it from spider or using some request.meta key - right?
",chekunkov,kmike
802,2015-09-15 16:09:20,"I faced a problem with retries today, so it seems I'm finally starting to understand what problems did you have :snail: 

Tweaking priorities (as in current RetryMiddleware or in @nramirezuy's example) is not enough: say we have 100 requests to the same server, server is temporarily down (e.g. for 5 seconds), we push back first 16, then next 16, then next (all this is really quick because server is dropping connections); in the end we have the first 16 requests processed again, without any waiting - they will turn out to be requests with a highest priority again soon. It means Scrapy will retry all these 100 requests for `RETRY_TIMES` times and fail them all (likely during these 5s of downtime) - instead of waiting a bit.

To make it work we need a way to say: ""please process this request, but no sooner than X seconds from now"". It is IMHO different from rate limiting (i.e. from delays between requests). You may need both: wait X seconds and then process the request, respecting rate and concurrency limits. So I think this ""pre-request delay"" is not an overridden value of a slot delay, it is a totally different thing. 

As usual, @dangra is right and it looks like the best place to handle it is a scheduler. 

Without scheduler support one can use callLater to schedule a request to a later time. It will have the same effect (""call no sooner than X seconds from now, respect rate limits""), but the downside is that the request is kept in memory for these X seconds, not in a scheduler queue. It means persistence is not supported, with all the consequences: requests are dropped in case of a restart, increased memory usage.

To implement it in a scheduler we'll have to make 2 big changes:
1. All queues must implement delays support.
2. There should be a way for scheduler/queue to say ""I have some requests, so don't stop the spider, but I won't give you anything now"".
",kmike,nramirezuy
802,2015-09-15 16:09:20,"I faced a problem with retries today, so it seems I'm finally starting to understand what problems did you have :snail: 

Tweaking priorities (as in current RetryMiddleware or in @nramirezuy's example) is not enough: say we have 100 requests to the same server, server is temporarily down (e.g. for 5 seconds), we push back first 16, then next 16, then next (all this is really quick because server is dropping connections); in the end we have the first 16 requests processed again, without any waiting - they will turn out to be requests with a highest priority again soon. It means Scrapy will retry all these 100 requests for `RETRY_TIMES` times and fail them all (likely during these 5s of downtime) - instead of waiting a bit.

To make it work we need a way to say: ""please process this request, but no sooner than X seconds from now"". It is IMHO different from rate limiting (i.e. from delays between requests). You may need both: wait X seconds and then process the request, respecting rate and concurrency limits. So I think this ""pre-request delay"" is not an overridden value of a slot delay, it is a totally different thing. 

As usual, @dangra is right and it looks like the best place to handle it is a scheduler. 

Without scheduler support one can use callLater to schedule a request to a later time. It will have the same effect (""call no sooner than X seconds from now, respect rate limits""), but the downside is that the request is kept in memory for these X seconds, not in a scheduler queue. It means persistence is not supported, with all the consequences: requests are dropped in case of a restart, increased memory usage.

To implement it in a scheduler we'll have to make 2 big changes:
1. All queues must implement delays support.
2. There should be a way for scheduler/queue to say ""I have some requests, so don't stop the spider, but I won't give you anything now"".
",kmike,dangra
802,2015-09-15 19:13:09,"@kmike is what my approach does. Send requests back to the scheduler if they didn't wait those `<per_request_delay>`(5 sec).



am I missing something?

EDIT: I guess you want to edit `_scheduled_time` after an exception.
",nramirezuy,kmike
802,2015-09-15 20:52:46,"@nramirezuy hm, isn't it inefficient? Let's say you have 1 request, you'll be sending it to scheduler and getting it back in a busy loop until its time will come. Or is it fixed by https://github.com/scrapy/scrapy/pull/1253?
",kmike,nramirezuy
802,2015-09-15 21:12:19,"@kmike first of all we change priority, so requests will rotate. I don't know what do you mean by a busy loop; but the heaviest task is reading the request from the queue and this is something the scheduler can't avoid.
",nramirezuy,kmike
802,2015-09-16 11:59:46,"I can only repeat myself

> @nramirezuy theoretically this solution fits problem I described. Nothing like this came to mind 9 months ago, so maybe not very obvious one. Wouldn't it be nice to have something like this handled out of the box by scheduler?

@kmike 

> To make it work we need a way to say: ""please process this request, but no sooner than X seconds from now"". It is IMHO different from rate limiting (i.e. from delays between requests).

yeah, agree, your explanation of the problem is much better, sorry for poor problem statement and examples. I think yes, this could be something completely different from slot delay - we should give a good name, maybe 'hold_delay', or just 'hold'. if it will be implemented in scheduler and described in documentation - no doubts it will be widely used.

> There should be a way for scheduler/queue to say ""I have some requests, so don't stop the spider, but I won't give you anything now"".

this could be challenging especially taking in account how overcomplicated is downloader implementation
",chekunkov,nramirezuy
802,2015-09-16 11:59:46,"I can only repeat myself

> @nramirezuy theoretically this solution fits problem I described. Nothing like this came to mind 9 months ago, so maybe not very obvious one. Wouldn't it be nice to have something like this handled out of the box by scheduler?

@kmike 

> To make it work we need a way to say: ""please process this request, but no sooner than X seconds from now"". It is IMHO different from rate limiting (i.e. from delays between requests).

yeah, agree, your explanation of the problem is much better, sorry for poor problem statement and examples. I think yes, this could be something completely different from slot delay - we should give a good name, maybe 'hold_delay', or just 'hold'. if it will be implemented in scheduler and described in documentation - no doubts it will be widely used.

> There should be a way for scheduler/queue to say ""I have some requests, so don't stop the spider, but I won't give you anything now"".

this could be challenging especially taking in account how overcomplicated is downloader implementation
",chekunkov,kmike
800,2014-07-22 15:42:04,"@kmike This change broke this function https://github.com/scrapy/scrapy/blob/master/scrapy/utils/url.py#L71 which broke `Scrapy` completely. 


",nramirezuy,kmike
800,2014-07-22 15:55:40,"@nramirezuy I can't reproduce this, and the tests work (both on Travis and locally via tox). 

Maybe it is an issue with some older versions of six? If so, we need to update six version listed in setup.py.
",kmike,nramirezuy
800,2014-07-22 15:57:47,"@kmike I think I have a newer one


",nramirezuy,kmike
800,2014-07-22 16:06:05,"@kmike Nope.




",nramirezuy,kmike
800,2014-07-22 16:19:43,"That's misterious. PyCharm also shows me that imports at top of scrapy.utils.url are all shadowed by `from w3lib.url import *`, but I don't understand how could it happen.

@nramirezuy could you try moving 'import *' to the top of the file to see if it helps?
",kmike,nramirezuy
791,2014-07-11 09:57:37,"@dangra , the current 0.24 code is sub-optimal indeed. Not sure it's critical
",redapple,dangra
790,2014-07-10 11:20:29,"@nramirezuy 

> It doesn't look like a bug if HtmlElement is an iterable.

Im actually think, that it looks. 
There is already such code:



`dict` and `BaseItem` are iterables, but thery are special - thery are more complex data structures, and so it is not desireable to iterate over them, and because of that there is a special check for them.

The same with `HtmlElement`. It is not flat data structure, it is a complex tree structure, which has many ieration optionos - similarly with `dict`.
So I think, that this is another special case. And istead of adding another check for that special case, I suggest to just checking for `list`/`tuple` types. Explicit better than implicit, etc.

ps.  
`Compose` does not fit my purposes.
",AndrewPashkin,nramirezuy
790,2014-07-11 15:20:09,"@dangra I still thinking that you can do `Compose(something, MapCompose(something_else))`.
",nramirezuy,dangra
790,2014-07-11 15:27:31,"@nramirezuy 
My code looks like:



`filter1` and `filter2` receive single, non-iterable items, and return single/non-iterable. And in case if they return `HtmlElement` `MapCompose` instead of passing it to next `filter`, distribute processing across its children.
",AndrewPashkin,nramirezuy
790,2014-07-11 16:59:51,"@nramirezuy 
In your example you don't have nested HTML tags, and your first filter in `MapCompose` does not return an instance of `HtmlElement`
I think I would be able to go with `Compose`, like that:



If Compose would terminate execution and return `None` if some of its filters returns `None`. But it [returns last value](https://github.com/scrapy/scrapy/blob/fb770852e87d97196e31f27c33ee8eee89aecc27/scrapy/contrib/loader/processor.py#L47)

ps.  

> @AndrewPashkin but if filter returns a HtmlElement you are breaking your own rules, because it is returning an iterable.

My ""rules"" is that only lists and tuples (and probably generators) should be treated as a containers by `MapCompose`
",AndrewPashkin,nramirezuy
790,2014-08-17 10:19:10,"@nramirezuy 

> Last value is None

Indeed! Yes, I can go with `Compose` in this case.
",AndrewPashkin,nramirezuy
787,2014-07-08 22:37:26,"Thanks @ivannotes!
",pablohoffman,ivannotes
787,2014-07-09 00:00:19,"@pablohoffman I have been using scrapy for a long time, really appreciate you guys' work :-)
",ivannotes,pablohoffman
779,2014-07-03 13:51:47,"@Digenis it casts repr on `_expr` via `%r` format specifier.
",kmike,Digenis
778,2014-07-21 16:34:45,"@felixonmars do you have an idea for better names?
",kmike,felixonmars
778,2014-07-21 22:49:40,"@nramirezuy these functions don't accept None. 

I think that's good they are not ""give me unicode from everything"" functions and that bytes and unicode are the only supported input types.
",kmike,nramirezuy
778,2014-07-22 15:23:29,"Oh, was the encoding. 

@kmike  Also Django `force_bytes`/`force_text` have some encoding guessing, right?
",nramirezuy,kmike
778,2014-07-22 17:30:40,"@nramirezuy no, they use utf8 by default, there is no guessing. Source code: https://github.com/django/django/blob/a2d0831e4242b97e0378414c4379ad8d3030ed6e/django/utils/encoding.py#L68.
",kmike,nramirezuy
778,2015-02-11 07:03:03,"> I wanted to say that it would be good to rename this functions. I don't know if in all places where this functions are called it would be ok to have bytes as output type. If not, maybe it will be necessary to add another pair of functions which will return str instead of bytes.

Not sure if I understood that myself. Talking about `str` is maybe confusing since it has different meanings depending on Python's version.

Strings come in two (and only two) ""flavors"", they are unicode objects, or they are bytes objects (this distinction was made clearer on Python 3). `str` either refers to bytes on Python 2 (It's an alias for it as @kmike said) or unicode on Python 3 (the actual `unicode` type was replaced for `str` on Python 3).

We should focus on making this distinction explicit to make code Python 2 and Python 3 compatible since they use different default types for strings, and that means to stop using `str` alias, have ""u"" and ""b"" prefixes on strings, and use `six.text_type` and `bytes` accordingly while checking string types.
",curita,kmike
772,2014-07-01 07:15:36,"@chekunkov see also: https://github.com/scrapy/scrapy/issues/568, https://github.com/scrapy/scrapy/pull/624

Currently the way to abstract ""join"" and ""extract()[0]"" is to use ItemLoader. I'm +0 for adding these shortcuts to Selector, but this needs some thought: it should be demonstrated that they make code simpler (compared to using ItemLoaders), and there is a question of what is better: to add more shortcuts or to try make ItemLoaders easier to use.
",kmike,chekunkov
772,2014-07-02 08:03:00,"@nramirezuy let me disagree

1) `.get()` method is for data extraction - fits your logic.
2) `.join()`. Yes, it's data processing. `ItemLoaders` are good for populating items. But data from `Selectors` also used in conditional expressions, parsing js, extracting json. Also it can be handy in some practical situations to prepare data before passing it to ItemLoader - without overriding project's default ItemLoader or changing default input/output processors which is error prone.

I have a project, where this functionality is implemented as utility function. And it happened to be quite popular:

![selection_006](https://cloud.githubusercontent.com/assets/744331/3452799/6a2d5bd4-01ba-11e4-9eb7-178e28d2e9c1.jpeg)

I don't see any reason why we should force people to write their own functions to implement this in every project is we can bring this functionality to them out of the box.

Some code examples follows (maybe not the best, but I hope they will show what I mean):








",chekunkov,nramirezuy
772,2014-07-03 19:58:17,"@nramirezuy 

1) promotions example

I don't think rewritten code looks much more readable or better or less error prone. Almost the same thing, except your example doesn't work the same way as original for some corner cases, for example when `'id(""product_price_was_holder"")//text()'` contains only spaces or newlines:



Which means you need additionally check `promotion_loader.get_output_value('promo_text')` or `promotion.get('promo_text')` before `loader.add_value('promotions', promotion)`. Correct me if I'm wrong.

2) iter_features

this line looks broken:



I suppose you meant something like



Unfortunatelly it seems to me that this rewritten code doesn't follow example logic as well. And if we theoretically have `.join()` method I can make your rewritten version even shorter:



Shorter, simpler, easier for understanding, 284 chars vs 418 (33% less, yes, I'm too lazy to type 130 chars more :smile: ) - without counting extra `from scrapy.contrib.loader import ItemLoader` which I don't need in last example
",chekunkov,nramirezuy
772,2014-07-03 20:40:36,"@chekunkov I didn't test the examples, because of time. Is not about if shortness, is more about concept.

> Scrapy comes with its own mechanism for extracting data. They’re called selectors because they “select” certain parts of the HTML document specified either by XPath or CSS expressions.

If you wanna join text use `ItemLoader`, python str method, be creative. 
Selector is not the place to do it.
",nramirezuy,chekunkov
772,2015-03-17 21:14:55,"@kmike I expect `.text(join=',')` to return string (considering xpath() returns SelectorsList with len >= 0). What would `.text()` return?

Also - do you mean `.text()` method should replace `/text()` xpath in some sense or that's just coincidence that you haven't used `/text()` or something like `/@value` in example xpath?
",chekunkov,kmike
772,2015-03-17 21:23:34,"@chekunkov I think .text() should also return a string, trying its best to get the raw text from the nodes selected by a selector. There are many ways to extract text from a set of nodes, but often the way to do it is either obvious or not that important. So `.text()` should try to do something reasonable by default - add a line break at the start of `<p>` element, don't add an extra whitespace in case of `<b>` elements, extract `@value` from a text input, etc. It is not a coincidence `/text()` is missing. 
",kmike,chekunkov
768,2014-08-27 21:22:03,"I like (3), it feels like the natural way to do it. I wouldn't even bother adding (1) to avoid having to support an additional mechanism, but if it's already implemented (I didn't see a test) I don't mind leaving it. I see it needs a rebase to be merged, @alexcepoi could you do it?
",pablohoffman,alexcepoi
768,2015-07-30 14:57:30,"@pablohoffman @alexcepoi it seems that this PR never got merged. Please let me know if you don't mind me finishing it - I'm quite interested in this feature :)
",maksim-kviatkouski,alexcepoi
768,2015-07-30 14:57:30,"@pablohoffman @alexcepoi it seems that this PR never got merged. Please let me know if you don't mind me finishing it - I'm quite interested in this feature :)
",maksim-kviatkouski,pablohoffman
757,2014-06-24 05:13:53,"Hi @dangra,
1. What do you think about fixing regression https://github.com/scrapy/scrapy/issues/546 before the release? I can do this if you defer the release.
2. I also wonder about https://github.com/scrapy/scrapy/pull/718 - have you figured out why tests fail with Twisted 14? I've restarted the build and it passes now. Maybe remove `<14.0` from setup.py?
3. One more ticket: https://github.com/scrapy/scrapy/issues/758
",kmike,dangra
757,2014-06-24 13:26:37,"> What do you think about fixing regression #546 before the release? I can do this if you defer the release.

@kmike: I can wait, I'm evaluating merging #737 for this release too. 

> I also wonder about #718 - have you figured out why tests fail with Twisted 14? I've restarted the build and it passes now. Maybe remove <14.0 from setup.py?

it is a regression that I would like to see fixed before releasing but not a blocker and we can fix it and release as 0.24.1

> One more ticket: #758

I second uploading wheels as part of releases. Will take it into count when uploading 0.24.0 to pypi.
",dangra,kmike
757,2014-06-25 16:10:07,"~~@kmike I dont want to push you but #546 is the only issue holding back this release now :)~~

May be done by #760.
",dangra,kmike
756,2014-07-02 15:48:26,"I coincide with @kmike, it is not worth maintaining multiple link extractors and #559 result is a promising replacement to rule them all. I think it is time to deprecate other than the lxml linkextractor.
",dangra,kmike
756,2014-07-02 20:55:05,"+1 to @dangra & @kmike.

But I'm happy to change my mind if someone shows me a good reason to leave any link extractor other than the lxml-based one.
",pablohoffman,dangra
756,2014-07-02 20:55:05,"+1 to @dangra & @kmike.

But I'm happy to change my mind if someone shows me a good reason to leave any link extractor other than the lxml-based one.
",pablohoffman,kmike
754,2017-03-20 14:46:54,"@redapple 
The second test is failing : 


I will look into it.
",Parth-Vader,redapple
751,2014-06-18 07:56:07,"Hi @dangra, 
updated according to your feedback.
",oliverrahner,dangra
751,2014-06-23 20:08:36,"Hey @dangra,

sorry, I'm kind of busy at the moment, as this is only a leisure project.
I will try to get to this soon.
The specific page I'm looking at has two clickables with the same name, only one of those has a value. I haven't yet tried what happens when I'm clicking the empty one and if I'm even able to click it.
However, I think this place in the code would be the wrong one to correct this issue. I think we should try to READ the values as they are and only handle empty values once we send them.
Then again, this is my first project with scrapy and I may well be mistaken.
",oliverrahner,dangra
747,2014-06-15 04:00:49,"@pablohoffman Thank you for your reply. 
But in my case tsocks can't work, I have to crawl several site use different proxy, because of performance and security reason. 

Something like this: 



I have implement Socks5DownloadHandler, but because it depends on 



so I don't know is it ok to pull a request? 

Here is my Socks5DownloadHandler code: https://gist.github.com/cydu/8a4b9855c5e21423c9c5 
",cydu,pablohoffman
747,2014-06-24 01:52:44,"Hi 
@pablohoffman thanks for your awesome scrapy! 

I was looking for something similar, i think is a big lack that such a complete software is missing socks support. 

My case is a bit different. I currently use a middleware to remotely request a proxy from a dispatcher server, then i assign the obtained proxy to the current request. Everything is working fine if they are http proxy, but I'm struggling to get the same behavior with socks.

Imagine i have a list of socks, and i want to use a random proxy per crawl. I didn't find a way to make this work with http-to-socks converters (polipo, privoxy, etc) and i thought i could write a customized server to handle that, maybe with special header to define to which sock to connect to..but I think is way too much complexity! 

Here is what i have for http proxies, and I would like to do something very similar for socks 


",disfasia,pablohoffman
747,2016-03-09 11:42:03,"@pablohoffman  There are a lot of cheap and good proxy that only support SOCKS4/5. This feature will be ridiculous useful.
I want to use scrapy but I can't because of it. :cry: And I really would like to use scrapy, because it's f***\* amazing.
",robsonpeixoto,pablohoffman
744,2014-06-12 16:20:01,"@crlane If you add every domains you have in your list, isn't better just disable `Offsite`?
",nramirezuy,crlane
744,2014-06-12 16:30:48,"@nramirezuy Not for our use case. We are scraping clusters of sites, but want to stay within the cluster (i.e., not follow links to sites external to the cluster).
",crlane,nramirezuy
743,2016-01-19 09:43:03,"@pablohoffman have you by chance changed your opinion? :)
",kmike,pablohoffman
743,2017-01-30 14:39:09,@kmike let's just say I'm not against this change :) - so feel free to merge,pablohoffman,kmike
741,2014-06-03 21:30:20,"@chekunkov Empty strings are filtered by `TakeFirst` not by the `ItemLoader` itself, so empty strings aren't an issue
",nramirezuy,chekunkov
741,2014-06-04 14:26:19,"@Digenis  Still not affecting you, this is the current implementation. Empty string aren't really filtered because are stored inside a list. The only ones affected are those who are returning a single item from the list, that usually is done with `TakeFirst` if you aren't using it, you should fix it.



If you still have doubts about the change share me an example to work with.
",nramirezuy,Digenis
741,2014-06-04 19:02:02,"@dangra FP is Functional Programming; I think @Digenis talks about libraries like https://github.com/kachayev/fn.py, https://github.com/Suor/funcy or https://github.com/pytoolz. But I'd also like to hear about how @Digenis uses them.
",kmike,dangra
741,2014-06-04 19:02:02,"@dangra FP is Functional Programming; I think @Digenis talks about libraries like https://github.com/kachayev/fn.py, https://github.com/Suor/funcy or https://github.com/pytoolz. But I'd also like to hear about how @Digenis uses them.
",kmike,Digenis
741,2014-06-05 13:40:29,"@Digenis  [SEP](https://github.com/scrapy/scrapy/tree/master/sep) documents are an option if you prefer to elaborate more about using functional programming libraries.
",dangra,Digenis
741,2017-02-22 13:40:35,"I think that's fine to make this change, but there must be a clear and easy upgrade path for users who want their code to work the same as before. We should either add an argument to ItemLoader constructor, or document a workaround in changelog. I share @chekunkov's concerns here.",kmike,chekunkov
740,2016-12-12 14:02:37,"Coming back to the issue with some code,
this is what I use.

The problem is that I made it non-blocking.
The method is called again while spider output is still being processed.
I can just make it block to solve my issue (I'm only yielding items in idled())
but an optimum solution shouldn't block, right?

@dangra, shouldn't we include in the specifications for the issue
the processing of spider output by the spider-mw and the handling of items?",Digenis,dangra
739,2014-06-02 21:45:48,"@nramirezuy You're probably right. I am running several spiders on the same file. I tried introducing a few seconds of delay between the spiders and haven't gotten the same error anymore.
",akurtovic,nramirezuy
737,2014-06-25 16:06:39,"I merged the PR after restoring `.overrides` attribute compatibility in f224ac13.

Thanks for the great work @curita!, now I am anxious to see the following tasks in this GSoC project implemented.
",dangra,curita
737,2014-06-25 18:20:02,"Thanks @dangra for fixing it! I'm sorry i couldn't work on it sooner.
",curita,dangra
735,2014-05-27 14:58:07,"@kmike You can always use `dont_filter` in such cases
",nramirezuy,kmike
735,2014-07-17 21:13:37,"@kmike Using `allowed_domains` and returning requests to different domains on `start_requests` or any other method is wrong. 
I +1 this proposal even if it breaks backwards compatibility, this is something that fits on ""weird behavior"" and have to be fixed. Having to add a meta is not a big deal.
",nramirezuy,kmike
733,2014-06-03 13:30:51,"ooops? :)) dunno how I missed that

@dangra can you merge this https://github.com/alexcepoi/scrapy/commit/b831400b581ef2405a41f0a11f5dd7fb2d27b285

or shall I make another PR?
",alexcepoi,dangra
732,2014-08-27 22:20:57,"Sorry, it's been a while.
@nramirezuy, because parse is pretty much _the_ `main()` function in a spider.
And I think the spider's public/advertised namespace should not go boom in the user's face (not requiring an understanding of the internal machinations first).

What I'm suggesting here is a solution which works for all Spiders, backwards compatible since all spiders inherit from `Spider` (BaseSpider).

It only calls a not-so-public method (`_parse`, since I'm bad with names) first, which instantly goes to `parse`, and does nothing for a normal Spider.
Now the not-so-public `_parse` is the entry-point, by which `Scraper` calls the `Spider`.
(The name should probably be `init` or `scrape` instead, to denote it being the entry-call from `Scraper`.)

Now in the case of special processing code in a Spider (which is ""non-public"" and functionally required, as in not user-added), it can be added to `_parse`, before going to the public `parse` method.

This allows mucking up `parse` without breaking the ""hidden voodoo"" (to the user) from the extended class for spiders such as `CrawlSpider`, `FeedSpider`, possibly `InitSpider` and more.

It's a ""hidden"" hook for the parent class's core functionality, and decouples it from the users extended functionality.
",nyov,nramirezuy
731,2014-07-02 22:16:33,"@redapple's lxml-based LinkExtractor (new in Scrapy 0.24) works fine for this website, and SgmlLinkExtractor is deprecated, so I'm closing this issue.
",kmike,redapple
727,2014-05-20 16:43:19,"thanks @tpeng , can you squash the commits as last request?
",dangra,tpeng
722,2015-04-01 17:12:37,"@nramirezuy I think this PR clearly fixes a bug; +1 to merge it once it has a test case.
",kmike,nramirezuy
722,2015-04-15 17:09:44,"ping @Digenis - can you add test case to merge this?
",pablohoffman,Digenis
722,2015-04-21 07:34:55,"Thanks @Digenis!
",kmike,Digenis
717,2014-05-12 15:45:28,"@JoshBradshaw in your examples you're using `spider_opened` and `spider_closed` names; @nramirezuy uses `open_spider` and `close_spider` (as in the docs). Can you confirm it doesn't work with `open_spider` and `close_spider`?
",kmike,nramirezuy
717,2014-05-12 17:43:15,"@kmike Good catch, I didn't noticed that.
",nramirezuy,kmike
716,2016-02-03 01:47:39,"sure @kmike I'd be interested in taking a look at Scrapy code to contribute! Although I'm new at it and I'll have to dive some time to understand how is the discussion going 
",abenassi,kmike
715,2014-05-15 10:14:11,"@dangra personally, ""drop_cookiejar_now"" is actually what I'd be interested in from this. From what I've seen it's a common use-case: a series of consecutive requests (e.g. made with inline-requests) need to share the same session that is not used by any other request. Usually you know what's the last request in a series is either before it has been executed or after processing the response. The ""after"" case probably needs some more thought, since ""drop_cookiejar_now"" meta flag won't help there much.

I also think that the feature makes sense only if it doesn't drop the cookie jar randomly based on timing issues, since making a request without cookies might mean that you'd have to redo not only the current request, but everything preceding it. So if the issues with the current implementation can't be fixed, I'm for replacing it with a simplified ""drop after request"" flag that won't have to track requests in a weak set.
This is also why I'm wary of timestamp expiration and LRUs: finding limit values that would guarantee that you'll never under any circumstances drop a cookie jar that's still needed might be hard.
",allait,dangra
715,2014-05-15 14:51:11,"@dangra one thing to be noted, every request that needs it's session to be closed, should set both `cookie_jar` and `autoclose_cookie_session` meta keys even though preceding requests of the same session had set the `autoclose_cookie_session`. For future requests which are going to share the same cookie session, these two variables (`cookie_jar` and `autoclose_cookie_session`) should be propagated. To recap, every request should set both `cookie_jar` and `autoclose_cookie_session` for the particular cookie session to be closed.
If i understand the _multiple cookie sessions in parallel_ problem that you mentioned correctly, that is the way to fix it.
I started this implementation, having 'drop_cookiejar_now' property only, in my mind. A flag that should be set by last request. Then it turned out that in response callback which yields another requests or items, it becomes harder to close the session especially if the callback is in the code path that is going to yield items, because the original request of the callback did not set the `drop_cookiejar_now` for a reason of possibility of using the session. Then i put a reference set there in order to ease that problem. Also things get messy especially if the url is redirected, not matter with `autoclose_cookie_session` implentation or `drop_cookiejar_now`.
As far as i see, http clients usually have an ability to manipulate the cookie sessions. We don't have that client notion maybe but it would be nice to have it in spider base class. Timestamp based solution sounds nice but it wouldn't easy to define the policy because networking is something very unpredictable and according to me, cookie manager should not decide which session is important or not (limiting the sessions). Besides that, IMO explicit actions from spider would be clearer and easy to debug.
",sardok,dangra
715,2014-05-15 21:43:42,"> @dangra personally, ""drop_cookiejar_now"" is actually what I'd be interested in from this. From what I've seen it's a common use-case: a series of consecutive requests (e.g. made with inline-requests) need to share the same session that is not used by any other request. Usually you know what's the last request in a series is either before it has been executed or after processing the response. The ""after"" case probably needs some more thought, since ""drop_cookiejar_now"" meta flag won't help there much.

You are right, the ""after"" case is what ""drop_cookiejar_now"" can't cover unless you send an spurious request to close the session. This is why I prefer to let the middleware autocleanup the jars after a configurable expiration time. 

> I also think that the feature makes sense only if it doesn't drop the cookie jar randomly based on timing issues, since making a request without cookies might mean that you'd have to redo not only the current request, but everything preceding it. 

right.

> So if the issues with the current implementation can't be fixed, I'm for replacing it with a simplified ""drop after request"" flag that won't have to track requests in a weak set.

I prefer it too.

> This is also why I'm wary of timestamp expiration and LRUs: finding limit values that would guarantee that you'll never under any circumstances drop a cookie jar that's still needed might be hard.

We can do both, the meta key for explicit removal and a conservative expiration time (like 10mins) for the lazy user or the complex ""after"" case. 
",dangra,dangra
715,2014-05-15 22:08:52,"> @dangra one thing to be noted, every request that needs it's session to be closed, should set both cookie_jar and autoclose_cookie_session meta keys even though preceding requests of the same session had set the autoclose_cookie_session. For future requests which are going to share the same cookie session, these two variables (cookie_jar and autoclose_cookie_session) should be propagated.

@sardok: I consider it a bug, only cookie_jar should be required after the first request created the session as autocloseable, the middleware should track future requests in the same session in its weakset.

> If i understand the multiple cookie sessions in parallel problem that you mentioned correctly, that is the way to fix it.

I dont think so, the problem I mention happens when there are more sessions than concurrent requests the downloader can handle, in this case the requests for some of those sessions will queue up in the scheduler. The middleware can only see the requests going into the downloader, so it will remove sessions while there are still pending requests scheduled for that sessions.  

> I started this implementation, having 'drop_cookiejar_now' property only, in my mind. A flag that should be set by last request. Then it turned out that in response callback which yields another requests or items, it becomes harder to close the session especially if the callback is in the code path that is going to yield items, because the original request of the callback did not set the drop_cookiejar_now for a reason of possibility of using the session. Then i put a reference set there in order to ease that problem. 

I think this is the same problem discussed in https://github.com/scrapy/scrapy/pull/715#issuecomment-43268973

except now I realize that if the request holding the ""drop_cookiejar_now"" mesage is discarded before it reaches cookies middleware (think of dupefilter) then the session will live forever. 

> Also things get messy especially if the url is redirected, not matter with autoclose_cookie_session implentation or drop_cookiejar_now.

Why are things messy on redirections? redirections propagates meta dict so ""auto_cookie_session"" should be set on redirected requests too.

> As far as i see, http clients usually have an ability to manipulate the cookie sessions. We don't have that client notion maybe but it would be nice to have it in spider base class.

Having access to cookiejar directly from the spider is a common request, one that we haven't solved in an easy way. You can always get into the internal apis of Scrapy and reach the cookiemiddleware and play whatever you like with it. But Scrapy doesn't provide a documented or recommended way to do it. That's true.

> Timestamp based solution sounds nice but it wouldn't easy to define the policy because networking is something very unpredictable and according to me, cookie manager should not decide which session is important or not (limiting the sessions). Besides that, IMO explicit actions from spider would be clearer and easy to debug.

Timestamp based solutions are not bad, in fact I think they are a must in most cases as last man resource. Think about how we handle stalled connections, we doesn't let the request stall forever, instead the connection timeouts using a default or per request configurable value. We can do the same with cookie sessions defaulting to a conservative value.
",dangra,dangra
713,2015-02-06 18:10:58,"@dufferzafar Create a PR, and make a commit per file. The title of this Issue is `Bring back docstrings`, so I guess some modules should have docstrings in his history.
",nramirezuy,dufferzafar
713,2015-02-06 18:15:50,"Okay, then I'll just extend @kmike's work.
",dufferzafar,kmike
713,2015-03-11 01:35:33,"Hi @dufferzafar -- are you working on this?
",eliasdorneles,dufferzafar
713,2015-03-11 10:27:51,"@eliasdorneles No, I am not. 
",dufferzafar,eliasdorneles
712,2014-05-07 10:38:02,"@kmike probably. I still think not being able to use `parse` in `CrawlSpider` (unless you know what you're doing), is a bit confusing.

When you know `Spider` and assume `parse` is the default method for responses, for `CrawlSpider` I would expect the `parse` method to be called at least for `start_urls` but for that you need work with `parse_start_url`.
http://doc.scrapy.org/en/latest/topics/spiders.html#scrapy.spider.Spider.parse
And why not assume `parse` to be default callback for `Rule`s ? (unless you say otherwise in the `Rule`).
",redapple,kmike
710,2014-05-18 09:51:03,"@Digenis proposed change reuses response reference passed to the `_itemproc_finished` method, so nothing really changed - the only change is that response object can be conveniently passed to the signal handler.

> I think this allows 100(by default) more response objects remaining referenced during the scraping process

If you look at code you'll see that `CONCURRENT_ITEMS` affects number of items that are processed given single request-response pair. 
",chekunkov,Digenis
710,2014-05-19 14:40:56,"I am with @Digenis about item pipeline holding references to Responses is a bad idea for memory usage, the `response` in `item_scraped` was an accidental addition (IMO) for convenience (it predates the open source release) and later the reimplentation of scraper component that holds that reference internally only to indirectly control the flow of core engine.

Although, this is a lost fight as `response` argument is documented for `item_scraped` having it in `item_dropped` doesn't harm us more.
",dangra,Digenis
710,2014-05-19 14:41:55,"@rafallo do you mind submitting a PR so we review and merge?
",dangra,rafallo
710,2014-05-19 15:20:40,"`DropItem` just works on ItemPipeline, so passing response through `item_dropped` doesn't make to much sense from that point. 

@rafallo Did you never thought about using `Spider Middlewares`, seems to be the right tool.
",nramirezuy,rafallo
710,2014-05-19 15:27:13,"My solution is quite simple. I can't see any performances issues with this change. It's just as simple as passing response to item_scraped signal.

@nramirezuy, it's good idea and probably I may use Spider Middleware, but don't you think that now signals API is quite inconsistent?
",rafallo,nramirezuy
710,2014-05-19 15:43:22,"@dangra I didn't quite understand what you mean here

> and later the reimplentation of scraper component that holds that reference internally only to indirectly control the flow of core engine

@dangra @nramirezuy I agree that keeping reference to the response object for too long can sometimes cause high memory usage. Alternatively we can consider to deprecate `response` usage in `item_scraped`. But `response` object is still used by logformatter (for ""Scraped from"" message) - so the change wouldn't be that trivial.
",chekunkov,nramirezuy
710,2014-05-19 15:43:22,"@dangra I didn't quite understand what you mean here

> and later the reimplentation of scraper component that holds that reference internally only to indirectly control the flow of core engine

@dangra @nramirezuy I agree that keeping reference to the response object for too long can sometimes cause high memory usage. Alternatively we can consider to deprecate `response` usage in `item_scraped`. But `response` object is still used by logformatter (for ""Scraped from"" message) - so the change wouldn't be that trivial.
",chekunkov,dangra
710,2014-05-19 15:54:16,"@rafallo Yes it is, we should be propagating `item_dropped` from Spider Middlewares too, and not just from Item Pipelines.
",nramirezuy,rafallo
708,2014-04-29 20:13:16,"@pablohoffman We can log something when it happen, I also think that `return Request(...` can be a valid thought and probably the first attempt of a newcomer to override `start_requests`.
",nramirezuy,pablohoffman
708,2014-07-03 20:46:06,"shall we add a the log message and merge this? @nramirezuy 
",pablohoffman,nramirezuy
708,2014-07-03 21:12:17,"@pablohoffman: No, this has to be fixed in the improvements @curita is working on. IMO the place to handle this error is https://github.com/scrapy/scrapy/blob/master/scrapy/crawler.py#L66. 
",dangra,pablohoffman
707,2014-04-28 13:36:42,"@redapple checking if it's `None` will be probably more accurate than `bool`
.
",nramirezuy,redapple
698,2014-04-24 16:26:56,"@allait we broke your PR with recent changes :)
",kmike,allait
698,2014-04-24 16:33:04,"@kmike I've rebased on top of new master, should be ok now
",allait,kmike
697,2014-04-17 14:01:24,"@kmike http://scrapy.org/community/ links to this documentation page as a place to start
",allait,kmike
691,2014-04-22 13:59:03,"Thanks for reply @redapple!
I've used `RegexLinkExtractor` instead of `SgmlLinkExtractor` and now it works perfectly, exactly as I excepted.

I haven't tested another two options:
- `LxmlParserLinkExtractor` is not subclass of `BaseSgmlLinkExtractor`, so it may not be compatibile
- Downloader Middleware is too advanced for me at this time; maybe I will read about it later
",kamil-forys,redapple
690,2014-04-15 13:54:22,"@kmike Keep `sel` as shortcut of `response.selector`, should be a bit more efficient since it's lazy.
",nramirezuy,kmike
690,2014-04-15 14:09:15,"@nramirezuy this is tricky because `sel = response.selector` will eagerly create the selector. I don't think efficiency is a problem because even if there are many Selectors for the same response the body should be parsed only once because of the LxmlDocument cache.
",kmike,nramirezuy
690,2014-04-24 03:16:26,"> We should also mention new shortcuts here: http://doc.scrapy.org/en/latest/topics/request-response.html#textresponse-objects. 

@kmike: done by b4593c2ae7 (review welcome)
",dangra,kmike
689,2014-04-14 21:00:48,"@dangra the fixing should be on the middleware, I usually look for meta keys I can use on the middlewares. I suppose the rest do the same.
",nramirezuy,dangra
689,2015-03-17 09:19:30,"@dangra are you OK with #821 merge?
",kmike,dangra
689,2015-03-17 16:21:43,"@kmike next time consider pinging dangra to merge :)
",pablohoffman,kmike
689,2015-03-17 16:57:03,"@pablohoffman I was going to close this issue, but saw that @dangra said he thinks the right place for this logic is the cache policy, not a global meta key.
",kmike,dangra
689,2015-03-17 16:57:03,"@pablohoffman I was going to close this issue, but saw that @dangra said he thinks the right place for this logic is the cache policy, not a global meta key.
",kmike,pablohoffman
688,2014-06-04 07:02:38,"@dangra these comments are for Python interpreter, not for text editors. Without such line you can't have non-ascii constants (including unicode non-ascii constants) in source code.





What you can do is to configure text editor to insert such lines for new files automatically, but this doesn't work for files generated by Scrapy.
",kmike,dangra
688,2014-06-25 18:18:31,"//cc @dangra this looks like a no-brainer for me, and objections to merge?
",kmike,dangra
686,2014-04-11 19:25:15,"@kmike : we (@redapple and me) were talking about doing this pull request in two steps, one to add the lazy selector and another for the response shortcuts.

the shortcuts are the problem that delay merging this pull request, you have to persuade us yet :-)

@nramirezuy : what do you think? 

we are looking for a pycon2014 release!
",dangra,kmike
686,2014-04-14 15:11:22,"@dangra I think this isn't necessary. I don't import `Selector` anymore for my new spiders, just use `ItemLoader`, as @kmike said `The end goal is to select the data.` and store it on items. `ItemLoader` already have everything `.selector`, `.get_css`, `.get_xpath`, and a parameter for `re`.
",nramirezuy,dangra
686,2014-04-14 15:11:22,"@dangra I think this isn't necessary. I don't import `Selector` anymore for my new spiders, just use `ItemLoader`, as @kmike said `The end goal is to select the data.` and store it on items. `ItemLoader` already have everything `.selector`, `.get_css`, `.get_xpath`, and a parameter for `re`.
",nramirezuy,kmike
686,2014-04-14 15:29:32,"@nramirezuy what's the advantage of baking `.get_css()` into ItemLoader instead of Response? This method doesn't populate an item.
",kmike,nramirezuy
686,2014-04-14 16:17:07,"@nyov do you have an real use case where adding a shortcut for `.re()` method worth it?

`.re()` is a special selection method because it doesn't return a selector, and it is commonly used only after applying another selection method like `.xpath()`.

We can always add a new method to the response later, taking it back isn't always possible without cumbersome deprecation paths.   
",dangra,nyov
686,2014-04-14 17:27:14,"@nramirezuy maybe I don't understand you; it also may be better to discuss this on some concrete examples. I tried to show how real-world code would look like with `response.xpath` and alike methods in https://github.com/kmike/pycon-speakers/compare/api-example. Is there a way to write it better using ItemLoaders?
",kmike,nramirezuy
686,2014-04-14 17:27:25,"@nramirezuy : ItemLoaders are orthogonal to response selectors, sometimes you won't just scrape items.

So far this PR looks good to me except for the `.re()` shortcut, I don't find a justifiably reason to include it yet.
",dangra,nramirezuy
686,2014-04-14 19:30:12,"@dangra, I thought it makes sense to have on a `scrapy.http.TextResponse`, where no xpath is avail; but I see that I was wrong and this `.re` is ofc from the selector, which we don't have on a `TextResponse`.

But then I don't see why we have this in class TextResponse, should this code not belong in HtmlResponse & XmlResponse?
",nyov,dangra
686,2014-04-14 19:36:32,"@nyov I've added it to TextResponse because Selectors treat TextResponse as HTML (see https://github.com/scrapy/scrapy/blob/master/scrapy/selector/unified.py#L36). I guess it is helpful when server sends a wrong content-type header (`text/...` where `...` is not `xml` or `text`).
",kmike,nyov
682,2014-04-07 13:55:50,"Hi @denysbutenko,

Thanks for this PR! The changes makes sense, but could you please split your PR into several smaller PRs to make reviewing and merging easier? There are some subtle points in your changes, for example: 
- io.StringIO and StringIO.StringIO are very different beasts;
- when `isinstance(foo, str)` is used in 2.x code it could mean either `isinstance(foo, bytes)` or `insinstance(foo, str)` in 3.x code (btw, `six.binary_type` is useless because `bytes` is supported since Python 2.6);
- some of urllib functions lost support for bytestrings in 3.x, so blindly using six.moves can cause bugs;
- your changes to scrapy/utils/python.py are backwards-incompatible and needs to be discussed - it's better to explain what is the issue and why is it necessary to break backwards compatibility; maybe other people would suggest better alternatives.

It will be very hard to merge a single large PR because a single controversial change will make PR unmergeable. I think creating PRs like ""PY3: use six.moves.urllib"" or ""PY3: port scrapy.commands.deploy"" is a better approach. Explaining non-obvious changes in PR description would also be great.
",kmike,denysbutenko
676,2014-04-01 17:23:26,"@dangra yes, _scrapy.selector.Selector_ on _XMLResponses_ is vulnerable too.
@kmike that library would be ideal
",csalazar,dangra
676,2014-04-01 17:23:26,"@dangra yes, _scrapy.selector.Selector_ on _XMLResponses_ is vulnerable too.
@kmike that library would be ideal
",csalazar,kmike
676,2014-04-03 02:39:57,"@csalazar are you going to update the PR with fixes for the other Selector?

Do you mind adding a testcase specially for LxmlDocument class?  

defusedxml looks good but it may require more work to integrate it, in the other hand resolve_entities=False looks quick enough to be merged soon.
",dangra,csalazar
676,2014-04-09 02:39:51,"Hi @kmike, I think that a valid XML file shows raw version of standard entities and not their html encoding. If you have an example, please paste it since we have to check if it breaks the XML document too.
",csalazar,kmike
673,2014-03-28 19:05:52,"@dangra 
""Twisted 14.0.0 is reported to have good enough support for Python3"" is a good news.

However, I hope all portions that Scrapy needs will be available. For example, Twisted 13.2.0 has setup3.py through which I was able to install a minimal copy twisted on Python 3.4. But some of the parts of twisted that Scrapy needs are still not available in Python 3.x version of twisted. A notable example (which I found through my trials/experimentation) is `twisted.conch`.
",emmanuelsa,dangra
673,2014-03-30 14:00:36,"@dangra 
Thanks for your helpful comment/suggestion: I will base my work on Twisted 14.0.0 henceforth. 

By the way, I have started working on running test suite with py.test and I will make sure the test suite works well in python 2.x first.
Perhaps too simple/little as a progress report, but here http://goo.gl/pyPJ1b is the output I obtained using the most basic command `python -m py.test &> ../RESULTOFPYTEST.txt` within the test suite directory scrapy/scrapy/tests . With this I believe I know what to do next and what needs to be changed. 
",emmanuelsa,dangra
673,2014-04-03 09:24:38,"Hi @dangra!  I'm currently working on it, I've just submitted a pull request.
",curita,dangra
673,2014-04-04 12:30:56,"@dangra I don't have python 3.3 (brew install python3). I think you must upgrade it to python 3.4 (now stable) or python 3 without minor version. I've used this in dev-env.
",denysbutenko,dangra
673,2014-04-04 15:36:17,"@denysbutenko : I don't see why you think python 3.4 is required to finish this pull request. Although python 3.3 is not really needed either, it is good enough. You need Python2.7 for sure.
",dangra,denysbutenko
673,2014-04-05 07:05:03,"@dangra : I have spent considerable time on this but I have not made much progress, and that is why I have not made any PR. If possible, your suggestion/guidance will be very helpful.

For example, almost all the tests that failled (please, see this log file http://goo.gl/WJ4jFb ) are because of ""inequality"" errors  from `assertEqual`. Since `assertEqual` is from `/usr/local/lib/python2.7/dist-packages/twisted/trial/_synctest.py` which is part of `twisted` _I am a bit confused_ on how to single out how `twisted` and `pytest` affect each other. And I do not think there is a need to change `assertEqual`.

If you have some time to look at this file  http://goo.gl/WJ4jFb , could you please suggest/let me know what you think I should do first/next. 
",emmanuelsa,dangra
672,2014-03-28 15:32:29,"hi @emmanuelsa, please see if you can complete what #673 started, that would help with what @kmike mentions as point 1.
",dangra,kmike
672,2014-03-28 18:47:50,"@kmike: 
[1] This is true. Indeed, I have a felling that I have not done enough testing. I had mainly run the tests by having many versions of Python (specially, Python 2.6, 2.7, 3.3, and 3.4) installed on my machine. This way I am able to specify which interpreted I want to use at any given time. Yes: MOST do not run on Python 3.x yet.
[2] I am not planning to use the my forked twisted to contribute to twisted. You and a friend had shared this link http://twistedmatrix.com/trac/wiki/ContributingToTwistedLabs with me in recently.

@dangra:
Thanks a lot: #673 is really helpful. I will soon start working on it.
",emmanuelsa,dangra
672,2014-03-28 18:47:50,"@kmike: 
[1] This is true. Indeed, I have a felling that I have not done enough testing. I had mainly run the tests by having many versions of Python (specially, Python 2.6, 2.7, 3.3, and 3.4) installed on my machine. This way I am able to specify which interpreted I want to use at any given time. Yes: MOST do not run on Python 3.x yet.
[2] I am not planning to use the my forked twisted to contribute to twisted. You and a friend had shared this link http://twistedmatrix.com/trac/wiki/ContributingToTwistedLabs with me in recently.

@dangra:
Thanks a lot: #673 is really helpful. I will soon start working on it.
",emmanuelsa,kmike
670,2014-03-28 08:41:33,"@kmike @dangra @pablohoffman can you merge?
",umrashrf,kmike
665,2015-03-13 19:44:43,"+1 I can work on this too. I've missed this more than once, when having to set some of the longer names that I haven't been able to commit to memory. :)

One question: should I try to set constants to the default values (may get out of sync with the defaults) or should I just put them all commented as per @redapple's example above?
",eliasdorneles,redapple
665,2015-03-13 22:38:38,"> One question: should I try to set constants to the default values (may get out of sync with the defaults) or should I just put them all commented as per @redapple's example above?

I like Paul's example. Users uncomment settings when they need non-default values. Some comments & suggestions & commonly used values are helpful in this case.
",kmike,redapple
665,2015-03-13 22:50:41,"@kmike Sure, I didn't mean to add _all_ settings, I meant as in ""any settings I end up adding would be commented"" :)

Yeah, agreed, I'll try to come up with some useful suggestions, and then we can discuss on the PR. :)
",eliasdorneles,kmike
664,2014-04-25 17:52:17,"@pablohoffman I think since it is a condition checking, so only converting the variable when it is necessary, therefore no unnecessary overhead. Or you are talking about it is unnecessary to check the type of variable every time?
",yyl,pablohoffman
661,2014-03-20 08:07:11,"Hey @ananana,

Thanks for this PR! Could you please add some tests for it?

I like how you used `arg_to_iter` because it makes the interface more consistent - other parameters for SgmlLinkExtractor already use it. But as it is a new feature (strings or None were not handled properly previously) it should be mentioned in docs for SgmlLinkExtractor. Also, there is `deny_extensions` that doesn't use `arg_to_iter` - maybe fix it as well?
",kmike,ananana
656,2014-03-19 13:14:45,"@nramirezuy are you sure it was this log line ? I recall about offsitemiddleware logging but not for httperror middleware.
",dangra,nramirezuy
656,2014-03-19 13:21:26,"@dangra I just remember a IgnoreRequest, I don't remember from where module it comes. Does it matters? We already know that those requests are banned by this middleware, maybe new comers want to see this but at some point it just spam. 
",nramirezuy,dangra
649,2014-03-18 03:05:39,"No need to workaround this on Scrapy if there is an alternative and sane way to install Python on Mac.

I second @kmike.
",dangra,kmike
648,2014-03-13 12:59:20,"@kmike I've seen your proposal #609 I'm that guy you mentioned ""they want to scrape information from some webpage, but haven't done this before"" And the overview only make sense to me after reading the dmoz tutorial page. I agree with everything you said. The tutorial - ish part in the overview section just confused me. I agree with all your suggestions  and also I think it will be better to add the simplest spider code and a couple of self.xpath statements that can be copied and pasted by readers, run it on their machine and play around with it. As someone new to scraping I want to first see scrapy scrape some data from a website and load that data in to my python shell and see the extracted data as python object. 
",ogiaquino,kmike
647,2014-04-24 21:59:08,"@pablohoffman updated architecture.rst 
https://github.com/coder46/scrapy/commit/5a247c5ccfc36f20c617f986aff70e6f5bbd69b0
",coder46,pablohoffman
647,2014-04-24 22:05:21,"thanks @coder46 - you should rebase to master due to #687 
",pablohoffman,coder46
647,2014-06-03 14:27:04,"@coder46 do you mind squashing your commits and rebasing on top of master? I'm cool if you don't have the time. thanks!
",dangra,coder46
647,2014-07-22 23:03:37,"@dangra Do we still want this link?
",nramirezuy,dangra
644,2014-03-11 13:33:01,"I'm agree with @pablohoffman, and also add a second parameter to filter by class. Because the biggest object on the most cases will be a response at least you are using lists or the request queue if you are not using the disk one.
",nramirezuy,pablohoffman
644,2014-03-16 22:36:16,"@darkrho , @kmike it seems to me pympler.asizeof module is the most convenient to use for getting objects approximate size. We may consider adding following snippet to doc which returns the _real_ biggest _tracked_ objects as the function name implies;



Using gc.get_objects() instead of tracked objects seems to be a bad idea, because since asizeof follows the referents of the objects, most of the items in returned list are objects related to running virtual machine instance like frames, modules etc., so far from being useful. 

Having a helper function that uses asizeof could be useful but i am not sure if having another library in requirements.txt is worth it.
",sardok,kmike
629,2014-03-07 13:51:10,"Great initiative @aspidites !
",pablohoffman,aspidites
629,2014-03-07 14:43:41,"Thanks @aspidites, so far only SEP-001 it's merged with 2 pull requests. For cleanness purposes, let's make sure each PR adds the RST and removes the trac file in the same commit. You can update existing PRs (with git push -f) or create new ones. I'm also happy with a larger PR with many SEPs migrated at once.
",pablohoffman,aspidites
629,2014-03-07 14:52:05,"Already a step ahead of you. You'll see the existing PRs have all been
updated already :-) As for combined PR, I'll write this last one separately
then commit the final ones as one large PR.
On Mar 7, 2014 9:43 AM, ""Pablo Hoffman"" notifications@github.com wrote:

> Thanks @aspidites https://github.com/aspidites, so far only SEP-001
> it's merged with 2 pull requests. For cleanness purposes, let's make sure
> each PR adds the RST and removes the trac file in the same commit. You can
> update existing PRs (with git push -f) or create new ones. I'm also happy
> with a larger PR with many SEPs migrated at once.
> 
> ## 
> 
> Reply to this email directly or view it on GitHubhttps://github.com/scrapy/scrapy/issues/629#issuecomment-37029738
> .
",aspidites,aspidites
629,2014-03-07 16:58:18,"@pablohoffman, my mistake. I actually preferred the smaller commits because it allows me to make micro changes without fear of breaking something else, which in turn means things that are ready can get merged sooner than things that are still getting worked on. 

That said...I apologize in advance, as I may or may not have forgone sleep in favor of finishing the remaining 7 SEP conversions...
",aspidites,pablohoffman
629,2014-03-10 10:59:25,"Thanks @aspidites!
",kmike,aspidites
624,2015-03-13 15:22:34,"@ananana hi! Sorry for such late follow up, we're planning on (finally) getting this pull request merged. 

There are some tasks still needed for doing so.
First, we should rebase current changes from `scrapy:master`. I tried to do it but there were some conflicts, maybe it's best to start a new pull requests and cherry-pick commits from this pull request.

Second, last commit (0587132282b3e7de15c5a491bc98ad686b4cfe15) is a merge, it'd be best if it wasn't, we use merge commits for merging pull requests, so it's preferred that all commits from a given pull request are simple commits.

Are you interested in making these changes? Let us know, we can implement them if that's not the case.
",curita,ananana
623,2014-04-28 15:47:12,"how about you @kmike ? :)
",pablohoffman,kmike
621,2016-10-17 09:15:05,"There was a recent change in ""master"" about this: https://github.com/scrapy/scrapy/pull/2190/files
I think we can close this. what do you think @nyov , @stummjr ?
",redapple,nyov
618,2014-02-27 19:22:55,"thanks @allait !
",pablohoffman,allait
616,2014-02-26 13:18:44,"@nramirezuy Yes, making copy outside is not so hard, but I'm talking about ease of use and unexpected results you can get without explicit copy creation.

Another example - what will happen with base item if I yield variant item like in example and then modify it in spider middleware? 



As far as I can see modification of variant item in spider middleware will result in modification of base item in response handler - because it is the same object. And if I yield another variant item after that - it still would be the same object with fields modified in spider middleware and response handler.
",chekunkov,nramirezuy
616,2014-02-26 13:36:52,"@chekunkov Over the whole documentation of item loaders it say that it populate the item, how this can be an unexpected result? http://doc.scrapy.org/en/latest/topics/loaders.html

The same is going to happen to you if you populate the item directly, using the dict-like API. That's why the Item Loader ask for an instance and not a class on his constructor.

You can also do something like:


",nramirezuy,chekunkov
616,2014-02-26 14:08:03,"@nramirezuy hm, good point, I reread documentation and you are right - I misunderstood the concept...

Still think it is error prone :stuck_out_tongue: But you can close the issue if you want to.
",chekunkov,nramirezuy
616,2014-04-08 20:09:57,"@kmike The main problem with the implicit copies is the entity check, so if you want a copy of an item do it by your self like a normal dict. If you need deepcopy we can add a deepcopy method to Item as a shortcut.

I'm agree with the `.add_value(None, base_item)` looks weird but works.
",nramirezuy,kmike
616,2016-09-15 16:22:26,"@chekunkov , I'm closing this issue as you seem(ed) ok with it.
@kmike , @chekunkov , feel free to object and re-open.
",redapple,chekunkov
616,2016-09-15 16:22:26,"@chekunkov , I'm closing this issue as you seem(ed) ok with it.
@kmike , @chekunkov , feel free to object and re-open.
",redapple,kmike
611,2014-02-24 12:34:03,"@deed02392 you are looking for this middleware (https://github.com/scrapy/scrapy/blob/master/scrapy/contrib/spidermiddleware/httperror.py), can you make a PR?
",nramirezuy,deed02392
611,2014-03-18 17:30:49,"Fixed by #612. Thanks @deed02392.
",kmike,deed02392
609,2014-03-05 04:48:06,"@barraponto it was changed, see https://github.com/scrapy/scrapy/commit/e31fb49320a5a2af0232b98ed509623db8a9b4c0
",kmike,barraponto
607,2015-03-24 09:13:50,"Closing it in favor of https://github.com/scrapy/scrapy/pull/1041. Thanks @staranjeet for working on it and @nyov for the heads up.
",kmike,nyov
605,2014-02-21 09:49:03,"Works like a charm. Thank you @kmike for good tips
",illarion,kmike
601,2017-02-22 13:04:12,"I'm with @nyov here, I'd say we close this PR. @kmike any objections?",redapple,nyov
601,2017-02-22 13:04:12,"I'm with @nyov here, I'd say we close this PR. @kmike any objections?",redapple,kmike
600,2014-02-20 09:05:17,"@kmike 

`python-request`'s [request object](http://docs.python-requests.org/en/latest/api/#requests.request) is well known for the newcomers.

One of the advantages would be to implement [Sessions](http://docs.python-requests.org/en/latest/user/advanced/#session-objects) for cookie persistence as well as common headers... instead of using middlewares or use meta as a session.

Implementing something like [oauth support](https://github.com/requests/requests-oauthlib/) would be awesome indeed.
",juanriaza,kmike
597,2014-04-25 21:06:56,"I'm happy to merge this, could you rebase it @chekunkov?.

It'd be better if we had the dupe filter interface documented too, but it can be a separate PR.
",pablohoffman,chekunkov
597,2014-04-26 12:51:10,"@pablohoffman done. I've rebased branch on top of the master + added short RFPDupeFilter.request_fingerprint interface description (just like @kmike suggested)
",chekunkov,kmike
597,2014-04-26 12:51:10,"@pablohoffman done. I've rebased branch on top of the master + added short RFPDupeFilter.request_fingerprint interface description (just like @kmike suggested)
",chekunkov,pablohoffman
592,2014-02-15 07:27:56,"@kmike , usually when sitemap spider works with a large site with a big hierarchy of sitemaps, the memory gets full of that large sitemaps. This approach is just generalized implementation of the pattern that is widely used, perhaps, in the project I am working on. I cannot share links here, because that project has closed sources, but I think you have access and we can discuss it with examples in chat.
",illarion,kmike
591,2015-03-25 20:53:28,"> 1) We can define a single class AddOnManager which will tackle the issue of importing modules for every such extension/middleware/pipeline etc.

+1 seems reasonable 

> 2) Users will have to define the name of the extension and any relevant settings (db username, password etc) in the settings.py. We can have all extensions clubbed under ADD_ONS dict.

I understand that adding an `ADD_ONS` setting in `settings.py` is more flexible (we won't have to implement anything new to handle it and we can use python directly), but I don't think we need this kind of flexibility, just a section to declare each addon and its settings is enough. `scrapy.cfg` is the first configuration file read (that's where we look for the `settings.py` path actually), so configuring the addons there could let us set extensions before `settings.py` is read. For example, a new extension could set a different `Settings` class. Not sure if this is needed, we can evaluate it, but I think I prefer sticking with `scrapy.cfg` if there are no apparent drawbacks.

> 3) To the _get_mwlist_from_settings method present in all the MiddlewareManagers, we add a call to AddOnManager
> 
> return build_components_list(...) + AddOnManager.get_add_ons(cls.name, settings, crawler)
> 
> The get_add_ons function will call the addon_configure function for each add-on and after finishing all the add-ons, check the crawler_ready function. The cls.name field will be used to match add-ons built for that component.

I haven't made a in-depth review yet, but I think this has a few implementation possibilities. For example, we could call in `MiddlewareManager._get_mwlist_from_settings` some method in AddOnManager that just calls `addon_configure` in all addons (though it doesn't even need to be called here, we can call it when we're updating the settings outside MiddlewareManager), and then call `crawler_ready` in `MiddlewareManager.from_settings`, when it's checking what addons are configured (maybe this could be outside the MidlewareManager too).

I'd personally like to keep the MiddlewareManagers as they are, say configuring the addons before or after running the managers (some managers can be configured by settings, I think that's my main reason for wanting to keep them untouched, that way we can maintain backward compatibility with user defined managers), but I'm not sure this is possible.  

> 4) We mandate each add-on to be an instance of an abstract AddOn class. This class defines functions such as update_settings (to change the value of a setting and mark it as unchangeable by future add-ons, useful incase of conflict; also to prefix addon name to setting name) , report_errors etc.

Makes sense.

> 5) Each add-on must add its name, version and requirements to the global settings. This way all add-ons which are instantiated after this, can access the settings to satisfy their dependencies. Any add-on requiring a module not present in crawler raises an error in the crawler_ready function. Any conflict in settings (between two add-ons, will be caught either in addon_configure or crawler_ready)

Makes sense, the AddOn class could have those ""name"", ""version"" and ""requirements"" as class attributes, and some method in AddOnManager could takes those and construct a dependency tree (I'd use an attribute in the crawler for this instead of the global settings, since those are user defined).

As @nyov said, it's kind of hard to get a robust dependency manager since there are a lot of components that could interact in different ways, maybe we could continue with the approach described in the SEP (setting the order of the middlewares in their '<component_MIDDLEWARE>' setting and checking dependencies manually in `crawler_ready`) for this GSoC unless we have spare time to implement it.

On a really side note, I hate the name ""crawler_ready"" :P if it's going to be used just for checking dependencies, let's change it to ""check_dependencies"" or ""check_configuration"" or something along those lines. I like changing ""addon_configure"" to ""update_settings"" as well, that's how it's called in the Spider class.

/cc @nramirezuy I recall you wanted to let different components update the settings, this idea is going to do that.
",curita,nyov
591,2015-04-05 00:47:14,"@SudShekhar:
About the rollback for updated settings when some add on fails, we should evaluate if this is necessary (we could just stop the execution if an add on fails as it was suggested). In case it’s necessary, this should be done entirely on our side to avoid add ons developers to duplicate the logic of updating the settings. Making a dry run could make sense then, but I think we should study different use-cases for settings updated by add ons, porting some extensions to this new add ons system should help. Take for example the spider middlewares, those you want to use are defined in SPIDER_MIDDLEWARES, but if something goes wrong when initializing them they just throw a NotConfigured error and they are disabled inside SpiderMiddlewareManager, no need to rollback settings. We could do something similar for add ons maybe, but we should review what settings can be updated and how to make sure.

About the `critical=[yes/no]` part, this is tied with closing the crawler immediately if dependencies aren't satisfied. Probably it’s easier to consider that any explicitly enabled add on is critical.

I think `update_settings` should work for setting things besides middlewares too (like the suggested `COMMANDS_MODULE` and `DUPEFILTER_CLASS`).
",curita,SudShekhar
591,2015-04-05 01:50:26,"@jdemaeyer
I’m thinking that an Addon class is good for enforcing having the methods we’re expecting them to have (`update_settings` and `check_configuration`) and maybe having a common parent for all add-on classes, but I don’t mind checking that they satisfy its interface using other way.

Providing additional variables to manage dependencies seems like a reasonable implementation. Those variables, if implemented, should be carefully considered to avoid backward incompatible changes or too many rewrites in existing addons after we promote their usage.
",curita,jdemaeyer
591,2015-04-05 10:50:12,"I'm agreeing on an AddonManager class as proposed to handle addons, but I would see an interface implementation (using [zope.interface as explained here](http://griddlenoise.blogspot.com/2007/05/abc-may-be-easy-as-123-but-it-cant-beat.html)) as more versatile alternative to requiring the subclassing of an `Addon` class.
@jdemaeyer, it would also be nice (IMO) if addons wouldn't be required to be thrown together in an `addons` directory, but rather keeping the distinctions between spidermw, downloadermw, pipelines etc. and the code separated. I hope that might be possible if we only need to check for interface implementers. (And for the proposal: `contrib` is going away, see #1063 .)

On the other hand, if all addons are required to be split into their own `.py` file, and be their own module (in python's sense), maybe it's possible to use `__name__, __version__, __requires__, __depends__` (etc.) module globals as Addon boilerplate instead. But not sure about that, not sure at all.
",nyov,jdemaeyer
591,2015-04-08 18:34:33,"Hi,
First of all, sorry for the late response. I had my university exams going on.

@Curita : So my basic idea goes like this: We just pass a variable `functionality` (say with value SPIDER_MIDDLEWARES) to the add_ons `update_settings` function. At this point, all add ons which don't implement this functionality (aren't SPIDER_MIDDLEWARES), raise the `NotConfigured` error. Others carry on. This way we can have add-ons which are enabled at multiple points (and the add-ons can decide what to do at which component). Any other error (apart from `NotConfigured`) at this stage stops crawling.

My reasons for proposing the `AddOn` class:
1. Lets us define an API like access to `crawler settings` and other parts of the code. We can use the existing priority count for settings (`default`, `command`, ..) or define a completely new framework independent of old code. We can define uniform error reporting interface (to server/console/file).
2. In general, we get more control over the add-ons.
3.  Iff we allow some add-ons to fail and still continue the crawling (the `critical=[yes/no` idea), this might make it easier to implement that. One example of the top of my head, An Ipython type add-on which visualizes crawling and logs to a local file and remote server. We might want to allow one or several of these functionalities to fail and not hinder crawling. 

@nyov : Indeed `zope.interfaces` is much more versatile in this regard. I guess the question comes down to whether we want to control the way add-ons update internal settings or not. I would love to have yours and others views on this issue.
And regarding the idea of splitting add-ons into different folders, it will make instantiating add-ons much easier (the `functionality` variable isn't needed) but then how do we handle add-ons with multiple hooks?
",SudShekhar,nyov
591,2015-05-25 13:25:25,"I will be working on an implementation of add-on management starting now!

While I finish up per-key settings priorities (#1149) as prerequisite (almost all add-ons will want to touch the dictionary settings), I will draft an updated version of the SEP this week. I think these are some of the major open issues (sorry, lots of text, feel free to skip sections):

##### 1. How is the add-on interface enforced?

I think our options here are:
1. Module: Promotes clear structure (one module per add-on), but does not allow us to provide any defaults (not all add-ons will want to do post-init tests, it might good to have a dummy `check_configuration()` callback)
2. Subclass of `AddOn`: Allows us to provide default callbacks, and, if we want that, to control how add-ons interact with the settings
3. `zope.interface`: Leaves the choice of object type (module, class) up to the developer

I tend to `zope.interface`, since it is the most versatile. Especially since we could still provide a base `AddOn` class with dummy callbacks and leave it up to the developer if they want to use it. Scrapy will have to deal with determining whether an addon path points to a module or a class, but that should only be a small update to `scrapy.utils.misc.load_object()`.

##### 2. Do we want to control how add-ons interact with settings?

@SudShekhar raised the question whether we want to provide some kind of API with which the add-ons interact with Scrapy's settings, rather than directly handing them the `Settings` object. I don't think this is necessary. After all, the people writing add-ons are developers; they should know what they're doing, I don't see a reason to restrict settings access. I'm open to being convinced otherwise though.

##### 3. Are add-ons configured in `scrapy.cfg` or `settings.py`?

I am not yet completely familiar with what file is intended for what. It seems reasonable to me to load add-ons as early as possible, since they might want to mess with / replace core components (including the `Settings` class). But if `scrapy.cfg` is not shipped by `scrapyd-deploy`, how do we make the add-on configuration portable?

##### 4. Can we keep (some) configuration local to the add-on?

It isn't 100% clear to me from the previous discussion whether people had this in mind already or not. Instead of exposing every single setting into the global settings space, we could hand over the configuration section (for this add-on) from `scrapy.cfg` to an add-on callback. The developer can then decide if she wants to insert them into the `Settings` instance or not.

For example, a pipeline add-on might define the add-on callbacks _and_ the pipeline class _in the same module_. This way, database settings (etc.) could be stored in module variables rather than in the global Scrapy settings, further reducing the chance of name clashes.

This example gives rise to a follow-up question: If we allowed setting not only paths, but also _objects_ as values in the (priority, value) middleware settings, the add-on could take care of instantiating a pipeline/middleware/extension. This could again be useful to avoid cluttering up the global settings space. (Plus, in the example above, the add-on callbacks and the pipeline callbacks could be defined in _the same class_, storing local settings in instance attributes. The same pipeline class could then be reused to write to multiple destinations, for example.)

##### 5. Where does the add-on manager live?

This is the question where I feel the most clueless as to what could be the best solution, and I've written down some thoughts on this in [my GSoC proposal](https://gist.github.com/jdemaeyer/7802befad56fa59fcfe1#integration-into-existing-start-up-process).

Right now I see two possible solutions, both of which I dislike:
1. Setting changes made by the add-ons are not very different from setting changes made by the user in `settings.py`, and should therefore be included in what is returned by `scrapy.utils.project.get_project_settings()`. However, the add-on manager cannot live inside of that function since it is left long before we want to call the `check_configuration` callbacks. If we don't want to start passing around the add-on manager, we will have to introduce a new singleton, e.g. `scrapy.addons.addonmanager`.
2. Spider-specific settings are loaded in `Crawler.__init__()`. Add-on management could follow this approach: instantiate the add-on manager and do the first round of callbacks here, then do the second round of callbacks in `Crawler.crawl()` or on the `engine_started` signal. This avoids the extra singleton but further rips apart compiling the complete configuration settings.

##### 6. Can add-ons interact, and how?

I have not yet thought this through, but I like the idea that add-ons can interact with each other. For example, instead of forcing every extension/middleware/pipeline to provide their own mongodb interface and the corresponding settings variables, there could be an extension (as in set in `EXTENSIONS`) providing a mongodb API (promoting UNIX ""do one thing and do it well"" philosophy). Other add-ons can then access this extension and its methods through `Crawler.addons['mongodb']` or similar.

Actually this can probably already be done through `Crawler.extensions`. Is there a way we can promote better modularity?

##### 7. Dependency management

So far I feel full-fledged, package-manager style dependency management might be a bit overkill for  the first version of an add-on manager. In [my proposal](https://gist.github.com/jdemaeyer/7802befad56fa59fcfe1#writing-add-ons), I suggested that each add-on _must_ provide the two attributes `NAME` and `VERSION`, and _should_ provide `REQUIRES`, `MODIFIES`, and `PROVIDES` to allow some basic dependency checks. I agree with @Curita that the variables which usages we promote should be carefully selected to avoid future rewrites and would love some feedback on these suggested here.

@nyov I agree that add-ons should not be required to be put into an `addons` directory, rather developers should be allowed to decide whether they want to provide the add-on interface alongside their pipeline/middleware/extension or provide it in an extra module/class. I had the `addons` directory in mind for the latter case.
",jdemaeyer,nyov
591,2015-05-25 13:25:25,"I will be working on an implementation of add-on management starting now!

While I finish up per-key settings priorities (#1149) as prerequisite (almost all add-ons will want to touch the dictionary settings), I will draft an updated version of the SEP this week. I think these are some of the major open issues (sorry, lots of text, feel free to skip sections):

##### 1. How is the add-on interface enforced?

I think our options here are:
1. Module: Promotes clear structure (one module per add-on), but does not allow us to provide any defaults (not all add-ons will want to do post-init tests, it might good to have a dummy `check_configuration()` callback)
2. Subclass of `AddOn`: Allows us to provide default callbacks, and, if we want that, to control how add-ons interact with the settings
3. `zope.interface`: Leaves the choice of object type (module, class) up to the developer

I tend to `zope.interface`, since it is the most versatile. Especially since we could still provide a base `AddOn` class with dummy callbacks and leave it up to the developer if they want to use it. Scrapy will have to deal with determining whether an addon path points to a module or a class, but that should only be a small update to `scrapy.utils.misc.load_object()`.

##### 2. Do we want to control how add-ons interact with settings?

@SudShekhar raised the question whether we want to provide some kind of API with which the add-ons interact with Scrapy's settings, rather than directly handing them the `Settings` object. I don't think this is necessary. After all, the people writing add-ons are developers; they should know what they're doing, I don't see a reason to restrict settings access. I'm open to being convinced otherwise though.

##### 3. Are add-ons configured in `scrapy.cfg` or `settings.py`?

I am not yet completely familiar with what file is intended for what. It seems reasonable to me to load add-ons as early as possible, since they might want to mess with / replace core components (including the `Settings` class). But if `scrapy.cfg` is not shipped by `scrapyd-deploy`, how do we make the add-on configuration portable?

##### 4. Can we keep (some) configuration local to the add-on?

It isn't 100% clear to me from the previous discussion whether people had this in mind already or not. Instead of exposing every single setting into the global settings space, we could hand over the configuration section (for this add-on) from `scrapy.cfg` to an add-on callback. The developer can then decide if she wants to insert them into the `Settings` instance or not.

For example, a pipeline add-on might define the add-on callbacks _and_ the pipeline class _in the same module_. This way, database settings (etc.) could be stored in module variables rather than in the global Scrapy settings, further reducing the chance of name clashes.

This example gives rise to a follow-up question: If we allowed setting not only paths, but also _objects_ as values in the (priority, value) middleware settings, the add-on could take care of instantiating a pipeline/middleware/extension. This could again be useful to avoid cluttering up the global settings space. (Plus, in the example above, the add-on callbacks and the pipeline callbacks could be defined in _the same class_, storing local settings in instance attributes. The same pipeline class could then be reused to write to multiple destinations, for example.)

##### 5. Where does the add-on manager live?

This is the question where I feel the most clueless as to what could be the best solution, and I've written down some thoughts on this in [my GSoC proposal](https://gist.github.com/jdemaeyer/7802befad56fa59fcfe1#integration-into-existing-start-up-process).

Right now I see two possible solutions, both of which I dislike:
1. Setting changes made by the add-ons are not very different from setting changes made by the user in `settings.py`, and should therefore be included in what is returned by `scrapy.utils.project.get_project_settings()`. However, the add-on manager cannot live inside of that function since it is left long before we want to call the `check_configuration` callbacks. If we don't want to start passing around the add-on manager, we will have to introduce a new singleton, e.g. `scrapy.addons.addonmanager`.
2. Spider-specific settings are loaded in `Crawler.__init__()`. Add-on management could follow this approach: instantiate the add-on manager and do the first round of callbacks here, then do the second round of callbacks in `Crawler.crawl()` or on the `engine_started` signal. This avoids the extra singleton but further rips apart compiling the complete configuration settings.

##### 6. Can add-ons interact, and how?

I have not yet thought this through, but I like the idea that add-ons can interact with each other. For example, instead of forcing every extension/middleware/pipeline to provide their own mongodb interface and the corresponding settings variables, there could be an extension (as in set in `EXTENSIONS`) providing a mongodb API (promoting UNIX ""do one thing and do it well"" philosophy). Other add-ons can then access this extension and its methods through `Crawler.addons['mongodb']` or similar.

Actually this can probably already be done through `Crawler.extensions`. Is there a way we can promote better modularity?

##### 7. Dependency management

So far I feel full-fledged, package-manager style dependency management might be a bit overkill for  the first version of an add-on manager. In [my proposal](https://gist.github.com/jdemaeyer/7802befad56fa59fcfe1#writing-add-ons), I suggested that each add-on _must_ provide the two attributes `NAME` and `VERSION`, and _should_ provide `REQUIRES`, `MODIFIES`, and `PROVIDES` to allow some basic dependency checks. I agree with @Curita that the variables which usages we promote should be carefully selected to avoid future rewrites and would love some feedback on these suggested here.

@nyov I agree that add-ons should not be required to be put into an `addons` directory, rather developers should be allowed to decide whether they want to provide the add-on interface alongside their pipeline/middleware/extension or provide it in an extra module/class. I had the `addons` directory in mind for the latter case.
",jdemaeyer,SudShekhar
589,2014-07-23 15:48:52,"@dangra ""lastest"" is 0.24 now. Can we close this?
",nramirezuy,dangra
589,2016-01-27 12:57:58,"https://github.com/rtfd/readthedocs.org/issues/635 is fixed.
I don't understand the issue, maybe because docs are fine now.
@dangra ?
",redapple,dangra
589,2016-01-27 14:23:20,"@dangra , ok, they look the same to me
",redapple,dangra
589,2016-02-08 13:42:22,"Right @Digenis , something is wrong.
I mean 1.1 is not stable yet. `master` is indeed at 1.2(dev)
https://github.com/scrapy/scrapy/commit/8f269558f164b0ad0e18545654ff4c2ff89d437b
",redapple,Digenis
589,2016-02-11 04:39:39,"@Digenis I think we're going to stop using `odd/even => unstable/stable` versioning scheme; 1.0 or 1.1 are new 'even' and 1.1dev or 1.2dev is a new 'odd'.
",kmike,Digenis
589,2016-02-11 04:41:12,"But that's a good catch @Digenis - http://doc.scrapy.org/en/latest/versioning.html mentions odd numbered versions as development.
",kmike,Digenis
584,2014-02-05 19:48:39,"Good catch @nramirezuy and good fix @dangra - these `__subclasscheck__` methods can drive anyone nuts.
",kmike,nramirezuy
584,2014-02-05 19:48:39,"Good catch @nramirezuy and good fix @dangra - these `__subclasscheck__` methods can drive anyone nuts.
",kmike,dangra
583,2014-07-23 16:37:04,"@Digenis should we name them:
`ServerTimeoutError` -> `UserTimeoutError`
`UserTimeoutError` -> `DeferUserTimeoutError`

Is that what are you looking for?
",nramirezuy,Digenis
583,2014-07-23 18:17:06,"@dangra #815 created
",nramirezuy,dangra
583,2014-07-31 12:44:02,"I am catching up with all the issues right now, I absented a while.
@nramirezuy yes, can't tell about DeferUserTimeoutError but ServerTimeoutError used to mislead
",Digenis,nramirezuy
580,2014-02-05 17:13:32,"@dangra +1
",kmike,dangra
579,2014-02-04 21:16:35,"@kmike , I can reproduce the `UnicodeError` in an ipython session:


",redapple,kmike
578,2014-04-15 14:33:21,"See also @nyov's comments: https://github.com/scrapy/scrapy/issues/568#issuecomment-34887656 and https://github.com/scrapy/scrapy/issues/568#issuecomment-34898235. @nramirezuy - are you proposing something similar?
",kmike,nramirezuy
578,2014-04-15 14:44:56,"@kmike We aren't going to remove `Selector` class, if people wants to write list comprehensions they can still do it. They can also forget about `Loader` and use the processor directly. 
The main reason of be classes instead of function is because some of them need instantiation data or hold a ""state"".

The `Loader` is the pipeline and the context mainly, I can think in nothing else right now. So `Loader` will be a pipeline that can work with a context. Right now `ItemLoader` make use of 2 static (in and out) pipelines per field and 1 optional before ""in"".

> This would give us a separation of concerns here:
> Selector handles Selector(Lists)
> Extractor handles extraction with processors
> LinkExtractors are special Extractor cases (processors)
> ItemLoaders get much more simple without the selector baggage and possibly reduced to a single 'output processor' chain?
> Haven't quite thought this through, but it looks neat in concept ;)

Selector will not change.
Extractor/Loader handles the processor chain and the context of the chain.
LinkExtractor, may not exists because it is a Extractor using processors, nothing special of doing a personalized class.
ItemLoaders will not change API or functionality, just cleaned to use Loader.
",nramirezuy,kmike
578,2014-04-15 21:53:46,"+1 to uplifting the `Link` object.
@kmike, wouldn't that be a nice place to keep some state about a URL and implement 'have-absolute-urls' in spiders? It could have the logic to figure out it's absolute URL based on context at instancing time, and maybe a place to attach other meta info about a link.

---

What my idea was in this quoted comment, IIRC, is that the 'output processor' could be relegated back to the Item with this logic.
We once had a `DefaultsItem` and the magic is still there. If this 'final loading/populating item' stage gets simplified/generalized enough, it could be put into the `Item` class and then customized by subclassing `Item` where necessary (drop None values or not?, bring back `DefaultsItem`?). I think that's an intuitive way to customize item population?

THEN, all that people would need to understand is this `Extractor` and processor chains.
And they would be a drop-in addition to the spider, instead of current need to rewrite logic for `ItemLoader`.
I think this could also scale more in both dimensions, why limit it to an 'In' and '~~Out~~ In2' stage. Need another stage, add an Extractor to the chain.
And in the Extractor, you can chain Processors as right now.

From an architecture overview, this would look like `Extractor` == another `Middleware` and `Processor` == `ItemPipeline` I think; sorry for the uglyfied image.

![scrapy_architecture2](https://cloud.githubusercontent.com/assets/438293/2713509/00444962-c4e8-11e3-8626-f74c98a05104.png)
",nyov,kmike
577,2014-02-03 13:26:26,"@dangra So, loaders are core now?
",nramirezuy,dangra
577,2014-02-05 16:34:04,"@nramirezuy yes, it's time to promote them. They were contrib for a long time, they are in good use by lot of projects, and no alternative come up.
",dangra,nramirezuy
577,2014-02-05 16:46:20,"@dangra I think they can be improved and reused https://github.com/scrapy/scrapy/issues/578, before promotion
",nramirezuy,dangra
574,2014-02-03 19:03:59,"Thanks for looking into this @redapple!
",pablohoffman,redapple
573,2014-07-23 17:01:26,"This looks like already solved? Am I right? @dangra 
",nramirezuy,dangra
573,2014-09-29 06:48:25,"I agre with @immerrr that the best (and maybe the only good) way to fix this issue is to port Scrapy to Python 3.x. Adding workarounds to Scrapy log system so that it prints container (list, set, dict) contents by recursively encoding individual elements to utf-8 and building a result string instead of using bultin `repr` doesn't feel right.

We may fix it for a common case of scrapy.Item, but I'd not go any further, and this fix won't be needed in Python 3.x. 

`unicode-escape` codec is a wrong way to tackle this issue because if you know a string is a repr of a Python object (e.g. of a container) it is better to work on this object directly, and if you don't know that you shouldn't use `unicode-escape` codec.
",kmike,immerrr
573,2014-10-06 17:07:31,"I agree with @immerrr too, he nailed it. Good research.
",dangra,immerrr
570,2014-01-31 17:18:17,"this work unblocked the queue of PRs.
thanks @redapple and @kmike! 
",dangra,redapple
570,2014-01-31 17:24:34,"you're welcome @dangra 
",redapple,dangra
568,2014-01-30 14:32:03,"As @dangra said, you can always select n-th element using xpath, so makes it sense to write something like this?
`sel.xpath('//p/text()').extract(2)`
",shirk3y,dangra
568,2014-02-01 20:53:41,"I like @nramirezuy `.extract(index)` and why not `.re('\d+', index)`?
",umrashrf,nramirezuy
568,2014-02-02 14:03:47,"I was also leaning towards `.extract(index)`, and `.re('\d+', index)`, as that seems more versatile. (It's similar to what [I'm currently using](https://gist.github.com/nyov/8768180).)
But after thinking this through, it's correct that xpath or css selectors can always be tuned to get the correct single item.

So +1 to @pablohoffman for `.extract1()` / `.re1()`. (How about an alias `.x()` / `.x1()` / `.r()` ?)

And for returning a single index (or range) as a list, there is still list slicing syntax `.extract()[-1:]`.

(edit)
I would also add that `extract1` / `x1` looks better, as it may be referred to as ""extract first"" or ""extract one""; because whether the return is a first/last/middle value is in the eye of the selector ;) But we know it'll always be a single value.
",nyov,pablohoffman
568,2014-02-02 15:36:52,"@kmike,
I would argue the opposite. If you name it `extract_first()`, people might looking for `extract_last()` and miss the point of using selectors to achieve that goal. (Happened to me.)

Certainly if the selector returns multiple elements, it'll pick the first value of that list. But more often I would have a selector tailored to only return a single element in the first place, say `xyz[last()]`.
And then from that perspective (of the selector) `extract_first()` might even look strange, since I'm trying for the last element.
I think `extract1()` would just as easily be remembered for both ""first"" and ""single"".
(The `1` vs `l` issue I would always argue as a 'using the wrong font' problem :)

@shirk3y, interesting - but then IMO it'd feel more natural again to name it .get() where that behaviour is more expected, or as a second argument to extract() as `extract(1, 'fail')`.
`.extract1() or 'fail'` might be enough? Though I'm not opposed in general.
",nyov,kmike
568,2014-02-03 17:54:06,"@nramirezuy I think there are several reasons for not using ItemLoaders. Let's try best practices:



That's a lot of new concepts and 7 extra lines of code in 2 files, while user just wanted to do `.extract()[0]` and avoid exceptions if nothing is extracted. Item loaders can be useful, and this boilerplate is justified if MyItemLoader is going to be used several times, or if it is possible to write some base item loader for many items, but having one item per spider is quite common.

Items loaders are powerful, but not simple. There are input processors, output processors (if you ask me I still can't tell the difference without re-reading the docs), Compose and MapCompose (the same), import locations to remember (`from scrapy.contrib.loader`), magic class attributes (`default_item_class`) and conventions for magic class attributes (`_out`) to remember, and no docstrings to lookup that all in IDE. I still can't remember what arguments should processor take (a value? an iterable with values? maybe something different for input and output processors?) and what should they return.
",kmike,nramirezuy
568,2014-02-03 19:14:52,"@kmike You can use ItemLoaders straight forward without define a custom one. 
`TakeFisrt` is not the same as `list[0]`


",nramirezuy,kmike
568,2014-02-05 17:41:55,"This is becoming a discussion on using item loaders or not :)
My 2c on that: I agree with @kmike that item loaders can be powerful but are not easy. AFAIK, Item loader do not help when you traverse document and select different portions and within each select a few data bits with short selectors. More often than not, I find myself working with selector extracted data and then using `add_value()`. That may not be best practice but that works for me.

And for the name, numbers in method names, especially `1`, makes me frown. And fonts? well sometimes you look at code from mobile phones, in stackoverflow, in google groups etc. you don't always choose it ;)
",redapple,kmike
568,2014-02-05 18:01:28,"@redapple, +1 to deprecate `.extract()` and add `.find()` and `.findall()` (or something along that names). The `.re()` can be a paremeter: `.find(re=""\d+"")`, just like the loader which allows the optional re parameter.
",rolando,redapple
568,2014-02-05 18:17:52,"@redapple I'm just trying to point that this feature is not needed, Loaders are the way to go. I think the examples I posted here are not complicated at all.

I would like to hear something from the Elders (@dangra, @pablohoffman), about this.


",nramirezuy,redapple
568,2014-02-05 18:17:52,"@redapple I'm just trying to point that this feature is not needed, Loaders are the way to go. I think the examples I posted here are not complicated at all.

I would like to hear something from the Elders (@dangra, @pablohoffman), about this.


",nramirezuy,dangra
568,2014-02-05 18:17:52,"@redapple I'm just trying to point that this feature is not needed, Loaders are the way to go. I think the examples I posted here are not complicated at all.

I would like to hear something from the Elders (@dangra, @pablohoffman), about this.


",nramirezuy,pablohoffman
568,2014-02-06 13:59:13,"we definitely need some wisdom from the ""elders"" @pablohoffman @dangra :)
",redapple,dangra
568,2014-02-06 13:59:13,"we definitely need some wisdom from the ""elders"" @pablohoffman @dangra :)
",redapple,pablohoffman
568,2014-02-12 17:38:41,"Let's compare one of the Nicolas' examples (very compelling, by the way) with an example of how it could be written without item loaders:



This is how it may look like without item loaders:



Pro item loader:
- it saves us from two `.extract()` boilerplate calls;
- in the final dict we must write `url=...` again (and do it for each key) if we compute values using local variables;
- it allows to create a ""pipeline"" of functions in a declarative way - this is what I like most.

Neutral:
- if the value is None it won't appear in the final dict; no special-casing of None values or pre-creating of empty lists is required.
- `take_first(sel.xpath('...'))` vs `ld.get_xpath('...', TakeFirst())` is a question of style; I think that both are fine. First is easier to grasp because it closely follows Python semantics - it is easier to understand just by looking at source code. The second is more declarative. There are packages like https://github.com/kachayev/fn.py that allows to combine functions in pipelines without too much (((())))) nesting if that is an issue.

Cons item loader:

For me the main issue it that they are stateful. When you call `sel.xpath(..)` or `.extract()` you get a new value; when you call `.add_xpath` you mutate some variable in a non-obvious way. I don't like `loader.url_out = Join(', ')` either.

It is named ""item loader"", but when you work with it you should think of it as of an item with some extra voodoo, not as of data transformation pipeline.

Another issue is #578 - as Nicolas said, there are item loaders, selectors and link extractors, and they can do basically the same. `ld.get_xpath` looks awfully like selectors. It is not good we have so many slightly different ways to do a basic task.

The third issue is complexity - I agree with @nyov here.

It is good this discussion is started before we merged `extract_first()` PR or promoted item loaders from contrib :)
",kmike,nyov
568,2014-02-19 00:15:55,"I learned a lot on using `ItemLoaders` from reading this thread.
I guess it could be commented in `scrapy/templates/spiders/crawl.tmpl` to expose more users to it.

and @pablohoffman, please, `extract1` is an awful name. :-1:  
",barraponto,pablohoffman
568,2014-02-20 05:24:10,"@barraponto so what was your suggestion?
",pablohoffman,barraponto
568,2014-02-27 14:37:00,"@Digenis I like your approach, it is even better than `extract_one()` because you can select the element you want on the list.


",nramirezuy,Digenis
568,2014-02-27 21:05:38,"

Ok, so we could write



But then we have a `ValueError` again for multiple list items. So we need at least



Or the other one



This isn't so bad, but `.extract()[:1] or [None]` vs. `.extract1()` is still a difference. Besides having to remember to do list unpacking on the left hand side.

> Do such idioms satisfy anyone else?

As @pablohoffman said, this is a method which is used very frequently in spider code, and I'd still prefer a shorthand instead of appending `[:1] or [None]` all the time.

Now if `extract()`/`re()` would already return `[None]` instead of an empty list, this could be shortened to `extract()[0]` or `extract()[-1:][0]`... which I might see as best solution yet, but maybe there'd be other issues with doing that.
",nyov,pablohoffman
567,2014-04-24 17:35:30,"@dangra I like the default for errback, errback is as important as callback. 90% of the projects I'm currently working ask for items on error. But how it solves the problem?

@darkrho You have the stat per status code. But we can add a one time message on INFO of the status code like in Dupefilter. This spider was on production or dev ?
",nramirezuy,dangra
567,2014-04-24 17:42:34,"@nramirezuy : you are right about it doesn't address the initial problem, it only helps in the synthetic snippet shown as demo. I proposed it after seeing @redapple comment and because I had the same need before.

anyway, in my opinion the main issue is already addressed by #612
",dangra,nramirezuy
567,2014-04-24 17:42:34,"@nramirezuy : you are right about it doesn't address the initial problem, it only helps in the synthetic snippet shown as demo. I proposed it after seeing @redapple comment and because I had the same need before.

anyway, in my opinion the main issue is already addressed by #612
",dangra,redapple
566,2014-01-29 14:19:03,"a lot of failed tests due to reordering of expected results.
@dangra , @kmike , any idea why? are the test too strict?
or did I miss something big?

Let's see after merging master in...
",redapple,dangra
566,2014-01-29 14:19:03,"a lot of failed tests due to reordering of expected results.
@dangra , @kmike , any idea why? are the test too strict?
or did I miss something big?

Let's see after merging master in...
",redapple,kmike
563,2014-02-03 19:06:43,"@nramirezuy are you sure we're not sacrificing too much convenience in favor of correctness?
",pablohoffman,nramirezuy
563,2014-02-03 19:41:00,"@pablohoffman I don't know anything about the schema. But if it can handle the alternative links by grouping them and that enough, for me it's fine to merge, just add the test (:
",nramirezuy,pablohoffman
562,2014-01-24 21:28:07,"@kmike I was thinking in this today and maybe `.extract()` can accept encoding as an argument and default unicode, sometimes we need the bytes.
",nramirezuy,kmike
562,2014-01-25 03:01:58,"> Selector.extract method returns unicode by calling tostring(..., encoding='unicode'), and this is handy, but it could make sense to have an another method (or an option) (for internal use?) - it should return byte strings in the response encoding.

@kmike : what is the internal use case you see for this?

> I was thinking in this today and maybe .extract() can accept encoding as an argument and default unicode, sometimes we need the bytes.

@nramirezuy : show me a case where returning bytes worth it instead of encoding outside. 
",dangra,nramirezuy
562,2014-01-25 03:01:58,"> Selector.extract method returns unicode by calling tostring(..., encoding='unicode'), and this is handy, but it could make sense to have an another method (or an option) (for internal use?) - it should return byte strings in the response encoding.

@kmike : what is the internal use case you see for this?

> I was thinking in this today and maybe .extract() can accept encoding as an argument and default unicode, sometimes we need the bytes.

@nramirezuy : show me a case where returning bytes worth it instead of encoding outside. 
",dangra,kmike
562,2014-01-25 11:31:57,"@dangra , patch makes sense to me
",redapple,dangra
562,2014-02-03 17:06:56,"@dangra a common case is when you select to create a request, we always use the returned unicode directly, this works because urls should be ascii compatible but this is wrong.
",nramirezuy,dangra
560,2014-07-02 21:53:01,"Indeed @nyov, go ahead with the PR @darkrho!
",pablohoffman,nyov
559,2014-02-03 16:55:43,"@redapple These are my thoughts about the subject: https://github.com/scrapy/scrapy/issues/578
",nramirezuy,redapple
559,2014-06-20 11:28:52,"@kmike , @darkrho , @nramirezuy , @pablohoffman , probably room for cleanup but this should contain all fixes from your comments.
Question: go that route? or add another extractor based on `Selector`?
",redapple,nramirezuy
559,2014-06-20 11:28:52,"@kmike , @darkrho , @nramirezuy , @pablohoffman , probably room for cleanup but this should contain all fixes from your comments.
Question: go that route? or add another extractor based on `Selector`?
",redapple,kmike
558,2014-01-28 14:36:34,"@pablohoffman OK, I will add the description to the documentation.
",rolando,pablohoffman
558,2014-07-27 00:52:17,"@nramirezuy I totally forgot about this PR. I'll do it in the next days, although first I'll test the dockerfile against the latest docker version.
",rolando,nramirezuy
556,2014-01-28 03:44:23,"@nyov, this doesn't affect the general behavior of an item, and you can always use `item[""field""] = None`.

> Example: Defuncts a SQL Database pipeline which uses None for NULL.

This has nothing to do with the pipelines. Moreover, it doesn't add any new behavior because `None` values already ignored by the item loader. For example, using Scrapy 0.20:


",rolando,nyov
556,2014-01-29 09:02:31,"as the behaviour is(was) different between add_value() and for processor return value, it makes sense to make it consistent (one way or the other).
but I understand @nyov uses case, so could `ItemLoader` have a parameter to ignore `None` values or not?
",redapple,nyov
554,2014-01-27 17:48:57,"@redapple: I thought that was the implicit idea, and the ticket was more like adding the shortcuts attached to the response.
",dangra,redapple
554,2014-02-03 13:35:25,"@nramirezuy , would that be handled by a `@property` decorator on `selector` attribute? i.e. parsing only if `response.selector` is accessed
",redapple,nramirezuy
554,2014-02-03 18:28:31,"> @nramirezuy , would that be handled by a @property decorator on selector attribute? i.e. parsing only if response.selector is accessed

We definitively wants it to be lazily evaluated.
",dangra,nramirezuy
553,2014-01-22 11:07:03,"@redapple This is relative because the CrawlSpiders will get all urls and attempt to crawl them, but I like the stat

+1
",nramirezuy,redapple
553,2014-01-22 11:14:08,"@nramirezuy , yes I guess for CrawlSpider this would mean a lot of log spam, but in my case in context of manual (Base)Spider I had issues with next pages link not getting next products (always returning the same page of paginated results) and I couldn't figure it out until I logged all duplicate requests.

Updated the PR description regarding CrawlSpider :)
",redapple,nramirezuy
553,2014-01-22 11:43:20,"@kmike done.
",redapple,kmike
553,2014-01-28 12:49:48,"@redapple: indeed :) can you ammend + squash? I'll merge as soon as is ready. 
",dangra,redapple
553,2014-02-04 11:53:50,"@redapple I think is something older, that almost nobody wants to disable, so nobody asked before.
As I can see it can be implemented as a SpiderMiddleware.
",nramirezuy,redapple
553,2014-02-05 13:30:02,"@pablohoffman @dangra @kmike @nramirezuy , could the potential refactoring at scheduler level be done in another PR?
is something missing before merging?
",redapple,kmike
553,2014-02-05 13:30:02,"@pablohoffman @dangra @kmike @nramirezuy , could the potential refactoring at scheduler level be done in another PR?
is something missing before merging?
",redapple,nramirezuy
553,2014-02-05 13:30:02,"@pablohoffman @dangra @kmike @nramirezuy , could the potential refactoring at scheduler level be done in another PR?
is something missing before merging?
",redapple,dangra
553,2014-02-05 13:30:02,"@pablohoffman @dangra @kmike @nramirezuy , could the potential refactoring at scheduler level be done in another PR?
is something missing before merging?
",redapple,pablohoffman
552,2014-07-24 02:17:36,"the PR looks good but as @kmike pointed rtfd/readthedocs.org#635 is a blocker IMO.
",dangra,kmike
548,2014-02-03 13:20:34,"> Although, I don't think selectors deserves this task, mostly because linkextractors (#559) are designed to deal with it instead, I will take a step toward this idea in the spirit of #494 and #554 and combine linkextractors into selectors API

@dangra now this idea became good? https://github.com/scrapy/scrapy/pull/331
",nramirezuy,dangra
548,2014-02-03 17:36:07,"> @dangra now this idea became good? #331

As I said, it doesn't convince me but worth revising it now that it gets more eyeballs, and can be part of the broad improvements to the API. 
",dangra,dangra
548,2014-02-03 18:20:15,"I like @nyov's, @dangra's and @nramirezuy's ideas to fix this issue by providing an easier way to extract absolute urls. This is an indirect solution, but it seems that `.links()` method could work well in practice.

No hard feelings from me too, just starting the discussion :)
",kmike,nyov
548,2014-02-03 18:20:15,"I like @nyov's, @dangra's and @nramirezuy's ideas to fix this issue by providing an easier way to extract absolute urls. This is an indirect solution, but it seems that `.links()` method could work well in practice.

No hard feelings from me too, just starting the discussion :)
",kmike,nramirezuy
548,2014-02-03 18:20:15,"I like @nyov's, @dangra's and @nramirezuy's ideas to fix this issue by providing an easier way to extract absolute urls. This is an indirect solution, but it seems that `.links()` method could work well in practice.

No hard feelings from me too, just starting the discussion :)
",kmike,dangra
548,2014-02-03 19:20:39,"@kmike @dangra How about this and just create a filter/processor? https://github.com/scrapy/scrapy/issues/578
",nramirezuy,dangra
548,2014-02-03 19:20:39,"@kmike @dangra How about this and just create a filter/processor? https://github.com/scrapy/scrapy/issues/578
",nramirezuy,kmike
548,2014-02-03 22:36:15,"@dangra 

> > Or make that Request(url_or_response) resp. FormRequest(url_or_response)?
> 
> hmm... I don't get it.

I was meaning `Request(url=)` might be simplified to take a response object instead of the more verbose `Request.from_response()`. But it's slightly unrelated.

> I don't see how self.request() could help with the absolute url problem.

No, but it would remove the need to import scrapy for `scrapy.Request()`. So in the interest of https://github.com/scrapy/scrapy/issues/554#issuecomment-33066705, I was thinking the class might have the request factory in that case, instead of the response.
",nyov,dangra
548,2014-06-07 11:22:02,"The Selector instance already references the response, no need to pass it as a parameter.
Although I like having shortcuts to lxml's methods, I wouldn't use this one, I don't want to change the tree in place, I 'd prefer it as an argument to the constructor.
P.S. Like @chekunkov sets it, but I 'd prefer False as default
",Digenis,chekunkov
548,2014-06-08 06:00:31,"1) Currently all Selectors for the same Response share the same copy of a parsed tree; inplace modification becomes more tricky because of that. We'll have to deepcopy the tree and change it if we don't want the changes to affect unrelated code.

2) `make_links_absolute` as a Selector constructor argument doesn't allow to customize how `response.xpath` and `response.css` methods work, and `response.css('.mycls a::attr(href)')` is an important use case for absolute links.

3) `make_links_absolute` as a Selector method (not as a Selector constructor argument) changes the selector state; imho it is better to avoid adding state-mutating methods because they are a source of bugs.

4) Agreed with @Digenis - `make_links_absolute` shouldn't be a default. For one, it is not possible to restore original url values after they are made absolute; we shouldn't mess with HTML by default.

~~I'm not sure I like that in @dangra's example `links()` method returns Link objects and not urls directly. But~~ I like an approach with link-extraction method more than `make_links_absolute` because it does not require to modify a shared state - its effect is local. Users request links, they get them, no gotchas.
",kmike,dangra
548,2014-06-08 06:00:31,"1) Currently all Selectors for the same Response share the same copy of a parsed tree; inplace modification becomes more tricky because of that. We'll have to deepcopy the tree and change it if we don't want the changes to affect unrelated code.

2) `make_links_absolute` as a Selector constructor argument doesn't allow to customize how `response.xpath` and `response.css` methods work, and `response.css('.mycls a::attr(href)')` is an important use case for absolute links.

3) `make_links_absolute` as a Selector method (not as a Selector constructor argument) changes the selector state; imho it is better to avoid adding state-mutating methods because they are a source of bugs.

4) Agreed with @Digenis - `make_links_absolute` shouldn't be a default. For one, it is not possible to restore original url values after they are made absolute; we shouldn't mess with HTML by default.

~~I'm not sure I like that in @dangra's example `links()` method returns Link objects and not urls directly. But~~ I like an approach with link-extraction method more than `make_links_absolute` because it does not require to modify a shared state - its effect is local. Users request links, they get them, no gotchas.
",kmike,Digenis
548,2014-06-08 08:12:24,"1) I haven't ever seen the case when several different `Selectors` created for a single `Response`. That really happens? Any reason for doing that? Even if spider downloads the same response second time it's often processed by the same `Selector`. So if developer decides that he needs `Selector` to return absolute links from response it's highly unlikely that for the same response later he will change his decision. And even if it will happen - he can use `Selector`s with disabled absolute links from the start.

2) Oh, those new shortcuts :( That's inconvenience. First solution that came to mind - for customizing response selectors developer could pass some value to `Request` meta (or maybe introduce new `Request` argument which will control that behavior).

4) From my experience we want `Selector` to return absolute urls almost all the time. I expect default value to be the most commonly used case, not one that doesn't change original values. If we will have default `make_links_absolute = False` that will introduce extra typing for the developers - which we are trying to avoid as I understood from ticket description.

> For one, it is not possible to restore original url values after they are made absolute

One can pass `make_links_absolute = False`, get original values and do whatever he wants with them.

5) @dangra's `.links()` proposal... I kinda dislike idea of adding extra methods to `Selector` API. But maybe you have a point.
",chekunkov,dangra
548,2014-06-08 09:03:13,"@kmike I really like the `with_absolute_links` idea. What would the `links` method return?
",juanriaza,kmike
548,2014-06-08 09:20:58,"@kmike 

> and there are (1), (2) and (3) to solve.

(1), (2). I think `make_links_absolute` should not be a separate method from the beginning - because of the reasons you've guys already mentioned :)

> The biggest disadvantage of links() is that it is a new API, but it dosn't have the problems that make_links_absolute has.

hm, ok, you almost convinced me, maybe `.links()` would be better solution. 
",chekunkov,kmike
548,2014-06-08 09:49:16,"@kmike agree.

Also imho [`elements should carry attributes as a dict`](http://lxml.de/tutorial.html#elements-carry-attributes-as-a-dict) but that is a concern for another issue
",juanriaza,kmike
548,2016-09-14 11:15:30,"@kmike , I have the feeling that the discussion for the remaining work is in https://github.com/scrapy/scrapy/issues/1940
If so, can this one be closed?
",redapple,kmike
548,2016-09-14 12:23:11,"@redapple I think we should fix it from both sides: provide .links() method in parsel and create another API for scheduling requests in scrapy.
",kmike,redapple
548,2016-09-14 13:08:00,"ok @kmike , 

> create another API for scheduling requests in scrapy.

this is https://github.com/scrapy/scrapy/issues/1940, correct?

and

> provide .links() method in parsel

can be covered withing https://github.com/scrapy/parsel/issues/26
",redapple,kmike
548,2017-02-22 14:14:36,"Scrapy now has `response.follow`, which is very similar to `response.newrequest` proposed by @dangra in https://github.com/scrapy/scrapy/issues/548#issuecomment-33066664. I think it solves the original issue - it allows to handle relative URLs correctly with minimal effor from the user.",kmike,dangra
547,2015-03-23 21:13:52,"The current PR looks inadequate to me. Sorry.
As an intermediate step, it should just fix the current issue, which is: appending doesn't work. This would mean producing `filename.out.1 filename.out.2 filename.out.3` filenames, instead.
(This could be the easy task)

Changing the behaviour to append/overwrite, looks more daunting. Doing it right would mean modifying the Exporter's codebase to handle append or overwrite options like a File interface (IMHO); implementation depending on the actual Exporter backend (local disk, s3, datastore, ...).
E.g. the FTP exporter would need to say ""APPE"" instead of ""STOR"" or even ftp-server dependend alternatives.

Also @kmike's proposal

> `scrapy crawl myspider -o combined.jl -O last.jl` can also be useful, and users will try writing this.

would require more thought. It sounds great, but if `--append`/`--overwrite` is not just a switch to `-o`, but an argument taking a destination, Exporters would have to be able to handle that (opening multiple ""filepointers"" as given). That doesn't warrant an `easy` tag, I think.
",nyov,kmike
546,2014-07-23 22:50:47,"#659 and #760 already merged, can we close this? @dangra @kmike 
",nramirezuy,dangra
546,2014-07-23 22:50:47,"#659 and #760 already merged, can we close this? @dangra @kmike 
",nramirezuy,kmike
546,2014-07-23 22:52:03,"@nramirezuy yes, a good catch.
",kmike,nramirezuy
545,2014-01-20 12:33:36,"I made 0.22 the default branch 2 days ago, but `master` never showed up on available [versions](https://readthedocs.org/dashboard/scrapy/versions/).

@kmike: have you used this setup in another project? I added you as RTD maintainer for Scrapy in case you want to take a look.

![image](https://f.cloud.github.com/assets/37369/1954114/0f66ca66-81cf-11e3-860d-ef4c1497720d.png)
",dangra,kmike
544,2014-01-17 21:48:47,"@nramirezuy I tried adding `HTTPERROR_ALLOW_ALL = True` to settings.py but scrapy still keeps waiting forever.
I added also `handle_httpstatus_list = [100]` to my spider with the same result. Am I missing something?

I think that this can be related with the way that IIS 5.1 response to a POST request. From the [StackOverflow discussion](http://stackoverflow.com/questions/21177090/scrapy-gets-stuck-with-iis-5-1-page):

Interaction using scrapy with working webpage (IIS 6.0)



Interaction using scrapy with not working webpage (IIS 5.1)



Notice that the IIS 5.1 server sends two consecutive responses (HTTP 100 and HTTP 302) to the POST request.
",llekn,nramirezuy
542,2014-01-17 16:09:35,"@dangra crawl() fixed it for me. Still I'd expect any requests scheduled to have the same treatment, whatever their ""entry point"" in the pipeline is. Feel free to close if you disagree.
",nside,dangra
539,2014-01-16 21:36:06,"good idea @dangra 
",pablohoffman,dangra
539,2014-01-16 22:49:55,"@dangra, thanks for the tip. I'll send PR to master tomorrow.

@darkrho, i'm glad to be useful :)
",Scorpil,dangra
539,2014-01-17 02:20:23,"@kmike pull request for rtfd/readthedocs.org? :)
",pablohoffman,kmike
535,2014-01-15 18:47:10,"@nramirezuy , yes this works. The ""smart"" strings thing is for string results, such as attribute values and text nodes.
",redapple,nramirezuy
535,2014-01-16 12:07:16,"@dangra do you want this smart strings off in FormRequest, Sitemap and LxmlLinkExtractor in the same PR?
",redapple,dangra
535,2014-01-16 23:07:10,"@dangra , I may have missed something but I didn't see any lxml related `.xpath()` call for text nodes or attributes in FormRequest, LxmlParserLinkExtractor or Sitemap.
",redapple,dangra
535,2014-01-17 17:19:14,"@redapple: you right, I confused the scope of smart_strings.

It's not mergeable as-is, do you mind rebasing on top of master, no need to squash. thx
",dangra,redapple
533,2014-01-16 07:34:24,"@dangra What that testcase should test? How output of `request_seen` method changes while overriding `request_fingerprint` method - did I get it right?

And If I write one - how can I submit it? New PR?
",chekunkov,dangra
533,2014-01-16 11:26:49,"@chekunkov Yes, it should test that extending the class and overriding the method actually uses the method. The goal is to prevent that a later change (for simplification sake) doesn't remove this method without notice.
",dangra,chekunkov
532,2014-12-11 20:34:17,"@nramirezuy In your eyes, does it make sense to solve this issue within our method? If yes, then I'd do a PR. It's the first issue I found, where I have the feeling I can solve it with an accepted PR.

(in my eyes it does make sense to provide a serializer which does all one-level-fields the same as now, but also handles multi-level-structures like lists, dicts, tuples)
",aufziehvogel,nramirezuy
532,2014-12-12 13:54:05,"@aufziehvogel I think it is ok; not sure why nobody picked it up. I also found this [`PythonItemExporter`](https://github.com/scrapy/scrapy/blob/master/scrapy/contrib/exporter/__init__.py#L231), but idk why it is not begin used.
",nramirezuy,aufziehvogel
532,2016-09-15 15:36:55,"Closing as per @aufziehvogel 's comment above.
",redapple,aufziehvogel
528,2014-07-02 22:20:31,"I think @dangra's idea of CrawlSpider improvements deserves its own ticket, and this ticket should be closed.
",kmike,dangra
528,2014-07-02 23:08:41,"@kmike: I agree. new ticket is #781 
",dangra,kmike
519,2014-01-09 13:50:54,"> Spider or BaseSpider is instantiated several times no matter the command you use, if it uses SpiderManager the spiders are going to be instantiated.

@nramirezuy: SpiderManager loads all spider _classes_ but it doesn't instantiate them. It's very rare to have mroe than one spider instantiate.
",dangra,nramirezuy
519,2014-01-09 13:58:14,"@dangra I think we may add a default filter which would show only the first warning; users can insert their own filters to override the default.
",kmike,dangra
519,2014-01-09 14:04:29,"@kmike: I don't mind if you submit a PR to show how filterwarnings should work :)
",dangra,kmike
516,2014-01-07 01:26:19,"@hoprocker thanks. The XPathItemLoader offers add_xpath aswell. I might update when I have a nicely working Scraper.
",busla,hoprocker
516,2014-01-07 14:33:34,"@hoprocker: can you point to the 0.20 docs that mention `ItemLoader.add_xpath()`, somehow I can't find references to it. And I have been scrutinizing the changes backported to 0.20 and none mentions `ItemLoader` migration. 
",dangra,hoprocker
516,2014-01-07 14:44:42,"@dangra my issue was that I was reading the ""latest"" version (0.21.0) while using 0.20.0. So ItemLoader.add_xpath() is not documented as such in 0.20.0
",busla,dangra
516,2014-01-10 11:28:13,"Good thing i found this post thanks to @busla. I was having the same issue.

@dangra  http://doc.scrapy.org/en/0.20/topics/loaders.html
scrapy version -v
Scrapy  : 0.20.2
",saitx,dangra
511,2014-01-17 11:43:18,"Hey @dangra I'm having more problems. At

""The shell also pre-instantiate a selector for this response in variable sel, the selector automatically chooses the best parsing rules (XML vs HTML) based on response’s type.

So let’s try it: ""



I need to use the commands from v.0.18 instead:



However, I'm wondering if it's because I'm using the `python-scrapy` package provided by Ubuntu, rather than the [apt-gettable packages](http://doc.scrapy.org/en/latest/topics/ubuntu.html#topics-ubuntu). I'm on Ubuntu Trusty Tahr, so although I've added `deb http://archive.scrapy.org/ubuntu trusty main` to my `/etc/apt/sources.list` it seems that the distribution isn't supported?

Let me know if you have any advice. Cheers!
",HoldenCaulfieldRye,dangra
511,2014-01-18 17:33:26,"Thanks @dangra ! In that case you might want to update the Ubuntu installation guide page, because it says to apt-get scrapy-0.18
",HoldenCaulfieldRye,dangra
511,2014-07-07 18:05:01,"@stav thanks for the version check idea, I had installed scrapy 0.24 after installing an old one, but was running into problems . It seems the version that was actually running was the old one.  I had to do the 'pip install' instead of the 'apt-get install' also for some reason.
",jeremy-rutman,stav
510,2014-01-01 05:43:11,"Hey @dangra, nice changes!
",kmike,dangra
508,2014-03-15 17:38:51,"@shane42, @dangra: Or we can use a try except block as well, inside `request.__init__` which checks for "":"", unquotes the URL if required and raises the exception in case "":"" is not found. Please let me know what you think of both the approaches.
",vatsalj,dangra
508,2014-03-17 20:46:13,"@dangra: `urllib.unquote()` will not throw any kind of error in case when library doesn't quote form url. After doing this change, I ran the tests and they pass. Just need to create test cases for the patch and will send the pull request. Just one small question, the ""|"" character remains as ""%7C"" after unquote, will that cause a problem? We'll need to unquote the URL again in that case.
",vatsalj,dangra
508,2014-03-18 07:44:26,"@dangra: Sorry, you're right... my changes fail in that case.. I'd like to suggest another change that works: 

We can use `urllib.quote(url, safelist)` in `_factory` method under `scrapy/selectors/lxmldocument.py`. I'm suggesting this because we get the form url from `etree.fromstring`. If we pass in URL's(by quoting using workaround suggested by @shane42) which do not trigger this bug, then we'll not get the error in `scrapy/http/request/__init__.py`. This way we can altogether avoid this error. However, there will still be one issue, for url's like `https://google.com/?q=look+for%2Band%23query`, the ""%"" characters will be quoted again as it is not a part of the safelist. Maybe we can add it in safelist and do changes to `_factory` method?

If the above is not a good solution, another workaround that I can suggest is checking for "":"" character in the URL inside `_get_form_url` method itself. If it is not present, we can unquote the URL once (as libxml2 would've quoted it already), else we can return the URL as it is.

I'd love to hear your opinion about both the approaches. I apologize if any of the approaches has some flaw. Btw, thank you for referencing the pull request, will remember to do that from now on. :)
",vatsalj,dangra
506,2013-12-30 16:45:40,"This looks good. Thanks @dangra and @ahbeng!
",kmike,dangra
504,2013-12-30 02:20:37,"@dangra you're right, but your suggestion doesn't help with #368: partials are not usable with ItemLoader because get_func_args doesn't support them (it ends up in an infinite recursion).
",kmike,dangra
504,2013-12-30 11:30:34,"@kmike: I don't get why you said my suggestion doesn't help with #368, take a look to #506 that's my take on what was missing in this PR to be ready. 
",dangra,kmike
504,2013-12-30 16:37:24,"@dangra I misunderstood you, sorry. I thought you were suggesting to use p.args and p.keywords instead of get_func_args and keep it as-is.
",kmike,dangra
504,2013-12-30 16:43:00,"@kmike: no worries, I wasn't probably clear expressing myself.

does #506 looks good to you? please merge if so. 
",dangra,kmike
504,2013-12-30 17:10:00,"Thanks @dangra for the fixes - yes it wasn't as robust as it should have been, sorry about that.
",ahbeng,dangra
504,2013-12-30 17:15:35,"@ahbeng your submit triggered a final fix, that's a lot to say. thanks!
",dangra,ahbeng
501,2013-12-30 15:07:29,"@dangra, a very good catch. I've added some magic to fix that - please check it; waiting for Travis to finish.
",kmike,dangra
500,2014-01-17 02:17:12,"@kmike did you manage to reproduce this consistently?

I'm a bit against changing the default backend, we had some performance issues before with large crawls and many files, and single file is certainly easier to manage (deleting could be immediate vs many minutes, S3 dot persistance in SH won't work well with FS backedn, etc)
",pablohoffman,kmike
499,2014-01-09 16:05:10,"thanks @kmike for the explanation. I almost missed the DOM :)
",umrashrf,kmike
499,2014-01-09 16:11:21,"@pablohoffman will removing Splash reference from this question make this PR acceptable? or is it so obvious to not add it there?
",umrashrf,pablohoffman
499,2014-07-24 20:27:17,"@pablohoffman does this work https://gist.github.com/nramirezuy/2a50ebf03a88bbba9041 ? Or should add some explanation on why.
",nramirezuy,pablohoffman
499,2014-07-24 22:06:14,"Thats a task for `scrapylib` don't you agree @dangra?
",nramirezuy,dangra
494,2013-12-20 16:35:11,"@kmike: you're right, exposing ItemLoaders and processors under `scrapy.loaders` makes sense.

ItemLoaders were placed under contrib as a test bed just in case a better proposal arises, but so far there aren't. It's time to graduate it.  
",dangra,kmike
493,2014-07-24 20:36:52,"This look like solved, not on 0.20 but in latest.
Can we close this? @kmike 
",nramirezuy,kmike
492,2013-12-31 15:36:18,"should we close this ticket or wait until we migrate to the generic ubuntu repo @dangra ?
",pablohoffman,dangra
490,2013-12-18 16:05:25,"@max-arnold: you rock. thanks!

It looks great to me, let's merge after travis-ci reports the success build of last commit.
",dangra,max-arnold
488,2014-02-01 19:41:17,"@redapple I like your suggestions about adding more imports to basic template, and maybe some (commented) example code to start with. Can you create a separate PR for that?
",pablohoffman,redapple
488,2014-02-01 23:08:45,"@pablohoffman #494 is an another way to handle issue with imports. What do you think about it?
",kmike,pablohoffman
487,2013-12-16 12:09:07,"@arijitchakraborty : does #353 covers your SEP proposal?

@nramirezuy : do you see any downside on #487 implementation of fields inheritance?

I like the SEP of #487 (this PR) and the tests of #353, having an unified view would be awesome.
",dangra,arijitchakraborty
487,2013-12-16 12:09:07,"@arijitchakraborty : does #353 covers your SEP proposal?

@nramirezuy : do you see any downside on #487 implementation of fields inheritance?

I like the SEP of #487 (this PR) and the tests of #353, having an unified view would be awesome.
",dangra,nramirezuy
487,2013-12-16 12:24:49,"@dangra #353 covers my SEP proposal
",arijitchakraborty,dangra
483,2014-07-24 22:48:29,"@dangra, does @curita's changes fix this?
",nramirezuy,dangra
481,2014-01-27 16:41:52,"@dangra Do you have any good idea about how to leave a test? I was doing it with a spider
",nramirezuy,dangra
481,2014-02-03 19:03:04,"@dangra I added tests but I'm getting an error that doesn't happen on the spider tests I made. Can you give me a hand?


",nramirezuy,dangra
475,2013-12-16 21:27:04,"Well, @dangra knows where to throw in his magic ;-) I had no clue in #474 
",redapple,dangra
475,2013-12-19 13:47:14,"@kmike: you're right, I'll add a test for Referer header and move the unittest to `scrapy.tests.test_http_headers`. 
",dangra,kmike
475,2013-12-24 00:00:47,"@kmike : review latest changes to tests cases.
",dangra,kmike
474,2013-11-25 17:27:36,"The complicated fitler doesn't look good, but I see why it is needed to filter None values there:



Instead (and taking @kmike initial idea) I'd evaluate this solution: 


",dangra,kmike
472,2014-01-14 11:59:26,"@pablohoffman how does it look to you?
",redapple,pablohoffman
472,2014-01-16 12:47:11,"@redapple is it ready to merge?
",dangra,redapple
472,2014-01-16 13:11:11,"it looks good to me. /cc @pablohoffman are you ok to merge now?
",dangra,pablohoffman
472,2014-01-16 13:28:49,"@dangra, yes, that's all I have for now :)
",redapple,dangra
471,2014-02-04 08:22:31,"@kmike , you mean _instead of_ the proposed `XPath` & `CSS` classes, or _in addition to_ them
",redapple,kmike
471,2016-01-19 10:02:13,"@kmike , I think so, yes.
",redapple,kmike
465,2013-11-19 22:30:40,"Hi @bjlange,

Good catch. But it is still unclear if these numbers mean ""order"" (pipelines with large numbers are applied last) or ""priority"" (pipelines with large numbers are applied first)
",kmike,bjlange
462,2013-11-20 01:56:23,"@llonchj hopes above fix works for you. let me know if it is not enough. thanks
",dangra,llonchj
462,2013-11-20 02:09:27,"@dangra, it works. Thanks a lot!

2013/11/20 Daniel Graña notifications@github.com

> @llonchj https://github.com/llonchj hopes above fix works for you. let
> me know if it is not enough. thanks
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/scrapy/scrapy/pull/462#issuecomment-28857681
> .
",llonchj,llonchj
462,2013-11-20 02:09:27,"@dangra, it works. Thanks a lot!

2013/11/20 Daniel Graña notifications@github.com

> @llonchj https://github.com/llonchj hopes above fix works for you. let
> me know if it is not enough. thanks
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/scrapy/scrapy/pull/462#issuecomment-28857681
> .
",llonchj,dangra
461,2013-11-16 11:00:08,"Thanks @kmike , I was thinking the same actually.
What do others think?

@pablo, @dangra any thoughts?
",redapple,kmike
461,2013-11-20 15:30:59,"Sure. I also forgot to do as @kmike said though, raising exception only when `.add_css()` or `.add_xpath()` are called, not at instantiation (backward compatibility)
",redapple,kmike
461,2013-11-25 17:31:12,"Nice to see this one merged!

@redapple next time ping me as @pablohoffman ;)
",pablohoffman,redapple
457,2013-11-16 15:29:28,"@barraponto I don't believe _file://_ is ever of any use, except when testing local files and there's no http server running. I also believe that those use cases are so rare, and have easy natural alternatives, therefore having file:// enabled by default is a security risk.
",mvsantos,barraponto
457,2013-11-16 16:35:33,"That's what I think as well. But I'd like to hear what @pablohoffman or @dangra have to say about it.
",barraponto,pablohoffman
457,2013-11-19 17:50:18,"@dangra I understand and appreciate your point about Scrapy's approach to ease development.
Your suggestion of a ""must read"" page sounds good.

I understand that Scrapy's maintainers want to make it as easy as possible for newcomers. 
And, we (scrapy users) are expected to do our homework when it comes to our enviroment's security.
But features that pose possible security risks and are not required by the system's core, should be enabled if required, not by default. 
So, in my opinion leaving `file://` enabled by default, is far from a _derisory_ issue that you claim. Please note that I don't want to sound like a scaremonger, but right now Scrapy's is leaving inexperienced users at risk of possible privilege escalation‎ attacks.
",mvsantos,dangra
457,2013-11-19 21:32:56,"@dangra Do you mean they _shouldn't_ work from HTML pages because of `OffsiteMiddleware`? Or is that the current behavior?
",barraponto,dangra
457,2013-11-21 00:02:11,"@kmike Any service that scrapes content and outputs near-raw data could be targeted by an attacker. Examples, but not limited to:
- Services that generate image or pdf from a given URL, i.e. http://snapito.com/ ;
- Services that calculate a website SEO's score (not so common these day);
- Services that provide extensive search for third parties. SaaS that does website indexing and offers custom search competing with Google's Custom Search Engine);

Any of these services could be powered by Scrapy, and an attacker could exploit them by creating traps such as `<a href=""file:///etc/passwd"">foo</a>` so that once the service iterates a sitemap or downloads whatever page where that trap is deployed to, the target spits out the data scraped.

Just to clarify, the initial point I raised:
A. HTTP redirects and html meta refresh, should only work with http[s]. So far, looks like we all agree with this one.
B. Some kind of master switch could be introduced to the settings file, making it clear that allowing requests to `file://` may result in unwanted results if `OffsiteMiddleware` is not defined or disabled or open to any domains.
",mvsantos,kmike
457,2013-11-22 10:40:03,"@redapple IMO this problem asks for a setting of its own, since there're security implications and therefore beyond the `DEBUG` scope.
",mvsantos,redapple
457,2013-11-22 11:06:01,"@redapple I personally don't like the idea of having security settings open by default, like you suggest in the second topic. But Scrapy's approach is clearly making as easy as possible to get the first ""HelloWorld"" spider working, then your 3 topics should work fine.

Also, @dangra idea of _adding a ""Recommendation for running spiders in production"" page to docs_ is great for newcomers like myself, to make sure we deploy Scrapy in a sane and secure fashion.
",mvsantos,redapple
457,2013-11-22 11:06:01,"@redapple I personally don't like the idea of having security settings open by default, like you suggest in the second topic. But Scrapy's approach is clearly making as easy as possible to get the first ""HelloWorld"" spider working, then your 3 topics should work fine.

Also, @dangra idea of _adding a ""Recommendation for running spiders in production"" page to docs_ is great for newcomers like myself, to make sure we deploy Scrapy in a sane and secure fashion.
",mvsantos,dangra
457,2013-12-02 18:02:08,"@dangra you are not protected by Offsite on redirects. But as @kmike said you need to send the file back to the server to be an issue.
",nramirezuy,dangra
457,2013-12-02 18:02:08,"@dangra you are not protected by Offsite on redirects. But as @kmike said you need to send the file back to the server to be an issue.
",nramirezuy,kmike
457,2013-12-02 18:17:14,"@nramirezuy: OffsiteMW is not a great ""protection"", but to put my words to work, it won't let `file://` urls pass:



test output:


",dangra,nramirezuy
457,2013-12-02 18:19:57,"@nramirezuy : OffisteMW doesn't apply to redirects, right, but I was mentioning it in the context of links extracted from html pages. The idea with redirects is to disallow others than `https?://`.
",dangra,nramirezuy
457,2014-06-24 09:38:55,"I'm ok with @kmike's `allowed_protocols` proposal

`RedirectMiddleware` needs other changes too, for example removing `proxy` key from Request.meta when scheme changes, and also removing some HTTP headers (get rid of proxy-specific header for example) in redirected requests.
This is a good opportunity to clean/tighten it up.

What's the core maintainers' take on this? CC @dangra , @nramirezuy , @pablohoffman , @kmike 
",redapple,pablohoffman
457,2014-06-24 09:38:55,"I'm ok with @kmike's `allowed_protocols` proposal

`RedirectMiddleware` needs other changes too, for example removing `proxy` key from Request.meta when scheme changes, and also removing some HTTP headers (get rid of proxy-specific header for example) in redirected requests.
This is a good opportunity to clean/tighten it up.

What's the core maintainers' take on this? CC @dangra , @nramirezuy , @pablohoffman , @kmike 
",redapple,nramirezuy
457,2014-06-24 09:38:55,"I'm ok with @kmike's `allowed_protocols` proposal

`RedirectMiddleware` needs other changes too, for example removing `proxy` key from Request.meta when scheme changes, and also removing some HTTP headers (get rid of proxy-specific header for example) in redirected requests.
This is a good opportunity to clean/tighten it up.

What's the core maintainers' take on this? CC @dangra , @nramirezuy , @pablohoffman , @kmike 
",redapple,dangra
457,2014-06-24 09:38:55,"I'm ok with @kmike's `allowed_protocols` proposal

`RedirectMiddleware` needs other changes too, for example removing `proxy` key from Request.meta when scheme changes, and also removing some HTTP headers (get rid of proxy-specific header for example) in redirected requests.
This is a good opportunity to clean/tighten it up.

What's the core maintainers' take on this? CC @dangra , @nramirezuy , @pablohoffman , @kmike 
",redapple,kmike
457,2014-06-24 14:37:55,"@redapple I don't know if removing the proxy is a good idea, also the cookies may not change. Did you checked how the browser handle it?
",nramirezuy,redapple
457,2014-06-24 14:43:17,"@nramirezuy , if the redirected URL of a proxied request changes scheme, the proxy doesn't change (e.g. use an HTTPS proxy for an http:// URL), that feels weird and wrong, and python-requests doesn't do that.
My comment was to remove the ""proxy"" key, so it's added in the rest of the downloader chain later if necessary. That's not strictly part of this issue.
",redapple,nramirezuy
456,2016-09-16 14:35:22,"@dangra @kmike @eliasdorneles @lopuhin @pawelmhm , what do you think?
Should we plan on supporting this?
",redapple,dangra
456,2016-11-14 13:22:21,"I really like the idea of yielding a special value as @kmike suggested, that's what I'd expect as an user (a name suggestion: `WaitUntilQueueEmpty`, to avoid the had to read `UntilIdle`).

Would it be hard to implement? Would it break compatibility too much?
",eliasdorneles,kmike
456,2016-12-09 17:55:35,@eliasdorneles I like `WaitUntilQueueEmpty` name. It will be backwards incompatible indeed. We may introduce a new method with explicit behavior and deprecate start_requests; deprecating start_requests is be a bold move though :) Maybe we can also add support for `async def start_requests(self): ...` and fix it only for async def start_requests.,kmike,eliasdorneles
455,2014-01-17 15:52:56,"@dangra I did a rebase in an attempt to update the PR but lost the PR :(

On Fri, Jan 17, 2014 at 11:46 AM, Daniel Graña notifications@github.comwrote:

> @darkrho https://github.com/darkrho why did you close the PR?
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/scrapy/scrapy/pull/455#issuecomment-32616586
> .
",rolando,dangra
450,2013-11-05 15:02:16,"@dangra Thank you! :)
",demji,dangra
449,2013-11-19 16:34:21,"as @barraponto said, it is this way by design, the function name is missleading.  
",dangra,barraponto
448,2013-10-30 12:50:57,"My list of pros & cons:

Pros:
- will allow us to focus on writing code, not on legacy support. Easy as that might be, they add up because we're constantly dealing with them (it's not a one time thing)
- no one objected on to the email sent to scrapy-users (even @kmike email wasn't strictly an objection)
- I have been bitten myself before with Python 2.6 compatibility issues, including new methods or parameters added in 2.7 (unittests comes to mind, but there have been others)
- we have shifted the approach from supporting very old versions of everything (twisted 8 ffs!) to requiring very new stuff (six 1.4.1), and this goes inline with this new philosophy
- python 2.6 just came to end (no more security patches or bug fixes)

Cons:
- RHEL last stable is still on Python 2.6 (they should be ashamed)

Overall: +1
",pablohoffman,kmike
448,2013-11-18 14:22:27,"@barraponto yes
",pablohoffman,barraponto
446,2013-10-29 15:33:09,"@dangra please take the lead, I'm bit busy this days.
",rolando,dangra
445,2013-10-31 01:17:45,"@pablohoffman It failed due the removed file requirements-lucid.txt. I did merge latest changes from master and now everything is OK.
",rolando,pablohoffman
441,2013-11-19 17:35:32,"@Blender3D : are you going to send a PR wit this change including tests? 
",dangra,Blender3D
441,2013-11-19 19:15:30,"Well, I posted an issue without a pull request to get feedback on the
different alternatives, as they both work and are equally simple to
implement.

On Tue, Nov 19, 2013 at 12:36 PM, Daniel Graña notifications@github.comwrote:

> @Blender3D https://github.com/Blender3D : are you going to send a PR
> wit this change including tests?
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/scrapy/scrapy/issues/441#issuecomment-28813314
> .
",Blender3D,Blender3D
441,2013-11-19 20:27:19,"@Blender3D : I dont get what are the alternatives here (plural).

I only see a new parameter named `mime` to `MailSender.send()`, and it makes sense to me.

I infer your mention it's fragile because of `*mime.split('/')`, which should be fine if you include tests.
",dangra,Blender3D
434,2014-07-24 23:07:37,"Was there a reason for this to not get merged? @pablohoffman @dangra 
",nramirezuy,pablohoffman
434,2014-07-25 15:52:30,"@nramirezuy you can clone @pedrofaustino repo, create a local branch based on his branch, squash his commits and add yours on top. Then you push your branch to your Scrapy fork at github and submit  a new PR for review.
",dangra,nramirezuy
429,2013-10-18 12:38:49,"indeed, nice catch @alexanderlukanin13 !
",pablohoffman,alexanderlukanin13
429,2013-10-18 12:44:19,"Oh right, f was effectively always equal to filename, thanks for the clarification @kmike. My question about simplifying the setup.py still holds btw.
",pablohoffman,kmike
427,2014-01-21 14:38:43,"@darkrho : why this guy named **@ daniel**  earned @dangra's karma points? ;)

I'm sure **@ rolando** will be mad too if eventually get this notification.
",dangra,dangra
427,2014-01-21 14:45:32,"@dangra @darkrho that shows choosing a popular common name as github username is a bad idea :)
",pablohoffman,dangra
427,2014-01-21 18:15:56,"Replying by email is so confusing :(

On Tue, Jan 21, 2014 at 10:45 AM, Pablo Hoffman notifications@github.comwrote:

> @dangra https://github.com/dangra @darkrho https://github.com/darkrhothat shows choosing a popular common name as github username is a bad idea
> :)
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/scrapy/scrapy/issues/427#issuecomment-32890688
> .
",rolando,dangra
426,2013-10-15 12:45:42,"@kmike:  
1. `ss` stands for _Scrapy Selector_, but I must admit `sel` is a good option. let's vote.
2. I don't like `contenttype` for the same reason, I prefer `flavor`.
   the problem with `parser` is that it isn't just about selecting the parser, it also chooses the serialization and css translation methods.  
",dangra,kmike
426,2013-10-15 12:48:16,"1. I find `ss` a bit awkward, `sel` as @kmike suggest is fine by me, `sltr` is cryptic I know :)
2. I also think `contenttype` hints at a MIME type.
",redapple,kmike
426,2013-10-15 12:50:27,"1. I would just use `s` as an anonymous, short-lived variable name, not worrying about namespace clashes
2. I prefer `parser` to `contenttype`, and don't worry as much as @dangra does about conditioning other parts, unless those other parts would be plugabble too. Another option could be just `type`? 
",pablohoffman,dangra
426,2013-10-15 13:15:46,"@redapple: the argument name is going to be exposed as a public attribute too.

We foresee JSONPath support, although I was thinking on using `json` as value for that.

your proposal restore the sense on `contenttype`. If we stay on it, using `text/html`, `text/xml`,... fits better.
",dangra,redapple
426,2013-10-15 17:26:25,"I think that following MIME types is not necessary:

1) as @nramirezuy said, this is more letters to type;
2) are there reasons why we want to support anything besides xml and html in Selector API? If not, supporting MIME types as values could be misleading.

Two more API ideas:
- `Selector(text=""<tag>...</tag>"", parse_as='xml')`
- `Selector(text=""<tag>...</tag>"", method='xml')   # this follows lxml API`  
",kmike,nramirezuy
426,2013-10-15 18:30:52,"@barraponto: if you have a response object, you won't use `contenttype` at all, instead do `Selector(response)`
",dangra,barraponto
426,2013-10-15 18:39:50,"@kmike: `parse_as` is as bad as `flavor` or `type` in the sense that it is a new term. `method` is lxml lingua for serialization only, it doesnt choose the underlying parser.
",dangra,kmike
425,2013-10-10 20:21:29,"@audiodude unrelated question (see #340): did you enable Travis for your scrapy branch manually or it just worked? I saw your previous PR haven't triggered Travis build.
",kmike,audiodude
425,2013-10-10 20:34:46,"@kmike I did not do anything for Travis CI for my branch or my pull request, it seems to have just worked. Also very confusing because my given name is Travis....
",audiodude,kmike
425,2013-10-10 20:46:44,"@audiodude lol :) thanks! 
",kmike,audiodude
424,2013-10-15 14:22:03,"Probably ""Ready To Merge"", but we use [MRG] for that @nramirezuy 
",pablohoffman,nramirezuy
424,2014-07-25 21:03:23,"@kmike suggestion implemented and tests expanded.

Also I found that `settings['NOT_EXISTS'] is None` is a pretty weird behavior.
",nramirezuy,kmike
414,2013-10-14 21:48:52,"@kmike, I did not use virtualenv. If my memory is correct, I first just followed the directions on http://www.scrapy.org/download/ and did ""easy_install -U Scrapy"" (or the pip equivalent) and got pages and pages of errors. So I started figuring out what dependencies were failing to build/install and started using yum to install those (via sudo). After enough guessing and installs via yum, eventually ""easy_install"" (or pip) worked. My impression is that lxml was not on my Fedora machine in the first place. I work on very minimal Linux virtual servers (no GUI desktop etc...) so I wouldn't be surprised that lxml wasn't installed at all before doing pip or easy_install.
",castedo,kmike
412,2014-03-25 11:21:26,"@dangra Don't forget about js and the need of removing things from the HTML Form.
",nramirezuy,dangra
412,2014-03-25 13:43:18,"@nramirezuy we can consider that a new feature for another PR, but to keep the discussion going on, what approach do you think we can use to ""delete"" fields? 

I see two options:
1. Any field passed in formdata whose value is `None` is removed
2. Add an extra parameter in `.from_response()` that drop fields.
",dangra,nramirezuy
405,2013-10-02 20:54:37,"It is actually valid to return a request from `process_request` to re-schedule that request (ignoring the next middleware stages) even if no built-in middleware makes use of this functionality.

The engine can receive a request from the downloader middleware (redirect uses it for example, but on process_response) and it works the same.

If you look in the code of the downloader middleware and engine, there are places that explicitly check for a Request type.

This seems to be a case of under documentation, so I think we should extend (rather than trim) the documentation to include this case.

@audiodude did you find anything problematic with this behavior?. Would you expect a different behaviour? I know it's a bit of a corner case but if we're gonna make it official (ie. document it) now it's the time to air the concerns. Would it make more sense if it follows the downloader middleware chain? (that may be harder than it seems to implement since the downloader currently keeps a reference to the original request, but if we agree it makes more sense we should do it).

Thanks for the pull request nevertheless, it's good to bring this into discussion as we approach Scrapy 1.0.
",pablohoffman,audiodude
405,2013-10-10 17:51:27,"@audiodude You're right, and I think we should document that behavior on process_request as well.
",pablohoffman,audiodude
397,2013-10-14 17:42:52,"@duendex can you add the issue to the PR description? https://github.com/scrapy/scrapy/issues/392
",nramirezuy,duendex
397,2013-11-15 16:58:12,"Today I was testing and discussing this pr with @dangra in irc.
I was trying to access an https url through an http proxy.

This is what I get from the scrapy shell:



http urls work fine.

Any idea on how to solve this error?
",r-marques,dangra
397,2013-11-25 18:39:20,"Hi @duendex. I'm doing some scraping with a proxy (Privoxy->Tor) to a some sites that I need to login first (https).
If you like I can do some tests over them.
",hfoffani,duendex
397,2013-11-25 18:41:24,"Definitively yes. Thx

On Mon, Nov 25, 2013 at 4:39 PM, hfoffani notifications@github.com wrote:

> Hi @duendex https://github.com/duendex. I'm doing some scraping with a
> proxy (Privoxy->Tor) to a some sites that I need to login first (https).
> If you like I can do some tests over them.
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/scrapy/scrapy/pull/397#issuecomment-29228903
> .
",duendex,duendex
397,2013-12-02 12:09:05,"I'm lucky. Just right when I got around to actually need that feature. Any idea when this PR will be merged? @dangra ?
",amrali,dangra
397,2013-12-02 19:41:40,"@duendex : please, rebase the PR so travis-ci can run tests and let's get this merged :)
",dangra,duendex
397,2013-12-03 18:37:11,"LGTM. /cc @pablohoffman
",dangra,pablohoffman
396,2016-11-29 12:12:00,"@redapple as we can see, this only happens with ipython. I was debugging for a while and found this:

file: `scrapy/utils/console.py`


So the wrapper for ipython calls the instance method. If we check the instance method, the doc string says this:

file: `site-packages/traitlets/config/configurable.py`

It's returning a global instance, so the first inspect_response is creating this shell and the next ones are reusing the same shell with old variables. Shall this be correct?",Tarliton,redapple
396,2016-11-30 07:59:29,"@Tarliton @redapple I've fixed it in #2418, please, verify it with the fix.",ahlinc,redapple
396,2016-11-30 10:17:27,"Thanks @ahlinc , I'll check on my end.",redapple,ahlinc
396,2016-11-30 10:33:53,"@ahlinc , #2418 works for me, thanks!
And thanks @Tarliton for spotting this",redapple,ahlinc
395,2013-09-24 19:29:39,"@andrix I prefer to use the default parser (html) in that case and leave the user the chance to force ""xml"" by constructor argument.
",dangra,andrix
395,2013-09-24 19:36:04,"@dangra Yes, I agree as it may be complicated to make an inspect of the text that works. It's preferable your approach. So +1 to the new API! :)
",andrix,dangra
395,2013-09-24 20:41:53,"@redapple: I don't get what #386 is about, but lets keep that discussion there.
",dangra,redapple
395,2013-09-25 00:54:43,"@kmike: is your question more like why aren't Scrapy selectors a generic selection library like parts of Nogokiri or lxml are?

I think it is because `Response` type is a vital component of Scrapy, and by taking it into account we can avoid reparsing the same response multiples times, and always parse using the best encoding for the response, select the DOM parser that fits better with the response type (XML vs HTML).

It simplifies the selectors api too by removing the burden of passing the text+encoding+parser everytime, you can pass body and assume utf8 and html parser but it isn't always the best option. When you need that level of flexibility it is better to use `lxml` directly.
",dangra,kmike
392,2013-11-15 17:04:03,"@dangra : feedback on #397 
",r-marques,dangra
388,2017-01-05 16:05:16,"Right @Digenis , this is pretty old. And belongs to scrapyd-client indeed.",redapple,Digenis
386,2013-09-24 22:43:03,"Yeah, totally agree with @dangra. Joining is just one special case. There are several others. Use loaders :)

From my experience, `w3lib` can help with 90% of the special cases, trivial or not (besides some too trivial cases where you shouldn't even need to rely on a external lib). If you find another common special case, it's a good place to add them :)
",panaggio,dangra
376,2013-09-03 19:44:46,"thanks for reporting @nyov, I think this bug is fixed now in 0.18.2.


",dangra,nyov
376,2013-09-04 00:09:40,"Yes, looking great. Thanks @dangra, thanks @alexcepoi!
",nyov,dangra
372,2013-08-21 14:30:34,"Good catch @kmike, I spotted it before but hadn't had the time to fix it yet.
",pablohoffman,kmike
370,2013-12-17 12:14:20,"@max-arnold I had commented in this thread some time ago around those ideas
https://github.com/scrapy/scrapy/pull/370#discussion-diff-5922616

A few ideas I'd like to see were in https://github.com/scrapy/scrapy/pull/250
",redapple,max-arnold
370,2013-12-17 13:13:52,"@max-arnold : no, that isn't enough because subclasses defining its own file_key() expects an url and won't handle a Request.

we need a new method to wrap `file_key()` and others.
",dangra,max-arnold
370,2013-12-17 13:29:39,"@redapple: join the party! what do you think about the new methods and its signatures? are you OK with this approach too?
",dangra,redapple
366,2013-08-14 05:00:54,"Since it's not documented, can someone (@kalessin @dangra?)  explain what's the purpose of this new item exporter?. Thanks.
",pablohoffman,kalessin
358,2014-05-12 14:19:34,"@dangra pyquery shares most of the same syntax as CSS, but extending it in a poweful way. attributtes and content based selectors, scoped search, and much more. 

But the best part is that jquery selectors API is much more known than xpath or CSS: we have been using it for decades. 
",mgaitan,dangra
356,2013-10-16 17:41:50,"@pablohoffman :+1:  if we can keep the old syntax with default to string.
",nramirezuy,pablohoffman
356,2013-12-30 22:37:40,"@barraponto because the types of the arguments are not known until they're used in the spider. When command line arguments are parsed we still don't know what is the expected type for each argument.

Is anyone working on a pull request for this? I'm also fond of making spiders advertise the arguments they support and come up with a better API to get spider arguments, similar (in spirit but not implementation) to what was discussed in #303.
",pablohoffman,barraponto
356,2015-02-12 12:49:25,"I like `--set-json` more because of two reasons:
1. we won't be inventing our own grammar for option values; 
2. `-a` and `-s` won't become ambiguous - it is not clear how to pass the exact `text1,text2,text3` value in @wutali's proposal  (@pablohoffman's proposal doesn't have this issue though).
",kmike,pablohoffman
356,2015-02-13 15:21:13,"@nramirezuy Sounds ok to me. It would be useful for hard-users, it can print help/usage, etc, but anyway user probably wants to have standard argument-parsing utility instead of implementing it by himself/herself. Imagine that JSON-style args would be added to Dash, and immediately, without any extra code, user can pass complex arguments/settings to his/her spiders via UI.
Why can't we have both?
",shirk3y,nramirezuy
356,2015-03-17 21:23:15,"I discussed this with @kmike and we agree on moving forward with `--set-json` and `--arg-json`.

We may add some shortcuts in the future (`--arg-list`?), if we agree they're needed, but let's start with the basic `--arg-json` which provides the most flexibility, at the expense of some extra chars.

@shirk3y will you have time this week to update/rebase your pull request?
",pablohoffman,kmike
356,2015-03-23 01:36:34,"Yes, @pablohoffman I can work on and finalize it tommorow.
",shirk3y,pablohoffman
355,2016-03-29 16:23:07,"The code in `xlib/tx/_newclient.py` hasn't changed from what @stav wrote down. So there is no fix there. But if the issue persists with Twisted > 13, then it's (still) a bug in the twisted project, as the bundled tx code isn't used with newer Twisted versions.

If there has been a fix for this upstream, it may still be too much trouble to backport it to the old pre-13 xlib/tx code. So I would propose closing this (and reporting it to Twisted if the issue persists).
",nyov,stav
353,2015-04-20 14:57:15,"@chekunkov anything to comment on its implementation? 

From my past comments I still don't get a test case for defining and/or overriding fields of its parents.
",dangra,chekunkov
353,2015-04-21 17:45:19,"@dangra https://github.com/scrapy/scrapy/pull/353/files#diff-b610b6c1bd7114fdb73120bf028608caR169 ?
",nramirezuy,dangra
349,2013-07-19 15:08:10,"@dellis23 actually, it's github's fault. You can't add a PR to an existing issue :P
",barraponto,dellis23
346,2013-10-08 15:44:51,"@redapple  Do you have any proposal for the api pipeline?
",nramirezuy,redapple
346,2013-10-08 16:00:31,"@nramirezuy, I haven't got any definite idea of an API for pipelining filters

Another thing, I find myself needing to exclude specific links, e.g. to exclude the homepage of a website

This is what I tried (which works)



which is probably not the smartest way to do that.
Could an `Exclude`class be added?

Sure, `Disallow` could be used with specific regexes `'^<url>$'` but it feels complicated.
",redapple,nramirezuy
345,2013-09-04 22:45:40,"are we gonna fix this one @dangra ?
",pablohoffman,dangra
345,2013-09-06 02:11:09,"@Tony36051: your problem is different, it was fixed in Scrapy development branch and in Scrapy 0.18.2 stable release. Create a new issue with an url to easily reproduce it if not. thanks. 

@pablohoffman: yes, happens that an extended http parser can't be easily hooked into twisted HTTP11 client. Want to take a look and discuss better approach?

I think the longterm option is to report the bug upstream and propose two things: 
- a fix for this specific problem in current twisted `HTTPClientParser` class
- a way to easily extend the `HTTPClientParser` used by `HTTP11ClientProtocol`
  (currently even monkey patching is hard) 

to access the parser from scrapy download handler we should go trough:
- `HTTPConnectionPool` has a private attribute named `_factory` that sets `_HTTP11ClientFactory`
- `_HTTP11ClientFactory` has a simple method named `buildProtocol` that instanciates a `HTTP11ClientProtocol`
- `HTTP11ClientProtocol` instanciate an `HTTPClientParser` and multiples others things inside [`request()` method](https://github.com/twisted/twisted/blob/twisted-13.1.0/twisted/web/_newclient.py#L1398)

Everything is easy except telling `HTTP11ClientProtocol` to use a different `HTTPClientParser`

While writing about this I realized a non-monkeypatch solution, extending `HTTP11ClientProtocol` and use a property getter and setter for `HTTP11ClientProtocol._parser` attribute, the setter will convert the twisted `HTTPClientParser` instance into our extended version. It's not pretty but I can't see any better option. :) 
",dangra,pablohoffman
345,2014-04-24 17:44:49,"Is there a way to reproduce this? I've tried different twisted versions (13.2.0, 13.1.0, 10.2.0) and different scrapy versions (0.18.4, 0.22.2, scrapy master), and scrapy fetch works fine. Maybe the website changed. I'm not sure I've understood @dangra comment about reasonless status line. Here is the current curl output:


",kmike,dangra
345,2016-09-15 15:46:47,"@redapple it could also be a bad proxy, not a bad server
",kmike,redapple
343,2013-10-14 16:04:04,"@kmike  where is this `Scrapy handles 'ajax crawlable' pages with '#!' in URLs by default.`?
",nramirezuy,kmike
343,2013-10-14 16:10:12,"@nramirezuy  this is the default behavior of Request class. Here `#!` urls are expanded to use `_escaped_fragment_`: 
- https://github.com/scrapy/scrapy/blob/19ff9ac4f925790b58b35536dea433797863df23/scrapy/http/request/__init__.py#L52
- https://github.com/scrapy/scrapy/blob/3fe2a32683bce14e1fa1576cdac3a13b9f579b0a/scrapy/utils/url.py#L77
",kmike,nramirezuy
343,2013-10-14 16:54:55,"@kmike If 4k is insufficient, why are we still using it as a default? 
Don't you think an undocumented setting should fit? It is less overhead for someone that have to workaround it and may he have time to report it. Plus if he have the skills to discover the setting probably have the same skills to override the attribute value.
",nramirezuy,kmike
343,2013-10-14 18:11:50,"@kmike I prefer the setting.
I see this middleware useful for broad crawls only or very specific sites, but few where probably is wrong used the tag.
Because if you have to develop a spider for a specific site you will review the site and manually add the fragment.

In other hand it is possible some kind of cache? Something like if domains doesn't have ajax avoid doing the lookup.
",nramirezuy,kmike
342,2013-07-22 14:22:43,"@barraponto: why?
",dangra,barraponto
340,2013-10-11 13:01:12,"there was w3lib and scrapyd but now they are using my username/token.

we need to add travis.yml for slybot and scrapely, let me know if you add it and I'll configure the hook.

thanks @kmike for looking at this!
",dangra,kmike
339,2013-09-02 13:39:40,"hi @alexcepoi, `scrapy check` fails here using your branch for this PR:


",dangra,alexcepoi
339,2013-09-02 16:56:08,"@dangra auch.. that was a stupid one.
Can you try again? Do let me know if I should rebase this PR and if you find any other bugs (or cosmetic changes).
",alexcepoi,dangra
336,2013-10-14 16:08:14,"@tpeng Are you still working on this?
",nramirezuy,tpeng
331,2016-01-27 13:49:39,"@nramirezuy , @dangra , @kmike , @eliasdorneles ,
should this be moved to https://github.com/scrapy/parsel ?
",redapple,nramirezuy
331,2016-01-27 13:49:39,"@nramirezuy , @dangra , @kmike , @eliasdorneles ,
should this be moved to https://github.com/scrapy/parsel ?
",redapple,dangra
331,2016-01-27 13:49:39,"@nramirezuy , @dangra , @kmike , @eliasdorneles ,
should this be moved to https://github.com/scrapy/parsel ?
",redapple,kmike
331,2016-01-27 19:25:50,"@redapple @dangra @kmike @eliasdorneles 
https://github.com/scrapy/parsel/issues/26
",nramirezuy,redapple
331,2016-01-27 19:25:50,"@redapple @dangra @kmike @eliasdorneles 
https://github.com/scrapy/parsel/issues/26
",nramirezuy,dangra
331,2016-01-27 19:25:50,"@redapple @dangra @kmike @eliasdorneles 
https://github.com/scrapy/parsel/issues/26
",nramirezuy,eliasdorneles
331,2016-01-27 19:25:50,"@redapple @dangra @kmike @eliasdorneles 
https://github.com/scrapy/parsel/issues/26
",nramirezuy,kmike
323,2013-06-19 17:01:48,"@nramirezuy : Content-Length isn't set when Transfer-Encoding header is used, the common case is http://en.wikipedia.org/wiki/Chunked_transfer_encoding 
",dangra,nramirezuy
320,2013-06-09 17:11:30,"@pablohoffman I think that a note checking for compatiliby is nice. And I must say, I did forget that there were wiki pages in Scrapy. I think it is better suited for this.

@darkrho I assume you propose changing the link to project page to the Pypi page of the package, is it ok?
",amferraz,pablohoffman
320,2013-06-09 18:50:54,"@amferraz Sorry, I meant: in the section of ""Projects not maintained by Scrapy devs"" add a link to the pypi's search results for scrapy instead of listing the 3rd party projects. Although I think could useful to have a list of featured related projects.

My suggestion address the fact that maintaining an accurate and up to date list of related projects can be time-consuming.

A wiki page might work well, though.
",rolando,amferraz
320,2013-07-23 19:27:48,"IMHO Wiki is a perfect place for such lists, and @pablohoffman already added a section there: https://github.com/scrapy/scrapy/wiki#projects-tools-and-libraries-using-scrapy. Wiki is open for all Github users, so feel free to add more projects. I think we should close this ticket.
",kmike,pablohoffman
320,2013-10-14 16:16:32,"@amferraz can you add the projects to the wiki page pointed by @kmike  and close the ticket when its ready?
",nramirezuy,amferraz
320,2013-10-14 16:16:32,"@amferraz can you add the projects to the wiki page pointed by @kmike  and close the ticket when its ready?
",nramirezuy,kmike
320,2013-10-16 11:18:09,"Done, @nramirezuy
",amferraz,nramirezuy
320,2013-10-16 17:46:54,"@amferraz Thank you
",nramirezuy,amferraz
306,2013-05-22 20:05:29,"@pablohoffman Excellent, thanks, will revise on fork.
",DeaconDesperado,pablohoffman
303,2013-10-14 16:20:16,"@redapple Are you still working on this?
",nramirezuy,redapple
303,2013-10-14 16:43:27,"@nramirezuy I haven't worked on it lately but still would find use for it
",redapple,nramirezuy
303,2013-10-14 17:54:07,"@redapple Do you need some help or feedback?
",nramirezuy,redapple
303,2015-03-30 17:26:28,"@redapple does this works for you? https://github.com/scrapy/scrapy/issues/356#issuecomment-74084006
",nramirezuy,redapple
295,2013-04-25 15:51:03,"@nramirezuy I don't follow. what do you mean by ""large pull of requests"" ? 

so far this change is about collecting inactive downloader slots.
",dangra,nramirezuy
295,2013-04-25 17:15:29,"@dangra  I mean when you use the disk queue on the scheduler and you need depth level grater than 1. Default will have no problem because is LIFO queue, but FIFO queues will have problems.
",nramirezuy,dangra
285,2013-10-14 16:34:46,"sgmllib is deprecated since python 2.6: http://docs.python.org/2/library/sgmllib.html#module-sgmllib
So `SgmlLinkExtractor` should be deprecated as well, I feel it have no sense fix this.

There is when this 2 PRs come handy:
- https://github.com/scrapy/scrapy/pull/331
- https://github.com/scrapy/scrapy/pull/346

@stav Can we close this ticket?
",nramirezuy,stav
284,2013-04-09 13:26:19,"+1 to this too. I think we can keep `--pipelines`, since it's the most obvious choice and, as @nramirezuy said, you can use settings to decide which pipelines to enable.
",pablohoffman,nramirezuy
276,2013-10-02 21:31:18,"It sounds more appropriate to add this flag in the download handler (`scrapy/core/downloader/handler/http11.py`), don't you think @nramirezuy?
",pablohoffman,nramirezuy
276,2013-10-09 16:03:39,"@pablohoffman  In which method do you think it will fit?




",nramirezuy,pablohoffman
272,2013-10-10 19:49:53,"@coagulant Did you solve it? Can we close the ticket?
",nramirezuy,coagulant
272,2013-10-11 14:19:03,"@coagulant Thank you for the fast answer, I hope you around soon.
@alexcepoi Did you solve this on your PR?
",nramirezuy,alexcepoi
272,2013-10-11 14:19:03,"@coagulant Thank you for the fast answer, I hope you around soon.
@alexcepoi Did you solve this on your PR?
",nramirezuy,coagulant
272,2013-10-11 14:38:58,"Yes, I reproduced and fixed this error as part of the PR (in the commit referenced above).

Can be closed imo.

> On 11 okt. 2013, at 16:19, Nicolás Alejandro Ramírez Quiros notifications@github.com wrote:
> 
> @coagulant Thank you for the fast answer, I hope you around soon.
> @alexcepoi Did you solve this on your PR?
> 
> —
> Reply to this email directly or view it on GitHub.
",alexcepoi,alexcepoi
272,2013-10-11 14:38:58,"Yes, I reproduced and fixed this error as part of the PR (in the commit referenced above).

Can be closed imo.

> On 11 okt. 2013, at 16:19, Nicolás Alejandro Ramírez Quiros notifications@github.com wrote:
> 
> @coagulant Thank you for the fast answer, I hope you around soon.
> @alexcepoi Did you solve this on your PR?
> 
> —
> Reply to this email directly or view it on GitHub.
",alexcepoi,coagulant
272,2013-10-11 15:31:04,"Thank you @alexcepoi 
",nramirezuy,alexcepoi
270,2013-10-14 17:29:04,"@barraponto Can you fix this PR?
",nramirezuy,barraponto
270,2015-04-16 12:54:13,"@pablohoffman @dangra we have 2 ways in [here](https://github.com/scrapy/scrapy/pull/1100/files#diff-0849d24738e0aba02977941095e8f4d6R69); we could also support custom shells.
",nramirezuy,dangra
270,2015-04-16 12:54:13,"@pablohoffman @dangra we have 2 ways in [here](https://github.com/scrapy/scrapy/pull/1100/files#diff-0849d24738e0aba02977941095e8f4d6R69); we could also support custom shells.
",nramirezuy,pablohoffman
269,2013-03-19 18:09:08,"@kalessin ok with that
",pablohoffman,kalessin
263,2013-05-16 19:49:09,"@nramirezuy there's a reference implementation for pep 3156 here: https://code.google.com/p/tulip/
",ariddell,nramirezuy
263,2015-03-19 10:12:14,"@kmike Hey Mikhail! You are a man of many projects :-) Glad to hear it is being worked on, I didn't get that impression from the early parts of this thread and couldn't see any other porting docs. I quite agree that this project (just like Flask et al.) is going to be hard, dealing with the interface to the outside world is horrid. I certainly didn't know that URLs themselves could have mixed encodings :-(
Given the continual migration to Python 3 for personal projects (50/50 according to the survey I linked vs Python 2.7) and >40% for work, the need for scrapy's Py3 support is only going to get stronger. Bon chance!
",ianozsvald,kmike
263,2015-07-27 12:26:59,"Hi @curita, thanks for the note. For my data science audience I think scrapy is the only non-python-3.4 package that matters, everything else that they (and I) use is already running with Python 3.4. I wish you all luck in the conversion, knowing the data science stack is almost fully 3.4 compliant really helps when planning larger-scale projects.
",ianozsvald,curita
263,2015-07-29 15:08:52,"@kmike hey, that's lovely to hear (and Graham Markall [of Continuum] told me about the sprint), we'll certainly note this when we talk next week. Cheers!
",ianozsvald,kmike
263,2016-01-27 17:44:33,"thanks a lot for this information, @redapple !
",stonebig,redapple
263,2016-02-02 18:40:22,"It has gone and past six days, @redapple! 

:)
",KeremTubluk,redapple
263,2016-03-18 09:04:15,"thanks for informing, @kmike @d0ugal 
looking forward to the new stable version compatible with py3
",ABSmiLT,kmike
263,2016-05-11 19:59:25,"Are we there yet? are we there yet are wethereyetarewethere — :boom: ...eh, WHAT?
:sob: _finally_!

Congrats. Great job there keeping up the backports, @redapple. (Probably made a nice bang there, punching that ""close""- button just then, right?)
And Thanks, everyone.
",nyov,redapple
263,2016-05-16 08:49:22,"@redapple FINALLY!
thanks @ all 

btw, the ""PY3 95%"" label is now out of date?
![image](https://cloud.githubusercontent.com/assets/8147261/15284790/1f83da08-1b86-11e6-9c9e-e657838406e8.png)
",ABSmiLT,redapple
263,2016-05-20 08:52:03,"@kmike thanks a lot~ :100: 
",ABSmiLT,kmike
263,2016-05-28 03:38:38,"@dangra okay, thanks a lot~
",ABSmiLT,dangra
261,2013-03-10 23:02:08,"Thanks @stav, I added some comments.

As for the Best Practices, I think they should go into `scrapy/docs/contributing.rst`
",pablohoffman,stav
260,2013-03-05 18:43:43,"@pablohoffman, was replied on the last comment. I moved the ""mess"" of commits into this pull request.
",llonchj,pablohoffman
260,2013-03-08 13:49:47,"I've just merged it. Good contribution @llonchj, thanks for working on it!
",pablohoffman,llonchj
259,2013-03-05 10:57:26,"@stav if the method has a reason to expect lists and only lists, it should explicitly convert whatever it gets into a list, shouldn't it? But I'm with @midiotthimble here: why not take a set?
",barraponto,stav
259,2013-03-05 11:15:37,"@stav sorry for my english, I don't understand about why not take a set.
",nuklea,stav
259,2013-03-06 05:23:50,"@stav does not matter :)
",nuklea,stav
255,2013-10-14 17:32:48,"@llonchj are you still working on this? can we close the ticket?
",nramirezuy,llonchj
255,2013-10-14 18:01:24,"@nramirezuy, kindly check the PR. Thanks
",llonchj,nramirezuy
252,2013-03-04 15:56:23,"Thanks @llonchj, I like how this is coming along. I made a comment on the entry points group and names (I'm really not sure what's the best practice there).

I also have a couple more questions:
1. do we need to extend support with colons for multiple modules names in `COMMANDS_MODULE` if we implement the entry points solution?. Doesn't it already cover the same functionality?. Do you think it's worth having both? Do you see a place where `COMMANDS_MODULE` would work where entry points wouldn't?
2. could we squash the commits before merging?. To avoid polluting the main commit history.
",pablohoffman,llonchj
252,2013-03-05 18:24:50,"1. Ok, I will remove the colon functionality.
2. I will close this pull request, fix the commit in a new pull request. Continues on #260

Regarding `pkg_resources.iter_entry_points`, all documentation found uses something like `scrapy.commands` as group parameter. @pablohoffman proposal on `pkg_resources.iter_entry_points(""scrapy"", ""commands"")` will not allow multiple commands (Duplicate entry point error) per package. The suggestion is to proceed with ""scrapy.commands"" as best(or more common) practice. 
",llonchj,pablohoffman
251,2013-02-18 01:15:38,"@llonchj I've added tests and a tiny change to preserve same behavior as dict(item).

Could you pull my branch? https://github.com/darkrho/scrapy/tree/item_serialization

Apparently I can't make a pull request to your branch :/
",rolando,llonchj
251,2013-02-18 01:22:12,"I'll do it. Thanks Rolando.

2013/2/18 Rolando Espinoza La fuente notifications@github.com

> @llonchj https://github.com/llonchj I've added tests and a tiny change
> to preserve same behavior as dict(item).
> 
> Could you pull my branch?
> https://github.com/darkrho/scrapy/tree/item_serialization
> 
> Apparently I can't make a pull request to your branch :/
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/scrapy/scrapy/pull/251#issuecomment-13702753.
",llonchj,llonchj
250,2013-10-10 20:15:03,"@redapple : any value on this PR or can I close it?
",dangra,redapple
250,2013-10-11 15:45:35,"I think it is useful. Disabling image conversion is one of the most common requests for ImagesPipeline. thanks @redapple 
",dangra,redapple
249,2013-02-27 08:15:59,"looks like this comment is related to https://github.com/scrapy/scrapy/issues/8 :) thanks @pablohoffman I'll try this approach
",artemdevel,pablohoffman
247,2013-02-28 17:02:50,"@pablohoffman can that rationale be explained or linked in scrapy docs?
",barraponto,pablohoffman
247,2013-03-01 13:24:20,"@barraponto sounds good, where would you add it?
",pablohoffman,barraponto
245,2013-02-15 15:35:27,"@Mimino666 if you are planning to fork Scrapy I would suggest you send a list of your motivations and goals to `scrapy-developers` list. There's a high chance we can work together on the improvements, and that many of those have been tried at some point but didn't work out. Scrapy code has a long history and a lot of discussions behind it, many of which never ended up in code.
",pablohoffman,Mimino666
241,2013-02-11 13:48:43,"This change breaks tests, so I had to revert it...

Please run tests (with `bin/runtests.sh`) and make sure your change doesn't break any previous one without a good reason. If there is a good reason (obsolete text, for example), it might be fine to consider removing or updating the broken test.

btw @zuhao why did you change the original implementation of the change that used `[-1]`?
",pablohoffman,zuhao
237,2013-03-05 10:53:57,"@whodatninja if you're referencing Pull Requests in your commit message, you can just reference it by issue number #237 or ""full path"" scrapy/scrapy#237

btw, this should be covered by tests, else modifications to support a proxy provider risk breaking previously developed support for other providers... 
",barraponto,whodatninja
237,2013-03-07 18:43:26,"@barraponto thanks for the note. I guess I wanted to make sure there was interest from the maintainers before spending more time. I'm of course happy to write some tests for the new functionality. With respect to breaking previous support, in theory the existing tests would catch that. 
",whodatninja,barraponto
237,2013-03-19 22:30:48,"I did some rework: https://github.com/dangra/scrapy/commit/70cc2b09e8de7a12c650fc743add596e15477309

before merging we are going to remove log lines and probably default http_version to 1.1 for all requests

@whodatninja: do you know if Proxy-Connection is actually needed? according to wikipedia [Proxy-Connection](http://en.wikipedia.org/wiki/List_of_HTTP_header_fields#Common_non-standard_request_headers) header is a missunderstanding.
",dangra,whodatninja
237,2013-03-24 23:49:15,"@dangra as it turns out this is a somewhat complicated question. From my perspective, I think the right answer is whatever makes scrapy look most similar to a garden variety browser. Since [Chrome](https://code.google.com/p/chromium/codesearch#chromium/src/net/http/proxy_client_socket.cc&q=proxy-connection&sq=package:chromium&l=28) and IE apparently still emit this header (Firefox [removed it](https://developer.mozilla.org/en-US/docs/Site_Compatibility_for_Firefox_18) in v18) I think scrapy should continue to emit it as well but don't feel strongly about it.
",whodatninja,dangra
237,2013-10-14 17:40:11,"@dangra Can we close this ticket? on https://github.com/scrapy/scrapy/issues/392 you said this is deprecated.
",nramirezuy,dangra
237,2013-10-14 17:48:42,"@nramirezuy: so true.
",dangra,nramirezuy
233,2013-02-27 06:21:34,"@llonchj can you confirm we should close this PR and continue on the new scrapyd repo?. (@dangra is on holidays now). Thanks!
",pablohoffman,llonchj
233,2013-02-27 07:26:00,"Yes, that's fine.

2013/2/27 Pablo Hoffman notifications@github.com

> @llonchj https://github.com/llonchj can you confirm we should close
> this PR and continue on the new scrapyd repo?. (@dangrahttps://github.com/dangrais on holidays now). Thanks!
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/scrapy/scrapy/pull/233#issuecomment-14159063
> .
",llonchj,llonchj
226,2013-10-14 18:10:43,"@dangra by the way, why is callback recommended to be passed as keyword argument? I found callback as a second positional argument a nice shortcut - almost every request needs URL and a callback. Are there downsides?
",kmike,dangra
226,2013-10-15 10:48:12,"@nramirezuy - it's not particularly resolved; I totally respect @pablohoffman 's take on callback/errback being conceptually similar, and that they should ideally be together; but it does seem like the majority is with me that the docs should reflect the order in the code.

Perhaps we should reflect the relatedness of callback/errback better in the documentation. Working on a modification to the patch now, should be up within the hour.
",scraperdragon,nramirezuy
226,2013-10-15 10:48:12,"@nramirezuy - it's not particularly resolved; I totally respect @pablohoffman 's take on callback/errback being conceptually similar, and that they should ideally be together; but it does seem like the majority is with me that the docs should reflect the order in the code.

Perhaps we should reflect the relatedness of callback/errback better in the documentation. Working on a modification to the patch now, should be up within the hour.
",scraperdragon,pablohoffman
221,2013-01-04 22:18:27,"Thanks @emschorsch, contributions to the doc are always welcome!
",pablohoffman,emschorsch
220,2015-02-17 00:18:29,"Hi! I want to start contributing to scrapy fixing this issue. I want to fix the bug using the solution proposed by @dangra: call `process_spider_output` til the generator fails and then call spidermiddleware's `process_spider_exception`. 

I have been looking scrapy source code for a while now and I think the solution must be somewhere in here [1](https://github.com/scrapy/scrapy/blob/master/scrapy/core/spidermw.py#L69), [2](https://github.com/scrapy/scrapy/blob/master/scrapy/core/scraper.py#L127) but still can't figure out where should the fix go. I also already looked at twisted Deferreds documentation, so I know more or less how it works :)

From what I have understood, the spidermiddleware `process_spider_output` and `process_spider_exception` are called [here](https://github.com/scrapy/scrapy/blob/master/scrapy/core/spidermw.py#L69):



But the spider `parse()` returns a generator right? so `process_spider_output` is called because no exception has been raise yet. Because `process_spider_output` is called, the scraper thinks that everything is fine, so each spidermiddleware `process_spider_output` is called, where finally the exception is invoked. 

I think that I have a good understanding of what's happening below the hood, but I still think that I'm missing something, that's why I wanted to ask if someone can guide me a little here. 

Thanks!!
",lezorich,dangra
212,2012-12-26 20:55:56,"Thanks @dangra!

In order to detach caching policy from storage backend I reckon I should create a new middleware?

What about the following:
- Create HttpRealCacheMiddleware as scrapy.contrib.downloadermiddleware.httpcache.HttpRealCacheMiddleware, inheriting from HttpCacheMiddleware.
- Add it to the DOWNLOADER_MIDDLEWARES_BASE dict with value=900 (same value as the dummy cache).
- Add just one new setting, HTTPCACHE_REAL_ENABLED with default value set to False. I would need to make sure users don't enable both middlewares at the same time (any particular exception I should be using?).
- Unit tests would go to HttpRealCacheMiddlewareTest, inheriting from HttpCacheMiddlewareTest

Should I create a new pull request and close this one? Also, I see you've added this feature to the 0.18 release, I'll add this to the docs (as in ""New in version 0.18."").

Anything I might have missed?
",pedrofaustino,dangra
212,2012-12-27 23:42:09,"Nice pull request, and it's great to see such nice progress on it, good work!

My suggestions:
- keep the same middleware
- decouple the caching policy, as @dangra was suggesting
- support two policies by default (`HTTPCACHE_POLICY` setting):
  - `dummy` - old behaviour
  - `http` | `http11` | `rfc2616` - the new ""real"" http cache
- maybe `HTTPCACHE_POLICY` setting could also support class paths, if you want to pass your own class (and there is a well-defined API). Another approach would be to have a separate `HTTPCACHE_POLICIES` dict similar to `FEED_EXPORTERS` which maps policy names to classes.

Either way, the `HTTPCACHE_POLICY` setting would make the other ones (`HTTPCACHE_USE_DUMMY`, `HTTPCACHE_REAL_ENABLED`) unneeded.
",pablohoffman,dangra
212,2012-12-27 23:44:33,"@mhlakhani I'm not sure a separate policy for requests & responses is the best way to go, from the API point of view. A single ""policy"" that implements two methods looks more convenient.
",pablohoffman,mhlakhani
212,2012-12-28 04:45:03,"@mhlakhani yes, something like that. Maybe call them `should_cache_response(response)` and `should_cache_request(request)`, and those would replace the current `is_cacheable` and `is_cacheable_response`. This class would need to support a `from_settings` class constructor to receive configuration arguments (such as `HTTPCACHE_IGNORE_HTTP_CODES`, etc).
",pablohoffman,mhlakhani
212,2012-12-28 12:31:33,"I hope this goes in the right direction. In any case, I've added @mhlakhani to my repo as a collaborator, so please free to modify.
",pedrofaustino,mhlakhani
212,2012-12-28 15:21:45,"@dangra The email GitHub sent with your comment didn't have the last paragraph, but in the end I think this is what you're saying. Let me know.
",pedrofaustino,dangra
212,2013-01-04 18:58:10,"@dangra changes look good to me
",pablohoffman,dangra
212,2013-01-06 19:09:11,"@dangra of course I think it's ok ;) Unfortunately I can't test it atm because the project I was involved with has finished.

I've added @dangra commits to this pull request, and updated the documentation.

I've unintentionally added a commit to .gitignore, please disregard.

Thank you,
Pedro
",pedrofaustino,dangra
205,2012-12-14 18:29:45,"Thanks for the update @joehillen, I hereby pronounce you  the official Scrapy PyPy ambassador! :)
",pablohoffman,joehillen
201,2012-12-02 15:37:49,"I've confirmed that FifoDiskQueue tests pass on Linux and Windows, so I'm merging this patch.

Very nice patch btw, one of the best I've seen recently, thanks @alexcepoi !
",pablohoffman,alexcepoi
193,2013-01-29 17:05:28,"@pablohoffman, what do you think if we follow browsers behavior and do not attempt uncompressing in httpcompression middleware  if `application/gzip` (and similars) is set as `Content-Type` ?
",dangra,pablohoffman
193,2013-08-08 19:21:36,"@dangra +1
",pablohoffman,dangra
176,2012-12-18 04:04:05,"@pablohoffman thanks. This is my first contribution to a Python project, so I'm not sure my code is thoroughly pythonic, particularly 0da6be1 where I duplicate some methods for my CSSMixin. The method names mimic the original methods, although there might be a better approach.

As for the documentation, I'll get down to it tomorrow.
",barraponto,pablohoffman
176,2012-12-18 04:49:55,"Great @barraponto, I look forward to those changes to merge this PR.

The CSSMixin code looks OK to me.

One thing I'm not so sure is about using `extras_require` instead of (the more standard) `install_requires`. On one hand, it's more elegant/correct to do it that way, but on the other it makes installation slightly more complex with a somewhat obscure setuptools feature.

My reasons in favor of using `install_requires`:
- CSS selectors would be a new nice core feature (anything not in scrapy.contrib is)
- simplifies setup by giving you a fully functional (batteries included) installation of Scrapy, regardless of how you install it. As a policy, all stuff not in scrapy.contrib should run with the default dependencies.
- cssselect is pure python, so multi-platform support shouldn't be an issue (because pip/easy_install should pull all dependencies automatically). If it were a binary package, I would worry much more about adding it as a mandatory dependency because of cross-platform issues.
- we did the same with PyOpenSSL (in fact, we should probably remove the 'ssl' from optional features) and it's even a binary package.
- cssselect seems like a reasonably well maintained project and not likely to go away soon. But, even if it goes away, we could replace it by another (better maintained) library like we did with libxml2 -> lxml, since the scrapy selectors API won't expose any internal cssselect API.
",pablohoffman,barraponto
176,2012-12-28 17:20:21,"Hey @barraponto, I hope you have a chance to make those minor changes soon, so we can integrate this new functionality into master branch and have some time to test it before the 0.18 release. AFAIK, the only two changes blocking this merge are:
- move csselect dependency to install_requires
- add some basic test
",pablohoffman,barraponto
176,2012-12-29 21:59:05,"@pablohoffman I'm currently working on documentation (slowly, blame the holidays). I thought someone in #scrapy@freenode was already working on making cssselect a hard dependency. That's something I can do on my own, though. But I have no experience in writing tests for scrapy, I don't think I'll be able to deliver that any soon, so I'd be glad if someone from scrapinghub stepped up either to write it or mentor me.
",barraponto,pablohoffman
176,2012-12-31 13:17:35,"@barraponto let's merge the PR once you finish the documentation, and we can add the tests afterwards (very simple, if you know how). The change to hard dependency is also a very simple, so I don't mind doing it once the PR is merged.
",pablohoffman,barraponto
176,2013-01-30 21:03:47,"XPath is great. By now I'm writing a lot of XPath, no issues so far. It is able to select stuff CSS can't. However, CSS selectors are _far more popular_, particularly with web developers. I thought this could be a great help for novice scrapers.

@dangra I agree with all of the proposed CSS pseudo-elements. Keep in mind that while pseudo-classes select elements, pseudo-elements don't. They select stuff that is ""imagined"" by the environment running the CSS selector (such as `::before` pseudo-elements). It fits our use case precisely. jQuery use of `:text` pseudo-class makes a lot of sense: it selects input elements by its type (a pseudo class). Our use of `::text` also makes sense: we're selecting the text content of the element (a pseudo element). Is it clear?

Now comes the issue of implementing it. `cssselect` already parses pseudo-classes, it just throws it away. There's room to implement our pseudo-elements, right?
",barraponto,dangra
176,2013-01-30 22:45:20,"On Wed, Jan 30, 2013 at 7:04 PM, Capi Etheriel notifications@github.comwrote:

> XPath is great. By now I'm writing a lot of XPath, no issues so far. It is
> able to select stuff CSS can't. However, CSS selectors are _far more
> popular_, particularly with web developers. I thought this could be a
> great help for novice scrapers.

I agree, XPath is more expressive but CSS is more friendly for newcomers.
Worth saying xpath is not going anywhere, it will be supported by Scrapy
selectors forever :)

> @dangra https://github.com/dangra I agree with all of the proposed CSS
> pseudo-elements.

Ok.

> Keep in mind that while pseudo-classes select elements, pseudo-elements
> don't. They select stuff that is ""imagined"" by the environment running the
> CSS selector (such as ::before pseudo-elements). It fits our use case
> precisely.

It does except pseudo-elements doesn't allow functions like the ones we
need to select attributes with `::attr(name)`

I can be missing something from the spec, but `cssselect` doesn't support
them and spec doesn't have an example like this.
We will need to extend `cssselect` parser (not just translator) to support
function pseudo elements.

> jQuery use of :text pseudo-class makes a lot of sense: it selects input
> elements by its type (a pseudo class). Our use of ::text also makes
> sense: we're selecting the text content of the element (a pseudo element).
> Is it clear?

I understand `:text` and `::text` are different things, but they
are undoubtedly similar.
Anyway, I have no problem with `::text` If we can keep everything as
pseudo-elements and that includes `::attr()`

> Now comes the issue of implementing it. cssselect already parses
> pseudo-classes, it just throws it away. There's room to implement our
> pseudo-elements, right?

I think you mean it already parse and throw _pseudo-elements_.

It does, but as said above, it can't parse function-like pseudo elements
and adding support will require `cssselect` upstream approval, or
maintaining a fork which doesn't worth it IMHO.

I am OK to continue with pseudo-elements only If we can get upstream
support to parse functions even if they are throw away like it happens now.

@barraponto: can you say if CSS selectors spec allow function-like psuedo
elements?
",dangra,dangra
176,2013-01-30 22:45:20,"On Wed, Jan 30, 2013 at 7:04 PM, Capi Etheriel notifications@github.comwrote:

> XPath is great. By now I'm writing a lot of XPath, no issues so far. It is
> able to select stuff CSS can't. However, CSS selectors are _far more
> popular_, particularly with web developers. I thought this could be a
> great help for novice scrapers.

I agree, XPath is more expressive but CSS is more friendly for newcomers.
Worth saying xpath is not going anywhere, it will be supported by Scrapy
selectors forever :)

> @dangra https://github.com/dangra I agree with all of the proposed CSS
> pseudo-elements.

Ok.

> Keep in mind that while pseudo-classes select elements, pseudo-elements
> don't. They select stuff that is ""imagined"" by the environment running the
> CSS selector (such as ::before pseudo-elements). It fits our use case
> precisely.

It does except pseudo-elements doesn't allow functions like the ones we
need to select attributes with `::attr(name)`

I can be missing something from the spec, but `cssselect` doesn't support
them and spec doesn't have an example like this.
We will need to extend `cssselect` parser (not just translator) to support
function pseudo elements.

> jQuery use of :text pseudo-class makes a lot of sense: it selects input
> elements by its type (a pseudo class). Our use of ::text also makes
> sense: we're selecting the text content of the element (a pseudo element).
> Is it clear?

I understand `:text` and `::text` are different things, but they
are undoubtedly similar.
Anyway, I have no problem with `::text` If we can keep everything as
pseudo-elements and that includes `::attr()`

> Now comes the issue of implementing it. cssselect already parses
> pseudo-classes, it just throws it away. There's room to implement our
> pseudo-elements, right?

I think you mean it already parse and throw _pseudo-elements_.

It does, but as said above, it can't parse function-like pseudo elements
and adding support will require `cssselect` upstream approval, or
maintaining a fork which doesn't worth it IMHO.

I am OK to continue with pseudo-elements only If we can get upstream
support to parse functions even if they are throw away like it happens now.

@barraponto: can you say if CSS selectors spec allow function-like psuedo
elements?
",dangra,barraponto
176,2013-01-30 23:36:50,"I can atleast attest to the CSS selectors working alright for me using @barraponto new additional the code base. It worked well for my use case of checking css code coverage on a site. 

Does it perhaps make sense to get started with just this implementation for now to get some basic CSS selector support in scrapy, and than revisit things like pseudo selectors later?
",brianDoherty,barraponto
176,2013-01-31 08:35:50,"@barraponto how would cssselect treat pseudo-elements in that new API?
",SimonSapin,barraponto
176,2013-01-31 19:26:23,"On Wed, Jan 30, 2013 at 9:37 PM, brianDoherty notifications@github.comwrote:

> I can atleast attest to the CSS selectors working alright for me using
> @barraponto https://github.com/barraponto new additional the code base.
> It worked well for my use case of checking css code coverage on a site.
> 
> Does it perhaps make sense to get started with just this implementation
> for now to get some basic CSS selector support in scrapy, and than revisit
> things like pseudo selectors later?
> 
> The initial implementation doesn't follow the Selectors API which basically
> demands for two methods: `.select()` and `.extract()`, this is why we
> looked at extending CSS selectors to address text nodes and attributes
> using pseudos.

Latest changes merged into this pull request already use pseudo-classes and
drops the selector's methods added by @barraponto.

The idea of using pseudo-classes hurt some people feelings because they are
not supposed to be used for this, but in the other side breaking scrapy
`Selector`s API breaks others.

_pseudo-elements_ are the best for both worlds, it's just more work.

As I see it, we have three options:
1. Add `.text()` and `.attr()` methods to Scrapy selectors
- pro: no need to extend `cssselect`
- cons: breaks Scrapy Selectors API
  1. Use psuedo-classes like `:text-content` and `:attr(name)`
- pro: It doesn't break Scrapy selectors API
- pro: we have a working Implementation that uses `cssselect` hooks to
  translate the new pseudo-classes to xpath.
- cons: pseudo-classes are more for filtering than to select things that
  are not real elements (like attributes and text)
- cons: `:text` doesn't make sense in this context and jquery already
  defined it to match `<input>`s of type text.
  1. Use pseudo-elements like `::text` and `::attr(name)`
- pro: psuedo-elements were desgined to select non-real elements (like
  attributes and text nodes).
- pro: It doesn't break Scrapy selectors API
- pro: functional pseudo-elements are under consideration (hooray!),
  makes sense to support them in `cssselect`.
- cons: `cssselect` parser can't parse functional pseudo-elements (but
  based on @SimonSapin comments he is fine to accept a patch)
",dangra,barraponto
176,2013-02-01 12:01:51,"@barraponto: I consider your fourth option a variant of option 1, in fact, if we go for option 1 adding `.xpath()` method is OK but removing `.text()` is not, because it is different to `.xpath('/text()')`

yeah, let's look for option 3.
",dangra,barraponto
176,2013-06-17 16:21:45,"@barraponto agreed to wrap this up a couple of weeks ago. If he's not able to, maybe someone else can pick it up. @barraponto ?
",pablohoffman,barraponto
176,2013-07-10 19:22:54,"@barraponto What part are you stuck on? I looked at cssselect's source and it doesn't seem make any assumptions about the names of the pseudo-elements.
",Blender3D,barraponto
176,2013-07-12 16:18:06,"@barraponto could you post your latest code so that we can help?
(you could create a new branch on your fork, distinct of this pull request if you want to show us your current status and dont want to mess with this pull request)
",redapple,barraponto
176,2013-09-08 22:32:23,"Looks like @barraponto went cold, can someone else take over this work?. I'd love to see CSS selectors merged soon.
",pablohoffman,barraponto
176,2013-09-14 22:20:20,"@redapple: this PR stalled because it needed someone to reimplement using pseudo-elements, you work for SimonSapin/cssselect#29 is very valuable! 
",dangra,redapple
176,2013-09-15 22:07:28,"Thanks to @redapple ’s work (though not exactly as in the PR), cssselect not has parser-only support for functional pseudo-elements. See [the tests](https://github.com/SimonSapin/cssselect/blob/a4b12ae07c1d7ef71b7aae15034b259d209d7960/cssselect/tests.py#L445) for an usage example.

Play around with it, feel free to send new issues or PRs to cssselect, and let me know when you think this is stable and would like it in a PyPI release.

Happy hacking!
",SimonSapin,redapple
176,2013-09-16 00:00:27,"Ok, I'm back from Neverland :)

I'm really really happy that @redapple's work motivated @SimonSapin to provide us the API needed for pseudo-elements (including functional pseudo-elements!). I don't think I can push this to the end right now, but I can commit to writing the documentation for CSS selectors once it's fixed (or even sooner).
",barraponto,redapple
176,2013-09-16 12:44:41,"thanks @redapple and @SimonSapin! 

@redapple: adapt PR to option 3 of this comment https://github.com/scrapy/scrapy/pull/176#issuecomment-12967716
- Use pseudo-element `::text` to extract text nodes similar to JQuery [`.text()`](http://api.jquery.com/text/)
- Use pseudo-element `::attr(name)` to extract attribute values simlar to [`.attr()`](http://api.jquery.com/attr/)
",dangra,redapple
176,2013-09-16 12:50:58,"@barraponto welcome back, as this PR started in your branch, can you pull @redapple changes once it is ready and add a short documentation on how to use cssselectors.

keep in mind that this PR must not change tutorial/docs examples to CSS selectors, and doesn't integrate with Loaders yet.
",dangra,redapple
176,2013-09-16 12:50:58,"@barraponto welcome back, as this PR started in your branch, can you pull @redapple changes once it is ready and add a short documentation on how to use cssselectors.

keep in mind that this PR must not change tutorial/docs examples to CSS selectors, and doesn't integrate with Loaders yet.
",dangra,barraponto
176,2013-09-16 12:57:59,"so @dangra , remove `:text`  and `:attribute()` pseudo-classes (and tests) and adapt to process `::attr(name)` instead of `::attribute(name)`?
",redapple,dangra
176,2013-09-16 12:58:43,"@redapple, yes.
",dangra,redapple
176,2013-09-20 22:03:07,"We should add `cssselect` to `requirements.txt` and `.travis/requirements-*.txt`

Other than that, great work and +1 to merge. @dangra will you do the honors? (looks like it'll need a rebase though)
",pablohoffman,dangra
169,2015-05-08 12:36:21,"@osya, This works fine for me in the latest version

@pablohoffman, Is it possible to make something on this as a part of Scrapy?

The use case is like this, we POST/Search through a form. That form generates a link. But it actually redirects you to that link after 15 seconds, when the results are generated. So the next url can be know but needs to be called only after 15 sec. Unfortunately I was using a sleep for this, which made scraper go to around 4 hours and now with the deferred approach it is just taking 22 mins. Would be nice to have some special meta or DelayedRequest class for this
",tarunlalwani,pablohoffman
167,2012-08-29 18:02:37,"@alexcepoi no, I think that's it.
",pablohoffman,alexcepoi
149,2012-06-23 00:51:27," @alexcepoi the new ""debugging spiders"" section looks very useful - thanks for writing it!
",pablohoffman,alexcepoi
135,2012-06-07 15:47:59,"@alexcepoi could you also improve documentation of the parse command?

I think parse is a very useful command (even more with this addition) and one of the reasons why many people don't use it is because it's not as extensively documented as it should be.

Perhaps we could add a new section in the documentation: ""Debugging spiders"", or something like that?
",pablohoffman,alexcepoi
132,2012-05-11 22:11:33,"Thanks for the pull request @dfdeshom.

Unfortunately, I don't think we can apply it as it is because it is not backwards compatible, meaning that for example all currently [enabled extensions(https://github.com/scrapy/scrapy/blob/master/scrapy/settings/default_settings.py#L102) will become disabled. This patch could update those, of course, but I don't think it'll be enough because users could be relying on this functionality in their projects. We should instead do it in two steps: 1. _deprecate_ using zero as value for next release, and 2. remove in the next+1 release. Or maybe just forbid (raise error) using 0 and force to use None or a number?. That would avoid the ambiguous case of zero, which is the main problematic issue right now.

Thoughts?

PS. The patch should add a unit test too.
",pablohoffman,dfdeshom
132,2012-06-04 21:19:54,"@dangra unfortunately, I don't see a documented way to disable an extension and I consider myself pretty familiar with scrapy and its docs. Maybe it's already there in the docs, but simply needs to be more prominent.

For me there is a larger issue: some extensions have settings associated with them that make it confusing to know exactly how/where to disable them. For example, the cookies extension seems to have 2 ways to disable it:
-  set `COOKIES_ENABLED` to `False` (https://github.com/scrapy/scrapy/blob/master/scrapy/contrib/downloadermiddleware/cookies.py)
- set the priority to `None`, ie set `scrapy.contrib.downloadermiddleware.CookiesMiddleware` to `None`

It would be nice to have just one way to disable extensions.
",dfdeshom,dangra
125,2016-09-21 11:49:36,"@eLRuLL I have the same requirement, be able to set settings on delay/concurrent request enforced per cookiejar.
",jmaynier,eLRuLL
125,2016-09-22 19:28:40,"@eLRuLL thanks for the detailed explanation !
",jmaynier,eLRuLL
109,2012-05-16 12:27:43,"hi @redapple, I have been working on your patch, see https://github.com/dangra/scrapy/tree/http11

as you are the original developer for this feature, can I ask you for a quick review?

It requires twisted 12.1 because it is using HTTPConnectionPool for persistent connections

compare link https://github.com/dangra/scrapy/compare/master...http11
",dangra,redapple
109,2012-05-16 12:50:40,"hi @dangra,
Sure, I'll review it.
I personally had problems with persistent connections and Twisted (outside of Scrapy), but I'll try with 12.1 and your code.
",redapple,dangra
109,2013-06-04 14:21:29,"Awesome @dangra. You rock!
",redapple,dangra
109,2013-06-04 14:22:52,"@redapple: you did the initial work, you are the rock star here :)
",dangra,redapple
86,2012-04-29 08:51:51,"@pablohoffman Hello, yes i was running it from Scrapyd. It was looking like this extension was enabled even if i don't have any FEED_URI setting configured. I went around this problem by changing the default scrapy settings (removed this exporter from list of enabled ones)
",piteer1,pablohoffman
86,2012-05-08 20:43:13,"@dfdeshom I agree that 0 should also disable middlewares, and I'm happy to merge a pull request for this change
@piteer1 Yes, Scrapyd since 0.15 stores items by default using the JSON serializer. We made this change because we thought it's more useful to have the data stored by default in the most common case, so you have a complete working environment when running spiders on scrapyd. Otherwise, by default, items are just scraped and not stored anywhere. If you want to disable this set the `items_dir` scrapyd setting to none as the documentation says: http://doc.scrapy.org/en/latest/topics/scrapyd.html#items-dir (but check out the latest code because I just submitted a fix).
",pablohoffman,dfdeshom
86,2013-01-02 14:30:08,"Hi @pablohoffman , sorry to disturb you on a closed feed, but can we totally disable the scrapy.contrib.feedexport.FeedExporter with scrapyd ? I use DEBUG log but I don't want to dump the serialized items in my logs and I store them in a mongo database with a pipeline.
Thank you !
",nmalzieu,pablohoffman
86,2013-01-02 15:27:42,"@pablohoffman  OK thanks
",nmalzieu,pablohoffman
50,2016-12-07 09:11:39,"@immerrr , do you have a reproducible example?",redapple,immerrr
50,2017-03-25 09:38:41,"@redapple @immerrr  I made the following change, and then it could work:

",imp0wd3r,immerrr
50,2017-03-25 09:38:41,"@redapple @immerrr  I made the following change, and then it could work:

",imp0wd3r,redapple
45,2013-10-14 17:46:02,"@nramirezuy: so true.
",dangra,nramirezuy
36,2013-11-04 15:44:25,"Any news on this? @dangra ?
",pedrofaustino,dangra
36,2013-11-04 15:48:07,"@pedrofaustino: news from who? I stated on https://github.com/scrapy/scrapy/issues/36#issuecomment-26352771 that no known developer is working on it.

from comments on other issues I suppose you (@pedrofaustino) and @srmaximiano are working together, so if you guys aren't implementing this feature, nobody is.
",dangra,srmaximiano
36,2013-11-14 18:22:12,"@dangra We've updated the code based on your recommendations. Also, we've introduced curl-like command line flags, as well as one to set the content-type. Thanks for reviewing.
",pedrofaustino,dangra
36,2014-07-24 22:55:52,"@pablohoffman #447
",nramirezuy,pablohoffman
33,2013-09-29 22:14:45,"Hi @pablohoffman, has anyone started this? We're happy to write it but need to know if we're duplicating efforts.
",pedrofaustino,pablohoffman
33,2013-10-01 16:23:44,"Hi @pablohoffman I'm working with @pedrofaustino. 
I have a question:
This command will be similar to shell command,
Where i should create files, in contrib folder or like shell command files.
",srmaximiano,pablohoffman
33,2013-10-14 17:15:12,"I like the approach of using a successful request (@csalazar).

What do you think about it @pablohoffman ?
",nramirezuy,csalazar
33,2013-10-14 17:15:12,"I like the approach of using a successful request (@csalazar).

What do you think about it @pablohoffman ?
",nramirezuy,pablohoffman
33,2013-11-04 15:43:53,"Any news on this? @nramirezuy and @pablohoffman ?
",pedrofaustino,nramirezuy
33,2013-11-04 15:43:53,"Any news on this? @nramirezuy and @pablohoffman ?
",pedrofaustino,pablohoffman
33,2013-11-17 13:31:55,"I like @csalazar's approach, it's basically what I did for manual probing. I'd like it a lot if it probed url query parameters as well. 
",barraponto,csalazar
33,2015-04-17 22:49:13,"any interest in resurrecting this one for Scrapy 1.0 @nramirezuy @kmike @dangra @Curita @redapple ?
",pablohoffman,nramirezuy
33,2015-04-17 23:00:02,"No interest from me :) `scrapy probe` looks a bit too specific; I'm not sure there is a single best solution, and it is a feature which can live fine outside Scrapy.

I like @csalazar's approach; it looks a bit similar to what parts of https://github.com/DRMacIver/hypothesis library are doing - given an example, simplify it until it fails.
",kmike,csalazar
33,2015-04-21 17:33:52,"I'm agree with @kmike 

---> Close ? \/
",nramirezuy,kmike
33,2015-05-01 04:11:29,"I quite like the `minreq` idea, I think it shares the same goal of the original `scrapy probe` idea.

What do you think about adding `minreq` as a scrapy command?. I think it's a useful thing to have in the ""scraping toolkit"" and thus makes a good fit for a scrapy command (in addition to getting more exposure, usage and contributions).

/cc @csalazar
",pablohoffman,csalazar
33,2015-05-06 07:43:17,"@pablohoffman sure, I'll add _POST_ support and tests. The command should be called `probe` or `minreq`?
",csalazar,pablohoffman
33,2015-05-06 13:48:58,"@csalazar I think `minreq` is fine, since we are just making your app more visible.
",nramirezuy,csalazar
33,2015-05-06 20:47:45,"`minreq` sounds good to me, thanks @csalazar!
",pablohoffman,csalazar
28,2014-03-08 03:26:46,"This ticket should be moved to scrapyd project, could you do that @aspidites ?
",pablohoffman,aspidites
15,2014-12-18 13:48:42,"@alexey-senexx There is no fix for this. But you can make your own by moving offsite request filtering logic to a `DownloaderMiddleware`.
",nramirezuy,alexey-senexx
12,2013-01-29 14:04:59,"@pablohoffman take a look to above fix
",dangra,pablohoffman
10,2012-11-12 13:48:25,"@kalessin. Thanks! You might want to check the contents of the `__main__` you've implemented.
",philjackson,kalessin
10,2013-01-29 12:44:41,"@kalessin: any news on adding tests so we can merge this?
",dangra,kalessin
10,2013-05-22 16:12:05,"@kalessin Is this currently integrated in scrapy?
",employ,kalessin
10,2013-05-22 16:34:51,"not yet, it still needs the appropiate tests for that. I was planning to do
that soon.

On 22 May 2013 14:12, James notifications@github.com wrote:

> @kalessin https://github.com/kalessin Is this currently integrated in
> scrapy?
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/scrapy/scrapy/issues/10#issuecomment-18289401
> .

## 

http://uruguayeconomico.blogspot.com/
",kalessin,kalessin
8,2013-02-27 08:42:23,"hi @pablohoffman this code https://gist.github.com/artem-dev/5046355 is working, is it a correct way to use the new SignalManager?
",artemdevel,pablohoffman
8,2015-05-05 17:37:58,"If anybody is currently working on this issue let us know, otherwise @eLRuLL: all yours!
",curita,eLRuLL
2,2011-06-12 02:22:06,"@pablohoffman Completely slipped my mind, have submitted a ticket with the patch attached: http://dev.scrapy.org/ticket/326. Hope thats good, Google Groups interface confuses the hell out of me..
",djm,pablohoffman
2661,2017-03-24 11:15:41,"hi @harshasrinivas , I have no way to test this. I'm on Xubuntu, and `make htmlview` works for me.
maybe @kmike, @lopuhin or @stummjr can check (I think they have Macs)",redapple,stummjr
2661,2017-03-24 11:15:41,"hi @harshasrinivas , I have no way to test this. I'm on Xubuntu, and `make htmlview` works for me.
maybe @kmike, @lopuhin or @stummjr can check (I think they have Macs)",redapple,lopuhin
2661,2017-03-24 11:15:41,"hi @harshasrinivas , I have no way to test this. I'm on Xubuntu, and `make htmlview` works for me.
maybe @kmike, @lopuhin or @stummjr can check (I think they have Macs)",redapple,kmike
2643,2017-03-12 20:43:32,"@redapple  Let me know if this sounds good, I'll start working on the unit test and docs too.",harshasrinivas,redapple
2633,2017-03-17 14:09:21,"Like @eLRuLL mentioned in #2657, it would be a nice feature to have the base classes of spider/downloader middleware and extensions self register to the settings with a priority defined in the subclass. 
I figured there are two ways of implementing:

1. Create a method in the base classes that registers itself to the default settings (e.g. `def register(self, priority)`. This method should in turn be called from the subclasses.
2. Metaclass hacking; register to the default settings once the base classes get subclassed with priority as an attribute of the subclasses.

What would be the best solution in your opion? @eLRuLL @kmike ",jorenham,eLRuLL
2632,2017-03-09 15:01:32,"@redapple The changes looks good to me, thanks Paul! cc @chekunkov",vshlapakov,chekunkov
2629,2017-03-06 18:50:53,"The current benchmarking is done on a very simple page, where the bot just follows all the links. This does not actually give a correct scraping speed for a standard web page. 

It could contain several images as well, along with some restrictions so that the benchmarking suite could give a realistic approximation of the scraping speed.

Any other ideas?
ping @dangra ",Parth-Vader,dangra
2626,2017-03-06 14:47:14,"Check `tox -e py36 -- tests/test_crawl.py -s` output. @redapple is it a testing issue, or a problem with the warning? I've first noticed it in https://travis-ci.org/scrapy/scrapy/jobs/208185821 output.",kmike,redapple
2622,2017-03-17 08:18:31,"@redapple @kmike This should be ready for review.

When the fix is not applied, the test fails as follows:


",rolando,redapple
2622,2017-03-17 08:18:31,"@redapple @kmike This should be ready for review.

When the fix is not applied, the test fails as follows:


",rolando,kmike
2612,2017-03-02 11:36:20,"Fixes #2181 
Continuation of @MrMenezes ' #2353",redapple,MrMenezes
2611,2017-03-03 11:32:50,"@kmike I applied your suggestions. But did not manage to also log the temp file FilesystemCacheStorage. This is because `_get_request_path` takes `spider` and `request` as arguments, which aren't accessible in `__init__`.",jorenham,kmike
2593,2017-03-02 14:39:45,ping @redapple @kmike ,rolando,redapple
2593,2017-03-02 14:39:45,ping @redapple @kmike ,rolando,kmike
2592,2017-02-23 20:49:23,"This raises from https://github.com/scrapy/scrapy/issues/2589 where a server returns a non-UTF-8 header value. According to [this RFC](https://tools.ietf.org/html/rfc7230#section-3.2.4):

>    Historically, HTTP has allowed field content with text in the
   ISO-8859-1 charset [ISO-8859-1], supporting other charsets only
   through use of [RFC2047] encoding.  In practice, most HTTP header
   field values use only a subset of the US-ASCII charset [USASCII].
   Newly defined header fields SHOULD limit their field values to
   US-ASCII octets.  A recipient SHOULD treat other octets in field
   content (obs-text) as opaque data.

So the `latin1` encoding may be preferred over `utf-8` which is the Scrapy's default.  At the other hand, `requests` in some places uses `utf-8` with a fallback to `latin1`: https://github.com/kennethreitz/requests/blob/d6f4818c0b40bc6c00433c013b7daaea83b2cd51/requests/models.py#L908

Following @kmike's [suggestion](https://github.com/scrapy/scrapy/pull/2588#issuecomment-282112022), it seems that decoding headers with `utf-8` having `iso-8859-1` as fallback is the best option.",rolando,kmike
2581,2017-03-02 11:53:07,"The tests are green finally, yay! @kmike @djunzu @redapple do you mind having another look at this? I updated the PR description with summary of changes and also checked that ``scrapy crawl`` and ``scrapy runspider`` commands both respect custom log settings.",lopuhin,redapple
2564,2017-02-14 15:22:22,"Closes #2553

~I left `XmlItemExporter` out of the modifications, it's the only class which does not seem to have problems writing to either `bytes` or `str` streams (`io.BytesIO` or `io.StringIO`, for instance).~


outputs:

/cc @kmike ",elacuesta,kmike
2555,2017-02-14 23:05:56,"@glyph thanks for all the help you're providing for debugging Twisted+Scrapy issues, it is really helpful! 

By default Scrapy warns in case of HTTPS failures, not raises an error; this required some copy-pasting which involved `_` method in Twisted < 17. There is a lot of half-broken https websites which users still want to scrape. I'm not sure what the problem is with `https://renegadeline.com` website though: it is not passing https checks when downloading with scrapy/twisted, but it works fine in a browser (firefox or chrome, OS X).

 @redapple must know more about all this stuff :) Relevant code is at https://github.com/scrapy/scrapy/blob/master/scrapy/core/downloader/contextfactory.py and https://github.com/scrapy/scrapy/blob/master/scrapy/core/downloader/tls.py.",kmike,redapple
2555,2017-02-15 00:14:17,"@kmike Glad to help!

@redapple - Looking forward to hearing more about the use-case :).",glyph,redapple
2543,2017-02-08 08:36:42,"@redapple I tried running scrapy with this patch on a crawl, and having per-error stats look useful indeed.",kmike,redapple
2530,2017-02-03 13:40:44,"Fixes #2526

At first I implemented this using [`w3lib.http.basic_auth_header`](http://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.http.basic_auth_header) but then I realized that would ignore the [`HTTPPROXY_AUTH_ENCODING`](https://scrapy.readthedocs.io/en/latest/topics/downloader-middleware.html#httpproxy-auth-encoding) setting. Would it make sense to add an `encoding` parameter to `w3lib.http.basic_auth_header`?

Also, I'm not sure how to add a test for this, I think I would need to publish credentials, but it works with something like the following (from the scrapy shell):



/cc: @vezunch1k, @redapple",elacuesta,redapple
2530,2017-02-03 16:03:00,"I fixed something that was making the build fail, please check it again @redapple ",elacuesta,redapple
2528,2017-02-02 21:43:39,"Thanks @redapple, @dangra and @glyph!",kmike,dangra
2510,2017-02-03 11:58:58,"Hello there @kmike, looking forward to reading your comments after the latest changes.",elacuesta,kmike
2505,2017-01-20 16:07:15,"I'm trying to revive #941.
I made the changes easier for review by rebasing it on master (dropped the merges),
moving some hunks from the last commit to its parent to make the diffs cleaner
and creating a separate commit from the hunk that adds the doc. 


Cc: @bernardotorres (the author) and @kmike (the reviewer)
Fixes both #1478 and #941",Digenis,kmike
2496,2017-01-13 15:28:59,"Fixes #2461 

As [@rolando noticed](https://github.com/scrapy/scrapy/issues/2461#issue-197219248), `DNS_TIMEOUT` setting was never actually used by Scrapy with 14<=Twisted<=16.6, because a `(1, 3, 11, 45)` tuple was always passed to the DNS resolver by Twisted internally:



And Twisted 16.7 actually stops passing this default value, but a tuple (or list) of ints is expected.",redapple,rolando
2494,2017-01-17 14:31:30,@redapple any idea here? would that be ok to create a `LOGHANDLER_CLASS` configurable settings variable?,eLRuLL,redapple
2475,2017-01-03 13:03:36,"@eliasdorneles , I believe you were working on this recently.",redapple,eliasdorneles
2464,2016-12-23 16:18:41,"Fixes #2420

~Just as a side note, I am intentionally leaving out floats because different values would
be converted to the same `int` (for instance: `int(1.1) == int(1.2) == 1` is `True`)~

/cc @redapple, @vshlapakov",elacuesta,redapple
2464,2017-01-09 12:39:22,"@redapple, @kmike The branch is still named `component_order_integer`, would you guys like me to close this PR and open a new one with a more suitable branch name?",elacuesta,redapple
2464,2017-01-31 15:48:01,"Still looks good to me.
@dangra , @kmike , @vshlapakov, what do you think?",redapple,dangra
2456,2017-01-06 18:34:05,"Hey @redapple, thanks for your comments. I added a few tests with distinct width values.
Regarding the default, I set it to `4` following @kmike's [comment](https://github.com/scrapy/scrapy/issues/1327#issuecomment-118660643) and my own desire, but I think it would be just fine to leave it optional.",elacuesta,kmike
2456,2017-02-08 17:03:30,"Alright, I set `FEED_EXPORT_INDENT_WIDTH=None` to maintain backwards compatibility.

Ping @redapple, @kmike: I'm sorry to bother you guys, but do you have any further comments on this? Thanks!",elacuesta,kmike
2444,2016-12-12 10:24:58,"@eliasdorneles proposed it here: https://github.com/scrapy/scrapy/issues/1729#issuecomment-175195461. I think that's fine to add it. Not sure if we need JsonResponse - maybe it'd be nice to have, to be able to check if json is available. But maybe it doesn't worths it, I'm not sure.",kmike,eliasdorneles
2435,2016-12-08 16:30:01,@kmike and @dangra pls have a look.,sibiryakov,dangra
2435,2016-12-08 16:30:01,@kmike and @dangra pls have a look.,sibiryakov,kmike
2434,2016-12-08 14:01:51,"+1 to adding colored logs.
There was a internal discussion with @kmike and @eliasdorneles where this commit was referenced, where qutebrowser reverted the use of `colorlog`
https://github.com/The-Compiler/qutebrowser/commit/c64e5c9bd595ef310825d07bf29e1d3eb7674714",redapple,eliasdorneles
2434,2016-12-08 14:01:51,"+1 to adding colored logs.
There was a internal discussion with @kmike and @eliasdorneles where this commit was referenced, where qutebrowser reverted the use of `colorlog`
https://github.com/The-Compiler/qutebrowser/commit/c64e5c9bd595ef310825d07bf29e1d3eb7674714",redapple,kmike
2433,2017-02-13 16:55:28,"@redapple @kmike I recently found this PR and realised that now Scrapy swallows import errors not only for `scrapy version` but for other commands like `scrapy list`. The problem is that we rely on `scrapy list` results as a part of deploy process: earlier if someone forgot to add some necessary dependency - the deploy was failed at once. Now the exception is swallowed and `scrapy list` returns 0 code with some stderr, as a result platform thinks that everything is fine and there're just 0 spiders.

Of course it's possible to catch `RuntimeWarning` when running `scrapy-list`, but it's more about guessing as client code can also raise same exceptions and parsing stderr is not a reliable way. For now we started to always pass stderr logs if any, but I wonder if there's an option to add some switch between exception/warning behaviours (via Scrapy setting for example). 

Please share your thoughts, I can start a separate issue to track it.",vshlapakov,kmike
2429,2016-12-07 20:41:21,"this is an effort to clean up (make code comply to pep8) scrapy codebase.
the line length and some other trivial rules have been relaxed.
only the tests are worked on currently, I can do the actual code in the next pr.

this is a better solution than just using isort (#2419)
closes #2144, #2207 

as @dangra said, it's better for linters like flake8 to be incorporated in travis. 
ping @redapple, @kmike ",darshanime,redapple
2429,2016-12-07 20:41:21,"this is an effort to clean up (make code comply to pep8) scrapy codebase.
the line length and some other trivial rules have been relaxed.
only the tests are worked on currently, I can do the actual code in the next pr.

this is a better solution than just using isort (#2419)
closes #2144, #2207 

as @dangra said, it's better for linters like flake8 to be incorporated in travis. 
ping @redapple, @kmike ",darshanime,dangra
2429,2016-12-07 20:41:21,"this is an effort to clean up (make code comply to pep8) scrapy codebase.
the line length and some other trivial rules have been relaxed.
only the tests are worked on currently, I can do the actual code in the next pr.

this is a better solution than just using isort (#2419)
closes #2144, #2207 

as @dangra said, it's better for linters like flake8 to be incorporated in travis. 
ping @redapple, @kmike ",darshanime,kmike
2419,2016-12-01 12:27:37,"Hey @redapple, Please have a look at this PR. `Django` uses isort. I thought `Scrapy` might find it useful too.",voith,redapple
2419,2016-12-01 17:11:42,"Thanks @redapple for your feedback. I have allowed tox env ""isort"" to fail on travis.
I would like to get the opinion of other maintainers if they'd think this PR would be useful for scrapy.
ping @kmike @eliasdorneles @dangra ",voith,eliasdorneles
2419,2016-12-01 17:11:42,"Thanks @redapple for your feedback. I have allowed tox env ""isort"" to fail on travis.
I would like to get the opinion of other maintainers if they'd think this PR would be useful for scrapy.
ping @kmike @eliasdorneles @dangra ",voith,dangra
2419,2016-12-01 17:11:42,"Thanks @redapple for your feedback. I have allowed tox env ""isort"" to fail on travis.
I would like to get the opinion of other maintainers if they'd think this PR would be useful for scrapy.
ping @kmike @eliasdorneles @dangra ",voith,kmike
2410,2016-12-21 11:35:38,"@eliasdorneles , @elacuesta , what do you think of this patch?",redapple,elacuesta
2410,2016-12-21 11:35:38,"@eliasdorneles , @elacuesta , what do you think of this patch?",redapple,eliasdorneles
2409,2016-11-23 17:43:19,@redapple What do you think about this? Do you think a link in the spider middleware section would be good too?,elacuesta,redapple
2403,2016-11-22 12:44:20,@nyov discovered in https://github.com/scrapy/scrapy/pull/1887#issuecomment-260047541 that Scrapy is using patched Twisted libs shipped under `scrapy.xlib.tx` always. I introduced this bug in https://github.com/scrapy/scrapy/commit/bb806fa0f8f77cba59506153e33cab96f91008ab#diff-e487b2562862226386f811e0659c70c0R6 so this PR is an attempt to address the issue.,dangra,nyov
2400,2017-02-16 15:33:38,"@kmike, about
> cookies should be UTF-8 in most cases (but other encodings are also possible, so the code must be robust)

I believe the current `CookieJar` implementation (`scrapy.http.cookies.CookieJar`, based on `http.cookiejar`) does not handle encodings other than UTF8 properly. A different implementation could solve that, but IMHO that would fall out of the scope of this PR. 
Check the following snippet:


and the output:
",elacuesta,kmike
2398,2017-03-07 11:04:03,"Chatting offline with @kmike , we decided to move this out of 1.4 milestone.
This needs more discussions as one could choose a few different strategies for un-decompressible HTTP response bodies: for redirects it does not matter much if Location header is fine, but even for 200s, one could accept it if you're only interested in headers for example, or on the contrary, one would not want to receive those raw bodies.",redapple,kmike
2359,2016-10-25 17:44:12,"@kmike , were you thinking of something like this?
(Name of header `'X-Scrapy-Cache-Timestamp'` is subject to discussion of course)
",redapple,kmike
2355,2016-10-22 13:47:56,"When you use 3rd party libraries inside crawler code, which log all sorts of information at various logging levels, it can become quite noisy. This code change helps out by moving the LOGGING dictionary with the list of `loggers` which we can override. Example:



Would silence boto while allowing you to keep your `LOG_LEVEL` at `DEBUG`.

This ties into issue #2231 as it would enable us to modify loggers. It does not touch handlers, but it does give you the ability to set them up however you want and assign loggers to specific handlers (if you know how to configure logging properly).

cc @redapple
",iserko,redapple
2355,2016-10-24 15:06:28,"My concern was about people using `scrapy.utils.log.DEFAULT_LOGGING` and expecting it to still be defined. Not sure it's that big of a concern.

In `default_settings.py` I prefer `LOGGING` over `DEFAULT_LOGGING`, but it also could be `LOG_CONFIG` or `LOGGING_CONFIG`.

what do @dangra , @kmike , @eliasdorneles think?
",redapple,dangra
2355,2016-10-24 15:06:28,"My concern was about people using `scrapy.utils.log.DEFAULT_LOGGING` and expecting it to still be defined. Not sure it's that big of a concern.

In `default_settings.py` I prefer `LOGGING` over `DEFAULT_LOGGING`, but it also could be `LOG_CONFIG` or `LOGGING_CONFIG`.

what do @dangra , @kmike , @eliasdorneles think?
",redapple,eliasdorneles
2355,2016-10-24 15:06:28,"My concern was about people using `scrapy.utils.log.DEFAULT_LOGGING` and expecting it to still be defined. Not sure it's that big of a concern.

In `default_settings.py` I prefer `LOGGING` over `DEFAULT_LOGGING`, but it also could be `LOG_CONFIG` or `LOGGING_CONFIG`.

what do @dangra , @kmike , @eliasdorneles think?
",redapple,kmike
2355,2016-11-15 09:20:29,"hi @iserko , looks good to me.
I'd like feedback from @dangra , @kmike and @eliasdorneles .
",redapple,dangra
2355,2016-11-15 09:20:29,"hi @iserko , looks good to me.
I'd like feedback from @dangra , @kmike and @eliasdorneles .
",redapple,eliasdorneles
2355,2016-11-15 09:20:29,"hi @iserko , looks good to me.
I'd like feedback from @dangra , @kmike and @eliasdorneles .
",redapple,kmike
2351,2016-12-19 16:33:10,"@dangra , @pablohoffman , I think you discussed amendments to this proposal about commercial uses of the artwork.
Could you add your comments in this ticket? Thanks.",redapple,dangra
2351,2016-12-19 16:33:10,"@dangra , @pablohoffman , I think you discussed amendments to this proposal about commercial uses of the artwork.
Could you add your comments in this ticket? Thanks.",redapple,pablohoffman
2343,2017-02-20 13:52:25,Thanks @kmike. I updated the PR with your suggestions.,redapple,kmike
2333,2016-12-07 15:01:59,"It looks like supporting DNS-server-advertized TTLs would be quite some work as the current implementation uses `twisted.internet.base.ThreadedResolver` which will only get us one IP address, without any TTL ([`socket.gethostbyname()`](https://docs.python.org/3/library/socket.html#socket.gethostbyname) is used under the hood)

What about merging this (with a lower default expiration timeout like @dangra would prefer)
and work on a newer implementation (IPv6 compatible, TTL-respecting) as another PR?

Note that there is also `twisted.internet.endpoints.HostnameEndpoint` (since Twisted 13.2) which uses `socket.getaddrinfo`. If someone wants to have a look at that.

>  * Twisted [13.2.0] now includes a HostnameEndpoint implementation which uses
IPv4 and IPv6 in parallel, speeding up the connection by using
whichever connects first (the 'Happy Eyeballs'/RFC 6555 algorithm).
(#4859)",redapple,dangra
2331,2016-11-30 10:56:11,"@moisesguimaraes , can you update the docs as @kmike commented?
Other than that, looks good. Thanks.",redapple,kmike
2331,2016-12-04 14:58:41,"@kmike , I was going to change from (list) to (str or list), but since any iterable works, I changed it to (str or iterable).

If you want me to change to (str or list), please let me know.",moisesguimaraes,kmike
2329,2016-10-18 13:58:15,"It's not exactly what @redapple suggested on #2202. What do you think of this alternative? 
",josericardo,redapple
2329,2016-10-18 14:31:58,"@redapple Sure: e12e364 :)
",josericardo,redapple
2328,2016-11-14 14:48:25,"@redapple @kmike I updated the description according to @redapple's suggestion. Does this look good or should I mention that this is not the only meta key that's not used to control scrapy behavior?
",stummjr,redapple
2328,2016-11-14 14:48:25,"@redapple @kmike I updated the description according to @redapple's suggestion. Does this look good or should I mention that this is not the only meta key that's not used to control scrapy behavior?
",stummjr,kmike
2321,2016-10-12 10:48:27,"Web servers [should use encoded URLs in their ""Location"" headers](http://stackoverflow.com/questions/7654207/what-charset-should-be-used-for-a-location-header-in-a-301-response), but [they don't always do](http://stackoverflow.com/questions/4400678/what-character-encoding-should-i-use-for-a-http-header/4410331#4410331).

This website for example, for this URL http://www.yjc.ir/fa/news/1815565/
redirects to www.yjc.ir/fa/news/1815565/اعزام-كوهنوردان-ايراني-به-كيليمانجارو

but the bytes received are UTF-8 encoded, and not percent-escaped:



`RedirectMiddleware` decodes the header as ""latin1"" ([this is new in Scrapy 1.1](https://github.com/scrapy/scrapy/pull/1488)) and issues a request to http://www.yjc.ir/fa/news/1815565/%C3%98%C2%A7%C3%98%C2%B9%C3%98%C2%B2%C3%98%C2%A7%C3%99%C2%85-%C3%99%C2%83%C3%99%C2%88%C3%99%C2%87%C3%99%C2%86%C3%99%C2%88%C3%98%C2%B1%C3%98%C2%AF%C3%98%C2%A7%C3%99%C2%86-%C3%98%C2%A7%C3%99%C2%8A%C3%98%C2%B1%C3%98%C2%A7%C3%99%C2%86%C3%99%C2%8A-%C3%98%C2%A8%C3%99%C2%87-%C3%99%C2%83%C3%99%C2%8A%C3%99%C2%84%C3%99%C2%8A%C3%99%C2%85%C3%98%C2%A7%C3%99%C2%86%C3%98%C2%AC%C3%98%C2%A7%C3%98%C2%B1%C3%99%C2%88

which is not correct.

`curl -i ""http://www.yjc.ir/fa/news/1815565/""` and `wget http://www.yjc.ir/fa/news/1815565/` handle it just fine and correctly follow http://www.yjc.ir/fa/news/1815565/%D8%A7%D8%B9%D8%B2%D8%A7%D9%85-%D9%83%D9%88%D9%87%D9%86%D9%88%D8%B1%D8%AF%D8%A7%D9%86-%D8%A7%D9%8A%D8%B1%D8%A7%D9%86%D9%8A-%D8%A8%D9%87-%D9%83%D9%8A%D9%84%D9%8A%D9%85%D8%A7%D9%86%D8%AC%D8%A7%D8%B1%D9%88

(curl [fixed the issue not too long ago](https://github.com/curl/curl/issues/473) )

Thanks @stav for reporting!
",redapple,stav
2320,2016-10-18 11:10:16,"@eliasdorneles, could you please assign me to this issue?
",moisesguimaraes,eliasdorneles
2317,2016-10-10 02:45:24,"@redapple   Hi, I need some help!!
I got a problem with scrapy version 1.2.0
Here is my code , it is very simple:



In the setting.py:



And I run `scrapy crawl StudyDemo`
It was success, and create a csv file in Scrapy dir, but the file is ZeroByte.

And I tryed run `scrapy crawl StudyDemo -o qiubai.json` it was the same as csv, it create a qiubai.json file ,but the file is ZeroByte too .

And I can get All The Result success in Terminal.
![image](https://cloud.githubusercontent.com/assets/13627665/19225902/a21506f4-8ed6-11e6-9034-fcfc21387029.png)

I don't kown  what's the matter with my Code.  
Need some help!
",iBotasky,redapple
2308,2016-10-06 08:15:23,"Hi @DharmeshPandav ,
[`ROBOTSTXT_OBEY` is still `False` by default](https://github.com/scrapy/scrapy/blob/129421c7e31b89b9b0f9c5f7d8ae59e47df36091/scrapy/settings/default_settings.py#L236). See also @kmike 's comment [here](https://github.com/scrapy/scrapy/pull/1867#issuecomment-200922527).
The note about it being `True` when using `scrapy startproject` is also [there](https://doc.scrapy.org/en/latest/topics/settings.html?#robotstxt-obey).
",redapple,kmike
2306,2017-01-18 13:58:19,"Alright, @kmike , @eliasdorneles , @lopuhin , @dangra , I think this is ready for review.
Thanks in advance!",redapple,lopuhin
2306,2017-01-18 13:58:19,"Alright, @kmike , @eliasdorneles , @lopuhin , @dangra , I think this is ready for review.
Thanks in advance!",redapple,eliasdorneles
2306,2017-01-18 13:58:19,"Alright, @kmike , @eliasdorneles , @lopuhin , @dangra , I think this is ready for review.
Thanks in advance!",redapple,dangra
2306,2017-01-18 13:58:19,"Alright, @kmike , @eliasdorneles , @lopuhin , @dangra , I think this is ready for review.
Thanks in advance!",redapple,kmike
2306,2017-02-21 16:21:59,"@kmike , I rebased, updated the docs with `autoclass` directives and renamed the setting to `REFERRER_POLICY`.
",redapple,kmike
2290,2016-09-28 15:32:59,"Scrapy shell doesn't follow redirects when you do `fetch('http://google.com')`, but it does if you do ` `fetch(scrapy.Request('http://google.com'))`.

I realize this is a historic behavior, but I'd argue that it breaks the expectations for most users, since the most common expectation is that Scrapy would try to do what a browser would do, so I think we should change it.

In my opinion, not following the redirect should be the exceptional behavior, and so that's what should require the user to build a `scrapy.Request` object.

I think ideally, we would be able to pass a keyword argument to `fetch`, like:



And only for that case it would set handle_httpstatus_all to True as explained by @redapple here: https://github.com/scrapy/scrapy/issues/2177#issuecomment-239430163",eliasdorneles,redapple
2287,2016-09-24 07:10:26,"- quote url
- use hash of url as filename

cc @kmike 
",pawelmhm,kmike
2285,2016-09-23 15:24:53,"Related to @dangra 's comment: https://github.com/scrapy/scrapy/pull/2212#issuecomment-248616113
",redapple,dangra
2273,2016-09-20 16:35:51,"@eliasdorneles , is your +1 a ""good to merge""? :)
",redapple,eliasdorneles
2268,2016-11-23 11:18:47,"Thanks @ahlinc . Sorry for the late feedback.
Looks suspiciously harmless ""à-priori"" but as this touches the heart of the orchestration, I would ask @curita if she can comment, being the one who last touched (and refactored) this part.
@jdemaeyer may also want to check how this affects the Add-Ons project (if it does).

@curita , @jdemaeyer , would you have time to check this? Thanks",redapple,curita
2268,2016-11-23 11:18:47,"Thanks @ahlinc . Sorry for the late feedback.
Looks suspiciously harmless ""à-priori"" but as this touches the heart of the orchestration, I would ask @curita if she can comment, being the one who last touched (and refactored) this part.
@jdemaeyer may also want to check how this affects the Add-Ons project (if it does).

@curita , @jdemaeyer , would you have time to check this? Thanks",redapple,jdemaeyer
2267,2016-09-20 09:17:01,"@ashkulz's https://github.com/scrapy/scrapy/pull/2166 on top of master branch.

This PR is to complement it with recommendation on using virtualenvs

Original description:

> [Official Ubuntu packages] are not currently updated and fail to install on Ubuntu 16.04.
> Also update the instructions to refer to the earliest supported LTS (Ubuntu 12.04).

Fixes #2137 and closes #2076
",redapple,ashkulz
2267,2016-09-28 15:17:21,"@kmike , @eliasdorneles , @lopuhin , @dangra ,
I updated the page with what we discussed.
how does it look now?
",redapple,lopuhin
2267,2016-09-28 15:17:21,"@kmike , @eliasdorneles , @lopuhin , @dangra ,
I updated the page with what we discussed.
how does it look now?
",redapple,eliasdorneles
2267,2016-09-28 15:17:21,"@kmike , @eliasdorneles , @lopuhin , @dangra ,
I updated the page with what we discussed.
how does it look now?
",redapple,dangra
2267,2016-09-28 15:17:21,"@kmike , @eliasdorneles , @lopuhin , @dangra ,
I updated the page with what we discussed.
how does it look now?
",redapple,kmike
2252,2016-09-20 18:05:00,"@redapple @stummjr I've address your comments, plus some other minor things. What do you think?
",eliasdorneles,redapple
2252,2016-09-20 21:20:59,"@kmike I think I've addressed all your comments as well. :)

Btw, thanks for reviewing you all! :bow: :bow: 
",eliasdorneles,kmike
2252,2016-09-21 14:08:47,"After chatting about it w/ @kmike, I changed the recommendation on how to learn Python.

For people experienced w/ other languages, the tutorial recommends Dive into Python 3 and the official Python tutorial as alternative.

For people learning to program, it recommends Learn Python the Hard Way and alternatively the list of resources for non-programmers from Python's wiki.
",eliasdorneles,kmike
2252,2016-09-22 14:08:06,"Thanks for reviewing @kmike, I've addressed your comments and made other small changes in the latest commit.
@redapple @kmike Does this look good to merge?
",eliasdorneles,redapple
2249,2016-09-16 19:00:06,"hey @kmike, I'll merge this to move things forward, feel free to point out if you have any other concern.
thanks for reviewing!
",eliasdorneles,kmike
2246,2016-09-20 13:26:31,"@lopuhin may have comments too.
",redapple,lopuhin
2243,2016-09-19 10:32:13,"@kmike @eliasdorneles , do you agree with the change?
",redapple,eliasdorneles
2243,2016-09-19 10:32:13,"@kmike @eliasdorneles , do you agree with the change?
",redapple,kmike
2241,2016-09-15 09:30:29,"@pawelmhm , I also think it makes sense to have `OffsiteMiddleware` (or it's functionality) as a downloader middleware, if only to handle offsite-redirects (#15, #184)
[@pablohoffman 's comment there](https://github.com/scrapy/scrapy/issues/15#issuecomment-2048222) recommends to not change `OffsiteMiddleware` (_""avoid modifying the behavior of a well established and known middleware such as the offsite one""_), but that was 5 years ago, it may be time to revisit the position.
I believe the main reason for offsite middleware being a spider middleware is that it is conceptually spider dependent (through its  `.allowed_domains`) and not really standalone or spider-agnostic.
But of course, downloader middlewares do have access to the spiders in their methods.
Other attempts at fixing #15 have patched `RedirectMiddleware` (a downloader middleware) to merge with the offsite feature  (#1002, #1028).
Such a change on `OffsiteMiddleware` would be backwards incompatible if we provide it as downloader mdw, as those who patched it may have 2 competing mdw or a broken one  (e.g. #1640) -- to be checked.
",redapple,pablohoffman
2236,2016-09-14 21:20:16,"@eliasdorneles: LGTM
",stummjr,eliasdorneles
2236,2016-09-15 10:06:02,"Many thanks @stummjr and @eliasdorneles !
",redapple,eliasdorneles
2233,2016-09-13 12:40:05,"[dmoz.org](https://www.dmoz.org/) changed layout a few months ago, breaking [scrapy's tutorial](http://doc.scrapy.org/en/1.1/intro/tutorial.html) badly.
We have lots of open issues or PRs from the community to fix it: #2023, #2032, #2090, #2130, #2180, not to mention the broken dirbot https://github.com/scrapy/dirbot/issues/19). Thanks to all of you for this.

The thing is dmoz.org may change layout again in the future and we need a more stable scraping-friendly website to serve scrapy tutorial's educational purpose.

Scrapinghub maintains a couple of fake-but-realistic websites that can be used for web scraping training, and scrapy know-how in particular: http://books.toscrape.com/ and http://quotes.toscrape.com/ (see http://sites.toscrape.com/)

/cc @eliasdorneles and @stummjr already working on this
",redapple,eliasdorneles
2233,2016-09-13 12:40:05,"[dmoz.org](https://www.dmoz.org/) changed layout a few months ago, breaking [scrapy's tutorial](http://doc.scrapy.org/en/1.1/intro/tutorial.html) badly.
We have lots of open issues or PRs from the community to fix it: #2023, #2032, #2090, #2130, #2180, not to mention the broken dirbot https://github.com/scrapy/dirbot/issues/19). Thanks to all of you for this.

The thing is dmoz.org may change layout again in the future and we need a more stable scraping-friendly website to serve scrapy tutorial's educational purpose.

Scrapinghub maintains a couple of fake-but-realistic websites that can be used for web scraping training, and scrapy know-how in particular: http://books.toscrape.com/ and http://quotes.toscrape.com/ (see http://sites.toscrape.com/)

/cc @eliasdorneles and @stummjr already working on this
",redapple,stummjr
2216,2016-09-22 16:58:42,"Superceded by https://github.com/scrapy/scrapy/pull/2280
Thanks @eliasdorneles 
",redapple,eliasdorneles
2212,2016-09-01 15:20:28,"I believe we said that Twisted 13.2 in Ubuntu Trusty 14.04 is a bit too old.
@dangra , can you confirm?
",redapple,dangra
2212,2016-09-07 08:16:46,"@dangra , are you ok with using Debian Jessie as baseline?
",redapple,dangra
2212,2016-09-19 08:51:06,"@dangra , ping ;-)
",redapple,dangra
2198,2016-08-26 11:40:08,"@briehanlombaard , so the trouble is indeed with that commit you linked.
Current implementation is looking for `ARTICLEIMAGESPIPELINE_IMAGES_THUMBS` without looking for `IMAGES_THUMBS`, contrary to what is done for `IMAGES_STORE`, and contrary to what the docs currently say.
@pawelmhm , it looks like we still have a backward incompatibility issue with https://github.com/scrapy/scrapy/pull/1891 and the subsequent fix in https://github.com/scrapy/scrapy/pull/1989
@briehanlombaard's use-case looks very reasonable:
- custom ImagesPipeline but only for converting, no class attribute change
- use of custom settings for `IMAGES_STORE` and `IMAGES_THUMBS`, like the docs recommend
",redapple,pawelmhm
2194,2016-08-23 17:47:19,"Among the pypi packages with a purpose similar to
`scrapy.utils.deprecate.create_deprecated_class`,
scrapy's implementation stands out
being the only one that warns when subclassing.

I think it deserves its own package
where further development can continue
without inflating scrapy's codebase.

@dangra, you have [a name in pypi](https://pypi.python.org/pypi/deprecated/) reserved.
Do you have a plan to create a package?
",Digenis,dangra
2194,2016-08-25 08:30:26,"Me and @dangra wanted to create this package quite a long time ago; the idea was indeed to move scrapy deprecation utilities to a separate package, likely with a different API. We never found time to do that, but it is still in plans :)
",kmike,dangra
2185,2016-08-19 09:42:25,"@josephdarkins , you may find better support on the [lxml mailing list](https://mailman-mail5.webfaction.com/listinfo/lxml).
[Their installation page](http://lxml.de/installation.html#installation) has a section on installing lxml on MacOS:

> On MacOS-X, use the following to build the source distribution, and make sure you have a working Internet connection, as this will download libxml2 and libxslt in order to build them:



I don't use MacOS so I can't help you further.
Maybe @kmike can.
",redapple,kmike
2176,2016-08-12 16:22:10,"@redapple @kmike  Please can you help me on this issue :cry: 
",kgrvamsi,redapple
2176,2016-08-12 16:22:10,"@redapple @kmike  Please can you help me on this issue :cry: 
",kgrvamsi,kmike
2166,2016-09-20 09:20:08,"@kmike , @dangra , @eliasdorneles ,
I've created https://github.com/scrapy/scrapy/pull/2267 with @ashkulz 's change to add installation guidelines (cf. https://github.com/scrapy/scrapy/pull/2166#r79451738).
It's using a new branch on the scrapy repo, `deprecate-ubuntu-packages`, so that you can commit any changes directly.
",redapple,dangra
2166,2016-09-20 09:20:08,"@kmike , @dangra , @eliasdorneles ,
I've created https://github.com/scrapy/scrapy/pull/2267 with @ashkulz 's change to add installation guidelines (cf. https://github.com/scrapy/scrapy/pull/2166#r79451738).
It's using a new branch on the scrapy repo, `deprecate-ubuntu-packages`, so that you can commit any changes directly.
",redapple,eliasdorneles
2166,2016-09-20 09:20:08,"@kmike , @dangra , @eliasdorneles ,
I've created https://github.com/scrapy/scrapy/pull/2267 with @ashkulz 's change to add installation guidelines (cf. https://github.com/scrapy/scrapy/pull/2166#r79451738).
It's using a new branch on the scrapy repo, `deprecate-ubuntu-packages`, so that you can commit any changes directly.
",redapple,kmike
2155,2016-08-11 08:49:36,"@lucab0ni , @pawelmhm , as a side note,
while at it, this telnet extension could be moved to scrapy-plugins.
what do you think? cc @kmike , @dangra , @eliasdorneles 
",redapple,dangra
2155,2016-08-11 08:49:36,"@lucab0ni , @pawelmhm , as a side note,
while at it, this telnet extension could be moved to scrapy-plugins.
what do you think? cc @kmike , @dangra , @eliasdorneles 
",redapple,eliasdorneles
2155,2016-08-11 08:49:36,"@lucab0ni , @pawelmhm , as a side note,
while at it, this telnet extension could be moved to scrapy-plugins.
what do you think? cc @kmike , @dangra , @eliasdorneles 
",redapple,kmike
2155,2016-08-11 13:44:12,"I think someone (@daniel? @pablohoffman?) was against removing telnet from Scrapy. It makes sense to have debugging features built in. We had a similar discussion about TELNET_ENABLED option: let's say it is False by default. Spider starts misbehaving, you want to debug it, then you learn about telnet extension and want to use it, but telnet is off by default, so you can't. You can kill the spider, change the option and start the spider again, but you may loose the specific state which is necessary to debug the problem.

The advantage of having manhole separate and making Scrapy depend on it is that it will allow separate release cycles. I'm not sure how important is it.
",kmike,pablohoffman
2152,2016-07-29 16:55:14,"@redapple do you think this is good enough for now?
It'll ease the pain of releasing in this branch, while still leaving the enforced release cycle in `master`.
Would you prefer to have this in master too?
",eliasdorneles,redapple
2144,2016-07-29 06:53:38,"ping @kmike, @redapple, @jdemaeyer 
",darshanime,redapple
2144,2016-07-29 06:53:38,"ping @kmike, @redapple, @jdemaeyer 
",darshanime,jdemaeyer
2144,2016-07-29 06:53:38,"ping @kmike, @redapple, @jdemaeyer 
",darshanime,kmike
2140,2016-08-01 14:29:27,"@jesuslosada , I agree that `IMAGES_EXPIRES` should be `90`, and if it's not, the tests are missing one case.
But I admit I'm having a hard time re-reading https://github.com/scrapy/scrapy/pull/1989 and especially https://github.com/scrapy/scrapy/pull/1989/files#diff-0441c35e69fd9bb28942a9b2c47dc565R43 where not only `IMAGES_EXPIRES` but `MIN_WIDTH`, `MIN_HEIGHT` and others set to `0`
Maybe @pawelmhm can comment on this one.
",redapple,pawelmhm
2124,2017-01-16 15:46:46,"If I understand [the docs](https://doc.scrapy.org/en/latest/topics/request-response.html#request-objects) correctly, the purpose of the `dont_merge_cookies` meta key is to indicate Scrapy not to add to the cookiejar cookies received in the `Set-Cookie` response header.

Consider the following spider:



The current behaviour with `meta={'dont_merge_cookies': False}` is all cookies, both send explicitely in the `cookies` Request keyword and received in the `Set-Cookie` are added to the cookiejar:



On the other hand, with `meta={'dont_merge_cookies': True}` the log shows that no cookies are set, not even the ones from the `cookies` Request keyword:



As suggested, commenting [these lines](https://github.com/scrapy/scrapy/blob/1.3.2/scrapy/downloadermiddlewares/cookies.py#L28-L29) makes for the (in my opinion) correct behaviour, i.e., sending to the server manually set cookies but not taking back any cookies from the server:


However, `tests/tests/test_downloadermiddleware_cookies.py` fails, but I suspect this is a problem with the test itself, not the implementation. Specifically, the assertion which fails is [this one](https://github.com/scrapy/scrapy/blob/1.3.2/tests/test_downloadermiddleware_cookies.py#L125), and I think that's ok because the `Cookie` key is set for that domain [here](https://github.com/scrapy/scrapy/blob/1.3.2/tests/test_downloadermiddleware_cookies.py#L120), when `dont_merge_cookies` was not `True`.

@redapple Am I misinterpreting something? Thanks!",elacuesta,redapple
2111,2016-07-11 07:28:12,"See https://github.com/scrapy/scrapy/issues/1435#issuecomment-182393180 by @nyov for motivation
",redapple,nyov
2085,2016-07-04 15:37:41,"I'm having second thoughts. Perhaps this shouldn't be configurable and require SSL always?
@redapple, @kmike ?
",nyov,redapple
2085,2016-07-04 15:37:41,"I'm having second thoughts. Perhaps this shouldn't be configurable and require SSL always?
@redapple, @kmike ?
",nyov,kmike
2061,2016-06-17 19:33:39,"This PR is a starting point to solve #220.
It could probably use some more test cases, mostly to figure out what exactly is the desired behaviour when processing the exceptions.
I can't take much credit for this: if it breaks, blame @dangra 😛 
",elacuesta,dangra
2061,2017-02-09 13:37:22,"Current status of this PR is:
* Catch exceptions from previous `process_spider_output` (also documented)
* Execute only non already called `process_spider_output` methods when `process_spider_exception` returns an iterable
* Raise `InvalidValue` exception (scrapy-specific, name suggestions are welcome) when an invalid value is returned from a spider middleware's processing method

@kmike I think that addresses your latest concerns, please let me know if there's anything more I can do to get this PR moving. Thanks!",elacuesta,kmike
2061,2017-03-06 13:05:57,"@kmike , what do you think of this PR now?",redapple,kmike
2049,2016-07-08 11:51:49,"Fixed in #2050!
@Tethik @robsonpeixoto @redapple thanks fellows!
",eliasdorneles,redapple
2048,2016-06-14 13:44:27,"@kmike , I've updated the entry. Reads better?
",redapple,kmike
2034,2016-06-07 14:20:13,"@dracony , thanks for this.
how will people use this? use their own JSON exporter subclass?
if yes, how about introducing a setting for this? it could be simpler.
Also, [@mgachhui was suggesting](https://github.com/scrapy/scrapy/issues/1965#issuecomment-218077754) having an encoding parameter, not only default UTF-8
(and applying this to XML exports too)
",redapple,mgachhui
2030,2016-06-05 18:31:50,"Fixes https://github.com/scrapy/scrapy/issues/8

This is by no means complete, specially as of the changes committed to this point,  most of the effort to this point dealt with porting relevant parts of `django.dispatch` to Scrapy and writing parts of it without dependency on `django.utils` and introducing methods for API consistency, but opening this PR so that the changes can be monitored by the community. I'll update this as the changes come along in detail about what the individual changes concern themselves with.

To this point we have:
- Introduced the `scrapy.dispatch` module
- Changed all standard signals to be objects of the `Signal` class instead of the generic `object`.

Most of the other stuff is outside the mainline and I'll be pushing it as soon as I'm done with it, I'll preferably have a working prototype by the end of this week(Saturday), latest by the start of the next one(the Monday after that). 

/cc @jdemaeyer 
",rootAvish,jdemaeyer
2030,2016-12-29 12:37:09,"@redapple, whatever issue Travis was having seems to be fixed now, waiting for @kmike 's comments on the usage of the weakrefmethod package, all the other stuff is taken care of. Sorry for the delay, was in a crunch at work.",rootAvish,kmike
2024,2016-10-20 10:48:57,"Right, @stummjr and I weren't sure to add it because those sample projects are more about Scrapinghub stack, but I suppose it makes sense to link them there too.
",eliasdorneles,stummjr
2023,2016-06-13 12:48:50,"hi @redapple I updated this PR.
",carlosp420,redapple
2023,2016-08-16 12:49:15,"LGTM, thanks @carlosp420 !
what do you think @kmike , @eliasdorneles , @stummjr ?
",redapple,stummjr
2023,2016-08-16 12:49:15,"LGTM, thanks @carlosp420 !
what do you think @kmike , @eliasdorneles , @stummjr ?
",redapple,eliasdorneles
2007,2016-06-03 12:37:52,"thanks @yssoe this sounds like good workaround and probably the only way to do things now, but I thought more about handling websockets protocol inside Scrapy itself. 

This way websockets code will be async and properly integrated with all other components. In your approach we'll be mixing blocking code from other networking library with Scrapy. It would be better to simply add support for websockets to Scrapy, so that for example users could do following thing:



I tried writing some test implementation, but I stumbled on following problems.

1) Downloader middleware is designed to deal with HTTP requests. Nothing that matters in middleware matters for websocket messages (headers, status codes, cookies etc this is all irrelevant for message sent via websockets). So websocket messages need some way to bypass downloader middleware

2) More important problem is dealing with server push. Many core components (engine, scraper etc) assume that download_request will return Twisted Deferrred. But handler.download_request() for websockets cannot return deferred for every message sent. This is because websockets allow for server push. So if user sends message he can get multiple responses for it. This means that Deferred would fire multiple times. Firing deferred multiple times raises Twisted exceptions. 

I could go forward and try to design some solution for this (if possible). I wonder though if we agree this is actually needed in Scrapy? What do you think @kmike @redapple @eliasdorneles ? Do you think it's good to have websockets support?
",pawelmhm,redapple
2007,2016-06-03 12:37:52,"thanks @yssoe this sounds like good workaround and probably the only way to do things now, but I thought more about handling websockets protocol inside Scrapy itself. 

This way websockets code will be async and properly integrated with all other components. In your approach we'll be mixing blocking code from other networking library with Scrapy. It would be better to simply add support for websockets to Scrapy, so that for example users could do following thing:



I tried writing some test implementation, but I stumbled on following problems.

1) Downloader middleware is designed to deal with HTTP requests. Nothing that matters in middleware matters for websocket messages (headers, status codes, cookies etc this is all irrelevant for message sent via websockets). So websocket messages need some way to bypass downloader middleware

2) More important problem is dealing with server push. Many core components (engine, scraper etc) assume that download_request will return Twisted Deferrred. But handler.download_request() for websockets cannot return deferred for every message sent. This is because websockets allow for server push. So if user sends message he can get multiple responses for it. This means that Deferred would fire multiple times. Firing deferred multiple times raises Twisted exceptions. 

I could go forward and try to design some solution for this (if possible). I wonder though if we agree this is actually needed in Scrapy? What do you think @kmike @redapple @eliasdorneles ? Do you think it's good to have websockets support?
",pawelmhm,eliasdorneles
2007,2016-06-03 12:37:52,"thanks @yssoe this sounds like good workaround and probably the only way to do things now, but I thought more about handling websockets protocol inside Scrapy itself. 

This way websockets code will be async and properly integrated with all other components. In your approach we'll be mixing blocking code from other networking library with Scrapy. It would be better to simply add support for websockets to Scrapy, so that for example users could do following thing:



I tried writing some test implementation, but I stumbled on following problems.

1) Downloader middleware is designed to deal with HTTP requests. Nothing that matters in middleware matters for websocket messages (headers, status codes, cookies etc this is all irrelevant for message sent via websockets). So websocket messages need some way to bypass downloader middleware

2) More important problem is dealing with server push. Many core components (engine, scraper etc) assume that download_request will return Twisted Deferrred. But handler.download_request() for websockets cannot return deferred for every message sent. This is because websockets allow for server push. So if user sends message he can get multiple responses for it. This means that Deferred would fire multiple times. Firing deferred multiple times raises Twisted exceptions. 

I could go forward and try to design some solution for this (if possible). I wonder though if we agree this is actually needed in Scrapy? What do you think @kmike @redapple @eliasdorneles ? Do you think it's good to have websockets support?
",pawelmhm,kmike
2005,2016-07-19 10:13:01,"Yup, looks great now. Thank you, @feliperuhland !

@redapple this looks ready. How do you feel about merging it for 1.2?
",eliasdorneles,redapple
2003,2016-05-21 15:21:01,"(sorry, that was for @dangra) 
",redapple,dangra
1994,2016-05-17 16:17:09,"It looks like it was me who persuaded @eliasdorneles to remove extract_first from this example: https://github.com/scrapy/scrapy/pull/1106#discussion_r27220120. I'm fine with adding it now :)
",kmike,eliasdorneles
1991,2016-05-18 18:07:34,"Adding some comments about the implementation:

Originally, I've suggested to implement the communication channel between scrapy and the external spider using the [ProcessProtocol](http://twistedmatrix.com/documents/current/core/howto/process.html) from twisted, and as pointed in the docs, each message ends with a line break `\n`.
I started an initial [POC](https://github.com/aron-bordin/POC-Streaming) to get an idea on how this should work.

However, this implementation could get some problems with buffering, because the messages sent by `transport.write` and received by `outReceived` can be buffered by the system.

Checking the @Preetwinder POC, he uses https://github.com/Preetwinder/ScrapyStreaming/blob/master/linereceiverprocess.py#L53 to wrap the process and avoid this buffer issues. 

Now I'm analyzing the best way to approach this possible problems with stdin/stdout buffering. 

As long as all messages must end with a line break, both implementations (streaming core and external spiders) could ""buffer"" the received data and process it after receiving the line break (end of the message).
Also, a different implementation could make it easier. Using the [LineReceiver](https://twistedmatrix.com/documents/9.0.0/api/twisted.protocols.basic.LineReceiver.html) in the streaming core could help while receiving data. But I'm still not sure about the best way to write in the process stdin, unfortunately `stdbuf` is not available in all platforms. 

As part of the communication protocol. the line break is defined as the end of the message. If the external spider developer uses this information and just analyze the received data after the line break, this should be enough.

Do you have any comments about the implementations and these possible issues ?
",aron-bordin,Preetwinder
1989,2016-07-08 10:57:13,"@kmike , @eliasdorneles ,
are you ok with merging this? (and backporting to 1.1 of course)
",redapple,kmike
1989,2016-07-12 12:00:50,"@kmike updated in c22cc1096be5aaa3f381976ba0b70a014405dc4f
- keeping line limit in docs
- added functools partial
- removed dead code 
- add doctest for key for pipe
",pawelmhm,kmike
1964,2016-05-02 14:21:59,"hi @Digenis , do you know if this lost logs issues is already tracked? I could not find similar issue on [scrapy/scrapyd](https://github.com/scrapy/scrapyd/issues)
",redapple,Digenis
1949,2016-04-27 18:05:32,"@kmike , with `safe_url_string` removed from link extraction (https://github.com/scrapy/scrapy/pull/1949/commits/54f1c2419e96c832932a9125f719b791bdd2694c), for Python2 Unicode URL input case in `Link` init, one actually needs UTF8 for `to_bytes`, otherwise `test_restrict_xpaths_with_html_entities` even fails (""iso8859-15"" can only encode part of the Unicode characters)

What happens with `to_bytes(url, encoding='utf8')` in `Link.__init__()` is illustrated below:


",redapple,kmike
1940,2016-04-25 20:25:35,"Another way to fix response.encoding problem is to create a middleware which sets request.encoding to response.encoding if it is not set explicitly. But it would be backwards incompatible, and @dangra didn't like middleware approach for relative URLs and ruled them out (which was a good decision).
",kmike,dangra
1909,2016-04-07 12:34:53,"@redapple here's that pull request you... requested.
",kneufeld,redapple
1905,2016-08-08 10:25:26,"Sorry, must've missed your last comment here.

LGTM, could you rebase? @redapple you recently merged changes to `scrapy.utils.gz`, but it seems that they're not related to this 'properly recover from illegal EOF' issue, right? /cc @kmike 
",jdemaeyer,redapple
1897,2016-04-13 12:49:37,"Hi @redapple can I get this one ?
I'm thinking about adding a `multipart` parameter, that defaults to False, in `from_response`. If False, uses the default `application/x-www-form-urlencoded`, if True, uses the `multipart/form-data` encoding type.
",aron-bordin,redapple
1891,2016-03-29 23:42:53,"fix #1850
Addressed issues:
1. Move default settings from `FilesPipelines.py` and `ImagesPipelines.py` to `settings/default_settings.py`
2. Change class attributes from `FilesPipelines.py` and `ImagesPipelines.py` to instance attributes. It allows that distinct spiders running together have distinct custom settings for the pipelines.

Note: I have no way to test AWS related stuff. So I haven't touched any code related to AWS.

@kmike as you have looked at the issue before, I think you should review the PR.
",djunzu,kmike
1887,2016-03-29 03:55:44,"Can someone recall what was wrong with Twisted 11 on precise?
(https://github.com/scrapy/scrapy/commit/8b0c9466d6733c6523aa8c630e70c607cd845128)

Or was txlib/tx only introduced to backport features? The commit message and Readme don't quite say the same thing.



It seems all the `xlib.tx` Exceptions should have been available since Tx 11.1 (except for `ResponseNeverReceived`, which doesn't seem to be explicitly used though), so
I hope partial deprecation of `xlib.tx` might be fast-tracked by pulling them in from twisted-proper instead, which only requires bumping to Tx 11.1 and not 13.0 yet.

That way only the HTTP11 download-handler code should still depend on `xlib.tx`.

ping @dangra 

(I did not remove the Exceptions so that any later diff against twisted won't throw a massive changeset.)
",nyov,dangra
1887,2016-12-07 15:10:37,"LGTM.
@dangra , @kmike , @eliasdorneles ?
This supercedes #2403 I believe, for master branch at least",redapple,eliasdorneles
1874,2016-03-23 15:51:30,"@dangra , @kmike , @lopuhin , @eliasdorneles ,
apart from the `safe_url_string()` from https://github.com/scrapy/w3lib/pull/45 , I don't have anything else to change.
could you have a look at the changes in the meantime the w3lib PR gets merged?
Thanks.
",redapple,lopuhin
1874,2016-03-23 15:51:30,"@dangra , @kmike , @lopuhin , @eliasdorneles ,
apart from the `safe_url_string()` from https://github.com/scrapy/w3lib/pull/45 , I don't have anything else to change.
could you have a look at the changes in the meantime the w3lib PR gets merged?
Thanks.
",redapple,dangra
1874,2016-03-23 15:51:30,"@dangra , @kmike , @lopuhin , @eliasdorneles ,
apart from the `safe_url_string()` from https://github.com/scrapy/w3lib/pull/45 , I don't have anything else to change.
could you have a look at the changes in the meantime the w3lib PR gets merged?
Thanks.
",redapple,eliasdorneles
1874,2016-03-23 15:51:30,"@dangra , @kmike , @lopuhin , @eliasdorneles ,
apart from the `safe_url_string()` from https://github.com/scrapy/w3lib/pull/45 , I don't have anything else to change.
could you have a look at the changes in the meantime the w3lib PR gets merged?
Thanks.
",redapple,kmike
1874,2016-03-31 13:57:30,"@kmike , so I think I'm done with the different canonicalization tests, with expected and unexpected input.
",redapple,kmike
1870,2016-10-18 19:17:09,"@eliasdorneles, this PR doesn't makes sense anymore. The canonalize_url() function moved to **w3lib**.
",moisesguimaraes,eliasdorneles
1820,2016-02-26 17:36:41,"Thanks @kmike ... I submitted a bit early. Just added release notes updates (but I can seperate)
",redapple,kmike
1818,2016-02-26 11:33:04,"@kmike , do we want it in 1.1?
",redapple,kmike
1806,2016-02-23 10:08:04,"@redapple, please review.
",darshanime,redapple
1803,2016-03-17 15:22:28,"@eliasdorneles , I'm revising my assumption on page encoding and URLs.
I now believe that this is the correct test, i.e. if input is Unicode, serialized URL should percent-encode UTF-8 encoded characters:



Encoding should only influence query or form bodies

The following would be another test, leaving non-UTF8-encoded percent-encodings untouched


",redapple,eliasdorneles
1802,2016-02-27 19:09:39,"I've also seen a problem with many requests to a single domain. The simplest solution is to decrease request priority for domain each N requests to this domain; this can be done either in a spider or in a middleware. It is not optimal because queuelib keeps a queue per each possible request priority, and so this is inefficient if there are many different request priorities; but if N is large enough it works ok. 

In other case I went as far as created a custom scheduler and custom request queues: one queue class for requests from a single domain, and another queue class for balancing between queues for a single domains. 

@KrisB88 The latter is very similar to your DomainPriorityQueue; instead of using a heap it uses a random sample (I like heap approach though), and for the extra flexibility domain is passed in request.meta, not extracted from URL: you can assing a custom 'scheduler slot' to request this way, not necesarily based on the domain. 

I think self.domains is not needed in your DomainPriorityQueue implementation - why can't you check self.pqueues directly?

In frontera this is solved in a different way: if I'm not mistaken, scheduler checks if downloader is busy processing requests for a single domain and asks queue not to send requests for this domain if there are many requests already (//cc @sibiryakov - please correct me if that's wrong).
",kmike,sibiryakov
1799,2016-03-24 11:13:27,"Thanks @Digenis , sorry for the delay in reviewing.

I do understand the need to customize what's extracted as text for the link from the elements.
However, I would find it difficult to grasp (say, as a new scrapy user) the new ""text"" argument as either:
- _string argument for XPath string() function applied to the element, defaulting to """"_
- or, _a callable returning a string when given a link-matching element as input_

(this would need to be documented I believe. Shame that LxmlParserLinkExtractor is not already)

I would prefer if `LxmlParserLinkExtractor` had a method taking the matching element and returning a string. it would default to `return lxml.etree.XPath('string()')(element)`. And you could subclass it to customize the behavior.
Would that work for you?

@eliasdorneles , @dangra , @kmike, any thoughts ?
",redapple,kmike
1799,2016-03-24 11:13:27,"Thanks @Digenis , sorry for the delay in reviewing.

I do understand the need to customize what's extracted as text for the link from the elements.
However, I would find it difficult to grasp (say, as a new scrapy user) the new ""text"" argument as either:
- _string argument for XPath string() function applied to the element, defaulting to """"_
- or, _a callable returning a string when given a link-matching element as input_

(this would need to be documented I believe. Shame that LxmlParserLinkExtractor is not already)

I would prefer if `LxmlParserLinkExtractor` had a method taking the matching element and returning a string. it would default to `return lxml.etree.XPath('string()')(element)`. And you could subclass it to customize the behavior.
Would that work for you?

@eliasdorneles , @dangra , @kmike, any thoughts ?
",redapple,eliasdorneles
1799,2016-03-24 11:13:27,"Thanks @Digenis , sorry for the delay in reviewing.

I do understand the need to customize what's extracted as text for the link from the elements.
However, I would find it difficult to grasp (say, as a new scrapy user) the new ""text"" argument as either:
- _string argument for XPath string() function applied to the element, defaulting to """"_
- or, _a callable returning a string when given a link-matching element as input_

(this would need to be documented I believe. Shame that LxmlParserLinkExtractor is not already)

I would prefer if `LxmlParserLinkExtractor` had a method taking the matching element and returning a string. it would default to `return lxml.etree.XPath('string()')(element)`. And you could subclass it to customize the behavior.
Would that work for you?

@eliasdorneles , @dangra , @kmike, any thoughts ?
",redapple,dangra
1796,2016-02-24 17:29:21,"@dangra , I would go with private by default, proper warning in release notes, and taking advantage of RC2 so users can tell us if it's a good move or not (sure, some users would discover only when 1.1 is officially baked)

@kmike , @eliasdorneles , @curita , @jdemaeyer , any comments ?
",redapple,curita
1796,2016-02-24 17:29:21,"@dangra , I would go with private by default, proper warning in release notes, and taking advantage of RC2 so users can tell us if it's a good move or not (sure, some users would discover only when 1.1 is officially baked)

@kmike , @eliasdorneles , @curita , @jdemaeyer , any comments ?
",redapple,jdemaeyer
1796,2016-02-24 17:29:21,"@dangra , I would go with private by default, proper warning in release notes, and taking advantage of RC2 so users can tell us if it's a good move or not (sure, some users would discover only when 1.1 is officially baked)

@kmike , @eliasdorneles , @curita , @jdemaeyer , any comments ?
",redapple,eliasdorneles
1796,2016-02-24 17:29:21,"@dangra , I would go with private by default, proper warning in release notes, and taking advantage of RC2 so users can tell us if it's a good move or not (sure, some users would discover only when 1.1 is officially baked)

@kmike , @eliasdorneles , @curita , @jdemaeyer , any comments ?
",redapple,kmike
1792,2016-02-18 21:51:11,"Originally from https://github.com/scrapy/scrapy/pull/1761

@lopuhin , I squashed the 22 commits into one. I did not manage to submit a correct PR with your individual commits.
Does it look correct?
",redapple,lopuhin
1791,2016-02-18 21:16:46,"Damn, why does this contain more than @lopuhin changes?? pfff
Closing.
",redapple,lopuhin
1789,2016-02-18 10:39:06,"See motivation https://github.com/scrapy/scrapy/issues/1788

From @kmike 

>  is not clear that Scrapy middlewares have the same features as extensions. Accessing settings is a common use case, I think we should provide examples in middleware docs, even if they are redundant.
",redapple,kmike
1789,2016-10-18 15:32:40,"Just a heads up, @lfmattossch is working on this -- thanks Luiz!
",eliasdorneles,lfmattossch
1783,2016-02-26 18:18:39,"I tried to fix the tests @redapple  added here https://github.com/lopuhin/w3lib/commit/70281a5b58d594fc634713502caaed8a0ae461fe, I'll check a few things and submit a PR if all is ok.
",lopuhin,redapple
1777,2016-02-14 18:39:36,"Scrapy currently assumes in a lot of it's functionality to be used as a Framework.

Work has been done in the past, and is ongoing, to make it more usable as a library as well.
I would like to see even more change in this regard.
## 

Is it feasible to consider decoupling scrapy's django-like concept of a ""project"" in the filesystem from ""scrapy core"" -- possibly moving the behavior of creating a 'scrapy project' (and all that entails, such as file templates) into a separate project? Or the opposite of creating a `libscrapy` project for the bare essentials?
## 

A lot of scrapy's environment configuration is done through the `Settings` object.
All configurable classes/functions that are defined through settings (anything that reads like `scrapy.downloadermiddlewares.redirect.RedirectMiddleware`), get loaded through `load_object` which in turn leverages `importlib.import_module` to find a module and pull in the code.

In a scripted environment we usually just want to load an already defined or imported class into scrapy, such as an `ItemPipeline`, and I would like to see support for loading objects directly in some manner there.

**/edit**: Being discussed in #1215 (thanks @kmike)
## 

I hope this is not too ""out there"".
(And if someone has other `scrapy-as-a-library`-related considerations, they are welcome to add them here as well.
",nyov,kmike
1768,2016-02-07 01:12:22,"The change for this was in commit c76190d491fca9f35b6758bdc06c34d77f5d9be9 .

@kalessin also already [noted another issue with this change](https://github.com/scrapy/scrapy/commit/c76190d491fca9f35b6758bdc06c34d77f5d9be9#diff-e167e9587f554a39ea79b2473f8eb868L255).

Not sure which is better; do people use exporters (instead of custom pipelines) with binary data?
CSV, XML, json -- these formats all seem line-based or at least string-based, unsuitable for binary output at all, so it might make sense to follow py3 standard of using 'Text I/O' for standard I/O there?
Should exporters be sub-divided into binary exporters and text exporters maybe?
",nyov,kalessin
1764,2016-03-27 11:35:03,"Sorry, that's what you did, I misread. Maybe someone on Mac OS can test
this. @kmike maybe?
Le 27 mars 2016 13:32, ""Paul Tremberth"" paul.tremberth@gmail.com a écrit :

> I tried without changing context factory. Scrapy 1.1rc3 uses better
> defaults, so you shouldn't need to tweak anything. Can you try with
> out-of-the-box scrapy 1.1rc3? (I used `scrapy shell https://...`)
> Le 27 mars 2016 03:51, ""@ntoinet"" notifications@github.com a écrit :
> 
> > @redapple https://github.com/redapple Thanks for your reply: I tried
> > it with scrapy 1.1.0rc3 over python 2.7 & python 3.4.
> > It works without TLSFlexibleContextFactory for
> > https://www.buyagift.co.uk/ and https://shop.clares.co.uk/
> > However it does not work for https://revonsunpeu.net/, with or without
> > TLSFlexibleContextFactory.
> > 
> > (scrapy1.1rc3p3)HeyHeyHey:scrapy_googleindex thatsme $ scrapy version -v
> > Scrapy    : 1.1.0rc3
> > lxml      : 3.6.0.0
> > libxml2   : 2.9.0
> > Twisted   : 16.0.0
> > Python    : 3.4.1 (v3.4.1:c0e311e010fc, May 18 2014, 00:54:21) - [GCC 4.2.1 (Apple Inc. build 5666) (dot 3)]
> > pyOpenSSL : 16.0.0 (OpenSSL 0.9.8zg 14 July 2015)
> > Platform  : Darwin-14.5.0-x86_64-i386-64bit
> > 
> > (scrapy1.1rc3p3)HeyHeyHey:scrapy_googleindex thatsme$ scrapy shell https://revonsunpeu.net
> > 2016-03-27 03:44:24 [scrapy] INFO: Scrapy 1.1.0rc3 started (bot: scrapy_googleindex)
> > 2016-03-27 03:44:24 [scrapy] INFO: Overridden settings: {'LOGSTATS_INTERVAL': 0, 'BOT_NAME': 'scrapy_googleindex', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'USER_AGENT': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10; rv:44.0) Gecko/20100101 Firefox/44.0', 'DOWNLOAD_DELAY': 0.5, 'NEWSPIDER_MODULE': 'scrapy_googleindex.spiders', 'SPIDER_MODULES': ['scrapy_googleindex.spiders']}
> > 2016-03-27 03:44:24 [scrapy] INFO: Enabled extensions:
> > ['scrapy.extensions.corestats.CoreStats']
> > 2016-03-27 03:44:24 [scrapy] INFO: Enabled downloader middlewares:
> > ['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
> >  'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
> >  'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
> >  'scrapy.downloadermiddlewares.retry.RetryMiddleware',
> >  'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
> >  'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
> >  'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
> >  'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
> >  'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
> >  'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
> >  'scrapy.downloadermiddlewares.stats.DownloaderStats']
> > 2016-03-27 03:44:24 [scrapy] INFO: Enabled spider middlewares:
> > ['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
> >  'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
> >  'scrapy.spidermiddlewares.referer.RefererMiddleware',
> >  'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
> >  'scrapy.spidermiddlewares.depth.DepthMiddleware']
> > 2016-03-27 03:44:24 [scrapy] INFO: Enabled item pipelines:
> > []
> > 2016-03-27 03:44:24 [scrapy] INFO: Spider opened
> > 2016-03-27 03:44:24 [scrapy] DEBUG: Retrying <GET https://revonsunpeu.net> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'SSL23_GET_SERVER_HELLO', 'sslv3 alert handshake failure')]>]
> > 2016-03-27 03:44:25 [scrapy] DEBUG: Retrying <GET https://revonsunpeu.net> (failed 2 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'SSL23_GET_SERVER_HELLO', 'sslv3 alert handshake failure')]>]
> > 2016-03-27 03:44:26 [scrapy] DEBUG: Gave up retrying <GET https://revonsunpeu.net> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'SSL23_GET_SERVER_HELLO', 'sslv3 alert handshake failure')]>]
> > Traceback (most recent call last):
> >   File ""/Users/thatsme/.virtualenvs/scrapy1.1rc3p3/bin/scrapy"", line 11, in <module>
> >     sys.exit(execute())
> >   File ""/Users/thatsme/.virtualenvs/scrapy1.1rc3p3/lib/python3.4/site-packages/scrapy/cmdline.py"", line 142, in execute
> >     _run_print_help(parser, _run_command, cmd, args, opts)
> >   File ""/Users/thatsme/.virtualenvs/scrapy1.1rc3p3/lib/python3.4/site-packages/scrapy/cmdline.py"", line 88, in _run_print_help
> >     func(_a, *_kw)
> >   File ""/Users/thatsme/.virtualenvs/scrapy1.1rc3p3/lib/python3.4/site-packages/scrapy/cmdline.py"", line 149, in _run_command
> >     cmd.run(args, opts)
> >   File ""/Users/thatsme/.virtualenvs/scrapy1.1rc3p3/lib/python3.4/site-packages/scrapy/commands/shell.py"", line 71, in run
> >     shell.start(url=url)
> >   File ""/Users/thatsme/.virtualenvs/scrapy1.1rc3p3/lib/python3.4/site-packages/scrapy/shell.py"", line 47, in start
> >     self.fetch(url, spider)
> >   File ""/Users/thatsme/.virtualenvs/scrapy1.1rc3p3/lib/python3.4/site-packages/scrapy/shell.py"", line 112, in fetch
> >     reactor, self._schedule, request, spider)
> >   File ""/Users/thatsme/.virtualenvs/scrapy1.1rc3p3/lib/python3.4/site-packages/twisted/internet/threads.py"", line 122, in blockingCallFromThread
> >     result.raiseException()
> >   File ""/Users/thatsme/.virtualenvs/scrapy1.1rc3p3/lib/python3.4/site-packages/twisted/python/failure.py"", line 368, in raiseException
> >     raise self.value.with_traceback(self.tb)
> > twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'SSL23_GET_SERVER_HELLO', 'sslv3 alert handshake failure')]>]
> > 
> > —
> > You are receiving this because you were mentioned.
> > Reply to this email directly or view it on GitHub
> > https://github.com/scrapy/scrapy/issues/1764#issuecomment-201971036
",redapple,kmike
1764,2016-03-28 19:42:38,"@redapple @kmike I just updated to the latest version of osx and the result is identical.
Also, when, I check openssl version, the result is:


",natoinet,kmike
1762,2016-02-05 15:08:12,"@dangra @kmike do you think it's worth to add a fallback for these imports with a DeprecationWarning or should we just document this in the release notes (since it isn't part of the core API)?
",eliasdorneles,dangra
1762,2016-02-05 15:08:12,"@dangra @kmike do you think it's worth to add a fallback for these imports with a DeprecationWarning or should we just document this in the release notes (since it isn't part of the core API)?
",eliasdorneles,kmike
1760,2016-02-05 02:47:30,"This is caused by changes here: https://github.com/scrapy/scrapy/pull/1423. With `defer.fail()` traceback points to Scrapy code, e.g.



if the code is changed to an initial @jdemaeyer's workaround



traceback makes more sense:


",kmike,jdemaeyer
1760,2016-02-05 13:26:33,"> if the code is changed to an initial @jdemaeyer's workaround
> 
> 
> 
> traceback makes more sense:

iirc the problem with that was that it didn't preserve the traceback at all in Python 2?
",jdemaeyer,jdemaeyer
1755,2016-02-03 21:25:02,"Hey, @eliasdorneles and me went through the release notes and did some editing.

Does it look good?
",stummjr,eliasdorneles
1746,2016-01-29 18:01:14,"@kmike , would that work for you for #1732 ?
",redapple,kmike
1742,2016-01-28 21:47:41,"Hey, @eliasdorneles ! I updated the methods names, as suggested.
",stummjr,eliasdorneles
1740,2016-01-27 20:00:25,"as per @nramirezuy's suggestion (https://github.com/scrapy/scrapy/pull/1730#issuecomment-175803818)
",kmike,nramirezuy
1731,2016-01-27 10:23:53,"Fixes https://github.com/scrapy/scrapy/issues/1309

This is a rebased version of https://github.com/scrapy/scrapy/pull/1583. 

Like in @akhillb's PR, default value is False, i.e. logs are switched to long names by default. Is that OK?

TODO:
- [x] tests
- [x] confirm that's OK to use long names by default
- [x] update output of examples in docs
- [x] add example of overriding log level for a specific Scrapy component
",kmike,akhillb
1731,2016-12-09 14:45:03,"To enable or disable it by default is a controversial question which needs a discussion :) In https://github.com/scrapy/scrapy/issues/1309 @curita seems to be +1 on long names, @nramirezuy seems to be -1. 

I like long names because they make debugging easier, and they makes scrapy components more discoverable: if you see e.g. a ""request is dropped"" message you can see that it is caused by `[scrapy.dupefilters]`; if it is dropped by an offsite middleware you get a log message from `[scrapy.spidermiddlewares.offsite`]; if a request is dropped with ""Ignoring response %(response)r: HTTP status code is not handled or not allowed"" message it will point you right to `[scrapy.spidermiddlewares.httperror`]. They are more verbose, but they provide additional debugging information, that's why I think they are useful. 

A related advantage is that it is easier to understand how to customize scrapy logging - there is a direct mapping between logger name and text shown in `[brackets]` - if you find some of the messages too verbose (e.g. https://github.com/scrapy/scrapy/issues/1308) you can change log level just for this logger, and this is easy to do because you can see the logger name right away. You can do the same if option is off by default by turning it on and re-running the crawl, but it means one extra crawl which is not always feasible.

https://github.com/scrapy/scrapy/pull/2434 may help to alleviate the pain for all us who got used to `[scrapy]`: maybe we can highlight logger name using a different color, or use a different color strength for it.",kmike,nramirezuy
1731,2016-12-09 14:45:03,"To enable or disable it by default is a controversial question which needs a discussion :) In https://github.com/scrapy/scrapy/issues/1309 @curita seems to be +1 on long names, @nramirezuy seems to be -1. 

I like long names because they make debugging easier, and they makes scrapy components more discoverable: if you see e.g. a ""request is dropped"" message you can see that it is caused by `[scrapy.dupefilters]`; if it is dropped by an offsite middleware you get a log message from `[scrapy.spidermiddlewares.offsite`]; if a request is dropped with ""Ignoring response %(response)r: HTTP status code is not handled or not allowed"" message it will point you right to `[scrapy.spidermiddlewares.httperror`]. They are more verbose, but they provide additional debugging information, that's why I think they are useful. 

A related advantage is that it is easier to understand how to customize scrapy logging - there is a direct mapping between logger name and text shown in `[brackets]` - if you find some of the messages too verbose (e.g. https://github.com/scrapy/scrapy/issues/1308) you can change log level just for this logger, and this is easy to do because you can see the logger name right away. You can do the same if option is off by default by turning it on and re-running the crawl, but it means one extra crawl which is not always feasible.

https://github.com/scrapy/scrapy/pull/2434 may help to alleviate the pain for all us who got used to `[scrapy]`: maybe we can highlight logger name using a different color, or use a different color strength for it.",kmike,curita
1731,2016-12-16 17:16:39,@redapple what do you think now? //cc @eliasdorneles @dangra @nyov @Digenis @curita and everyone else,kmike,Digenis
1731,2016-12-16 17:16:39,@redapple what do you think now? //cc @eliasdorneles @dangra @nyov @Digenis @curita and everyone else,kmike,dangra
1731,2016-12-16 17:16:39,@redapple what do you think now? //cc @eliasdorneles @dangra @nyov @Digenis @curita and everyone else,kmike,eliasdorneles
1731,2016-12-16 17:16:39,@redapple what do you think now? //cc @eliasdorneles @dangra @nyov @Digenis @curita and everyone else,kmike,nyov
1731,2016-12-16 17:16:39,@redapple what do you think now? //cc @eliasdorneles @dangra @nyov @Digenis @curita and everyone else,kmike,curita
1731,2016-12-19 13:56:01,"Alright, so we need another ""official"" +1. @eliasdorneles , @dangra or @curita perhaps?",redapple,curita
1731,2016-12-19 13:56:01,"Alright, so we need another ""official"" +1. @eliasdorneles , @dangra or @curita perhaps?",redapple,dangra
1722,2016-01-26 14:30:54,"@kmike , @eliasdorneles , sorry, it seems I pushed the wrong version:
with this patch in master, you currently get the short names:



instead of `'scrapy.extensions.corestats.CoreStats'` and the likes

I will submit a new PR on top of this
",redapple,kmike
1712,2016-02-24 23:02:15,"@lopuhin , do you know what minimal `botocore` version is needed for scrapy?
I added a `trusty` env here and I'm getting on Travis



`trusty`is using quite [an old `botocore` (0.29)](http://packages.ubuntu.com/fr/trusty/python-botocore)
",redapple,lopuhin
1710,2016-01-22 12:24:06,"@alecxe , @kmike , I added some tests for various cases,
but I'm not sure current implementation from https://github.com/scrapy/scrapy/pull/1579 covers your usecases.
For example, it doesn't test if the argument is a file, just guesses it.
Toughts?

(sorry @nyov for wrong mention)
",redapple,nyov
1710,2016-01-22 12:24:06,"@alecxe , @kmike , I added some tests for various cases,
but I'm not sure current implementation from https://github.com/scrapy/scrapy/pull/1579 covers your usecases.
For example, it doesn't test if the argument is a file, just guesses it.
Toughts?

(sorry @nyov for wrong mention)
",redapple,kmike
1710,2016-01-26 09:48:44,"@alecxe , @nramirezuy , @kmike ,
are you ok with the updated docs?
",redapple,nramirezuy
1705,2016-01-21 18:15:46,"@dangra I set this to discuss because I'm not sure if there is any valid reason to force 'JPEG'.
",nramirezuy,dangra
1701,2016-02-23 13:38:31,"In fact, this does not work with hostname as bytes.
I just got the error while testing scrapy 1.1 with Crawlera, the string ending up as `b""CONNECT b'httpbin.org':443 HTTP/1.1\r\n""`
I'll submit a PR for this.

@kmike , @eliasdorneles , @lopuhin , which has your preference?

the original 



or a fix on bytes param with


",redapple,eliasdorneles
1701,2016-02-23 13:38:31,"In fact, this does not work with hostname as bytes.
I just got the error while testing scrapy 1.1 with Crawlera, the string ending up as `b""CONNECT b'httpbin.org':443 HTTP/1.1\r\n""`
I'll submit a PR for this.

@kmike , @eliasdorneles , @lopuhin , which has your preference?

the original 



or a fix on bytes param with


",redapple,kmike
1689,2016-01-20 19:08:39,"I've already +1 -- @redapple wanna do the honors here? :)
",eliasdorneles,redapple
1686,2016-02-02 17:22:36,"good point @Digenis 
",redapple,Digenis
1676,2016-01-16 09:06:40,"Hey @lopuhin, nice to see you here! 

> scrapy.http.Request has unicode attributes (url, method, etc),

I don't 100% recall why, but after lengthy discussions with @dangra we settled on URLs as 'native strings' - i.e. bytes in Python 2 and unicode in Python 3. Likely the two main reasons were (1) backwards compatibility and (2) spotty/incorrect support for unicode URLs in Python 2.x stdlib. 

> response from twisted (body) is in unicode

hm, that's surprising - what if response is not text?
",kmike,dangra
1664,2016-01-07 17:46:45,"This PR assumes non-ASCII characters were percent-encoded using a known encoding to be passed to `canonicalize_url`, if it was not UTF-8 that is
and normalized URL uses that same encoding before percent-encoding at output.

I would have liked to follow recommended UTF-8 encoding before percent-encoding as output in all cases, but I was unable to found an implementation that passed such tests with Python 2 (Python2 `unquote` or `urlencode` don't have encoding parameters)

@kmike , what do you think?
",redapple,kmike
1661,2016-01-12 10:12:07,"@kmike , I haven't included https://github.com/scrapy/scrapy/pull/1671 in this but I could merge the 2 PRs
",redapple,kmike
1645,2016-01-06 02:11:24,"Hey, 

I think adding a copy of http://contributor-covenant.org/version/1/3/0/code_of_conduct.txt to Scrapy repo and keeping to follow it is a no-brainer. @pablohoffman @shaneaevans @dangra @eliasdorneles @curita @nramirezuy what do you think?
",kmike,dangra
1645,2016-01-06 02:11:24,"Hey, 

I think adding a copy of http://contributor-covenant.org/version/1/3/0/code_of_conduct.txt to Scrapy repo and keeping to follow it is a no-brainer. @pablohoffman @shaneaevans @dangra @eliasdorneles @curita @nramirezuy what do you think?
",kmike,nramirezuy
1645,2016-01-06 02:11:24,"Hey, 

I think adding a copy of http://contributor-covenant.org/version/1/3/0/code_of_conduct.txt to Scrapy repo and keeping to follow it is a no-brainer. @pablohoffman @shaneaevans @dangra @eliasdorneles @curita @nramirezuy what do you think?
",kmike,shaneaevans
1645,2016-01-06 02:11:24,"Hey, 

I think adding a copy of http://contributor-covenant.org/version/1/3/0/code_of_conduct.txt to Scrapy repo and keeping to follow it is a no-brainer. @pablohoffman @shaneaevans @dangra @eliasdorneles @curita @nramirezuy what do you think?
",kmike,eliasdorneles
1645,2016-01-06 02:11:24,"Hey, 

I think adding a copy of http://contributor-covenant.org/version/1/3/0/code_of_conduct.txt to Scrapy repo and keeping to follow it is a no-brainer. @pablohoffman @shaneaevans @dangra @eliasdorneles @curita @nramirezuy what do you think?
",kmike,curita
1645,2016-01-06 02:11:24,"Hey, 

I think adding a copy of http://contributor-covenant.org/version/1/3/0/code_of_conduct.txt to Scrapy repo and keeping to follow it is a no-brainer. @pablohoffman @shaneaevans @dangra @eliasdorneles @curita @nramirezuy what do you think?
",kmike,pablohoffman
1631,2015-12-09 20:30:33,"@redapple's response was true for older logging, but I can't confirm right now if is still active after the migration (should be because is still there).

@sieira Now if you want to contribute to Scrapy. I would follow something like [this](https://github.com/scrapy/scrapy/blob/master/scrapy/dupefilters.py#L62). Where you have a setting to allow `DEBUG` mode, I know is not backwards compatible, but it follows some sort of ""standardization"".

Another thing I would wait for @kmike or @curita on the matter before start re factoring the PR, they might have something to comment :smile:
",nramirezuy,kmike
1631,2015-12-09 20:30:33,"@redapple's response was true for older logging, but I can't confirm right now if is still active after the migration (should be because is still there).

@sieira Now if you want to contribute to Scrapy. I would follow something like [this](https://github.com/scrapy/scrapy/blob/master/scrapy/dupefilters.py#L62). Where you have a setting to allow `DEBUG` mode, I know is not backwards compatible, but it follows some sort of ""standardization"".

Another thing I would wait for @kmike or @curita on the matter before start re factoring the PR, they might have something to comment :smile:
",nramirezuy,curita
1618,2015-11-26 17:28:52,"In private chat with @dangra we agreed to move the heartbeat field to `Slot` class and stopping heartbeats in the same place where `nextcall` is cancelled.
",sibiryakov,dangra
1618,2015-11-26 17:35:15," @kmike anything to comment ? 
",dangra,kmike
1618,2015-11-30 17:06:12,"Guys, @dangra, @nramirezuy, @kmike any more thoughts on this?
",sibiryakov,kmike
1614,2016-10-27 20:26:56,"I talked about this with @moisesguimaraes, he did some tests locally to see what browsers do and checked some RFC, it seems that it makes sense to strip spaces from beginning and end of the URL.

@moisesguimaraes can you add some more detail? Thank you!
",eliasdorneles,moisesguimaraes
1612,2015-11-23 10:00:24,"Looks like a bug, although support for setting `LOG_LEVEL` on the spider _might_ be dropped in the future (#1580)

@curita Probably because we're using `settings` instead of `self.settings` [in this line](https://github.com/scrapy/scrapy/blob/master/scrapy/crawler.py#L38)?
",jdemaeyer,curita
1610,2015-11-21 05:15:52,"Kindly review @curita, @nramirezuy. 
The `codecov` is complaining because I need to write tests yet, correct ?
Where should they go ? (I am thinking about `test.util_reqser.py`)
",darshanime,nramirezuy
1610,2015-11-21 05:15:52,"Kindly review @curita, @nramirezuy. 
The `codecov` is complaining because I need to write tests yet, correct ?
Where should they go ? (I am thinking about `test.util_reqser.py`)
",darshanime,curita
1610,2016-02-08 06:34:40,"@nramirezuy, can you please review the PR? Thanks. 
",darshanime,nramirezuy
1610,2016-07-25 13:01:59,"@redapple kindly review! 
Travis builds, codecov wants tests:/
",darshanime,redapple
1610,2016-07-29 10:39:01,"Fine by me. @kmike , @eliasdorneles , @dangra , what do you think?
",redapple,eliasdorneles
1610,2016-07-29 10:39:01,"Fine by me. @kmike , @eliasdorneles , @dangra , what do you think?
",redapple,dangra
1610,2016-07-29 10:39:01,"Fine by me. @kmike , @eliasdorneles , @dangra , what do you think?
",redapple,kmike
1598,2015-11-13 18:09:07,"#1083 again, implementing @nramirezuy's suggestion instead.
",Digenis,nramirezuy
1597,2015-11-19 09:55:07,"I changed it according to @redapple's suggestion.
Is there anything else left for this PR?
",Digenis,redapple
1597,2015-11-23 08:33:54,"@redapple what do you think about the new implementation?
",kmike,redapple
1596,2015-12-04 12:57:31,"Fixed by #1597 - thanks @Digenis!
",kmike,Digenis
1595,2015-12-04 12:56:52,"Fixed by #1597 - thanks @Digenis!
",kmike,Digenis
1586,2015-11-06 15:17:04,"This is a partial reversal of #1149 that provides per-key priorities in a backwards compatible fashion.

The backwards incompatible API change in #1149 was that when writing to dictionary settings, their contents were updated, not completely overwritten. This PR brings back the old behaviour of completely discarding the previous contents of the dictionary. If the dictionary setting written to is a `BaseSettings` instance, it is not overwritten but cleared and then updated (resulting in the same contents, but conserving per-key priorities).

The price for this is:
- we cannot get rid of the `_BASE` settings
- it will again be much harder to disable single components from the command line or from a spider's `custom_settings` (because disabling a single component requires writing down the complete dictionary without that component). @kmike had the idea that we could add some helper magic for updating (not overwriting) dictionaries, e.g. allow people to do `scrapy ... -s update:EXTENSIONS={""blah"": null}`, or `custom_settings: { ""update:EXTENSIONS"": {""blah"": None}}`. I'll implement that in a follow-up PR to discuss it and see if it makes unforeseen trouble.

As a bonus, I added backwards compatibility to the `build_component_list()` utility function, albeit it not being public. @rolando maybe you could check whether it is indeed backwards compatible with how you use it? Also take a look at the now-public [`BaseSettings.getcomposite()` method](https://github.com/jdemaeyer/scrapy/blob/fix/backwards-compatible-per-key-priorities/scrapy/settings/__init__.py#L197). It helps in separating a utility function that's meant to convert a dictionary into a sorted list from how that dictionary is created.
",jdemaeyer,rolando
1586,2015-11-06 15:17:04,"This is a partial reversal of #1149 that provides per-key priorities in a backwards compatible fashion.

The backwards incompatible API change in #1149 was that when writing to dictionary settings, their contents were updated, not completely overwritten. This PR brings back the old behaviour of completely discarding the previous contents of the dictionary. If the dictionary setting written to is a `BaseSettings` instance, it is not overwritten but cleared and then updated (resulting in the same contents, but conserving per-key priorities).

The price for this is:
- we cannot get rid of the `_BASE` settings
- it will again be much harder to disable single components from the command line or from a spider's `custom_settings` (because disabling a single component requires writing down the complete dictionary without that component). @kmike had the idea that we could add some helper magic for updating (not overwriting) dictionaries, e.g. allow people to do `scrapy ... -s update:EXTENSIONS={""blah"": null}`, or `custom_settings: { ""update:EXTENSIONS"": {""blah"": None}}`. I'll implement that in a follow-up PR to discuss it and see if it makes unforeseen trouble.

As a bonus, I added backwards compatibility to the `build_component_list()` utility function, albeit it not being public. @rolando maybe you could check whether it is indeed backwards compatible with how you use it? Also take a look at the now-public [`BaseSettings.getcomposite()` method](https://github.com/jdemaeyer/scrapy/blob/fix/backwards-compatible-per-key-priorities/scrapy/settings/__init__.py#L197). It helps in separating a utility function that's meant to convert a dictionary into a sorted list from how that dictionary is created.
",jdemaeyer,kmike
1586,2015-11-10 22:05:05,"+1 to merge. //cc @curita @dangra
",kmike,curita
1586,2015-11-10 22:05:05,"+1 to merge. //cc @curita @dangra
",kmike,dangra
1582,2015-11-09 14:45:03,"@Digenis a good suggestion, +1
",kmike,Digenis
1582,2016-09-09 20:30:06,"The PR looks good; I think it misses a rebase, and probably it makes sense to apply @Digenis's suggestion.
",kmike,Digenis
1582,2016-09-09 22:40:17,"Thank you @kmike and @Digenis :)
Please have a look at this updated PR.
",starrify,Digenis
1581,2016-09-22 14:36:46,"@redapple @kmike do you think if we document the backward incompatibility, we could get this in 1.2 as well?
",eliasdorneles,redapple
1580,2015-11-02 15:35:42,"This is an implementation of @chekunkov's suggested changes from #1305. I will be uploading consecutive commits one-by-one in the next minutes so we can see which tests break at what point.
",jdemaeyer,chekunkov
1556,2015-10-28 16:37:34,"//cc @rolando (he's working on an official conda package for Scrapy)
",kmike,rolando
1529,2015-10-06 13:43:58,"sounds good!. Make sure we release to conda as well @rolando 
",pablohoffman,rolando
1524,2015-10-05 11:12:59,"Thanks @Digenis! 

We need to think about backwards compatiility: say, people disabled Telnet extension using EXTENSIONS option (as shown in the [example](https://github.com/scrapy/scrapy/blob/master/scrapy/templates/project/module/settings.py.tmpl#L58) in generated settings.py) - it shouldn't become enabled after the relocation. It is better to have a test for it. Adding it [here](https://github.com/scrapy/scrapy/blob/91cbf974150309e8b0401e7d0f5b0c4757bf260f/scrapy/utils/deprecate.py#L126) may be enough, but I'm not sure. //cc @curita  
",kmike,curita
1514,2015-09-29 07:37:52,"I think I've seen somewhere in this bug tracker
@pablohoffman mentioning that there is a long term plan
to document more internal components like modules from `scrapy.utils`.
If this is true then such PRs are probably welcome
but not meant to be merged into master
before the missing documentation your link points to, is written.
",Digenis,pablohoffman
1499,2016-01-21 23:56:41,"So, today I got together with @stummjr for a day sprint wrapping up the work for this port.

We followed the advice from @kmike on the above comments and in #1080 and we were finally able to run scrapy with Python 3 exporting items to JSON, XML and CSV -- so nice to see this working! =)

We weren't sure of some of the decisions, looking forward to your comments.

Thank you!
",eliasdorneles,stummjr
1488,2015-09-13 15:21:33,"LGTM :+1: 
I'm leaving it for @dangra to merge.
",kmike,dangra
1482,2015-10-15 04:22:05,"@curita Actually, I have read this post before and I have a version of python installed additionally. Right now I removed a line of code in the ~/.bash_profile:  PATH=""/Library/Frameworks/Python.framework/Versions/3.5/bin:${PATH}""export PATH

and try to run the python script again. Now the error shifted to : 
ImportError                               Traceback (most recent call last)
<ipython-input-4-934a058a3d8d> in <module>()
      1 from twisted.internet import reactor
----> 2 from scrapy.crawler import Crawler
      3 from scrapy.settings import Settings
      4 from scrapy import log
      5 from testspiders.spiders.followall import FollowAllSpider

/Users/lcc/.virtualenvs/virtual/lib/python2.7/site-packages/scrapy/**init**.py in <module>()
     25 
     26 # Apply monkey patches to fix issues in external libraries
---> 27 from . import _monkeypatches
     28 del _monkeypatches
     29 

ImportError: cannot import name _monkeypatches

It seems that my bash-profile cause the issue. All of the code of bash-profile is as below:

export PATH=/usr/local/bin:/usr/local/sbin:$PATH

export PATH=$HOME/anaconda/bin:$PATH

export PATH=/usr/local/bin:$PATH

export JAVA_HOME=`/usr/libexec/java_home -v 1.8`

export WORKON_HOME=$HOME/.virtualenvs
export PROJECT_HOME=$HOME/Devel
source /usr/local/bin/virtualenvwrapper.sh
",rylanchiu,curita
1481,2015-09-14 19:01:39,"@kmike @dangra are we missing tests for the code changed?
",nramirezuy,dangra
1481,2015-09-14 19:01:39,"@kmike @dangra are we missing tests for the code changed?
",nramirezuy,kmike
1467,2015-09-04 16:53:11,"@nramirezuy you are the all-mighty ItemLoaders reviewer and evangelist, what do you think? 
",dangra,nramirezuy
1466,2015-08-29 17:48:23,"Fixes #669 

Solution provided by @dangra https://github.com/scrapy/scrapy/pull/670#issuecomment-38921769

I can add tests but should I add this dependency for FTP tests? https://github.com/giampaolo/pyftpdlib 

Current FTP tests uses Twisted so I'm afraid they won't work for PY3 https://github.com/scrapy/scrapy/blob/master/tests/test_downloader_handlers.py#L523-L543
",umrashrf,dangra
1466,2015-09-02 04:43:55,"thanks for reviewing @dangra, I'll do the changes soon.
",umrashrf,dangra
1466,2016-10-18 10:56:17,"@umrashrf , @nyov , @dangra , @eliasdorneles 
here are my suggestions: https://github.com/umrashrf/scrapy/pull/1
",redapple,eliasdorneles
1443,2015-08-20 08:08:02,"Another take on #740

@dangra,
I require some clarification here: Is a `Crawler`<>`Spider` relationship always 1:1 nowadays? Or does a Crawler object ever handle multiple Spiders, in parallel?

`crawler.signals.connect(self.idle, signals.spider_idle)` inside the `Spider` class connects the Spider/s to the SignalManager for the Crawler instance.
So we shouldn't get any signals from any other Crawler instance, correct?
Now in the past, there has been code checking in a spider instance like this:`def idle(self): if spider != self: return`.

If I am correct, a Crawler will always only have one Spider, so `spider` will always only be `self` here now, right? So this can be a staticmethod.
",nyov,dangra
1439,2015-08-27 17:08:35,"Understood, perfect. About the renaming.. I like the names as they are now because they are similar to what we're used to, regarding debian packages in general. 

What do you think, @agusc ?
",cyberplant,agusc
1434,2015-09-01 08:11:26,"@dangra i meet the same problem when i use proxy and crawl https site 
",ianzhang001,dangra
1431,2015-08-12 17:25:03,"/cc @eliasdorneles 
",dangra,eliasdorneles
1423,2015-10-26 15:15:15,"Thanks for the feedback everybody, and apologies for the huge delay :(

I've implemented @nramirezuy's solution to keep the traceback and got rid of the mock in the test (thanks @kmike)
",jdemaeyer,nramirezuy
1423,2015-10-26 15:15:15,"Thanks for the feedback everybody, and apologies for the huge delay :(

I've implemented @nramirezuy's solution to keep the traceback and got rid of the mock in the test (thanks @kmike)
",jdemaeyer,kmike
1423,2015-10-29 14:10:53,"Thanks @jdemaeyer, @curita and @nramirezuy.
",kmike,nramirezuy
1418,2015-08-10 18:48:00,"Thanks for checking it!

/cc @dangra what about adding service_identity to Scrapy install_requires?
",kmike,dangra
1400,2015-08-05 12:14:01,"Hey @nyov! 
- [x] _split requirements into files_ - looks good;
- [ ] _fix test cmdline_ - changes look good. It seems we don't have tests for `--profile` (see `_run_command_profiled` and `scrapy.xlib.lsprofcalltree`). To port cmdline we should add a test for this option and make sure it works.
- [ ] _renames (six types)_ - just some renames using six library - it looks good, but it seems there is a conflict with @dangra's merged cookies PR;
- [x] _fix test loader_ - LGTM;
- [ ] _fix tests pipelines files_ - Referrer header should be decoded for nicer log messages. There is also a tricky part when headers are passed to boto, but it looks like we're fine here. I also wonder if we should use canonicalize_url to create a hash (as we do in fingerprints), but this is out of scope.
- [x] _fix tests pipelines images_ - looks good, depends on files pipeline;
- [x] _response bodies as bytes_ - looks good.
",kmike,dangra
1398,2015-07-31 20:28:44,"/cc @kmike 
",dangra,kmike
1395,2015-08-20 19:54:12,"+1 to add a function for this to scrapy itself; check @dangra's idea about the implementation: https://github.com/scrapy/scrapy/pull/816#discussion_r16187182.
",kmike,dangra
1393,2015-07-29 20:41:49,"> So I've been through all the remaining testcases now, and I sorted those by what is blocking them (from a quick look).

Awesome!

> `## twisted.mail`

Most of these components don't require twisted.mail to work; I think we should make twisted.main optional for them.

> `## sgml (dropped in py3)`

I think we should split tests into two files and ignore all the deprecated stuff in Python 3. //cc @arijitchakraborty 
",kmike,arijitchakraborty
1392,2016-03-01 14:04:18,"Thanks @redapple. I think I'll close this though.
Unless there is merit in what @nramirezuy said about removing some of them?
Feel free to reopen, in that case.
",nyov,redapple
1389,2015-07-28 20:34:59,"I wouldn't focus the docstring on raising this TypeError - it is just an expected type of an argument, not the main function purpose. What about this?



//cc @dangra @eliasdorneles 
",kmike,dangra
1389,2015-07-28 20:34:59,"I wouldn't focus the docstring on raising this TypeError - it is just an expected type of an argument, not the main function purpose. What about this?



//cc @dangra @eliasdorneles 
",kmike,eliasdorneles
1386,2015-07-25 16:48:25,"@kmike, @dangra 
nice sprint :)
i'm probably already too late with this
",nyov,dangra
1386,2015-07-25 16:48:25,"@kmike, @dangra 
nice sprint :)
i'm probably already too late with this
",nyov,kmike
1386,2015-07-29 08:25:45,"LGTM. I'll leave it to @dangra to merge because he was working on porting cookie code to Python 3.
",kmike,dangra
1384,2015-07-29 15:18:32,"@dangra are we bold enough to merge this PR to master?  I'm in favor of it.
",kmike,dangra
1371,2015-07-18 17:28:29,"Currently downloader [slots](https://github.com/scrapy/scrapy/blob/f93acffff4400da2cc132aa32ef39f127bbd9634/scrapy/core/downloader/__init__.py#L27) use `collections.deque` for requests queue. It means that once request came from a scheduler to downloader, its priority is no longer respected.

Let's say global concurrency limit is 10, scheduler returned 10 requests with a low priority (all for a single downloader slot), then user scheduled a request with a high priority (for the same slot), then one of 10 low-priority requests was processed, and downloader fetched high-priority request from a scheduler. In this case this new high-priority request will be only handled after 9 existing low-priority requests. 

What about using a priority queue from queuelib instead of deque?

//cc @dangra @shirk3y
",kmike,dangra
1350,2015-09-07 20:42:18,"Forgot to push some changes I made a while ago taking some of the suggestions, @kmike could you review them?
",curita,kmike
1319,2015-06-25 18:37:23,"It makes sense, but I wonder why was `-` [added](https://github.com/scrapy/scrapy/commit/ff987fb5a571ba91cb4e8bd0472e94579acbf2a2) in first place. /cc @curita
",kmike,curita
1305,2015-11-05 20:50:40,"> I also think every component should be allowed to change settings

This is what @jdemaeyer's add-on [PR](https://github.com/scrapy/scrapy/pull/1272) is about, to provide a consistent interface for components to change settings of other components.
",kmike,jdemaeyer
1295,2015-07-28 20:44:26,"Me and @dangra were trying to fix `canonicalize_url` for non-ascii data in https://github.com/scrapy/scrapy/pull/1384/, but so far failed :) 

One problem with 'encoding' argument is that Scrapy can't pass it in a dupefilter (which uses canonicalize_url) because response is not available there. Another problem is that in Python 3 urlparse works on unicode strings, not on byte strings. One more problem: `encoding` argument may have several meanings - an encoding to encode the unicode string to, or an encoding to decode from after unescaping of `%`-encoded sequences. Another surprising issue: different parts of URL can use different encodings. 

It is kind of hard to account for all this without real use cases. So I wonder what is your use case? Did you have some problems problems with the existing `canonicalize_url` implementation?
",kmike,dangra
1290,2015-06-08 13:39:07,"It provides an extension point where crawler instance is available; it should make it easier to write alternative CrawlerRunner.crawl implementations.

My use case: listen to signals from each crawler. As as @chekunkov pointed in https://github.com/scrapy/scrapy/pull/1256#issuecomment-105241481, currently it requires copy-pasting ~~large~~ chunks of code. After this change user can override CrawlerRunner._crawl method and connect signals there.

I don't know if we should make this method public & documented or not.
",kmike,chekunkov
1289,2015-06-09 18:03:23,"Yo have my +1, but we need @pablohoffman or @dangra or @shaneaevans to confirm removing HTTP 400 from retry list is OK.
",kmike,shaneaevans
1289,2015-06-09 18:03:23,"Yo have my +1, but we need @pablohoffman or @dangra or @shaneaevans to confirm removing HTTP 400 from retry list is OK.
",kmike,dangra
1289,2015-06-09 18:03:23,"Yo have my +1, but we need @pablohoffman or @dangra or @shaneaevans to confirm removing HTTP 400 from retry list is OK.
",kmike,pablohoffman
1283,2015-06-05 20:34:55,"//cc @Curita @dangra 
",kmike,dangra
1276,2015-06-10 10:50:44,"@dangra please check comment above, maybe you can explain what design decision is behind current initialization order? I forgot to mention you in the comment, and you're obviously the person who knows almost everything about Scrapy.
",chekunkov,dangra
1267,2015-06-01 15:31:04,"Hate to see a backward incompatible change out of all the module relocations, but it surely makes sense to raise an error if we can't guarantee the same behavior from last stable release, specially if we could have avoided an actual related issue. Mixing paths in the same setting dict is still something that should be uncommon, so users shouldn't get this error too often.

I have updated the pr, what do you think? /cc @jdemaeyer @nramirezuy @dangra
",curita,dangra
1265,2015-05-29 14:14:45,"Reported by @dangra

This issue manifests when mixing old paths and new ones for extensions and middlewares (this can happen for example while using a newer version of Scrapy in a project that hasn't updated to the new paths yet). Since paths aren't normalized, the same component can be loaded twice.

Take these settings for example:





While merging both dictionaries to build the list of components, the same StackTraceDump class is going to be loaded twice since it appears in two different keys. 
",curita,dangra
1263,2015-05-29 10:50:08,"Inspired by today's mail from @dangra.

Before:



After: 



A bit easier to read and detect problems IMO
",chekunkov,dangra
1255,2015-05-26 14:16:51,"Hey @stphivos!

-1 from me to add this to Scrapy, BUT I like the syntax and the general idea.

I'd love to see Scrapy supporting other ways to extract data like yours, pyquery, BeautifulSoup, cssselect2, as well as other backends like html5lib (see https://github.com/scrapy/scrapy/pull/1043) or gumbo-parser in addition to lxml.

Currently we're moving builtin Scrapy selectors to a separate library which can be used without Scrapy: https://github.com/scrapy/scrapy-selectors. Scrapy still will be providing shortcuts like response.xpath(..) or response.css(..), but the actual work will be done by this separate parsing library.

I'm keen to an idea of providing more (optional) shortcuts, like WebTest [does](http://webtest.pythonpaste.org/en/latest/api.html#webtest-response-testresponse): there are response.html, response.xml, response.json, response.pyquery, response.lxml attributes which provide entry points for various parsing libraries: BS, ElementTree, json, pyquery and lxml. We should think about the names, but it looks solvable.

So I think that what we need is to either refactor scrapy-selectors to allow extensions from the user code (/cc @redapple), or provide a way for users to extend Scrapy Response with more shortcuts, or do both. 

If a query language uses LXML XPaths then extending Selectors looks better because it allows to use several query languages in a single chained query - like we allow to chain CSS and XPath selectors. If the query library is not based on lxml then providing a separate method sounds fine to me.

Either way, your library could work better as a separate project in a separate repo. I'm fine with putting it under scrapy github organization if you want (/cc @dangra @pablohoffman), or creating an organization with a curated set of user-contributed projects like this (of course, if you want it there). Until scrapy-selectors or Scrapy is refactored, it'd be possible to use this library without shortcuts:



Proposals on how to refactor Scrapy or Selectors are very welcome! Someone needs to get the ball rolling.

---

Regarding syntax: why not do the following?


",kmike,pablohoffman
1255,2015-05-26 14:16:51,"Hey @stphivos!

-1 from me to add this to Scrapy, BUT I like the syntax and the general idea.

I'd love to see Scrapy supporting other ways to extract data like yours, pyquery, BeautifulSoup, cssselect2, as well as other backends like html5lib (see https://github.com/scrapy/scrapy/pull/1043) or gumbo-parser in addition to lxml.

Currently we're moving builtin Scrapy selectors to a separate library which can be used without Scrapy: https://github.com/scrapy/scrapy-selectors. Scrapy still will be providing shortcuts like response.xpath(..) or response.css(..), but the actual work will be done by this separate parsing library.

I'm keen to an idea of providing more (optional) shortcuts, like WebTest [does](http://webtest.pythonpaste.org/en/latest/api.html#webtest-response-testresponse): there are response.html, response.xml, response.json, response.pyquery, response.lxml attributes which provide entry points for various parsing libraries: BS, ElementTree, json, pyquery and lxml. We should think about the names, but it looks solvable.

So I think that what we need is to either refactor scrapy-selectors to allow extensions from the user code (/cc @redapple), or provide a way for users to extend Scrapy Response with more shortcuts, or do both. 

If a query language uses LXML XPaths then extending Selectors looks better because it allows to use several query languages in a single chained query - like we allow to chain CSS and XPath selectors. If the query library is not based on lxml then providing a separate method sounds fine to me.

Either way, your library could work better as a separate project in a separate repo. I'm fine with putting it under scrapy github organization if you want (/cc @dangra @pablohoffman), or creating an organization with a curated set of user-contributed projects like this (of course, if you want it there). Until scrapy-selectors or Scrapy is refactored, it'd be possible to use this library without shortcuts:



Proposals on how to refactor Scrapy or Selectors are very welcome! Someone needs to get the ball rolling.

---

Regarding syntax: why not do the following?


",kmike,redapple
1255,2015-05-26 14:16:51,"Hey @stphivos!

-1 from me to add this to Scrapy, BUT I like the syntax and the general idea.

I'd love to see Scrapy supporting other ways to extract data like yours, pyquery, BeautifulSoup, cssselect2, as well as other backends like html5lib (see https://github.com/scrapy/scrapy/pull/1043) or gumbo-parser in addition to lxml.

Currently we're moving builtin Scrapy selectors to a separate library which can be used without Scrapy: https://github.com/scrapy/scrapy-selectors. Scrapy still will be providing shortcuts like response.xpath(..) or response.css(..), but the actual work will be done by this separate parsing library.

I'm keen to an idea of providing more (optional) shortcuts, like WebTest [does](http://webtest.pythonpaste.org/en/latest/api.html#webtest-response-testresponse): there are response.html, response.xml, response.json, response.pyquery, response.lxml attributes which provide entry points for various parsing libraries: BS, ElementTree, json, pyquery and lxml. We should think about the names, but it looks solvable.

So I think that what we need is to either refactor scrapy-selectors to allow extensions from the user code (/cc @redapple), or provide a way for users to extend Scrapy Response with more shortcuts, or do both. 

If a query language uses LXML XPaths then extending Selectors looks better because it allows to use several query languages in a single chained query - like we allow to chain CSS and XPath selectors. If the query library is not based on lxml then providing a separate method sounds fine to me.

Either way, your library could work better as a separate project in a separate repo. I'm fine with putting it under scrapy github organization if you want (/cc @dangra @pablohoffman), or creating an organization with a curated set of user-contributed projects like this (of course, if you want it there). Until scrapy-selectors or Scrapy is refactored, it'd be possible to use this library without shortcuts:



Proposals on how to refactor Scrapy or Selectors are very welcome! Someone needs to get the ball rolling.

---

Regarding syntax: why not do the following?


",kmike,dangra
1240,2015-05-18 14:31:30,"`logfmt % ""Stored""` happens before logger formats the message, so `%` vs `%%` needs to be reversed.

This fixes the following error during shutdown:



Export is successful, but error handler is called because there is the same exception in success handler.

To get it try @eliasdorneles's example from here: https://github.com/scrapy/scrapy.github.io/pull/29 (`scrapy runspider example.py -o res.json`)
",kmike,eliasdorneles
1238,2015-05-15 17:45:11,"@preetis19 you're right, this overview should have used `scrapy runspider`. It also had other issues; @eliasdorneles has rewritten the overview, so this is not a problem in scrapy master. The release is not too far away, but I'm fine with merging a PR which fixes an error 0.24 overview.

In readthedocs ""latest"" (somewhat confusingly) means ""latest released version"", so it doesn't correspond to the latest code.
",kmike,eliasdorneles
1228,2015-05-13 18:56:13,"Do we need documentation updates?

anyone want to comment before merging? /cc @Curita @kmike 
",dangra,kmike
1227,2015-05-13 03:10:33,"@nyov aaeb837db409442579d260f703e2f2ca705020ca is firing back.
",dangra,nyov
1225,2015-05-12 19:14:10,"//cc @sibiryakov - this is related to 'canonical URLs' feature you're working on for frontera; how are you solving it there?
",kmike,sibiryakov
1224,2015-05-18 14:38:08,"After a discussion with @Curita, @dangra, @pablohoffman and @eliasdorneles, I've made `[]` unsupported again.
",kmike,dangra
1224,2015-05-18 14:38:08,"After a discussion with @Curita, @dangra, @pablohoffman and @eliasdorneles, I've made `[]` unsupported again.
",kmike,eliasdorneles
1224,2015-05-18 14:38:08,"After a discussion with @Curita, @dangra, @pablohoffman and @eliasdorneles, I've made `[]` unsupported again.
",kmike,pablohoffman
1218,2015-05-14 17:39:03,"ping @kmike and @dangra, I need this pr merged so I can move on with the `base.py` relocations.
",curita,dangra
1218,2015-05-14 17:39:03,"ping @kmike and @dangra, I need this pr merged so I can move on with the `base.py` relocations.
",curita,kmike
1214,2015-08-27 21:40:12,"//cc @dangra @curita @nramirezuy could please somebody check it and merge if it is OK?
",kmike,curita
1214,2015-08-27 21:40:12,"//cc @dangra @curita @nramirezuy could please somebody check it and merge if it is OK?
",kmike,nramirezuy
1214,2015-08-27 21:40:12,"//cc @dangra @curita @nramirezuy could please somebody check it and merge if it is OK?
",kmike,dangra
1212,2015-05-07 05:35:11,"@dangra  I would use .rpartition if there was some assurance about the filename .tmpl extension.
Since there is none, I switched to something equivalent of the previous regex
(and to what I think was intended on the original expression)

What I would do in order of preference is:
either always pass the path to the output file and let the func append the .tmpl extension where needed,
or assert inside the function body that the extension is there.

Should I go for the first one?
",Digenis,dangra
1212,2015-05-07 05:43:29,"> @dangra I would use .rpartition if there was some assurance about the filename .tmpl extension.
> Since there is none, I switched to something equivalent of the previous regex
> (and to what I think was intended on the original expression)
> 
> What I would do in order of preference is:
> either always pass the path to the output file and let the func append the .tmpl extension where needed,
> or assert inside the function body that the extension is there.
> 
> Should I go for the first one?

+1 to keep same behavior. Ignore the rpartition suggestion. 
",dangra,dangra
1185,2015-04-21 19:30:27,"Fixed an issue with https://github.com/scrapy/scrapy/pull/1166 which @dangra spotted.
",kmike,dangra
1176,2015-04-19 18:17:02,"Likely it was fixed by https://github.com/scrapy/scrapy/pull/856, but the fix is not backported to 0.24.x branch. Could you try reproduce it using Scrapy master?

//cc @dangra - what about backporting https://github.com/scrapy/scrapy/pull/856 for 0.24.6?
",kmike,dangra
1169,2015-04-17 22:52:23,"thanks @bagratte!  /cc @eliasdorneles 
",pablohoffman,eliasdorneles
1164,2015-04-15 14:26:10,"/cc @pablohoffman 
",kmike,pablohoffman
1151,2015-04-13 13:33:40,"Thanks for the tests @marven!

As discussed in #994, we should add `scrapy/contrib/downloadermiddleware/httpcache.py` to `tests/py3-ignores.txt` (and probably `tests/test_downloadermiddleware_httpcache.py` too if not already there), that's why the travis build is failing.

 A nitpick, can we rebase @jameysharp commits in this PR instead of merging them, so we can avoid the merge commit ce38129? Merge commits in Scrapy right now are only done when merging PRs to `scrapy:master`.
",curita,jameysharp
1151,2015-04-22 13:02:25,"@kmike, updated the docs based on your comments
",marven,kmike
1149,2015-05-29 13:14:28,"Thank you guys for the feedback!

I now have:
1. Disabled auto-promotion of dicts, only dicts from `default_settings` will now be promoted to `BaseSettings` instances
2. Updated the `SettingsAttribute.priority` behaviour for SettingsAttributes that contain a `BaseSettings` instance. It now always reflects the maximum of:
   - the highest priority in the `BaseSettings` instance
   - the highest priority ever given to `SettingsAttribute.set()` (even when that priority is higher than anything in `BaseSettings`)
3. Updated `BaseSettings.update()` so it can also deal with JSON-encoded strings, this solves [the issue raised by](https://github.com/scrapy/scrapy/pull/1149#discussion_r31134086) @nramirezuy and avoids downgrading `BaseSettings` to `dicts` (losing all per-key priorities) wherever possible
4. ~~Deprecated `scrapy.utils.conf.build_component_list()` since the `(base, custom)` signature is no longer needed. I did not want to reuse the function name though since people might be using it in their extensions.~~
5. ~~As replacement, introduced `scrapy.utils.conf.build_components()` that only takes a _single_ dict, removes all entries containing `None` as value, then builds the list or returns the dict, depending on the parameter `to_list=True`.~~
6. ~~Removed all references to `_BASE` settings (and their merging with the non-`_BASE` setting) in the complete codebase, instead replacing it with calls to  the function `scrapy.utils.conf._get_composite_setting(settings, settingname)`. This function, and the calls to it, should only be in the code until we decide to finally remove all support for the `_BASE` settings. I guess this is not exactly what @Curita had hoped for, since we will still have to replace all calls to `_get_composite_setting()` when we remove `_BASE` support, and replace them with a simple `settings[settingname]`. But the alternative (that I could see) would have been to include `settings` in the signature of `build_components()`, which seemed not very modular.~~
7. Enabled disabling items through setting their dict value to `None` for _all_ default dict settings (including the default request headers) by placing calls to `build_component_list()` and `remove_none_values()` where appropriate.

**To do**:
- ~~Update first post with summary of changes~~
- Tests
  - ~~Call `Settings.update()` with json string~~
  - ~~`build_components()`~~
  - ~~Updating default dicts from the command line~~
- ~~Check PEP8 compliancy~~
",jdemaeyer,nramirezuy
1145,2015-05-04 19:24:10,"Thanks @dangra, updated as per your suggestion.
Also added a test case and a mention in the docs.
",bosnj,dangra
1144,2015-04-10 11:50:01,"I think we haven't created a ticket for this yet, we discussed integrating https://github.com/darkrho/scrapy-inline-requests a couple of weeks ago with @pablohoffman, @kmike and @nramirezuy.

I'm copying the whole discussion here if anyone else wants to join in:

From Pablo:

> I've heard many people using (and speaking good things about) inline requests recently. Would you consider it a feature to include in 1.0?

From Mikhail:

> I'd like to have something like inline-requests builtin; it even was one of the draft ideas for 2014 GSoC :) +1 to add it to Scrapy 1.0 if we have time for that.
> 
> Past me wrote the following in one of the emails:
> 
> > It needs a bit more thought before becoming a part of Scrapy: there are no tests, I'm not fan of how callbacks are handled, and downsides of the 'yield' approach should be clearly documented - e.g. state inside the callback lives longer, and it could lead to increased memory usage; it is also unclear how does it work with on-disk queues. There are also some useful features that present in other alike libraries (e.g. adisp) but not in scrapy-inline-requests - for example, waiting for several requests to be executed in parallel: syntax could be resp1, resp2 = yield Request(url1), Request(url2)

From Nicolás:

> I like the idea behind inline requests, but not the API of it. It kinda doesn't fit the callback approach since it doesn't work with  a callback and you have to manage several requests with in a callback. 
> 
> I would prefer to see something like:
> def callback:
>    return chain_requests(request1, request2, request3)
> 
> and the callbacks handled normally.

From Mikhail:

> Nicolás: I think the point of inline-requests is to allow writing code without callbacks and handle several related requests in a single function :) It is a common trick to ""linearize"" callbacks into a generator.
> 
> Callbacks + CPython reference counting (no PyPy) provide a nice approach to resource deallocation: if a variable is not referenced from outside then it is deallocated as soon as the callback exits, without invoking garbage collector. With generators if user writes ""response1 = yield ...; response2 = yield ..."" then these responses are kept alive, possibly for long. Even with ""response = yield ...; response = yield ..."" response is kept in memory longer than needed (if I'm not mistaken, until the second request finishes). One can write ""del response"", but it'd be nice to have some clever solution for that.

From Julia:

> I wouldn't promote it as the preferred way for dealing with requests/responses because of the already mention issues. It's not as flexible as using explicit callbacks (we should document that yielding a request with `callback` not being `None` breaks the chain btw) and it's hacky, debugging it is kind of hard.
> 
> Still, it's a really good helper for its primary use-case of downloading some additional page and handling errors (as opposed to using errbacks or downloading the page with another library) so I'd also like to include it in Scrapy.

NOTE: I didn't mean that I breaks the chain as in raising an exception, just that it won't wait for yielded requests if they have callbacks.

From Pablo:

> I think we're pretty much in agreement that it would be a nice feature for 1.0 (well, if we're not gonna have python 3 ... :). It needs to go with good documentation (explaining the downsides), tests and better error check (raising exception if it's used with a request having a callback).
> 
> Shall we make a ticket for this?. I think there's already enough content in this thread for one :).

/cc @darkrho 
",curita,kmike
1144,2015-04-10 11:50:01,"I think we haven't created a ticket for this yet, we discussed integrating https://github.com/darkrho/scrapy-inline-requests a couple of weeks ago with @pablohoffman, @kmike and @nramirezuy.

I'm copying the whole discussion here if anyone else wants to join in:

From Pablo:

> I've heard many people using (and speaking good things about) inline requests recently. Would you consider it a feature to include in 1.0?

From Mikhail:

> I'd like to have something like inline-requests builtin; it even was one of the draft ideas for 2014 GSoC :) +1 to add it to Scrapy 1.0 if we have time for that.
> 
> Past me wrote the following in one of the emails:
> 
> > It needs a bit more thought before becoming a part of Scrapy: there are no tests, I'm not fan of how callbacks are handled, and downsides of the 'yield' approach should be clearly documented - e.g. state inside the callback lives longer, and it could lead to increased memory usage; it is also unclear how does it work with on-disk queues. There are also some useful features that present in other alike libraries (e.g. adisp) but not in scrapy-inline-requests - for example, waiting for several requests to be executed in parallel: syntax could be resp1, resp2 = yield Request(url1), Request(url2)

From Nicolás:

> I like the idea behind inline requests, but not the API of it. It kinda doesn't fit the callback approach since it doesn't work with  a callback and you have to manage several requests with in a callback. 
> 
> I would prefer to see something like:
> def callback:
>    return chain_requests(request1, request2, request3)
> 
> and the callbacks handled normally.

From Mikhail:

> Nicolás: I think the point of inline-requests is to allow writing code without callbacks and handle several related requests in a single function :) It is a common trick to ""linearize"" callbacks into a generator.
> 
> Callbacks + CPython reference counting (no PyPy) provide a nice approach to resource deallocation: if a variable is not referenced from outside then it is deallocated as soon as the callback exits, without invoking garbage collector. With generators if user writes ""response1 = yield ...; response2 = yield ..."" then these responses are kept alive, possibly for long. Even with ""response = yield ...; response = yield ..."" response is kept in memory longer than needed (if I'm not mistaken, until the second request finishes). One can write ""del response"", but it'd be nice to have some clever solution for that.

From Julia:

> I wouldn't promote it as the preferred way for dealing with requests/responses because of the already mention issues. It's not as flexible as using explicit callbacks (we should document that yielding a request with `callback` not being `None` breaks the chain btw) and it's hacky, debugging it is kind of hard.
> 
> Still, it's a really good helper for its primary use-case of downloading some additional page and handling errors (as opposed to using errbacks or downloading the page with another library) so I'd also like to include it in Scrapy.

NOTE: I didn't mean that I breaks the chain as in raising an exception, just that it won't wait for yielded requests if they have callbacks.

From Pablo:

> I think we're pretty much in agreement that it would be a nice feature for 1.0 (well, if we're not gonna have python 3 ... :). It needs to go with good documentation (explaining the downsides), tests and better error check (raising exception if it's used with a request having a callback).
> 
> Shall we make a ticket for this?. I think there's already enough content in this thread for one :).

/cc @darkrho 
",curita,nramirezuy
1144,2015-04-10 11:50:01,"I think we haven't created a ticket for this yet, we discussed integrating https://github.com/darkrho/scrapy-inline-requests a couple of weeks ago with @pablohoffman, @kmike and @nramirezuy.

I'm copying the whole discussion here if anyone else wants to join in:

From Pablo:

> I've heard many people using (and speaking good things about) inline requests recently. Would you consider it a feature to include in 1.0?

From Mikhail:

> I'd like to have something like inline-requests builtin; it even was one of the draft ideas for 2014 GSoC :) +1 to add it to Scrapy 1.0 if we have time for that.
> 
> Past me wrote the following in one of the emails:
> 
> > It needs a bit more thought before becoming a part of Scrapy: there are no tests, I'm not fan of how callbacks are handled, and downsides of the 'yield' approach should be clearly documented - e.g. state inside the callback lives longer, and it could lead to increased memory usage; it is also unclear how does it work with on-disk queues. There are also some useful features that present in other alike libraries (e.g. adisp) but not in scrapy-inline-requests - for example, waiting for several requests to be executed in parallel: syntax could be resp1, resp2 = yield Request(url1), Request(url2)

From Nicolás:

> I like the idea behind inline requests, but not the API of it. It kinda doesn't fit the callback approach since it doesn't work with  a callback and you have to manage several requests with in a callback. 
> 
> I would prefer to see something like:
> def callback:
>    return chain_requests(request1, request2, request3)
> 
> and the callbacks handled normally.

From Mikhail:

> Nicolás: I think the point of inline-requests is to allow writing code without callbacks and handle several related requests in a single function :) It is a common trick to ""linearize"" callbacks into a generator.
> 
> Callbacks + CPython reference counting (no PyPy) provide a nice approach to resource deallocation: if a variable is not referenced from outside then it is deallocated as soon as the callback exits, without invoking garbage collector. With generators if user writes ""response1 = yield ...; response2 = yield ..."" then these responses are kept alive, possibly for long. Even with ""response = yield ...; response = yield ..."" response is kept in memory longer than needed (if I'm not mistaken, until the second request finishes). One can write ""del response"", but it'd be nice to have some clever solution for that.

From Julia:

> I wouldn't promote it as the preferred way for dealing with requests/responses because of the already mention issues. It's not as flexible as using explicit callbacks (we should document that yielding a request with `callback` not being `None` breaks the chain btw) and it's hacky, debugging it is kind of hard.
> 
> Still, it's a really good helper for its primary use-case of downloading some additional page and handling errors (as opposed to using errbacks or downloading the page with another library) so I'd also like to include it in Scrapy.

NOTE: I didn't mean that I breaks the chain as in raising an exception, just that it won't wait for yielded requests if they have callbacks.

From Pablo:

> I think we're pretty much in agreement that it would be a nice feature for 1.0 (well, if we're not gonna have python 3 ... :). It needs to go with good documentation (explaining the downsides), tests and better error check (raising exception if it's used with a request having a callback).
> 
> Shall we make a ticket for this?. I think there's already enough content in this thread for one :).

/cc @darkrho 
",curita,pablohoffman
1140,2015-04-06 17:43:05,"This should fix HTTP downloader errors on missing HTTP status code strings (""OK"", ""Gone"", ""Bad Gateway"", ...) for #345.
I followed the suggested solution by @dangra (https://github.com/scrapy/scrapy/issues/345#issuecomment-23914813), but the outcome is truly suckish.

I haven't particularly cleaned this up, as I don't think anyone really cares to merge this ugly.
Supposedly putting a monkey patch (as posted in #345) in `scrapy/_monkeypatches.py` or such would feel cleaner than this.

Also I tried hard, but ultimately couldn't figure out how to write a testcase for this. The mockserver seems to generate it's own correct status lines from integer status codes, and I didn't manage to create raw responses (including a custom status code line).
",nyov,dangra
1134,2015-04-07 16:01:16,"I don't recall why we decided on removing xmliter really (/cc @nramirezuy, @pablohoffman). I'm OK with keeping it too.
",curita,nramirezuy
1134,2015-04-07 16:01:16,"I don't recall why we decided on removing xmliter really (/cc @nramirezuy, @pablohoffman). I'm OK with keeping it too.
",curita,pablohoffman
1125,2015-03-31 13:10:10,"A ticket to discuss and reference Spider development in other languages ([GSoC idea](http://gsoc2015.scrapinghub.com/ideas/#other-languages)).

The goal is to allow creating spiders in any language, that can be executed by the scrapy framework like a builtin spider. (And I would like a solution that is generic enough to decouple other scrapy components in the future the same way, like item pipelines.)

As I see it, this is about defining an _a)_ interface and _b)_ protocol which should be suitable for use across different programming languages. Then there are actual _c)_ implementation details to consider (how might scrapy's codebase need to change, to support this).
## 

a) The interface, should be at the smallest common denominator (across languages): OS (POSIX/SYSV) IPC (Inter Process Communication), giving us these options, I believe:
shared memory, mmapped files, ~~message queues~~ (not really on OSX), sockets (incl. [unix sockets](http://www.thomasstover.com/uds.html)), pipes. (For an overview, see [Beej's Guide to Unix IPC](http://beej.us/guide/bgipc/output/html/singlepage/bgipc.html))

(I would prefer sticking to local IPC, meaning no inet sockets, and let people who need networked IPC write their own transport middleware, for example using zeromq or json-rpc.)

b) The protocol needs consideration on what kind of data needs to be exchanged and how we're doing it. Pipes are unidirectional (unless you're on Solaris) while sockets work full-duplex.
(The GSoC ideas page also refers to Hadoop Streaming, which is using line-based communication through pipes. Q: How does this handle binary data?)
We might use some custom line-based interactions like this hadoop streaming style, or there are other standardized protocols: Protocol buffers/[protobuf](https://github.com/google/protobuf), [BERT](http://bert-rpc.org/) comes to mind.

c) To this I would put questions on how such a foreign language Spider fits into the current scrapy framework. For example inside a project the `SpiderManager` currently detects available spiders, with the help of the `SPIDER_MODULES` settings, how would it adapt?
Then there is statefulness to consider, do we need to know which response for the spider returned which new requests and/or items? Should it be workload-based (wait until the spider processed a response and returned everything, signaling a finish) or queue/stream-based (independent input and output, serial bus) or async callbacks. (The current Spider being asynchronous, it might be nice to have a similar, callback-based, protocol.)
How to best handle setup and teardown (open/close_spider), for example defining signals for spiders to trap and exit codes to return.
Not to forget error handling - if a spider dies, is it restartable or fatal (SIGSEGV, incompatible protocol version) and stopping the crawler?
## 

Hopefully this is a good collection of things to take into account before starting hacking. I tried to keep it brief, if anything is missing or wrong please let me know.
Let's get this party started :)

// cc @shaneaevans, @pablohoffman 
",nyov,shaneaevans
1125,2015-03-31 13:10:10,"A ticket to discuss and reference Spider development in other languages ([GSoC idea](http://gsoc2015.scrapinghub.com/ideas/#other-languages)).

The goal is to allow creating spiders in any language, that can be executed by the scrapy framework like a builtin spider. (And I would like a solution that is generic enough to decouple other scrapy components in the future the same way, like item pipelines.)

As I see it, this is about defining an _a)_ interface and _b)_ protocol which should be suitable for use across different programming languages. Then there are actual _c)_ implementation details to consider (how might scrapy's codebase need to change, to support this).
## 

a) The interface, should be at the smallest common denominator (across languages): OS (POSIX/SYSV) IPC (Inter Process Communication), giving us these options, I believe:
shared memory, mmapped files, ~~message queues~~ (not really on OSX), sockets (incl. [unix sockets](http://www.thomasstover.com/uds.html)), pipes. (For an overview, see [Beej's Guide to Unix IPC](http://beej.us/guide/bgipc/output/html/singlepage/bgipc.html))

(I would prefer sticking to local IPC, meaning no inet sockets, and let people who need networked IPC write their own transport middleware, for example using zeromq or json-rpc.)

b) The protocol needs consideration on what kind of data needs to be exchanged and how we're doing it. Pipes are unidirectional (unless you're on Solaris) while sockets work full-duplex.
(The GSoC ideas page also refers to Hadoop Streaming, which is using line-based communication through pipes. Q: How does this handle binary data?)
We might use some custom line-based interactions like this hadoop streaming style, or there are other standardized protocols: Protocol buffers/[protobuf](https://github.com/google/protobuf), [BERT](http://bert-rpc.org/) comes to mind.

c) To this I would put questions on how such a foreign language Spider fits into the current scrapy framework. For example inside a project the `SpiderManager` currently detects available spiders, with the help of the `SPIDER_MODULES` settings, how would it adapt?
Then there is statefulness to consider, do we need to know which response for the spider returned which new requests and/or items? Should it be workload-based (wait until the spider processed a response and returned everything, signaling a finish) or queue/stream-based (independent input and output, serial bus) or async callbacks. (The current Spider being asynchronous, it might be nice to have a similar, callback-based, protocol.)
How to best handle setup and teardown (open/close_spider), for example defining signals for spiders to trap and exit codes to return.
Not to forget error handling - if a spider dies, is it restartable or fatal (SIGSEGV, incompatible protocol version) and stopping the crawler?
## 

Hopefully this is a good collection of things to take into account before starting hacking. I tried to keep it brief, if anything is missing or wrong please let me know.
Let's get this party started :)

// cc @shaneaevans, @pablohoffman 
",nyov,pablohoffman
1124,2015-03-31 20:38:10,"+1 to @nyov suggestions.

I'd link http://doc.scrapinghub.com/scrapy-cloud.html#deploying-a-scrapy-spider and maybe mention that you are deploying to ""Scrapy Cloud"" specifically, though I'm not sure if this could be confusing. 

Maybe we could briefly introduce (or just link) this other tool for deploying: https://github.com/TeamHG-Memex/scrapy-dockerhub, that is not related to scrapyd. Maybe @shirk3y can help us with that.

/cc @eliasdorneles 
",curita,eliasdorneles
1123,2015-03-31 09:14:03,"@kmike @nyov
",sibiryakov,nyov
1123,2015-03-31 09:14:03,"@kmike @nyov
",sibiryakov,kmike
1120,2015-03-30 14:56:53,"Continuing of https://github.com/scrapy/scrapy/pull/1092

@kmike
",sibiryakov,kmike
1120,2015-03-31 15:32:29,"@kmike 
True, I thought about 1104 with passing multiple arguments, but not exclusively.
Rather it was along the lines that other (possibly 3rd party) resolvers here might support some parameters, while others may not.

This in mind, I think the arguments should be keyworded, so possibly passable as a dict, and allowing for a construtor of `def __init__(self, reactor, **kw)`.
I agree that not the whole settings structure needs to get passed in, and that decoupling code is a good thing(TM).

edit: otoh, I see that could stay specific to a an implementation since it's hardcoded here for now.

What would you think of allowing a custom resolver to be loaded using `load_object`, and a scrapy setting to support this?
",nyov,kmike
1119,2015-06-23 09:23:57,"@pablohoffman ^ looks like he got a wrong version of Scrapy using `pip install scrapy`. I'm not sure if that's possible.

I'm checking out some old issues. I think we can close this issue already?
",MojoJolo,pablohoffman
1112,2015-03-27 20:59:50,"Hey folks, some minor grammar fixes here -- thanks @breno!

@nyov maybe you want to review this too?
",eliasdorneles,nyov
1112,2015-03-27 20:59:50,"Hey folks, some minor grammar fixes here -- thanks @breno!

@nyov maybe you want to review this too?
",eliasdorneles,breno
1106,2015-03-26 18:38:52,"//cc @pablohoffman @shaneaevans @dangra and everyone else - thoughts? Use https://github.com/eliasdorneles/scrapy/blob/overview-page-improvements/docs/intro/overview.rst link to read it.

I think this introduction is nearly perfect :) 
+1 to merge it once we have the required PRs merged.
",kmike,shaneaevans
1106,2015-03-26 18:38:52,"//cc @pablohoffman @shaneaevans @dangra and everyone else - thoughts? Use https://github.com/eliasdorneles/scrapy/blob/overview-page-improvements/docs/intro/overview.rst link to read it.

I think this introduction is nearly perfect :) 
+1 to merge it once we have the required PRs merged.
",kmike,dangra
1106,2015-03-26 18:38:52,"//cc @pablohoffman @shaneaevans @dangra and everyone else - thoughts? Use https://github.com/eliasdorneles/scrapy/blob/overview-page-improvements/docs/intro/overview.rst link to read it.

I think this introduction is nearly perfect :) 
+1 to merge it once we have the required PRs merged.
",kmike,pablohoffman
1100,2015-03-24 17:13:34,"/cc @kmike do you know how we can achieve this?
",nramirezuy,kmike
1092,2015-03-23 12:26:31,"Hey @pablohoffman please take a look.
",sibiryakov,pablohoffman
1092,2015-03-25 18:16:46,"@kmike ping.
",sibiryakov,kmike
1088,2015-03-23 21:29:07,"@pablohoffman @Curita I am working on gsoc project 'simplified scrapy add-ons'. I have submitted my proposal on google-melange for feedback. I read [this](http://doc.scrapy.org/en/latest/topics/extensions.html) , and have few questions, how the order to the extensions are assigned? Is there any rules for the ordering number? Other than settings which files needs edit for adding a new extensions. Also please communicate me the appropriate medium for discussion or timings for IRC chat.
",karan10,pablohoffman
1087,2016-01-19 09:34:15,"As I recall, this change breaks a lot (all?) of custom user middlewares, right? 
@nramirezuy @dangra you've discussed it further, what was the resolution?
",kmike,dangra
1086,2015-03-24 12:27:46,"I've added a simple test, and an override to TextResponse for also checking on baseurl, as @dangra mentioned.

[Here's the diff](https://github.com/Curita/scrapy/compare/response-urljoin...nyov:response-urljoin). wow, with merge button :)

I don't want to open another PR, 
If this is okay, please pull it into the existing one here. And feel free to modify it, if I missed anything.


",nyov,dangra
1084,2015-03-18 23:05:59,"It seems the longer lines are truncated in 0.24.5 [release notes](https://github.com/scrapy/scrapy/blob/master/docs/news.rst) - check ""S3DownloadHandler.."", ""Tentative attention message..."", ""Patches Twisted issue..."" and ""SgmlLinkExtractor...""  lines. 

It is easy to fix, but I wonder why it happened. //cc @dangra 
",kmike,dangra
1081,2015-03-19 00:35:29,"Please check - all docs except for overview & tutorial should be updated.

What do you think about adding FEED_EXPORT_FIELDS option, to allow defining a list of fields to export? Without Item classes CSV exporter can't figure out the header robustly (currently fields of a first item are used). @nramirezuy also mentioned this feature [here](https://github.com/scrapy/scrapy/issues/1050).
",kmike,nramirezuy
1081,2015-03-23 13:21:30,"This PR doesn't allow items to be arbitrary dict-like objects, like @shaneaevans proposes in https://github.com/scrapy/scrapy/issues/1064#issuecomment-78196751 - item must be either a subclass of BaseItem or a dict / subclass of a dict.

Maybe instead of checking for dict/BaseItem explicitly we can start checking for MutableMapping, but it is more risky. I think that starting with more strict requirements on spider output is better - be can make them less strict in future.
",kmike,shaneaevans
1076,2015-03-17 11:26:22,"@kmike Can help you with this. I'm not good at documenting :tongue: 

But if you want to open a PR by your own we'll be glad to review it.
",nramirezuy,kmike
1073,2015-03-17 15:49:56,"I've changed the detail settings for autothrottle and HTTP cache to the default values.

It seems more sensible in these cases that have an on/off setting, because a user enabling all of them together would expect the defaults. Thanks @redapple for the heads up. ;)
",eliasdorneles,redapple
1063,2015-03-05 20:37:19,"What do you think about getting rid of contrib and contrib_exp? Most of contrib is enabled by default anyways, it is not really contrib.

For example, we can do the following:
1. [scrapy.contrib.downloadermiddleware](https://github.com/scrapy/scrapy/tree/master/scrapy/contrib/downloadermiddleware) -> scrapy.downloadermiddleware;
2. [scrapy.contrib.exporter](https://github.com/scrapy/scrapy/tree/master/scrapy/contrib/exporter) -> scrapy.exporter;
3. [scrapy.contrib.linkextractors](https://github.com/scrapy/scrapy/tree/master/scrapy/contrib/linkextractors) -> scrapy.linkextractors;
4. [scrapy.contrib.loader](https://github.com/scrapy/scrapy/tree/master/scrapy/contrib/loader) -> scrapy.loader;
5. [scrapy.contrib.pipeline](https://github.com/scrapy/scrapy/tree/master/scrapy/contrib/pipeline) -> scrapy.pipeline;
6. [scrapy.contrib.spidermiddleware](https://github.com/scrapy/scrapy/tree/master/scrapy/contrib/spidermiddleware) -> scrapy.spidermiddleware;
7. [scrapy.contrib.spiders](https://github.com/scrapy/scrapy/tree/master/scrapy/contrib/spiders) -> scrapy.spiders;
8. files from [scrapy.contrib](https://github.com/scrapy/scrapy/tree/master/scrapy/contrib) top-level folder -> scrapy.extensions;
9. [contrib_exp](https://github.com/scrapy/scrapy/tree/master/scrapy/contrib_exp) - should we just remove it, or move contrib_exp.iterators to scrapy.util.iterators and contrib_exp.downloadermiddleware to scrapy.downloadermiddleware?

Also, @dangra or @pablohoffman said something about moving scrapy.core to scrapy.http or to a separate scrapy-core package.

I think we should also move scrapy.contrib.djangoitem to a separate repo, something like scrapy-django.

We are also inconsistent in singular-plural names (pipeline and loader, but spiders and linkextractors). I'm not sure if it worths fixing, but maybe it worths.
",kmike,dangra
1063,2015-03-05 20:37:19,"What do you think about getting rid of contrib and contrib_exp? Most of contrib is enabled by default anyways, it is not really contrib.

For example, we can do the following:
1. [scrapy.contrib.downloadermiddleware](https://github.com/scrapy/scrapy/tree/master/scrapy/contrib/downloadermiddleware) -> scrapy.downloadermiddleware;
2. [scrapy.contrib.exporter](https://github.com/scrapy/scrapy/tree/master/scrapy/contrib/exporter) -> scrapy.exporter;
3. [scrapy.contrib.linkextractors](https://github.com/scrapy/scrapy/tree/master/scrapy/contrib/linkextractors) -> scrapy.linkextractors;
4. [scrapy.contrib.loader](https://github.com/scrapy/scrapy/tree/master/scrapy/contrib/loader) -> scrapy.loader;
5. [scrapy.contrib.pipeline](https://github.com/scrapy/scrapy/tree/master/scrapy/contrib/pipeline) -> scrapy.pipeline;
6. [scrapy.contrib.spidermiddleware](https://github.com/scrapy/scrapy/tree/master/scrapy/contrib/spidermiddleware) -> scrapy.spidermiddleware;
7. [scrapy.contrib.spiders](https://github.com/scrapy/scrapy/tree/master/scrapy/contrib/spiders) -> scrapy.spiders;
8. files from [scrapy.contrib](https://github.com/scrapy/scrapy/tree/master/scrapy/contrib) top-level folder -> scrapy.extensions;
9. [contrib_exp](https://github.com/scrapy/scrapy/tree/master/scrapy/contrib_exp) - should we just remove it, or move contrib_exp.iterators to scrapy.util.iterators and contrib_exp.downloadermiddleware to scrapy.downloadermiddleware?

Also, @dangra or @pablohoffman said something about moving scrapy.core to scrapy.http or to a separate scrapy-core package.

I think we should also move scrapy.contrib.djangoitem to a separate repo, something like scrapy-django.

We are also inconsistent in singular-plural names (pipeline and loader, but spiders and linkextractors). I'm not sure if it worths fixing, but maybe it worths.
",kmike,pablohoffman
1063,2015-03-13 21:33:09,"In a meeting with @pablohoffman, @dangra, @redapple and @Curita we agreed on the following:
1. relocate scrapy.contrib;
2. don't do the scrapy.core / scrapy.http relocation;
3. remove contrib_exp;
4. move djangoitem to a separate project named scrapy-djangoitem;
5. keep backwards compatibility (what to do with contrib_exp?);
6. use plural names consistently (of course, when it makes sense).

As @nramirezuy said, it would be nice to have this in Scrapy 1.0 release.
",kmike,redapple
1063,2015-03-13 21:33:09,"In a meeting with @pablohoffman, @dangra, @redapple and @Curita we agreed on the following:
1. relocate scrapy.contrib;
2. don't do the scrapy.core / scrapy.http relocation;
3. remove contrib_exp;
4. move djangoitem to a separate project named scrapy-djangoitem;
5. keep backwards compatibility (what to do with contrib_exp?);
6. use plural names consistently (of course, when it makes sense).

As @nramirezuy said, it would be nice to have this in Scrapy 1.0 release.
",kmike,dangra
1063,2015-03-13 21:33:09,"In a meeting with @pablohoffman, @dangra, @redapple and @Curita we agreed on the following:
1. relocate scrapy.contrib;
2. don't do the scrapy.core / scrapy.http relocation;
3. remove contrib_exp;
4. move djangoitem to a separate project named scrapy-djangoitem;
5. keep backwards compatibility (what to do with contrib_exp?);
6. use plural names consistently (of course, when it makes sense).

As @nramirezuy said, it would be nice to have this in Scrapy 1.0 release.
",kmike,pablohoffman
1060,2015-03-30 17:00:19,"Is it posisble to route logs from each spider/Crawler to a separate file when several spiders are executed in parallel? 

~~Scrapyrt does that.~~ no quite: as @chekunkov said, if many spider logs files are opened in the same process twisted log observer cannot filter those messages and they appear in all opened log files
",kmike,chekunkov
1060,2015-04-02 11:55:48,"@Curita 

As far as Scrapy is used to run one spider per process that's not a problem. But unfortunately Scrapy is used in other projects, like ScrapyRT, and by design ScrapyRT runs all crawls in one process. That wasn't my decision, it was discussed and agreed with @shaneaevans, reason for this is performance considerations. So, when you have multiple crawlers in a single process and logging is not designed provide you a way to filter messages related only to one particular crawl - messages from all crawls running at the same time would mix which makes logging unusable.

> First of all, Scrapy needs to log things before crawlers are instantiated

Can you provide an example? I think you mean logging that's coming from commands code. In my opinion it's perfectly fine that those log messages doesn't have crawler object in extra as it wasn't instantiated yet and there's no need to associate such messages with any crawler.

>  if LOG_STDOUT is set print messages will be redirected to the logging system, and those clearly won’t have a Crawler attached to them. The same goes for messages logged outside Scrapy by other libraries, like Twisted.

that's not a big deal as well, at least in context of ScrapyRT such messages will be filtered and no one will miss them if they wouldn't appear in crawl log files.

> Loggers are referenced by their name, so we should have a unique string representation for every instance. I know that the Spider.name is the first thing that comes to mind, but this is just not robust enough. It could change at any time during the Spider execution. It could be defined in its **init** call, not as class attribute, so we won’t be able to log anything until then. And more importantly, we can actually run two spiders at the same time with the same name, so we won’t be able to differentiate their loggers.

In case of ScrapyRT one crawler creates exactly one spider like it expected by latest Scrapy version - crawler doesn't support having multiple Spider objects any more. So no, I'm not talking about one logger per spider object, I'm talking about one logger per crawler object. And what I expect from Scrapy logging is to be able to create filter which will take messages related only to one crawler object and then send them to log file (or use in any other way). All messages coming from one spider should be handled by crawler logger. Also crawler contains references to engine, scraper, downloader, middlewares, pipelines, extensions objects (as far as I know we don't have singletones there, different crawlers reference different objects) - all messages coming from those objects should also be handled by crawler logger.

Regards possible name conventions and implementation - unfortunately have no brilliant ideas. Crawl logger should be descendant of 'scrapy', so it could be 'scrapy.crawler_{object_id}'. You said:

> but this will make the log output non-deterministic, and loggers will be hard to reference manually.

To deal with this crawler object may have method which will return proper logger object and this way of obtaining logger should be used in _all_ pipelines, middlewares, pipelines, extensions, engine, scraper, schdeduler, downloader etc. And it's not clear what you mean by 'non-deterministic' - if loggers with have the same prefix that shouldn't cause problems.

> Also, we’re switching to python logging, so discouraging the users to use it directly, forcing them to call Scrapy wrappers over it (like with a crawler.log() call) goes against one of the main ideas for this change.

You say we want users to use it directly - so why `Spider.logger` was added? This goes against direct usage of logger, as direct usage would be defining logger like `logger = logging.getLogger(__name__)` at the top of spider module or during `__init__`.

Switching to python logging has much more other benefits then ability to get logger by using `logger = logging.getLogger(__name__)` - loggers hierarchy, configuration flexibility, all these facilities like Handlers, Filters, Formatters etc.

To summarize - I tried to show why proposed logging design is not perfect and how it will break depending projects. If we merge it like it is now - I have no good ideas how to support it in ScrapyRT. Bad ideas:

1) log everything in a single file - I think this have no sense 
2) log messages if they have reference to crawler of spider object - logging will miss a lot of useful log messages 
3) disable logging completely - yeah, really who needs those log messages :) 
4) return to the implementation where each crawl is started in separate process <- @shaneaevans ?
",chekunkov,shaneaevans
1051,2015-03-26 20:46:26,"This looks more like it, in theory. But if you reference `self`, it can no longer be a `staticmethod`.
Though I'm not sure I'm up-to-date with current internals. (Do we still have multiple spiders per crawler actually?)

@dangra, @kmike, could you take a look at this? I'd love to see this make the next release if we can make it work.
",nyov,dangra
1051,2015-03-26 20:46:26,"This looks more like it, in theory. But if you reference `self`, it can no longer be a `staticmethod`.
Though I'm not sure I'm up-to-date with current internals. (Do we still have multiple spiders per crawler actually?)

@dangra, @kmike, could you take a look at this? I'd love to see this make the next release if we can make it work.
",nyov,kmike
1047,2015-07-28 19:00:33,"Thanks @klangner! Me, @eliasdorneles and @dangra went forward and merged https://github.com/scrapy/scrapy/pull/1379, copy-pasting some of your changes. 
",kmike,eliasdorneles
1047,2015-07-28 19:00:33,"Thanks @klangner! Me, @eliasdorneles and @dangra went forward and merged https://github.com/scrapy/scrapy/pull/1379, copy-pasting some of your changes. 
",kmike,dangra
1045,2015-02-10 20:02:36,"Hi,

I think that scrapy.contrib.loader.processor should be separated from scrapy.contrib.loader. It should have its own docs and motivation. IMHO Scrapy [Item loaders](http://doc.scrapy.org/en/latest/topics/loaders.html) are hard to graps because they are documented in a top-bottom order: first docs show item loaders, then built-in processors are shown, and then an example of custom processors are given.

I think that the following order is better:
- show a complex non-composable data extraction example;
- show how to make it better by using functions;
- show how to combine functions declaratively using Compose and MapCompose;
- show other scrapy built-in processors;
- only then show ItemLoaders as a way to simplify populating of items.

I've started to write a SEP to discuss the existing modules and their alternatives. The end result could be either a better docs for existing modules, or some changes to them, I'm not sure. SEP is very incomplete.

The SEP starts very opinionated; I'm going to remove ""I like / don't like"" parts after we discuss them.

Please let me know what do you think. 

//cc @nramirezuy @dangra @redapple @nyov @pablohoffman @Digenis and anyone interested.
",kmike,redapple
1045,2015-02-10 20:02:36,"Hi,

I think that scrapy.contrib.loader.processor should be separated from scrapy.contrib.loader. It should have its own docs and motivation. IMHO Scrapy [Item loaders](http://doc.scrapy.org/en/latest/topics/loaders.html) are hard to graps because they are documented in a top-bottom order: first docs show item loaders, then built-in processors are shown, and then an example of custom processors are given.

I think that the following order is better:
- show a complex non-composable data extraction example;
- show how to make it better by using functions;
- show how to combine functions declaratively using Compose and MapCompose;
- show other scrapy built-in processors;
- only then show ItemLoaders as a way to simplify populating of items.

I've started to write a SEP to discuss the existing modules and their alternatives. The end result could be either a better docs for existing modules, or some changes to them, I'm not sure. SEP is very incomplete.

The SEP starts very opinionated; I'm going to remove ""I like / don't like"" parts after we discuss them.

Please let me know what do you think. 

//cc @nramirezuy @dangra @redapple @nyov @pablohoffman @Digenis and anyone interested.
",kmike,Digenis
1045,2015-02-10 20:02:36,"Hi,

I think that scrapy.contrib.loader.processor should be separated from scrapy.contrib.loader. It should have its own docs and motivation. IMHO Scrapy [Item loaders](http://doc.scrapy.org/en/latest/topics/loaders.html) are hard to graps because they are documented in a top-bottom order: first docs show item loaders, then built-in processors are shown, and then an example of custom processors are given.

I think that the following order is better:
- show a complex non-composable data extraction example;
- show how to make it better by using functions;
- show how to combine functions declaratively using Compose and MapCompose;
- show other scrapy built-in processors;
- only then show ItemLoaders as a way to simplify populating of items.

I've started to write a SEP to discuss the existing modules and their alternatives. The end result could be either a better docs for existing modules, or some changes to them, I'm not sure. SEP is very incomplete.

The SEP starts very opinionated; I'm going to remove ""I like / don't like"" parts after we discuss them.

Please let me know what do you think. 

//cc @nramirezuy @dangra @redapple @nyov @pablohoffman @Digenis and anyone interested.
",kmike,nramirezuy
1045,2015-02-10 20:02:36,"Hi,

I think that scrapy.contrib.loader.processor should be separated from scrapy.contrib.loader. It should have its own docs and motivation. IMHO Scrapy [Item loaders](http://doc.scrapy.org/en/latest/topics/loaders.html) are hard to graps because they are documented in a top-bottom order: first docs show item loaders, then built-in processors are shown, and then an example of custom processors are given.

I think that the following order is better:
- show a complex non-composable data extraction example;
- show how to make it better by using functions;
- show how to combine functions declaratively using Compose and MapCompose;
- show other scrapy built-in processors;
- only then show ItemLoaders as a way to simplify populating of items.

I've started to write a SEP to discuss the existing modules and their alternatives. The end result could be either a better docs for existing modules, or some changes to them, I'm not sure. SEP is very incomplete.

The SEP starts very opinionated; I'm going to remove ""I like / don't like"" parts after we discuss them.

Please let me know what do you think. 

//cc @nramirezuy @dangra @redapple @nyov @pablohoffman @Digenis and anyone interested.
",kmike,nyov
1045,2015-02-10 20:02:36,"Hi,

I think that scrapy.contrib.loader.processor should be separated from scrapy.contrib.loader. It should have its own docs and motivation. IMHO Scrapy [Item loaders](http://doc.scrapy.org/en/latest/topics/loaders.html) are hard to graps because they are documented in a top-bottom order: first docs show item loaders, then built-in processors are shown, and then an example of custom processors are given.

I think that the following order is better:
- show a complex non-composable data extraction example;
- show how to make it better by using functions;
- show how to combine functions declaratively using Compose and MapCompose;
- show other scrapy built-in processors;
- only then show ItemLoaders as a way to simplify populating of items.

I've started to write a SEP to discuss the existing modules and their alternatives. The end result could be either a better docs for existing modules, or some changes to them, I'm not sure. SEP is very incomplete.

The SEP starts very opinionated; I'm going to remove ""I like / don't like"" parts after we discuss them.

Please let me know what do you think. 

//cc @nramirezuy @dangra @redapple @nyov @pablohoffman @Digenis and anyone interested.
",kmike,dangra
1045,2015-02-10 20:02:36,"Hi,

I think that scrapy.contrib.loader.processor should be separated from scrapy.contrib.loader. It should have its own docs and motivation. IMHO Scrapy [Item loaders](http://doc.scrapy.org/en/latest/topics/loaders.html) are hard to graps because they are documented in a top-bottom order: first docs show item loaders, then built-in processors are shown, and then an example of custom processors are given.

I think that the following order is better:
- show a complex non-composable data extraction example;
- show how to make it better by using functions;
- show how to combine functions declaratively using Compose and MapCompose;
- show other scrapy built-in processors;
- only then show ItemLoaders as a way to simplify populating of items.

I've started to write a SEP to discuss the existing modules and their alternatives. The end result could be either a better docs for existing modules, or some changes to them, I'm not sure. SEP is very incomplete.

The SEP starts very opinionated; I'm going to remove ""I like / don't like"" parts after we discuss them.

Please let me know what do you think. 

//cc @nramirezuy @dangra @redapple @nyov @pablohoffman @Digenis and anyone interested.
",kmike,pablohoffman
1039,2016-09-14 17:08:07,"Hey @markbaas , are you still interested in this issue? (I know, it's been quiet for 18 months...)
You can check @eliasdorneles 's https://github.com/scrapy/parsel/pull/54 and maybe help it get merged in scrapy/parsel with some feedback.
In particular, we could provide a working html5lib-based parser for parsel (as the one shipped with lxml has issues; see https://github.com/scrapy/parsel/pull/54#discussion_r74402632 and https://mailman-mail5.webfaction.com/pipermail/lxml/2016-August/007758.html)
I also like your `html5` type addition in https://github.com/scrapy/scrapy/pull/1043 , makes sense to me, as lxml's HTML parser is not really html5-ready
",redapple,eliasdorneles
1035,2015-02-13 16:29:53,"How do you feel about supporting multiple folders? Our function returning existent folders and the command being able to look into these folders?

/cc @SudShekhar @kmike 
",nramirezuy,kmike
1016,2015-02-02 17:28:24,"@kmike Can you review this? I'm looking for py3 and documentation feedback.
",nramirezuy,kmike
1012,2015-02-09 16:18:06,"@kmike What do you think about this? I kinda don't like the hard coded filtering (`x != """" or x is not None`), or the list support for processors like `Strip` or `Replace`. But if we remove that they all can  be substituted with a `lambda`.
",nramirezuy,kmike
1007,2015-04-21 19:35:53,"As @pablohoffman noted, selectors is not a good name because of https://docs.python.org/3/library/selectors.html
",kmike,pablohoffman
1007,2015-04-29 06:34:58,"> What about https://github.com/scrapy/scrapy/issues/586#issuecomment-94919509?

What about it? What's the context for mentioning it?
Actually I don't agree with the comment, scrapy is still a data extraction tool without Selectors, to me. They are non-essential and people also use regex', beautiful soup and other libraries on source content.  Extraction logic may also be elsewhere, say the Image pipeline, so I disagree.

> As @pablohoffman noted, selectors is not a good name [...]

An alternative name - that is hard. Maybe defining the project scope first is important for that. Would we consider expanding this to binary format ""selectors"" for example? Is it considered to be a parser library, a data extraction kit or a bunch of convenience wrappers? Just some thoughts.
",nyov,pablohoffman
998,2015-10-27 13:29:15,"Here is a snippet to reproduce:
file: snowman.html



~~run: `scrapy shell ~/snowman.html`~~
The above doesn't work (probably some bug `utils.url`),
on 1.1 neither `scrapy shell file:///home/digenis/snowman.html` does.
Only
`scrapy shell file://localhost/home/digenis/snowman.html`



 `UnicodeEncodeError: 'charmap' codec can't encode character u'\u2603' in position 24: character maps to <undefined>`

lxml decodes the entity inside the href attribute
and since`Windows-1251` doesn't have the :snowman: character
it can't be decoded ""back"" to cp1251.

As @kmike suggests in https://github.com/scrapy/scrapy/issues/1403#issuecomment-149194449
the path part needs to be encoded to utf8, as browsers do.
It should need to be able to be encoded in the response's encoding
",Digenis,kmike
996,2014-12-29 13:01:15,"I've created this ticket so we can discuss the Scrapy 1.0 release. @kmike has created a milestone so we can begin adding issues when we are ready. This ticket will be a good place to keep track of any discussions/ideas related to the release, including our motivations behind a 1.0 release, and of course what we would like to see go into it. It would be good if we could look at why we want to do a 1.0 release, so we know what's best to include.

I know Mikhail is interested in seeing API and documentation related issues go into this release, as well as Python 3 support. Below are the examples he has provided:

> Examples of API-related tickets:
> - https://github.com/scrapy/scrapy/issues/906 (move Selector or its parts to its own library)
> - https://github.com/scrapy/scrapy/issues/548 (add a better support for relative URLs);
> - https://github.com/scrapy/scrapy/issues/578, https://github.com/scrapy/scrapy/issues/568 and alike tickets - now there are link extractors, selectors and item loaders which do very similar things;
> - allow to return dicts instead of Items from spiders (I don't think there is a ticket for that); it will make Scrapy easier to use for quick scripts and prevent tickets like https://github.com/scrapy/scrapy/issues/968.
> 
> Docs-related tickets:
> - https://github.com/scrapy/scrapy/issues/609 (improve the tutorial - I like http://hopefulramble.blogspot.ru/2014/08/web-scraping-with-scrapy-first-steps_30.html a lot);
> - https://github.com/scrapy/scrapy/issues/713 (move docstrings from docs to the source code);

@eliasdorneles has also mentioned this specific issue: https://github.com/scrapy/scrapy/issues/712

The 1.0 release will also be a good opportunity to improve things like versioning. Currently odd numbers correspond to development versions, and even numbers correspond to releases--this is something we probably want to change. Any changes which would require breaking backwards compatibility would be suited to this release too, as it would be reasonable to expect such changes in moving from 0.2x to 1.0.
",rdowinton,eliasdorneles
996,2014-12-29 13:01:15,"I've created this ticket so we can discuss the Scrapy 1.0 release. @kmike has created a milestone so we can begin adding issues when we are ready. This ticket will be a good place to keep track of any discussions/ideas related to the release, including our motivations behind a 1.0 release, and of course what we would like to see go into it. It would be good if we could look at why we want to do a 1.0 release, so we know what's best to include.

I know Mikhail is interested in seeing API and documentation related issues go into this release, as well as Python 3 support. Below are the examples he has provided:

> Examples of API-related tickets:
> - https://github.com/scrapy/scrapy/issues/906 (move Selector or its parts to its own library)
> - https://github.com/scrapy/scrapy/issues/548 (add a better support for relative URLs);
> - https://github.com/scrapy/scrapy/issues/578, https://github.com/scrapy/scrapy/issues/568 and alike tickets - now there are link extractors, selectors and item loaders which do very similar things;
> - allow to return dicts instead of Items from spiders (I don't think there is a ticket for that); it will make Scrapy easier to use for quick scripts and prevent tickets like https://github.com/scrapy/scrapy/issues/968.
> 
> Docs-related tickets:
> - https://github.com/scrapy/scrapy/issues/609 (improve the tutorial - I like http://hopefulramble.blogspot.ru/2014/08/web-scraping-with-scrapy-first-steps_30.html a lot);
> - https://github.com/scrapy/scrapy/issues/713 (move docstrings from docs to the source code);

@eliasdorneles has also mentioned this specific issue: https://github.com/scrapy/scrapy/issues/712

The 1.0 release will also be a good opportunity to improve things like versioning. Currently odd numbers correspond to development versions, and even numbers correspond to releases--this is something we probably want to change. Any changes which would require breaking backwards compatibility would be suited to this release too, as it would be reasonable to expect such changes in moving from 0.2x to 1.0.
",rdowinton,kmike
991,2015-01-06 07:01:43,"> Considering compatibility, one's literally assuming that the 2nd request is important and cannot be missed. But this seems counterintuitive for a scraper/spider, because we're sure that the request is already encountered/crawled before. So, I consider the current situation more of a bug.

I agree on being counterintuitive, don't know the reasons behind that change from way back (/cc @pablohoffman),  and it seems at least that is something not well communicated. I think that using `dont_filter` as ""skip the duplicates filter"" is a valid use-case, though having the first request filtered by default is an odd choice, since as you said it's not the default. This can be addressed locally by overriding the make_requests_from_url method (see [here](http://scrapy.readthedocs.org/en/latest/topics/spiders.html#scrapy.spider.Spider.make_requests_from_url)), where `dont_filter` being set is documented, but you're right we could do better.

> We strongly believe the version before Jun 2011 is much more intuitive. It requires no revamps on docs, nor does it create any conflicts in the overloaded meaning of dont_filter in OffsiteMiddleware.

Main problem I see here is maintaining backward compatibility. Docs could stay the same (though I'd rather state the behavior explicitly to avoid further misinterpretations) but spiders already coded making assumptions on how `dont_filter` works could break (i.e. using intentionally the fact that `dont_filter` urls are not stored).

Personally, I like the behavior introduced on this PR better, and since we're planning a new release with some backward incompatible changes this is a good time to introduce it if it gets accepted. Nonetheless, we should get more feedback on why this was needed (or not) in the first place, how `dont_filter` is understood by the users, and then decide how much backward incompatible this should be (ranging from making a new flag to deleting the possibility of using it as before).

Some side notes:

> I'd imagine adding an additional flag like dont_store would result in a much more overcrowded documentation on 4 different states: a/ (dont_filter=True, dont_store=True), b/ (dont_filter=True, dont_store=False), c/ (dont_filter=False, dont_store=True), d/ (dont_filter=False, dont_store=False), and further compounded with cases for the 1st request and 2nd request, each holding one of the state.

It's about how well is communicated, I haven't given it much thought, but for example we could simply add the documentation for `dont_store` and say that `dont_filter` takes precedence over it. I don't think that'll confuse the users too much. Still, we have to decide if adding a new flag is what we want.

> I hope you understand, and so I'd be happy to add a test case to justify this PR instead of further complicating the problem with a new PR.

I proposed creating a new PR if we'd have gone with the route of adding this change as a separated flag since the description/title wouldn't match that (though we can edit it), but yes, if we keep the approach as it is is better to keep further changes (docs and test) in the same PR.
",curita,pablohoffman
981,2015-03-29 13:17:43,"Can someone tell me what is the problem with SSL in this website https://www.techinasia.com/

Ping @dangra 
",jbinfo,dangra
968,2014-12-06 17:36:36,"Adding items to scrapy can be a tedious work when there are lot of fields to create and at the and all items are same type `scrapy.Field()`. I think creating a command like **Ruby on Rails** have, like:

 `rails generate model ad name:string description:text price:decimal seller_id:integer email:string img_url:string`

In our case it can be like this:

`scrapy genitem Product name url price brand image`

This will help a lot to focus on writing spiders itself and simplify the task of defining data items.

What do you think?? @pablohoffman 
",codeadict,pablohoffman
961,2014-12-02 14:51:05,"I'm OK with the api changes, @dangra ?
",pablohoffman,dangra
952,2015-04-02 12:54:43,"@Digenis I don't think we're still supporting 0.22.x. A backport to 0.24.x is more likely, but I'm not sure there will be another 0.24.x release. @dangra @pablohoffman - are there plans to do one more 0.24.x release?
",kmike,dangra
952,2015-04-02 12:54:43,"@Digenis I don't think we're still supporting 0.22.x. A backport to 0.24.x is more likely, but I'm not sure there will be another 0.24.x release. @dangra @pablohoffman - are there plans to do one more 0.24.x release?
",kmike,pablohoffman
951,2016-02-05 08:34:58,"Compression middleware behaviour on gzipped sitemaps still seems to be wrong. 

In case of amazon it does not work anymore, because it sets header ""Content-Type"" to ""application/octet-stream"" instead of ""application/x-gzip"". So this for this issue - [Issue #193](https://github.com/scrapy/scrapy/issues/193) - does not work anymore. It still tries to decompress the file two times - once in HTTPCompressionMiddleware and once in Sitemap spider.

I've encountered opposite issue on other site. Sitemap file is also gzipped and has extension "".gz"", but when site gets header ""Accept-Encoding""=""x-gzip,gzip,deflate"" (which is added by compression middleware) it compresses the file once more. Response contents in this case is twice compressed. This is of course an error on site hosters side, but in my opinion Scrapy should account for this anyway - I imagine there are a lot of such sites in the web.

To get rid of all of these issues I suggest to move the whole compression behavior to HTTPCompressionMiddleware, and it should perform decompressing in presence of only ""Content-Encoding"" header (disregard check for ""Content-Type"") and do decompression in a loop until gets an error (easier to ask forgiveness than permission) - gzip module will fail earlier of first read so should not add much overhead. Another option is to check gzip file format, as suggested by @dangra in [Issue #193](https://github.com/scrapy/scrapy/issues/193). This would be a more complex solution, but potentially will perform better.

Also, to account for cases when HTTPCompressionMiddleware is disabled, Sitemap can still try to decompress response content if extension is "".gz"".
",juraseg,dangra
946,2014-11-19 09:04:06,"/cc @pablohoffman 
",tpeng,pablohoffman
946,2014-11-19 10:11:53,"To be clear: I haven't checked if the implementation works as intended; @pablohoffman seems to have a better understanding of it.
",kmike,pablohoffman
946,2014-11-25 12:36:42,"@kmike according to @dangra the failures are not related to the change:


",tpeng,dangra
934,2014-12-30 16:15:32,"@nyov 
extras/_scrapy_zsh_completion /usr/share/zsh/vendor-completions/
will this change work ?
",Dineshs91,nyov
927,2015-04-02 22:14:04,"I wonder why nobody looked at this yet. It seems to fit the requirements from #907 and rebased nicely.
@kmike , @redapple would make sense to merge before contrib disappears :]

And I do wonder why only `LxmlParserLinkExtractor` does a `unique_list` pass over the collected links. Are the others filtering duplicates inherently?
",nyov,redapple
927,2015-04-02 22:14:04,"I wonder why nobody looked at this yet. It seems to fit the requirements from #907 and rebased nicely.
@kmike , @redapple would make sense to merge before contrib disappears :]

And I do wonder why only `LxmlParserLinkExtractor` does a `unique_list` pass over the collected links. Are the others filtering duplicates inherently?
",nyov,kmike
921,2014-10-12 06:44:16,"I talked with @dangra about that recently, maybe he can provide more details why question about this change was raised /cc @shane42 @pablohoffman 
",chekunkov,dangra
921,2014-10-12 06:44:16,"I talked with @dangra about that recently, maybe he can provide more details why question about this change was raised /cc @shane42 @pablohoffman 
",chekunkov,pablohoffman
914,2014-10-07 13:12:32,"@nramirezuy  You could achieve the same thing using `python setup.py develop`, right?  
Also, if you did. You would not have to customize `$PYTHONPATH`, which I guess you have modified to make your little setup thingy working. And manually copy over the script. Seems like unnecessary work, when `python setup.py develop` does it all for you.. (and for everybody else that have a similar need) In my opinion this would be the right way to do it these days.. ;)  

Take a look at the docs if you are not acquainted with the [setuptools development mode.](https://pythonhosted.org/setuptools/setuptools.html#development-mode)

@kmike What do you think?
",brunsgaard,kmike
908,2014-10-07 16:07:59,"It makes sense but I am not a user of DjangoItem to say if it is best to reset the model instance or update the underlying django model field in case `self._instance` is not None.

Summoning past contributors of DjangoItem @void @djm @midiotthimble @Mimino666 
",dangra,void
908,2014-10-07 16:07:59,"It makes sense but I am not a user of DjangoItem to say if it is best to reset the model instance or update the underlying django model field in case `self._instance` is not None.

Summoning past contributors of DjangoItem @void @djm @midiotthimble @Mimino666 
",dangra,Mimino666
908,2014-10-07 16:07:59,"It makes sense but I am not a user of DjangoItem to say if it is best to reset the model instance or update the underlying django model field in case `self._instance` is not None.

Summoning past contributors of DjangoItem @void @djm @midiotthimble @Mimino666 
",dangra,djm
900,2014-11-28 23:21:07,"/cc @pablohoffman @dangra because the changes could touch Request class 
",kmike,dangra
900,2014-11-28 23:21:07,"/cc @pablohoffman @dangra because the changes could touch Request class 
",kmike,pablohoffman
891,2014-09-20 03:23:27,"Sometimes it is helpful to ""reset"" depth, e.g. when going to an external domain. It seems this is not possible with existing [DepthMiddleware](https://github.com/scrapy/scrapy/blob/master/scrapy/spidermiddlewares/depth.py) because it increases `response.meta['depth'] + 1` unconditionally. 

It seems that to fix it DepthMiddleware can be changed to look at `request.meta['depth']` first. 

~~Why was [this](https://github.com/scrapy/scrapy/commit/05ce20dab399c5fb28675a21a3bbc200316fd83c) change made? //cc @dangra~~
",kmike,dangra
884,2014-09-16 06:38:21,"hello scrapy developers, could you take a look at my PR and let it pass so I get to finish my project? it sort of require this behavior get fixed, many thanks. @dangra 

### update

no hurry, I'm now using my own fork, BTW I've tested this change, it works.
",timfeirg,dangra
881,2014-09-11 18:05:26,"@curita what do you think about it?
",rocioar,curita
877,2014-09-05 21:21:39,"I was having issues suppressing loglines to stdout, as mentioned here:

[https://groups.google.com/forum/#!topic/scrapy-users/pJfN8BfF_fM](https://groups.google.com/forum/#!topic/scrapy-users/pJfN8BfF_fM)

The logging settings I used were:



I was logging from a spider, and had set `log.start(loglines=""INFO"")` in the spider's init method. When I tailed the LOG_FILE, I was only seeing messages that were WARNING or higher, but I was still seeing INFO on stdout. 

@nramirezuy made a great proof of concept here that shows the desired behavior:
[https://gist.github.com/nramirezuy/e75d8c041b07a8edb44f](https://gist.github.com/nramirezuy/e75d8c041b07a8edb44f)

But if you add a `log.start()` statement to the init method of the DummySpider, then you'll see everything on stdout, and only warnings and above in the log file.



This might actually be a bug with Twisted, as I see Scrapy's `log.msg()` is a pretty thin wrapper around theirs. Just wanted to drop it here first as it might be a problem with Scrapy's use of their logger:

[https://github.com/scrapy/scrapy/blob/master/scrapy/log.py#L132](https://github.com/scrapy/scrapy/blob/master/scrapy/log.py#L132)
",hartleybrody,nramirezuy
875,2014-09-03 20:04:12,"The order of execution is as listed:
- https://github.com/scrapy/scrapy/pull/816/files#diff-017ca5ab6671590721d197e95de3cea3R91
- https://github.com/scrapy/scrapy/pull/816/files#diff-017ca5ab6671590721d197e95de3cea3R130

/cc @curita
",nramirezuy,curita
874,2014-09-03 20:00:04,"I found that this information isn't being printed anymore.

The responsible of this bug is [this line](https://github.com/scrapy/scrapy/pull/816/files#diff-fee03a44ad4de98d9361d89947c8aba3R83), seems that `spider` is `None` on `eventDict` at the moment the components are instantiated.

I'm not sure how to fix it because I 'm not quite sure what it is attempting to block.

/cc @curita
",nramirezuy,curita
859,2014-08-15 18:43:53,"/cc @pablohoffman 
",dangra,pablohoffman
856,2014-08-14 09:18:51,"In https://github.com/scrapy/scrapy/pull/623 @dangra said there are some issues with `embed()`. If `embed` worked differently in 0.11 and we want to support IPython 0.11 then we can check IPython version. I'm also fine with dropping support for ancient IPythons.
",kmike,dangra
854,2014-08-14 17:30:22,"@dangra I've fixed both issues and rebased the new changes in the api-cleanup pull request on this branch.
",curita,dangra
830,2014-07-31 15:39:34,"~~**WARNING** This PR is developed on top of #827, the only change worth reviewing is 3f7f1574~~

Lot of people have made an effort to make Scrapy codebase Python3 compatible, but new changes needs manual review and sometimes new features or bugfixes are merged without considering Python3 compatibility.

This PR aims to minimize the regressions and to document what areas are pending to be migrated by collecting the list of yet to be migrated testcases at [tests/py3-ignores.txt](https://github.com/dangra/scrapy/blob/py3-tests/tests/py3-ignores.txt).

New test files are required to be python3 compatible or explicitly added to the ignore list.

/cc @kmike @felixonmars
",dangra,felixonmars
830,2014-07-31 15:39:34,"~~**WARNING** This PR is developed on top of #827, the only change worth reviewing is 3f7f1574~~

Lot of people have made an effort to make Scrapy codebase Python3 compatible, but new changes needs manual review and sometimes new features or bugfixes are merged without considering Python3 compatibility.

This PR aims to minimize the regressions and to document what areas are pending to be migrated by collecting the list of yet to be migrated testcases at [tests/py3-ignores.txt](https://github.com/dangra/scrapy/blob/py3-tests/tests/py3-ignores.txt).

New test files are required to be python3 compatible or explicitly added to the ignore list.

/cc @kmike @felixonmars
",dangra,kmike
828,2014-08-01 05:42:51,"@kmike take another look when you have a change. thx.
",dangra,kmike
825,2014-08-14 17:14:41,"@nramirezuy You're right that this change is backards incompatible, but I agree with @dangra that it is unlikely people are using False values (unless they make a mistake like `{'dont_cache': self.dont_cache}`). Separating ""user data"" meta from ""control meta"" may be worth investigating, but moving ""dont"" flags alone doesn't solve an issue with copying meta, and this new ""dont"" requires deprecation of old meta options, auto-inserting ""dont"" item to meta and changes to user code, so I'd prefer just to allow False values.

> but this must happen in a single change including docs and tests

@rocioar said she can work on it :)
",kmike,rocioar
818,2014-07-29 00:45:42,"@nramirezuy indeed, the implementation is kinda hacky but that's the idea. It needs a try/finally around the yield to avoid your bug.

I would like to hear more thoughts on this. @redapple @dangra 
",rolando,redapple
818,2014-07-29 00:45:42,"@nramirezuy indeed, the implementation is kinda hacky but that's the idea. It needs a try/finally around the yield to avoid your bug.

I would like to hear more thoughts on this. @redapple @dangra 
",rolando,dangra
816,2014-07-24 16:10:04,"A few comments raised by @kmike about if it is possible to use Scrapy as a library after this changes, and it directly touch this point ""Merge CrawlerProcess with Crawler"".

The idea is that after this changes users can import Scrapy as a library, instantiate Crawlers and run them on an already setup ioloop. It means the reactor is already up and it is managed by users code, not by the Crawler instance.

This is useful for example in a web service and starting Crawlers on demand along other things within the same python interpreter.
",dangra,kmike
816,2014-08-14 17:23:23,"@kmike @dangra I've implemented your suggestions.
",curita,kmike
809,2014-07-21 16:13:42,"@dangra @chekunkov  happy?
",nramirezuy,chekunkov
809,2014-07-21 16:13:42,"@dangra @chekunkov  happy?
",nramirezuy,dangra
809,2014-07-21 16:41:32,"> @dangra @chekunkov happy?

:smile_cat: 
",dangra,chekunkov
809,2014-07-21 16:41:32,"> @dangra @chekunkov happy?

:smile_cat: 
",dangra,dangra
803,2014-07-15 11:06:22,"So, some differences between `cStringIO.StringIO`, `six.BytesIO` and `io.BytesIO` (aka `StringIO.StringIO`):
1. Speed. `six.BytesIO` should be the slowest, `cStringIO.StringIO` should be the fastest, `io.BytesIO` should be in between (in Python 2.7). But this really depends on how are they used. I recall trying to switch one codebase from cStringIO to BytesIO, and the code became several times slower despite the fact both are written in C. StringIO was even slower. On the other hand, I tried to replicate the results now using %timeit and failed to do so (it looks like there is something wrong with my tests - six.BytesIO was several times faster than its competitors). 
2. When cStringIO is initialzed with data, it looses ""write"" method.
3. `io.BytesIO` raises an exception if unicode string is passed to it. `six.BytesIO` and `cStringIO.StringIO` let ASCII strings in.

I think that we shouldn't use `six.BytesIO`. It looks like a compatibility wrapper for Python < 2.6.

Tornado uses the following approach:



Which is wrong - it effectively means ""use io.BytesIO everywhere"" because BytesIO is available since Python 2.6, and Tornado doesn't support Python 2.5.

It seems we have 3 options:

1) use `io.BytesIO` everywhere;
2) use `io.BytesIO` where speed doesn't matter (e.g. tests or scrapy.responsetypes) and use `cStringIO` in 2.x where speed matters (after checking that it makes code faster);
3) use `cStringIO / io.BytesIO` fallback everywhere.

I'm leaning towards (1). I don't like (3) much because using `io.BytesIO` in tests is good - it will check that Scrapy code uses byte strings correctly. 

If some Scrapy code uses cStringIO - specific features it should be changed to work with io.BytesIO / io.StringIO because there is no cStringIO  in Python 3.x.

What do you think?

Also, //cc @dangra @pablohoffman 
",kmike,dangra
803,2014-07-15 11:06:22,"So, some differences between `cStringIO.StringIO`, `six.BytesIO` and `io.BytesIO` (aka `StringIO.StringIO`):
1. Speed. `six.BytesIO` should be the slowest, `cStringIO.StringIO` should be the fastest, `io.BytesIO` should be in between (in Python 2.7). But this really depends on how are they used. I recall trying to switch one codebase from cStringIO to BytesIO, and the code became several times slower despite the fact both are written in C. StringIO was even slower. On the other hand, I tried to replicate the results now using %timeit and failed to do so (it looks like there is something wrong with my tests - six.BytesIO was several times faster than its competitors). 
2. When cStringIO is initialzed with data, it looses ""write"" method.
3. `io.BytesIO` raises an exception if unicode string is passed to it. `six.BytesIO` and `cStringIO.StringIO` let ASCII strings in.

I think that we shouldn't use `six.BytesIO`. It looks like a compatibility wrapper for Python < 2.6.

Tornado uses the following approach:



Which is wrong - it effectively means ""use io.BytesIO everywhere"" because BytesIO is available since Python 2.6, and Tornado doesn't support Python 2.5.

It seems we have 3 options:

1) use `io.BytesIO` everywhere;
2) use `io.BytesIO` where speed doesn't matter (e.g. tests or scrapy.responsetypes) and use `cStringIO` in 2.x where speed matters (after checking that it makes code faster);
3) use `cStringIO / io.BytesIO` fallback everywhere.

I'm leaning towards (1). I don't like (3) much because using `io.BytesIO` in tests is good - it will check that Scrapy code uses byte strings correctly. 

If some Scrapy code uses cStringIO - specific features it should be changed to work with io.BytesIO / io.StringIO because there is no cStringIO  in Python 3.x.

What do you think?

Also, //cc @dangra @pablohoffman 
",kmike,pablohoffman
803,2014-07-15 13:59:27,"LGTM. I'll leave it to @dangra or @pablohoffman to merge.
",kmike,dangra
803,2014-07-15 13:59:27,"LGTM. I'll leave it to @dangra or @pablohoffman to merge.
",kmike,pablohoffman
798,2014-07-14 10:17:22,"Making changes to bundled 3rd-party libraries is controversial, but as per @dangra's comment (https://github.com/scrapy/scrapy/pull/431#discussion_r7131763) it is OK to modify pydispatch. So merging it. Thanks!
",kmike,dangra
795,2014-10-07 18:40:57,"what do you think about this one @dangra?
",pablohoffman,dangra
794,2014-07-10 16:13:02,"I was doing some testing on Scrapy shell and I found this bug (in Ipython standalone works OK):



I have confirmed this issue with @kmike and @kalessin.
",andrix,kalessin
794,2014-07-10 16:13:02,"I was doing some testing on Scrapy shell and I found this bug (in Ipython standalone works OK):



I have confirmed this issue with @kmike and @kalessin.
",andrix,kmike
789,2014-07-09 04:54:27,"citing @cyberplant and myself from chat history:


",dangra,cyberplant
789,2014-07-09 06:02:53,"After digging a bit into travis source code (thx @omab), it seems travis-ci converts the condition to a bash test expression at https://github.com/travis-ci/travis-build/blob/ae614eb090014c815b121a9739ad6296aa31d743/lib/travis/build/shell/node.rb#L81

so I tried this condition:



output:


",dangra,omab
780,2014-07-02 22:26:30,"There is either a bug or some stray code in https://github.com/scrapy/scrapy/blob/master/scrapy/contrib/linkextractors/lxmlhtml.py#L37: `tag = _nons(el.tag)` local variable is not used, and so `_nons` function is also unused. @redapple - what was the intended behavior?
",kmike,redapple
778,2015-07-23 13:43:57,"We (@dangra, @eliasdorneles and me) decided to go with `to_bytes` and `to_unicode` names.
",kmike,dangra
778,2015-07-23 13:43:57,"We (@dangra, @eliasdorneles and me) decided to go with `to_bytes` and `to_unicode` names.
",kmike,eliasdorneles
772,2014-07-03 20:31:49,"+0.5 :)

I like this proposal but I would also like to see item loaders being easier to use (or even replaced by something better, if such thing exists - @Digenis still waiting to hear more about those ""python libraries for FP"" that you mentioned on #741). The documentation should also outline the differences in both approaches and explain when is better to use one or the other.

Item processors were designed with a more ""declarative"" API in mind (_what_ to map from response to fields, not _how_ to do it) and that naturally adds some overhead when combined with a more imperative API like selectors.
",pablohoffman,Digenis
770,2014-06-27 14:35:19,"fix for #769
/cc @kmike 
",dangra,kmike
770,2014-06-27 14:36:30,"/cc @curita
",dangra,curita
768,2014-07-02 13:16:56,"I was talking with @tpeng and not sure which would be the best direction to steer scrapy contracts towards. I've written a short description of some options: https://gist.github.com/alexcepoi/200128ad93648825e09f

They are not mutually exclusive. I personally like 1 and 3, not sure about 2.

@dangra @pablohoffman care to give an opinion?
",alexcepoi,dangra
768,2014-07-02 13:16:56,"I was talking with @tpeng and not sure which would be the best direction to steer scrapy contracts towards. I've written a short description of some options: https://gist.github.com/alexcepoi/200128ad93648825e09f

They are not mutually exclusive. I personally like 1 and 3, not sure about 2.

@dangra @pablohoffman care to give an opinion?
",alexcepoi,tpeng
768,2014-07-02 13:16:56,"I was talking with @tpeng and not sure which would be the best direction to steer scrapy contracts towards. I've written a short description of some options: https://gist.github.com/alexcepoi/200128ad93648825e09f

They are not mutually exclusive. I personally like 1 and 3, not sure about 2.

@dangra @pablohoffman care to give an opinion?
",alexcepoi,pablohoffman
768,2014-08-01 06:00:47,"before merging I would like to gather more feedback from other people involved in the original _contracts_ idea. /cc @pablohoffman 
",dangra,pablohoffman
761,2014-06-25 17:36:36,"on top of the amazing work of @redapple in #559, this PR includes an alias to LxmlLinkExtractor importable from `scrapy.contrib.linkextractors.LinkExtractor` and update docs. 
",dangra,redapple
741,2014-06-03 16:48:39,"LGTM

/cc @redapple @pablohoffman 
",dangra,redapple
741,2014-06-03 16:48:39,"LGTM

/cc @redapple @pablohoffman 
",dangra,pablohoffman
728,2014-05-21 17:05:43,"https://github.com/scrapy/scrapy/blob/master/scrapy/utils/python.py#L149

Today I was working on a project were I have to skip the first item of a list, and then join the rest. Instead of writing the typical slice I tried something much more good looking `Compose(itemgetter(slice(1, None)), Join())` but I found out this maximum recursion. I did some research and ask @dangra about it, but nothing came up.
I think the main problem is that `inspect` isn't able recognize `itemgetter` as `something`.



EDIT: Looks like the reason was C functions weren't covered by inspect module until Python 3.4 (http://bugs.python.org/issue17481)
",nramirezuy,dangra
722,2015-04-01 15:24:00,"@kmike what do you think?
",nramirezuy,kmike
721,2014-06-03 13:58:58,"there are three changes here:
1. shortcut check for empty response in lxml linkextractor.
2. Wider response class to TextResponse from HtmlResponse
3. Moving `seen` set to spider attribute class _and_ filtering by urls instead of Link instances.

All of them needs tests and possible updating docs.

(1) looks harmless to me, I'm ok to merge it until  #528 get resolved.
(2) may cause problems to existent spiders and can degrade performance for those spiders relying on always extracting links from HtmlResponses. We can make this class configurable as spider attribute defaulting to HtmlResponse.
(3) a spider attribute to store seen urls is a bad idea from memory point of view, and we are relying on core DuplicateFilter component to filter duplicate requests. Also comparing by `link.url` it is not the same than comparing links instances. This change is not backward compatible, can you explain what we gain by merging it?

/CC @nramirezuy @redapple @kmike
",dangra,nramirezuy
721,2014-06-03 13:58:58,"there are three changes here:
1. shortcut check for empty response in lxml linkextractor.
2. Wider response class to TextResponse from HtmlResponse
3. Moving `seen` set to spider attribute class _and_ filtering by urls instead of Link instances.

All of them needs tests and possible updating docs.

(1) looks harmless to me, I'm ok to merge it until  #528 get resolved.
(2) may cause problems to existent spiders and can degrade performance for those spiders relying on always extracting links from HtmlResponses. We can make this class configurable as spider attribute defaulting to HtmlResponse.
(3) a spider attribute to store seen urls is a bad idea from memory point of view, and we are relying on core DuplicateFilter component to filter duplicate requests. Also comparing by `link.url` it is not the same than comparing links instances. This change is not backward compatible, can you explain what we gain by merging it?

/CC @nramirezuy @redapple @kmike
",dangra,redapple
721,2014-06-03 13:58:58,"there are three changes here:
1. shortcut check for empty response in lxml linkextractor.
2. Wider response class to TextResponse from HtmlResponse
3. Moving `seen` set to spider attribute class _and_ filtering by urls instead of Link instances.

All of them needs tests and possible updating docs.

(1) looks harmless to me, I'm ok to merge it until  #528 get resolved.
(2) may cause problems to existent spiders and can degrade performance for those spiders relying on always extracting links from HtmlResponses. We can make this class configurable as spider attribute defaulting to HtmlResponse.
(3) a spider attribute to store seen urls is a bad idea from memory point of view, and we are relying on core DuplicateFilter component to filter duplicate requests. Also comparing by `link.url` it is not the same than comparing links instances. This change is not backward compatible, can you explain what we gain by merging it?

/CC @nramirezuy @redapple @kmike
",dangra,kmike
716,2015-03-13 22:26:31,"In the overview page (even after https://github.com/scrapy/scrapy/pull/1023) there is a gotcha: `-o scraped_data.json` will produce invaid JSON if called second time. We won't have to document this wart in the tutorial if this issue is fixed. So this feature is nice to have in Scrapy 1.0. 

This PR looks like a good start. I think after addressing @nramirezuy's comments and rebasing on master it can be merged.

//cc @pablohoffman @Curita @dangra
",kmike,nramirezuy
716,2015-03-13 22:26:31,"In the overview page (even after https://github.com/scrapy/scrapy/pull/1023) there is a gotcha: `-o scraped_data.json` will produce invaid JSON if called second time. We won't have to document this wart in the tutorial if this issue is fixed. So this feature is nice to have in Scrapy 1.0. 

This PR looks like a good start. I think after addressing @nramirezuy's comments and rebasing on master it can be merged.

//cc @pablohoffman @Curita @dangra
",kmike,dangra
716,2015-03-13 22:26:31,"In the overview page (even after https://github.com/scrapy/scrapy/pull/1023) there is a gotcha: `-o scraped_data.json` will produce invaid JSON if called second time. We won't have to document this wart in the tutorial if this issue is fixed. So this feature is nice to have in Scrapy 1.0. 

This PR looks like a good start. I think after addressing @nramirezuy's comments and rebasing on master it can be merged.

//cc @pablohoffman @Curita @dangra
",kmike,pablohoffman
710,2014-05-18 10:03:28,"@dangra can you look at this? change is simple and seems to be useful in some situations.
",chekunkov,dangra
708,2014-07-03 21:12:17,"@pablohoffman: No, this has to be fixed in the improvements @curita is working on. IMO the place to handle this error is https://github.com/scrapy/scrapy/blob/master/scrapy/crawler.py#L66. 
",dangra,curita
700,2014-04-24 18:46:25,"I've followed @kmike's decision to backport XXE fixes (#695) since they are pretty similar: both introduce a backwards incompatible change for security reasons.

Unless bind address change is a particularly undesirable breaking change, I think it'd be best to make the same decision on this and #695. If there's going to be a possibly-breaking release for 0.18 with XXE, might as well release this too.
",allait,kmike
699,2014-04-17 14:45:32,"As discussed with @shane42 and @kmike.
I'll also send separate PRs for 0.18, 0.20 and 0.22 branches.
",allait,kmike
697,2014-04-17 13:54:22,"Since @kmike created a mailing list for security issues we should direct people there from the documentation.
Wording taken mostly from the Django docs: https://docs.djangoproject.com/en/dev/internals/contributing/bugs-and-features/#reporting-security-issues
",allait,kmike
690,2014-04-14 23:50:25,"@kmike ^^
",dangra,kmike
686,2014-04-09 22:20:20,"TODO:
- [x] persuade everyone `response.xpath()`, `response.css()` ~~and `response.re()`~~ are cute;
- [ ] add docs;
- [ ] fix tutorial?

/cc @dangra, @pablohoffman, @redapple, @nramirezuy, @shane42, @nyov and everybody else who is interested in API changes.

See #554.
",kmike,redapple
686,2014-04-09 22:20:20,"TODO:
- [x] persuade everyone `response.xpath()`, `response.css()` ~~and `response.re()`~~ are cute;
- [ ] add docs;
- [ ] fix tutorial?

/cc @dangra, @pablohoffman, @redapple, @nramirezuy, @shane42, @nyov and everybody else who is interested in API changes.

See #554.
",kmike,nramirezuy
686,2014-04-09 22:20:20,"TODO:
- [x] persuade everyone `response.xpath()`, `response.css()` ~~and `response.re()`~~ are cute;
- [ ] add docs;
- [ ] fix tutorial?

/cc @dangra, @pablohoffman, @redapple, @nramirezuy, @shane42, @nyov and everybody else who is interested in API changes.

See #554.
",kmike,nyov
686,2014-04-09 22:20:20,"TODO:
- [x] persuade everyone `response.xpath()`, `response.css()` ~~and `response.re()`~~ are cute;
- [ ] add docs;
- [ ] fix tutorial?

/cc @dangra, @pablohoffman, @redapple, @nramirezuy, @shane42, @nyov and everybody else who is interested in API changes.

See #554.
",kmike,dangra
686,2014-04-09 22:20:20,"TODO:
- [x] persuade everyone `response.xpath()`, `response.css()` ~~and `response.re()`~~ are cute;
- [ ] add docs;
- [ ] fix tutorial?

/cc @dangra, @pablohoffman, @redapple, @nramirezuy, @shane42, @nyov and everybody else who is interested in API changes.

See #554.
",kmike,pablohoffman
686,2014-04-11 19:25:15,"@kmike : we (@redapple and me) were talking about doing this pull request in two steps, one to add the lazy selector and another for the response shortcuts.

the shortcuts are the problem that delay merging this pull request, you have to persuade us yet :-)

@nramirezuy : what do you think? 

we are looking for a pycon2014 release!
",dangra,nramirezuy
686,2014-04-11 19:25:15,"@kmike : we (@redapple and me) were talking about doing this pull request in two steps, one to add the lazy selector and another for the response shortcuts.

the shortcuts are the problem that delay merging this pull request, you have to persuade us yet :-)

@nramirezuy : what do you think? 

we are looking for a pycon2014 release!
",dangra,redapple
670,2014-03-28 08:41:33,"@kmike @dangra @pablohoffman can you merge?
",umrashrf,pablohoffman
670,2014-03-28 08:41:33,"@kmike @dangra @pablohoffman can you merge?
",umrashrf,dangra
660,2014-03-21 17:23:10,"It took a while for the new commit to arrive, but it has! @dangra was right about having the Content-Type and Content-Encoding checks happen after the processing. I tried following the working of other tests as closely as possible though and all should be good now.
",rubenvereecken,dangra
650,2014-03-17 20:16:21,"LGTM. /cc @dangra 
",kmike,dangra
644,2014-03-16 11:11:28,"I have discovered `heapq.nlargest(100, gc.get_objects(), sys.getsizeof)`  gives the same result as @kmike suggested. By having said that, the only advantage of using find_biggest_objects could be `ignore` parameter. It is open to discussion whether this function is still useful or not, if not this issue could be closed.
",sardok,kmike
621,2016-10-17 09:15:05,"There was a recent change in ""master"" about this: https://github.com/scrapy/scrapy/pull/2190/files
I think we can close this. what do you think @nyov , @stummjr ?
",redapple,stummjr
614,2016-09-14 11:11:07,"Do we want to improve this behavior?
Or is it a matter of making the documentation explicit about this?
I'm not an item loader user myself so I don't have an opinion on this one.

/cc @kmike , @eliasdorneles , @pawelmhm 
",redapple,pawelmhm
614,2016-09-14 11:11:07,"Do we want to improve this behavior?
Or is it a matter of making the documentation explicit about this?
I'm not an item loader user myself so I don't have an opinion on this one.

/cc @kmike , @eliasdorneles , @pawelmhm 
",redapple,eliasdorneles
614,2016-09-14 11:11:07,"Do we want to improve this behavior?
Or is it a matter of making the documentation explicit about this?
I'm not an item loader user myself so I don't have an opinion on this one.

/cc @kmike , @eliasdorneles , @pawelmhm 
",redapple,kmike
607,2015-03-24 03:03:43,"Is this still relevant and mergeable?
@kmike, seems like docs are your specialty at the moment :D - have a quick look?
",nyov,kmike
600,2014-02-19 23:48:24,"I'm not that familiar with python-requests API. @dangra, @kmike, any thoughts?
",redapple,dangra
600,2014-02-19 23:48:24,"I'm not that familiar with python-requests API. @dangra, @kmike, any thoughts?
",redapple,kmike
591,2014-02-12 05:20:46,"A rather more complex system.

For the uninitiated scrapy-user, per-addon post-initialization checks may not be safe enough,
if they happy-go-lucky mash up whatever addons they find somewhere.

Since add-ons would encapsulate the whole spectrum of scrapy hooks and settings,
without regards to existing settings or other addons, strange cross-dependencies could occur.

For a truly robust addon system we may need something like a dependency tree in scrapy components?
Maybe I'm overthinking this, but one add-on may need to replace an existing component which would break another add-on.

Mostly this won't be an issue for simple pipeline and middleware addons,
but if one needs to replace itself with the enabled default, or is an exclusive component like `SCHEDULER`, this might break other, unsuspecting, addon bundles in unpredictable ways.

Now the addon could be paranoid, and maybe check for all the settings or extensions actually being what it would expect (e.g. ""I need httpcache_storage to be FilesystemCacheStorage""),
but even then this would be error prone since it can't possibly predict the whole target system setup (oops, my httpcache might not implement httpcache_storage).

But I wonder if it makes sense to address this issue.
I can't really envision this, implementation-wise, but if we could say
`""ExtensionManager implements extensions""`,
`""FeedExporter requires extensions""`
or
`""Scheduler implements scheduler, has DUPEFILTER_CLASS""`,
`""RFPDupeFilter requires DUPEFILTER_CLASS""`
and then I am using a `CustomFrontierScheduler` which exposes no `DUPEFILTER_CLASS`,
my add-on, bundling `RFPDupeFilter` and setting `DUPEFILTER_CLASS`, may fail in a defined manner.

But this could also work instead of fixed priority-based ordering for middlewares/pipelines/extensions, which would seem a more robust base for complex add-on trees:
`""HttpCacheMiddleware implements downloadermw, provides httpcache, has httpcache_storage""`,
`""HttpCacheMiddleware runs-after $most-dmws""`, 
`""HttpCacheMiddleware runs-before (nothing)""`,
where `$most-dmws` is a meta keyword and defines something like
`""$most-dmws runs-after httpauth, redirect, cookies""` (rather, more $keyword ""levels"" here)
`""httpauth runs-after robotstxt, runs-before httpproxy""`
and so on.

With these hints, scrapy could then auto-sort middlewares, extensions,... and find unresolvable or circular dependencies in other components.
And my component could say ""I depend on (any) httpcache"" or ""I implement a mongo-backed stats system, I depend on mongocache, fail/deactivate if it's not in use"" (`EXTENSION` depending on a `MIDDLEWARE`)
In such a manner, there should be no limit to cascading dependencies and throwing all manner of well-defined modules together in a project and switch on/off whole dependency-trees with only some settings.

oh, and /cc @pablohoffman
",nyov,pablohoffman
591,2015-03-25 20:53:28,"> 1) We can define a single class AddOnManager which will tackle the issue of importing modules for every such extension/middleware/pipeline etc.

+1 seems reasonable 

> 2) Users will have to define the name of the extension and any relevant settings (db username, password etc) in the settings.py. We can have all extensions clubbed under ADD_ONS dict.

I understand that adding an `ADD_ONS` setting in `settings.py` is more flexible (we won't have to implement anything new to handle it and we can use python directly), but I don't think we need this kind of flexibility, just a section to declare each addon and its settings is enough. `scrapy.cfg` is the first configuration file read (that's where we look for the `settings.py` path actually), so configuring the addons there could let us set extensions before `settings.py` is read. For example, a new extension could set a different `Settings` class. Not sure if this is needed, we can evaluate it, but I think I prefer sticking with `scrapy.cfg` if there are no apparent drawbacks.

> 3) To the _get_mwlist_from_settings method present in all the MiddlewareManagers, we add a call to AddOnManager
> 
> return build_components_list(...) + AddOnManager.get_add_ons(cls.name, settings, crawler)
> 
> The get_add_ons function will call the addon_configure function for each add-on and after finishing all the add-ons, check the crawler_ready function. The cls.name field will be used to match add-ons built for that component.

I haven't made a in-depth review yet, but I think this has a few implementation possibilities. For example, we could call in `MiddlewareManager._get_mwlist_from_settings` some method in AddOnManager that just calls `addon_configure` in all addons (though it doesn't even need to be called here, we can call it when we're updating the settings outside MiddlewareManager), and then call `crawler_ready` in `MiddlewareManager.from_settings`, when it's checking what addons are configured (maybe this could be outside the MidlewareManager too).

I'd personally like to keep the MiddlewareManagers as they are, say configuring the addons before or after running the managers (some managers can be configured by settings, I think that's my main reason for wanting to keep them untouched, that way we can maintain backward compatibility with user defined managers), but I'm not sure this is possible.  

> 4) We mandate each add-on to be an instance of an abstract AddOn class. This class defines functions such as update_settings (to change the value of a setting and mark it as unchangeable by future add-ons, useful incase of conflict; also to prefix addon name to setting name) , report_errors etc.

Makes sense.

> 5) Each add-on must add its name, version and requirements to the global settings. This way all add-ons which are instantiated after this, can access the settings to satisfy their dependencies. Any add-on requiring a module not present in crawler raises an error in the crawler_ready function. Any conflict in settings (between two add-ons, will be caught either in addon_configure or crawler_ready)

Makes sense, the AddOn class could have those ""name"", ""version"" and ""requirements"" as class attributes, and some method in AddOnManager could takes those and construct a dependency tree (I'd use an attribute in the crawler for this instead of the global settings, since those are user defined).

As @nyov said, it's kind of hard to get a robust dependency manager since there are a lot of components that could interact in different ways, maybe we could continue with the approach described in the SEP (setting the order of the middlewares in their '<component_MIDDLEWARE>' setting and checking dependencies manually in `crawler_ready`) for this GSoC unless we have spare time to implement it.

On a really side note, I hate the name ""crawler_ready"" :P if it's going to be used just for checking dependencies, let's change it to ""check_dependencies"" or ""check_configuration"" or something along those lines. I like changing ""addon_configure"" to ""update_settings"" as well, that's how it's called in the Spider class.

/cc @nramirezuy I recall you wanted to let different components update the settings, this idea is going to do that.
",curita,nramirezuy
586,2014-07-23 15:52:03,"@kmike You been working on the docs, is this still useful? Or should we close it?
",nramirezuy,kmike
586,2015-03-25 13:26:47,"I was discussing doc changes with @eliasdorneles, and it became clear ""screen scraping"" is confusing. Wikipedia [article](http://en.wikipedia.org/wiki/Data_scraping#Screen_scraping) thinks it is something related to OCR, and the term is not generally well-known. I haven't heard it until I started to work with Scrapy :) 

What about using ""data extraction"" instead?
Thoughts?
",kmike,eliasdorneles
586,2015-04-21 19:58:18,"@pablohoffman made a good point: if we move scrapy.selectors to a separate library (https://github.com/scrapy/scrapy/pull/1007) scrapy itself won't be focused on data extraction. So +1 to Web Scraping.
",kmike,pablohoffman
584,2014-02-05 18:17:03,"/cc @kmike as you wrote the initial version of this util 
",dangra,kmike
578,2014-04-15 14:33:21,"See also @nyov's comments: https://github.com/scrapy/scrapy/issues/568#issuecomment-34887656 and https://github.com/scrapy/scrapy/issues/568#issuecomment-34898235. @nramirezuy - are you proposing something similar?
",kmike,nyov
570,2014-01-31 17:18:17,"this work unblocked the queue of PRs.
thanks @redapple and @kmike! 
",dangra,kmike
568,2014-01-30 20:42:31,"/cc our naming tsar @pablohoffman 
",kmike,pablohoffman
564,2014-02-03 03:58:39,"I find negative forms harder to follow too, do you have something against using @redapple version?
",dangra,redapple
564,2014-02-05 10:22:36,"The text for deny is wrong:
""If given, (absolute) URLs that match any of the regular expressions will
be discarded. If not given (or empty), it will match all links.""

If deny is empty, 'it' matches all links.
But links which match will be discarded.
Therefore, all links are discarded.
This isn't the behaviour of deny!

I'm not sure whether this is an issue with ""it"" or ""match"" in ""it will
match all links.""

Mostly I'd prefer double-negatives to incorrect documentation.
Tried to rework, too.

On Mon, Feb 3, 2014 at 3:59 AM, Daniel Graña notifications@github.comwrote:

> I find negative forms harder to follow too, do you have something against
> using @redapple https://github.com/redapple version?
> 
> ## 
> 
> Reply to this email directly or view it on GitHubhttps://github.com/scrapy/scrapy/pull/564#issuecomment-33923398
> .
",scraperdragon,redapple
563,2014-02-01 18:24:54,"Looks much better now.

Implementation-wise, LGTM - /cc @dangra @kmike @nramirezuy @redapple

What about updating the docs to reflect this accordingly?
",pablohoffman,nramirezuy
563,2014-02-01 18:24:54,"Looks much better now.

Implementation-wise, LGTM - /cc @dangra @kmike @nramirezuy @redapple

What about updating the docs to reflect this accordingly?
",pablohoffman,redapple
563,2014-02-01 18:24:54,"Looks much better now.

Implementation-wise, LGTM - /cc @dangra @kmike @nramirezuy @redapple

What about updating the docs to reflect this accordingly?
",pablohoffman,dangra
563,2014-02-01 18:24:54,"Looks much better now.

Implementation-wise, LGTM - /cc @dangra @kmike @nramirezuy @redapple

What about updating the docs to reflect this accordingly?
",pablohoffman,kmike
562,2014-01-25 03:03:23,"> It's a bummer that the most widely used link extractor and the only one that supports the handy argument restrict_xpaths fails in such simple and very common case.

Let's put our eyes on #559 ! (@redapple) :)
",dangra,redapple
562,2014-01-25 03:38:14,"/cc @stav as you originally proposed #285 
",dangra,stav
559,2014-02-01 22:47:05,"@dangra @kmike @nramirezuy @pablohoffman @darkrho , any thoughts so far?

The extractor could even be renamed to `SelectorLinkExtractor` but users may expect that it takes a `Selector` parameter somehow...
",redapple,pablohoffman
559,2014-02-01 22:47:05,"@dangra @kmike @nramirezuy @pablohoffman @darkrho , any thoughts so far?

The extractor could even be renamed to `SelectorLinkExtractor` but users may expect that it takes a `Selector` parameter somehow...
",redapple,nramirezuy
559,2014-02-01 22:47:05,"@dangra @kmike @nramirezuy @pablohoffman @darkrho , any thoughts so far?

The extractor could even be renamed to `SelectorLinkExtractor` but users may expect that it takes a `Selector` parameter somehow...
",redapple,dangra
559,2014-02-01 22:47:05,"@dangra @kmike @nramirezuy @pablohoffman @darkrho , any thoughts so far?

The extractor could even be renamed to `SelectorLinkExtractor` but users may expect that it takes a `Selector` parameter somehow...
",redapple,kmike
559,2014-06-20 11:28:52,"@kmike , @darkrho , @nramirezuy , @pablohoffman , probably room for cleanup but this should contain all fixes from your comments.
Question: go that route? or add another extractor based on `Selector`?
",redapple,pablohoffman
553,2014-01-22 11:57:43,"Collecting dupe stats could be default indeed. /cc @dangra @pablohoffman ?
",redapple,dangra
553,2014-01-22 11:57:43,"Collecting dupe stats could be default indeed. /cc @dangra @pablohoffman ?
",redapple,pablohoffman
549,2014-01-20 16:31:48,"/cc @pablohoffman @redapple @kmike 

review and feel free to merge if fine for you.
",dangra,kmike
549,2014-01-20 16:31:48,"/cc @pablohoffman @redapple @kmike 

review and feel free to merge if fine for you.
",dangra,redapple
549,2014-01-20 16:31:48,"/cc @pablohoffman @redapple @kmike 

review and feel free to merge if fine for you.
",dangra,pablohoffman
543,2014-01-17 15:25:57,"/cc @pablohoffman @redapple @kmike @nramirezuy 

Any typo or wording improvement is welcome.
",dangra,kmike
543,2014-01-17 15:25:57,"/cc @pablohoffman @redapple @kmike @nramirezuy 

Any typo or wording improvement is welcome.
",dangra,nramirezuy
543,2014-01-17 15:25:57,"/cc @pablohoffman @redapple @kmike @nramirezuy 

Any typo or wording improvement is welcome.
",dangra,redapple
543,2014-01-17 15:25:57,"/cc @pablohoffman @redapple @kmike @nramirezuy 

Any typo or wording improvement is welcome.
",dangra,pablohoffman
541,2014-01-17 00:09:02,"To be or not to be part of 0.22? /cc @pablohoffman @shane42 @redapple
",dangra,redapple
541,2014-01-17 00:09:02,"To be or not to be part of 0.22? /cc @pablohoffman @shane42 @redapple
",dangra,pablohoffman
525,2014-01-14 02:25:47,"LGTM. /cc @pablohoffman @kmike 
",dangra,kmike
525,2014-01-14 02:25:47,"LGTM. /cc @pablohoffman @kmike 
",dangra,pablohoffman
524,2014-01-10 18:42:29,"Help new users find the right module to import for the code examples in documentation.  There are multiple import paths, so it also helps to point new users tot he canonical path (as @pablohoffman did for me).
",hobson,pablohoffman
519,2014-01-08 23:02:46,"/cc @kmike 
",dangra,kmike
519,2014-01-09 07:30:56,"I agree that the original idea of showing a warning for each subclass is not great - that's not how warnings usually work. But I think that `create_deprecated_class` is a wrong place to control how much warnings to show - what if user wants all these warnings (to find all places the code needs to be updated)?

Maybe, as @pablohoffman said, we can use http://docs.python.org/2/library/warnings.html#warnings.filterwarnings instead?

Also, I think it is fine to show multiple warnings on instantiation - by default Python suppresses warnings only for duplicate lineno/module. It won't be annoying for `scrapy list` because only single spider is instantiated there. It will be annoying in tests as it should.
",kmike,pablohoffman
510,2013-12-31 18:18:18,"@kmike: Github doesn't allow me to submit a PR against your scrapy fork (#501), so I did against main repo, take a look at the changes and pull if you like them.

thanks. 
",dangra,kmike
508,2014-08-04 15:02:40,"> Why ""|"" is in reserved characters?, When isn't mentioned here

@kmike addressed it in scrapy/w3lib#25 but still I don't think the fix (and the problem) is related to this issue.
",dangra,kmike
506,2013-12-30 16:45:40,"This looks good. Thanks @dangra and @ahbeng!
",kmike,ahbeng
504,2013-12-29 19:59:49,"Return args of underlying function, as in [sage.misc.sageinspect.sage_getargspec](http://trac.sagemath.org/browser/src/sage/misc/sageinspect.py) - simplest solution for scrapy's needs in practice, as pointed to by @kmike in [#368](https://github.com/scrapy/scrapy/issues/368#issuecomment-22816747).
",ahbeng,kmike
499,2013-12-31 18:51:05,"LGTM. @pablohoffman anything blocking to merge this? 
",dangra,pablohoffman
494,2013-12-18 23:20:52,"Hey,

I think that @redapple is right in https://github.com/scrapy/scrapy/issues/488 that base spider template needs more imports by default, but IMHO introducing more code generation is not a solution.

Scrapy top-level namespace is a bit funky now: 



it contains boto and django, but doesn't contain shortcuts like Request, Selector and BaseSpider. What do you think about making them available as `scrapy.Request` etc and removing unintended exports? 

Currently to write a basic spider user has to remember a lot of import locations. Scrapy docs omit imports sometimes (see #493), this also doesn't help.

My current list is BaseSpider (why isn't it named Spider, by the way?), Request, FormRequest and Selector, but I don't see much downsides in being more generous (Item? Field?). 
",kmike,redapple
492,2013-12-20 12:21:10,"Note: after a quick discussion with @pablohoffman, we settled on exposing the ubuntu packages under a single url like:


",dangra,pablohoffman
490,2013-12-20 11:38:59,"LGTM

anyone else wants to take a look @pablohoffman @kmike @nramirezuy ?
",dangra,kmike
490,2013-12-20 11:38:59,"LGTM

anyone else wants to take a look @pablohoffman @kmike @nramirezuy ?
",dangra,nramirezuy
490,2013-12-20 11:38:59,"LGTM

anyone else wants to take a look @pablohoffman @kmike @nramirezuy ?
",dangra,pablohoffman
485,2013-12-16 13:02:17,"LGTM

/cc @pablohoffman 
",dangra,pablohoffman
483,2014-07-24 22:48:29,"@dangra, does @curita's changes fix this?
",nramirezuy,curita
482,2013-12-05 10:55:16,"Hi,

@nramirezuy and me were debugging memory issue with one of the spiders  some time ago, and it seems to be caused by ImagesPipeline + [S3FilesStore](https://github.com/scrapy/scrapy/blob/master/scrapy/contrib/pipeline/files.py#L66). I haven't confirmed that it was the cause of memory issue, this ticket is based solely on reading the source code.

FilesPipeline reads the whole file to memory and then defers the uploading to thread (via `S3FilesStore.persist_file`, passing file contents as bytes). So there could be many files loaded to memory at the same time, and as soon as files are downloaded faster than they are are uploaded to s3, memory usage will grow. This is not unlikely IMHO because s3 is not super-fast. For ImagesPipeline it is worse because it uploads not only the image itself, but also the generated thumbnails.

I think S3FilesStore should persist files to temporary location before uploading them to S3 (at least optionally). This would allow streaming files without storing them in memory. 
",kmike,nramirezuy
481,2013-12-27 16:08:08,"@dangra  Is anything wrong with this?
",nramirezuy,dangra
469,2013-11-21 18:10:42,"Having ""x-gzip"" as first encoding in ""Accept-Encoding"" header seems to cause encoding issues with some (IIS?) web servers (response not compressed, valid but not compressed)

""x-gzip"" is not included in ""Accept-Encoding"" headers sent by latest Chrome or Firefox releases

After discussing with @dangra and @pablohoffman (https://github.com/scrapy/scrapy/commit/81fbe8c9a4b3940cd1cdf27ff0848a7ec5360512#commitcomment-4617865) the consensus is to remove it in requests still accepting it in responses

Moving ""x-gzip"" to the end of the accepted encoding list also works (with the server exhibiting the issue)
",redapple,dangra
469,2013-11-21 18:10:42,"Having ""x-gzip"" as first encoding in ""Accept-Encoding"" header seems to cause encoding issues with some (IIS?) web servers (response not compressed, valid but not compressed)

""x-gzip"" is not included in ""Accept-Encoding"" headers sent by latest Chrome or Firefox releases

After discussing with @dangra and @pablohoffman (https://github.com/scrapy/scrapy/commit/81fbe8c9a4b3940cd1cdf27ff0848a7ec5360512#commitcomment-4617865) the consensus is to remove it in requests still accepting it in responses

Moving ""x-gzip"" to the end of the accepted encoding list also works (with the server exhibiting the issue)
",redapple,pablohoffman
467,2013-11-21 15:47:03,"Please, document settings added by #466.

/cc @kalessin
",dangra,kalessin
461,2013-11-16 11:00:08,"Thanks @kmike , I was thinking the same actually.
What do others think?

@pablo, @dangra any thoughts?
",redapple,dangra
461,2013-11-25 17:31:12,"Nice to see this one merged!

@redapple next time ping me as @pablohoffman ;)
",pablohoffman,pablohoffman
457,2013-11-16 16:35:33,"That's what I think as well. But I'd like to hear what @pablohoffman or @dangra have to say about it.
",barraponto,dangra
457,2013-11-20 18:28:27,"That is the current behavior when `allowed_domains` spider attribute is set (default spider templates does)

I admit this is not the strong security-by-default you are expecting but It minimizes this issue for newbies again.

@kmike, @redapple, @nramirezuy  what do you think? 
",dangra,nramirezuy
457,2013-11-20 18:28:27,"That is the current behavior when `allowed_domains` spider attribute is set (default spider templates does)

I admit this is not the strong security-by-default you are expecting but It minimizes this issue for newbies again.

@kmike, @redapple, @nramirezuy  what do you think? 
",dangra,redapple
457,2013-11-20 18:28:27,"That is the current behavior when `allowed_domains` spider attribute is set (default spider templates does)

I admit this is not the strong security-by-default you are expecting but It minimizes this issue for newbies again.

@kmike, @redapple, @nramirezuy  what do you think? 
",dangra,kmike
456,2016-09-16 14:35:22,"@dangra @kmike @eliasdorneles @lopuhin @pawelmhm , what do you think?
Should we plan on supporting this?
",redapple,pawelmhm
456,2016-09-16 14:35:22,"@dangra @kmike @eliasdorneles @lopuhin @pawelmhm , what do you think?
Should we plan on supporting this?
",redapple,lopuhin
456,2016-09-16 14:35:22,"@dangra @kmike @eliasdorneles @lopuhin @pawelmhm , what do you think?
Should we plan on supporting this?
",redapple,eliasdorneles
456,2016-09-16 14:35:22,"@dangra @kmike @eliasdorneles @lopuhin @pawelmhm , what do you think?
Should we plan on supporting this?
",redapple,kmike
447,2014-07-29 17:02:29,"@dangra does this have anything to fix?
",nramirezuy,dangra
438,2013-10-23 16:55:04,"@pablohoffman : is `from_settings` deprecated in favor of `from_crawler` or not? may time to add a deprecation warning to components instanciating using `from_settings`.
",dangra,pablohoffman
434,2014-07-24 23:07:37,"Was there a reason for this to not get merged? @pablohoffman @dangra 
",nramirezuy,dangra
426,2013-10-15 12:28:54,"hey, before I merge I want to bring two naming changes to discussion
1. This PR replaces `hxs` and `xxs` by `ss` in docs, spiders code and shell 
   
   @pablohoffman suggested using just `s`, I find `ss` less namespace clashing specially on shell, and easier to grep for. which one do you prefer?
2. `Selector` class grow a new constructor argument named `contenttype`, it chooses the parser + serialization-method + css-translation flavor to use with the response.
   
   To force XML specifics, you instanciate the selector like:
   
    `ss = Selector(text='<tag>...</tag>', contenttype='xml')`, 
   
   and similarly for HTML using `contenttype='html'`
   
   The name of this new argument is under discussion, so far the options are: `contenttype`, `flavor` or `parser`.
   I think it is important to get this right, it is going to be part of the public API.
   which one do you prefer?  
",dangra,pablohoffman
413,2014-07-29 16:46:27,"@dangra Anything preventing this to be merged? Besides `__future__.print_function`
",nramirezuy,dangra
411,2013-10-03 00:59:48,"Symptoms are the same as in https://github.com/scrapy/scrapy/issues/83, but the reason is different, so I opened a new ticket. This issue starts to happen after this commit: https://github.com/alexcepoi/scrapy/commit/902208ca58aa99bacea57488642e0ea4129bd180. @alexcepoi any ideas?
",kmike,alexcepoi
396,2016-11-29 14:07:34,"Hm, I'm pretty new to embedded shell and all.
@eliasdorneles worked around the IPython bits you reference (https://github.com/scrapy/scrapy/pull/856#issuecomment-52560693) so he may have an idea",redapple,eliasdorneles
388,2013-09-16 23:00:16,"A couple of comments:
1. I like the proposed functionality change, it's something that we discussed internally some time ago and I'm happy to see it proposed by someone from the community.
2. ""scrapy deploy"" is indeed deprecated and the changes should be done on scrapyd-deploy (please submit a pull request on scrapyd project). Also, scrapyd-deploy still lacks this feature.
3. @kmike is right in that the scrapy project `setup.py` must not be overwritten. Even though packaging here is a mechanism for deployment, project must be able to customize how packaging is done (by writing their own `setup.py`) for example when you need to deploy resources/static files.

My best suggestion here to implement this functionality is to modify the generated egg changing the scrapy settings entry_point to the desired value. The egg is a zip file and the entry point is in the file `EGG-INFO/entry_points.txt`, which has something like this:



So we need to open the egg archive, modify that file and save it back. This can all be done with the Python `zipfile` module, I'd expect. I know, it's ugly but so are eggs, and python packaging in general (according to common consensus).
",pablohoffman,kmike
376,2013-09-04 00:09:40,"Yes, looking great. Thanks @dangra, thanks @alexcepoi!
",nyov,alexcepoi
372,2013-08-21 15:12:34,"All kudos are to @alexcepoi, he let me submit a PR :)
",kmike,alexcepoi
370,2013-08-21 16:58:15,"@redapple and @whodatninja worked on the same problem in #250, what do you think guys?

I like this patch keeps compatibility for ImagesPipeline API and add tests for FilesPipelines.

#250 also adds some extra settings and functionality to ImagesPipeline that are better covered by an extra patch.
",dangra,redapple
370,2013-08-21 16:58:15,"@redapple and @whodatninja worked on the same problem in #250, what do you think guys?

I like this patch keeps compatibility for ImagesPipeline API and add tests for FilesPipelines.

#250 also adds some extra settings and functionality to ImagesPipeline that are better covered by an extra patch.
",dangra,whodatninja
366,2013-08-14 05:00:54,"Since it's not documented, can someone (@kalessin @dangra?)  explain what's the purpose of this new item exporter?. Thanks.
",pablohoffman,dangra
353,2014-07-24 22:37:20,"@dangra Tests added and a behavior fix. Do you want to add the @arijitchakraborty SEP from #487 to the sep folder?
",nramirezuy,arijitchakraborty
353,2014-07-24 22:37:20,"@dangra Tests added and a behavior fix. Do you want to add the @arijitchakraborty SEP from #487 to the sep folder?
",nramirezuy,dangra
352,2013-07-18 15:33:07,"@kmike have you any comment? feel free to merge if this meet your criteria on how cookies are supposed to work. thanks
",dangra,kmike
343,2013-12-20 16:09:50,"LGTM

/cc @pablohoffman feel free to merge if you are ok.
",dangra,pablohoffman
339,2013-08-19 21:46:05,"As discussed with @dangra, migrated all commands to the new CrawlerProcess, deprecated `Command.crawler` property and improved backwards compatibility.

Other thoughts?

Not to be merged yet, tests may need fixing.
",alexcepoi,dangra
331,2013-10-08 12:39:29,"Is this compatible with #395 (soon to be merged by @dangra) ?
",pablohoffman,dangra
331,2016-01-27 13:49:39,"@nramirezuy , @dangra , @kmike , @eliasdorneles ,
should this be moved to https://github.com/scrapy/parsel ?
",redapple,eliasdorneles
330,2013-06-27 18:07:19,"@pablohoffman  fixed
",nramirezuy,pablohoffman
322,2013-06-18 16:27:36,"@pablohoffman  updated
",nramirezuy,pablohoffman
296,2013-04-26 19:33:32,"#297 takes a different approach by adding a garbage collector to the downloader, which is more efficient for vertical crawls (no need to check for slot status on each request), and not too bad for broad crawls.

@dangra and our preference is that one
",pablohoffman,dangra
295,2013-04-25 15:04:45,"+1

@dangra: yesterday said,  but beware of not breaking download delays by removing inactive slots too fast.
",nramirezuy,dangra
263,2015-07-14 16:29:31,"There's a comprehensive status of the twisted dependencies in Berker's proposal. @berkerpeksag, would you mind if we put it up on our wiki for reference?
",curita,berkerpeksag
263,2015-07-29 13:53:20,"@ianozsvald I know your pain; Scrapy is the only reason I'm using Python 2 now :) At EuroPython me and @dangra tried to unblock the further porting - the bottleneck was in Request and Response objects, and they are ported now in https://github.com/scrapy/scrapy/pull/1384. It is still a long road to full Python 3 support, but we're in a much better shape now - ~~480~~ 507 tests are passing in Python 3, compared to 248 before the sprint. Working Request and Response objects open a gate for other's contributions, so I expect Python 3 Scrapy support to get more love soon.
",kmike,dangra
250,2013-02-15 19:41:20,"Proposition to have a generic pipeline for files, `FileMediaPipeline` (as suggested by @dangra on IRC)

Enabled using an item with `file_urls` and `files` fields (similar to what is done in `ImagesPipeline`).
File system storage is specified a with new setting, `FILE_STORE` (again, like in `ImagesPipeline`)

`ImagesPipeline` is refactored using the new pipeline and has some new settings/options:
- `IMAGES_ORIGINAL_SAVE` (default `False`): saves a copy of the original image, with original extension
- `IMAGES_ORIGINAL_CONVERT` (default `True`): to turn off the conversion of the original image to JPEG
- `IMAGES_REPORT_DIMENSIONS` (default `False`): to add `width` and `height` in `image_info_or_error` result dict

This is not intended as a final implementation, but a starting point for discussion.
Note: I have tested this only with file system storage.
",redapple,dangra
249,2013-02-15 12:38:09,"I understand and sympathize with the motivation behind this change.

However, I think the change, as it is, would leave the log way too verbose (more than it already is).

We could do something similar to the offsite middleware that only logs the first request filtered to any given domain (but doesn't repeat domains). In this case, we could log the first N ones, up to N ones per minute, or something more sophisticated?

I would like to hear what other think. @dangra @shane42 ?
",pablohoffman,dangra
233,2013-02-27 06:21:34,"@llonchj can you confirm we should close this PR and continue on the new scrapyd repo?. (@dangra is on holidays now). Thanks!
",pablohoffman,dangra
198,2013-01-07 15:52:21,"Why this PR mix changes to items, exporter **and http proxy**? 

As said by @pablohoffman, this functionality can be provided by simple extending `.keys()`. I don't see a need to add yet another method to do the same. 
",dangra,pablohoffman
167,2012-08-29 16:38:11,"@pablohoffman I incorporated the changes

still missing: pretty printing errors and documentation
Is there anything else I missed?
",alexcepoi,pablohoffman
165,2012-08-10 03:03:26,"I was looking at the queues implementation and started to play with them, then I realized this improvement was nice.

Test script to test performance: https://gist.github.com/3310581 (include old and patched queue)

On 1 M items:



On 10M items:



Improvement:
- FifoMemoryQueue on push ~4x
- LifoMemoryQueue on push ~4x

_UPDATE:_ @dangra point me to test the case of the empty queue, now include them.
",andrix,dangra
157,2012-09-03 17:02:16,"@dangra what do you think?
",pablohoffman,dangra
92,2012-02-23 19:31:28,"I'm on OS X 10.7.3 with python 2.7.2+, boto 2.2.2, and scrapy 0.14.1

When I try to store an image on s3 with the images pipeline I get this S3 Response Error:

`""Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed.""`

Often, the first couple stores will create keys in the bucket, but most of the files have 0 bytes, or if they are not empty, they are garbled and not in a jpg format. 

I found that if I changed line 124 of the images pipeline to not use twisted threads, it works fine.
From:
`return threads.deferToThread(k.set_contents_from_file, buf, \
                headers=self.HEADERS, policy=self.POLICY)`

To: 
`k.set_contents_from_file( buf, headers=self.HEADERS, policy=self.POLICY)`

I talked to @dangra in irc and he suggested I comment out lines 258 and 257 with the reasoning that ""I  am afraid that seek(0) runs in main thread while persist_image does the same in another thread""
https://github.com/scrapy/scrapy/blob/0.14.1/scrapy/contrib/pipeline/images.py#L257
",ryross,dangra
50,2016-12-07 09:02:33,"@redapple can you reopen this? The problem seems to have resurfaced recently, it happens in 1.0.3 and i didn't see any changes touching this in recent releases.",immerrr,redapple
45,2013-10-14 17:40:45,"@dangra Can we close this ticket? on https://github.com/scrapy/scrapy/issues/392 you said this is deprecated.
",nramirezuy,dangra
33,2015-04-17 22:49:13,"any interest in resurrecting this one for Scrapy 1.0 @nramirezuy @kmike @dangra @Curita @redapple ?
",pablohoffman,redapple
33,2015-04-17 22:49:13,"any interest in resurrecting this one for Scrapy 1.0 @nramirezuy @kmike @dangra @Curita @redapple ?
",pablohoffman,dangra
33,2015-04-17 22:49:13,"any interest in resurrecting this one for Scrapy 1.0 @nramirezuy @kmike @dangra @Curita @redapple ?
",pablohoffman,kmike
28,2014-03-08 00:26:58,"Moved to [scrapyd issue 40](https://github.com/scrapy/scrapyd/issues/40).

Could you please close this, @pablohoffman? Actually, it might be cleaner if you (or someone else with access) copies the context of this issue to that one, as it would reduce the amount of indirection. 
",aspidites,pablohoffman
